{"created":"2024-02-08 18:59:53","title":"InstaGen: Enhancing Object Detection by Training on Synthetic Dataset","abstract":"In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models. Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images. The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector. This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection. We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios.","sentences":["In this paper, we introduce a novel paradigm to enhance the ability of object detector, e.g., expanding categories or improving detection performance, by training on synthetic dataset generated from diffusion models.","Specifically, we integrate an instance-level grounding head into a pre-trained, generative diffusion model, to augment it with the ability of localising arbitrary instances in the generated images.","The grounding head is trained to align the text embedding of category names with the regional visual feature of the diffusion model, using supervision from an off-the-shelf object detector, and a novel self-training scheme on (novel) categories not covered by the detector.","This enhanced version of diffusion model, termed as InstaGen, can serve as a data synthesizer for object detection.","We conduct thorough experiments to show that, object detector can be enhanced while training on the synthetic dataset from InstaGen, demonstrating superior performance over existing state-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to 5.2 AP) scenarios."],"url":"http://arxiv.org/abs/2402.05937v1","category":"cs.CV"}
{"created":"2024-02-08 18:59:52","title":"Closure properties in positively decreasing and related distributions under dependence","abstract":"We consider closure properties in the class of positively decreasing distributions. Our results stem from different types of dependence, but each type belongs in the family of asymptotically independent dependence structure. Namely we examine the closure property with respect to minimum, maximum, convolution product and convolution. Furthermore, we take into account some closure properties of the class of generalized subexponential positively decreasing distributions, as also we introduce and study the class of the generalized long-tailed positively decreasing distributions. Also we revisited the (independent) convolution closure problem of subexponentiality, in the case of subexponential positively decreasing class. In some classes we discuss the closedness of randomly stopped sums. In the last section we study the closure property with respect to minimum for two classes of random vectors.","sentences":["We consider closure properties in the class of positively decreasing distributions.","Our results stem from different types of dependence, but each type belongs in the family of asymptotically independent dependence structure.","Namely we examine the closure property with respect to minimum, maximum, convolution product and convolution.","Furthermore, we take into account some closure properties of the class of generalized subexponential positively decreasing distributions, as also we introduce and study the class of the generalized long-tailed positively decreasing distributions.","Also we revisited the (independent) convolution closure problem of subexponentiality, in the case of subexponential positively decreasing class.","In some classes we discuss the closedness of randomly stopped sums.","In the last section we study the closure property with respect to minimum for two classes of random vectors."],"url":"http://arxiv.org/abs/2402.05936v1","category":"math.PR"}
{"created":"2024-02-08 18:59:48","title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models","abstract":"We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. Code and models are released at https://github.com/Alpha-VLLM/LLaMA2-Accessory","sentences":["We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX.","To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm.","To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks.","We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality.","By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities.","Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales.","Code and models are released at https://github.com/Alpha-VLLM/LLaMA2-Accessory"],"url":"http://arxiv.org/abs/2402.05935v1","category":"cs.CV"}
{"created":"2024-02-08 18:59:05","title":"Time Series Diffusion in the Frequency Domain","abstract":"Fourier analysis has been an instrumental tool in the development of signal processing. This leads us to wonder whether this framework could similarly benefit generative modelling. In this paper, we explore this question through the scope of time series diffusion models. More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models. By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components. Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain. This results in frequency diffusion models, which we compare to canonical time diffusion models. Our empirical evaluation on real-world datasets, covering various domains like healthcare and finance, shows that frequency diffusion models better capture the training distribution than time diffusion models. We explain this observation by showing that time series from these datasets tend to be more localized in the frequency domain than in the time domain, which makes them easier to model in the former case. All our observations point towards impactful synergies between Fourier analysis and diffusion models.","sentences":["Fourier analysis has been an instrumental tool in the development of signal processing.","This leads us to wonder whether this framework could similarly benefit generative modelling.","In this paper, we explore this question through the scope of time series diffusion models.","More specifically, we analyze whether representing time series in the frequency domain is a useful inductive bias for score-based diffusion models.","By starting from the canonical SDE formulation of diffusion in the time domain, we show that a dual diffusion process occurs in the frequency domain with an important nuance: Brownian motions are replaced by what we call mirrored Brownian motions, characterized by mirror symmetries among their components.","Building on this insight, we show how to adapt the denoising score matching approach to implement diffusion models in the frequency domain.","This results in frequency diffusion models, which we compare to canonical time diffusion models.","Our empirical evaluation on real-world datasets, covering various domains like healthcare and finance, shows that frequency diffusion models better capture the training distribution than time diffusion models.","We explain this observation by showing that time series from these datasets tend to be more localized in the frequency domain than in the time domain, which makes them easier to model in the former case.","All our observations point towards impactful synergies between Fourier analysis and diffusion models."],"url":"http://arxiv.org/abs/2402.05933v1","category":"cs.LG"}
{"created":"2024-02-08 18:59:03","title":"Driving Everywhere with Large Language Model Policy Adaptation","abstract":"Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations. LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook. Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations. We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics. Please check our website for more details: https://boyiliee.github.io/llada.","sentences":["Adapting driving behavior to new environments, customs, and laws is a long-standing problem in autonomous driving, precluding the widespread deployment of autonomous vehicles (AVs).","In this paper, we present LLaDA, a simple yet powerful tool that enables human drivers and autonomous vehicles alike to drive everywhere by adapting their tasks and motion plans to traffic rules in new locations.","LLaDA achieves this by leveraging the impressive zero-shot generalizability of large language models (LLMs) in interpreting the traffic rules in the local driver handbook.","Through an extensive user study, we show that LLaDA's instructions are useful in disambiguating in-the-wild unexpected situations.","We also demonstrate LLaDA's ability to adapt AV motion planning policies in real-world datasets; LLaDA outperforms baseline planning approaches on all our metrics.","Please check our website for more details: https://boyiliee.github.io/llada."],"url":"http://arxiv.org/abs/2402.05932v1","category":"cs.RO"}
{"created":"2024-02-08 18:58:02","title":"An Interactive Agent Foundation Model","abstract":"The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications. We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks. Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework. We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare. Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area. The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning. Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems.","sentences":["The development of artificial intelligence systems is transitioning from creating static, task-specific models to dynamic, agent-based systems capable of performing well in a wide range of applications.","We propose an Interactive Agent Foundation Model that uses a novel multi-task agent training paradigm for training AI agents across a wide range of domains, datasets, and tasks.","Our training paradigm unifies diverse pre-training strategies, including visual masked auto-encoders, language modeling, and next-action prediction, enabling a versatile and adaptable AI framework.","We demonstrate the performance of our framework across three separate domains -- Robotics, Gaming AI, and Healthcare.","Our model demonstrates its ability to generate meaningful and contextually relevant outputs in each area.","The strength of our approach lies in its generality, leveraging a variety of data sources such as robotics sequences, gameplay data, large-scale video datasets, and textual information for effective multimodal and multi-task learning.","Our approach provides a promising avenue for developing generalist, action-taking, multimodal systems."],"url":"http://arxiv.org/abs/2402.05929v1","category":"cs.AI"}
{"created":"2024-02-08 18:58:02","title":"WebLINX: Real-World Website Navigation with Multi-Turn Dialogue","abstract":"We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion. To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation. Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios. Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time. To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements. We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web. Our experiments span from small text-only to proprietary multimodal LLMs. We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots. However, all finetuned models struggle to generalize to unseen websites. Our findings highlight the need for large multimodal models that can generalize to novel settings. Our code, data and models are available for research: https://mcgill-nlp.github.io/weblinx","sentences":["We propose the problem of conversational web navigation, where a digital agent controls a web browser and follows user instructions to solve real-world tasks in a multi-turn dialogue fashion.","To support this problem, we introduce WEBLINX - a large-scale benchmark of 100K interactions across 2300 expert demonstrations of conversational web navigation.","Our benchmark covers a broad range of patterns on over 150 real-world websites and can be used to train and evaluate agents in diverse scenarios.","Due to the magnitude of information present, Large Language Models (LLMs) cannot process entire web pages in real-time.","To solve this bottleneck, we design a retrieval-inspired model that efficiently prunes HTML pages by ranking relevant elements.","We use the selected elements, along with screenshots and action history, to assess a variety of models for their ability to replicate human behavior when navigating the web.","Our experiments span from small text-only to proprietary multimodal LLMs.","We find that smaller finetuned decoders surpass the best zero-shot LLMs (including GPT-4V), but also larger finetuned multimodal models which were explicitly pretrained on screenshots.","However, all finetuned models struggle to generalize to unseen websites.","Our findings highlight the need for large multimodal models that can generalize to novel settings.","Our code, data and models are available for research: https://mcgill-nlp.github.io/weblinx"],"url":"http://arxiv.org/abs/2402.05930v1","category":"cs.CL"}
{"created":"2024-02-08 18:57:42","title":"Sharp Rates in Dependent Learning Theory: Avoiding Sample Size Deflation for the Square Loss","abstract":"In this work, we study statistical learning with dependent ($\\beta$-mixing) data and square loss in a hypothesis class $\\mathscr{F}\\subset L_{\\Psi_p}$ where $\\Psi_p$ is the norm $\\|f\\|_{\\Psi_p} \\triangleq \\sup_{m\\geq 1} m^{-1/p} \\|f\\|_{L^m} $ for some $p\\in [2,\\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data. Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \\emph{multiplicatively} by the mixing time of the underlying covariates process. We show that whenever the topologies of $L^2$ and $\\Psi_p$ are comparable on our hypothesis class $\\mathscr{F}$ -- that is, $\\mathscr{F}$ is a weakly sub-Gaussian class: $\\|f\\|_{\\Psi_p} \\lesssim \\|f\\|_{L^2}^\\eta$ for some $\\eta\\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term. Our result holds whether the problem is realizable or not and we refer to this as a \\emph{near mixing-free rate}, since direct dependence on mixing is relegated to an additive higher order term. We arrive at our result by combining the above notion of a weakly sub-Gaussian class with mixed tail generic chaining. This combination allows us to compute sharp, instance-optimal rates for a wide range of problems. %Our approach, reliant on mixed tail generic chaining, allows us to obtain sharp, instance-optimal rates. Examples that satisfy our framework include sub-Gaussian linear regression, more general smoothly parameterized function classes, finite hypothesis classes, and bounded smoothness classes.","sentences":["In this work, we study statistical learning with dependent ($\\beta$-mixing) data and square loss in a hypothesis class $\\mathscr{F}\\subset L_{\\Psi_p}$ where $\\Psi_p$ is the norm $\\|f\\|_{\\Psi_p} \\triangleq \\sup_{m\\geq 1} m^{-1/p} \\|f\\|_{L^m} $ for some $p\\in [2,\\infty]$. Our inquiry is motivated by the search for a sharp noise interaction term, or variance proxy, in learning with dependent data.","Absent any realizability assumption, typical non-asymptotic results exhibit variance proxies that are deflated \\emph{multiplicatively} by the mixing time of the underlying covariates process.","We show that whenever the topologies of $L^2$ and $\\Psi_p$ are comparable on our hypothesis class $\\mathscr{F}$ -- that is, $\\mathscr{F}$ is a weakly sub-Gaussian class: $\\|f\\|_{\\Psi_p} \\lesssim \\|f\\|_{L^2}^\\eta$ for some $\\eta\\in (0,1]$ -- the empirical risk minimizer achieves a rate that only depends on the complexity of the class and second order statistics in its leading term.","Our result holds whether the problem is realizable or not and we refer to this as a \\emph{near mixing-free rate}, since direct dependence on mixing is relegated to an additive higher order term.","We arrive at our result by combining the above notion of a weakly sub-Gaussian class with mixed tail generic chaining.","This combination allows us to compute sharp, instance-optimal rates for a wide range of problems.","%Our approach, reliant on mixed tail generic chaining, allows us to obtain sharp, instance-optimal rates.","Examples that satisfy our framework include sub-Gaussian linear regression, more general smoothly parameterized function classes, finite hypothesis classes, and bounded smoothness classes."],"url":"http://arxiv.org/abs/2402.05928v1","category":"cs.LG"}
{"created":"2024-02-08 18:56:45","title":"Yamabe metrics on conical manifolds","abstract":"We prove existence of Yamabe metrics on singular manifolds with conical points and conical links of Einstein type that include orbifold structures. We deal with metrics of generic type and derive a counterpart of Aubin's classical result. Interestingly, the singular nature of the metric determines a different condition on the dimension, compared to the regular case. We derive asymptotic expansions on the Yamabe quotient by adding a proper and implicit lower-order correction to standard bubbles, whose contribution to the expansion of the quotient can be determined combining the decomposition of symmetric 2-tensor fields and Fourier analysis on the conical links.","sentences":["We prove existence of Yamabe metrics on singular manifolds with conical points and conical links of Einstein type that include orbifold structures.","We deal with metrics of generic type and derive a counterpart of Aubin's classical result.","Interestingly, the singular nature of the metric determines a different condition on the dimension, compared to the regular case.","We derive asymptotic expansions on the Yamabe quotient by adding a proper and implicit lower-order correction to standard bubbles, whose contribution to the expansion of the quotient can be determined combining the decomposition of symmetric 2-tensor fields and Fourier analysis on the conical links."],"url":"http://arxiv.org/abs/2402.05927v1","category":"math.DG"}
{"created":"2024-02-08 18:56:22","title":"YOLO-CIANNA: Galaxy detection with deep learning in radio data. I. A new YOLO-inspired source detection method applied to the SKAO SDC1","abstract":"The upcoming Square Kilometer Array (SKA) will set a new standard regarding data volume generated by an astronomical instrument, which is likely to challenge widely adopted data analysis tools that scale inadequately with the data size. This study aims to develop a new source detection and characterization method for massive radio astronomical datasets by adapting modern deep-learning object detection techniques. These approaches have proved their efficiency on complex computer vision tasks, and we seek to identify their specific strengths and weaknesses when applied to astronomical data. We introduce YOLO-CIANNA, a highly customized deep-learning object detector designed specifically for astronomical datasets. This paper presents the method and describes all the low-level adaptations required to address the specific challenges of radio-astronomical images. We demonstrate this method's capabilities using simulated 2D continuum images from the SKAO SDC1 dataset. Our method outperforms every other published result on the specific SDC1 dataset. Using the SDC1 metric, we improve the challenge-winning score by +139\\% and the score of the only other post-challenge participation by +61\\%. Our catalog has a detection purity of 94\\% while detecting 40 to 60 \\% more sources than previous top-score results. The trained model can also be forced to reach 99\\% purity in post-process and still detect 10 to 30\\% more sources than the other top-score methods. It is also capable of real-time detection, with a peak prediction speed of 500 images of 512x512 pixels per second on a single GPU. YOLO-CIANNA achieves state-of-the-art detection and characterization results on the simulated SDC1 dataset. The method is open source and included in the wider CIANNA framework. We provide scripts to train and apply this method to the SDC1 dataset in the CIANNA repository.","sentences":["The upcoming Square Kilometer Array (SKA) will set a new standard regarding data volume generated by an astronomical instrument, which is likely to challenge widely adopted data analysis tools that scale inadequately with the data size.","This study aims to develop a new source detection and characterization method for massive radio astronomical datasets by adapting modern deep-learning object detection techniques.","These approaches have proved their efficiency on complex computer vision tasks, and we seek to identify their specific strengths and weaknesses when applied to astronomical data.","We introduce YOLO-CIANNA, a highly customized deep-learning object detector designed specifically for astronomical datasets.","This paper presents the method and describes all the low-level adaptations required to address the specific challenges of radio-astronomical images.","We demonstrate this method's capabilities using simulated 2D continuum images from the SKAO SDC1 dataset.","Our method outperforms every other published result on the specific SDC1 dataset.","Using the SDC1 metric, we improve the challenge-winning score by +139\\% and the score of the only other post-challenge participation by +61\\%.","Our catalog has a detection purity of 94\\% while detecting 40 to 60 \\% more sources than previous top-score results.","The trained model can also be forced to reach 99\\% purity in post-process and still detect 10 to 30\\% more sources than the other top-score methods.","It is also capable of real-time detection, with a peak prediction speed of 500 images of 512x512 pixels per second on a single GPU.","YOLO-CIANNA achieves state-of-the-art detection and characterization results on the simulated SDC1 dataset.","The method is open source and included in the wider CIANNA framework.","We provide scripts to train and apply this method to the SDC1 dataset in the CIANNA repository."],"url":"http://arxiv.org/abs/2402.05925v1","category":"astro-ph.IM"}
{"created":"2024-02-08 18:53:35","title":"On the Use of Decision Tree Regression for Predicting Vibration Frequency Response of Handheld Probes","abstract":"This article focuses on the prediction of the vibration frequency response of handheld probes. A novel approach that involves machine learning and readily available data from probes was explored. Vibration probes are efficient and affordable devices that provide information about testing airborne sound insulation in building acoustics. However, fixing a probe to a vibrating surface downshifts sensor resonances and underestimates levels. Therefore, the calibration response of the sensor included in a probe differs from the frequency response of that same probe. Simulation techniques of complex mechanical systems may describe this issue, but they include hardly obtainable parameters, ultimately restricting the model. Thus, this study discusses an alternative method, which comprises different parts. Firstly, the vibration frequency responses of 85 probes were measured and labelled according to six features. Then, Linear Regression, Decision Tree Regression and Artificial Neural Networks algorithms were analysed. It was revealed that decision tree regression is the more appropriate technique for this data. The best decision tree models, in terms of scores and model structure, were fine-tuned. Eventually, the final suggested model employs only four out of the six original features. A trade-off solution that involved a simple structure, an interpretable model and accurate predictions was accomplished. It showed a maximum average deviation from test measurements ranging from 0.6 dB in low-frequency to 3 dB in high-frequency while remaining at a low computational load. This research developed an original and reliable prediction tool that provides the vibration frequency response of handheld probes.","sentences":["This article focuses on the prediction of the vibration frequency response of handheld probes.","A novel approach that involves machine learning and readily available data from probes was explored.","Vibration probes are efficient and affordable devices that provide information about testing airborne sound insulation in building acoustics.","However, fixing a probe to a vibrating surface downshifts sensor resonances and underestimates levels.","Therefore, the calibration response of the sensor included in a probe differs from the frequency response of that same probe.","Simulation techniques of complex mechanical systems may describe this issue, but they include hardly obtainable parameters, ultimately restricting the model.","Thus, this study discusses an alternative method, which comprises different parts.","Firstly, the vibration frequency responses of 85 probes were measured and labelled according to six features.","Then, Linear Regression, Decision Tree Regression and Artificial Neural Networks algorithms were analysed.","It was revealed that decision tree regression is the more appropriate technique for this data.","The best decision tree models, in terms of scores and model structure, were fine-tuned.","Eventually, the final suggested model employs only four out of the six original features.","A trade-off solution that involved a simple structure, an interpretable model and accurate predictions was accomplished.","It showed a maximum average deviation from test measurements ranging from 0.6 dB in low-frequency to 3 dB in high-frequency while remaining at a low computational load.","This research developed an original and reliable prediction tool that provides the vibration frequency response of handheld probes."],"url":"http://arxiv.org/abs/2402.05921v1","category":"physics.app-ph"}
{"created":"2024-02-08 18:53:22","title":"Simple Representations of BPS Algebras: the case of $Y(\\widehat{\\mathfrak{gl}}_2)$","abstract":"BPS algebras are the symmetries of a wide class of brane-inspired models. They are closely related to Yangians -- the peculiar and somewhat sophisticated limit of DIM algebras. Still they possess some simple and explicit representations. We explain here that for $Y(\\widehat{\\mathfrak{gl}}_r)$ these representations are related to Uglov polynomials, whose families are also labeled by natural $r$. They arise in the limit $\\hbar\\longrightarrow 0$ from Macdonald polynomials, and generalize the well-known Jack polynomials ($\\beta$-deformation of Schur functions), associated with $r=1$. For $r=2$ they approximate Macdonald polynomials with the accuracy $O(\\hbar^2)$, so that they are eigenfunctions of {\\it two} immediately available commuting operators, arising from the $\\hbar$-expansion of the first Macdonald Hamiltonian. These operators have a clear structure, which is easily generalizable, -- what provides a technically simple way to build an explicit representation of Yangian $Y(\\widehat{\\mathfrak{gl}}_2)$, where $U^{(2)}$ are associated with the states $|\\lambda\\rangle$, parametrized by chess-colored Young diagrams. An interesting feature of this representation is that the odd time-variables $p_{2n+1}$ can be expressed through mutually commuting operators from Yangian, however even time-variables $p_{2n}$ are inexpressible. Implications to higher $r$ become now straightforward, yet we describe them only in a sketchy way.","sentences":["BPS algebras are the symmetries of a wide class of brane-inspired models.","They are closely related to Yangians -- the peculiar and somewhat sophisticated limit of DIM algebras.","Still they possess some simple and explicit representations.","We explain here that for $Y(\\widehat{\\mathfrak{gl}}_r)$ these representations are related to Uglov polynomials, whose families are also labeled by natural $r$. They arise in the limit $\\hbar\\longrightarrow 0$ from Macdonald polynomials, and generalize the well-known Jack polynomials ($\\beta$-deformation of Schur functions), associated with $r=1$. For $r=2$ they approximate Macdonald polynomials with the accuracy $O(\\hbar^2)$, so that they are eigenfunctions of {\\it two} immediately available commuting operators, arising from the $\\hbar$-expansion of the first Macdonald Hamiltonian.","These operators have a clear structure, which is easily generalizable, -- what provides a technically simple way to build an explicit representation of Yangian $Y(\\widehat{\\mathfrak{gl}}_2)$, where $U^{(2)}$ are associated with the states $|\\lambda\\rangle$, parametrized by chess-colored Young diagrams.","An interesting feature of this representation is that the odd time-variables $p_{2n+1}$ can be expressed through mutually commuting operators from Yangian, however even time-variables $p_{2n}$ are inexpressible.","Implications to higher $r$ become now straightforward, yet we describe them only in a sketchy way."],"url":"http://arxiv.org/abs/2402.05920v1","category":"hep-th"}
{"created":"2024-02-08 18:53:21","title":"Collaborative Control for Geometry-Conditioned PBR Image Generation","abstract":"Current 3D content generation builds on generative models that output RGB images. Modern graphics pipelines, however, require physically-based rendering (PBR) material properties. We propose to model the PBR image distribution directly to avoid photometric inaccuracies in RGB generation and the inherent ambiguity in extracting PBR from RGB. Existing paradigms for cross-modal finetuning are not suited for PBR generation due to a lack of data and the high dimensionality of the output modalities: we overcome both challenges by retaining a frozen RGB model and tightly linking a newly trained PBR model using a novel cross-network communication paradigm. As the base RGB model is fully frozen, the proposed method does not risk catastrophic forgetting during finetuning and remains compatible with techniques such as IPAdapter pretrained for the base RGB model. We validate our design choices, robustness to data sparsity, and compare against existing paradigms with an extensive experimental section.","sentences":["Current 3D content generation builds on generative models that output RGB images.","Modern graphics pipelines, however, require physically-based rendering (PBR) material properties.","We propose to model the PBR image distribution directly to avoid photometric inaccuracies in RGB generation and the inherent ambiguity in extracting PBR from RGB.","Existing paradigms for cross-modal finetuning are not suited for PBR generation due to a lack of data and the high dimensionality of the output modalities: we overcome both challenges by retaining a frozen RGB model and tightly linking a newly trained PBR model using a novel cross-network communication paradigm.","As the base RGB model is fully frozen, the proposed method does not risk catastrophic forgetting during finetuning and remains compatible with techniques such as IPAdapter pretrained for the base RGB model.","We validate our design choices, robustness to data sparsity, and compare against existing paradigms with an extensive experimental section."],"url":"http://arxiv.org/abs/2402.05919v1","category":"cs.CV"}
{"created":"2024-02-08 18:52:23","title":"Point-VOS: Pointing Up Video Object Segmentation","abstract":"Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing. This requires time-consuming and costly video annotation mechanisms. We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort. We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos. Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results. We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points. In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task. We will make our code and annotations available at https://pointvos.github.io.","sentences":["Current state-of-the-art Video Object Segmentation (VOS) methods rely on dense per-object mask annotations both during training and testing.","This requires time-consuming and costly video annotation mechanisms.","We propose a novel Point-VOS task with a spatio-temporally sparse point-wise annotation scheme that substantially reduces the annotation effort.","We apply our annotation scheme to two large-scale video datasets with text descriptions and annotate over 19M points across 133K objects in 32K videos.","Based on our annotations, we propose a new Point-VOS benchmark, and a corresponding point-based training mechanism, which we use to establish strong baseline results.","We show that existing VOS methods can easily be adapted to leverage our point annotations during training, and can achieve results close to the fully-supervised performance when trained on pseudo-masks generated from these points.","In addition, we show that our data can be used to improve models that connect vision and language, by evaluating it on the Video Narrative Grounding (VNG) task.","We will make our code and annotations available at https://pointvos.github.io."],"url":"http://arxiv.org/abs/2402.05917v1","category":"cs.CV"}
{"created":"2024-02-08 18:51:55","title":"GenEFT: Understanding Statics and Dynamics of Model Generalization via Effective Theory","abstract":"We present GenEFT: an effective theory framework for shedding light on the statics and dynamics of neural network generalization, and illustrate it with graph learning examples. We first investigate the generalization phase transition as data size increases, comparing experimental results with information-theory-based approximations. We find generalization in a Goldilocks zone where the decoder is neither too weak nor too powerful. We then introduce an effective theory for the dynamics of representation learning, where latent-space representations are modeled as interacting particles (repons), and find that it explains our experimentally observed phase transition between generalization and overfitting as encoder and decoder learning rates are scanned. This highlights the power of physics-inspired effective theories for bridging the gap between theoretical predictions and practice in machine learning.","sentences":["We present GenEFT: an effective theory framework for shedding light on the statics and dynamics of neural network generalization, and illustrate it with graph learning examples.","We first investigate the generalization phase transition as data size increases, comparing experimental results with information-theory-based approximations.","We find generalization in a Goldilocks zone where the decoder is neither too weak nor too powerful.","We then introduce an effective theory for the dynamics of representation learning, where latent-space representations are modeled as interacting particles (repons), and find that it explains our experimentally observed phase transition between generalization and overfitting as encoder and decoder learning rates are scanned.","This highlights the power of physics-inspired effective theories for bridging the gap between theoretical predictions and practice in machine learning."],"url":"http://arxiv.org/abs/2402.05916v1","category":"cs.LG"}
{"created":"2024-02-08 18:51:42","title":"Cosmic-ray induced ionization rates and non-thermal emissions from nuclei of starburst galaxies","abstract":"Cosmic rays are the only agent capable of ionizing the interior of dense molecular clouds and, thus, they are believed to play an essential role in determining the physical and chemical evolution of star-forming regions. In this work, we aim to study cosmic-ray induced ionization rates in starburst environments using non-thermal emissions of cosmic rays from starburst nuclei. To this end, we first revisit cosmic-ray models which could explain data of non-thermal emissions from radio to X-ray and gamma-ray from nuclei of three prototypical starburst galaxies NGC 253, M82, and Arp 220. These models are then applied to predict ionization rates in starburst environments which gives values around $10^{-14}$ s$^{-1}$. Such a high value of the ionization rate, which is 2 to 3 orders of magnitude higher than the typical values found in the Milky Way, is probably due to relatively high rates of supernova explosions occurring within the nuclei of these starburst galaxies. We also discuss in more details the case of NGC 253 where our predicted ionization rate is found to be, in most cases, a few times smaller than the values inferred from molecular line observations of clouds in the starburst nucleus. The general framework provided in this work illustrates how the use of non-thermal emission data could help to provide more insights into ionization rates or, more generally, cosmic-ray impact in starburst environments.","sentences":["Cosmic rays are the only agent capable of ionizing the interior of dense molecular clouds and, thus, they are believed to play an essential role in determining the physical and chemical evolution of star-forming regions.","In this work, we aim to study cosmic-ray induced ionization rates in starburst environments using non-thermal emissions of cosmic rays from starburst nuclei.","To this end, we first revisit cosmic-ray models which could explain data of non-thermal emissions from radio to X-ray and gamma-ray from nuclei of three prototypical starburst galaxies NGC 253, M82, and Arp 220.","These models are then applied to predict ionization rates in starburst environments which gives values around $10^{-14}$ s$^{-1}$. Such a high value of the ionization rate, which is 2 to 3 orders of magnitude higher than the typical values found in the Milky Way, is probably due to relatively high rates of supernova explosions occurring within the nuclei of these starburst galaxies.","We also discuss in more details the case of NGC 253 where our predicted ionization rate is found to be, in most cases, a few times smaller than the values inferred from molecular line observations of clouds in the starburst nucleus.","The general framework provided in this work illustrates how the use of non-thermal emission data could help to provide more insights into ionization rates or, more generally, cosmic-ray impact in starburst environments."],"url":"http://arxiv.org/abs/2402.05915v1","category":"astro-ph.HE"}
{"created":"2024-02-08 18:47:10","title":"Towards a holistic magnetic braking model -- II: explaining several long-term internal- and surface-spin properties of solar-like stars and the Sun","abstract":"We extend our model of magnetic braking (MB) from fully convective M-dwarfs (FCMDs) to explain the surface and internal spin $P_\\mathrm{spin}$ evolution of partly convective dwarfs (PCDs) starting from disc-dispersal stage to main-sequence turnoff. In our model, the spin of the core is governed by shear at the core-envelope boundary while the spin of the envelope is governed by MB and shear. We show that (1) the most massive FCMDs experience a stronger spin-down than PCDs and less massive FCMDs; (2) the stalled spin-down and enhanced activity of K-dwarfs and the pileup of G-dwarfs older than a few Gyr are stellar-structure- and MB-dependent, and weakly dependent on core-envelope coupling effects; (3) our empirical expression of the core-envelope convergence time-scale $\\tau_\\mathrm{converge}(M_\\ast,\\,P_\\mathrm{spin})$ between a few 10 to 100 Myr is strongly dependent on stellar structure and weakly dependent on MB strength and shear, where fast and massive rotators achieve corotation earlier; (4) our estimates of the surface magnetic fields are in general agreement with observations and our wind mass loss evolution explains the weak winds from the solar analog $\\pi^1$ UMa; (5) the massive young Sun theory as a solution to the faint young Sun problem, which states that the early Sun was sufficiently more massive to maintain liquid water on Earth when the Sun's luminosity would have been about 30 percent lower, can likely be ruled out because the maximum mass lost by winds from our Sun with our model is about $0.001M_\\odot$, an order of magnitude smaller than required to solve the problem with this theory.","sentences":["We extend our model of magnetic braking (MB) from fully convective M-dwarfs (FCMDs) to explain the surface and internal spin $P_\\mathrm{spin}$ evolution of partly convective dwarfs (PCDs) starting from disc-dispersal stage to main-sequence turnoff.","In our model, the spin of the core is governed by shear at the core-envelope boundary while the spin of the envelope is governed by MB and shear.","We show that (1) the most massive FCMDs experience a stronger spin-down than PCDs and less massive FCMDs; (2) the stalled spin-down and enhanced activity of K-dwarfs and the pileup of G-dwarfs older than a few Gyr are stellar-structure- and MB-dependent, and weakly dependent on core-envelope coupling effects; (3) our empirical expression of the core-envelope convergence time-scale $\\tau_\\mathrm{converge}(M_\\ast,\\,P_\\mathrm{spin})$ between a few 10 to 100 Myr is strongly dependent on stellar structure and weakly dependent on MB strength and shear, where fast and massive rotators achieve corotation earlier; (4) our estimates of the surface magnetic fields are in general agreement with observations and our wind mass loss evolution explains the weak winds from the solar analog $\\pi^1$ UMa; (5) the massive young Sun theory as a solution to the faint young Sun problem, which states that the early Sun was sufficiently more massive to maintain liquid water on Earth when the Sun's luminosity would have been about 30 percent lower, can likely be ruled out because the maximum mass lost by winds from our Sun with our model is about $0.001M_\\odot$, an order of magnitude smaller than required to solve the problem with this theory."],"url":"http://arxiv.org/abs/2402.05912v1","category":"astro-ph.SR"}
{"created":"2024-02-08 18:44:44","title":"Evaluation of the Real-time El Ni\u00f1o Forecasts by the Climate Network Approach between 2011 and Present","abstract":"El Ni\\~no episodes are part of the El Ni\\~no-Southern Oscillation (ENSO), which is the strongest driver of interannual climate variability, and can trigger extreme weather events and disasters in various parts of the globe. Previously we have described a network approach that allows to forecast El Ni\\~no events about 1 year ahead. Here we evaluate the real-time forecasts of this approach between 2011 and 2022. We find that the approach correctly predicted (in 2013 and 2017) the onset of both El Ni\\~no periods (2014-2016 and 2018-2019) and generated only 1 false alarm in 2019. In June 2022, the approach correctly forecasted the onset of an El Ni\\~no event in 2023. We show how to determine the $p$-value of the 12 real-time forecasts between 2011 and 2022 and find $p\\cong 0.005$, this way strongly rejecting the null hypothesis that the same quality of the forecast can be obtained by random guessing. We also discuss how the algorithm can be further improved by reducing the number of false alarms in the network model forecast. When combined with other statistical methods, a more detailed forecast, including the magnitude of the event and its type, can be obtained. For 2024, the method indicates the absence of a new El Ni\\~no event.","sentences":["El Ni\\~no episodes are part of the El Ni\\~no-Southern Oscillation (ENSO), which is the strongest driver of interannual climate variability, and can trigger extreme weather events and disasters in various parts of the globe.","Previously we have described a network approach that allows to forecast El Ni\\~no events about 1 year ahead.","Here we evaluate the real-time forecasts of this approach between 2011 and 2022.","We find that the approach correctly predicted (in 2013 and 2017) the onset of both El Ni\\~no periods (2014-2016 and 2018-2019) and generated only 1 false alarm in 2019.","In June 2022, the approach correctly forecasted the onset of an El Ni\\~no event in 2023.","We show how to determine the $p$-value of the 12 real-time forecasts between 2011 and 2022 and find $p\\cong 0.005$, this way strongly rejecting the null hypothesis that the same quality of the forecast can be obtained by random guessing.","We also discuss how the algorithm can be further improved by reducing the number of false alarms in the network model forecast.","When combined with other statistical methods, a more detailed forecast, including the magnitude of the event and its type, can be obtained.","For 2024, the method indicates the absence of a new El Ni\\~no event."],"url":"http://arxiv.org/abs/2402.05911v1","category":"physics.ao-ph"}
{"created":"2024-02-08 18:43:39","title":"Transition dynamics in the $\u039b_{s}$CDM model: Implications for bound cosmic structures","abstract":"We explore the predictions of $\\Lambda_s$CDM (a novel framework that postulates a rapid anti-de Sitter (AdS) to de Sitter (dS) vacua transition in the late Universe) on bound structures. In its simplest version, the cosmological constant, $\\Lambda_s$, abruptly switches sign from negative to positive, attaining its present-day value at a redshift of ${z_\\dagger\\sim 2}$. The $\\Lambda_{\\rm s}$CDM model emerges as a promising solution to major cosmological tensions, particularly the $H_0$ and $S_8$ tensions, as well as other less definite tensions. A key aspect of our investigation is examining the impact of the abrupt transition of the $\\Lambda_s$CDM model on the formation and evolution of bound cosmic structures. We identify three primary influences: (i) the negative cosmological constant (AdS) phase for $z > z_\\dagger$, (ii) the abrupt transition marked by a type II (sudden) singularity, leading to an abrupt increase in the universe's expansion rate at $z=z_\\dagger$, and (iii) an increased expansion rate in the late universe under a positive cosmological constant for $z < z_\\dagger$, compared to $\\Lambda$CDM. Utilizing the spherical collapse model, we investigate the non-linear evolution of bound cosmic structures within the $\\Lambda_s$CDM framework. We find that the virialization process of cosmic structures, and consequently their matter overdensity, varies depending on whether the AdS-dS transition precedes or follows the turnaround. Specifically, structures virialize with either increased or reduced matter overdensity compared to the Planck-$\\Lambda$CDM model, contingent on the timing of the transition. Additionally, our results demonstrate that the sudden past singularity does not result in the dissociation of bound systems. Despite its profound nature, the singularity exerts only relatively weak effects on such systems, thereby reinforcing the model's viability in this context.","sentences":["We explore the predictions of $\\Lambda_s$CDM (a novel framework that postulates a rapid anti-de Sitter (AdS) to de Sitter (dS) vacua transition in the late Universe) on bound structures.","In its simplest version, the cosmological constant, $\\Lambda_s$, abruptly switches sign from negative to positive, attaining its present-day value at a redshift of ${z_\\dagger\\sim 2}$.","The $\\Lambda_{\\rm s}$CDM model emerges as a promising solution to major cosmological tensions, particularly the $H_0$ and $S_8$ tensions, as well as other less definite tensions.","A key aspect of our investigation is examining the impact of the abrupt transition of the $\\Lambda_s$CDM model on the formation and evolution of bound cosmic structures.","We identify three primary influences: (i) the negative cosmological constant (AdS) phase for $z > z_\\dagger$, (ii) the abrupt transition marked by a type II (sudden) singularity, leading to an abrupt increase in the universe's expansion rate at $z=z_\\dagger$, and (iii) an increased expansion rate in the late universe under a positive cosmological constant for $z < z_\\dagger$, compared to $\\Lambda$CDM.","Utilizing the spherical collapse model, we investigate the non-linear evolution of bound cosmic structures within the $\\Lambda_s$CDM framework.","We find that the virialization process of cosmic structures, and consequently their matter overdensity, varies depending on whether the AdS-dS transition precedes or follows the turnaround.","Specifically, structures virialize with either increased or reduced matter overdensity compared to the Planck-$\\Lambda$CDM model, contingent on the timing of the transition.","Additionally, our results demonstrate that the sudden past singularity does not result in the dissociation of bound systems.","Despite its profound nature, the singularity exerts only relatively weak effects on such systems, thereby reinforcing the model's viability in this context."],"url":"http://arxiv.org/abs/2402.05908v1","category":"astro-ph.CO"}
{"created":"2024-02-08 18:43:32","title":"Transverse Geometry of Lorentzian foliations with applications to Lorentzian orbifolds","abstract":"We prove a transverse diameter theorem in the context of Lorentzian foliations, which can be interpreted as a Hawking--Penrose-type singularity theorem for timelike geodesics transverse to the foliation. In order to develop the necessary machinery we introduce and study a novel causality structure on the leaf space via the transverse Lorentzian geometry on the foliated manifold. We describe the initial rungs of a transverse causal ladder and relate them to their standard counterparts on an underlying foliated spacetime. We show how these results can be interpreted as doing Lorentzian (and more generally semi-Riemannian) geometry on low-regularity spaces that can be realized as leaf spaces of foliations. Accordingly, we discuss how all of these concepts and results apply to Lorentzian orbifolds, insofar as these can be seen as leaf spaces of a specific class of Lorentzian foliations. In particular, we derive an associated Lorentzian timelike diameter theorem on orbifolds.","sentences":["We prove a transverse diameter theorem in the context of Lorentzian foliations, which can be interpreted as a Hawking--Penrose-type singularity theorem for timelike geodesics transverse to the foliation.","In order to develop the necessary machinery we introduce and study a novel causality structure on the leaf space via the transverse Lorentzian geometry on the foliated manifold.","We describe the initial rungs of a transverse causal ladder and relate them to their standard counterparts on an underlying foliated spacetime.","We show how these results can be interpreted as doing Lorentzian (and more generally semi-Riemannian) geometry on low-regularity spaces that can be realized as leaf spaces of foliations.","Accordingly, we discuss how all of these concepts and results apply to Lorentzian orbifolds, insofar as these can be seen as leaf spaces of a specific class of Lorentzian foliations.","In particular, we derive an associated Lorentzian timelike diameter theorem on orbifolds."],"url":"http://arxiv.org/abs/2402.05907v1","category":"math.DG"}
{"created":"2024-02-08 18:43:27","title":"Risk-Sensitive Multi-Agent Reinforcement Learning in Network Aggregative Markov Games","abstract":"Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents. However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem. This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies. In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk. CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities. We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC. Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion of Markov perfect Nash equilibrium in NAMGs. The experimental results show that subjective CPT policies obtained by our algorithm can be different from the risk-neutral ones, and agents with a higher loss aversion are more inclined to socially isolate themselves in an NAMG.","sentences":["Classical multi-agent reinforcement learning (MARL) assumes risk neutrality and complete objectivity for agents.","However, in settings where agents need to consider or model human economic or social preferences, a notion of risk must be incorporated into the RL optimization problem.","This will be of greater importance in MARL where other human or non-human agents are involved, possibly with their own risk-sensitive policies.","In this work, we consider risk-sensitive and non-cooperative MARL with cumulative prospect theory (CPT), a non-convex risk measure and a generalization of coherent measures of risk.","CPT is capable of explaining loss aversion in humans and their tendency to overestimate/underestimate small/large probabilities.","We propose a distributed sampling-based actor-critic (AC) algorithm with CPT risk for network aggregative Markov games (NAMGs), which we call Distributed Nested CPT-AC.","Under a set of assumptions, we prove the convergence of the algorithm to a subjective notion of Markov perfect Nash equilibrium in NAMGs.","The experimental results show that subjective CPT policies obtained by our algorithm can be different from the risk-neutral ones, and agents with a higher loss aversion are more inclined to socially isolate themselves in an NAMG."],"url":"http://arxiv.org/abs/2402.05906v1","category":"cs.LG"}
{"created":"2024-02-08 18:43:12","title":"Stable and Hurwitz slices, a degree principle and a generalized Grace-Walsh-Szeg\u0151 theorem","abstract":"Univariate polynomials are called stable with respect to a circular region $\\mathcal{A}$, if all of their roots are in $\\mathcal{A}$. We consider the special case where $\\mathcal{A}$ is a half-plane and investigate affine slices of the set of stable polynomials. In this setup, we show that an affine slice of codimension $k$ always contains a stable polynomial that possesses at most $2(k+2)$ distinct roots on the boundary and at most $(k+2)$ distinct roots in the interior of $\\mathcal{A}$. This result also extends to affine slices of weakly Hurwitz polynomials, i.e. real, univariate, left half-plane stable polynomials. Subsequently, we apply these results to symmetric polynomials and varieties. Here we show that a variety described by polynomials in few multiaffine polynomials has no root in $\\mathcal{A}^n$, if and only if it has no root in $\\mathcal{A}^n$ with few distinct coordinates. This is at the same time a generalization of the degree principle to stable polynomials and a generalization of Grace-Walsh-Szeg\\H{o}'s coincidence theorem.","sentences":["Univariate polynomials are called stable with respect to a circular region $\\mathcal{A}$, if all of their roots are in $\\mathcal{A}$. We consider the special case where $\\mathcal{A}$ is a half-plane and investigate affine slices of the set of stable polynomials.","In this setup, we show that an affine slice of codimension $k$ always contains a stable polynomial that possesses at most $2(k+2)$ distinct roots on the boundary and at most $(k+2)$ distinct roots in the interior of $\\mathcal{A}$. This result also extends to affine slices of weakly Hurwitz polynomials, i.e. real, univariate, left half-plane stable polynomials.","Subsequently, we apply these results to symmetric polynomials and varieties.","Here we show that a variety described by polynomials in few multiaffine polynomials has no root in $\\mathcal{A}^n$, if and only if it has no root in $\\mathcal{A}^n$ with few distinct coordinates.","This is at the same time a generalization of the degree principle to stable polynomials and a generalization of Grace-Walsh-Szeg\\H{o}'s coincidence theorem."],"url":"http://arxiv.org/abs/2402.05905v1","category":"math.AG"}
{"created":"2024-02-08 18:41:41","title":"ClickSAM: Fine-tuning Segment Anything Model using click prompts for ultrasound image segmentation","abstract":"The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design. However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images. Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures. In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images. ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts. By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated. Positive clicks are generated using the true positive and false negative segments, and negative clicks are generated using the false positive segments. The Centroidal Voronoi Tessellation algorithm is then employed to collect positive and negative click prompts in each segment that are used to enhance the model performance during the second stage of training. With click-train methods, ClickSAM exhibits superior performance compared to other existing models for ultrasound image segmentation.","sentences":["The newly released Segment Anything Model (SAM) is a popular tool used in image processing due to its superior segmentation accuracy, variety of input prompts, training capabilities, and efficient model design.","However, its current model is trained on a diverse dataset not tailored to medical images, particularly ultrasound images.","Ultrasound images tend to have a lot of noise, making it difficult to segment out important structures.","In this project, we developed ClickSAM, which fine-tunes the Segment Anything Model using click prompts for ultrasound images.","ClickSAM has two stages of training: the first stage is trained on single-click prompts centered in the ground-truth contours, and the second stage focuses on improving the model performance through additional positive and negative click prompts.","By comparing the first stage predictions to the ground-truth masks, true positive, false positive, and false negative segments are calculated.","Positive clicks are generated using the true positive and false negative segments, and negative clicks are generated using the false positive segments.","The Centroidal Voronoi Tessellation algorithm is then employed to collect positive and negative click prompts in each segment that are used to enhance the model performance during the second stage of training.","With click-train methods, ClickSAM exhibits superior performance compared to other existing models for ultrasound image segmentation."],"url":"http://arxiv.org/abs/2402.05902v1","category":"cs.CV"}
{"created":"2024-02-08 18:39:04","title":"Scalar Radiation with a Quartic Galileon","abstract":"The class of Galileon scalar fields theories encapsulate the Vainshtein screening mechanism which is characteristic of a large range of infrared modified theories of gravity. Such theories can lead to testable departures from General Relativity through fifth forces and new scalar modes of gravitational radiation. However, the inherent non-linearity of the Vainshtein mechanism has limited analytic attempts to describe Galileon theories with both cubic and quartic interactions. To improve on this, we perform direct numerical simulations of the quartic Galileon model for a rotating binary source and infer the power spectrum of given multipoles. To tame numerical instabilities we utilize a low-pass filter, extending previous work on the cubic Galileon. Our findings show that the multipole expansion is well-defined and under control. Moreover, our results confirm that despite being a non-linear scalar, the dominant Galileon radiation is quadrupole, and we find a new scaling behaviour deep inside the Vainshtein region.","sentences":["The class of Galileon scalar fields theories encapsulate the Vainshtein screening mechanism which is characteristic of a large range of infrared modified theories of gravity.","Such theories can lead to testable departures from General Relativity through fifth forces and new scalar modes of gravitational radiation.","However, the inherent non-linearity of the Vainshtein mechanism has limited analytic attempts to describe Galileon theories with both cubic and quartic interactions.","To improve on this, we perform direct numerical simulations of the quartic Galileon model for a rotating binary source and infer the power spectrum of given multipoles.","To tame numerical instabilities we utilize a low-pass filter, extending previous work on the cubic Galileon.","Our findings show that the multipole expansion is well-defined and under control.","Moreover, our results confirm that despite being a non-linear scalar, the dominant Galileon radiation is quadrupole, and we find a new scaling behaviour deep inside the Vainshtein region."],"url":"http://arxiv.org/abs/2402.05898v1","category":"hep-th"}
{"created":"2024-02-08 18:38:57","title":"Simulating a numerical UV Completion of Quartic Galileons","abstract":"The Galileon theory is a prototypical effective field theory that incorporates the Vainshtein screening mechanism--a feature that arises in some extensions of General Relativity, such as massive gravity. The Vainshtein effect requires that the theory contain higher order derivative interactions, which results in Galileons, and theories like them, failing to be technically well-posed. While this is not a fundamental issue when the theory is correctly treated as an effective field theory, it nevertheless poses significant practical problems when numerically simulating this model. These problems can be tamed using a number of different approaches: introducing an active low-pass filter and/or constructing a UV completion at the level of the equations of motion, which controls the high momentum modes. These methods have been tested on cubic Galileon interactions, and have been shown to reproduce the correct low-energy behavior. Here we show how the numerical UV-completion method can be applied to quartic Galileon interactions, and present the first simulations of the quartic Galileon model using this technique. We demonstrate that our approach can probe physics in the regime of the effective field theory in which the quartic term dominates, while successfully reproducing the known results for cubic interactions.","sentences":["The Galileon theory is a prototypical effective field theory that incorporates the Vainshtein screening mechanism--a feature that arises in some extensions of General Relativity, such as massive gravity.","The Vainshtein effect requires that the theory contain higher order derivative interactions, which results in Galileons, and theories like them, failing to be technically well-posed.","While this is not a fundamental issue when the theory is correctly treated as an effective field theory, it nevertheless poses significant practical problems when numerically simulating this model.","These problems can be tamed using a number of different approaches: introducing an active low-pass filter and/or constructing a UV completion at the level of the equations of motion, which controls the high momentum modes.","These methods have been tested on cubic Galileon interactions, and have been shown to reproduce the correct low-energy behavior.","Here we show how the numerical UV-completion method can be applied to quartic Galileon interactions, and present the first simulations of the quartic Galileon model using this technique.","We demonstrate that our approach can probe physics in the regime of the effective field theory in which the quartic term dominates, while successfully reproducing the known results for cubic interactions."],"url":"http://arxiv.org/abs/2402.05897v1","category":"hep-th"}
{"created":"2024-02-08 18:36:37","title":"Probing the small scale structure of the Inter-Galactic Medium with ESPRESSO: spectroscopy of the lensed QSO UM673","abstract":"The gravitationally lensed quasar J014516.6-094517 at z=2.719 has been observed with the ESPRESSO instrument at the ESO VLT to obtain high-fidelity spectra of the two images A and B with a resolving power R=70000. At the redshifts under investigation (2.1 < z < 2.7), the Lyman forests along the two sightlines are separated by sub-kiloparsec physical distances and exhibit a strong correlation. We find that the two forests are indistinguishable at the present level of signal-to-noise ratio and do not show any global velocity shift, with the cross-correlation peaking at $\\Delta v = 12 \\pm 48$ m/s. The distribution of the difference in velocity of individual Lyman-$\\alpha$ features is compatible with a null average and a mean absolute deviation of 930 m/s. Significant differences in NHI column density are not detected, putting a limit to the RMS fluctuation in the baryon density on $\\leq 1$ proper kpc scales of $\\Delta \\rho / \\rho < 3$%. On the other hand, metal lines show significant differences both in velocity structure and in column density. A toy model shows that the difference in velocity of the metal features between the two sightlines is compatible with the the motions of the baryonic component associated to dark matter halos of typical mass $M\\simeq 2\\times 10^{10} M_\\odot$, also compatible with the observed incidence of the metal systems. The present observations confirm the feasibility of the Sandage test of the cosmic redshift drift with high-fidelity spectroscopy of the Lyman forest of distant, bright quasars, but also provide an element of caution about the intrinsic noise associated to the usage of metal features for the same purpose.","sentences":["The gravitationally lensed quasar J014516.6-094517 at z=2.719 has been observed with the ESPRESSO instrument at the ESO VLT to obtain high-fidelity spectra of the two images A and B with a resolving power R=70000.","At the redshifts under investigation (2.1 < z < 2.7), the Lyman forests along the two sightlines are separated by sub-kiloparsec physical distances and exhibit a strong correlation.","We find that the two forests are indistinguishable at the present level of signal-to-noise ratio and do not show any global velocity shift, with the cross-correlation peaking at $\\Delta v = 12 \\pm 48$ m/s.","The distribution of the difference in velocity of individual Lyman-$\\alpha$ features is compatible with a null average and a mean absolute deviation of 930 m/s. Significant differences in NHI column density are not detected, putting a limit to the RMS fluctuation in the baryon density on $\\leq 1$ proper kpc scales of $\\Delta \\rho / \\rho < 3$%.","On the other hand, metal lines show significant differences both in velocity structure and in column density.","A toy model shows that the difference in velocity of the metal features between the two sightlines is compatible with the the motions of the baryonic component associated to dark matter halos of typical mass $M\\simeq 2\\times 10^{10} M_\\odot$, also compatible with the observed incidence of the metal systems.","The present observations confirm the feasibility of the Sandage test of the cosmic redshift drift with high-fidelity spectroscopy of the Lyman forest of distant, bright quasars, but also provide an element of caution about the intrinsic noise associated to the usage of metal features for the same purpose."],"url":"http://arxiv.org/abs/2402.05896v1","category":"astro-ph.CO"}
{"created":"2024-02-08 18:33:21","title":"Large Language Model Meets Graph Neural Network in Knowledge Distillation","abstract":"Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference. Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications. To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation. It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the teacher LLM and the student GNN in latent space, employing a layer-adaptive contrastive learning strategy. Through extensive experiments on a variety of LLM and GNN models and multiple benchmark datasets, the proposed LinguGKD significantly boosts the student GNN's predictive accuracy and convergence rate, without the need of extra data or model parameters. Compared to teacher LLM, distilled GNN achieves superior inference speed equipped with much fewer computing and storage demands, when surpassing the teacher LLM's classification performance on some of benchmark datasets.","sentences":["Despite recent community revelations about the advancements and potential of Large Language Models (LLMs) in understanding Text-Attributed Graphs (TAG), the deployment of LLMs for production is hindered by their high computational and storage requirements, as well as long latencies during inference.","Simultaneously, although traditional Graph Neural Networks (GNNs) are light weight and adept at learning structural features of graphs, their ability to grasp the complex semantics in TAGs is somewhat constrained for real applications.","To address these limitations, we concentrate on the downstream task of node classification in TAG and propose a novel graph knowledge distillation framework, termed Linguistic Graph Knowledge Distillation (LinguGKD), using LLMs as teacher models and GNNs as student models for knowledge distillation.","It involves TAG-oriented instruction tuning of LLM on designed node classification prompts, followed by aligning the hierarchically learned node features of the teacher LLM and the student GNN in latent space, employing a layer-adaptive contrastive learning strategy.","Through extensive experiments on a variety of LLM and GNN models and multiple benchmark datasets, the proposed LinguGKD significantly boosts the student GNN's predictive accuracy and convergence rate, without the need of extra data or model parameters.","Compared to teacher LLM, distilled GNN achieves superior inference speed equipped with much fewer computing and storage demands, when surpassing the teacher LLM's classification performance on some of benchmark datasets."],"url":"http://arxiv.org/abs/2402.05894v1","category":"cs.AI"}
{"created":"2024-02-08 18:32:10","title":"Personalizing Driver Safety Interfaces via Driver Cognitive Factors Inference","abstract":"Recent advances in AI and intelligent vehicle technology hold promise to revolutionize mobility and transportation, in the form of advanced driving assistance (ADAS) interfaces. Although it is widely recognized that certain cognitive factors, such as impulsivity and inhibitory control, are related to risky driving behavior, play a significant role in on-road risk-taking, existing systems fail to leverage such factors. Varying levels of these cognitive factors could influence the effectiveness and acceptance of driver safety interfaces.   We demonstrate an approach for personalizing driver interaction via driver safety interfaces that are triggered based on a learned recurrent neural network. The network is trained from a population of human drivers to infer impulsivity and inhibitory control from recent driving behavior. Using a high-fidelity vehicle motion simulator, we demonstrate the ability to deduce these factors from driver behavior. We then use these inferred factors to make instantaneous determinations on whether or not to engage a driver safety interface. This interface aims to decrease a driver's speed during yellow lights and reduce their inclination to run through them.","sentences":["Recent advances in AI and intelligent vehicle technology hold promise to revolutionize mobility and transportation, in the form of advanced driving assistance (ADAS) interfaces.","Although it is widely recognized that certain cognitive factors, such as impulsivity and inhibitory control, are related to risky driving behavior, play a significant role in on-road risk-taking, existing systems fail to leverage such factors.","Varying levels of these cognitive factors could influence the effectiveness and acceptance of driver safety interfaces.   ","We demonstrate an approach for personalizing driver interaction via driver safety interfaces that are triggered based on a learned recurrent neural network.","The network is trained from a population of human drivers to infer impulsivity and inhibitory control from recent driving behavior.","Using a high-fidelity vehicle motion simulator, we demonstrate the ability to deduce these factors from driver behavior.","We then use these inferred factors to make instantaneous determinations on whether or not to engage a driver safety interface.","This interface aims to decrease a driver's speed during yellow lights and reduce their inclination to run through them."],"url":"http://arxiv.org/abs/2402.05893v1","category":"cs.HC"}
{"created":"2024-02-08 18:30:50","title":"Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data","abstract":"In recent years, Transformers have become the de-facto architecture for sequence modeling on text and a variety of multi-dimensional data, such as images and video. However, the use of self-attention layers in a Transformer incurs prohibitive compute and memory complexity that scales quadratically w.r.t. the sequence length. A recent architecture, Mamba, based on state space models has been shown to achieve comparable performance for modeling text sequences, while scaling linearly with the sequence length. In this work, we present Mamba-ND, a generalized design extending the Mamba architecture to arbitrary multi-dimensional data. Our design alternatively unravels the input data across different dimensions following row-major orderings. We provide a systematic comparison of Mamba-ND with several other alternatives, based on prior multi-dimensional extensions such as Bi-directional LSTMs and S4ND. Empirically, we show that Mamba-ND demonstrates performance competitive with the state-of-the-art on a variety of multi-dimensional benchmarks, including ImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather forecasting.","sentences":["In recent years, Transformers have become the de-facto architecture for sequence modeling on text and a variety of multi-dimensional data, such as images and video.","However, the use of self-attention layers in a Transformer incurs prohibitive compute and memory complexity that scales quadratically w.r.t.","the sequence length.","A recent architecture, Mamba, based on state space models has been shown to achieve comparable performance for modeling text sequences, while scaling linearly with the sequence length.","In this work, we present Mamba-ND, a generalized design extending the Mamba architecture to arbitrary multi-dimensional data.","Our design alternatively unravels the input data across different dimensions following row-major orderings.","We provide a systematic comparison of Mamba-ND with several other alternatives, based on prior multi-dimensional extensions such as Bi-directional LSTMs and S4ND.","Empirically, we show that Mamba-ND demonstrates performance competitive with the state-of-the-art on a variety of multi-dimensional benchmarks, including ImageNet-1K classification, HMDB-51 action recognition, and ERA5 weather forecasting."],"url":"http://arxiv.org/abs/2402.05892v1","category":"cs.CV"}
{"created":"2024-02-08 18:27:22","title":"CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion","abstract":"Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters. This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning. We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models. Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality. It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation. Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additional modalities. We validate our method on video-3D, video-audio, and video-language reasoning tasks and achieve better/equivalent performance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and SeViLA while using 96% fewer trainable parameters. We provide extensive analyses of CREMA, including the impact of each modality on reasoning domains, the design of the fusion module, and example visualizations.","sentences":["Despite impressive advancements in multimodal compositional reasoning approaches, they are still limited in their flexibility and efficiency by processing fixed modality inputs while updating a lot of model parameters.","This paper tackles these critical challenges and proposes CREMA, an efficient and modular modality-fusion framework for injecting any new modality into video reasoning.","We first augment multiple informative modalities (such as optical flow, 3D point cloud, audio) from given videos without extra human annotation by leveraging existing pre-trained models.","Next, we introduce a query transformer with multiple parameter-efficient modules associated with each accessible modality.","It projects diverse modality features to the LLM token embedding space, allowing the model to integrate different data types for response generation.","Furthermore, we propose a fusion module designed to compress multimodal queries, maintaining computational efficiency in the LLM while combining additional modalities.","We validate our method on video-3D, video-audio, and video-language reasoning tasks and achieve better/equivalent performance against strong multimodal LLMs, including BLIP-2, 3D-LLM, and SeViLA while using 96% fewer trainable parameters.","We provide extensive analyses of CREMA, including the impact of each modality on reasoning domains, the design of the fusion module, and example visualizations."],"url":"http://arxiv.org/abs/2402.05889v1","category":"cs.CV"}
{"created":"2024-02-08 18:23:05","title":"EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance","abstract":"The need to identify graphs having small structural distance from a query arises in biology, chemistry, recommender systems, and social network analysis. Among several methods to measure inter graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, yet hindered by the NP-hardness of its computation. State-of-the-art GED approximations predominantly employ neural methods, which, however, (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset. In this paper, we propose an efficient algebraic unsuper vised method, EUGENE, that approximates GED and yields edit paths corresponding to the approx imated cost, while eliminating the need for ground truth generation and data-specific training. Extensive experimental evaluation demonstrates that the aforementioned benefits of EUGENE do not come at the cost of efficacy. Specifically, EUGENE consistently ranks among the most accurate methods across all of the benchmark datasets and outperforms majority of the neural approaches.","sentences":["The need to identify graphs having small structural distance from a query arises in biology, chemistry, recommender systems, and social network analysis.","Among several methods to measure inter graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, yet hindered by the NP-hardness of its computation.","State-of-the-art GED approximations predominantly employ neural methods, which, however, (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset.","In this paper, we propose an efficient algebraic unsuper vised method, EUGENE, that approximates GED and yields edit paths corresponding to the approx imated cost, while eliminating the need for ground truth generation and data-specific training.","Extensive experimental evaluation demonstrates that the aforementioned benefits of EUGENE do not come at the cost of efficacy.","Specifically, EUGENE consistently ranks among the most accurate methods across all of the benchmark datasets and outperforms majority of the neural approaches."],"url":"http://arxiv.org/abs/2402.05885v1","category":"cs.LG"}
{"created":"2024-02-08 18:16:47","title":"GET-Tok: A GenAI-Enriched Multimodal TikTok Dataset Documenting the 2022 Attempted Coup in Peru","abstract":"TikTok is one of the largest and fastest-growing social media sites in the world. TikTok features, however, such as voice transcripts, are often missing and other important features, such as OCR or video descriptions, do not exist. We introduce the Generative AI Enriched TikTok (GET-Tok) data, a pipeline for collecting TikTok videos and enriched data by augmenting the TikTok Research API with generative AI models. As a case study, we collect videos about the attempted coup in Peru initiated by its former President, Pedro Castillo, and its accompanying protests. The data includes information on 43,697 videos published from November 20, 2022 to March 1, 2023 (102 days). Generative AI augments the collected data via transcripts of TikTok videos, text descriptions of what is shown in the videos, what text is displayed within the video, and the stances expressed in the video. Overall, this pipeline will contribute to a better understanding of online discussion in a multimodal setting with applications of Generative AI, especially outlining the utility of this pipeline in non-English-language social media. Our code used to produce the pipeline is in a public Github repository: https://github.com/gabbypinto/GET-Tok-Peru.","sentences":["TikTok is one of the largest and fastest-growing social media sites in the world.","TikTok features, however, such as voice transcripts, are often missing and other important features, such as OCR or video descriptions, do not exist.","We introduce the Generative AI Enriched TikTok (GET-Tok) data, a pipeline for collecting TikTok videos and enriched data by augmenting the TikTok Research API with generative AI models.","As a case study, we collect videos about the attempted coup in Peru initiated by its former President, Pedro Castillo, and its accompanying protests.","The data includes information on 43,697 videos published from November 20, 2022 to March 1, 2023 (102 days).","Generative AI augments the collected data via transcripts of TikTok videos, text descriptions of what is shown in the videos, what text is displayed within the video, and the stances expressed in the video.","Overall, this pipeline will contribute to a better understanding of online discussion in a multimodal setting with applications of Generative AI, especially outlining the utility of this pipeline in non-English-language social media.","Our code used to produce the pipeline is in a public Github repository: https://github.com/gabbypinto/GET-Tok-Peru."],"url":"http://arxiv.org/abs/2402.05882v1","category":"cs.SI"}
{"created":"2024-02-08 18:15:41","title":"Localized and Distributed Beyond Diagonal Reconfigurable Intelligent Surfaces with Lossy Interconnections: Modeling and Optimization","abstract":"Reconfigurable intelligent surface (RIS) is a key technology to control the communication environment in future wireless networks. Recently, beyond diagonal RIS (BD-RIS) emerged as a generalization of RIS achieving larger coverage through additional tunable impedance components interconnecting the RIS elements. However, conventional RIS and BD-RIS can effectively serve only users in their proximity, resulting in limited coverage. To overcome this limitation, in this paper, we investigate distributed RIS, whose elements are distributed over a wide region, in opposition to localized RIS commonly considered in the literature. The scaling laws of distributed BD-RIS reveal that it offers significant gains over distributed conventional RIS and localized BD-RIS, enabled by its interconnections allowing signal propagation within the BD-RIS. To assess the practical performance of distributed BD-RIS, we model and optimize BD-RIS with lossy interconnections through transmission line theory. Our model accounts for phase changes and losses over the BD-RIS interconnections arising when the interconnection lengths are not much smaller than the wavelength. Numerical results show that the performance of localized BD-RIS is only slightly impacted by losses, given the short interconnection lengths. Besides, distributed BD-RIS can achieve orders of magnitude of gains over conventional RIS, even in the presence of low losses.","sentences":["Reconfigurable intelligent surface (RIS) is a key technology to control the communication environment in future wireless networks.","Recently, beyond diagonal RIS (BD-RIS) emerged as a generalization of RIS achieving larger coverage through additional tunable impedance components interconnecting the RIS elements.","However, conventional RIS and BD-RIS can effectively serve only users in their proximity, resulting in limited coverage.","To overcome this limitation, in this paper, we investigate distributed RIS, whose elements are distributed over a wide region, in opposition to localized RIS commonly considered in the literature.","The scaling laws of distributed BD-RIS reveal that it offers significant gains over distributed conventional RIS and localized BD-RIS, enabled by its interconnections allowing signal propagation within the BD-RIS.","To assess the practical performance of distributed BD-RIS, we model and optimize BD-RIS with lossy interconnections through transmission line theory.","Our model accounts for phase changes and losses over the BD-RIS interconnections arising when the interconnection lengths are not much smaller than the wavelength.","Numerical results show that the performance of localized BD-RIS is only slightly impacted by losses, given the short interconnection lengths.","Besides, distributed BD-RIS can achieve orders of magnitude of gains over conventional RIS, even in the presence of low losses."],"url":"http://arxiv.org/abs/2402.05881v1","category":"cs.IT"}
{"created":"2024-02-08 18:14:33","title":"Generative Echo Chamber? Effects of LLM-Powered Search Systems on Diverse Information Seeking","abstract":"Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search. However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search. We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect. Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias. These results present critical implications for the development of LLMs and conversational search systems, and the policy governing these technologies.","sentences":["Large language models (LLMs) powered conversational search systems have already been used by hundreds of millions of people, and are believed to bring many benefits over conventional search.","However, while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers -- limiting exposure to diverse opinions and leading to opinion polarization, little is known about such a risk of LLM-powered conversational search.","We conduct two experiments to investigate: 1) whether and how LLM-powered conversational search increases selective exposure compared to conventional search; 2) whether and how LLMs with opinion biases that either reinforce or challenge the user's view change the effect.","Overall, we found that participants engaged in more biased information querying with LLM-powered conversational search, and an opinionated LLM reinforcing their views exacerbated this bias.","These results present critical implications for the development of LLMs and conversational search systems, and the policy governing these technologies."],"url":"http://arxiv.org/abs/2402.05880v1","category":"cs.CL"}
{"created":"2024-02-08 18:08:58","title":"Generators and presentations of inverse subsemigroups of the monogenic free inverse semigroup","abstract":"It was proved by Oliveira and Silva (2005) that every finitely generated inverse subsemigroup of the monogenic free inverse semigroup $FI_1$ is finitely presented. The present paper continues this development, and gives generating sets and presentations for general (i.e. not necessarily finitely generated) inverse subsemigroups of $FI_1$. For an inverse semigroup $S$ and an inverse subsemigroup $T$ of $S$, we say $S$ is finitely generated modulo $T$ if there is a finite set $A$ such that $S = \\langle T, A \\rangle$. Likewise, we say that $S$ is finitely presented modulo $T $ if $S$ can be defined by a presentation of the form $\\text{Inv}\\langle X, Y \\mid R, Q\\rangle$, where $\\text{Inv}\\langle X\\mid R\\rangle$ is a presentation for $T$ and $Y$ and $Q$ are finite. We show that every inverse subsemigroup $S$ of $FI_1$ is finitely generated modulo its semilattice of idempotents $E(S)$. By way of contrast, we show that when $S\\neq E(S)$, it can never be finitely presented modulo $E(S)$. However, in the process we establish some nice (albeit infinite) presentations for $S$ modulo $E(S)$.","sentences":["It was proved by Oliveira and Silva (2005) that every finitely generated inverse subsemigroup of the monogenic free inverse semigroup $FI_1$ is finitely presented.","The present paper continues this development, and gives generating sets and presentations for general (i.e. not necessarily finitely generated) inverse subsemigroups of $FI_1$. For an inverse semigroup $S$ and an inverse subsemigroup $T$ of $S$, we say $S$ is finitely generated modulo $T$ if there is a finite set $A$ such that $S = \\langle T, A \\rangle$. Likewise, we say that $S$ is finitely presented modulo $T $ if $S$ can be defined by a presentation of the form $\\text{Inv}\\langle X, Y \\mid R, Q\\rangle$, where $\\text{Inv}\\langle X\\mid R\\rangle$ is a presentation for $T$ and $Y$ and $Q$ are finite.","We show that every inverse subsemigroup $S$ of $FI_1$ is finitely generated modulo its semilattice of idempotents $E(S)$. By way of contrast, we show that when $S\\neq E(S)$, it can never be finitely presented modulo $E(S)$. However, in the process we establish some nice (albeit infinite) presentations for $S$ modulo $E(S)$."],"url":"http://arxiv.org/abs/2402.05875v1","category":"math.GR"}
{"created":"2024-02-08 18:08:09","title":"Complexity of graph-state preparation by Clifford circuits","abstract":"In this work, we study a complexity of graph-state preparation. We consider general quantum algorithms consisting of the Clifford operations on at most two qubits for graph-state preparations. We define the CZ-complexity of graph state $|G\\rangle$ as the minimum number of two-qubit Clifford operations (excluding single-qubit Clifford operations) for generating $|G\\rangle$ from a trivial state $|0\\rangle^{\\otimes n}$. We first prove that a graph state $|G\\rangle$ is generated by at most $t$ two-qubit Clifford operations if and only if $|G\\rangle$ is generated by at most $t$ controlled-Z (CZ) operations. We next prove that a graph state $|G\\rangle$ is generated from another graph state $|H\\rangle$ by $t$ CZ operations if and only if the graph $G$ is generated from $H$ by some combinatorial graph transformation with cost $t$. As the main results, we show a connection between the CZ-complexity of graph state $|G\\rangle$ and the rank-width of the graph $G$. Indeed, we prove that for any graph $G$ with $n$ vertices and rank-width $r$,   1. The CZ-complexity of $|G\\rangle$ is $O(rn\\log n)$.   2. If $G$ is connected, the CZ-complexity of $|G\\rangle$ is at least $n + r - 2$.   We also show the existence of graph states whose CZ-complexities are close to the upper and lower bounds. Finally, we present quantum algorithms preparing $|G\\rangle$ with $O(n)$ CZ-complexity when $G$ is included in special classes of graphs, namely, cographs, interval graphs, permutation graphs and circle graphs.","sentences":["In this work, we study a complexity of graph-state preparation.","We consider general quantum algorithms consisting of the Clifford operations on at most two qubits for graph-state preparations.","We define the CZ-complexity of graph state $|G\\rangle$ as the minimum number of two-qubit Clifford operations (excluding single-qubit Clifford operations) for generating $|G\\rangle$ from a trivial state $|0\\rangle^{\\otimes n}$. We first prove that a graph state $|G\\rangle$ is generated by at most $t$ two-qubit Clifford operations if and only if $|G\\rangle$ is generated by at most $t$ controlled-Z (CZ) operations.","We next prove that a graph state $|G\\rangle$ is generated from another graph state $|H\\rangle$ by $t$ CZ operations if and only if the graph $G$ is generated from $H$ by some combinatorial graph transformation with cost $t$. As the main results, we show a connection between the CZ-complexity of graph state $|G\\rangle$ and the rank-width of the graph $G$. Indeed, we prove that for any graph $G$ with $n$ vertices and rank-width $r$,   1.","The CZ-complexity of $|G\\rangle$ is $O(rn\\log n)$.   2.","If $G$ is connected, the CZ-complexity of $|G\\rangle$ is at least $n + r - 2$.   We also show the existence of graph states whose CZ-complexities are close to the upper and lower bounds.","Finally, we present quantum algorithms preparing $|G\\rangle$ with $O(n)$ CZ-complexity when $G$ is included in special classes of graphs, namely, cographs, interval graphs, permutation graphs and circle graphs."],"url":"http://arxiv.org/abs/2402.05874v1","category":"quant-ph"}
{"created":"2024-02-08 18:01:18","title":"Kilonova Light-Curve Interpolation with Neural Networks","abstract":"Kilonovae are the electromagnetic transients created by the radioactive decay of freshly synthesized elements in the environment surrounding a neutron star merger. To study the fundamental physics in these complex environments, kilonova modeling requires, in part, the use of radiative transfer simulations. The microphysics involved in these simulations results in high computational cost, prompting the use of emulators for parameter inference applications. Utilizing a training set of 22248 high-fidelity simulations, we use a neural network to efficiently train on existing radiative transfer simulations and predict light curves for new parameters in a fast and computationally efficient manner. Our neural network can generate millions of new light curves in under a minute. We discuss our emulator's degree of off-sample reliability and parameter inference of the AT2017gfo observational data. Finally, we discuss tension introduced by multi-band inference in the parameter inference results, particularly with regard to the neural network's recovery of viewing angle.","sentences":["Kilonovae are the electromagnetic transients created by the radioactive decay of freshly synthesized elements in the environment surrounding a neutron star merger.","To study the fundamental physics in these complex environments, kilonova modeling requires, in part, the use of radiative transfer simulations.","The microphysics involved in these simulations results in high computational cost, prompting the use of emulators for parameter inference applications.","Utilizing a training set of 22248 high-fidelity simulations, we use a neural network to efficiently train on existing radiative transfer simulations and predict light curves for new parameters in a fast and computationally efficient manner.","Our neural network can generate millions of new light curves in under a minute.","We discuss our emulator's degree of off-sample reliability and parameter inference of the AT2017gfo observational data.","Finally, we discuss tension introduced by multi-band inference in the parameter inference results, particularly with regard to the neural network's recovery of viewing angle."],"url":"http://arxiv.org/abs/2402.05871v1","category":"astro-ph.HE"}
{"created":"2024-02-08 17:58:23","title":"Revisiting Schwarzschild black hole singularity through string theory","abstract":"The resolution of black hole singularities represents an essential problem in the realm of quantum gravity. Due to the Belinskii, Khalatnikov and Lifshitz (BKL) proposal, the structure of the black hole interior in vacuum Einstein's equations can be described by the Kasner universe, which possesses the $O\\left(d,d\\right)$ symmetry. It motivates us to use the anisotropic Hohm-Zwiebach action, known as the string effective action with all orders $\\alpha^{\\prime}$ corrections for the $O\\left(d,d\\right)$ symmetric background, to study the singularity problem of black hole. In this letter, we obtain the singular condition for black holes and demonstrate that it is possible to resolve the Schwarzschild black hole singularity through the non-perturbative $\\alpha^{\\prime}$ corrections of string theory.","sentences":["The resolution of black hole singularities represents an essential problem in the realm of quantum gravity.","Due to the Belinskii, Khalatnikov and Lifshitz (BKL) proposal, the structure of the black hole interior in vacuum Einstein's equations can be described by the Kasner universe, which possesses the $O\\left(d,d\\right)$ symmetry.","It motivates us to use the anisotropic Hohm-Zwiebach action, known as the string effective action with all orders $\\alpha^{\\prime}$ corrections for the $O\\left(d,d\\right)$ symmetric background, to study the singularity problem of black hole.","In this letter, we obtain the singular condition for black holes and demonstrate that it is possible to resolve the Schwarzschild black hole singularity through the non-perturbative $\\alpha^{\\prime}$ corrections of string theory."],"url":"http://arxiv.org/abs/2402.05870v1","category":"hep-th"}
{"created":"2024-02-08 17:57:59","title":"Adaptive Surface Normal Constraint for Geometric Estimation from Monocular Images","abstract":"We introduce a novel approach to learn geometries such as depth and surface normal from images while incorporating geometric context. The difficulty of reliably capturing geometric context in existing methods impedes their ability to accurately enforce the consistency between the different geometric properties, thereby leading to a bottleneck of geometric estimation quality. We therefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet efficient method. Our approach extracts geometric context that encodes the geometric variations present in the input image and correlates depth estimation with geometric constraints. By dynamically determining reliable local geometry from randomly sampled candidates, we establish a surface normal constraint, where the validity of these candidates is evaluated using the geometric context. Furthermore, our normal estimation leverages the geometric context to prioritize regions that exhibit significant geometric variations, which makes the predicted normals accurately capture intricate and detailed geometric information. Through the integration of geometric context, our method unifies depth and surface normal estimations within a cohesive framework, which enables the generation of high-quality 3D geometry from images. We validate the superiority of our approach over state-of-the-art methods through extensive evaluations and comparisons on diverse indoor and outdoor datasets, showcasing its efficiency and robustness.","sentences":["We introduce a novel approach to learn geometries such as depth and surface normal from images while incorporating geometric context.","The difficulty of reliably capturing geometric context in existing methods impedes their ability to accurately enforce the consistency between the different geometric properties, thereby leading to a bottleneck of geometric estimation quality.","We therefore propose the Adaptive Surface Normal (ASN) constraint, a simple yet efficient method.","Our approach extracts geometric context that encodes the geometric variations present in the input image and correlates depth estimation with geometric constraints.","By dynamically determining reliable local geometry from randomly sampled candidates, we establish a surface normal constraint, where the validity of these candidates is evaluated using the geometric context.","Furthermore, our normal estimation leverages the geometric context to prioritize regions that exhibit significant geometric variations, which makes the predicted normals accurately capture intricate and detailed geometric information.","Through the integration of geometric context, our method unifies depth and surface normal estimations within a cohesive framework, which enables the generation of high-quality 3D geometry from images.","We validate the superiority of our approach over state-of-the-art methods through extensive evaluations and comparisons on diverse indoor and outdoor datasets, showcasing its efficiency and robustness."],"url":"http://arxiv.org/abs/2402.05869v1","category":"cs.CV"}
{"created":"2024-02-08 17:57:11","title":"PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models","abstract":"Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the model's performance remains unaffected. We conduct experiments on three tasks, personalized recommendation, sentiment analysis, and tabular data analysis. Experiment results reveal that PromptCrypt can encrypt personal information within prompts in such a manner that not only prevents the discernment of sensitive data by humans or LLM itself, but also maintains or even improves the precision without further tuning, achieving comparable or even better task accuracy than directly prompting the LLM without prompt encryption. These results highlight the practicality of adopting encryption measures that safeguard user privacy without compromising the functional integrity and performance of LLMs. Code and dataset are available at https://github.com/agiresearch/PromptCrypt.","sentences":["Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications.","While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services.","To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy.","It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the model's performance remains unaffected.","We conduct experiments on three tasks, personalized recommendation, sentiment analysis, and tabular data analysis.","Experiment results reveal that PromptCrypt can encrypt personal information within prompts in such a manner that not only prevents the discernment of sensitive data by humans or LLM itself, but also maintains or even improves the precision without further tuning, achieving comparable or even better task accuracy than directly prompting the LLM without prompt encryption.","These results highlight the practicality of adopting encryption measures that safeguard user privacy without compromising the functional integrity and performance of LLMs.","Code and dataset are available at https://github.com/agiresearch/PromptCrypt."],"url":"http://arxiv.org/abs/2402.05868v1","category":"cs.CL"}
{"created":"2024-02-08 17:57:09","title":"Exploring pseudorandom value addition operations in datasets: A layered approach to escape from normal-Gaussian patterns","abstract":"In the realm of statistical exploration, the manipulation of pseudo-random values to discern their impact on data distribution presents a compelling avenue of inquiry. This article investigates the question: Is it possible to add pseudo-random values without compelling a shift towards a normal distribution?. Employing Python techniques, the study explores the nuances of pseudo-random value addition within the context of additions, aiming to unravel the interplay between randomness and resulting statistical characteristics. The Materials and Methods chapter details the construction of datasets comprising up to 300 billion pseudo-random values, employing three distinct layers of manipulation. The Results chapter visually and quantitatively explores the generated datasets, emphasizing distribution and standard deviation metrics. The study concludes with reflections on the implications of pseudo-random value manipulation and suggests avenues for future research. In the layered exploration, the first layer introduces subtle normalization with increasing summations, while the second layer enhances normality. The third layer disrupts typical distribution patterns, leaning towards randomness despite pseudo-random value summation. Standard deviation patterns across layers further illuminate the dynamic interplay of pseudo-random operations on statistical characteristics. While not aiming to disrupt academic norms, this work modestly contributes insights into data distribution complexities. Future studies are encouraged to delve deeper into the implications of data manipulation on statistical outcomes, extending the understanding of pseudo-random operations in diverse contexts.","sentences":["In the realm of statistical exploration, the manipulation of pseudo-random values to discern their impact on data distribution presents a compelling avenue of inquiry.","This article investigates the question: Is it possible to add pseudo-random values without compelling a shift towards a normal distribution?.","Employing Python techniques, the study explores the nuances of pseudo-random value addition within the context of additions, aiming to unravel the interplay between randomness and resulting statistical characteristics.","The Materials and Methods chapter details the construction of datasets comprising up to 300 billion pseudo-random values, employing three distinct layers of manipulation.","The Results chapter visually and quantitatively explores the generated datasets, emphasizing distribution and standard deviation metrics.","The study concludes with reflections on the implications of pseudo-random value manipulation and suggests avenues for future research.","In the layered exploration, the first layer introduces subtle normalization with increasing summations, while the second layer enhances normality.","The third layer disrupts typical distribution patterns, leaning towards randomness despite pseudo-random value summation.","Standard deviation patterns across layers further illuminate the dynamic interplay of pseudo-random operations on statistical characteristics.","While not aiming to disrupt academic norms, this work modestly contributes insights into data distribution complexities.","Future studies are encouraged to delve deeper into the implications of data manipulation on statistical outcomes, extending the understanding of pseudo-random operations in diverse contexts."],"url":"http://arxiv.org/abs/2402.05867v1","category":"math.NA"}
{"created":"2024-02-08 17:54:23","title":"Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs","abstract":"In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding","sentences":["In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder.","It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder.","We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder.","The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy.","Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding.","The code is available at https://github.com/XuandongZhao/pf-decoding"],"url":"http://arxiv.org/abs/2402.05864v1","category":"cs.CL"}
{"created":"2024-02-08 17:51:48","title":"How Well Can LLMs Negotiate? NegotiationArena Platform and Analysis","abstract":"Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources. With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate. In this paper, we study how well LLMs can negotiate with each other. We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents. We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations). Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations. Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics. For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20\\% when negotiating against the standard GPT-4. We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans. Together, \\NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities.","sentences":["Negotiation is the basis of social interactions; humans negotiate everything from the price of cars to how to share common resources.","With rapidly growing interest in using large language models (LLMs) to act as agents on behalf of human users, such LLM agents would also need to be able to negotiate.","In this paper, we study how well LLMs can negotiate with each other.","We develop NegotiationArena: a flexible framework for evaluating and probing the negotiation abilities of LLM agents.","We implemented three types of scenarios in NegotiationArena to assess LLM's behaviors in allocating shared resources (ultimatum games), aggregate resources (trading games) and buy/sell goods (price negotiations).","Each scenario allows for multiple turns of flexible dialogues between LLM agents to allow for more complex negotiations.","Interestingly, LLM agents can significantly boost their negotiation outcomes by employing certain behavioral tactics.","For example, by pretending to be desolate and desperate, LLMs can improve their payoffs by 20\\% when negotiating against the standard GPT-4.","We also quantify irrational negotiation behaviors exhibited by the LLM agents, many of which also appear in humans.","Together, \\NegotiationArena offers a new environment to investigate LLM interactions, enabling new insights into LLM's theory of mind, irrationality, and reasoning abilities."],"url":"http://arxiv.org/abs/2402.05863v1","category":"cs.AI"}
{"created":"2024-02-08 17:51:44","title":"Let Your Graph Do the Talking: Encoding Structured Data for LLMs","abstract":"How can we best encode structured data into sequential form for use in large language models (LLMs)? In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs. Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information. Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks. We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks. Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark.","sentences":["How can we best encode structured data into sequential form for use in large language models (LLMs)?","In this work, we introduce a parameter-efficient method to explicitly represent structured data for LLMs.","Our method, GraphToken, learns an encoding function to extend prompts with explicit structured information.","Unlike other work which focuses on limited domains (e.g. knowledge graph representation), our work is the first effort focused on the general encoding of structured data to be used for various reasoning tasks.","We show that explicitly representing the graph structure allows significant improvements to graph reasoning tasks.","Specifically, we see across the board improvements - up to 73% points - on node, edge and, graph-level tasks from the GraphQA benchmark."],"url":"http://arxiv.org/abs/2402.05862v1","category":"cs.LG"}
{"created":"2024-02-08 17:44:06","title":"Privacy-Preserving Synthetic Continual Semantic Segmentation for Robotic Surgery","abstract":"Deep Neural Networks (DNNs) based semantic segmentation of the robotic instruments and tissues can enhance the precision of surgical activities in robot-assisted surgery. However, in biological learning, DNNs cannot learn incremental tasks over time and exhibit catastrophic forgetting, which refers to the sharp decline in performance on previously learned tasks after learning a new one. Specifically, when data scarcity is the issue, the model shows a rapid drop in performance on previously learned instruments after learning new data with new instruments. The problem becomes worse when it limits releasing the dataset of the old instruments for the old model due to privacy concerns and the unavailability of the data for the new or updated version of the instruments for the continual learning model. For this purpose, we develop a privacy-preserving synthetic continual semantic segmentation framework by blending and harmonizing (i) open-source old instruments foreground to the synthesized background without revealing real patient data in public and (ii) new instruments foreground to extensively augmented real background. To boost the balanced logit distillation from the old model to the continual learning model, we design overlapping class-aware temperature normalization (CAT) by controlling model learning utility. We also introduce multi-scale shifted-feature distillation (SD) to maintain long and short-range spatial relationships among the semantic objects where conventional short-range spatial features with limited information reduce the power of feature distillation. We demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018 instrument segmentation dataset with a generalized continual learning setting. Code is available at~\\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}.","sentences":["Deep Neural Networks (DNNs) based semantic segmentation of the robotic instruments and tissues can enhance the precision of surgical activities in robot-assisted surgery.","However, in biological learning, DNNs cannot learn incremental tasks over time and exhibit catastrophic forgetting, which refers to the sharp decline in performance on previously learned tasks after learning a new one.","Specifically, when data scarcity is the issue, the model shows a rapid drop in performance on previously learned instruments after learning new data with new instruments.","The problem becomes worse when it limits releasing the dataset of the old instruments for the old model due to privacy concerns and the unavailability of the data for the new or updated version of the instruments for the continual learning model.","For this purpose, we develop a privacy-preserving synthetic continual semantic segmentation framework by blending and harmonizing (i) open-source old instruments foreground to the synthesized background without revealing real patient data in public and (ii) new instruments foreground to extensively augmented real background.","To boost the balanced logit distillation from the old model to the continual learning model, we design overlapping class-aware temperature normalization (CAT) by controlling model learning utility.","We also introduce multi-scale shifted-feature distillation (SD) to maintain long and short-range spatial relationships among the semantic objects where conventional short-range spatial features with limited information reduce the power of feature distillation.","We demonstrate the effectiveness of our framework on the EndoVis 2017 and 2018 instrument segmentation dataset with a generalized continual learning setting.","Code is available at~\\url{https://github.com/XuMengyaAmy/Synthetic_CAT_SD}."],"url":"http://arxiv.org/abs/2402.05860v1","category":"cs.CV"}
{"created":"2024-02-08 17:43:22","title":"Learning to Route Among Specialized Experts for Zero-Shot Generalization","abstract":"Recently, there has been a widespread proliferation of \"expert\" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning. How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks? In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning. Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model. Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained. In experiments covering a range of specialized model collections and zero-shot generalization benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc routing and, in some cases, outperforms explicit multitask training (which requires simultaneous data access). To better understand the routing strategy learned by PHATGOOSE, we perform qualitative experiments to validate that PHATGOOSE's performance stems from its ability to make adaptive per-token and per-module expert choices. We release all of our code to support future work on improving zero-shot generalization by recycling specialized experts.","sentences":["Recently, there has been a widespread proliferation of \"expert\" language models that are specialized to a specific task or domain through parameter-efficient fine-tuning.","How can we recycle large collections of expert language models to improve zero-shot generalization to unseen tasks?","In this work, we propose Post-Hoc Adaptive Tokenwise Gating Over an Ocean of Specialized Experts (PHATGOOSE), which learns to route among specialized modules that were produced through parameter-efficient fine-tuning.","Unlike past methods that learn to route among specialized models, PHATGOOSE explores the possibility that zero-shot generalization will be improved if different experts can be adaptively chosen for each token and at each layer in the model.","Crucially, our method is post-hoc - it does not require simultaneous access to the datasets used to create the specialized models and only requires a modest amount of additional compute after each expert model is trained.","In experiments covering a range of specialized model collections and zero-shot generalization benchmarks, we find that PHATGOOSE outperforms past methods for post-hoc routing and, in some cases, outperforms explicit multitask training (which requires simultaneous data access).","To better understand the routing strategy learned by PHATGOOSE, we perform qualitative experiments to validate that PHATGOOSE's performance stems from its ability to make adaptive per-token and per-module expert choices.","We release all of our code to support future work on improving zero-shot generalization by recycling specialized experts."],"url":"http://arxiv.org/abs/2402.05859v1","category":"cs.LG"}
{"created":"2024-02-08 17:38:29","title":"(Almost) Affine Higher-Order Tree Transducers","abstract":"We investigate the tree-to-tree functions computed by \\enquote{affine$\\lambda$-transducers}: tree automata whose memory consists of an affine $\\lambda$-term instead of a finite state. They can be seen as variations on Gallot, Lemay and Salvati's Linear High-Order Deterministic Tree Transducers. When the memory is almost purely affine (\\textit{\\`a la} Kanazawa), we show that these machines can be translated to tree-walking transducers (and with a purely affine memory, we get a reversible tree-walking transducer). This leads to a proof of an inexpressivity conjecture of \\titocecilia on \\enquote{implicit automata} in an affine $\\lambda$-calculus. The key technical tool in our proofs is the Interaction Abstract Machine (IAM), an operational avatar of the \\enquote{geometry of interaction} semantics of linear logic. We work with ad-hoc specializations to (almost) affine $\\lambda$-terms of a tree-generating version of the IAM.","sentences":["We investigate the tree-to-tree functions computed by \\enquote{affine$\\lambda$-transducers}: tree automata whose memory consists of an affine $\\lambda$-term instead of a finite state.","They can be seen as variations on Gallot, Lemay and Salvati's Linear High-Order Deterministic Tree Transducers.","When the memory is almost purely affine (\\textit{\\`a la} Kanazawa), we show that these machines can be translated to tree-walking transducers (and with a purely affine memory, we get a reversible tree-walking transducer).","This leads to a proof of an inexpressivity conjecture of \\titocecilia on \\enquote{implicit automata} in an affine $\\lambda$-calculus.","The key technical tool in our proofs is the Interaction Abstract Machine (IAM), an operational avatar of the \\enquote{geometry of interaction} semantics of linear logic.","We work with ad-hoc specializations to (almost) affine $\\lambda$-terms of a tree-generating version of the IAM."],"url":"http://arxiv.org/abs/2402.05854v1","category":"cs.FL"}
{"created":"2024-02-08 17:35:47","title":"On Experimental Emulation of Printability and Fleet Aware Generic Mesh Decomposition for Enabling Aerial 3D Printing","abstract":"This article introduces an experimental emulation of a novel chunk-based flexible multi-DoF aerial 3D printing framework. The experimental demonstration of the overall autonomy focuses on precise motion planning and task allocation for a UAV, traversing through a series of planned space-filling paths involved in the aerial 3D printing process without physically depositing the overlaying material. The flexible multi-DoF aerial 3D printing is a newly developed framework and has the potential to strategically distribute the envisioned 3D model to be printed into small, manageable chunks suitable for distributed 3D printing. Moreover, by harnessing the dexterous flexibility due to the 6 DoF motion of UAV, the framework enables the provision of integrating the overall autonomy stack, potentially opening up an entirely new frontier in additive manufacturing. However, it's essential to note that the feasibility of this pioneering concept is still in its very early stage of development, which yet needs to be experimentally verified. Towards this direction, experimental emulation serves as the crucial stepping stone, providing a pseudo mockup scenario by virtual material deposition, helping to identify technological gaps from simulation to reality. Experimental emulation results, supported by critical analysis and discussion, lay the foundation for addressing the technological and research challenges to significantly push the boundaries of the state-of-the-art 3D printing mechanism.","sentences":["This article introduces an experimental emulation of a novel chunk-based flexible multi-DoF aerial 3D printing framework.","The experimental demonstration of the overall autonomy focuses on precise motion planning and task allocation for a UAV, traversing through a series of planned space-filling paths involved in the aerial 3D printing process without physically depositing the overlaying material.","The flexible multi-DoF aerial 3D printing is a newly developed framework and has the potential to strategically distribute the envisioned 3D model to be printed into small, manageable chunks suitable for distributed 3D printing.","Moreover, by harnessing the dexterous flexibility due to the 6 DoF motion of UAV, the framework enables the provision of integrating the overall autonomy stack, potentially opening up an entirely new frontier in additive manufacturing.","However, it's essential to note that the feasibility of this pioneering concept is still in its very early stage of development, which yet needs to be experimentally verified.","Towards this direction, experimental emulation serves as the crucial stepping stone, providing a pseudo mockup scenario by virtual material deposition, helping to identify technological gaps from simulation to reality.","Experimental emulation results, supported by critical analysis and discussion, lay the foundation for addressing the technological and research challenges to significantly push the boundaries of the state-of-the-art 3D printing mechanism."],"url":"http://arxiv.org/abs/2402.05853v1","category":"cs.RO"}
{"created":"2024-02-08 17:29:56","title":"Reconfigurable Intelligent Surface-Aided Dual-Function Radar and Communication Systems With MU-MIMO Communication","abstract":"In this paper, we investigate an reconfigurable intelligent surface (RIS)-aided integrated sensing and communication (ISAC) system. Our objective is to maximize the achievable sum rate of the multi-antenna communication users through the joint active and passive beamforming. {Specifically}, the weighted minimum mean-square error (WMMSE) method is { first} used to reformulate the original problem into an equivalent one. Then, we utilize an alternating optimization (AO) { algorithm} to decouple the optimization variables and decompose this challenging problem into two subproblems. Given reflecting coefficients, a penalty-based algorithm is utilized to deal with the non-convex radar signal-to-noise ratio (SNR) constraints. For the given beamforming matrix of the BS, we apply majorization-minimization (MM) to transform the problem into a quadratic constraint quadratic programming (QCQP) problem, which is ultimately solved using a semidefinite relaxation (SDR)-based algorithm. Simulation results illustrate the advantage of deploying RIS in the considered multi-user MIMO (MU-MIMO) ISAC systems.","sentences":["In this paper, we investigate an reconfigurable intelligent surface (RIS)-aided integrated sensing and communication (ISAC) system.","Our objective is to maximize the achievable sum rate of the multi-antenna communication users through the joint active and passive beamforming.","{Specifically}, the weighted minimum mean-square error (WMMSE) method is { first} used to reformulate the original problem into an equivalent one.","Then, we utilize an alternating optimization (AO) { algorithm} to decouple the optimization variables and decompose this challenging problem into two subproblems.","Given reflecting coefficients, a penalty-based algorithm is utilized to deal with the non-convex radar signal-to-noise ratio (SNR) constraints.","For the given beamforming matrix of the BS, we apply majorization-minimization (MM) to transform the problem into a quadratic constraint quadratic programming (QCQP) problem, which is ultimately solved using a semidefinite relaxation (SDR)-based algorithm.","Simulation results illustrate the advantage of deploying RIS in the considered multi-user MIMO (MU-MIMO) ISAC systems."],"url":"http://arxiv.org/abs/2402.05847v1","category":"eess.SP"}
{"created":"2024-02-08 17:22:20","title":"Fluid and kinetic studies of tokamak disruptions using Bayesian optimization","abstract":"When simulating runaway electron dynamics in tokamak disruptions, fluid models with lower numerical cost are often preferred to more accurate kinetic models. The aim of this work is to compare fluid and kinetic simulations of a large variety of different disruption scenarios in ITER. We consider both non-activated and activated scenarios; for the latter we derive and implement kinetic sources for the Compton scattering and tritium beta decay runaway electron generation mechanisms in our simulation tool DREAM [M. Hoppe et al 2021 Comp. Phys. Commun. 268, 108098]. To achieve a diverse set of disruption scenarios, Bayesian optimization is utilized to explore a range of massive material injection densities for deuterium and neon. The cost function is designed to distinguish between successful and unsuccessful disruption mitigation based on the runaway current, current quench time and transported fraction of the heat loss. In the non-activated scenarios, we find that fluid and kinetic disruption simulations can have significantly different runaway electron dynamics, due to an overestimation of the hot-tail generation rate by the fluid model. The primary cause of this is that the fluid hot-tail generation model neglects superthermal electron transport losses during the thermal quench. In the activated scenarios the fluid and kinetic models give similar predictions, which can be explained by the activated sources' significant influence on the RE dynamics and the seed.","sentences":["When simulating runaway electron dynamics in tokamak disruptions, fluid models with lower numerical cost are often preferred to more accurate kinetic models.","The aim of this work is to compare fluid and kinetic simulations of a large variety of different disruption scenarios in ITER.","We consider both non-activated and activated scenarios; for the latter we derive and implement kinetic sources for the Compton scattering and tritium beta decay runaway electron generation mechanisms in our simulation tool DREAM","[M. Hoppe et al 2021 Comp.","Phys.","Commun.","268, 108098].","To achieve a diverse set of disruption scenarios, Bayesian optimization is utilized to explore a range of massive material injection densities for deuterium and neon.","The cost function is designed to distinguish between successful and unsuccessful disruption mitigation based on the runaway current, current quench time and transported fraction of the heat loss.","In the non-activated scenarios, we find that fluid and kinetic disruption simulations can have significantly different runaway electron dynamics, due to an overestimation of the hot-tail generation rate by the fluid model.","The primary cause of this is that the fluid hot-tail generation model neglects superthermal electron transport losses during the thermal quench.","In the activated scenarios the fluid and kinetic models give similar predictions, which can be explained by the activated sources' significant influence on the RE dynamics and the seed."],"url":"http://arxiv.org/abs/2402.05843v1","category":"physics.plasm-ph"}
{"created":"2024-02-08 17:20:25","title":"Study and derivation of closures in the volume-filtered framework for particle-laden flows","abstract":"The volume-filtering of the Navier-Stokes equations allows to consider the effect that particles have on the fluid without further assumptions, but closures arise of which the implications are not fully understood. In the present paper, we carefully study every closure in the volume-filtered fluid momentum equation and investigate their impact on the momentum and energy transfer dependent on the filtering characteristics. We provide an analytical expression for the viscous closure that arises because filter and spatial derivative in the viscous term do not commute. An analytical expression for the regularization of the particle momentum source of a single sphere in the Stokes regime is derived. Furthermore, we propose a model for the subfilter stress tensor, which originates from filtering the advective term. The model for the subfilter stress tensor is shown to agree well with the subfilter stress tensor for small filter widths relative to the size of the particle. We show that the subfilter stress tensor requires modeling and should not be neglected. For small filter widths, we find that the commonly applied Gaussian regularization of the particle momentum source is a poor approximation of the spatial distribution of the particle momentum source, but for larger filter widths the spatial distribution approaches a Gaussian. Furthermore, we propose a modified advective term in the volume-filtered momentum equation that consistently circumvents the common stability issues observed at locally small fluid volume fractions and identify inconsistencies in previous studies of the phase-averaged kinetic energy of the volume-filtered fluid velocity. Finally, we propose a generally applicable form of the volume-filtered momentum equation and its closures based on clear and well-founded assumptions and propose guidelines for point-particle simulations based on the new findings.","sentences":["The volume-filtering of the Navier-Stokes equations allows to consider the effect that particles have on the fluid without further assumptions, but closures arise of which the implications are not fully understood.","In the present paper, we carefully study every closure in the volume-filtered fluid momentum equation and investigate their impact on the momentum and energy transfer dependent on the filtering characteristics.","We provide an analytical expression for the viscous closure that arises because filter and spatial derivative in the viscous term do not commute.","An analytical expression for the regularization of the particle momentum source of a single sphere in the Stokes regime is derived.","Furthermore, we propose a model for the subfilter stress tensor, which originates from filtering the advective term.","The model for the subfilter stress tensor is shown to agree well with the subfilter stress tensor for small filter widths relative to the size of the particle.","We show that the subfilter stress tensor requires modeling and should not be neglected.","For small filter widths, we find that the commonly applied Gaussian regularization of the particle momentum source is a poor approximation of the spatial distribution of the particle momentum source, but for larger filter widths the spatial distribution approaches a Gaussian.","Furthermore, we propose a modified advective term in the volume-filtered momentum equation that consistently circumvents the common stability issues observed at locally small fluid volume fractions and identify inconsistencies in previous studies of the phase-averaged kinetic energy of the volume-filtered fluid velocity.","Finally, we propose a generally applicable form of the volume-filtered momentum equation and its closures based on clear and well-founded assumptions and propose guidelines for point-particle simulations based on the new findings."],"url":"http://arxiv.org/abs/2402.05842v1","category":"physics.flu-dyn"}
{"created":"2024-02-08 17:18:01","title":"Dirichlet Flow Matching with Applications to DNA Sequence Design","abstract":"Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models. We show that na\\\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies. To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths. In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance. Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models. On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences. Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets. Code is available at https://github.com/HannesStark/dirichlet-flow-matching.","sentences":["Discrete diffusion or flow models could enable faster and more controllable sequence generation than autoregressive models.","We show that na\\\"ive linear flow matching on the simplex is insufficient toward this goal since it suffers from discontinuities in the training target and further pathologies.","To overcome this, we develop Dirichlet flow matching on the simplex based on mixtures of Dirichlet distributions as probability paths.","In this framework, we derive a connection between the mixtures' scores and the flow's vector field that allows for classifier and classifier-free guidance.","Further, we provide distilled Dirichlet flow matching, which enables one-step sequence generation with minimal performance hits, resulting in $O(L)$ speedups compared to autoregressive models.","On complex DNA sequence generation tasks, we demonstrate superior performance compared to all baselines in distributional metrics and in achieving desired design targets for generated sequences.","Finally, we show that our classifier-free guidance approach improves unconditional generation and is effective for generating DNA that satisfies design targets.","Code is available at https://github.com/HannesStark/dirichlet-flow-matching."],"url":"http://arxiv.org/abs/2402.05841v1","category":"q-bio.BM"}
{"created":"2024-02-08 17:17:06","title":"uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception Uncertainties","abstract":"The availability of a reliable map and a robust localization system is critical for the operation of an autonomous vehicle. In a modern system, both mapping and localization solutions generally employ convolutional neural network (CNN) --based perception. Hence, any algorithm should consider potential errors in perception for safe and robust functioning. In this work, we present uncertainty-aware panoptic Localization and Mapping (uPLAM), which employs perception uncertainty as a bridge to fuse the perception information with classical localization and mapping approaches. We introduce an uncertainty-based map aggregation technique to create a long-term panoptic bird's eye view map and provide an associated mapping uncertainty. Our map consists of surface semantics and landmarks with unique IDs. Moreover, we present panoptic uncertainty-aware particle filter-based localization. To this end, we propose an uncertainty-based particle importance weight calculation for the adaptive incorporation of perception information into localization. We also present a new dataset for evaluating long-term panoptic mapping and map-based localization. Extensive evaluations showcase that our proposed uncertainty incorporation leads to better mapping with reliable uncertainty estimates and accurate localization. We make our dataset and code available at: \\url{http://uplam.cs.uni-freiburg.de}","sentences":["The availability of a reliable map and a robust localization system is critical for the operation of an autonomous vehicle.","In a modern system, both mapping and localization solutions generally employ convolutional neural network (CNN) --based perception.","Hence, any algorithm should consider potential errors in perception for safe and robust functioning.","In this work, we present uncertainty-aware panoptic Localization and Mapping (uPLAM), which employs perception uncertainty as a bridge to fuse the perception information with classical localization and mapping approaches.","We introduce an uncertainty-based map aggregation technique to create a long-term panoptic bird's eye view map and provide an associated mapping uncertainty.","Our map consists of surface semantics and landmarks with unique IDs.","Moreover, we present panoptic uncertainty-aware particle filter-based localization.","To this end, we propose an uncertainty-based particle importance weight calculation for the adaptive incorporation of perception information into localization.","We also present a new dataset for evaluating long-term panoptic mapping and map-based localization.","Extensive evaluations showcase that our proposed uncertainty incorporation leads to better mapping with reliable uncertainty estimates and accurate localization.","We make our dataset and code available at: \\url{http://uplam.cs.uni-freiburg.de}"],"url":"http://arxiv.org/abs/2402.05840v1","category":"cs.RO"}
{"created":"2024-02-08 17:15:13","title":"Introducing q-deformed binomial coefficients of words","abstract":"Gaussian binomial coefficients are q-analogues of the binomial coefficients of integers. On the other hand, binomial coefficients have been extended to finite words, i.e., elements of the finitely generated free monoids. In this paper we bring together these two notions by introducing q-analogues of binomial coefficients of words. We study their basic properties, e.g., by extending classical formulas such as the q-Vandermonde and Manvel's et al. identities to our setting. As a consequence, we get information about the structure of the considered words: these q-deformations of binomial coefficients of words contain much richer information than the original coefficients. From an algebraic perspective, we introduce a q-shuffle and a family q-infiltration products for non-commutative formal power series. Finally, we apply our results to generalize a theorem of Eilenberg characterizing so-called p-group languages. We show that a language is of this type if and only if it is a Boolean combination of specific languages defined through q-binomial coefficients seen as polynomials over $\\mathbb{F}_p$.","sentences":["Gaussian binomial coefficients are q-analogues of the binomial coefficients of integers.","On the other hand, binomial coefficients have been extended to finite words, i.e., elements of the finitely generated free monoids.","In this paper we bring together these two notions by introducing q-analogues of binomial coefficients of words.","We study their basic properties, e.g., by extending classical formulas such as the q-Vandermonde and Manvel's et al. identities to our setting.","As a consequence, we get information about the structure of the considered words: these q-deformations of binomial coefficients of words contain much richer information than the original coefficients.","From an algebraic perspective, we introduce a q-shuffle and a family q-infiltration products for non-commutative formal power series.","Finally, we apply our results to generalize a theorem of Eilenberg characterizing so-called p-group languages.","We show that a language is of this type if and only if it is a Boolean combination of specific languages defined through q-binomial coefficients seen as polynomials over $\\mathbb{F}_p$."],"url":"http://arxiv.org/abs/2402.05838v1","category":"math.CO"}
{"created":"2024-02-08 17:13:17","title":"Shape Optimization of Eigenfrequencies in MEMS Gyroscopes","abstract":"Microelectromechanical systems (MEMS) gyroscopes are widely used in consumer and automotive applications. They have to fulfill a vast number of product requirements which lead to complex mechanical designs of the resonating structure. Arriving at a final design is a cumbersome process that relies heavily on human experience in conjunction with design optimization methods. In this work, we apply node-based shape optimization to the design of a MEMS gyroscope. For that purpose, we parametrize the coordinates of the nodes of the finite element method (FEM) mesh that discretize the shapes of the springs. We then implement the gradients of the mechanical eigenfrequencies and typical MEMS manufacturability constraints, with respect to the design parameters, in a FEM code. Using gradient-based optimization we tune the gyroscope's frequency split and shift spurious modes away from the first three multiples of the gyroscope's drive frequency while manufacturability constraints are fulfilled. The resulting optimized design exhibits novel geometrical shapes which defy any human intuition. Overall, we demonstrate that shape optimization can not only solve optimization problems in MEMS design without required human intervention, but also explores geometry solutions which can otherwise not be addressed. In this way, node-based shape optimization opens up a much larger space of possible design solutions, which is crucial for facing the ever increasing product requirements. Our approach is generic and applicable to many other types of MEMS resonators.","sentences":["Microelectromechanical systems (MEMS) gyroscopes are widely used in consumer and automotive applications.","They have to fulfill a vast number of product requirements which lead to complex mechanical designs of the resonating structure.","Arriving at a final design is a cumbersome process that relies heavily on human experience in conjunction with design optimization methods.","In this work, we apply node-based shape optimization to the design of a MEMS gyroscope.","For that purpose, we parametrize the coordinates of the nodes of the finite element method (FEM) mesh that discretize the shapes of the springs.","We then implement the gradients of the mechanical eigenfrequencies and typical MEMS manufacturability constraints, with respect to the design parameters, in a FEM code.","Using gradient-based optimization we tune the gyroscope's frequency split and shift spurious modes away from the first three multiples of the gyroscope's drive frequency while manufacturability constraints are fulfilled.","The resulting optimized design exhibits novel geometrical shapes which defy any human intuition.","Overall, we demonstrate that shape optimization can not only solve optimization problems in MEMS design without required human intervention, but also explores geometry solutions which can otherwise not be addressed.","In this way, node-based shape optimization opens up a much larger space of possible design solutions, which is crucial for facing the ever increasing product requirements.","Our approach is generic and applicable to many other types of MEMS resonators."],"url":"http://arxiv.org/abs/2402.05837v1","category":"cs.CE"}
{"created":"2024-02-08 17:12:18","title":"Scalar curvature rigidity and the higher mapping degree","abstract":"A closed connected oriented Riemannian manifold $N$ with non-vanishing Euler characteristic, non-negative curvature operator and $0<2\\text{Ric}_N<\\text{scal}_N$ is area-rigid in the sense that any area non-increasing spin map $f\\colon M\\to N$ from a closed connected oriented Riemannian manifold $M$ with non-vanishing $\\hat{A}$-degree and $\\text{scal}_M\\geq \\text{scal}_N\\mathop{\\circ}\\nolimits f$ is a Riemannian submersion. This is due to Goette and Semmelmann and generalizes a result by Llarull. In this article, we show area-rigidity for not necessarily orientable manifolds with respect to a larger class of maps $f\\colon M\\to N$ by replacing the topological condition on the $\\hat{A}$-degree by a less restrictive condition involving the so-called higher mapping degree. This includes fiber bundles over even dimensional spheres with enlargeable fibers, e.g. $pr_1\\colon S^{2n}\\times T^k \\to S^{2n}$. We develop a technique to extract from a non-vanishing higher index a geometrically useful family of almost $\\mathcal{D}$-harmonic sections. This also leads to a new proof of the fact that any closed connected spin manifold with non-negative scalar curvature and non-trivial Rosenberg index is Ricci flat.","sentences":["A closed connected oriented Riemannian manifold $N$ with non-vanishing Euler characteristic, non-negative curvature operator and $0<2\\text{Ric}_N<\\text{scal}_N$ is area-rigid in the sense that any area non-increasing spin map $f\\colon M\\to N$ from a closed connected oriented Riemannian manifold $M$ with non-vanishing $\\hat{A}$-degree and $\\text{scal}_M\\geq \\text{scal}_N\\mathop{\\circ}\\nolimits f$ is a Riemannian submersion.","This is due to Goette and Semmelmann and generalizes a result by Llarull.","In this article, we show area-rigidity for not necessarily orientable manifolds with respect to a larger class of maps $f\\colon M\\to N$ by replacing the topological condition on the $\\hat{A}$-degree by a less restrictive condition involving the so-called higher mapping degree.","This includes fiber bundles over even dimensional spheres with enlargeable fibers, e.g. $pr_1\\colon S^{2n}\\times T^k","\\to S^{2n}$.","We develop a technique to extract from a non-vanishing higher index a geometrically useful family of almost $\\mathcal{D}$-harmonic sections.","This also leads to a new proof of the fact that any closed connected spin manifold with non-negative scalar curvature and non-trivial Rosenberg index is Ricci flat."],"url":"http://arxiv.org/abs/2402.05834v1","category":"math.DG"}
{"created":"2024-02-08 17:09:35","title":"Phenomenology of the dispersion law in three-dimensional quantum space","abstract":"The concept of a quantum structure underlying space has been proposed for quite some time, yet it has largely eluded direct observation. There is a prevailing notion that the minuscule effects stemming from quantum space could, under certain astronomical conditions, accumulate to an observable magnitude. Typically, these effects are perturbative, limited to the first one or two orders. This study presents a full dispersion law derived from a three-dimensional quantum space model. Our primary focus is investigating the in-vacuo dispersion phenomenon and the threshold anomaly. Furthermore, we briefly explore the implications of additional spatial dimensions within this framework and suggest a way of distinguishing between various theories that agree in the leading order approximation.","sentences":["The concept of a quantum structure underlying space has been proposed for quite some time, yet it has largely eluded direct observation.","There is a prevailing notion that the minuscule effects stemming from quantum space could, under certain astronomical conditions, accumulate to an observable magnitude.","Typically, these effects are perturbative, limited to the first one or two orders.","This study presents a full dispersion law derived from a three-dimensional quantum space model.","Our primary focus is investigating the in-vacuo dispersion phenomenon and the threshold anomaly.","Furthermore, we briefly explore the implications of additional spatial dimensions within this framework and suggest a way of distinguishing between various theories that agree in the leading order approximation."],"url":"http://arxiv.org/abs/2402.05832v1","category":"gr-qc"}
{"created":"2024-02-08 17:09:35","title":"Maximizing the electromagnetic efficiency of spintronic terahertz emitters","abstract":"Spintronic Terahertz Emitters (STE) represent a significant advancement in source technology, exploiting the ultrafast demagnetization process of spin-electrons to unveil a 30THz wide, gapless spectrum, accessible through femtosecond lasers across the full VIS-IR range. This innovation not only positions STEs as a pivotal advancement in THz source technology but also underscores their role as a cost-effective, high-performance solution, thereby redefining standards within the field. However, the inherent spintronic nature of these devices introduces a challenge: a lower optical-to-terahertz conversion efficiency, which positions them at a notable disadvantage relative to other sources. In response, this work aims to substantially improve the electromagnetic efficiency of these emitters. This is accomplished by maximizing the energy conversion from the pumping laser for spin-electron generation. Our design integrates STEs with an optimized 1D trapping cavity, specifically engineered to fulfill critical aspects of ultrafast excitation and THz extraction. As a result, we have realized a 245% enhancement in emission and an increase of 8dB in overall intensity, positions our results among the most substantial improvements documented in this field. Furthermore, we delineate the optimal geometry for the deployment of STEs and explore the strategic selection of substrates in depth. Such enhanced emitters advance spintronic emitters towards broader applications in time-domain spectroscopy, ellipsometry, and nonlinear THz-pump spectroscopy. Enhancing spintronic emitter efficiency, combined with rapid magnetic field modulation, indicates the potential for dynamic ranges that rival traditional sources. Our predictions of STE's efficiencies, made through an electromagnetic approach, highlight its capability to uncover overlooked aspects from an optical standpoint, leading to subsequent improvements.","sentences":["Spintronic Terahertz Emitters (STE) represent a significant advancement in source technology, exploiting the ultrafast demagnetization process of spin-electrons to unveil a 30THz wide, gapless spectrum, accessible through femtosecond lasers across the full VIS-IR range.","This innovation not only positions STEs as a pivotal advancement in THz source technology but also underscores their role as a cost-effective, high-performance solution, thereby redefining standards within the field.","However, the inherent spintronic nature of these devices introduces a challenge: a lower optical-to-terahertz conversion efficiency, which positions them at a notable disadvantage relative to other sources.","In response, this work aims to substantially improve the electromagnetic efficiency of these emitters.","This is accomplished by maximizing the energy conversion from the pumping laser for spin-electron generation.","Our design integrates STEs with an optimized 1D trapping cavity, specifically engineered to fulfill critical aspects of ultrafast excitation and THz extraction.","As a result, we have realized a 245% enhancement in emission and an increase of 8dB in overall intensity, positions our results among the most substantial improvements documented in this field.","Furthermore, we delineate the optimal geometry for the deployment of STEs and explore the strategic selection of substrates in depth.","Such enhanced emitters advance spintronic emitters towards broader applications in time-domain spectroscopy, ellipsometry, and nonlinear THz-pump spectroscopy.","Enhancing spintronic emitter efficiency, combined with rapid magnetic field modulation, indicates the potential for dynamic ranges that rival traditional sources.","Our predictions of STE's efficiencies, made through an electromagnetic approach, highlight its capability to uncover overlooked aspects from an optical standpoint, leading to subsequent improvements."],"url":"http://arxiv.org/abs/2402.05833v1","category":"physics.optics"}
{"created":"2024-02-08 17:09:34","title":"Positive orthogonalizing weights on the unit circle for the generalized Bessel polynomials","abstract":"In this paper we study the generalized Bessel polynomials $y_n(x,a,b)$ (in the notation of Krall and Frink). Let $a>1$, $b\\in(-1/3,1/3)\\backslash\\{ 0\\}$. In this case we present the following positive continuous weights $p(\\theta) = p(\\theta,a,b)$ on the unit circle for $y_n(x,a,b)$: $$ 2\\pi p(\\theta,a,b) = -1 + 2(a-1) \\int_0^1 e^{-bu\\cos\\theta} \\cos(bu\\sin\\theta) (1-u)^{a-2} du, $$ where $\\theta\\in[0,2\\pi]$. Namely, we have $$ \\int_0^{2\\pi} y_n(e^{i\\theta},a,b) y_m(e^{i\\theta},a,b) p(\\theta,a,b) d\\theta = C_n \\delta_{n,m},\\qquad C_n\\not=0,\\ n,m\\in\\mathbb{Z}_+. $$ Notice that this orthogonality differs from the usual orthogonality of OPUC. Some applications of the above orthogonality are given.","sentences":["In this paper we study the generalized Bessel polynomials $y_n(x,a,b)$ (in the notation of Krall and Frink).","Let $a>1$, $b\\in(-1/3,1/3)\\backslash\\{ 0\\}$.","In this case we present the following positive continuous weights $p(\\theta) = p(\\theta,a,b)$ on the unit circle for $y_n(x,a,b)$: $$ 2\\pi p(\\theta,a,b) =","-1 + 2(a-1) \\int_0^1 e^{-bu\\cos\\theta} \\cos(bu\\sin\\theta) (1-u)^{a-2} du, $$ where $\\theta\\in[0,2\\pi]$. Namely, we have $$ \\int_0^{2\\pi} y_n(e^{i\\theta},a,b) y_m(e^{i\\theta},a,b) p(\\theta,a,b) d\\theta = C_n \\delta_{n,m},\\qquad C_n\\not=0,\\ n,m\\in\\mathbb{Z}_+. $$ Notice that this orthogonality differs from the usual orthogonality of OPUC.","Some applications of the above orthogonality are given."],"url":"http://arxiv.org/abs/2402.05831v1","category":"math.CA"}
{"created":"2024-02-08 17:09:12","title":"Sparse-VQ Transformer: An FFN-Free Framework with Vector Quantization for Enhanced Time Series Forecasting","abstract":"Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain. Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments. Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels. To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ). Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture. Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting. Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in MAE for univariate and multivariate time series forecasting, respectively. Moreover, it can be seamlessly integrated with existing transformer-based models to elevate their performance.","sentences":["Time series analysis is vital for numerous applications, and transformers have become increasingly prominent in this domain.","Leading methods customize the transformer architecture from NLP and CV, utilizing a patching technique to convert continuous signals into segments.","Yet, time series data are uniquely challenging due to significant distribution shifts and intrinsic noise levels.","To address these two challenges,we introduce the Sparse Vector Quantized FFN-Free Transformer (Sparse-VQ).","Our methodology capitalizes on a sparse vector quantization technique coupled with Reverse Instance Normalization (RevIN) to reduce noise impact and capture sufficient statistics for forecasting, serving as an alternative to the Feed-Forward layer (FFN) in the transformer architecture.","Our FFN-free approach trims the parameter count, enhancing computational efficiency and reducing overfitting.","Through evaluations across ten benchmark datasets, including the newly introduced CAISO dataset, Sparse-VQ surpasses leading models with a 7.84% and 4.17% decrease in MAE for univariate and multivariate time series forecasting, respectively.","Moreover, it can be seamlessly integrated with existing transformer-based models to elevate their performance."],"url":"http://arxiv.org/abs/2402.05830v1","category":"cs.LG"}
{"created":"2024-02-08 17:08:08","title":"Limitations of Agents Simulated by Predictive Models","abstract":"There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models. We outline two structural reasons for why these models can fail when turned into agents. First, we discuss auto-suggestive delusions. Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations. Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence. When a model generates a sequence of actions, the model's implicit prediction of the policy that generated those actions can serve as a confounding variable. The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative. We show that both of those failures are fixed by including a feedback loop from the environment, that is, re-training the models on their own actions. We give simple demonstrations of both limitations using Decision Transformers and confirm that empirical results agree with our conceptual and formal analysis. Our treatment provides a unifying view of those failure modes, and informs the question of why fine-tuning offline learned policies with online learning makes them more effective.","sentences":["There is increasing focus on adapting predictive models into agent-like systems, most notably AI assistants based on language models.","We outline two structural reasons for why these models can fail when turned into agents.","First, we discuss auto-suggestive delusions.","Prior work has shown theoretically that models fail to imitate agents that generated the training data if the agents relied on hidden observations: the hidden observations act as confounding variables, and the models treat actions they generate as evidence for nonexistent observations.","Second, we introduce and formally study a related, novel limitation: predictor-policy incoherence.","When a model generates a sequence of actions, the model's implicit prediction of the policy that generated those actions can serve as a confounding variable.","The result is that models choose actions as if they expect future actions to be suboptimal, causing them to be overly conservative.","We show that both of those failures are fixed by including a feedback loop from the environment, that is, re-training the models on their own actions.","We give simple demonstrations of both limitations using Decision Transformers and confirm that empirical results agree with our conceptual and formal analysis.","Our treatment provides a unifying view of those failure modes, and informs the question of why fine-tuning offline learned policies with online learning makes them more effective."],"url":"http://arxiv.org/abs/2402.05829v1","category":"cs.AI"}
{"created":"2024-02-08 17:07:42","title":"Discovering Temporally-Aware Reinforcement Learning Algorithms","abstract":"Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions. To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution. However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or \"training horizon\". In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability. For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities. This paper contends that ignoring the optimization time horizon significantly restricts the expressive potential of discovered learning algorithms. We propose a simple augmentation to two existing objective discovery approaches that allows the discovered algorithm to dynamically update its objective function throughout the agent's training procedure, resulting in expressive schedules and increased generalization across different training horizons. In the process, we find that commonly used meta-gradient approaches fail to discover such adaptive objective functions while evolution strategies discover highly dynamic learning rules. We demonstrate the effectiveness of our approach on a wide range of tasks and analyze the resulting learned algorithms, which we find effectively balance exploration and exploitation by modifying the structure of their learning rules throughout the agent's lifetime.","sentences":["Recent advancements in meta-learning have enabled the automatic discovery of novel reinforcement learning algorithms parameterized by surrogate objective functions.","To improve upon manually designed algorithms, the parameterization of this learned objective function must be expressive enough to represent novel principles of learning (instead of merely recovering already established ones) while still generalizing to a wide range of settings outside of its meta-training distribution.","However, existing methods focus on discovering objective functions that, like many widely used objective functions in reinforcement learning, do not take into account the total number of steps allowed for training, or \"training horizon\".","In contrast, humans use a plethora of different learning objectives across the course of acquiring a new ability.","For instance, students may alter their studying techniques based on the proximity to exam deadlines and their self-assessed capabilities.","This paper contends that ignoring the optimization time horizon significantly restricts the expressive potential of discovered learning algorithms.","We propose a simple augmentation to two existing objective discovery approaches that allows the discovered algorithm to dynamically update its objective function throughout the agent's training procedure, resulting in expressive schedules and increased generalization across different training horizons.","In the process, we find that commonly used meta-gradient approaches fail to discover such adaptive objective functions while evolution strategies discover highly dynamic learning rules.","We demonstrate the effectiveness of our approach on a wide range of tasks and analyze the resulting learned algorithms, which we find effectively balance exploration and exploitation by modifying the structure of their learning rules throughout the agent's lifetime."],"url":"http://arxiv.org/abs/2402.05828v1","category":"cs.LG"}
{"created":"2024-02-08 17:06:45","title":"Is it Possible to Edit Large Language Models Robustly?","abstract":"Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization. To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation. However, the robustness of model editing remains an open question. This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI. Concretely, we conduct extensive analysis to address the three key research questions. Q1: Can edited LLMs behave consistently resembling communicative AI in realistic situations? Q2: To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory? Q3: Which knowledge features are correlated with the performance and robustness of editing? Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs. On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline. Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively.","sentences":["Large language models (LLMs) have played a pivotal role in building communicative AI to imitate human behaviors but face the challenge of efficient customization.","To tackle this challenge, recent studies have delved into the realm of model editing, which manipulates specific memories of language models and changes the related language generation.","However, the robustness of model editing remains an open question.","This work seeks to understand the strengths and limitations of editing methods, thus facilitating robust, realistic applications of communicative AI.","Concretely, we conduct extensive analysis to address the three key research questions.","Q1:","Can edited LLMs behave consistently resembling communicative AI in realistic situations?","Q2:","To what extent does the rephrasing of prompts lead LLMs to deviate from the edited knowledge memory?","Q3:","Which knowledge features are correlated with the performance and robustness of editing?","Our experimental results uncover a substantial disparity between existing editing methods and the practical application of LLMs.","On rephrased prompts that are complex and flexible but common in realistic applications, the performance of editing experiences a significant decline.","Further analysis shows that more popular knowledge is memorized better, easier to recall, and more challenging to edit effectively."],"url":"http://arxiv.org/abs/2402.05827v1","category":"cs.CL"}
{"created":"2024-02-08 17:06:15","title":"Predicting concentration changes via discrete sampling","abstract":"To successfully navigate chemical gradients, microorganisms need to predict how the ligand concentration changes in space. Due to their limited size, they do not take a spatial derivative over their body length but rather a temporal derivative, comparing the current signal with that in the recent past, over the so-called adaptation time. This strategy is pervasive in biology, but it remains unclear what determines the accuracy of such measurements. Using a generalized version of the previously established sampling framework, we investigate how resource limitations and the statistics of the input signal set the optimal design of a well-characterized network that measures temporal concentration changes: the Escherichia coli chemotaxis network. Our results show how an optimal adaptation time arises from the trade-off between the sampling error, caused by the stochastic nature of the network, and the dynamical error, caused by uninformative fluctuations in the input. A larger resource availability reduces the sampling error, which allows for a smaller adaptation time, thereby simultaneously decreasing the dynamical error. Similarly, we find that the optimal adaptation time scales inversely with the gradient steepness, because steeper gradients lift the signal above the noise and reduce the sampling error. These findings shed light on the principles that govern the optimal design of the E. coli chemotaxis network specifically, and any system measuring temporal changes more broadly.","sentences":["To successfully navigate chemical gradients, microorganisms need to predict how the ligand concentration changes in space.","Due to their limited size, they do not take a spatial derivative over their body length but rather a temporal derivative, comparing the current signal with that in the recent past, over the so-called adaptation time.","This strategy is pervasive in biology, but it remains unclear what determines the accuracy of such measurements.","Using a generalized version of the previously established sampling framework, we investigate how resource limitations and the statistics of the input signal set the optimal design of a well-characterized network that measures temporal concentration changes: the Escherichia coli chemotaxis network.","Our results show how an optimal adaptation time arises from the trade-off between the sampling error, caused by the stochastic nature of the network, and the dynamical error, caused by uninformative fluctuations in the input.","A larger resource availability reduces the sampling error, which allows for a smaller adaptation time, thereby simultaneously decreasing the dynamical error.","Similarly, we find that the optimal adaptation time scales inversely with the gradient steepness, because steeper gradients lift the signal above the noise and reduce the sampling error.","These findings shed light on the principles that govern the optimal design of the E. coli chemotaxis network specifically, and any system measuring temporal changes more broadly."],"url":"http://arxiv.org/abs/2402.05825v1","category":"q-bio.MN"}
{"created":"2024-02-08 17:03:10","title":"FusionSF: Fuse Heterogeneous Modalities in a Vector Quantized Framework for Robust Solar Power Forecasting","abstract":"Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety. This problem becomes more demanding for those newly installed solar plants which lack sufficient data. Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities. In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance. We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting. Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants. Moreover, we collect and release a multi-modal solar power (MMSP) dataset from real-world plants to further promote the research of multi-modal solar forecasting algorithms. Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both zero-shot forecasting and scenarios rich with training data, surpassing leading models. We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a capacity of over 15GW.","sentences":["Accurate solar power forecasting is crucial to integrate photovoltaic plants into the electric grid, schedule and secure the power grid safety.","This problem becomes more demanding for those newly installed solar plants which lack sufficient data.","Current research predominantly relies on historical solar power data or numerical weather prediction in a single-modality format, ignoring the complementary information provided in different modalities.","In this paper, we propose a multi-modality fusion framework to integrate historical power data, numerical weather prediction, and satellite images, significantly improving forecast performance.","We introduce a vector quantized framework that aligns modalities with varying information densities, striking a balance between integrating sufficient information and averting model overfitting.","Our framework demonstrates strong zero-shot forecasting capability, which is especially useful for those newly installed plants.","Moreover, we collect and release a multi-modal solar power (MMSP) dataset from real-world plants to further promote the research of multi-modal solar forecasting algorithms.","Our extensive experiments show that our model not only operates with robustness but also boosts accuracy in both zero-shot forecasting and scenarios rich with training data, surpassing leading models.","We have incorporated it into our eForecaster platform and deployed it for more than 300 solar plants with a capacity of over 15GW."],"url":"http://arxiv.org/abs/2402.05823v1","category":"cs.LG"}
{"created":"2024-02-08 16:54:46","title":"$L$-systems and the Lov\u00e1sz number","abstract":"Given integers $n > k > 0$, and a set of integers $L \\subset [0, k-1]$, an $L$-system is a family of sets $\\mathcal{F} \\subset \\binom{[n]}{k}$ such that $|F \\cap F'| \\in L$ for distinct $F, F'\\in \\mathcal{F}$. $L$-systems correspond to independent sets in a certain generalized Johnson graph $G(n, k, L)$, so that the maximum size of an $L$-system is equivalent to finding the independence number of the graph $G(n, k, L)$. The Lov\\'asz number $\\vartheta(G)$ is a semidefinite programming approximation of the independence number of a graph $G$.   In this paper, we determine the order of magnitude of $\\vartheta(G(n, k, L))$ of any generalized Johnson graph with $k$ and $L$ fixed and $n\\rightarrow \\infty$. As an application of this theorem, we give an explicit construction of a graph $G$ on $n$ vertices with large gap between the Lov\\'asz number and the Shannon capacity $c(G)$. Specifically, we prove that for any $\\epsilon > 0$, for infinitely many $n$ there is a generalized Johnson graph $G$ on $n$ vertices which has ratio $\\vartheta(G)/c(G) = \\Omega(n^{1-\\epsilon})$, which greatly improves on the best known explicit construction.","sentences":["Given integers $n > k > 0$, and a set of integers $L \\subset [0, k-1]$, an $L$-system is a family of sets $\\mathcal{F} \\subset \\binom{[n]}{k}$ such that $|F \\cap F'| \\in L$ for distinct $F, F'\\in \\mathcal{F}$. $L$-systems correspond to independent sets in a certain generalized Johnson graph $G(n, k, L)$, so that the maximum size of an $L$-system is equivalent to finding the independence number of the graph $G(n, k, L)$.","The Lov\\'asz number $\\vartheta(G)$ is a semidefinite programming approximation of the independence number of a graph $G$.   In this paper, we determine the order of magnitude of $\\vartheta(G(n, k, L))$ of any generalized Johnson graph with $k$ and $L$ fixed and $n\\rightarrow \\infty$. As an application of this theorem, we give an explicit construction of a graph $G$ on $n$ vertices with large gap between the Lov\\'asz number and the Shannon capacity $c(G)$. Specifically, we prove that for any $\\epsilon > 0$, for infinitely many $n$ there is a generalized Johnson graph $G$ on $n$ vertices which has ratio $\\vartheta(G)/c(G) = \\Omega(n^{1-\\epsilon})$, which greatly improves on the best known explicit construction."],"url":"http://arxiv.org/abs/2402.05818v1","category":"math.CO"}
{"created":"2024-02-08 16:54:20","title":"Using YOLO v7 to Detect Kidney in Magnetic Resonance Imaging: A Supervised Contrastive Learning","abstract":"Introduction This study explores the use of the latest You Only Look Once (YOLO V7) object detection method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats. Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys. A total of 5657 MRI scans for 1084 patients were retrieved. 326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors. A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set). The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model. Ten benchmark training sets were created with generated coordinates on not-segmented patients. The final model used to predict the kidney in the primary test set. We reported the positive predictive value (PPV), sensitivity, and mean average precision (mAP). Results The primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of 0.87 +/- 0.04, and mAP of 0.91 +/- 0.02. The best primary model yielded a PPV of 0.97, sensitivity of 0.92, and mAP of 0.95. The final model demonstrated an average PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95 +/- 0.01. Conclusion Using a semi-supervised approach with a medical image library, we developed a high-performing model for kidney detection. Further external validation is required to assess the model's generalizability.","sentences":["Introduction This study explores the use of the latest You Only Look Once (YOLO V7) object detection method to enhance kidney detection in medical imaging by training and testing a modified YOLO V7 on medical image formats.","Methods Study includes 878 patients with various subtypes of renal cell carcinoma (RCC) and 206 patients with normal kidneys.","A total of 5657 MRI scans for 1084 patients were retrieved.","326 patients with 1034 tumors recruited from a retrospective maintained database, and bounding boxes were drawn around their tumors.","A primary model was trained on 80% of annotated cases, with 20% saved for testing (primary test set).","The best primary model was then used to identify tumors in the remaining 861 patients and bounding box coordinates were generated on their scans using the model.","Ten benchmark training sets were created with generated coordinates on not-segmented patients.","The final model used to predict the kidney in the primary test set.","We reported the positive predictive value (PPV), sensitivity, and mean average precision (mAP).","Results The primary training set showed an average PPV of 0.94 +/- 0.01, sensitivity of 0.87 +/- 0.04, and mAP of 0.91 +/- 0.02.","The best primary model yielded a PPV of 0.97, sensitivity of 0.92, and mAP of 0.95.","The final model demonstrated an average PPV of 0.95 +/- 0.03, sensitivity of 0.98 +/- 0.004, and mAP of 0.95 +/- 0.01.","Conclusion Using a semi-supervised approach with a medical image library, we developed a high-performing model for kidney detection.","Further external validation is required to assess the model's generalizability."],"url":"http://arxiv.org/abs/2402.05817v1","category":"eess.IV"}
{"created":"2024-02-08 16:50:02","title":"Connectedness of level sets for non-degenerate integrable systems that extend complexity one torus actions","abstract":"In this paper we study the connectedness of the level sets of integrable systems that have only non-degenerate singular points and extend complexity one $T$-spaces with proper moment maps. Our main result states that if there are no singular points with a hyperbolic block and connected $T$-stabilizer, then each level set is connected. Moreover, we prove that the above condition is necessary if either some reduced space is simply connected or the moment map for the integrable system is generic in a natural sense.","sentences":["In this paper we study the connectedness of the level sets of integrable systems that have only non-degenerate singular points and extend complexity one $T$-spaces with proper moment maps.","Our main result states that if there are no singular points with a hyperbolic block and connected $T$-stabilizer, then each level set is connected.","Moreover, we prove that the above condition is necessary if either some reduced space is simply connected or the moment map for the integrable system is generic in a natural sense."],"url":"http://arxiv.org/abs/2402.05814v1","category":"math.SG"}
{"created":"2024-02-08 16:50:01","title":"Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models","abstract":"The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data. Here, a novel approach is introduced to achieve precise and selective forgetting within language models. Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks. Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination. To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies. The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline annotation entails a robust two-stage process based on Large Language Models (LLMs).","sentences":["The aim of this study is to investigate Machine Unlearning (MU), a burgeoning field focused on addressing concerns related to neural models inadvertently retaining personal or sensitive data.","Here, a novel approach is introduced to achieve precise and selective forgetting within language models.","Unlike previous methodologies that adopt completely opposing training objectives, this approach aims to mitigate adverse effects on language model performance, particularly in generation tasks.","Furthermore, two innovative evaluation metrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and Sensitive Information Memory Accuracy (S-MA), designed to gauge the effectiveness of sensitive information elimination.","To reinforce the forgetting framework, an effective method for annotating sensitive scopes is presented, involving both online and offline strategies.","The online selection mechanism leverages language probability scores to ensure computational efficiency, while the offline annotation entails a robust two-stage process based on Large Language Models (LLMs)."],"url":"http://arxiv.org/abs/2402.05813v1","category":"cs.CL"}
{"created":"2024-02-08 16:49:41","title":"FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension","abstract":"Frequently Asked Questions (FAQs) refer to the most common inquiries about specific content. They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information. In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models. We present a literature review covering traditional question-answering systems, highlighting their limitations when applied directly to the FAQ generation task. We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance. We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the question-answer pairs to maximise human comprehension. Qualitative human evaluation showcases the generated FAQs to be well-constructed and readable, while also utilising domain-specific constructs to highlight domain-based nuances and jargon in the original content.","sentences":["Frequently Asked Questions (FAQs) refer to the most common inquiries about specific content.","They serve as content comprehension aids by simplifying topics and enhancing understanding through succinct presentation of information.","In this paper, we address FAQ generation as a well-defined Natural Language Processing (NLP) task through the development of an end-to-end system leveraging text-to-text transformation models.","We present a literature review covering traditional question-answering systems, highlighting their limitations when applied directly to the FAQ generation task.","We propose our system capable of building FAQs from textual content tailored to specific domains, enhancing their accuracy and relevance.","We utilise self-curated algorithms for obtaining optimal representation of information to be provided as input and also for ranking the question-answer pairs to maximise human comprehension.","Qualitative human evaluation showcases the generated FAQs to be well-constructed and readable, while also utilising domain-specific constructs to highlight domain-based nuances and jargon in the original content."],"url":"http://arxiv.org/abs/2402.05812v1","category":"cs.CL"}
{"created":"2024-02-08 16:47:43","title":"You Only Need One Color Space: An Efficient Network for Low-light Image Enhancement","abstract":"Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images. Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space. Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process. Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images. To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI). It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters. Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two branches dedicated to processing the decoupled image brightness and color in the HVI space. Within CIDNet, we introduce the Lightweight Cross-Attention (LCA) module to facilitate interaction between image structure and content information in both branches, while also suppressing noise in low-light images. Finally, we conducted 22 quantitative and qualitative experiments to show that the proposed CIDNet outperforms the state-of-the-art methods on 11 datasets. The code will be available at https://github.com/Fediory/HVI-CIDNet.","sentences":["Low-Light Image Enhancement (LLIE) task tends to restore the details and visual information from corrupted low-light images.","Most existing methods learn the mapping function between low/normal-light images by Deep Neural Networks (DNNs) on sRGB and HSV color space.","Nevertheless, enhancement involves amplifying image signals, and applying these color spaces to low-light images with a low signal-to-noise ratio can introduce sensitivity and instability into the enhancement process.","Consequently, this results in the presence of color artifacts and brightness artifacts in the enhanced images.","To alleviate this problem, we propose a novel trainable color space, named Horizontal/Vertical-Intensity (HVI).","It not only decouples brightness and color from RGB channels to mitigate the instability during enhancement but also adapts to low-light images in different illumination ranges due to the trainable parameters.","Further, we design a novel Color and Intensity Decoupling Network (CIDNet) with two branches dedicated to processing the decoupled image brightness and color in the HVI space.","Within CIDNet, we introduce the Lightweight Cross-Attention (LCA) module to facilitate interaction between image structure and content information in both branches, while also suppressing noise in low-light images.","Finally, we conducted 22 quantitative and qualitative experiments to show that the proposed CIDNet outperforms the state-of-the-art methods on 11 datasets.","The code will be available at https://github.com/Fediory/HVI-CIDNet."],"url":"http://arxiv.org/abs/2402.05809v1","category":"cs.CV"}
{"created":"2024-02-08 16:46:26","title":"Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning","abstract":"In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models. The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization. Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation. R$^3$ overcomes these limitations by learning from correct demonstrations. Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages. Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors. Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average. Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models.","sentences":["In this paper, we propose R$^3$: Learning Reasoning through Reverse Curriculum Reinforcement Learning (RL), a novel method that employs only outcome supervision to achieve the benefits of process supervision for large language models.","The core challenge in applying RL to complex reasoning is to identify a sequence of actions that result in positive rewards and provide appropriate supervision for optimization.","Outcome supervision provides sparse rewards for final results without identifying error locations, whereas process supervision offers step-wise rewards but requires extensive manual annotation.","R$^3$ overcomes these limitations by learning from correct demonstrations.","Specifically, R$^3$ progressively slides the start state of reasoning from a demonstration's end to its beginning, facilitating easier model exploration at all stages.","Thus, R$^3$ establishes a step-wise curriculum, allowing outcome supervision to offer step-level signals and precisely pinpoint errors.","Using Llama2-7B, our method surpasses RL baseline on eight reasoning tasks by $4.1$ points on average.","Notebaly, in program-based reasoning on GSM8K, it exceeds the baseline by $4.2$ points across three backbone models, and without any extra data, Codellama-7B + R$^3$ performs comparable to larger models or closed-source models."],"url":"http://arxiv.org/abs/2402.05808v1","category":"cs.AI"}
{"created":"2024-02-08 16:41:41","title":"InkSight: Offline-to-Online Handwriting Conversion by Learning to Read and Write","abstract":"Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink. However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority. Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering. Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains. Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain. To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds. Furthermore, it generalizes beyond its training domain into simple sketches. Our human evaluation reveals that 87% of the samples produced by our model on the challenging HierText dataset are considered as a valid tracing of the input image and 67% look like a pen trajectory traced by a human.","sentences":["Digital note-taking is gaining popularity, offering a durable, editable, and easily indexable way of storing notes in the vectorized form, known as digital ink.","However, a substantial gap remains between this way of note-taking and traditional pen-and-paper note-taking, a practice still favored by a vast majority.","Our work, InkSight, aims to bridge the gap by empowering physical note-takers to effortlessly convert their work (offline handwriting) to digital ink (online handwriting), a process we refer to as Derendering.","Prior research on the topic has focused on the geometric properties of images, resulting in limited generalization beyond their training domains.","Our approach combines reading and writing priors, allowing training a model in the absence of large amounts of paired samples, which are difficult to obtain.","To our knowledge, this is the first work that effectively derenders handwritten text in arbitrary photos with diverse visual characteristics and backgrounds.","Furthermore, it generalizes beyond its training domain into simple sketches.","Our human evaluation reveals that 87% of the samples produced by our model on the challenging HierText dataset are considered as a valid tracing of the input image and 67% look like a pen trajectory traced by a human."],"url":"http://arxiv.org/abs/2402.05804v1","category":"cs.CV"}
{"created":"2024-02-08 16:41:20","title":"AvatarMMC: 3D Head Avatar Generation and Editing with Multi-Modal Conditioning","abstract":"We introduce an approach for 3D head avatar generation and editing with multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and a Latent Diffusion Model (LDM). 3D GANs can generate high-quality head avatars given a single or no condition. However, it is challenging to generate samples that adhere to multiple conditions of different modalities. On the other hand, LDMs excel at learning complex conditional distributions. To this end, we propose to exploit the conditioning capabilities of LDMs to enable multi-modal control over the latent space of a pre-trained 3D GAN. Our method can generate and edit 3D head avatars given a mixture of control signals such as RGB input, segmentation masks, and global attributes. This provides better control over the generation and editing of synthetic avatars both globally and locally. Experiments show that our proposed approach outperforms a solely GAN-based approach both qualitatively and quantitatively on generation and editing tasks. To the best of our knowledge, our approach is the first to introduce multi-modal conditioning to 3D avatar generation and editing. \\\\href{avatarmmc-sig24.github.io}{Project Page}","sentences":["We introduce an approach for 3D head avatar generation and editing with multi-modal conditioning based on a 3D Generative Adversarial Network (GAN) and a Latent Diffusion Model (LDM).","3D GANs can generate high-quality head avatars given a single or no condition.","However, it is challenging to generate samples that adhere to multiple conditions of different modalities.","On the other hand, LDMs excel at learning complex conditional distributions.","To this end, we propose to exploit the conditioning capabilities of LDMs to enable multi-modal control over the latent space of a pre-trained 3D GAN.","Our method can generate and edit 3D head avatars given a mixture of control signals such as RGB input, segmentation masks, and global attributes.","This provides better control over the generation and editing of synthetic avatars both globally and locally.","Experiments show that our proposed approach outperforms a solely GAN-based approach both qualitatively and quantitatively on generation and editing tasks.","To the best of our knowledge, our approach is the first to introduce multi-modal conditioning to 3D avatar generation and editing.","\\\\href{avatarmmc-sig24.github.io}{Project Page}"],"url":"http://arxiv.org/abs/2402.05803v1","category":"cs.CV"}
{"created":"2024-02-08 16:39:41","title":"Random choice spanning trees","abstract":"In this paper we introduce a new model of random spanning trees that we call choice spanning trees, constructed from so-called choice random walks. These are random walks for which each step is chosen from a subset of random options, according to some pre-defined rule. The choice spanning trees are constructed by running a choice modified version of Wilson's algorithm or the Aldous-Broder algorithm on the complete graph. We show that the scaling limits of these choice spanning trees are slight variants of random aggregation trees previously considered by Curien and Haas (2017). Moreover, we show that the loop-erasure of a choice random walk run on the complete graph converges after rescaling to a generalized Rayleigh process, extending a result of Evans, Pitman and Winter (2006). These are all natural extensions of similar results for uniform spanning trees.","sentences":["In this paper we introduce a new model of random spanning trees that we call choice spanning trees, constructed from so-called choice random walks.","These are random walks for which each step is chosen from a subset of random options, according to some pre-defined rule.","The choice spanning trees are constructed by running a choice modified version of Wilson's algorithm or the Aldous-Broder algorithm on the complete graph.","We show that the scaling limits of these choice spanning trees are slight variants of random aggregation trees previously considered by Curien and Haas (2017).","Moreover, we show that the loop-erasure of a choice random walk run on the complete graph converges after rescaling to a generalized Rayleigh process, extending a result of Evans, Pitman and Winter (2006).","These are all natural extensions of similar results for uniform spanning trees."],"url":"http://arxiv.org/abs/2402.05800v1","category":"math.PR"}
{"created":"2024-02-08 16:38:13","title":"Visual Harmony: Text-Visual Interplay in Circular Infographics","abstract":"Infographics are visual representations designed for efficient and effective communication of data and knowledge. One crucial aspect of infographic design is the interplay between text and visual elements, particularly in circular visualizations where the textual descriptions can either be embedded within the graphics or placed adjacent to the visual representation. While several studies have examined text layout design in visualizations in general, the text-visual interplay in infographics and its subsequent perceptual effects remain underexplored. To address this, our study investigates how varying text placement and descriptiveness impact pleasantness, comprehension and overall memorability in the infographics viewing experience. We recruited 30 participants and presented them with a collection of 15 infographics across a diverse set of topics, including media and public events, health and nutrition, science and research, and sustainability. The text placement (embed, side-to-side) and descriptiveness (simplistic, normal, descriptive) were systematically manipulated, resulting in a total of six experimental conditions. Our key findings indicate that text placement can significantly influence the memorability of infographics, whereas descriptiveness can significantly impact the pleasantness of the viewing experience. Embedding text placement and simplistic text can potentially contribute to more effective infographic designs. These results offer valuable insights for infographic designers, contributing to the creation of more effective and memorable visual representations.","sentences":["Infographics are visual representations designed for efficient and effective communication of data and knowledge.","One crucial aspect of infographic design is the interplay between text and visual elements, particularly in circular visualizations where the textual descriptions can either be embedded within the graphics or placed adjacent to the visual representation.","While several studies have examined text layout design in visualizations in general, the text-visual interplay in infographics and its subsequent perceptual effects remain underexplored.","To address this, our study investigates how varying text placement and descriptiveness impact pleasantness, comprehension and overall memorability in the infographics viewing experience.","We recruited 30 participants and presented them with a collection of 15 infographics across a diverse set of topics, including media and public events, health and nutrition, science and research, and sustainability.","The text placement (embed, side-to-side) and descriptiveness (simplistic, normal, descriptive) were systematically manipulated, resulting in a total of six experimental conditions.","Our key findings indicate that text placement can significantly influence the memorability of infographics, whereas descriptiveness can significantly impact the pleasantness of the viewing experience.","Embedding text placement and simplistic text can potentially contribute to more effective infographic designs.","These results offer valuable insights for infographic designers, contributing to the creation of more effective and memorable visual representations."],"url":"http://arxiv.org/abs/2402.05798v1","category":"cs.HC"}
{"created":"2024-02-08 16:36:19","title":"The Unruh-DeWitt model and its joint interacting Hilbert space","abstract":"In this work we make the connection between the Unruh-DeWitt particle detector model applied to quantum field theory in curved spacetimes and the rigorous construction of the spin-boson model. With some modifications, we show that existing results about the existence of a spin-boson ground state can be adapted to the Unruh-DeWitt model. In the most relevant scenario involving massless scalar fields in (3+1)-dimensional globally hyperbolic spacetimes, where the Unruh-DeWitt model describes a simplified model of light-matter interaction, we argue that common choices of the spacetime smearing functions regulate the ultraviolet behaviour of the model but can still exhibit infrared divergences. In particular, this implies the well-known expectation that the joint interacting Hilbert space of the model cannot be described by the tensor product of a two-dimensional complex Hilbert space and the Fock space of the vacuum representation. We discuss the conditions under which this problem does not arise and the relevance of the operator-algebraic approach for better understanding of particle detector models and their applications.","sentences":["In this work we make the connection between the Unruh-DeWitt particle detector model applied to quantum field theory in curved spacetimes and the rigorous construction of the spin-boson model.","With some modifications, we show that existing results about the existence of a spin-boson ground state can be adapted to the Unruh-DeWitt model.","In the most relevant scenario involving massless scalar fields in (3+1)-dimensional globally hyperbolic spacetimes, where the Unruh-DeWitt model describes a simplified model of light-matter interaction, we argue that common choices of the spacetime smearing functions regulate the ultraviolet behaviour of the model but can still exhibit infrared divergences.","In particular, this implies the well-known expectation that the joint interacting Hilbert space of the model cannot be described by the tensor product of a two-dimensional complex Hilbert space and the Fock space of the vacuum representation.","We discuss the conditions under which this problem does not arise and the relevance of the operator-algebraic approach for better understanding of particle detector models and their applications."],"url":"http://arxiv.org/abs/2402.05795v1","category":"quant-ph"}
{"created":"2024-02-08 16:36:11","title":"Phonetically rich corpus construction for a low-resourced language","abstract":"Speech technologies rely on capturing a speaker's voice variability while obtaining comprehensive language information. Textual prompts and sentence selection methods have been proposed in the literature to comprise such adequate phonetic data, referred to as a phonetically rich \\textit{corpus}. However, they are still insufficient for acoustic modeling, especially critical for languages with limited resources. Hence, this paper proposes a novel approach and outlines the methodological aspects required to create a \\textit{corpus} with broad phonetic coverage for a low-resourced language, Brazilian Portuguese. Our methodology includes text dataset collection up to a sentence selection algorithm based on triphone distribution. Furthermore, we propose a new phonemic classification according to acoustic-articulatory speech features since the absolute number of distinct triphones, or low-probability triphones, does not guarantee an adequate representation of every possible combination. Using our algorithm, we achieve a 55.8\\% higher percentage of distinct triphones -- for samples of similar size -- while the currently available phonetic-rich corpus, CETUC and TTS-Portuguese, 12.6\\% and 12.3\\% in comparison to a non-phonetically rich dataset.","sentences":["Speech technologies rely on capturing a speaker's voice variability while obtaining comprehensive language information.","Textual prompts and sentence selection methods have been proposed in the literature to comprise such adequate phonetic data, referred to as a phonetically rich \\textit{corpus}.","However, they are still insufficient for acoustic modeling, especially critical for languages with limited resources.","Hence, this paper proposes a novel approach and outlines the methodological aspects required to create a \\textit{corpus} with broad phonetic coverage for a low-resourced language, Brazilian Portuguese.","Our methodology includes text dataset collection up to a sentence selection algorithm based on triphone distribution.","Furthermore, we propose a new phonemic classification according to acoustic-articulatory speech features since the absolute number of distinct triphones, or low-probability triphones, does not guarantee an adequate representation of every possible combination.","Using our algorithm, we achieve a 55.8\\% higher percentage of distinct triphones -- for samples of similar size -- while the currently available phonetic-rich corpus, CETUC and TTS-Portuguese, 12.6\\% and 12.3\\% in comparison to a non-phonetically rich dataset."],"url":"http://arxiv.org/abs/2402.05794v1","category":"cs.CL"}
{"created":"2024-02-08 16:35:32","title":"Exact quantum sensing limits for bosonic dephasing channels","abstract":"Dephasing is a prominent noise mechanism that afflicts quantum information carriers, and it is one of the main challenges towards realizing useful quantum computation, communication, and sensing. Here we consider discrimination and estimation of bosonic dephasing channels, when using the most general adaptive strategies allowed by quantum mechanics. We reduce these difficult quantum problems to simple classical ones based on the probability densities defining the bosonic dephasing channels. By doing so, we rigorously establish the optimal performance of various distinguishability and estimation tasks and construct explicit strategies to achieve this performance. To the best of our knowledge, this is the first example of a non-Gaussian bosonic channel for which there are exact solutions for these tasks.","sentences":["Dephasing is a prominent noise mechanism that afflicts quantum information carriers, and it is one of the main challenges towards realizing useful quantum computation, communication, and sensing.","Here we consider discrimination and estimation of bosonic dephasing channels, when using the most general adaptive strategies allowed by quantum mechanics.","We reduce these difficult quantum problems to simple classical ones based on the probability densities defining the bosonic dephasing channels.","By doing so, we rigorously establish the optimal performance of various distinguishability and estimation tasks and construct explicit strategies to achieve this performance.","To the best of our knowledge, this is the first example of a non-Gaussian bosonic channel for which there are exact solutions for these tasks."],"url":"http://arxiv.org/abs/2402.05793v1","category":"quant-ph"}
{"created":"2024-02-08 16:24:44","title":"How do Transformers perform In-Context Autoregressive Learning?","abstract":"Transformers have achieved state-of-the-art performance in language modeling tasks. However, the reasons behind their tremendous success are still unclear. In this paper, towards a better understanding, we train a Transformer model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = W s_t$. We show how a trained Transformer predicts the next token by first learning $W$ in-context, then applying a prediction mapping. We call the resulting procedure in-context autoregressive learning. More precisely, focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer linear Transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens. When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head Transformer. Importantly, we exhibit orthogonality between heads and show that positional encoding captures trigonometric relations in the data. On the experimental side, we consider the general case of non-commuting orthogonal matrices and generalize our theoretical findings.","sentences":["Transformers have achieved state-of-the-art performance in language modeling tasks.","However, the reasons behind their tremendous success are still unclear.","In this paper, towards a better understanding, we train a Transformer model on a simple next token prediction task, where sequences are generated as a first-order autoregressive process $s_{t+1} = W s_t$.","We show how a trained Transformer predicts the next token by first learning $W$ in-context, then applying a prediction mapping.","We call the resulting procedure in-context autoregressive learning.","More precisely, focusing on commuting orthogonal matrices $W$, we first show that a trained one-layer linear Transformer implements one step of gradient descent for the minimization of an inner objective function, when considering augmented tokens.","When the tokens are not augmented, we characterize the global minima of a one-layer diagonal linear multi-head Transformer.","Importantly, we exhibit orthogonality between heads and show that positional encoding captures trigonometric relations in the data.","On the experimental side, we consider the general case of non-commuting orthogonal matrices and generalize our theoretical findings."],"url":"http://arxiv.org/abs/2402.05787v1","category":"stat.ML"}
{"created":"2024-02-08 16:24:40","title":"Prompting Fairness: Artificial Intelligence as Game Players","abstract":"Utilitarian games such as dictator games to measure fairness have been studied in the social sciences for decades. These games have given us insight into not only how humans view fairness but also in what conditions the frequency of fairness, altruism and greed increase or decrease. While these games have traditionally been focused on humans, the rise of AI gives us the ability to study how these models play these games. AI is becoming a constant in human interaction and examining how these models portray fairness in game play can give us some insight into how AI makes decisions. Over 101 rounds of the dictator game, I conclude that AI has a strong sense of fairness that is dependant of it it deems the person it is playing with as trustworthy, framing has a strong effect on how much AI gives a recipient when designated the trustee, and there may be evidence that AI experiences inequality aversion just as humans.","sentences":["Utilitarian games such as dictator games to measure fairness have been studied in the social sciences for decades.","These games have given us insight into not only how humans view fairness but also in what conditions the frequency of fairness, altruism and greed increase or decrease.","While these games have traditionally been focused on humans, the rise of AI gives us the ability to study how these models play these games.","AI is becoming a constant in human interaction and examining how these models portray fairness in game play can give us some insight into how AI makes decisions.","Over 101 rounds of the dictator game",", I conclude that AI has a strong sense of fairness that is dependant of it it deems the person it is playing with as trustworthy, framing has a strong effect on how much AI gives a recipient when designated the trustee, and there may be evidence that AI experiences inequality aversion just as humans."],"url":"http://arxiv.org/abs/2402.05786v1","category":"cs.AI"}
{"created":"2024-02-08 16:23:29","title":"Limits of Transformer Language Models on Algorithmic Learning","abstract":"We analyze the capabilities of Transformer language models on learning discrete algorithms. To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks. On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives. We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition. We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient.","sentences":["We analyze the capabilities of Transformer language models on learning discrete algorithms.","To this end, we introduce two new tasks demanding the composition of several discrete sub-tasks.","On both training LLaMA models from scratch and prompting on GPT-4 and Gemini we measure learning compositions of learned primitives.","We observe that the compositional capabilities of state-of-the-art Transformer language models are very limited and sample-wise scale worse than relearning all sub-tasks for a new algorithmic composition.","We also present a theorem in complexity theory, showing that gradient descent on memorizing feedforward models can be exponentially data inefficient."],"url":"http://arxiv.org/abs/2402.05785v1","category":"cs.LG"}
{"created":"2024-02-08 16:20:41","title":"Tsallis holographic dark energy with power law ansatz approach","abstract":"Holographic principles have proven to be a very interesting approach towards dealing with the issues of the late-time acceleration of the universe, which has resulted in a great amount of work on holographic dark energy models. We consider one such very interesting holographic scenario, namely the Tsallis Holographic dark energy model and consider an ansatz based approach to such models. We consider three cosmological scenarios in such models, namely those with viscous , non-viscous and Chaplygin gas scenarios, discussing various crucial aspects related to these models. We discuss various crucial properties of the Tsallis model in such scenarios and see how the phantom divide is crossed in each case, but it's only the Chaplygin gas models which give a better view on stability issues.","sentences":["Holographic principles have proven to be a very interesting approach towards dealing with the issues of the late-time acceleration of the universe, which has resulted in a great amount of work on holographic dark energy models.","We consider one such very interesting holographic scenario, namely the Tsallis Holographic dark energy model and consider an ansatz based approach to such models.","We consider three cosmological scenarios in such models, namely those with viscous , non-viscous and Chaplygin gas scenarios, discussing various crucial aspects related to these models.","We discuss various crucial properties of the Tsallis model in such scenarios and see how the phantom divide is crossed in each case, but it's only the Chaplygin gas models which give a better view on stability issues."],"url":"http://arxiv.org/abs/2402.05784v1","category":"gr-qc"}
{"created":"2024-02-08 16:17:24","title":"Text-to-Code Generation with Modality-relative Pre-training","abstract":"Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language. Such approaches effectively map both modalities of the sequence into the same embedding space. However, programming language keywords (e.g. ``while'') often have very strictly defined semantics. As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa. Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task. We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives. We focus on text-to-code generation and observe consistent improvements across two backbone models and two test sets, measuring pass@$k$ and a novel incremental variation.","sentences":["Large pre-trained language models have recently been expanded and applied to programming language tasks with great success, often through further pre-training of a strictly-natural language model--where training sequences typically contain both natural and (linearised) programming language.","Such approaches effectively map both modalities of the sequence into the same embedding space.","However, programming language keywords (e.g. ``while'') often have very strictly defined semantics.","As such, transfer learning from their natural language usage may not necessarily be beneficial to their code application and vise versa.","Assuming an already pre-trained language model, in this work we investigate how sequence tokens can be adapted and represented differently, depending on which modality they belong to, and to the ultimate benefit of the downstream task.","We experiment with separating embedding spaces between modalities during further model pre-training with modality-relative training objectives.","We focus on text-to-code generation and observe consistent improvements across two backbone models and two test sets, measuring pass@$k$ and a novel incremental variation."],"url":"http://arxiv.org/abs/2402.05783v1","category":"cs.CL"}
{"created":"2024-02-08 16:17:18","title":"Analysing the Sample Complexity of Opponent Shaping","abstract":"Learning in general-sum games often yields collectively sub-optimal results. Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings. Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps. Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game. In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework. Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging. In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis. R-FOS discretises the continuous meta-game MDP into a tabular MDP. Within this discretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to derive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm. We derive a sample complexity bound that is exponential in the cardinality of the inner state and action space and the number of agents. Our bound guarantees that, with high probability, the final policy learned by an R-FOS agent is close to the optimal policy, apart from a constant factor. Finally, we investigate how R-FOS's sample complexity scales in the size of state-action space. Our theoretical results on scaling are supported empirically in the Matching Pennies environment.","sentences":["Learning in general-sum games often yields collectively sub-optimal results.","Addressing this, opponent shaping (OS) methods actively guide the learning processes of other agents, empirically leading to improved individual and group performances in many settings.","Early OS methods use higher-order derivatives to shape the learning of co-players, making them unsuitable for shaping multiple learning steps.","Follow-up work, Model-free Opponent Shaping (M-FOS), addresses these by reframing the OS problem as a meta-game.","In contrast to early OS methods, there is little theoretical understanding of the M-FOS framework.","Providing theoretical guarantees for M-FOS is hard because A) there is little literature on theoretical sample complexity bounds for meta-reinforcement learning B) M-FOS operates in continuous state and action spaces, so theoretical analysis is challenging.","In this work, we present R-FOS, a tabular version of M-FOS that is more suitable for theoretical analysis.","R-FOS discretises the continuous meta-game MDP into a tabular MDP.","Within this discretised MDP, we adapt the $R_{max}$ algorithm, most prominently used to derive PAC-bounds for MDPs, as the meta-learner in the R-FOS algorithm.","We derive a sample complexity bound that is exponential in the cardinality of the inner state and action space and the number of agents.","Our bound guarantees that, with high probability, the final policy learned by an R-FOS agent is close to the optimal policy, apart from a constant factor.","Finally, we investigate how R-FOS's sample complexity scales in the size of state-action space.","Our theoretical results on scaling are supported empirically in the Matching Pennies environment."],"url":"http://arxiv.org/abs/2402.05782v1","category":"cs.LG"}
{"created":"2024-02-08 16:11:23","title":"Examining Gender and Racial Bias in Large Vision-Language Models Using a Novel Dataset of Parallel Images","abstract":"Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged. Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc. Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images. To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios). The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white). By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted.","sentences":["Following on recent advances in large language models (LLMs) and subsequent chat models, a new wave of large vision-language models (LVLMs) has emerged.","Such models can incorporate images as input in addition to text, and perform tasks such as visual question answering, image captioning, story generation, etc.","Here, we examine potential gender and racial biases in such systems, based on the perceived characteristics of the people in the input images.","To accomplish this, we present a new dataset PAIRS (PArallel Images for eveRyday Scenarios).","The PAIRS dataset contains sets of AI-generated images of people, such that the images are highly similar in terms of background and visual content, but differ along the dimensions of gender (man, woman) and race (Black, white).","By querying the LVLMs with such images, we observe significant differences in the responses according to the perceived gender or race of the person depicted."],"url":"http://arxiv.org/abs/2402.05779v1","category":"cs.CY"}
{"created":"2024-02-08 16:08:23","title":"Groups of permutations preserving orientation (parity) of subsets of a fixed size","abstract":"We study permutations on $n$ elements preserving orientation (parity) of every subset of size $k$. We describe all groups of these permutations. Unexpectedly, these groups (except some special cases) are either trivial or cyclic or dihedral. We produce these results while defining semigroups generalizing semigroups of order-preserving mappings and semigroups of orientation-preserving mappings.","sentences":["We study permutations on $n$ elements preserving orientation (parity) of every subset of size $k$. We describe all groups of these permutations.","Unexpectedly, these groups (except some special cases) are either trivial or cyclic or dihedral.","We produce these results while defining semigroups generalizing semigroups of order-preserving mappings and semigroups of orientation-preserving mappings."],"url":"http://arxiv.org/abs/2402.05777v1","category":"math.CO"}
{"created":"2024-02-08 16:01:24","title":"Stable Autonomous Flow Matching","abstract":"In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape. In control theory, it is well-known that energy can serve as an effective Lyapunov function. Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points. In this paper, we focus on such data and a recent class of deep generative models called flow matching. We apply tools of stochastic stability for time-independent systems to flow matching models. In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles. We demonstrate our theoretical results on two examples.","sentences":["In contexts where data samples represent a physically stable state, it is often assumed that the data points represent the local minima of an energy landscape.","In control theory, it is well-known that energy can serve as an effective Lyapunov function.","Despite this, connections between control theory and generative models in the literature are sparse, even though there are several machine learning applications with physically stable data points.","In this paper, we focus on such data and a recent class of deep generative models called flow matching.","We apply tools of stochastic stability for time-independent systems to flow matching models.","In doing so, we characterize the space of flow matching models that are amenable to this treatment, as well as draw connections to other control theory principles.","We demonstrate our theoretical results on two examples."],"url":"http://arxiv.org/abs/2402.05774v1","category":"cs.LG"}
{"created":"2024-02-08 16:00:25","title":"UAV-Rain1k: A Benchmark for Raindrop Removal from UAV Aerial Imagery","abstract":"Raindrops adhering to the lens of UAVs can obstruct visibility of the background scene and degrade image quality. Despite recent progress in image deraining methods and datasets, there is a lack of focus on raindrop removal from UAV aerial imagery due to the unique challenges posed by varying angles and rapid movement during drone flight. To fill the gap in this research, we first construct a new benchmark dataset for removing raindrops from UAV images, called UAV-Rain1k. In this letter, we provide a dataset generation pipeline, which includes modeling raindrop shapes using Blender, collecting background images from various UAV angles, random sampling of rain masks and etc. Based on the proposed benchmark, we further present a comprehensive evaluation of existing representative image deraining algorithms, and reveal future research opportunities worth exploring. The proposed dataset will be publicly available at https://github.com/cschenxiang/UAV-Rain1k.","sentences":["Raindrops adhering to the lens of UAVs can obstruct visibility of the background scene and degrade image quality.","Despite recent progress in image deraining methods and datasets, there is a lack of focus on raindrop removal from UAV aerial imagery due to the unique challenges posed by varying angles and rapid movement during drone flight.","To fill the gap in this research, we first construct a new benchmark dataset for removing raindrops from UAV images, called UAV-Rain1k.","In this letter, we provide a dataset generation pipeline, which includes modeling raindrop shapes using Blender, collecting background images from various UAV angles, random sampling of rain masks and etc.","Based on the proposed benchmark, we further present a comprehensive evaluation of existing representative image deraining algorithms, and reveal future research opportunities worth exploring.","The proposed dataset will be publicly available at https://github.com/cschenxiang/UAV-Rain1k."],"url":"http://arxiv.org/abs/2402.05773v1","category":"cs.CV"}
{"created":"2024-02-08 15:58:48","title":"Cyclic sieving on noncrossing (1,2)-configurations","abstract":"Verifying a suspicion of Propp and Reiner concerning the cyclic sieving phenomenon (CSP), M. Thiel introduced a Catalan object called noncrossing $(1,2)$-configurations (denoted by $X_n$), which is a class of set partitions of $[n-1]$. More precisely, Thiel proved that, with a natural action of the cyclic group $C_{n-1}$ on $X_n$, the triple $\\left(X_n,C_{n-1},\\text{Cat}_n(q)\\right)$ exhibits the CSP, where $\\text{Cat}_n(q):=\\frac{1}{[n+1]_q}\\begin{bmatrix}   2n\\\\ n \\end{bmatrix}_q$ is MacMahon's $q$-Catalan number. Recently, in a study of the fermionic diagonal coinvariant ring $FDR_n$, J. Kim found a combinatorial basis for $FDR_n$ indexed by $X_n$. In this paper, we continue to study $X_n$ and obtain the following results:   (1) We define a statistic $cwt$ on $X_n$ whose generating function is $\\text{Cat}_n(q)$, which answers a problem of Thiel.   (2) We show that $\\text{Cat}_n(q)$ is equivalent to $$\\sum_{\\substack{k,x,y\\\\2k+x+y=n-1}}\\begin{bmatrix}   n-1   2k,x,y   \\end{bmatrix}_q\\text{Cat}_k   (q)q^{k+\\binom{x}{2}+\\binom{y}{2}+\\binom{n}{2}}$$   modulo $q^{n-1}-1$, which answers a problem of Kim. As mentioned by Kim, this result leads to a representation theoretic proof of the above cyclic sieving result of Thiel.   (3) We consider the dihedral sieving, a generalization of the CSP, which was recently introduced by Rao and Suk. Under a natural action of the dihedral group $I_2(n-1)$ (for even $n$), we prove a dihedral sieving result on $X_n$.","sentences":["Verifying a suspicion of Propp and Reiner concerning the cyclic sieving phenomenon (CSP), M. Thiel introduced a Catalan object called noncrossing $(1,2)$-configurations (denoted by $X_n$), which is a class of set partitions of $[n-1]$. More precisely, Thiel proved that, with a natural action of the cyclic group $C_{n-1}$ on $X_n$, the triple $\\left(X_n,C_{n-1},\\text{Cat}_n(q)\\right)$ exhibits the CSP, where $\\text{Cat}_n(q):=\\frac{1}{[n+1]_q}\\begin{bmatrix}   2n\\\\ n \\end{bmatrix}_q$ is MacMahon's $q$-Catalan number.","Recently, in a study of the fermionic diagonal coinvariant ring $FDR_n$, J. Kim found a combinatorial basis for $FDR_n$ indexed by $X_n$. In this paper, we continue to study $X_n$ and obtain the following results:   (1) We define a statistic $cwt$ on $X_n$ whose generating function is $\\text{Cat}_n(q)$, which answers a problem of Thiel.   ","(2) We show that $\\text{Cat}_n(q)$ is equivalent to $$\\sum_{\\substack{k,x,y\\\\2k+x+y=n-1}}\\begin{bmatrix}   n-1   2k,x,y   \\end{bmatrix}_q\\text{Cat}_k   (q)q^{k+\\binom{x}{2}+\\binom{y}{2}+\\binom{n}{2}}$$   modulo $q^{n-1}-1$, which answers a problem of Kim.","As mentioned by Kim, this result leads to a representation theoretic proof of the above cyclic sieving result of Thiel.   ","(3) We consider the dihedral sieving, a generalization of the CSP, which was recently introduced by Rao and Suk.","Under a natural action of the dihedral group $I_2(n-1)$ (for even $n$), we prove a dihedral sieving result on $X_n$."],"url":"http://arxiv.org/abs/2402.05771v1","category":"math.CO"}
{"created":"2024-02-08 15:50:52","title":"Analysis of practical fractional vortex beams at far field","abstract":"The subject of calculating the topological charge (TC) or vortex strength of optical vortices has generated divided opinions among scientists. This is due to the fact that proper analytical results are hard to support from the experimental point of view, leading to different results and conclusions. In this work we will present numerical data that shows the limits of TC measurements for practical fractional vortices and the possible challenges that high order measurements may pose. By analyzing the far field phase and the behavior of the transitions we have shown that they follow specific curves that depend not only on the TC but also on the beam waist. This leads us to present a new \"strength staircase\" for practical vortices. Our aim is to give some insight in practical scenarios that have not been taken into account in previous results.","sentences":["The subject of calculating the topological charge (TC) or vortex strength of optical vortices has generated divided opinions among scientists.","This is due to the fact that proper analytical results are hard to support from the experimental point of view, leading to different results and conclusions.","In this work we will present numerical data that shows the limits of TC measurements for practical fractional vortices and the possible challenges that high order measurements may pose.","By analyzing the far field phase and the behavior of the transitions we have shown that they follow specific curves that depend not only on the TC but also on the beam waist.","This leads us to present a new \"strength staircase\" for practical vortices.","Our aim is to give some insight in practical scenarios that have not been taken into account in previous results."],"url":"http://arxiv.org/abs/2402.05765v1","category":"physics.optics"}
{"created":"2024-02-08 15:46:10","title":"On the growth and evolution of low-mass planets in pressure bumps","abstract":"Observations of protoplanetary discs have revealed dust rings which are likely due to the presence of pressure bumps in the disc. Because these structures tend to trap drifting pebbles, it has been proposed that pressure bumps may play an important role in the planet formation process. In this paper, we investigate the orbital evolution of a $0.1$ $M_\\oplus$ protoplanet embedded in a pressure bump using 2-dimensional hydrodynamical simulations of protoplanetary discs consisting of gas and pebbles. We examine the role of thermal forces generated by the pebble accretion-induced heat release, taking into account the feedback between luminosity and eccentricity. We also study the effect of the pebble-scattered flow on the planet's orbital evolution. Due to accumulation of pebbles at the pressure bump, the planet's accretion luminosity is high enough to induce significant eccentricity growth through thermal forces. Accretion luminosity is also responsible for vortex formation at the planet position through baroclinic effects, which cause the planet escape from the dust ring if dust feedback onto the gas is neglected. Including the effect of the dust back-reaction leads to weaker vortices, which enable the planet to remain close to the pressure maximum on an eccentric orbit. Simulations in which the planet mass is allowed to increase as a consequence of pebble accretion resulted in the formation of giant planet cores with mass in the range $5-20$ $M_\\oplus$ over $\\sim 2\\times 10^4$ yrs. This occurs for moderate values of the Stokes number $St \\approx 0.01$ such that the pebble drift velocity is not too high and the dust ring mass not too small. Our results suggest that pressure bumps mays be preferred locations for the formation of giant planets, but this requires a moderate level of grain growth within the disc.","sentences":["Observations of protoplanetary discs have revealed dust rings which are likely due to the presence of pressure bumps in the disc.","Because these structures tend to trap drifting pebbles, it has been proposed that pressure bumps may play an important role in the planet formation process.","In this paper, we investigate the orbital evolution of a $0.1$ $M_\\oplus$ protoplanet embedded in a pressure bump using 2-dimensional hydrodynamical simulations of protoplanetary discs consisting of gas and pebbles.","We examine the role of thermal forces generated by the pebble accretion-induced heat release, taking into account the feedback between luminosity and eccentricity.","We also study the effect of the pebble-scattered flow on the planet's orbital evolution.","Due to accumulation of pebbles at the pressure bump, the planet's accretion luminosity is high enough to induce significant eccentricity growth through thermal forces.","Accretion luminosity is also responsible for vortex formation at the planet position through baroclinic effects, which cause the planet escape from the dust ring if dust feedback onto the gas is neglected.","Including the effect of the dust back-reaction leads to weaker vortices, which enable the planet to remain close to the pressure maximum on an eccentric orbit.","Simulations in which the planet mass is allowed to increase as a consequence of pebble accretion resulted in the formation of giant planet cores with mass in the range $5-20$ $M_\\oplus$ over $\\sim 2\\times 10^4$ yrs.","This occurs for moderate values of the Stokes number $St \\approx 0.01$ such that the pebble drift velocity is not too high and the dust ring mass not too small.","Our results suggest that pressure bumps mays be preferred locations for the formation of giant planets, but this requires a moderate level of grain growth within the disc."],"url":"http://arxiv.org/abs/2402.05760v1","category":"astro-ph.EP"}
{"created":"2024-02-08 15:41:22","title":"When is Mean-Field Reinforcement Learning Tractable and Relevant?","abstract":"Mean-field reinforcement learning has become a popular theoretical framework for efficiently approximating large-scale multi-agent reinforcement learning (MARL) problems exhibiting symmetry. However, questions remain regarding the applicability of mean-field approximations: in particular, their approximation accuracy of real-world systems and conditions under which they become computationally tractable. We establish explicit finite-agent bounds for how well the MFG solution approximates the true $N$-player game for two popular mean-field solution concepts. Furthermore, for the first time, we establish explicit lower bounds indicating that MFGs are poor or uninformative at approximating $N$-player games assuming only Lipschitz dynamics and rewards. Finally, we analyze the computational complexity of solving MFGs with only Lipschitz properties and prove that they are in the class of \\textsc{PPAD}-complete problems conjectured to be intractable, similar to general sum $N$ player games. Our theoretical results underscore the limitations of MFGs and complement and justify existing work by proving difficulty in the absence of common theoretical assumptions.","sentences":["Mean-field reinforcement learning has become a popular theoretical framework for efficiently approximating large-scale multi-agent reinforcement learning (MARL) problems exhibiting symmetry.","However, questions remain regarding the applicability of mean-field approximations: in particular, their approximation accuracy of real-world systems and conditions under which they become computationally tractable.","We establish explicit finite-agent bounds for how well the MFG solution approximates the true $N$-player game for two popular mean-field solution concepts.","Furthermore, for the first time, we establish explicit lower bounds indicating that MFGs are poor or uninformative at approximating $N$-player games assuming only Lipschitz dynamics and rewards.","Finally, we analyze the computational complexity of solving MFGs with only Lipschitz properties and prove that they are in the class of \\textsc{PPAD}-complete problems conjectured to be intractable, similar to general sum $N$ player games.","Our theoretical results underscore the limitations of MFGs and complement and justify existing work by proving difficulty in the absence of common theoretical assumptions."],"url":"http://arxiv.org/abs/2402.05757v1","category":"cs.GT"}
{"created":"2024-02-08 15:41:02","title":"Non-Markovian Quantum Mpemba effect","abstract":"Since it's rediscovery in the twentieth century, the Mpemba effect, where a far-from-equilibrium state may relax faster than a state closer to equilibrium, has been extensively studied in classical systems and has recently received significant attention in quantum systems. Many theories explaining this counter-intuitive behavior in classical systems rely on memory effects. However, in quantum systems, the relation between the Mpemba effect and memory has remained unexplored. In this work, we consider a general non-Markovian open quantum setting and reveal new classes of quantum Mpemba effects, with no analog in Markovian quantum dynamics. Generically, open quantum dynamics possess a finite memory time and a unique steady state. Due to non-Markovian dynamics, even if the system is initialized in the steady state it can take a long time to relax back. We find other initial states that reach the steady state much faster. Most notably, we demonstrate that there can be an initial state in which the system reaches the steady state within the finite memory time itself, therefore giving the fastest possible relaxation to stationarity. We verify the effect for quantum dot systems coupled to electronic reservoirs in equilibrium and non-equilibrium setups at weak, intermediate and strong coupling, and both with and without interactions. Our work provides new insights into the rich physics underlying accelerated relaxation in quantum systems.","sentences":["Since it's rediscovery in the twentieth century, the Mpemba effect, where a far-from-equilibrium state may relax faster than a state closer to equilibrium, has been extensively studied in classical systems and has recently received significant attention in quantum systems.","Many theories explaining this counter-intuitive behavior in classical systems rely on memory effects.","However, in quantum systems, the relation between the Mpemba effect and memory has remained unexplored.","In this work, we consider a general non-Markovian open quantum setting and reveal new classes of quantum Mpemba effects, with no analog in Markovian quantum dynamics.","Generically, open quantum dynamics possess a finite memory time and a unique steady state.","Due to non-Markovian dynamics, even if the system is initialized in the steady state it can take a long time to relax back.","We find other initial states that reach the steady state much faster.","Most notably, we demonstrate that there can be an initial state in which the system reaches the steady state within the finite memory time itself, therefore giving the fastest possible relaxation to stationarity.","We verify the effect for quantum dot systems coupled to electronic reservoirs in equilibrium and non-equilibrium setups at weak, intermediate and strong coupling, and both with and without interactions.","Our work provides new insights into the rich physics underlying accelerated relaxation in quantum systems."],"url":"http://arxiv.org/abs/2402.05756v1","category":"quant-ph"}
{"created":"2024-02-08 15:38:18","title":"Cops and Robber on Hyperbolic Manifolds","abstract":"The Cops and Robber game on geodesic spaces is a pursuit-evasion game with discrete steps which captures the behavior of the game played on graphs, as well as that of continuous pursuit-evasion games. One of the outstanding open problems about the game on graphs is to determine which graphs embeddable in a surface of genus $g$ have largest cop number. It is known that the cop number of genus $g$ graphs is $O(g)$ and that there are examples whose cop number is $\\tilde\\Omega(\\sqrt{g}\\,)$. The same phenomenon occurs when the game is played on geodesic surfaces. In this paper we obtain a surprising result about the game on a surface with constant curvature. It is shown that two cops have a strategy to come arbitrarily close to the robber, independently of the genus. We also discuss upper bounds on the number of cops needed to catch the robber. Our results generalize to higher-dimensional hyperbolic manifolds.","sentences":["The Cops and Robber game on geodesic spaces is a pursuit-evasion game with discrete steps which captures the behavior of the game played on graphs, as well as that of continuous pursuit-evasion games.","One of the outstanding open problems about the game on graphs is to determine which graphs embeddable in a surface of genus $g$ have largest cop number.","It is known that the cop number of genus $g$ graphs is $O(g)$ and that there are examples whose cop number is $\\tilde\\Omega(\\sqrt{g}\\,)$. The same phenomenon occurs when the game is played on geodesic surfaces.","In this paper we obtain a surprising result about the game on a surface with constant curvature.","It is shown that two cops have a strategy to come arbitrarily close to the robber, independently of the genus.","We also discuss upper bounds on the number of cops needed to catch the robber.","Our results generalize to higher-dimensional hyperbolic manifolds."],"url":"http://arxiv.org/abs/2402.05753v1","category":"math.CO"}
{"created":"2024-02-08 15:36:21","title":"Constraining the EdGB Theory with Extreme Mass-Ratio Inspirals","abstract":"The Einstein-dilaton-Gauss-Bonnet (EdGB) theory is a modified theory which include a scalar field to couple with the higher order curvature terms. It has already been constrained with various observations include the gravitational wave (GW) with LIGO, Virgo and KAGRA (LVK) Collaboration. In this work, we study the problem of using the GW of Extreme Mass-Ratio Inspiral (EMRI) to constrain the EdGB theory. We use the \"numerical kludge (NK)\" method to construct the waveform of EMRI in the EdGB theory, focusing on the case when the central black hole is spinless. We then study how a future space-borne gravitational wave detector, TianQin, for example, can place constraints on the EdGB theory through the detection of EMRIs. With the analysis using mismatch and Fisher Information Matrix (FM), we find that the EdGB parameter $\\sqrt{\\alpha}$ is expected to be constrained to the level of $\\sim\\mathcal{O}(0.1)$ km.","sentences":["The Einstein-dilaton-Gauss-Bonnet (EdGB) theory is a modified theory which include a scalar field to couple with the higher order curvature terms.","It has already been constrained with various observations include the gravitational wave (GW) with LIGO, Virgo and KAGRA (LVK) Collaboration.","In this work, we study the problem of using the GW of Extreme Mass-Ratio Inspiral (EMRI) to constrain the EdGB theory.","We use the \"numerical kludge (NK)\" method to construct the waveform of EMRI in the EdGB theory, focusing on the case when the central black hole is spinless.","We then study how a future space-borne gravitational wave detector, TianQin, for example, can place constraints on the EdGB theory through the detection of EMRIs.","With the analysis using mismatch and Fisher Information Matrix (FM), we find that the EdGB parameter $\\sqrt{\\alpha}$ is expected to be constrained to the level of $\\sim\\mathcal{O}(0.1)$ km."],"url":"http://arxiv.org/abs/2402.05752v1","category":"gr-qc"}
{"created":"2024-02-08 15:34:41","title":"Limit theorems for a strongly irreducible product of independent random matrices with optimal moment conditions","abstract":"Let $ \\nu $ be a probability distribution over the linear semi-group $ \\mathrm{End}(E) $ for $ E $ a finite dimensional vector space over a locally compact field. We assume that $ \\nu $ is proximal, strongly irreducible and that $ \\nu^{*n}\\{0\\}=0 $ for all integers $ n\\in\\mathbb{N} $. We consider the random sequence $ \\overline\\gamma_n := \\gamma_0 \\cdots \\gamma_{n-1} $ for $ (\\gamma_k)_{k \\ge 0} $ independents of distribution law $ \\nu $. We define the logarithmic singular gap as $ \\mathrm{sqz} = \\log\\left( \\frac{\\mu_1}{\\mu_2} \\right) $ , where $ \\mu_1 $ and $ \\mu_2 $ are the two largest singular values. We show that $ (\\mathrm{sqz}(\\overline{\\gamma}_n))_{n\\in\\mathbb{N}} $ escapes to infinity linearly and satisfies exponential large deviations estimates below its escape rate. Using this escape speed, we also show that the image of a generic line by $ \\overline{\\gamma}_n $ as well as its eigenspace of maximal eigenvalue both converge to the same random line $ l_\\infty $ at an exponential speed.   If we moreover assume that the push-forward distribution $ N(\\nu) $ is $ \\mathrm{L}^p $ for $ N:g\\mapsto\\log\\left(\\|g\\|\\|g^{-1}\\|\\right) $ and for some $ p\\ge 1 $ , then we show that $ \\log|w(l_\\infty)| $ is $ \\mathrm{L}^p $ for all unitary linear form $ w $ and the logarithm of each coefficient of $ \\overline{\\gamma}_n $ is almost surely equivalent to the logarithm of the norm.   To prove these results, we do not rely on any classical results for random products of invertible matrices with $ \\mathrm{L}^1 $ moment assumption. Instead we describe an effective way to group the i.i.d factors into i.i.d random words that are aligned in the Cartan projection. We moreover have an explicit control over the moments.","sentences":["Let $ \\nu $ be a probability distribution over the linear semi-group $ \\mathrm{End}(E) $ for $ E $ a finite dimensional vector space over a locally compact field.","We assume that $ \\nu $ is proximal, strongly irreducible and that $ \\nu^{*n}\\{0\\}=0 $ for all integers $ n\\in\\mathbb{N} $.","We consider the random sequence $ \\overline\\gamma_n := \\gamma_0 \\cdots \\gamma_{n-1} $ for $ (\\gamma_k)_{k \\ge 0} $ independents of distribution law $ \\nu $.","We define the logarithmic singular gap as $ \\mathrm{sqz} = \\log\\left( \\frac{\\mu_1}{\\mu_2} \\right) $ , where $ \\mu_1 $ and $ \\mu_2 $ are the two largest singular values.","We show that $ (\\mathrm{sqz}(\\overline{\\gamma}_n))_{n\\in\\mathbb{N}} $ escapes to infinity linearly and satisfies exponential large deviations estimates below its escape rate.","Using this escape speed, we also show that the image of a generic line by $ \\overline{\\gamma}_n $ as well as its eigenspace of maximal eigenvalue both converge to the same random line $ l_\\infty $ at an exponential speed.   ","If we moreover assume that the push-forward distribution $ N(\\nu) $ is $ \\mathrm{L}^p $ for $ N:g\\mapsto\\log\\left(\\|g\\|\\|g^{-1}\\|\\right) $ and for some $ p\\ge 1 $ , then we show that $ \\log|w(l_\\infty)| $ is $ \\mathrm{L}^p $ for all unitary linear form $ w $ and the logarithm of each coefficient of $ \\overline{\\gamma}_n $ is almost surely equivalent to the logarithm of the norm.   ","To prove these results, we do not rely on any classical results for random products of invertible matrices with $ \\mathrm{L}^1 $ moment assumption.","Instead we describe an effective way to group the i.i.d factors into i.i.d random words that are aligned in the Cartan projection.","We moreover have an explicit control over the moments."],"url":"http://arxiv.org/abs/2402.05751v1","category":"math.PR"}
{"created":"2024-02-08 15:33:09","title":"Generalized Preference Optimization: A Unified Approach to Offline Alignment","abstract":"Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices. We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions. GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants. The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss. Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation. In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners.","sentences":["Offline preference optimization allows fine-tuning large models directly from offline data, and has proved effective in recent alignment practices.","We propose generalized preference optimization (GPO), a family of offline losses parameterized by a general class of convex functions.","GPO enables a unified view over preference optimization, encompassing existing algorithms such as DPO, IPO and SLiC as special cases, while naturally introducing new variants.","The GPO framework also sheds light on how offline algorithms enforce regularization, through the design of the convex function that defines the loss.","Our analysis and experiments reveal the connections and subtle differences between the offline regularization and the KL divergence regularization intended by the canonical RLHF formulation.","In all, our results present new algorithmic toolkits and empirical insights to alignment practitioners."],"url":"http://arxiv.org/abs/2402.05749v1","category":"cs.LG"}
{"created":"2024-02-08 15:32:22","title":"Jacquard V2: Refining Datasets using the Human In the Loop Data Correction Method","abstract":"In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role. In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects. Creating datasets from scratch is a time and labor-intensive process. Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge. Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp. We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality. This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping. Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators. After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN). FN are then subcategorized into either missing annotations or catastrophic labeling errors. Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed. The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images. The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks.","sentences":["In the context of rapid advancements in industrial automation, vision-based robotic grasping plays an increasingly crucial role.","In order to enhance visual recognition accuracy, the utilization of large-scale datasets is imperative for training models to acquire implicit knowledge related to the handling of various objects.","Creating datasets from scratch is a time and labor-intensive process.","Moreover, existing datasets often contain errors due to automated annotations aimed at expediency, making the improvement of these datasets a substantial research challenge.","Consequently, several issues have been identified in the annotation of grasp bounding boxes within the popular Jacquard Grasp.","We propose utilizing a Human-In-The-Loop(HIL) method to enhance dataset quality.","This approach relies on backbone deep learning networks to predict object positions and orientations for robotic grasping.","Predictions with Intersection over Union (IOU) values below 0.2 undergo an assessment by human operators.","After their evaluation, the data is categorized into False Negatives(FN) and True Negatives(TN).","FN are then subcategorized into either missing annotations or catastrophic labeling errors.","Images lacking labels are augmented with valid grasp bounding box information, whereas images afflicted by catastrophic labeling errors are completely removed.","The open-source tool Labelbee was employed for 53,026 iterations of HIL dataset enhancement, leading to the removal of 2,884 images and the incorporation of ground truth information for 30,292 images.","The enhanced dataset, named the Jacquard V2 Grasping Dataset, served as the training data for a range of neural networks."],"url":"http://arxiv.org/abs/2402.05747v1","category":"cs.CV"}
{"created":"2024-02-08 15:26:28","title":"Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents","abstract":"Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data. However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration. To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets. To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework. To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method. Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering. Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos.","sentences":["Scene simulation in autonomous driving has gained significant attention because of its huge potential for generating customized data.","However, existing editable scene simulation approaches face limitations in terms of user interaction efficiency, multi-camera photo-realistic rendering and external digital assets integration.","To address these challenges, this paper introduces ChatSim, the first system that enables editable photo-realistic 3D driving scene simulations via natural language commands with external digital assets.","To enable editing with high command flexibility,~ChatSim leverages a large language model (LLM) agent collaboration framework.","To generate photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance field method.","Furthermore, to unleash the potential of extensive high-quality digital assets, ChatSim employs a novel multi-camera lighting estimation method to achieve scene-consistent assets' rendering.","Our experiments on Waymo Open Dataset demonstrate that ChatSim can handle complex language commands and generate corresponding photo-realistic scene videos."],"url":"http://arxiv.org/abs/2402.05746v1","category":"cs.CV"}
{"created":"2024-02-08 15:19:50","title":"Real-World Robot Applications of Foundation Models: A Review","abstract":"Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.","sentences":["Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities.","Their impact spans various fields, including healthcare, education, and robotics.","This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems.","The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics.","This paper concludes with a discussion of future challenges and implications for practical robot applications."],"url":"http://arxiv.org/abs/2402.05741v1","category":"cs.RO"}
{"created":"2024-02-08 15:18:54","title":"CounterCLR: Counterfactual Contrastive Learning with Non-random Missing Data in Recommendation","abstract":"Recommender systems are designed to learn user preferences from observed feedback and comprise many fundamental tasks, such as rating prediction and post-click conversion rate (pCVR) prediction. However, the observed feedback usually suffer from two issues: selection bias and data sparsity, where biased and insufficient feedback seriously degrade the performance of recommender systems in terms of accuracy and ranking. Existing solutions for handling the issues, such as data imputation and inverse propensity score, are highly susceptible to additional trained imputation or propensity models. In this work, we propose a novel counterfactual contrastive learning framework for recommendation, named CounterCLR, to tackle the problem of non-random missing data by exploiting the advances in contrast learning. Specifically, the proposed CounterCLR employs a deep representation network, called CauNet, to infer non-random missing data in recommendations and perform user preference modeling by further introducing a self-supervised contrastive learning task. Our CounterCLR mitigates the selection bias problem without the need for additional models or estimators, while also enhancing the generalization ability in cases of sparse data. Experiments on real-world datasets demonstrate the effectiveness and superiority of our method.","sentences":["Recommender systems are designed to learn user preferences from observed feedback and comprise many fundamental tasks, such as rating prediction and post-click conversion rate (pCVR) prediction.","However, the observed feedback usually suffer from two issues: selection bias and data sparsity, where biased and insufficient feedback seriously degrade the performance of recommender systems in terms of accuracy and ranking.","Existing solutions for handling the issues, such as data imputation and inverse propensity score, are highly susceptible to additional trained imputation or propensity models.","In this work, we propose a novel counterfactual contrastive learning framework for recommendation, named CounterCLR, to tackle the problem of non-random missing data by exploiting the advances in contrast learning.","Specifically, the proposed CounterCLR employs a deep representation network, called CauNet, to infer non-random missing data in recommendations and perform user preference modeling by further introducing a self-supervised contrastive learning task.","Our CounterCLR mitigates the selection bias problem without the need for additional models or estimators, while also enhancing the generalization ability in cases of sparse data.","Experiments on real-world datasets demonstrate the effectiveness and superiority of our method."],"url":"http://arxiv.org/abs/2402.05740v1","category":"cs.IR"}
{"created":"2024-02-08 15:13:23","title":"Numerical solution of the Newtonian plane Couette flow with linear dynamic wall slip","abstract":"An efficient numerical approach based on weighted average finite differences is used to solve the Newtonian plane Couette flow with wall slip, obeying a dynamic slip law that generalizes the Navier slip law with the inclusion of a relaxation term. Slip is exhibited only along the fixed plate, and the motion is triggered by the motion of the other plate. Three different cases are considered for the motion of the moving plate, i.e., constant speed, oscillating speed, and a single-period sinusoidal speed. The velocity and the volumetric flow rate are calculated in all cases and comparisons are made with the results of other methods and available results in the literature. The numerical outcomes confirm the damping with time and the lagging effects arising from the Navier and dynamic wall slip conditions and demonstrate the hysteretic behavior of the slip velocity in following the harmonic boundary motion.","sentences":["An efficient numerical approach based on weighted average finite differences is used to solve the Newtonian plane Couette flow with wall slip, obeying a dynamic slip law that generalizes the Navier slip law with the inclusion of a relaxation term.","Slip is exhibited only along the fixed plate, and the motion is triggered by the motion of the other plate.","Three different cases are considered for the motion of the moving plate, i.e., constant speed, oscillating speed, and a single-period sinusoidal speed.","The velocity and the volumetric flow rate are calculated in all cases and comparisons are made with the results of other methods and available results in the literature.","The numerical outcomes confirm the damping with time and the lagging effects arising from the Navier and dynamic wall slip conditions and demonstrate the hysteretic behavior of the slip velocity in following the harmonic boundary motion."],"url":"http://arxiv.org/abs/2402.05736v1","category":"physics.flu-dyn"}
{"created":"2024-02-08 15:07:21","title":"A Framework for Assessing Proportionate Intervention with Face Recognition Systems in Real-Life Scenarios","abstract":"Face recognition (FR) has reached a high technical maturity. However, its use needs to be carefully assessed from an ethical perspective, especially in sensitive scenarios. This is precisely the focus of this paper: the use of FR for the identification of specific subjects in moderately to densely crowded spaces (e.g. public spaces, sports stadiums, train stations) and law enforcement scenarios. In particular, there is a need to consider the trade-off between the need to protect privacy and fundamental rights of citizens as well as their safety. Recent Artificial Intelligence (AI) policies, notably the European AI Act, propose that such FR interventions should be proportionate and deployed only when strictly necessary. Nevertheless, concrete guidelines on how to address the concept of proportional FR intervention are lacking to date. This paper proposes a framework to contribute to assessing whether an FR intervention is proportionate or not for a given context of use in the above mentioned scenarios. It also identifies the main quantitative and qualitative variables relevant to the FR intervention decision (e.g. number of people in the scene, level of harm that the person(s) in search could perpetrate, consequences to individual rights and freedoms) and propose a 2D graphical model making it possible to balance these variables in terms of ethical cost vs security gain. Finally, different FR scenarios inspired by real-world deployments validate the proposed model. The framework is conceived as a simple support tool for decision makers when confronted with the deployment of an FR system.","sentences":["Face recognition (FR) has reached a high technical maturity.","However, its use needs to be carefully assessed from an ethical perspective, especially in sensitive scenarios.","This is precisely the focus of this paper: the use of FR for the identification of specific subjects in moderately to densely crowded spaces (e.g. public spaces, sports stadiums, train stations) and law enforcement scenarios.","In particular, there is a need to consider the trade-off between the need to protect privacy and fundamental rights of citizens as well as their safety.","Recent Artificial Intelligence (AI) policies, notably the European AI Act, propose that such FR interventions should be proportionate and deployed only when strictly necessary.","Nevertheless, concrete guidelines on how to address the concept of proportional FR intervention are lacking to date.","This paper proposes a framework to contribute to assessing whether an FR intervention is proportionate or not for a given context of use in the above mentioned scenarios.","It also identifies the main quantitative and qualitative variables relevant to the FR intervention decision (e.g. number of people in the scene, level of harm that the person(s) in search could perpetrate, consequences to individual rights and freedoms) and propose a 2D graphical model making it possible to balance these variables in terms of ethical cost vs security gain.","Finally, different FR scenarios inspired by real-world deployments validate the proposed model.","The framework is conceived as a simple support tool for decision makers when confronted with the deployment of an FR system."],"url":"http://arxiv.org/abs/2402.05731v1","category":"cs.CY"}
{"created":"2024-02-08 15:00:44","title":"Monetary Policy and the Gendered Labor Market Dynamics: Evidence from Developing Economies","abstract":"Using a Taylor rule amended with official reserves movements, we derive country-specific monetary shocks and employ a local projections-estimator for tracking gender-disaggregated labor-market responses in 99 developing economies from 2009 to 2021. Results show that women experience more negative post-shock employment responses than men, contributing to a deepening of the gender gaps on the labor market. After the shock, women leave the labor market more so than men, which results in an apparently intact or even improved unemployment outcome for women. We find limited evidence of sector-specific reaction to interest rates. Additionally, we identify an intense worsening of women-s position on the labor market in high-growth environments as well under monetary policy tightening. Developing Asia and Latin America experience the most significant detrimental effects on women's employment, Africa exhibits a slower manifestation of the monetary shocks-impact and developing Europe shows the mildest effects.","sentences":["Using a Taylor rule amended with official reserves movements, we derive country-specific monetary shocks and employ a local projections-estimator for tracking gender-disaggregated labor-market responses in 99 developing economies from 2009 to 2021.","Results show that women experience more negative post-shock employment responses than men, contributing to a deepening of the gender gaps on the labor market.","After the shock, women leave the labor market more so than men, which results in an apparently intact or even improved unemployment outcome for women.","We find limited evidence of sector-specific reaction to interest rates.","Additionally, we identify an intense worsening of women-s position on the labor market in high-growth environments as well under monetary policy tightening.","Developing Asia and Latin America experience the most significant detrimental effects on women's employment, Africa exhibits a slower manifestation of the monetary shocks-impact and developing Europe shows the mildest effects."],"url":"http://arxiv.org/abs/2402.05729v1","category":"econ.GN"}
{"created":"2024-02-08 15:00:15","title":"CTGAN: Semantic-guided Conditional Texture Generator for 3D Shapes","abstract":"The entertainment industry relies on 3D visual content to create immersive experiences, but traditional methods for creating textured 3D models can be time-consuming and subjective. Generative networks such as StyleGAN have advanced image synthesis, but generating 3D objects with high-fidelity textures is still not well explored, and existing methods have limitations. We propose the Semantic-guided Conditional Texture Generator (CTGAN), producing high-quality textures for 3D shapes that are consistent with the viewing angle while respecting shape semantics. CTGAN utilizes the disentangled nature of StyleGAN to finely manipulate the input latent codes, enabling explicit control over both the style and structure of the generated textures. A coarse-to-fine encoder architecture is introduced to enhance control over the structure of the resulting textures via input segmentation. Experimental results show that CTGAN outperforms existing methods on multiple quality metrics and achieves state-of-the-art performance on texture generation in both conditional and unconditional settings.","sentences":["The entertainment industry relies on 3D visual content to create immersive experiences, but traditional methods for creating textured 3D models can be time-consuming and subjective.","Generative networks such as StyleGAN have advanced image synthesis, but generating 3D objects with high-fidelity textures is still not well explored, and existing methods have limitations.","We propose the Semantic-guided Conditional Texture Generator (CTGAN), producing high-quality textures for 3D shapes that are consistent with the viewing angle while respecting shape semantics.","CTGAN utilizes the disentangled nature of StyleGAN to finely manipulate the input latent codes, enabling explicit control over both the style and structure of the generated textures.","A coarse-to-fine encoder architecture is introduced to enhance control over the structure of the resulting textures via input segmentation.","Experimental results show that CTGAN outperforms existing methods on multiple quality metrics and achieves state-of-the-art performance on texture generation in both conditional and unconditional settings."],"url":"http://arxiv.org/abs/2402.05728v1","category":"cs.CV"}
{"created":"2024-02-08 14:56:40","title":"Dual-modal Tactile E-skin: Enabling Bidirectional Human-Robot Interaction via Integrated Tactile Perception and Feedback","abstract":"To foster an immersive and natural human-robot interaction, the implementation of tactile perception and feedback becomes imperative, effectively bridging the conventional sensory gap. In this paper, we propose a dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing and vibration feedback for enhanced human-robot interaction. The dual-modal tactile e-skin offers multi-functional tactile sensing and programmable haptic feedback, underpinned by a layered structure comprised of flexible magnetic films, soft silicone, a Hall sensor and actuator array, and a microcontroller unit. The e-skin captures the magnetic field changes caused by subtle deformations through Hall sensors, employing deep learning for accurate tactile perception. Simultaneously, the actuator array generates mechanical vibrations to facilitate haptic feedback, delivering diverse mechanical stimuli. Notably, the dual-modal e-skin is capable of transmitting tactile information bidirectionally, enabling object recognition and fine-weighing operations. This bidirectional tactile interaction framework will enhance the immersion and efficiency of interactions between humans and robots.","sentences":["To foster an immersive and natural human-robot interaction, the implementation of tactile perception and feedback becomes imperative, effectively bridging the conventional sensory gap.","In this paper, we propose a dual-modal electronic skin (e-skin) that integrates magnetic tactile sensing and vibration feedback for enhanced human-robot interaction.","The dual-modal tactile e-skin offers multi-functional tactile sensing and programmable haptic feedback, underpinned by a layered structure comprised of flexible magnetic films, soft silicone, a Hall sensor and actuator array, and a microcontroller unit.","The e-skin captures the magnetic field changes caused by subtle deformations through Hall sensors, employing deep learning for accurate tactile perception.","Simultaneously, the actuator array generates mechanical vibrations to facilitate haptic feedback, delivering diverse mechanical stimuli.","Notably, the dual-modal e-skin is capable of transmitting tactile information bidirectionally, enabling object recognition and fine-weighing operations.","This bidirectional tactile interaction framework will enhance the immersion and efficiency of interactions between humans and robots."],"url":"http://arxiv.org/abs/2402.05725v1","category":"cs.RO"}
{"created":"2024-02-08 14:54:47","title":"Model-Based RL for Mean-Field Games is not Statistically Harder than Single-Agent RL","abstract":"We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy. We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity. Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \\citet{huang2023statistical}. We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED. Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \\emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}. We further extend our results to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple types of agents. This extension implies statistical tractability of a broader class of Markov Games through the efficacy of mean-field approximation. Finally, inspired by our theoretical algorithm, we present a heuristic approach with improved computational efficiency and empirically demonstrate its effectiveness.","sentences":["We study the sample complexity of reinforcement learning (RL) in Mean-Field Games (MFGs) with model-based function approximation that requires strategic exploration to find a Nash Equilibrium policy.","We introduce the Partial Model-Based Eluder Dimension (P-MBED), a more effective notion to characterize the model class complexity.","Notably, P-MBED measures the complexity of the single-agent model class converted from the given mean-field model class, and potentially, can be exponentially lower than the MBED proposed by \\citet{huang2023statistical}.","We contribute a model elimination algorithm featuring a novel exploration strategy and establish sample complexity results polynomial w.r.t.~P-MBED.","Crucially, our results reveal that, under the basic realizability and Lipschitz continuity assumptions, \\emph{learning Nash Equilibrium in MFGs is no more statistically challenging than solving a logarithmic number of single-agent RL problems}.","We further extend our results to Multi-Type MFGs, generalizing from conventional MFGs and involving multiple types of agents.","This extension implies statistical tractability of a broader class of Markov Games through the efficacy of mean-field approximation.","Finally, inspired by our theoretical algorithm, we present a heuristic approach with improved computational efficiency and empirically demonstrate its effectiveness."],"url":"http://arxiv.org/abs/2402.05724v1","category":"cs.LG"}
{"created":"2024-02-08 14:54:17","title":"In-Context Learning Can Re-learn Forbidden Tasks","abstract":"Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities. One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries. To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer. Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them. We first examine a toy example of refusing sentiment classification to demonstrate the problem. Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles. Finally, we investigate whether ICL can undo safety training, which could represent a major security risk. For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starling-7B and Vicuna-7B but fails on Llama2-7B. Finally, we propose an ICL attack that uses the chat template tokens like a prompt injection attack to achieve a better attack success rate on Vicuna-7B and Starling-7B.   Trigger Warning: the appendix contains LLM-generated text with violence, suicide, and misinformation.","sentences":["Despite significant investment into safety training, large language models (LLMs) deployed in the real world still suffer from numerous vulnerabilities.","One perspective on LLM safety training is that it algorithmically forbids the model from answering toxic or harmful queries.","To assess the effectiveness of safety training, in this work, we study forbidden tasks, i.e., tasks the model is designed to refuse to answer.","Specifically, we investigate whether in-context learning (ICL) can be used to re-learn forbidden tasks despite the explicit fine-tuning of the model to refuse them.","We first examine a toy example of refusing sentiment classification to demonstrate the problem.","Then, we use ICL on a model fine-tuned to refuse to summarise made-up news articles.","Finally, we investigate whether ICL can undo safety training, which could represent a major security risk.","For the safety task, we look at Vicuna-7B, Starling-7B, and Llama2-7B. We show that the attack works out-of-the-box on Starling-7B and Vicuna-7B but fails on Llama2-7B.","Finally, we propose an ICL attack that uses the chat template tokens like a prompt injection attack to achieve a better attack success rate on Vicuna-7B and Starling-7B.   Trigger Warning: the appendix contains LLM-generated text with violence, suicide, and misinformation."],"url":"http://arxiv.org/abs/2402.05723v1","category":"cs.LG"}
{"created":"2024-02-08 14:52:17","title":"Measurements of the Low-Acceleration Gravitational Anomaly from the Normalized Velocity Profile of Gaia Wide Binary Stars and Statistical Testing of Newtonian and Milgromian Theories","abstract":"Low-acceleration gravitational anomaly is investigated with a new method of exploiting the normalized velocity profile $\\tilde{v}\\equiv v_p/v_c$ of wide binary stars as a function of the normalized sky-projected radius $s/r_{\\rm{M}}$ where $v_p$ is the sky-projected relative velocity between the pair, $v_c$ is the Newtonian circular velocity at the sky-projected separation $s$, and $r_{\\rm{M}}$ is the MOND radius. With a Monte Carlo method Gaia observed binaries and their virtual Newtonian counterparts are probabilistically distributed on the $s/r_{\\rm{M}}$ versus $\\tilde{v}$ plane and a logarithmic velocity ratio parameter $\\Gamma$ is measured in the bins of $s/r_{\\rm{M}}$. With three samples of binaries covering a broad range in size, data quality, and implied fraction of hierarchical systems including a new sample of 6389 binaries selected with accurate distances and radial velocities, I find a unanimous systematic variation from the Newtonian flat line. With $\\Gamma=0$ at $s/r_{\\rm{M}}\\lesssim 0.15$ or $s\\lesssim 1$~kilo astronomical units (kau), I get $\\Gamma=0.068\\pm 0.015$ (stat) $_{-0.015}^{+0.024}$ (syst) for $s/r_{\\rm{M}} \\gtrsim 0.7$ or $s\\gtrsim 5$~kau. The gravitational anomaly (i.e.\\ acceleration boost) factor given by $\\gamma_g = 10^{2\\Gamma}$ is measured to be $\\gamma_g = 1.37_{-0.09}^{+0.10}$ (stat) $_{-0.09}^{+0.16}$ (syst). With a reduced $\\chi^2$ test of Newtonian and Milgromian nonrelativistic theories, I find that Newtonian gravity is ruled out at $5.8\\sigma$ ($\\chi^2_\\nu=9.4$) by the new sample (and $9.2\\sigma$ by the largest sample used). The Milgromian AQUAL theory is acceptable with $0.5\\lesssim \\chi^2_\\nu\\lesssim 3.1$. These results agree well with earlier results with the ``acceleration-plane analysis'' for a variety of samples and the ``stacked velocity profile analysis'' for a pure binary sample.","sentences":["Low-acceleration gravitational anomaly is investigated with a new method of exploiting the normalized velocity profile $\\tilde{v}\\equiv v_p/v_c$ of wide binary stars as a function of the normalized sky-projected radius $s/r_{\\rm{M}}$ where $v_p$ is the sky-projected relative velocity between the pair, $v_c$ is the Newtonian circular velocity at the sky-projected separation $s$, and $r_{\\rm{M}}$ is the MOND radius.","With a Monte Carlo method Gaia observed binaries and their virtual Newtonian counterparts are probabilistically distributed on the $s/r_{\\rm{M}}$ versus $\\tilde{v}$ plane and a logarithmic velocity ratio parameter $\\Gamma$ is measured in the bins of $s/r_{\\rm{M}}$. With three samples of binaries covering a broad range in size, data quality, and implied fraction of hierarchical systems including a new sample of 6389 binaries selected with accurate distances and radial velocities, I find a unanimous systematic variation from the Newtonian flat line.","With $\\Gamma=0$ at $s/r_{\\rm{M}}\\lesssim 0.15$ or $s\\lesssim 1$~kilo astronomical units (kau), I get $\\Gamma=0.068\\pm 0.015$ (stat) $_{-0.015}^{+0.024}$ (syst) for $s/r_{\\rm{M}} \\gtrsim 0.7$ or $s\\gtrsim 5$~kau.","The gravitational anomaly (i.e.\\ acceleration boost) factor given by $\\gamma_g = 10^{2\\Gamma}$ is measured to be $\\gamma_g = 1.37_{-0.09}^{+0.10}$ (stat) $_{-0.09}^{+0.16}$ (syst).","With a reduced $\\chi^2$ test of Newtonian and Milgromian nonrelativistic theories, I find that Newtonian gravity is ruled out at $5.8\\sigma$ ($\\chi^2_\\nu=9.4$) by the new sample (and $9.2\\sigma$ by the largest sample used).","The Milgromian AQUAL theory is acceptable with $0.5\\lesssim \\chi^2_\\nu\\lesssim 3.1$. These results agree well with earlier results with the ``acceleration-plane analysis'' for a variety of samples and the ``stacked velocity profile analysis'' for a pure binary sample."],"url":"http://arxiv.org/abs/2402.05720v1","category":"astro-ph.GA"}
{"created":"2024-02-08 14:50:07","title":"Exact capacity of the \\emph{wide} hidden layer treelike neural networks with generic activations","abstract":"Recent progress in studying \\emph{treelike committee machines} (TCM) neural networks (NN) in \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \\emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis. Here, we consider \\emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear. In particular, we employ recently developed \\emph{fully lifted} (fl) RDT to characterize the \\emph{wide} ($d\\rightarrow \\infty$) TCM nets capacity. We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations. While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical work. To get the concrete capacity values, we take four very famous activations examples: \\emph{\\textbf{ReLU}}, \\textbf{\\emph{quadratic}}, \\textbf{\\emph{erf}}, and \\textbf{\\emph{tanh}}. After successfully conducting all the residual numerical work for all of them, we uncover that the whole lifting mechanism exhibits a remarkably rapid convergence with the relative improvements no better than $\\sim 0.1\\%$ happening already on the 3-rd level of lifting. As a convenient bonus, we also uncover that the capacity characterizations obtained on the first and second level of lifting precisely match those obtained through the statistical physics replica theory methods in \\cite{ZavPeh21} for the generic and in \\cite{BalMalZech19} for the ReLU activations.","sentences":["Recent progress in studying \\emph{treelike committee machines} (TCM) neural networks (NN) in \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23,Stojnictcmspnncapdiffactrdt23} showed that the Random Duality Theory (RDT) and its a \\emph{partially lifted}(pl RDT) variant are powerful tools that can be used for very precise networks capacity analysis.","Here, we consider \\emph{wide} hidden layer networks and uncover that certain aspects of numerical difficulties faced in \\cite{Stojnictcmspnncapdiffactrdt23} miraculously disappear.","In particular, we employ recently developed \\emph{fully lifted} (fl) RDT to characterize the \\emph{wide} ($d\\rightarrow \\infty$) TCM nets capacity.","We obtain explicit, closed form, capacity characterizations for a very generic class of the hidden layer activations.","While the utilized approach significantly lowers the amount of the needed numerical evaluations, the ultimate fl RDT usefulness and success still require a solid portion of the residual numerical work.","To get the concrete capacity values, we take four very famous activations examples: \\emph{\\textbf{ReLU}}, \\textbf{\\emph{quadratic}}, \\textbf{\\emph{erf}}, and \\textbf{\\emph{tanh}}.","After successfully conducting all the residual numerical work for all of them, we uncover that the whole lifting mechanism exhibits a remarkably rapid convergence with the relative improvements no better than $\\sim 0.1\\%$ happening already on the 3-rd level of lifting.","As a convenient bonus, we also uncover that the capacity characterizations obtained on the first and second level of lifting precisely match those obtained through the statistical physics replica theory methods in \\cite{ZavPeh21} for the generic and in \\cite{BalMalZech19} for the ReLU activations."],"url":"http://arxiv.org/abs/2402.05719v1","category":"stat.ML"}
{"created":"2024-02-08 14:47:37","title":"REMEDI: Corrective Transformations for Improved Neural Entropy Estimation","abstract":"Information theoretic quantities play a central role in machine learning. The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities. However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions. To address this issue, in this work, we introduce $\\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity. The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density. Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data. Further, we extend important theoretical consistency results to a more generalized setting required by our approach. We illustrate how the framework can be naturally extended to information theoretic supervised learning models, with a specific focus on the Information Bottleneck approach. It is demonstrated that the method delivers better accuracy compared to the existing methods in Information Bottleneck. In addition, we explore a natural connection between $\\texttt{REMEDI}$ and generative modeling using rejection sampling and Langevin dynamics.","sentences":["Information theoretic quantities play a central role in machine learning.","The recent surge in the complexity of data and models has increased the demand for accurate estimation of these quantities.","However, as the dimension grows the estimation presents significant challenges, with existing methods struggling already in relatively low dimensions.","To address this issue, in this work, we introduce $\\texttt{REMEDI}$ for efficient and accurate estimation of differential entropy, a fundamental information theoretic quantity.","The approach combines the minimization of the cross-entropy for simple, adaptive base models and the estimation of their deviation, in terms of the relative entropy, from the data density.","Our approach demonstrates improvement across a broad spectrum of estimation tasks, encompassing entropy estimation on both synthetic and natural data.","Further, we extend important theoretical consistency results to a more generalized setting required by our approach.","We illustrate how the framework can be naturally extended to information theoretic supervised learning models, with a specific focus on the Information Bottleneck approach.","It is demonstrated that the method delivers better accuracy compared to the existing methods in Information Bottleneck.","In addition, we explore a natural connection between $\\texttt{REMEDI}$ and generative modeling using rejection sampling and Langevin dynamics."],"url":"http://arxiv.org/abs/2402.05718v1","category":"stat.ML"}
{"created":"2024-02-08 14:45:04","title":"Who is in equilibrium?","abstract":"In order to describe the properties of the observed distribution of wealth in a population, most economic models rely on the existence of an asymptotic equilibrium state. In addition, the process generating the equilibrium distribution is usually assumed to be ergodic, with a finite asymptotic average and bounded inequality. Here we show, using data from Bank of Italy's Survey on Household Income and Wealth and Forbes Italian billionaires lists, that the last hypothesis is not justified in Italy. We find that, even if an equilibrium asymptotic distribution exists, the average wealth has no finite asymptotic value. As a consequence we find that - without changes in the parameters of the wealth evolution process - wealth inequality is bound to diverge with time. In addition we evaluate the equilibration time of the evolution process when its parameters are chosen in order to admit both an equilibrium distribution and a finite equilibrium average wealth. Even when both the equilibrium hypotheses are satisfied, we find equilibration times much longer than the typical time span between economic shocks.","sentences":["In order to describe the properties of the observed distribution of wealth in a population, most economic models rely on the existence of an asymptotic equilibrium state.","In addition, the process generating the equilibrium distribution is usually assumed to be ergodic, with a finite asymptotic average and bounded inequality.","Here we show, using data from Bank of Italy's Survey on Household Income and Wealth and Forbes Italian billionaires lists, that the last hypothesis is not justified in Italy.","We find that, even if an equilibrium asymptotic distribution exists, the average wealth has no finite asymptotic value.","As a consequence we find that - without changes in the parameters of the wealth evolution process - wealth inequality is bound to diverge with time.","In addition we evaluate the equilibration time of the evolution process when its parameters are chosen in order to admit both an equilibrium distribution and a finite equilibrium average wealth.","Even when both the equilibrium hypotheses are satisfied, we find equilibration times much longer than the typical time span between economic shocks."],"url":"http://arxiv.org/abs/2402.05716v1","category":"physics.soc-ph"}
{"created":"2024-02-08 14:40:32","title":"Hidden in Plain Sight: Undetectable Adversarial Bias Attacks on Vulnerable Patient Populations","abstract":"The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations. While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging. In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance. Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data.","sentences":["The proliferation of artificial intelligence (AI) in radiology has shed light on the risk of deep learning (DL) models exacerbating clinical biases towards vulnerable patient populations.","While prior literature has focused on quantifying biases exhibited by trained DL models, demographically targeted adversarial bias attacks on DL models and its implication in the clinical environment remains an underexplored field of research in medical imaging.","In this work, we demonstrate that demographically targeted label poisoning attacks can introduce adversarial underdiagnosis bias in DL models and degrade performance on underrepresented groups without impacting overall model performance.","Moreover, our results across multiple performance metrics and demographic groups like sex, age, and their intersectional subgroups indicate that a group's vulnerability to undetectable adversarial bias attacks is directly correlated with its representation in the model's training data."],"url":"http://arxiv.org/abs/2402.05713v1","category":"cs.LG"}
{"created":"2024-02-08 14:39:16","title":"DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer","abstract":"Speech-driven 3D facial animation is important for many multimedia applications. Recent work has shown promise in using either Diffusion models or Transformer architectures for this task. However, their mere aggregation does not lead to improved performance. We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework. To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules. These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions. We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm. Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel.","sentences":["Speech-driven 3D facial animation is important for many multimedia applications.","Recent work has shown promise in using either Diffusion models or Transformer architectures for this task.","However, their mere aggregation does not lead to improved performance.","We suspect this is due to a shortage of paired audio-4D data, which is crucial for the Transformer to effectively perform as a denoiser within the Diffusion framework.","To tackle this issue, we present DiffSpeaker, a Transformer-based network equipped with novel biased conditional attention modules.","These modules serve as substitutes for the traditional self/cross-attention in standard Transformers, incorporating thoughtfully designed biases that steer the attention mechanisms to concentrate on both the relevant task-specific and diffusion-related conditions.","We also explore the trade-off between accurate lip synchronization and non-verbal facial expressions within the Diffusion paradigm.","Experiments show our model not only achieves state-of-the-art performance on existing benchmarks, but also fast inference speed owing to its ability to generate facial motions in parallel."],"url":"http://arxiv.org/abs/2402.05712v1","category":"cs.CV"}
{"created":"2024-02-08 14:36:48","title":"On the role of parametrization in models with a misspecified nuisance component","abstract":"The paper is concerned with inference for a parameter of interest in models that share a common interpretation for that parameter, but that may differ appreciably in other respects. We study the general structure of models under which the maximum likelihood estimator of the parameter of interest is consistent under arbitrary misspecification of the nuisance part of the model. A specialization of the general results to matched-comparison and two-groups problems gives a more explicit condition in terms of a new notion of symmetric parametrization, leading to an appreciable broadening and unification of existing results in those problems. The role of a generalized definition of parameter orthogonality is highlighted, as well as connections to Neyman orthogonality. The issues involved in obtaining inferential guarantees beyond consistency are discussed.","sentences":["The paper is concerned with inference for a parameter of interest in models that share a common interpretation for that parameter, but that may differ appreciably in other respects.","We study the general structure of models under which the maximum likelihood estimator of the parameter of interest is consistent under arbitrary misspecification of the nuisance part of the model.","A specialization of the general results to matched-comparison and two-groups problems gives a more explicit condition in terms of a new notion of symmetric parametrization, leading to an appreciable broadening and unification of existing results in those problems.","The role of a generalized definition of parameter orthogonality is highlighted, as well as connections to Neyman orthogonality.","The issues involved in obtaining inferential guarantees beyond consistency are discussed."],"url":"http://arxiv.org/abs/2402.05708v1","category":"math.ST"}
{"created":"2024-02-08 14:35:09","title":"Unified Speech-Text Pretraining for Spoken Dialog Modeling","abstract":"While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation. This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions. Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM. We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics. Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines. Detailed comparative studies reveal that, despite the cascaded approach being stronger in individual components, the joint speech-text modeling improves robustness against recognition errors and speech quality. Demo is available at https://unifiedsdm.github.io.","sentences":["While recent work shows promising results in expanding the capabilities of large language models (LLM) to directly understand and synthesize speech, an LLM-based strategy for modeling spoken dialogs remains elusive and calls for further investigation.","This work proposes an extensive speech-text LLM framework, named the Unified Spoken Dialog Model (USDM), to generate coherent spoken responses with organic prosodic features relevant to the given input speech without relying on automatic speech recognition (ASR) or text-to-speech (TTS) solutions.","Our approach employs a multi-step speech-text inference scheme that leverages chain-of-reasoning capabilities exhibited by the underlying LLM.","We also propose a generalized speech-text pretraining scheme that helps with capturing cross-modal semantics.","Automatic and human evaluations show that the proposed approach is effective in generating natural-sounding spoken responses, outperforming both prior and cascaded baselines.","Detailed comparative studies reveal that, despite the cascaded approach being stronger in individual components, the joint speech-text modeling improves robustness against recognition errors and speech quality.","Demo is available at https://unifiedsdm.github.io."],"url":"http://arxiv.org/abs/2402.05706v1","category":"cs.CL"}
{"created":"2024-02-08 14:29:24","title":"On the Optimal Communication Weights in Distributed Optimization Algorithms","abstract":"We establish that in distributed optimization, the prevalent strategy of minimizing the second-largest eigenvalue modulus (SLEM) of the averaging matrix for selecting communication weights, while optimal for existing theoretical performance bounds, is generally not optimal regarding the exact worst-case performance of the algorithms. This exact performance can be computed using the Performance Estimation Problem (PEP) approach. We thus rely on PEP to formulate an optimization problem that determines the optimal communication weights for a distributed optimization algorithm deployed on a specified undirected graph. Our results show that the optimal weights can outperform the weights minimizing the second-largest eigenvalue modulus (SLEM) of the averaging matrix. This suggests that the SLEM is not the best characterization of weighted network performance for decentralized optimization. Additionally, we explore and compare alternative heuristics for weight selection in distributed optimization.","sentences":["We establish that in distributed optimization, the prevalent strategy of minimizing the second-largest eigenvalue modulus (SLEM) of the averaging matrix for selecting communication weights, while optimal for existing theoretical performance bounds, is generally not optimal regarding the exact worst-case performance of the algorithms.","This exact performance can be computed using the Performance Estimation Problem (PEP) approach.","We thus rely on PEP to formulate an optimization problem that determines the optimal communication weights for a distributed optimization algorithm deployed on a specified undirected graph.","Our results show that the optimal weights can outperform the weights minimizing the second-largest eigenvalue modulus (SLEM) of the averaging matrix.","This suggests that the SLEM is not the best characterization of weighted network performance for decentralized optimization.","Additionally, we explore and compare alternative heuristics for weight selection in distributed optimization."],"url":"http://arxiv.org/abs/2402.05705v1","category":"math.OC"}
{"created":"2024-02-08 14:27:34","title":"Offline Risk-sensitive RL with Partial Observability to Enhance Performance in Human-Robot Teaming","abstract":"The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system. This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents. Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge. To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human's state. Recent research suggests to learn a Partially Observable Markov Decision Process (POMDP) model from a data set of previously collected experiences that can be solved using Offline Reinforcement Learning (ORL) methods. In the present work, we not only highlight the potential of partially observable representations and physiological measurements to improve human operator state estimation and performance, but also enhance the overall mission effectiveness of a human-robot team. Importantly, as the fixed data set may not contain enough information to fully represent complex stochastic processes, we propose a method to incorporate model uncertainty, thus enabling risk-sensitive sequential decision-making. Experiments were conducted with a group of twenty-six human participants within a simulated robot teleoperation environment, yielding empirical evidence of the method's efficacy. The obtained adaptive task allocation policy led to statistically significant higher scores than the one that was used to collect the data set, allowing for generalization across diverse participants also taking into account risk-sensitive metrics.","sentences":["The integration of physiological computing into mixed-initiative human-robot interaction systems offers valuable advantages in autonomous task allocation by incorporating real-time features as human state observations into the decision-making system.","This approach may alleviate the cognitive load on human operators by intelligently allocating mission tasks between agents.","Nevertheless, accommodating a diverse pool of human participants with varying physiological and behavioral measurements presents a substantial challenge.","To address this, resorting to a probabilistic framework becomes necessary, given the inherent uncertainty and partial observability on the human's state.","Recent research suggests to learn a Partially Observable Markov Decision Process (POMDP) model from a data set of previously collected experiences that can be solved using Offline Reinforcement Learning (ORL) methods.","In the present work, we not only highlight the potential of partially observable representations and physiological measurements to improve human operator state estimation and performance, but also enhance the overall mission effectiveness of a human-robot team.","Importantly, as the fixed data set may not contain enough information to fully represent complex stochastic processes, we propose a method to incorporate model uncertainty, thus enabling risk-sensitive sequential decision-making.","Experiments were conducted with a group of twenty-six human participants within a simulated robot teleoperation environment, yielding empirical evidence of the method's efficacy.","The obtained adaptive task allocation policy led to statistically significant higher scores than the one that was used to collect the data set, allowing for generalization across diverse participants also taking into account risk-sensitive metrics."],"url":"http://arxiv.org/abs/2402.05703v1","category":"cs.MA"}
{"created":"2024-02-08 14:25:52","title":"Exact functional integration of radial and complex slave-boson fields: thermodynamics and dynamics of the two-site extended Hubbard model","abstract":"The functional integral formulation of the Hubbard Model when treated in its Kotliar-Ruckenstein representation in the radial gauge involves fermionic, as well as complex and radial slave boson fields. In order to improve on the understanding of the interplay of the three types of fields, and on the nature of the latter, we perform a comprehensive investigation of an exactly solvable two-site cluster, as it entails all pitfalls embodied in this approach. It is first shown that the exact partition function is recovered, even when incorporating in the calculation the square root factors that are at the heart of the representation, when suitably regularized. We show that using radial slave boson fields allows to overcome all hurdles following from the normal ordering procedure. We then demonstrate that this applies to the Green's function too, as well as to the correlation functions of physical interest, thereby answering the criticisms raised by Sch\\\"onhammer [K. Sch\\\"onhammer, Phys. Rev. B \\textbf{42}, 2591 (1990)]. In addition, the investigation generalizes the calculations to the Hubbard Model extended by a non-local Coulomb interaction.","sentences":["The functional integral formulation of the Hubbard Model when treated in its Kotliar-Ruckenstein representation in the radial gauge involves fermionic, as well as complex and radial slave boson fields.","In order to improve on the understanding of the interplay of the three types of fields, and on the nature of the latter, we perform a comprehensive investigation of an exactly solvable two-site cluster, as it entails all pitfalls embodied in this approach.","It is first shown that the exact partition function is recovered, even when incorporating in the calculation the square root factors that are at the heart of the representation, when suitably regularized.","We show that using radial slave boson fields allows to overcome all hurdles following from the normal ordering procedure.","We then demonstrate that this applies to the Green's function too, as well as to the correlation functions of physical interest, thereby answering the criticisms raised by Sch\\\"onhammer [K. Sch\\\"onhammer, Phys. Rev. B \\textbf{42}, 2591 (1990)].","In addition, the investigation generalizes the calculations to the Hubbard Model extended by a non-local Coulomb interaction."],"url":"http://arxiv.org/abs/2402.05701v1","category":"cond-mat.str-el"}
{"created":"2024-02-08 14:21:03","title":"Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation","abstract":"Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse. Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation. To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding. MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself. To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed. We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions. Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks. As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values. Code is available at https://github.com/pangxianghe/MATRIX.","sentences":["Aligning large language models (LLMs) with human values is imperative to mitigate potential adverse effects resulting from their misuse.","Drawing from the sociological insight that acknowledging all parties' concerns is a key factor in shaping human values, this paper proposes a novel direction to align LLMs by themselves: social scene simulation.","To achieve this, we present MATRIX, a novel social scene simulator that emulates realistic scenes around a user's input query, enabling the LLM to take social consequences into account before responding.","MATRIX serves as a virtual rehearsal space, akin to a Monopolylogue, where the LLM performs diverse roles related to the query and practice by itself.","To inject this alignment, we fine-tune the LLM with MATRIX-simulated data, ensuring adherence to human values without compromising inference speed.","We theoretically show that the LLM with MATRIX outperforms Constitutional AI under mild assumptions.","Finally, extensive experiments validate that our method outperforms over 10 baselines across 4 benchmarks.","As evidenced by 875 user ratings, our tuned 13B-size LLM exceeds GPT-4 in aligning with human values.","Code is available at https://github.com/pangxianghe/MATRIX."],"url":"http://arxiv.org/abs/2402.05699v1","category":"cs.CL"}
{"created":"2024-02-08 14:19:47","title":"Evolving AI for Wellness: Dynamic and Personalized Real-time Loneliness Detection Using Passive Sensing","abstract":"Loneliness is a growing health concern as it can lead to depression and other associated mental health problems for people who experience feelings of loneliness over prolonged periods of time. Utilizing passive sensing methods that use smartphone and wearable sensor data to capture daily behavioural patterns offers a promising approach for the early detection of loneliness. Given the subjective nature of loneliness and people's varying daily routines, past detection approaches using machine learning models often face challenges with effectively detecting loneliness. This paper proposes a methodologically novel approach, particularly developing a loneliness detection system that evolves over time, adapts to new data, and provides real-time detection. Our study utilized the Globem dataset, a comprehensive collection of passive sensing data acquired over 10 weeks from university students. The base of our approach is the continuous identification and refinement of similar behavioural groups among students using an incremental clustering method. As we add new data, the model improves based on changing behavioural patterns. Parallel to this, we create and update classification models to detect loneliness among the evolving behavioural groups of students. When unique behavioural patterns are observed among student data, specialized classification models have been created. For predictions of loneliness, a collaborative effort between the generalized and specialized models is employed, treating each prediction as a vote. This study's findings reveal that group-based loneliness detection models exhibit superior performance compared to generic models, underscoring the necessity for more personalized approaches tailored to specific behavioural patterns. These results pave the way for future research, emphasizing the development of finely-tuned, individualized mental health interventions.","sentences":["Loneliness is a growing health concern as it can lead to depression and other associated mental health problems for people who experience feelings of loneliness over prolonged periods of time.","Utilizing passive sensing methods that use smartphone and wearable sensor data to capture daily behavioural patterns offers a promising approach for the early detection of loneliness.","Given the subjective nature of loneliness and people's varying daily routines, past detection approaches using machine learning models often face challenges with effectively detecting loneliness.","This paper proposes a methodologically novel approach, particularly developing a loneliness detection system that evolves over time, adapts to new data, and provides real-time detection.","Our study utilized the Globem dataset, a comprehensive collection of passive sensing data acquired over 10 weeks from university students.","The base of our approach is the continuous identification and refinement of similar behavioural groups among students using an incremental clustering method.","As we add new data, the model improves based on changing behavioural patterns.","Parallel to this, we create and update classification models to detect loneliness among the evolving behavioural groups of students.","When unique behavioural patterns are observed among student data, specialized classification models have been created.","For predictions of loneliness, a collaborative effort between the generalized and specialized models is employed, treating each prediction as a vote.","This study's findings reveal that group-based loneliness detection models exhibit superior performance compared to generic models, underscoring the necessity for more personalized approaches tailored to specific behavioural patterns.","These results pave the way for future research, emphasizing the development of finely-tuned, individualized mental health interventions."],"url":"http://arxiv.org/abs/2402.05698v1","category":"cs.HC"}
{"created":"2024-02-08 14:19:29","title":"Fixed width treelike neural networks capacity analysis -- generic activations","abstract":"We consider the capacity of \\emph{treelike committee machines} (TCM) neural networks. Relying on Random Duality Theory (RDT), \\cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis. An upgrade based on the so-called \\emph{partially lifted} RDT (pl RDT) was then presented in \\cite{Stojnictcmspnncapliftedrdt23}. Both lines of work focused on the networks with the most typical, \\emph{sign}, activations. Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well. In addition to the standard \\emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \\emph{quadratic} and \\emph{rectified linear unit (ReLU)} ones. In more concrete terms, for each of these activations, we obtain both the RDT and pl RDT based memory capacities upper bound characterization for \\emph{any} given (even) number of the hidden layer neurons, $d$. In the process, we also uncover the following two, rather remarkable, facts: 1) contrary to the common wisdom, both sets of results show that the bounding capacity decreases for large $d$ (the width of the hidden layer) while converging to a constant value; and 2) the maximum bounding capacity is achieved for the networks with precisely \\textbf{\\emph{two}} hidden layer neurons! Moreover, the large $d$ converging values are observed to be in excellent agrement with the statistical physics replica theory based predictions.","sentences":["We consider the capacity of \\emph{treelike committee machines} (TCM) neural networks.","Relying on Random Duality Theory (RDT), \\cite{Stojnictcmspnncaprdt23} recently introduced a generic framework for their capacity analysis.","An upgrade based on the so-called \\emph{partially lifted} RDT (pl RDT) was then presented in \\cite{Stojnictcmspnncapliftedrdt23}.","Both lines of work focused on the networks with the most typical, \\emph{sign}, activations.","Here, on the other hand, we focus on networks with other, more general, types of activations and show that the frameworks of \\cite{Stojnictcmspnncaprdt23,Stojnictcmspnncapliftedrdt23} are sufficiently powerful to enable handling of such scenarios as well.","In addition to the standard \\emph{linear} activations, we uncover that particularly convenient results can be obtained for two very commonly used activations, namely, the \\emph{quadratic} and \\emph{rectified linear unit (ReLU)} ones.","In more concrete terms, for each of these activations, we obtain both the RDT and pl RDT based memory capacities upper bound characterization for \\emph{any} given (even) number of the hidden layer neurons, $d$. In the process, we also uncover the following two, rather remarkable, facts: 1) contrary to the common wisdom, both sets of results show that the bounding capacity decreases for large $d$ (the width of the hidden layer) while converging to a constant value; and 2) the maximum bounding capacity is achieved for the networks with precisely \\textbf{\\emph{two}} hidden layer neurons!","Moreover, the large $d$ converging values are observed to be in excellent agrement with the statistical physics replica theory based predictions."],"url":"http://arxiv.org/abs/2402.05696v1","category":"stat.ML"}
{"created":"2024-02-08 14:12:55","title":"Avoiding lateral mode leakage in thin film lithium niobate waveguides for the generation of spectrally pure photons at telecom wavelengths","abstract":"Photonic integrated optical components, notably straight waveguides, serve as pivotal elements for on-chip generation and manipulation of quantum states of light. In this work, we focus on optimizing waveguides based on lithium niobate on insulator (LNOI) to generate photon pairs at telecom wavelength using spontaneous parametric down-conversion (SPDC). Specifically, we investigate lateral leakage for all possible SPDC processes involving type 0, type I and type II phase matching conditions in an X-cut lithium niobate waveguide and provide a recipe to avoid leakage loss for the interacting photons. Furthermore, focusing on type II phase matching, we engineer the waveguide in the single mode regime such that it also satisfies group index matching for generating spectrally pure single photons with high purity (99.33%). We also address fabrication imperfections of the optimized design and found that the spectral purity of the generated photons is robust to fabrication errors. This work serves as a tutorial for the appropriate selection of morphological parameters to obtain lossless, single mode LNOI waveguides for building linear optical circuits and photon pair generation at telecom wavelengths using desired phase-matching conditions.","sentences":["Photonic integrated optical components, notably straight waveguides, serve as pivotal elements for on-chip generation and manipulation of quantum states of light.","In this work, we focus on optimizing waveguides based on lithium niobate on insulator (LNOI) to generate photon pairs at telecom wavelength using spontaneous parametric down-conversion (SPDC).","Specifically, we investigate lateral leakage for all possible SPDC processes involving type 0, type I and type II phase matching conditions in an X-cut lithium niobate waveguide and provide a recipe to avoid leakage loss for the interacting photons.","Furthermore, focusing on type II phase matching, we engineer the waveguide in the single mode regime such that it also satisfies group index matching for generating spectrally pure single photons with high purity (99.33%).","We also address fabrication imperfections of the optimized design and found that the spectral purity of the generated photons is robust to fabrication errors.","This work serves as a tutorial for the appropriate selection of morphological parameters to obtain lossless, single mode LNOI waveguides for building linear optical circuits and photon pair generation at telecom wavelengths using desired phase-matching conditions."],"url":"http://arxiv.org/abs/2402.05694v1","category":"quant-ph"}
{"created":"2024-02-08 14:09:17","title":"Adaptive Methods for Variational Inequalities under Relaxed Smoothness Assumption","abstract":"Variational Inequality (VI) problems have attracted great interest in the machine learning (ML) community due to their application in adversarial and multi-agent training. Despite its relevance in ML, the oft-used strong-monotonicity and Lipschitz continuity assumptions on VI problems are restrictive and do not hold in practice. To address this, we relax smoothness and monotonicity assumptions and study structured non-monotone generalized smoothness. The key idea of our results is in adaptive stepsizes. We prove the first-known convergence results for solving generalized smooth VIs for the three popular methods, namely, projection, Korpelevich, and Popov methods. Our convergence rate results for generalized smooth VIs match or improve existing results on smooth VIs. We present numerical experiments that support our theoretical guarantees and highlight the efficiency of proposed adaptive stepsizes.","sentences":["Variational Inequality (VI) problems have attracted great interest in the machine learning (ML) community due to their application in adversarial and multi-agent training.","Despite its relevance in ML, the oft-used strong-monotonicity and Lipschitz continuity assumptions on VI problems are restrictive and do not hold in practice.","To address this, we relax smoothness and monotonicity assumptions and study structured non-monotone generalized smoothness.","The key idea of our results is in adaptive stepsizes.","We prove the first-known convergence results for solving generalized smooth VIs for the three popular methods, namely, projection, Korpelevich, and Popov methods.","Our convergence rate results for generalized smooth VIs match or improve existing results on smooth VIs.","We present numerical experiments that support our theoretical guarantees and highlight the efficiency of proposed adaptive stepsizes."],"url":"http://arxiv.org/abs/2402.05691v1","category":"math.OC"}
{"created":"2024-02-08 14:07:36","title":"Overcoming Noise Limitations in QKD with Quantum Privacy Amplification","abstract":"High-quality, distributed quantum entanglement is the distinctive resource for quantum communication and forms the foundation for the unequalled level of security that can be assured in quantum key distribution. While the entanglement provider does not need to be trusted, the secure key rate drops to zero if the entanglement used is too noisy. In this paper, we show experimentally that QPA is able to increase the secure key rate achievable with QKD by improving the quality of distributed entanglement, thus increasing the quantum advantage in QKD. Beyond that, we show that QPA enables key generation at noise levels that previously prevented key generation. These remarkable results were only made possible by the efficient implementation exploiting hyperentanglement in the polarisation and energy-time degrees of freedom. We provide a detailed characterisation of the gain in secure key rate achieved in our proof-of-principle experiment at different noise levels. The results are paramount for the implementation of a global quantum network linking quantum processors and ensuring future-proof data security.","sentences":["High-quality, distributed quantum entanglement is the distinctive resource for quantum communication and forms the foundation for the unequalled level of security that can be assured in quantum key distribution.","While the entanglement provider does not need to be trusted, the secure key rate drops to zero if the entanglement used is too noisy.","In this paper, we show experimentally that QPA is able to increase the secure key rate achievable with QKD by improving the quality of distributed entanglement, thus increasing the quantum advantage in QKD.","Beyond that, we show that QPA enables key generation at noise levels that previously prevented key generation.","These remarkable results were only made possible by the efficient implementation exploiting hyperentanglement in the polarisation and energy-time degrees of freedom.","We provide a detailed characterisation of the gain in secure key rate achieved in our proof-of-principle experiment at different noise levels.","The results are paramount for the implementation of a global quantum network linking quantum processors and ensuring future-proof data security."],"url":"http://arxiv.org/abs/2402.05690v1","category":"quant-ph"}
{"created":"2024-02-08 13:59:53","title":"Gribov Problem in the Eletroweak and Gluon Confined Theories","abstract":"We show that gauge fields after a spontaneous symmetry breaking (SSB) mechanism do not have a Gribov problem along the broken directions. In order to make this proof, we describe a gauge fixing procedure inspired on Morse theory leading to the concept of a gauge fixing generating functional. This approach is specially suited in order to understand the unitary gauges of t Hooft. The conclusion is that after a SSB process, the generalized Faddeev-Popov operator acquires a positive definite value along the broken directions. We show how this works in the eletroweak $SU(2)XU(1)$ theory, where such a result is in fact expected as in this case the gauge fields acquire masses after the SSB. Then we apply this development to the $SL(3,c)$ confining model of [1]. The final result explains why such confining mechanism is actually a solution to Gribov problem for the confined gauge fields. These cases show that although confinement is not a general effect of the solution of the Gribov problem, as we can infer from the eletroweak example, its solution indeed seems to be necessary in order to achieve confinement. In the end, these conclusions allow us to speculate that the strong interaction confinement may be the result of some hidden SSB mechanism yet to be described.","sentences":["We show that gauge fields after a spontaneous symmetry breaking (SSB) mechanism do not have a Gribov problem along the broken directions.","In order to make this proof, we describe a gauge fixing procedure inspired on Morse theory leading to the concept of a gauge fixing generating functional.","This approach is specially suited in order to understand the unitary gauges of t Hooft.","The conclusion is that after a SSB process, the generalized Faddeev-Popov operator acquires a positive definite value along the broken directions.","We show how this works in the eletroweak $SU(2)XU(1)$ theory, where such a result is in fact expected as in this case the gauge fields acquire masses after the SSB.","Then we apply this development to the $SL(3,c)$ confining model of [1].","The final result explains why such confining mechanism is actually a solution to Gribov problem for the confined gauge fields.","These cases show that although confinement is not a general effect of the solution of the Gribov problem, as we can infer from the eletroweak example, its solution indeed seems to be necessary in order to achieve confinement.","In the end, these conclusions allow us to speculate that the strong interaction confinement may be the result of some hidden SSB mechanism yet to be described."],"url":"http://arxiv.org/abs/2402.05683v1","category":"hep-th"}
{"created":"2024-02-08 13:58:16","title":"Interpretable classifiers for tabular data via discretization and feature selection","abstract":"We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data. The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting. We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature. In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers. We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from.","sentences":["We introduce a method for computing immediately human interpretable yet accurate classifiers from tabular data.","The classifiers obtained are short DNF-formulas, computed via first discretizing the original data to Boolean form and then using feature selection coupled with a very fast algorithm for producing the best possible Boolean classifier for the setting.","We demonstrate the approach via 14 experiments, obtaining results with accuracies mainly similar to ones obtained via random forests, XGBoost, and existing results for the same datasets in the literature.","In several cases, our approach in fact outperforms the reference results in relation to accuracy, even though the main objective of our study is the immediate interpretability of our classifiers.","We also prove a new result on the probability that the classifier we obtain from real-life data corresponds to the ideally best classifier with respect to the background distribution the data comes from."],"url":"http://arxiv.org/abs/2402.05680v1","category":"cs.LG"}
{"created":"2024-02-08 13:55:52","title":"Heterogeneous drivers of overnight and same-day visits","abstract":"This paper aims to explore the factors stimulating different tourism behaviours, with specific reference to same-day visits and overnight stays. To this aim, we employ mobile network data referred to the area of Lombardy. The paper highlights that larger availability of tourism accommodations, cultural and natural endowments are relevant factors explaining overnight stays. Conversely, temporary entertainment and transportation facilities increase municipalities attractiveness for same-day visits. The results also highlight a trade-off in the capability of municipalities of being attractive in connection to both the tourism behaviours, with higher overnight stays in areas with more limited same-day visits. Mobile data offer a spatial and temporal granularity allowing to detect relevant patterns and support the design of tourism precision policies.","sentences":["This paper aims to explore the factors stimulating different tourism behaviours, with specific reference to same-day visits and overnight stays.","To this aim, we employ mobile network data referred to the area of Lombardy.","The paper highlights that larger availability of tourism accommodations, cultural and natural endowments are relevant factors explaining overnight stays.","Conversely, temporary entertainment and transportation facilities increase municipalities attractiveness for same-day visits.","The results also highlight a trade-off in the capability of municipalities of being attractive in connection to both the tourism behaviours, with higher overnight stays in areas with more limited same-day visits.","Mobile data offer a spatial and temporal granularity allowing to detect relevant patterns and support the design of tourism precision policies."],"url":"http://arxiv.org/abs/2402.05679v1","category":"econ.GN"}
{"created":"2024-02-08 13:54:50","title":"Vectorial Negabent Concepts: Similarities, Differences, and Generalizations","abstract":"In Pasalic et al., IEEE Trans. Inform. Theory 69 (2023), 2702--2712, and in Anbar, Meidl, Cryptogr. Commun. 10 (2018), 235--249, two different vectorial negabent and vectorial bent-negabent concepts are introduced, which leads to seemingly contradictory results. One of the main motivations for this article is to clarify the differences and similarities between these two concepts. Moreover, the negabent concept is extended to generalized Boolean functions from \\(\\mathbb{F}_2^n\\) to the cyclic group \\(\\mathbb{Z}_{2^k}\\). It is shown how to obtain nega-\\(\\mathbb{Z}_{2^k}\\)-bent functions from \\(\\mathbb{Z}_{2^k}\\)-bent functions, or equivalently, corresponding non-splitting relative difference sets from the splitting relative difference sets. This generalizes the shifting results for Boolean bent and negabent functions. We finally point to constructions of \\(\\mathbb{Z}_8\\)-bent functions employing permutations with the \\((\\mathcal{A}_m)\\) property, and more generally we show that the inverse permutation gives rise to \\(\\mathbb{Z}_{2^k}\\)-bent functions.","sentences":["In Pasalic et al., IEEE Trans.","Inform.","Theory 69 (2023), 2702--2712, and in Anbar, Meidl, Cryptogr.","Commun.","10 (2018), 235--249, two different vectorial negabent and vectorial bent-negabent concepts are introduced, which leads to seemingly contradictory results.","One of the main motivations for this article is to clarify the differences and similarities between these two concepts.","Moreover, the negabent concept is extended to generalized Boolean functions from \\(\\mathbb{F}_2^n\\) to the cyclic group \\(\\mathbb{Z}_{2^k}\\).","It is shown how to obtain nega-\\(\\mathbb{Z}_{2^k}\\)-bent functions from \\(\\mathbb{Z}_{2^k}\\)-bent functions, or equivalently, corresponding non-splitting relative difference sets from the splitting relative difference sets.","This generalizes the shifting results for Boolean bent and negabent functions.","We finally point to constructions of \\(\\mathbb{Z}_8\\)-bent functions employing permutations with the \\((\\mathcal{A}_m)\\) property, and more generally we show that the inverse permutation gives rise to \\(\\mathbb{Z}_{2^k}\\)-bent functions."],"url":"http://arxiv.org/abs/2402.05677v1","category":"math.CO"}
{"created":"2024-02-08 13:53:45","title":"Using nodal coordinates as variables for the dimensional synthesis of mechanisms","abstract":"The method of the lower deformation energy has been successfully used for the synthesis of mechanisms for quite a while. It has shown to be a versatile, yet powerful method for assisting in the design of mechanisms. Until now, most of the implementations of this method used the dimensions of the mechanism as the synthesis variables, which has some advantages and some drawbacks. For example, the assembly configuration is not taken into account in the optimization process, and this means that the same initial configuration is used when computing the deformed positions in each synthesis point. This translates into a reduction of the total search space. A possible solution to this problem is the use of a set of initial coordinates as variables for the synthesis, which has been successfully applied to other methods. This also has some additional advantages, such as the fact that any generated mechanism can be assembled. Another advantage is that the fixed joint locations are also included in the optimization at no additional cost. But the change from dimensions to initial coordinates means a reformulation of the optimization problem when using derivatives if one wants them to be analytically derived. This paper tackles this reformulation, along with a proper comparison of the use of both alternatives using sequential quadratic programming methods. In order to do so, some examples are developed and studied.","sentences":["The method of the lower deformation energy has been successfully used for the synthesis of mechanisms for quite a while.","It has shown to be a versatile, yet powerful method for assisting in the design of mechanisms.","Until now, most of the implementations of this method used the dimensions of the mechanism as the synthesis variables, which has some advantages and some drawbacks.","For example, the assembly configuration is not taken into account in the optimization process, and this means that the same initial configuration is used when computing the deformed positions in each synthesis point.","This translates into a reduction of the total search space.","A possible solution to this problem is the use of a set of initial coordinates as variables for the synthesis, which has been successfully applied to other methods.","This also has some additional advantages, such as the fact that any generated mechanism can be assembled.","Another advantage is that the fixed joint locations are also included in the optimization at no additional cost.","But the change from dimensions to initial coordinates means a reformulation of the optimization problem when using derivatives if one wants them to be analytically derived.","This paper tackles this reformulation, along with a proper comparison of the use of both alternatives using sequential quadratic programming methods.","In order to do so, some examples are developed and studied."],"url":"http://arxiv.org/abs/2402.05676v1","category":"math.DS"}
{"created":"2024-02-08 13:53:11","title":"Is Adversarial Training with Compressed Datasets Effective?","abstract":"Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The proposed method is (1) obtained by one-time computation and is applicable for any model, (2) more effective than DC methods when applying adversarial training over MFC, (3) provably robust by minimizing the generalized adversarial loss. Additionally, empirical evaluation on three datasets shows that the proposed method is able to achieve better robustness and performance trade-off compared to DC methods such as distribution matching.","sentences":["Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset.","This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset.","Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness.","In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets.","We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models.","As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset.","The proposed method is (1) obtained by one-time computation and is applicable for any model, (2) more effective than DC methods when applying adversarial training over MFC, (3) provably robust by minimizing the generalized adversarial loss.","Additionally, empirical evaluation on three datasets shows that the proposed method is able to achieve better robustness and performance trade-off compared to DC methods such as distribution matching."],"url":"http://arxiv.org/abs/2402.05675v1","category":"cs.LG"}
{"created":"2024-02-08 13:52:35","title":"A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs","abstract":"This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust features during training, identifying a uniform protection as an inherently effective defence mechanism.","sentences":["This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature.","Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses.","Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric.","In particular, we unveil the existence of directions which can be defended without penalising accuracy.","Finally, we show the advantage of defending non-robust features during training, identifying a uniform protection as an inherently effective defence mechanism."],"url":"http://arxiv.org/abs/2402.05674v1","category":"stat.ML"}
{"created":"2024-02-08 13:49:34","title":"Maximally entangled mixed states for a fixed spectrum do not always exist","abstract":"Entanglement is a resource under local operations assisted by classical communication (LOCC). Given a set of states $S$, if there is one state in $S$ that can be transformed by LOCC into all other states in $S$, then this state is maximally entangled in $S$. It is a well-known result that the $d$-dimensional Bell state is the maximally entangled state in the set of all bipartite states of local dimension $d$. Since in practical applications noise renders every state mixed, it is interesting to study whether sets of mixed states of relevance enable the notion of a maximally entangled state. A natural choice is the set of all states with the same spectrum. In fact, for any given spectrum distribution on two-qubit states, previous work has shown that several entanglement measures are all maximized by one particular state in this set. This has led to consider the possibility that this family of states could be the maximally entangled states in the set of all states with the same spectrum, which should then maximize \\emph{all} entanglement measures. In this work I answer this question in the negative: there are no maximally entangled states for a fixed spectrum in general, i.e. for every possible choice of the spectrum. In order to do so, I consider the case of rank-2 states and show that for particular values of the eigenvalues there exists no state that can be transformed to all other isospectral states not only under LOCC but also under the larger class of non-entangling operations. This in particular implies that in these cases the state that maximizes a given entanglement measure among all states with the same spectrum depends on the choice of entanglement measure, i.e. it cannot be that the aforementioned family of states maximizes all entanglement measures.","sentences":["Entanglement is a resource under local operations assisted by classical communication (LOCC).","Given a set of states $S$, if there is one state in $S$ that can be transformed by LOCC into all other states in $S$, then this state is maximally entangled in $S$. It is a well-known result that the $d$-dimensional Bell state is the maximally entangled state in the set of all bipartite states of local dimension $d$. Since in practical applications noise renders every state mixed, it is interesting to study whether sets of mixed states of relevance enable the notion of a maximally entangled state.","A natural choice is the set of all states with the same spectrum.","In fact, for any given spectrum distribution on two-qubit states, previous work has shown that several entanglement measures are all maximized by one particular state in this set.","This has led to consider the possibility that this family of states could be the maximally entangled states in the set of all states with the same spectrum, which should then maximize \\emph{all} entanglement measures.","In this work I answer this question in the negative: there are no maximally entangled states for a fixed spectrum in general, i.e. for every possible choice of the spectrum.","In order to do so, I consider the case of rank-2 states and show that for particular values of the eigenvalues there exists no state that can be transformed to all other isospectral states not only under LOCC but also under the larger class of non-entangling operations.","This in particular implies that in these cases the state that maximizes a given entanglement measure among all states with the same spectrum depends on the choice of entanglement measure, i.e. it cannot be that the aforementioned family of states maximizes all entanglement measures."],"url":"http://arxiv.org/abs/2402.05673v1","category":"quant-ph"}
{"created":"2024-02-08 13:42:50","title":"Comprehensive Assessment of Jailbreak Attacks Against LLMs","abstract":"Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhibit robustness across different LLMs. Some jailbreak prompt datasets, available from the Internet, can also achieve high attack success rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2. Despite the claims from many organizations regarding the coverage of violation categories in their policies, the attack success rates from these categories remain high, indicating the challenges of effectively aligning LLM policies and the ability to counter jailbreak attacks. We also discuss the trade-off between the attack performance and efficiency, as well as show that the transferability of the jailbreak prompts is still viable, becoming an option for black-box models. Overall, our research highlights the necessity of evaluating different jailbreak methods. We hope our study can provide insights for future research on jailbreak attacks and serve as a benchmark tool for evaluating them for practitioners.","sentences":["Misuse of the Large Language Models (LLMs) has raised widespread concern.","To address this issue, safeguards have been taken to ensure that LLMs align with social ethics.","However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks.","By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response.","While researchers have studied several categories of jailbreak attacks, they have done so in isolation.","To fill this gap, we present the first large-scale measurement of various jailbreak attack methods.","We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs.","Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhibit robustness across different LLMs.","Some jailbreak prompt datasets, available from the Internet, can also achieve high attack success rates on many LLMs, such as ChatGLM3, GPT-3.5, and PaLM2.","Despite the claims from many organizations regarding the coverage of violation categories in their policies, the attack success rates from these categories remain high, indicating the challenges of effectively aligning LLM policies and the ability to counter jailbreak attacks.","We also discuss the trade-off between the attack performance and efficiency, as well as show that the transferability of the jailbreak prompts is still viable, becoming an option for black-box models.","Overall, our research highlights the necessity of evaluating different jailbreak methods.","We hope our study can provide insights for future research on jailbreak attacks and serve as a benchmark tool for evaluating them for practitioners."],"url":"http://arxiv.org/abs/2402.05668v1","category":"cs.CR"}
{"created":"2024-02-08 13:29:34","title":"A Family of Quantile Dependence Coefficients","abstract":"A popular measure of association is the tail dependence coefficient which measures the strength of dependence in either the lower-left or upper-right tail of a bivariate distribution. In this paper, we develop the idea of quantile dependence, which generalizes the notion of tail dependence and could be used to detect dependence in specific regions of the domain of a joint distribution function. Properties of the proposed quantile dependence coefficient are studied and several examples illustrate our results.","sentences":["A popular measure of association is the tail dependence coefficient which measures the strength of dependence in either the lower-left or upper-right tail of a bivariate distribution.","In this paper, we develop the idea of quantile dependence, which generalizes the notion of tail dependence and could be used to detect dependence in specific regions of the domain of a joint distribution function.","Properties of the proposed quantile dependence coefficient are studied and several examples illustrate our results."],"url":"http://arxiv.org/abs/2402.05665v1","category":"math.ST"}
{"created":"2024-02-08 13:27:10","title":"Mesoscale Traffic Forecasting for Real-Time Bottleneck and Shockwave Prediction","abstract":"Accurate real-time traffic state forecasting plays a pivotal role in traffic control research. In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays. After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment. In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting. We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time.","sentences":["Accurate real-time traffic state forecasting plays a pivotal role in traffic control research.","In particular, the CIRCLES consortium project necessitates predictive techniques to mitigate the impact of data source delays.","After the success of the MegaVanderTest experiment, this paper aims at overcoming the current system limitations and develop a more suited approach to improve the real-time traffic state estimation for the next iterations of the experiment.","In this paper, we introduce the SA-LSTM, a deep forecasting method integrating Self-Attention (SA) on the spatial dimension with Long Short-Term Memory (LSTM) yielding state-of-the-art results in real-time mesoscale traffic forecasting.","We extend this approach to multi-step forecasting with the n-step SA-LSTM, which outperforms traditional multi-step forecasting methods in the trade-off between short-term and long-term predictions, all while operating in real-time."],"url":"http://arxiv.org/abs/2402.05663v1","category":"cs.LG"}
{"created":"2024-02-08 13:27:01","title":"Stochastic COLREGs Evaluation for Safe Navigation under Uncertainty","abstract":"The encounter situation between marine vessels determines how they should navigate to obey COLREGs, but time-varying and stochastic uncertainty in estimation of angles of encounter, and of closest point of approach, easily give rise to different assessment of situation at two approaching vessels. This may lead to high-risk conditions and could cause collision. This article considers decision making under uncertainty and suggests a novel method for probabilistic interpretation of vessel encounters that is explainable and provides a measure of uncertainty in the evaluation. The method is equally useful for decision support on a manned bridge as on Marine Autonomous Surface Ships (MASS) where it provides input for automated navigation. The method makes formal safety assessment and validation feasible. We obtain a resilient algorithm for machine interpretation of COLREGs under uncertainty and show its efficacy by simulations.","sentences":["The encounter situation between marine vessels determines how they should navigate to obey COLREGs, but time-varying and stochastic uncertainty in estimation of angles of encounter, and of closest point of approach, easily give rise to different assessment of situation at two approaching vessels.","This may lead to high-risk conditions and could cause collision.","This article considers decision making under uncertainty and suggests a novel method for probabilistic interpretation of vessel encounters that is explainable and provides a measure of uncertainty in the evaluation.","The method is equally useful for decision support on a manned bridge as on Marine Autonomous Surface Ships (MASS) where it provides input for automated navigation.","The method makes formal safety assessment and validation feasible.","We obtain a resilient algorithm for machine interpretation of COLREGs under uncertainty and show its efficacy by simulations."],"url":"http://arxiv.org/abs/2402.05662v1","category":"eess.SY"}
{"created":"2024-02-08 13:26:13","title":"Quantum ontology de-naturalized: What we can't learn from quantum mechanics","abstract":"Philosophers of science commonly connect ontology and science, stating that these disciplines maintain a two-way relationship: on the one hand, we can extract ontology from scientific theories; on the other hand, ontology provides the realistic content of our scientific theories. In this article, we will critically examine the process of naturalizing ontology, i.e., confining the work of ontologists merely to the task of pointing out which entities certain theories commit themselves to. We will use non-relativistic quantum mechanics as a case study. We begin by distinguishing two roles for ontology: the first would be characterized by cataloging existing entities according to quantum mechanics; the second would be characterized by establishing more general ontological categories in which existing entities must be classified. We argue that only the first step is available for a naturalistic approach; the second step not being open for determination or anchoring in science. Finally, we also argue that metaphysics is still a step beyond ontology, not contained in either of the two tasks of ontology, being thus even farther from science.","sentences":["Philosophers of science commonly connect ontology and science, stating that these disciplines maintain a two-way relationship: on the one hand, we can extract ontology from scientific theories; on the other hand, ontology provides the realistic content of our scientific theories.","In this article, we will critically examine the process of naturalizing ontology, i.e., confining the work of ontologists merely to the task of pointing out which entities certain theories commit themselves to.","We will use non-relativistic quantum mechanics as a case study.","We begin by distinguishing two roles for ontology: the first would be characterized by cataloging existing entities according to quantum mechanics; the second would be characterized by establishing more general ontological categories in which existing entities must be classified.","We argue that only the first step is available for a naturalistic approach; the second step not being open for determination or anchoring in science.","Finally, we also argue that metaphysics is still a step beyond ontology, not contained in either of the two tasks of ontology, being thus even farther from science."],"url":"http://arxiv.org/abs/2402.05661v1","category":"physics.hist-ph"}
{"created":"2024-02-08 13:24:57","title":"Rethinking Propagation for Unsupervised Graph Domain Adaptation","abstract":"Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains. Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs). However, the inherent generalization capability of GNNs has been largely overlooked. Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains. We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs. By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph. Based on the empirical and theoretical analysis mentioned above, we propose a simple yet effective approach called A2GNN for graph domain adaptation. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed A2GNN framework.","sentences":["Unsupervised Graph Domain Adaptation (UGDA) aims to transfer knowledge from a labelled source graph to an unlabelled target graph in order to address the distribution shifts between graph domains.","Previous works have primarily focused on aligning data from the source and target graph in the representation space learned by graph neural networks (GNNs).","However, the inherent generalization capability of GNNs has been largely overlooked.","Motivated by our empirical analysis, we reevaluate the role of GNNs in graph domain adaptation and uncover the pivotal role of the propagation process in GNNs for adapting to different graph domains.","We provide a comprehensive theoretical analysis of UGDA and derive a generalization bound for multi-layer GNNs.","By formulating GNN Lipschitz for k-layer GNNs, we show that the target risk bound can be tighter by removing propagation layers in source graph and stacking multiple propagation layers in target graph.","Based on the empirical and theoretical analysis mentioned above, we propose a simple yet effective approach called A2GNN for graph domain adaptation.","Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our proposed A2GNN framework."],"url":"http://arxiv.org/abs/2402.05660v1","category":"cs.LG"}
{"created":"2024-02-08 13:22:55","title":"A Mechanical Origin of Cooperative Transport","abstract":"Cooperative transport is a striking phenomenon where multiple agents join forces to transit a payload too heavy for the individual. While social animals such as ants are routinely observed to coordinate transport at scale, reproducing the effect in artificial swarms remains challenging, as it requires synchronization in a noisy many-body system. Here we show that cooperative transport spontaneously emerges in swarms of stochastic self-propelled agents, without requiring any form of sensing, feedback, or control. We find that a minute modification to the mechanical design of the individual agent dramatically changes its alignment response to an external force. We then show experimentally that with the proper design, a swarm of active particles spontaneously cooperates in the directional transport of larger objects. Surprisingly, transport increases with increasing payload size. A mechanical, coarse-grained description reveals that force-alignment is intrinsic and captured by a signed, charge-like parameter with units of curvature. Numerical simulations of swarms of active particles with a negative active charge corroborate the experimental findings. We analytically derive a geometrical criterion for cooperative transport which results from a bifurcation in a non-linear dynamical system. Our findings generalize existing models of active particles, offer new design rules for distributed robotic systems, and shed light on cooperation in natural swarms.","sentences":["Cooperative transport is a striking phenomenon where multiple agents join forces to transit a payload too heavy for the individual.","While social animals such as ants are routinely observed to coordinate transport at scale, reproducing the effect in artificial swarms remains challenging, as it requires synchronization in a noisy many-body system.","Here we show that cooperative transport spontaneously emerges in swarms of stochastic self-propelled agents, without requiring any form of sensing, feedback, or control.","We find that a minute modification to the mechanical design of the individual agent dramatically changes its alignment response to an external force.","We then show experimentally that with the proper design, a swarm of active particles spontaneously cooperates in the directional transport of larger objects.","Surprisingly, transport increases with increasing payload size.","A mechanical, coarse-grained description reveals that force-alignment is intrinsic and captured by a signed, charge-like parameter with units of curvature.","Numerical simulations of swarms of active particles with a negative active charge corroborate the experimental findings.","We analytically derive a geometrical criterion for cooperative transport which results from a bifurcation in a non-linear dynamical system.","Our findings generalize existing models of active particles, offer new design rules for distributed robotic systems, and shed light on cooperation in natural swarms."],"url":"http://arxiv.org/abs/2402.05659v1","category":"cond-mat.soft"}
{"created":"2024-02-08 13:21:43","title":"Discovery and characterisation of a new Galactic Planetary Nebula","abstract":"Planetary nebulae are one of the final stages in the evolution of low and intermediate mass stars. They occur in a variety of shapes. Older and fainter ones are generally more difficult to identify because of the lower surface brightness. This paper reports the serendipitous discovery of a new faint Galactic planetary nebula (PN), during a campaign to identify dwarf galaxies, companions of the spiral galaxy NGC 2403. We aim at confirming the nature as PN of a diffuse object identified in the Camelopardalis constellation. We obtained narrow-band filter images and spectra of the nebula and its central star with amateur and professional telescopes having diameters from 20 cm to 6 m. We detected a dense triangular nebula, surrounded by an elliptical region, named Cam nebula. They are part of a larger and fainter circular nebular structure, named TBG-1, at the centre of which we have identified the possible central star, a white dwarf with a temperature of about 22 000 K. The analysis of the spectrum made it possible to measure the physical characteristics of the nebula, in particular its electronic density and temperature. Analysis of the images, of the spectra of the nebula and of the central star confirm the PN nature of TBG-1, located at the distance of about 1 kpc. This work reaffirms the potential for fruitful collaborations between astronomers and amateur astronomers in the detection and study of new objects.","sentences":["Planetary nebulae are one of the final stages in the evolution of low and intermediate mass stars.","They occur in a variety of shapes.","Older and fainter ones are generally more difficult to identify because of the lower surface brightness.","This paper reports the serendipitous discovery of a new faint Galactic planetary nebula (PN), during a campaign to identify dwarf galaxies, companions of the spiral galaxy NGC 2403.","We aim at confirming the nature as PN of a diffuse object identified in the Camelopardalis constellation.","We obtained narrow-band filter images and spectra of the nebula and its central star with amateur and professional telescopes having diameters from 20 cm to 6 m. We detected a dense triangular nebula, surrounded by an elliptical region, named Cam nebula.","They are part of a larger and fainter circular nebular structure, named TBG-1, at the centre of which we have identified the possible central star, a white dwarf with a temperature of about 22 000","K. The analysis of the spectrum made it possible to measure the physical characteristics of the nebula, in particular its electronic density and temperature.","Analysis of the images, of the spectra of the nebula and of the central star confirm the PN nature of TBG-1, located at the distance of about 1 kpc.","This work reaffirms the potential for fruitful collaborations between astronomers and amateur astronomers in the detection and study of new objects."],"url":"http://arxiv.org/abs/2402.05658v1","category":"astro-ph.SR"}
{"created":"2024-02-08 13:21:26","title":"q-Parikh Matrices and q-deformed binomial coefficients of words","abstract":"We have introduced a q-deformation, i.e., a polynomial in q with natural coefficients, of the binomial coefficient of two finite words u and v counting the number of occurrences of v as a subword of u. In this paper, we examine the q-deformation of Parikh matrices as introduced by E\\u{g}ecio\\u{g}lu in 2004.   Many classical results concerning Parikh matrices generalize to this new framework: Our first important observation is that the elements of such a matrix are in fact q-deformations of binomial coefficients of words. We also study their inverses and as an application, we obtain new identities about q-binomials.   For a finite word z and for the sequence $(p_n)_{n\\ge 0}$ of prefixes of an infinite word, we show that the polynomial sequence $\\binom{p_n}{z}_q$ converges to a formal series. We present links with additive number theory and k-regular sequences. In the case of a periodic word $u^\\omega$, we generalize a result of Salomaa: the sequence $\\binom{u^n}{z}_q$ satisfies a linear recurrence relation with polynomial coefficients. Related to the theory of integer partition, we describe the growth and the zero set of the coefficients of the series associated with $u^\\omega$.   Finally, we show that the minors of a q-Parikh matrix are polynomials with natural coefficients and consider a generalization of Cauchy's inequality. We also compare q-Parikh matrices associated with an arbitrary word with those associated with a canonical word $12\\cdots k$ made of pairwise distinct symbols.","sentences":["We have introduced a q-deformation, i.e., a polynomial in q with natural coefficients, of the binomial coefficient of two finite words u and v counting the number of occurrences of v as a subword of u. In this paper, we examine the q-deformation of Parikh matrices as introduced by E\\u{g}ecio\\u{g}lu in 2004.   ","Many classical results concerning Parikh matrices generalize to this new framework: Our first important observation is that the elements of such a matrix are in fact q-deformations of binomial coefficients of words.","We also study their inverses and as an application, we obtain new identities about q-binomials.   ","For a finite word z and for the sequence $(p_n)_{n\\ge 0}$ of prefixes of an infinite word, we show that the polynomial sequence $\\binom{p_n}{z}_q$ converges to a formal series.","We present links with additive number theory and k-regular sequences.","In the case of a periodic word $u^\\omega$, we generalize a result of Salomaa: the sequence $\\binom{u^n}{z}_q$ satisfies a linear recurrence relation with polynomial coefficients.","Related to the theory of integer partition, we describe the growth and the zero set of the coefficients of the series associated with $u^\\omega$.   Finally, we show that the minors of a q-Parikh matrix are polynomials with natural coefficients and consider a generalization of Cauchy's inequality.","We also compare q-Parikh matrices associated with an arbitrary word with those associated with a canonical word $12\\cdots k$ made of pairwise distinct symbols."],"url":"http://arxiv.org/abs/2402.05657v1","category":"cs.FL"}
{"created":"2024-02-08 13:07:31","title":"Rocks Coding, Not Development--A Human-Centric, Experimental Evaluation of LLM-Supported SE Tasks","abstract":"Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT. Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers. Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks. In a controlled 2 $\\times$ 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT. We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good. We also observed the interactions between participants and ChatGPT and found the relations between the interactions and the outcomes. Our study thus provides first-hand insights into using ChatGPT to fulfill software engineering tasks with real-world developers and motivates the need for novel interaction mechanisms that help developers effectively work with large language models to achieve desired outcomes.","sentences":["Recently, large language models (LLM) based generative AI has been gaining momentum for their impressive high-quality performances in multiple domains, particularly after the release of the ChatGPT.","Many believe that they have the potential to perform general-purpose problem-solving in software development and replace human software developers.","Nevertheless, there are in a lack of serious investigation into the capability of these LLM techniques in fulfilling software development tasks.","In a controlled 2 $\\times$ 2 between-subject experiment with 109 participants, we examined whether and to what degree working with ChatGPT was helpful in the coding task and typical software development task and how people work with ChatGPT.","We found that while ChatGPT performed well in solving simple coding problems, its performance in supporting typical software development tasks was not that good.","We also observed the interactions between participants and ChatGPT and found the relations between the interactions and the outcomes.","Our study thus provides first-hand insights into using ChatGPT to fulfill software engineering tasks with real-world developers and motivates the need for novel interaction mechanisms that help developers effectively work with large language models to achieve desired outcomes."],"url":"http://arxiv.org/abs/2402.05650v1","category":"cs.SE"}
{"created":"2024-02-08 13:02:38","title":"A reverse isoperimetric inequality for convex shapes with inclusion constraint","abstract":"The convex shape contained in a disk having prescribed area and maximal perimeter is completely characterized in terms of the area fraction. The solution is always a polygon having all but one sides equal. The lengths of the sides are characterized through explicit equations. The case of more general containing shapes is also discussed from both theoretical and numerical perspectives.","sentences":["The convex shape contained in a disk having prescribed area and maximal perimeter is completely characterized in terms of the area fraction.","The solution is always a polygon having all but one sides equal.","The lengths of the sides are characterized through explicit equations.","The case of more general containing shapes is also discussed from both theoretical and numerical perspectives."],"url":"http://arxiv.org/abs/2402.05648v1","category":"math.MG"}
{"created":"2024-02-08 12:59:47","title":"FuncGrasp: Learning Object-Centric Neural Grasp Functions from Single Annotated Example Object","abstract":"We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors. Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances. To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations. Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge. We showcase the effectiveness through extensive experiments in both simulators and the real world. Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps.","sentences":["We present FuncGrasp, a framework that can infer dense yet reliable grasp configurations for unseen objects using one annotated object and single-view RGB-D observation via categorical priors.","Unlike previous works that only transfer a set of grasp poses, FuncGrasp aims to transfer infinite configurations parameterized by an object-centric continuous grasp function across varying instances.","To ease the transfer process, we propose Neural Surface Grasping Fields (NSGF), an effective neural representation defined on the surface to densely encode grasp configurations.","Further, we exploit function-to-function transfer using sphere primitives to establish semantically meaningful categorical correspondences, which are learned in an unsupervised fashion without any expert knowledge.","We showcase the effectiveness through extensive experiments in both simulators and the real world.","Remarkably, our framework significantly outperforms several strong baseline methods in terms of density and reliability for generated grasps."],"url":"http://arxiv.org/abs/2402.05644v1","category":"cs.RO"}
{"created":"2024-02-08 12:58:07","title":"Improving Token-Based World Models with Parallel Observation Prediction","abstract":"Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods. In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence. However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations. To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism. POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting. We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs. REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours. Our code is available at \\url{https://github.com/leor-c/REM}.","sentences":["Motivated by the success of Transformers when applied to sequences of discrete symbols, token-based world models (TBWMs) were recently proposed as sample-efficient methods.","In TBWMs, the world model consumes agent experience as a language-like sequence of tokens, where each observation constitutes a sub-sequence.","However, during imagination, the sequential token-by-token generation of next observations results in a severe bottleneck, leading to long training times, poor GPU utilization, and limited representations.","To resolve this bottleneck, we devise a novel Parallel Observation Prediction (POP) mechanism.","POP augments a Retentive Network (RetNet) with a novel forward mode tailored to our reinforcement learning setting.","We incorporate POP in a novel TBWM agent named REM (Retentive Environment Model), showcasing a 15.4x faster imagination compared to prior TBWMs.","REM attains superhuman performance on 12 out of 26 games of the Atari 100K benchmark, while training in less than 12 hours.","Our code is available at \\url{https://github.com/leor-c/REM}."],"url":"http://arxiv.org/abs/2402.05643v1","category":"cs.LG"}
{"created":"2024-02-08 12:56:26","title":"An Optimization-based Baseline for Rigid 2D/3D Registration Applied to Spine Surgical Navigation Using CMA-ES","abstract":"A robust and efficient optimization-based 2D/3D registration framework is crucial for the navigation system of orthopedic surgical robots. It can provide precise position information of surgical instruments and implants during surgery. While artificial intelligence technology has advanced rapidly in recent years, traditional optimization-based registration methods remain indispensable in the field of 2D/3D registration.he exceptional precision of this method enables it to be considered as a post-processing step of the learning-based methods, thereby offering a reliable assurance for registration. In this paper, we present a coarse-to-fine registration framework based on the CMA-ES algorithm. We conducted intensive testing of our method using data from different parts of the spine. The results shows the effectiveness of the proposed framework on real orthopedic spine surgery clinical data. This work can be viewed as an additional extension that complements the optimization-based methods employed in our previous studies.","sentences":["A robust and efficient optimization-based 2D/3D registration framework is crucial for the navigation system of orthopedic surgical robots.","It can provide precise position information of surgical instruments and implants during surgery.","While artificial intelligence technology has advanced rapidly in recent years, traditional optimization-based registration methods remain indispensable in the field of 2D/3D registration.he exceptional precision of this method enables it to be considered as a post-processing step of the learning-based methods, thereby offering a reliable assurance for registration.","In this paper, we present a coarse-to-fine registration framework based on the CMA-ES algorithm.","We conducted intensive testing of our method using data from different parts of the spine.","The results shows the effectiveness of the proposed framework on real orthopedic spine surgery clinical data.","This work can be viewed as an additional extension that complements the optimization-based methods employed in our previous studies."],"url":"http://arxiv.org/abs/2402.05642v1","category":"eess.IV"}
{"created":"2024-02-08 12:50:09","title":"Internal Maps With Dense Periodicity","abstract":"We consider the class of interval maps with dense set of periodic points CP and its closure CP equipped with the metric of uniform convergence. Besides studying basic topological properties and density results in the spaces CP and CP we prove that CP is dynamically characterized as the set of interval maps for which every point is chain-recurrent. Furthermore, we prove that a strong topological expansion property called topological exactness (or leo property) is attained on the open dense set of maps in CP and on a residual set in CP. Moreover, we show that every second category set in CP and CP is rich in a sense that it contains uncountably many conjugacy classes. An analogous conclusion also holds in the setting of interval maps preserving any fixed non-atomic probability measure with full support. Finally, we give a detailed description of the structure of periodic points of generic maps in CP and CP and show that generic maps in CP and CP satisfy the shadowing property.","sentences":["We consider the class of interval maps with dense set of periodic points CP and its closure CP equipped with the metric of uniform convergence.","Besides studying basic topological properties and density results in the spaces CP and CP we prove that CP is dynamically characterized as the set of interval maps for which every point is chain-recurrent.","Furthermore, we prove that a strong topological expansion property called topological exactness (or leo property) is attained on the open dense set of maps in CP and on a residual set in CP.","Moreover, we show that every second category set in CP and CP is rich in a sense that it contains uncountably many conjugacy classes.","An analogous conclusion also holds in the setting of interval maps preserving any fixed non-atomic probability measure with full support.","Finally, we give a detailed description of the structure of periodic points of generic maps in CP and CP and show that generic maps in CP and CP satisfy the shadowing property."],"url":"http://arxiv.org/abs/2402.05638v1","category":"math.DS"}
{"created":"2024-02-08 12:47:57","title":"The Impact of AI Tool on Engineering at ANZ Bank An Emperical Study on GitHub Copilot within Coporate Environment","abstract":"The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering. This study explores the integration of AI tools in software engineering practices within a large organization. We focus on ANZ Bank, which employs over 5000 engineers covering all aspects of the software development life cycle. This paper details an experiment conducted using GitHub Copilot, a notable AI tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks. Additionally, this paper shares initial findings on the productivity improvements observed after GitHub Copilot was adopted on a large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment with GitHub Copilot included two weeks of preparation and four weeks of active testing. The study evaluated participant sentiment and the tool's impact on productivity, code quality, and security. Initially, participants used GitHub Copilot for proposed use-cases, with their feedback gathered through regular surveys. In the second phase, they were divided into Control and Copilot groups, each tackling the same Python challenges, and their experiences were again surveyed. Results showed a notable boost in productivity and code quality with GitHub Copilot, though its impact on code security remained inconclusive. Participant responses were overall positive, confirming GitHub Copilot's effectiveness in large-scale software engineering environments. Early data from 1000 engineers also indicated a significant increase in productivity and job satisfaction.","sentences":["The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering.","This study explores the integration of AI tools in software engineering practices within a large organization.","We focus on ANZ Bank, which employs over 5000 engineers covering all aspects of the software development life cycle.","This paper details an experiment conducted using GitHub Copilot, a notable AI tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks.","Additionally, this paper shares initial findings on the productivity improvements observed after GitHub Copilot was adopted on a large scale, with about 1000 engineers using it.","ANZ Bank's six-week experiment with GitHub Copilot included two weeks of preparation and four weeks of active testing.","The study evaluated participant sentiment and the tool's impact on productivity, code quality, and security.","Initially, participants used GitHub Copilot for proposed use-cases, with their feedback gathered through regular surveys.","In the second phase, they were divided into Control and Copilot groups, each tackling the same Python challenges, and their experiences were again surveyed.","Results showed a notable boost in productivity and code quality with GitHub Copilot, though its impact on code security remained inconclusive.","Participant responses were overall positive, confirming GitHub Copilot's effectiveness in large-scale software engineering environments.","Early data from 1000 engineers also indicated a significant increase in productivity and job satisfaction."],"url":"http://arxiv.org/abs/2402.05636v1","category":"cs.SE"}
{"created":"2024-02-08 12:36:29","title":"Merging Facts, Crafting Fallacies: Evaluating the Contradictory Nature of Aggregated Factual Claims in Long-Form Generations","abstract":"Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult. To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently. The factuality of the generation is the proportion of verifiable facts among all the facts. Such methods assume that combining factual claims forms a factual paragraph. This paper shows that the assumption can be violated due to entity ambiguity. We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity. We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs. To address this, we introduce an enhanced metric, D-FActScore, specifically designed for content with ambiguous entities. We evaluate the D-FActScores of people biographies generated with retrieval-augmented generation (RAG). We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore. We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs.","sentences":["Long-form generations from large language models (LLMs) contains a mix of factual and non-factual claims, making evaluating factuality difficult.","To evaluate factual precision of long-form generations in a more fine-grained way, prior works propose to decompose long-form generations into multiple verifiable facts and verify those facts independently.","The factuality of the generation is the proportion of verifiable facts among all the facts.","Such methods assume that combining factual claims forms a factual paragraph.","This paper shows that the assumption can be violated due to entity ambiguity.","We show that LLMs can generate paragraphs that contain verifiable facts, but the facts are combined to form a non-factual paragraph due to entity ambiguity.","We further reveal that existing factual precision metrics, including FActScore and citation recall, cannot properly evaluate the factuality of these non-factual paragraphs.","To address this, we introduce an enhanced metric, D-FActScore, specifically designed for content with ambiguous entities.","We evaluate the D-FActScores of people biographies generated with retrieval-augmented generation (RAG).","We show that D-FActScore can better assess the factuality of paragraphs with entity ambiguity than FActScore.","We also find that four widely used open-source LLMs tend to mix information of distinct entities to form non-factual paragraphs."],"url":"http://arxiv.org/abs/2402.05629v1","category":"cs.CL"}
{"created":"2024-02-08 12:31:08","title":"Binding Dynamics in Rotating Features","abstract":"In human cognition, the binding problem describes the open question of how the brain flexibly integrates diverse information into cohesive object representations. Analogously, in machine learning, there is a pursuit for models capable of strong generalization and reasoning by learning object-centric representations in an unsupervised manner. Drawing from neuroscientific theories, Rotating Features learn such representations by introducing vector-valued features that encapsulate object characteristics in their magnitudes and object affiliation in their orientations. The \"$\\chi$-binding\" mechanism, embedded in every layer of the architecture, has been shown to be crucial, but remains poorly understood. In this paper, we propose an alternative \"cosine binding\" mechanism, which explicitly computes the alignment between features and adjusts weights accordingly, and we show that it achieves equivalent performance. This allows us to draw direct connections to self-attention and biological neural processes, and to shed light on the fundamental dynamics for object-centric representations to emerge in Rotating Features.","sentences":["In human cognition, the binding problem describes the open question of how the brain flexibly integrates diverse information into cohesive object representations.","Analogously, in machine learning, there is a pursuit for models capable of strong generalization and reasoning by learning object-centric representations in an unsupervised manner.","Drawing from neuroscientific theories, Rotating Features learn such representations by introducing vector-valued features that encapsulate object characteristics in their magnitudes and object affiliation in their orientations.","The \"$\\chi$-binding\" mechanism, embedded in every layer of the architecture, has been shown to be crucial, but remains poorly understood.","In this paper, we propose an alternative \"cosine binding\" mechanism, which explicitly computes the alignment between features and adjusts weights accordingly, and we show that it achieves equivalent performance.","This allows us to draw direct connections to self-attention and biological neural processes, and to shed light on the fundamental dynamics for object-centric representations to emerge in Rotating Features."],"url":"http://arxiv.org/abs/2402.05627v1","category":"cs.LG"}
{"created":"2024-02-08 12:28:18","title":"Efficient Models for the Detection of Hate, Abuse and Profanity","abstract":"Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web. This data is prone to having content with Hate, Abuse and Profanity (HAP). For a detailed definition of HAP, please refer to the Appendix. Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content. For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score. This is unacceptable in civil discourse.The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only for English, but for all languages. In this article, we briefly describe the creation of HAP detectors and various ways of using them to make models civil and acceptable in the output they generate.","sentences":["Large Language Models (LLMs) are the cornerstone for many Natural Language Processing (NLP) tasks like sentiment analysis, document classification, named entity recognition, question answering, summarization, etc. LLMs are often trained on data which originates from the web.","This data is prone to having content with Hate, Abuse and Profanity (HAP).","For a detailed definition of HAP, please refer to the Appendix.","Due to the LLMs being exposed to HAP content during training, the models learn it and may then generate hateful or profane content.","For example, when the open-source RoBERTa model (specifically, the RoBERTA base model) from the HuggingFace (HF) Transformers library is prompted to replace the mask token in `I do not know that Persian people are that MASK` it returns the word `stupid` with the highest score.","This is unacceptable in civil discourse.","The detection of Hate, Abuse and Profanity in text is a vital component of creating civil and unbiased LLMs, which is needed not only for English, but for all languages.","In this article, we briefly describe the creation of HAP detectors and various ways of using them to make models civil and acceptable in the output they generate."],"url":"http://arxiv.org/abs/2402.05624v1","category":"cs.CL"}
{"created":"2024-02-08 12:27:23","title":"Cosmelkology: Elko fermions in FLRW space-time","abstract":"Cosmelkology is the study of Elko in cosmology. Elko is a massive spin-half field of mass dimension one. Elko differs from the Dirac and Majorana fermions because it furnishes the irreducible representation of the extended Poincare group with a two-fold Wigner degeneracy where the particle and anti-particle states both have four degrees of freedom. We study Elko in the spatially flat FLRW space-time and find exact solutions in the de Sitter space. By choosing the appropriate solutions and phases, the fields satisfy the canonical anti-commutation relations and have the correct time evolutions in the flat space limit.","sentences":["Cosmelkology is the study of Elko in cosmology.","Elko is a massive spin-half field of mass dimension one.","Elko differs from the Dirac and Majorana fermions because it furnishes the irreducible representation of the extended Poincare group with a two-fold Wigner degeneracy where the particle and anti-particle states both have four degrees of freedom.","We study Elko in the spatially flat FLRW space-time and find exact solutions in the de Sitter space.","By choosing the appropriate solutions and phases, the fields satisfy the canonical anti-commutation relations and have the correct time evolutions in the flat space limit."],"url":"http://arxiv.org/abs/2402.05623v1","category":"hep-th"}
{"created":"2024-02-08 12:23:22","title":"Linking Vision and Multi-Agent Communication through Visible Light Communication using Event Cameras","abstract":"Various robots, rovers, drones, and other agents of mass-produced products are expected to encounter scenes where they intersect and collaborate in the near future. In such multi-agent systems, individual identification and communication play crucial roles. In this paper, we explore camera-based visible light communication using event cameras to tackle this problem. An event camera captures the events occurring in regions with changes in brightness and can be utilized as a receiver for visible light communication, leveraging its high temporal resolution. Generally, agents with identical appearances in mass-produced products are visually indistinguishable when using conventional CMOS cameras. Therefore, linking visual information with information acquired through conventional radio communication is challenging. We empirically demonstrate the advantages of a visible light communication system employing event cameras and LEDs for visual individual identification over conventional CMOS cameras with ArUco marker recognition. In the simulation, we also verified scenarios where our event camera-based visible light communication outperforms conventional radio communication in situations with visually indistinguishable multi-agents. Finally, our newly implemented multi-agent system verifies its functionality through physical robot experiments.","sentences":["Various robots, rovers, drones, and other agents of mass-produced products are expected to encounter scenes where they intersect and collaborate in the near future.","In such multi-agent systems, individual identification and communication play crucial roles.","In this paper, we explore camera-based visible light communication using event cameras to tackle this problem.","An event camera captures the events occurring in regions with changes in brightness and can be utilized as a receiver for visible light communication, leveraging its high temporal resolution.","Generally, agents with identical appearances in mass-produced products are visually indistinguishable when using conventional CMOS cameras.","Therefore, linking visual information with information acquired through conventional radio communication is challenging.","We empirically demonstrate the advantages of a visible light communication system employing event cameras and LEDs for visual individual identification over conventional CMOS cameras with ArUco marker recognition.","In the simulation, we also verified scenarios where our event camera-based visible light communication outperforms conventional radio communication in situations with visually indistinguishable multi-agents.","Finally, our newly implemented multi-agent system verifies its functionality through physical robot experiments."],"url":"http://arxiv.org/abs/2402.05619v1","category":"cs.MA"}
{"created":"2024-02-08 12:19:32","title":"Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks","abstract":"We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks. Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch. Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing. We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks. We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success.","sentences":["We propose that small pretrained foundational generative language models with millions of parameters can be utilized as a general learning framework for sequence-based tasks.","Our proposal overcomes the computational resource, skill set, and timeline challenges associated with training neural networks and language models from scratch.","Further, our approach focuses on creating small and highly specialized models that can accurately execute a challenging task of which the base model is incapable of performing.","We demonstrate that 125M, 350M, and 1.3B parameter pretrained foundational language models can be instruction fine-tuned with 10,000-to-1,000,000 instruction examples to achieve near state-of-the-art results on challenging cheminformatics tasks.","We also demonstrate the role of successive language model fine-tuning epochs on improved outcomes, as well as the importance of both data formatting and pretrained foundational language model selection for instruction fine-tuning success."],"url":"http://arxiv.org/abs/2402.05616v1","category":"cs.CL"}
{"created":"2024-02-08 12:18:39","title":"DAPlankton: Benchmark Dataset for Multi-instrument Plankton Recognition via Fine-grained Domain Adaptation","abstract":"Plankton recognition provides novel possibilities to study various environmental aspects and an interesting real-world context to develop domain adaptation (DA) methods. Different imaging instruments cause domain shift between datasets hampering the development of general plankton recognition methods. A promising remedy for this is DA allowing to adapt a model trained on one instrument to other instruments. In this paper, we present a new DA dataset called DAPlankton which consists of phytoplankton images obtained with different instruments. Phytoplankton provides a challenging DA problem due to the fine-grained nature of the task and high class imbalance in real-world datasets. DAPlankton consists of two subsets. DAPlankton_LAB contains images of cultured phytoplankton providing a balanced dataset with minimal label uncertainty. DAPlankton_SEA consists of images collected from the Baltic Sea providing challenging real-world data with large intra-class variance and class imbalance. We further present a benchmark comparison of three widely used DA methods.","sentences":["Plankton recognition provides novel possibilities to study various environmental aspects and an interesting real-world context to develop domain adaptation (DA) methods.","Different imaging instruments cause domain shift between datasets hampering the development of general plankton recognition methods.","A promising remedy for this is DA allowing to adapt a model trained on one instrument to other instruments.","In this paper, we present a new DA dataset called DAPlankton which consists of phytoplankton images obtained with different instruments.","Phytoplankton provides a challenging DA problem due to the fine-grained nature of the task and high class imbalance in real-world datasets.","DAPlankton consists of two subsets.","DAPlankton_LAB contains images of cultured phytoplankton providing a balanced dataset with minimal label uncertainty.","DAPlankton_SEA consists of images collected from the Baltic Sea providing challenging real-world data with large intra-class variance and class imbalance.","We further present a benchmark comparison of three widely used DA methods."],"url":"http://arxiv.org/abs/2402.05615v1","category":"cs.CV"}
{"created":"2024-02-08 12:17:19","title":"Gravitational wave turbulence: a multiple time scale approach for quartic wave interactions","abstract":"Wave turbulence is by nature a multiple time scale problem for which there is a natural asymptotic closure. The main result of this analytical theory is the kinetic equation that describes the long-time statistical behaviour of such turbulence composed of a set of weakly nonlinear interacting waves. In the case of gravitational waves, it involves four-wave interactions and two invariants, energy and wave action. Although the kinetic equation of gravitational wave turbulence has been published with the Hadad-Zakharov metric, along with their physical properties, the detailed derivation has not been shown. Following the seminal work of Newell (1968) for gravity/surface waves, we present the multiple time scale method, rarely used to derive the kinetic equations, and clarify the underlying assumptions and methodology. This formalism is applied to a wave amplitude equation obtained using an Eulerian approach. It leads to a kinetic equation slightly different from the one originally published, with a wave equation obtained using a Hamiltonian approach; we verify, however, that the two formulations are fully compatible when the number of symmetries used is the same. We also show that the exact solutions (Kolmogorov-Zakharov spectra) exhibit the same power laws and cascade directions. Furthermore, the use of the multiple time scale method reveals that the system retains the memory of the initially condition up to a certain level (second order) of development in time.","sentences":["Wave turbulence is by nature a multiple time scale problem for which there is a natural asymptotic closure.","The main result of this analytical theory is the kinetic equation that describes the long-time statistical behaviour of such turbulence composed of a set of weakly nonlinear interacting waves.","In the case of gravitational waves, it involves four-wave interactions and two invariants, energy and wave action.","Although the kinetic equation of gravitational wave turbulence has been published with the Hadad-Zakharov metric, along with their physical properties, the detailed derivation has not been shown.","Following the seminal work of Newell (1968) for gravity/surface waves, we present the multiple time scale method, rarely used to derive the kinetic equations, and clarify the underlying assumptions and methodology.","This formalism is applied to a wave amplitude equation obtained using an Eulerian approach.","It leads to a kinetic equation slightly different from the one originally published, with a wave equation obtained using a Hamiltonian approach; we verify, however, that the two formulations are fully compatible when the number of symmetries used is the same.","We also show that the exact solutions (Kolmogorov-Zakharov spectra) exhibit the same power laws and cascade directions.","Furthermore, the use of the multiple time scale method reveals that the system retains the memory of the initially condition up to a certain level (second order) of development in time."],"url":"http://arxiv.org/abs/2402.05614v1","category":"gr-qc"}
{"created":"2024-02-08 12:16:20","title":"Parking on supercritical geometric Bienaym\u00e9--Galton--Watson trees","abstract":"Consider a supercritical Bienaym\\'e--Galton--Watson tree $ \\mathcal{T}$ with geometric offspring distribution. Each vertex of this tree represents a parking spot which can accommodate at most one car. On the top of this tree, we add $(A_u : u \\in \\mathcal{T})$ i.i.d.\\ non negative integers sampled according to a given law $ \\mu$, which are the car arrivals on $ \\mathcal{T}$. Each car tries to park on its arriving vertex and if the spot is already occupied, it drives towards the root and takes the first available spot. If no spot is found, then it exits the tree without parking. In this paper, we provide a criterion to determine the phase of the parking process (subcritical, critical, or supercritical) depending on the generating function of $ \\mu$.","sentences":["Consider a supercritical Bienaym\\'e--Galton--Watson tree $ \\mathcal{T}$ with geometric offspring distribution.","Each vertex of this tree represents a parking spot which can accommodate at most one car.","On the top of this tree, we add $(A_u : u \\in \\mathcal{T})$ i.i.d.\\ non negative integers sampled according to a given law $ \\mu$, which are the car arrivals on $ \\mathcal{T}$. Each car tries to park on its arriving vertex and if the spot is already occupied, it drives towards the root and takes the first available spot.","If no spot is found, then it exits the tree without parking.","In this paper, we provide a criterion to determine the phase of the parking process (subcritical, critical, or supercritical) depending on the generating function of $ \\mu$."],"url":"http://arxiv.org/abs/2402.05612v1","category":"math.PR"}
{"created":"2024-02-08 12:08:52","title":"Extending 6D Object Pose Estimators for Stereo Vision","abstract":"Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task. However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results. Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion. Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size. To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset. Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms.","sentences":["Estimating the 6D pose of objects accurately, quickly, and robustly remains a difficult task.","However, recent methods for directly regressing poses from RGB images using dense features have achieved state-of-the-art results.","Stereo vision, which provides an additional perspective on the object, can help reduce pose ambiguity and occlusion.","Moreover, stereo can directly infer the distance of an object, while mono-vision requires internalized knowledge of the object's size.","To extend the state-of-the-art in 6D object pose estimation to stereo, we created a BOP compatible stereo version of the YCB-V dataset.","Our method outperforms state-of-the-art 6D pose estimation algorithms by utilizing stereo vision and can easily be adopted for other dense feature-based algorithms."],"url":"http://arxiv.org/abs/2402.05610v1","category":"cs.CV"}
{"created":"2024-02-08 12:08:42","title":"Scalable Diffusion Models with State Space Backbone","abstract":"This paper presents a new exploration into a category of diffusion models built upon state space architecture. We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space. Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens. Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size. Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops. DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID. In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\\times$256 and 512$\\times$512, while significantly reducing the computational burden. The code and models are available at: https://github.com/feizc/DiS.","sentences":["This paper presents a new exploration into a category of diffusion models built upon state space architecture.","We endeavor to train diffusion models for image data, wherein the traditional U-Net backbone is supplanted by a state space backbone, functioning on raw patches or latent space.","Given its notable efficacy in accommodating long-range dependencies, Diffusion State Space Models (DiS) are distinguished by treating all inputs including time, condition, and noisy image patches as tokens.","Our assessment of DiS encompasses both unconditional and class-conditional image generation scenarios, revealing that DiS exhibits comparable, if not superior, performance to CNN-based or Transformer-based U-Net architectures of commensurate size.","Furthermore, we analyze the scalability of DiS, gauged by the forward pass complexity quantified in Gflops.","DiS models with higher Gflops, achieved through augmentation of depth/width or augmentation of input tokens, consistently demonstrate lower FID.","In addition to demonstrating commendable scalability characteristics, DiS-H/2 models in latent space achieve performance levels akin to prior diffusion models on class-conditional ImageNet benchmarks at the resolution of 256$\\times$256 and 512$\\times$512, while significantly reducing the computational burden.","The code and models are available at: https://github.com/feizc/DiS."],"url":"http://arxiv.org/abs/2402.05608v1","category":"cs.CV"}
{"created":"2024-02-08 12:04:43","title":"Optimizing Delegation in Collaborative Human-AI Hybrid Teams","abstract":"When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively. We refer to team members as agents. In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team. To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team. The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent. We restrict the manager task by introducing a set of constraints. The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention. To ensure minimal added complexity or potential inefficiency for the team, the manager should attempt to minimize the number of times the team reaches a constraint violation and requires subsequent manager intervention. Therefore our manager is optimizing its selection of authorized agents to boost overall team performance while minimizing the frequency of manager intervention. We demonstrate our manager performance in a simulated driving scenario representing the case of a hybrid team of agents composed of a human driver and autonomous driving system. We perform experiments for our driving scenario with interfering vehicles, indicating the need for collision avoidance and proper speed control. Our results indicate a positive impact of our manager, with some cases resulting in increased team performance up to ~187% that of the best solo agent performance.","sentences":["When humans and autonomous systems operate together as what we refer to as a hybrid team, we of course wish to ensure the team operates successfully and effectively.","We refer to team members as agents.","In our proposed framework, we address the case of hybrid teams in which, at any time, only one team member (the control agent) is authorized to act as control for the team.","To determine the best selection of a control agent, we propose the addition of an AI manager (via Reinforcement Learning) which learns as an outside observer of the team.","The manager learns a model of behavior linking observations of agent performance and the environment/world the team is operating in, and from these observations makes the most desirable selection of a control agent.","We restrict the manager task by introducing a set of constraints.","The manager constraints indicate acceptable team operation, so a violation occurs if the team enters a condition which is unacceptable and requires manager intervention.","To ensure minimal added complexity or potential inefficiency for the team, the manager should attempt to minimize the number of times the team reaches a constraint violation and requires subsequent manager intervention.","Therefore our manager is optimizing its selection of authorized agents to boost overall team performance while minimizing the frequency of manager intervention.","We demonstrate our manager performance in a simulated driving scenario representing the case of a hybrid team of agents composed of a human driver and autonomous driving system.","We perform experiments for our driving scenario with interfering vehicles, indicating the need for collision avoidance and proper speed control.","Our results indicate a positive impact of our manager, with some cases resulting in increased team performance up to ~187% that of the best solo agent performance."],"url":"http://arxiv.org/abs/2402.05605v1","category":"cs.AI"}
{"created":"2024-02-08 12:01:24","title":"AttnLRP: Attention-Aware Layer-wise Relevance Propagation for Transformers","abstract":"Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process. However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge. By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively. While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass. Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations. We provide an open-source implementation on GitHub https://github.com/rachtibat/LRP-for-Transformers.","sentences":["Large Language Models are prone to biased predictions and hallucinations, underlining the paramount importance of understanding their model-internal reasoning process.","However, achieving faithful attributions for the entirety of a black-box transformer model and maintaining computational efficiency is an unsolved challenge.","By extending the Layer-wise Relevance Propagation attribution method to handle attention layers, we address these challenges effectively.","While partial solutions exist, our method is the first to faithfully and holistically attribute not only input but also latent representations of transformer models with the computational efficiency similar to a singular backward pass.","Through extensive evaluations against existing methods on Llama 2, Flan-T5 and the Vision Transformer architecture, we demonstrate that our proposed approach surpasses alternative methods in terms of faithfulness and enables the understanding of latent representations, opening up the door for concept-based explanations.","We provide an open-source implementation on GitHub https://github.com/rachtibat/LRP-for-Transformers."],"url":"http://arxiv.org/abs/2402.05602v1","category":"cs.CL"}
{"created":"2024-02-08 11:49:42","title":"Improved upper bounds for wide-sense frameproof codes","abstract":"Frameproof codes have been extensively studied for many years due to their application in copyright protection and their connection to extremal set theory. In this paper, we investigate upper bounds on the cardinality of wide-sense $t$-frameproof codes. For $t=2$, we apply results from Sperner theory to give a better upper bound, which significantly improves a recent bound by Zhou and Zhou. For $t\\geq 3$, we provide a general upper bound by establishing a relation between wide-sense frameproof codes and cover-free families. Finally, when the code length $n$ is at most $\\frac{15+\\sqrt{33}}{24}(t-1)^2$, we show that a wide-sense $t$-frameproof code has at most $n$ codewords, and the unique optimal code consists of all weight-one codewords. As byproducts, our results improve several best known results on binary $t$-frameproof codes.","sentences":["Frameproof codes have been extensively studied for many years due to their application in copyright protection and their connection to extremal set theory.","In this paper, we investigate upper bounds on the cardinality of wide-sense $t$-frameproof codes.","For $t=2$, we apply results from Sperner theory to give a better upper bound, which significantly improves a recent bound by Zhou and Zhou.","For $t\\geq 3$, we provide a general upper bound by establishing a relation between wide-sense frameproof codes and cover-free families.","Finally, when the code length $n$ is at most $\\frac{15+\\sqrt{33}}{24}(t-1)^2$, we show that a wide-sense $t$-frameproof code has at most $n$ codewords, and the unique optimal code consists of all weight-one codewords.","As byproducts, our results improve several best known results on binary $t$-frameproof codes."],"url":"http://arxiv.org/abs/2402.05596v1","category":"math.CO"}
{"created":"2024-02-08 11:49:24","title":"Faster Quantum Algorithms with \"Fractional''-Truncated Series","abstract":"Quantum algorithms frequently rely on truncated series approximations, necessitating a high truncation order to achieve even moderate accuracy and consequently resulting in intensive circuit complexity. In response, we propose a general framework, the Randomized Truncated Series (RTS), which offers two avenues for simplifying circuits: a quadratic improvement on the truncation error and enabling a continuously adjustable effective truncation order. The core idea is that the random mixing of two series of specific forms generates a substantial reduction in the truncation error. We present an error analysis for RTS with a new mixing lemma accounting for near-unitary operators. To demonstrate the effectiveness and versatility of RTS, we provide four illustrative examples within the context of Linear Combination of Unitary, Quantum Signal Processing, and solving Quantum Differential Equations. RTS shed light on the path towards practical quantum advantage.","sentences":["Quantum algorithms frequently rely on truncated series approximations, necessitating a high truncation order to achieve even moderate accuracy and consequently resulting in intensive circuit complexity.","In response, we propose a general framework, the Randomized Truncated Series (RTS), which offers two avenues for simplifying circuits: a quadratic improvement on the truncation error and enabling a continuously adjustable effective truncation order.","The core idea is that the random mixing of two series of specific forms generates a substantial reduction in the truncation error.","We present an error analysis for RTS with a new mixing lemma accounting for near-unitary operators.","To demonstrate the effectiveness and versatility of RTS, we provide four illustrative examples within the context of Linear Combination of Unitary, Quantum Signal Processing, and solving Quantum Differential Equations.","RTS shed light on the path towards practical quantum advantage."],"url":"http://arxiv.org/abs/2402.05595v1","category":"quant-ph"}
{"created":"2024-02-08 11:48:43","title":"Long-time behaviors of some stochastic differential equations driven by L\u00e9vy noise","abstract":"Using key tools such as It\\^o formula for general semi-martingales, moments estimates for L\\'{e}vy-type stochastic integrals and properties of regular varying functions we find conditions under which solutions of stochastic differential equation with jumps are almost sure asymptotically equivalent nonrandom function with $t\\to \\infty$.","sentences":["Using key tools such as It\\^o formula for general semi-martingales, moments estimates for L\\'{e}vy-type stochastic integrals and properties of regular varying functions we find conditions under which solutions of stochastic differential equation with jumps are almost sure asymptotically equivalent nonrandom function with $t\\to \\infty$."],"url":"http://arxiv.org/abs/2402.05594v1","category":"math.PR"}
{"created":"2024-02-08 11:46:26","title":"A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only","abstract":"In medieval times, stuccoworkers used a red color, called sinopia, to first create a sketch of the to-be-made statue on the wall. Today, many of these statues are destroyed, but using the original drawings, deriving from the red color also called sinopia, we can reconstruct how the final statue might have looked.We propose a fully-automated approach to reconstruct a point cloud and show preliminary results by generating a color-image, a depth-map, as well as surface normals requiring only a single sketch, and without requiring a collection of other, similar samples. Our proposed solution allows real-time reconstruction on-site, for instance, within an exhibition, or to generate a useful starting point for an expert, trying to manually reconstruct the statue, all while using only synthetic data for training.","sentences":["In medieval times, stuccoworkers used a red color, called sinopia, to first create a sketch of the to-be-made statue on the wall.","Today, many of these statues are destroyed, but using the original drawings, deriving from the red color also called sinopia, we can reconstruct how the final statue might have looked.","We propose a fully-automated approach to reconstruct a point cloud and show preliminary results by generating a color-image, a depth-map, as well as surface normals requiring only a single sketch, and without requiring a collection of other, similar samples.","Our proposed solution allows real-time reconstruction on-site, for instance, within an exhibition, or to generate a useful starting point for an expert, trying to manually reconstruct the statue, all while using only synthetic data for training."],"url":"http://arxiv.org/abs/2402.05593v1","category":"cs.CV"}
{"created":"2024-02-08 11:44:25","title":"SoftEDA: Rethinking Rule-Based Data Augmentation with Soft Labels","abstract":"Rule-based text data augmentation is widely used for NLP tasks due to its simplicity. However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model. To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data. We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach. We have publicly opened our source code for reproducibility.","sentences":["Rule-based text data augmentation is widely used for NLP tasks due to its simplicity.","However, this method can potentially damage the original meaning of the text, ultimately hurting the performance of the model.","To overcome this limitation, we propose a straightforward technique for applying soft labels to augmented data.","We conducted experiments across seven different classification tasks and empirically demonstrated the effectiveness of our proposed approach.","We have publicly opened our source code for reproducibility."],"url":"http://arxiv.org/abs/2402.05591v1","category":"cs.CL"}
{"created":"2024-02-08 11:44:05","title":"Deformed Fr\u00e9chet law for Wigner and sample covariance matrices with tail in crossover regime","abstract":"Given $A_n:=\\frac{1}{\\sqrt{n}}(a_{ij})$ an $n\\times n$ symmetric random matrix, with elements above the diagonal given by i.i.d. random variables having mean zero and unit variance. It is known that when $\\lim_{x\\to\\infty}x^4\\mathbb{P}(|a_{ij}|>x)=0$, then fluctuation of the largest eigenvalue of $A_n$ follows a Tracy-Widom distribution. When the law of $a_{ij}$ is regularly varying with index $\\alpha\\in(0,4)$, then the largest eigenvalue has a Fr\\'echet distribution. An intermediate regime is recently uncovered in \\cite{diaconu2023more}: when $\\lim_{x\\to\\infty}x^4\\mathbb{P}(|a_{ij}|>x)=c\\in(0,\\infty)$, then the law of the largest eigenvalue follows a deformed Fr\\'echet distribution. In this work we vastly extend the scope where the latter distribution may arise. We show that the same deformed Fr\\'echet distribution arises (1) for sparse Wigner matrices with an average of $n^{O(1)}$ nonzero entries on each row; (2) for periodically banded Wigner matrices with bandwidth $d_n=n^{O(1)}$; and more generally for weighted adjacency matrices of any $k_n$-regular graphs with $k_n=n^{O(1)}$. In all these cases, we further prove that the joint distribution of the finitely many largest eigenvalues of $A_n$ form a deformed Poisson process, and that eigenvectors of the outlying eigenvalues of $A_n$ are localized, implying a mobility edge phenomenon at the spectral edge $2$. The sparser case with average degree $n^{o(1)}$ is also explored. Our technique extends to sample covariance matrices, proving for the first time that its largest eigenvalue still follows a deformed Fr\\'echet distribution, assuming the matrix entries satisfy $\\lim_{x\\to\\infty}x^4\\mathbb{P}(|a_{ij}|>x)=c\\in(0,\\infty)$.","sentences":["Given $A_n:=\\frac{1}{\\sqrt{n}}(a_{ij})$ an $n\\times n$ symmetric random matrix, with elements above the diagonal given by i.i.d. random variables having mean zero and unit variance.","It is known that when $\\lim_{x\\to\\infty}x^4\\mathbb{P}(|a_{ij}|>x)=0$, then fluctuation of the largest eigenvalue of $A_n$ follows a Tracy-Widom distribution.","When the law of $a_{ij}$ is regularly varying with index $\\alpha\\in(0,4)$, then the largest eigenvalue has a Fr\\'echet distribution.","An intermediate regime is recently uncovered in \\cite{diaconu2023more}: when $\\lim_{x\\to\\infty}x^4\\mathbb{P}(|a_{ij}|>x)=c\\in(0,\\infty)$, then the law of the largest eigenvalue follows a deformed Fr\\'echet distribution.","In this work we vastly extend the scope where the latter distribution may arise.","We show that the same deformed Fr\\'echet distribution arises (1) for sparse Wigner matrices with an average of $n^{O(1)}$ nonzero entries on each row; (2) for periodically banded Wigner matrices with bandwidth $d_n=n^{O(1)}$; and more generally for weighted adjacency matrices of any $k_n$-regular graphs with $k_n=n^{O(1)}$. In all these cases, we further prove that the joint distribution of the finitely many largest eigenvalues of $A_n$ form a deformed Poisson process, and that eigenvectors of the outlying eigenvalues of $A_n$ are localized, implying a mobility edge phenomenon at the spectral edge $2$. The sparser case with average degree $n^{o(1)}$ is also explored.","Our technique extends to sample covariance matrices, proving for the first time that its largest eigenvalue still follows a deformed Fr\\'echet distribution, assuming the matrix entries satisfy $\\lim_{x\\to\\infty}x^4\\mathbb{P}(|a_{ij}|>x)=c\\in(0,\\infty)$."],"url":"http://arxiv.org/abs/2402.05590v1","category":"math.PR"}
{"created":"2024-02-08 11:36:23","title":"AutoAugment Is What You Need: Enhancing Rule-based Augmentation Methods in Low-resource Regimes","abstract":"Text data augmentation is a complex problem due to the discrete nature of sentences. Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage. Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem. However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult. In this paper, we propose adapting AutoAugment to solve this problem. The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pre-trained language models. We offer the source code.","sentences":["Text data augmentation is a complex problem due to the discrete nature of sentences.","Although rule-based augmentation methods are widely adopted in real-world applications because of their simplicity, they suffer from potential semantic damage.","Previous researchers have suggested easy data augmentation with soft labels (softEDA), employing label smoothing to mitigate this problem.","However, finding the best factor for each model and dataset is challenging; therefore, using softEDA in real-world applications is still difficult.","In this paper, we propose adapting AutoAugment to solve this problem.","The experimental results suggest that the proposed method can boost existing augmentation methods and that rule-based methods can enhance cutting-edge pre-trained language models.","We offer the source code."],"url":"http://arxiv.org/abs/2402.05584v1","category":"cs.CL"}
{"created":"2024-02-08 11:33:16","title":"Joint End-to-End Image Compression and Denoising: Leveraging Contrastive Learning and Multi-Scale Self-ONNs","abstract":"Noisy images are a challenge to image compression algorithms due to the inherent difficulty of compressing noise. As noise cannot easily be discerned from image details, such as high-frequency signals, its presence leads to extra bits needed for compression. Since the emerging learned image compression paradigm enables end-to-end optimization of codecs, recent efforts were made to integrate denoising into the compression model, relying on clean image features to guide denoising. However, these methods exhibit suboptimal performance under high noise levels, lacking the capability to generalize across diverse noise types. In this paper, we propose a novel method integrating a multi-scale denoiser comprising of Self Organizing Operational Neural Networks, for joint image compression and denoising. We employ contrastive learning to boost the network ability to differentiate noise from high frequency signal components, by emphasizing the correlation between noisy and clean counterparts. Experimental results demonstrate the effectiveness of the proposed method both in rate-distortion performance, and codec speed, outperforming the current state-of-the-art.","sentences":["Noisy images are a challenge to image compression algorithms due to the inherent difficulty of compressing noise.","As noise cannot easily be discerned from image details, such as high-frequency signals, its presence leads to extra bits needed for compression.","Since the emerging learned image compression paradigm enables end-to-end optimization of codecs, recent efforts were made to integrate denoising into the compression model, relying on clean image features to guide denoising.","However, these methods exhibit suboptimal performance under high noise levels, lacking the capability to generalize across diverse noise types.","In this paper, we propose a novel method integrating a multi-scale denoiser comprising of Self Organizing Operational Neural Networks, for joint image compression and denoising.","We employ contrastive learning to boost the network ability to differentiate noise from high frequency signal components, by emphasizing the correlation between noisy and clean counterparts.","Experimental results demonstrate the effectiveness of the proposed method both in rate-distortion performance, and codec speed, outperforming the current state-of-the-art."],"url":"http://arxiv.org/abs/2402.05582v1","category":"eess.IV"}
{"created":"2024-02-08 11:26:12","title":"Superhydrophobic/superoleophilic magnetic elastomers by laser ablation","abstract":"We report the development of magnetic nanocomposite sheets with superhydrophobic and superoleophilic surfaces generated by laser ablation. Polydimethylsiloxane elastomer freestanding films, loaded homogeneously with 2% wt. carbon coated iron nanoparticles, were ablated by UV (248 nm), nanosecond laser pulses. The laser irradiation induces chemical and structural changes (both in micro-and nano-scale) to the surfaces of the nanocomposites rendering them superhydrophobic. The use of nanoparticles increases the UV light absorption eficiency of the nanocomposite samples, and thus facilitates the ablation process, since the number of pulses and the laser fluence required are greatly reduced compared to the bare polymer. Additionally the magnetic nanoparticles enhance significantly the super-hydrophobic and oleophilic properties of the PDMS sheets, and provide to PDMS magnetic properties making possible its actuation by a weak external magnetic field. These nanocomposite elastomers can be considered for applications requiring magnetic MEMS for the controlled separation of liquids.","sentences":["We report the development of magnetic nanocomposite sheets with superhydrophobic and superoleophilic surfaces generated by laser ablation.","Polydimethylsiloxane elastomer freestanding films, loaded homogeneously with 2% wt. carbon coated iron nanoparticles, were ablated by UV (248 nm), nanosecond laser pulses.","The laser irradiation induces chemical and structural changes (both in micro-and nano-scale) to the surfaces of the nanocomposites rendering them superhydrophobic.","The use of nanoparticles increases the UV light absorption eficiency of the nanocomposite samples, and thus facilitates the ablation process, since the number of pulses and the laser fluence required are greatly reduced compared to the bare polymer.","Additionally the magnetic nanoparticles enhance significantly the super-hydrophobic and oleophilic properties of the PDMS sheets, and provide to PDMS magnetic properties making possible its actuation by a weak external magnetic field.","These nanocomposite elastomers can be considered for applications requiring magnetic MEMS for the controlled separation of liquids."],"url":"http://arxiv.org/abs/2402.05578v1","category":"physics.app-ph"}
{"created":"2024-02-08 11:23:11","title":"Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry","abstract":"Many of the foundations of machine learning rely on the idealized premise that all input and output spaces are infinite, e.g.~$\\mathbb{R}^d$. This core assumption is systematically violated in practice due to digital computing limitations from finite machine precision, rounding, and limited RAM. In short, digital computers operate on finite grids in $\\mathbb{R}^d$. By exploiting these discrete structures, we show the curse of dimensionality in statistical learning is systematically broken when models are implemented on real computers. Consequentially, we obtain new generalization bounds with dimension-free rates for kernel and deep ReLU MLP regressors, which are implemented on real-world machines.   Our results are derived using a new non-asymptotic concentration of measure result between a probability measure over any finite metric space and its empirical version associated with $N$ i.i.d. samples when measured in the $1$-Wasserstein distance. Unlike standard concentration of measure results, the concentration rates in our bounds do not hold uniformly for all sample sizes $N$; instead, our rates can adapt to any given $N$. This yields significantly tighter bounds for realistic sample sizes while achieving the optimal worst-case rate of $\\mathcal{O}(1/N^{1/2})$ for massive. Our results are built on new techniques combining metric embedding theory with optimal transport","sentences":["Many of the foundations of machine learning rely on the idealized premise that all input and output spaces are infinite, e.g.~$\\mathbb{R}^d$.","This core assumption is systematically violated in practice due to digital computing limitations from finite machine precision, rounding, and limited RAM.","In short, digital computers operate on finite grids in $\\mathbb{R}^d$. By exploiting these discrete structures, we show the curse of dimensionality in statistical learning is systematically broken when models are implemented on real computers.","Consequentially, we obtain new generalization bounds with dimension-free rates for kernel and deep ReLU MLP regressors, which are implemented on real-world machines.   ","Our results are derived using a new non-asymptotic concentration of measure result between a probability measure over any finite metric space and its empirical version associated with $N$ i.i.d. samples when measured in the $1$-Wasserstein distance.","Unlike standard concentration of measure results, the concentration rates in our bounds do not hold uniformly for all sample sizes $N$; instead, our rates can adapt to any given $N$. This yields significantly tighter bounds for realistic sample sizes while achieving the optimal worst-case rate of $\\mathcal{O}(1/N^{1/2})$ for massive.","Our results are built on new techniques combining metric embedding theory with optimal transport"],"url":"http://arxiv.org/abs/2402.05576v1","category":"cs.LG"}
{"created":"2024-02-08 11:19:58","title":"Simultaneously Achieving Group Exposure Fairness and Within-Group Meritocracy in Stochastic Bandits","abstract":"Existing approaches to fairness in stochastic multi-armed bandits (MAB) primarily focus on exposure guarantee to individual arms. When arms are naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which considers two levels of fairness. At the first level, Bi-Level Fairness guarantees a certain minimum exposure to each group. To address the unbalanced allocation of pulls to individual arms within a group, we consider meritocratic fairness at the second level, which ensures that each arm is pulled according to its merit within the group. Our work shows that we can adapt a UCB-based algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic Fairness within each group. We first show that one can decompose regret bounds into two components: (a) regret due to anytime group exposure fairness and (b) regret due to meritocratic fairness within each group. Our proposed algorithm BF-UCB balances these two regrets optimally to achieve the upper bound of $O(\\sqrt{T})$ on regret; $T$ being the stopping time. With the help of simulated experiments, we further show that BF-UCB achieves sub-linear regret; provides better group and individual exposure guarantees compared to existing algorithms; and does not result in a significant drop in reward with respect to UCB algorithm, which does not impose any fairness constraint.","sentences":["Existing approaches to fairness in stochastic multi-armed bandits (MAB) primarily focus on exposure guarantee to individual arms.","When arms are naturally grouped by certain attribute(s), we propose Bi-Level Fairness, which considers two levels of fairness.","At the first level, Bi-Level Fairness guarantees a certain minimum exposure to each group.","To address the unbalanced allocation of pulls to individual arms within a group, we consider meritocratic fairness at the second level, which ensures that each arm is pulled according to its merit within the group.","Our work shows that we can adapt a UCB-based algorithm to achieve a Bi-Level Fairness by providing (i) anytime Group Exposure Fairness guarantees and (ii) ensuring individual-level Meritocratic Fairness within each group.","We first show that one can decompose regret bounds into two components: (a) regret due to anytime group exposure fairness and (b) regret due to meritocratic fairness within each group.","Our proposed algorithm BF-UCB balances these two regrets optimally to achieve the upper bound of $O(\\sqrt{T})$ on regret; $T$ being the stopping time.","With the help of simulated experiments, we further show that BF-UCB achieves sub-linear regret; provides better group and individual exposure guarantees compared to existing algorithms; and does not result in a significant drop in reward with respect to UCB algorithm, which does not impose any fairness constraint."],"url":"http://arxiv.org/abs/2402.05575v1","category":"cs.LG"}
{"created":"2024-02-08 11:11:38","title":"Design and Prototyping of Transmissive RIS-Aided Wireless Communication","abstract":"Reconfigurable Intelligent Surfaces (RISs) exhibit promising enhancements in coverage and data rates for wireless communication systems, particularly in the context of 5G and beyond. This paper introduces a novel approach by focusing on the design and prototyping of a transmissive RIS, contrasting with existing research predominantly centered on reflective RIS. The achievement of 1-bit transmissive RIS through the antisymmetry configuration of the two PIN diodes, nearly uniform transmission magnitudes but inversed phase states in a wide band can be obtained. A transmissive RIS prototype consisting of 16 $\\times$ 16 elements is meticulously designed, fabricated, and subjected to measurement to validate the proposed design. The results demonstrate that the proposed RIS unit cell achieves effective 1-bit phase tuning with minimal insertion loss and a transmission bandwidth of 3 dB exceeding $20\\%$ at 5.8GHz. By dynamically modulating the quantized code distributions on the RIS, it becomes possible to construct scanning beams. The experimental outcomes of the RIS-assisted communication system validate that, in comparison to scenarios without RIS, the signal receiving power experiences an increase of approximately 7dB when RIS is deployed to overcome obstacles. This underscores the potential applicability of mobile RIS in practical communication.","sentences":["Reconfigurable Intelligent Surfaces (RISs) exhibit promising enhancements in coverage and data rates for wireless communication systems, particularly in the context of 5G and beyond.","This paper introduces a novel approach by focusing on the design and prototyping of a transmissive RIS, contrasting with existing research predominantly centered on reflective RIS.","The achievement of 1-bit transmissive RIS through the antisymmetry configuration of the two PIN diodes, nearly uniform transmission magnitudes but inversed phase states in a wide band can be obtained.","A transmissive RIS prototype consisting of 16 $\\times$ 16 elements is meticulously designed, fabricated, and subjected to measurement to validate the proposed design.","The results demonstrate that the proposed RIS unit cell achieves effective 1-bit phase tuning with minimal insertion loss and a transmission bandwidth of 3 dB exceeding $20\\%$ at 5.8GHz.","By dynamically modulating the quantized code distributions on the RIS, it becomes possible to construct scanning beams.","The experimental outcomes of the RIS-assisted communication system validate that, in comparison to scenarios without RIS, the signal receiving power experiences an increase of approximately 7dB when RIS is deployed to overcome obstacles.","This underscores the potential applicability of mobile RIS in practical communication."],"url":"http://arxiv.org/abs/2402.05570v1","category":"eess.SY"}
{"created":"2024-02-08 11:10:39","title":"Hypergraph Node Classification With Graph Neural Networks","abstract":"Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not only higher classification accuracy compared to state-of-the-art HyperGNNs, but also superior memory and runtime efficiency.","sentences":["Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data.","The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions.","This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs).","GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies.","However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph.","This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification.","Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not only higher classification accuracy compared to state-of-the-art HyperGNNs, but also superior memory and runtime efficiency."],"url":"http://arxiv.org/abs/2402.05569v1","category":"cs.LG"}
{"created":"2024-02-08 11:05:49","title":"Listening Between the Lines: Synthetic Speech Detection Disregarding Verbal Content","abstract":"Recent advancements in synthetic speech generation have led to the creation of forged audio data that are almost indistinguishable from real speech. This phenomenon poses a new challenge for the multimedia forensics community, as the misuse of synthetic media can potentially cause adverse consequences. Several methods have been proposed in the literature to mitigate potential risks and detect synthetic speech, mainly focusing on the analysis of the speech itself. However, recent studies have revealed that the most crucial frequency bands for detection lie in the highest ranges (above 6000 Hz), which do not include any speech content. In this work, we extensively explore this aspect and investigate whether synthetic speech detection can be performed by focusing only on the background component of the signal while disregarding its verbal content. Our findings indicate that the speech component is not the predominant factor in performing synthetic speech detection. These insights provide valuable guidance for the development of new synthetic speech detectors and their interpretability, together with some considerations on the existing work in the audio forensics field.","sentences":["Recent advancements in synthetic speech generation have led to the creation of forged audio data that are almost indistinguishable from real speech.","This phenomenon poses a new challenge for the multimedia forensics community, as the misuse of synthetic media can potentially cause adverse consequences.","Several methods have been proposed in the literature to mitigate potential risks and detect synthetic speech, mainly focusing on the analysis of the speech itself.","However, recent studies have revealed that the most crucial frequency bands for detection lie in the highest ranges (above 6000 Hz), which do not include any speech content.","In this work, we extensively explore this aspect and investigate whether synthetic speech detection can be performed by focusing only on the background component of the signal while disregarding its verbal content.","Our findings indicate that the speech component is not the predominant factor in performing synthetic speech detection.","These insights provide valuable guidance for the development of new synthetic speech detectors and their interpretability, together with some considerations on the existing work in the audio forensics field."],"url":"http://arxiv.org/abs/2402.05567v1","category":"cs.SD"}
{"created":"2024-02-08 11:03:13","title":"Duality for condensed cohomology of the Weil group of a p-adic field","abstract":"We use the theory of Condensed Mathematics to build a condensed cohomology theory for the Weil group of a $p$-adic field. The cohomology groups are proved to be locally compact abelian groups of finite ranks in some special cases. This allows us to enlarge the local Tate Duality to a more general category of non-necessarily discrete coefficients, where it takes the form of a Pontryagin duality between locally compact abelian groups.","sentences":["We use the theory of Condensed Mathematics to build a condensed cohomology theory for the Weil group of a $p$-adic field.","The cohomology groups are proved to be locally compact abelian groups of finite ranks in some special cases.","This allows us to enlarge the local Tate Duality to a more general category of non-necessarily discrete coefficients, where it takes the form of a Pontryagin duality between locally compact abelian groups."],"url":"http://arxiv.org/abs/2402.05565v1","category":"math.NT"}
{"created":"2024-02-08 11:02:06","title":"Neural Multigrid Architectures","abstract":"We propose a convenient matrix-free neural architecture for the multigrid method. The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers. We argue that a fixed neural network without dense layers can not realize an efficient iterative method. Because of that, standard training protocols do not lead to competitive solvers. To overcome this difficulty, we use parameter sharing and serialization of layers. The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns. From the point of view of numerical linear algebra network's training corresponds to finding optimal smoothers for the geometric multigrid method. We demonstrate our approach on a few second-order elliptic equations. For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a basic linear multigrid with Jacobi smoother.","sentences":["We propose a convenient matrix-free neural architecture for the multigrid method.","The architecture is simple enough to be implemented in less than fifty lines of code, yet it encompasses a large number of distinct multigrid solvers.","We argue that a fixed neural network without dense layers can not realize an efficient iterative method.","Because of that, standard training protocols do not lead to competitive solvers.","To overcome this difficulty, we use parameter sharing and serialization of layers.","The resulting network can be trained on linear problems with thousands of unknowns and retains its efficiency on problems with millions of unknowns.","From the point of view of numerical linear algebra network's training corresponds to finding optimal smoothers for the geometric multigrid method.","We demonstrate our approach on a few second-order elliptic equations.","For tested linear systems, we obtain from two to five times smaller spectral radius of the error propagation matrix compare to a basic linear multigrid with Jacobi smoother."],"url":"http://arxiv.org/abs/2402.05563v1","category":"math.NA"}
{"created":"2024-02-08 10:52:37","title":"Flashback: Understanding and Mitigating Forgetting in Federated Learning","abstract":"In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients. This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts. Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting. We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition. Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge. Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds.","sentences":["In Federated Learning (FL), forgetting, or the loss of knowledge across rounds, hampers algorithm convergence, particularly in the presence of severe data heterogeneity among clients.","This study explores the nuances of this issue, emphasizing the critical role of forgetting in FL's inefficient learning within heterogeneous data contexts.","Knowledge loss occurs in both client-local updates and server-side aggregation steps; addressing one without the other fails to mitigate forgetting.","We introduce a metric to measure forgetting granularly, ensuring distinct recognition amid new knowledge acquisition.","Leveraging these insights, we propose Flashback, an FL algorithm with a dynamic distillation approach that is used to regularize the local models, and effectively aggregate their knowledge.","Across different benchmarks, Flashback outperforms other methods, mitigates forgetting, and achieves faster round-to-target-accuracy, by converging in 6 to 16 rounds."],"url":"http://arxiv.org/abs/2402.05558v1","category":"cs.LG"}
{"created":"2024-02-08 10:47:41","title":"Dao numbers and the asymptotic behaviour of fullness","abstract":"In the present paper, we study the Dao numbers $\\mathfrak{d}_1(I),\\mathfrak{d}_2(I)$ and $\\mathfrak{d}_3(I)$ of an ideal $I$ of a Noetherian local ring $(R,\\mathfrak{m},K)$ or a standard graded Noetherian $K$-algebra. They are defined as the smallest $\\ell\\ge0$ such that $I\\mathfrak{m}^k$ is $\\mathfrak{m}$-full, full, weakly $\\mathfrak{m}$-full, respectively, for all $k\\ge\\ell$. We provide general bounds for the Dao numbers in terms of the Castelnuovo-Mumford regularity of certain modules over the Rees algebra $\\mathcal{R}(\\mathfrak{m})$. If $R$ is a Koszul algebra, we prove that the Dao numbers are less or equal to $\\text{reg}_{\\text{gr}_\\mathfrak{m}(R)}\\text{gr}_\\mathfrak{m}(I)$, where $\\text{gr}_\\mathfrak{m}(I)$ is the associated graded module of $I$. Finally, for monomial ideals, we combinatorially bound the Dao numbers in terms of asymptotic linear quotients and bounding multidegrees.","sentences":["In the present paper, we study the Dao numbers $\\mathfrak{d}_1(I),\\mathfrak{d}_2(I)$ and $\\mathfrak{d}_3(I)$ of an ideal $I$ of a Noetherian local ring $(R,\\mathfrak{m},K)$ or a standard graded Noetherian $K$-algebra.","They are defined as the smallest $\\ell\\ge0$ such that $I\\mathfrak{m}^k$ is $\\mathfrak{m}$-full, full, weakly $\\mathfrak{m}$-full, respectively, for all $k\\ge\\ell$. We provide general bounds for the Dao numbers in terms of the Castelnuovo-Mumford regularity of certain modules over the Rees algebra $\\mathcal{R}(\\mathfrak{m})$.","If $R$ is a Koszul algebra, we prove that the Dao numbers are less or equal to $\\text{reg}_{\\text{gr}_\\mathfrak{m}(R)}\\text{gr}_\\mathfrak{m}(I)$, where $\\text{gr}_\\mathfrak{m}(I)$ is the associated graded module of $I$. Finally, for monomial ideals, we combinatorially bound the Dao numbers in terms of asymptotic linear quotients and bounding multidegrees."],"url":"http://arxiv.org/abs/2402.05555v1","category":"math.AC"}
{"created":"2024-02-08 10:35:23","title":"Transfer learning of optimal QAOA parameters in combinatorial optimization","abstract":"Solving combinatorial optimization problems (COPs) is a promising application of quantum computation, with the Quantum Approximate Optimization Algorithm (QAOA) being one of the most studied quantum algorithms for solving them. However, multiple factors make the parameter search of the QAOA a hard optimization problem. In this work, we study transfer learning (TL), a methodology to reuse pre-trained QAOA parameters of one problem instance into different COP instances. To this end, we select small cases of the traveling salesman problem (TSP), the bin packing problem (BPP), the knapsack problem (KP), the weighted maximum cut (MaxCut) problem, the maximal independent set (MIS) problem, and portfolio optimization (PO), and find optimal $\\beta$ and $\\gamma$ parameters for $p$ layers. We compare how well the parameters found for one problem adapt to the others. Among the different problems, BPP is the one that produces the best transferable parameters, maintaining the probability of finding the optimal solution above a quadratic speedup for problem sizes up to 42 qubits and p = 10 layers. Using the BPP parameters, we perform experiments on IonQ Harmony and Aria, Rigetti Aspen-M-3, and IBM Brisbane of MIS instances for up to 18 qubits. The results indicate IonQ Aria yields the best overlap with the ideal probability distribution. Additionally, we show that cross-platform TL is possible using the D-Wave Advantage quantum annealer with the parameters found for BPP. We show an improvement in performance compared to the default protocols for MIS with up to 170 qubits. Our results suggest that there are QAOA parameters that generalize well for different COPs and annealing protocols.","sentences":["Solving combinatorial optimization problems (COPs) is a promising application of quantum computation, with the Quantum Approximate Optimization Algorithm (QAOA) being one of the most studied quantum algorithms for solving them.","However, multiple factors make the parameter search of the QAOA a hard optimization problem.","In this work, we study transfer learning (TL), a methodology to reuse pre-trained QAOA parameters of one problem instance into different COP instances.","To this end, we select small cases of the traveling salesman problem (TSP), the bin packing problem (BPP), the knapsack problem (KP), the weighted maximum cut (MaxCut) problem, the maximal independent set (MIS) problem, and portfolio optimization (PO), and find optimal $\\beta$ and $\\gamma$ parameters for $p$ layers.","We compare how well the parameters found for one problem adapt to the others.","Among the different problems, BPP is the one that produces the best transferable parameters, maintaining the probability of finding the optimal solution above a quadratic speedup for problem sizes up to 42 qubits and p = 10 layers.","Using the BPP parameters, we perform experiments on IonQ Harmony and Aria, Rigetti Aspen-M-3, and IBM Brisbane of MIS instances for up to 18 qubits.","The results indicate IonQ Aria yields the best overlap with the ideal probability distribution.","Additionally, we show that cross-platform TL is possible using the D-Wave Advantage quantum annealer with the parameters found for BPP.","We show an improvement in performance compared to the default protocols for MIS with up to 170 qubits.","Our results suggest that there are QAOA parameters that generalize well for different COPs and annealing protocols."],"url":"http://arxiv.org/abs/2402.05549v1","category":"quant-ph"}
{"created":"2024-02-08 10:32:06","title":"Benchmarking Large Language Models on Communicative Medical Coaching: a Novel System and Dataset","abstract":"Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems. However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored. We introduce ``ChatCoach,'' an integrated human-AI cooperative framework. Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations. Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent. Simultaneously, a coaching agent provides real-time feedback to the doctor. To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess their effectiveness in communicative medical coaching tasks. Our comparative analysis demonstrates that instruction-tuned Llama2 significantly outperforms ChatGPT's prompting-based approaches.","sentences":["Traditional applications of natural language processing (NLP) in healthcare have predominantly focused on patient-centered services, enhancing patient interactions and care delivery, such as through medical dialogue systems.","However, the potential of NLP to benefit inexperienced doctors, particularly in areas such as communicative medical coaching, remains largely unexplored.","We introduce ``ChatCoach,'' an integrated human-AI cooperative framework.","Within this framework, both a patient agent and a coaching agent collaboratively support medical learners in practicing their medical communication skills during consultations.","Unlike traditional dialogue systems, ChatCoach provides a simulated environment where a human doctor can engage in medical dialogue with a patient agent.","Simultaneously, a coaching agent provides real-time feedback to the doctor.","To construct the ChatCoach system, we developed a dataset and integrated Large Language Models such as ChatGPT and Llama2, aiming to assess their effectiveness in communicative medical coaching tasks.","Our comparative analysis demonstrates that instruction-tuned Llama2 significantly outperforms ChatGPT's prompting-based approaches."],"url":"http://arxiv.org/abs/2402.05547v1","category":"cs.CL"}
{"created":"2024-02-08 10:29:46","title":"Offline Actor-Critic Reinforcement Learning Scales to Large Models","abstract":"We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning. We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks. We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules. Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data.","sentences":["We show that offline actor-critic reinforcement learning can scale to large models - such as transformers - and follows similar scaling laws as supervised learning.","We find that offline actor-critic algorithms can outperform strong, supervised, behavioral cloning baselines for multi-task training on a large dataset containing both sub-optimal and expert behavior on 132 continuous control tasks.","We introduce a Perceiver-based actor-critic model and elucidate the key model features needed to make offline RL work with self- and cross-attention modules.","Overall, we find that: i) simple offline actor critic algorithms are a natural choice for gradually moving away from the currently predominant paradigm of behavioral cloning, and ii) via offline RL it is possible to learn multi-task policies that master many domains simultaneously, including real robotics tasks, from sub-optimal demonstrations or self-generated data."],"url":"http://arxiv.org/abs/2402.05546v1","category":"cs.LG"}
{"created":"2024-02-08 10:29:11","title":"Named Entity Recognition for Address Extraction in Speech-to-Text Transcriptions Using Synthetic Data","abstract":"This paper introduces an approach for building a Named Entity Recognition (NER) model built upon a Bidirectional Encoder Representations from Transformers (BERT) architecture, specifically utilizing the SlovakBERT model. This NER model extracts address parts from data acquired from speech-to-text transcriptions. Due to scarcity of real data, a synthetic dataset using GPT API was generated. The importance of mimicking spoken language variability in this artificial data is emphasized. The performance of our NER model, trained solely on synthetic data, is evaluated using small real test dataset.","sentences":["This paper introduces an approach for building a Named Entity Recognition (NER) model built upon a Bidirectional Encoder Representations from Transformers (BERT) architecture, specifically utilizing the SlovakBERT model.","This NER model extracts address parts from data acquired from speech-to-text transcriptions.","Due to scarcity of real data, a synthetic dataset using GPT API was generated.","The importance of mimicking spoken language variability in this artificial data is emphasized.","The performance of our NER model, trained solely on synthetic data, is evaluated using small real test dataset."],"url":"http://arxiv.org/abs/2402.05545v1","category":"cs.CL"}
{"created":"2024-02-08 10:22:12","title":"Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions","abstract":"Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for continuous control of aggregation weights, an innovative client selection method based on model parameter distances, and a reward mechanism guided by validation set performance. Empirically, extensive experiments demonstrate that, in terms of robustness, RFL outperforms the state-of-the-art methods, while maintaining comparable levels of fairness, offering a promising solution to build resilient and fair federated systems.","sentences":["Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples.","However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness.","Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities.","To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings.","To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for continuous control of aggregation weights, an innovative client selection method based on model parameter distances, and a reward mechanism guided by validation set performance.","Empirically, extensive experiments demonstrate that, in terms of robustness, RFL outperforms the state-of-the-art methods, while maintaining comparable levels of fairness, offering a promising solution to build resilient and fair federated systems."],"url":"http://arxiv.org/abs/2402.05541v1","category":"cs.LG"}
{"created":"2024-02-08 10:20:36","title":"Associators from an operadic point of view","abstract":"This is a survey on Drinfeld associators and their generalizations, where we focus on operadic aspects.","sentences":["This is a survey on Drinfeld associators and their generalizations, where we focus on operadic aspects."],"url":"http://arxiv.org/abs/2402.05539v1","category":"math.QA"}
{"created":"2024-02-08 10:15:41","title":"Empowering machine learning models with contextual knowledge for enhancing the detection of eating disorders in social media posts","abstract":"Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments. These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context. We introduce a novel hybrid approach combining community-maintained knowledge graphs (like Wikidata) with deep learning to enhance the categorization of social media posts. This method uses advanced entity recognizers and linkers (like Falcon 2.0) to connect short post entities to knowledge graphs. Knowledge graph embeddings (KGEs) and contextualized word embeddings (like BERT) are then employed to create rich, context-based representations of these posts.   Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis. We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging word embeddings with knowledge graph information enhances the predictive models' reliability. This methodology aims to assist health experts in spotting patterns indicative of mental disorders, thereby improving early detection and accurate diagnosis for personalized medicine.","sentences":["Social networks are vital for information sharing, especially in the health sector for discussing diseases and treatments.","These platforms, however, often feature posts as brief texts, posing challenges for Artificial Intelligence (AI) in understanding context.","We introduce a novel hybrid approach combining community-maintained knowledge graphs (like Wikidata) with deep learning to enhance the categorization of social media posts.","This method uses advanced entity recognizers and linkers (like Falcon 2.0) to connect short post entities to knowledge graphs.","Knowledge graph embeddings (KGEs) and contextualized word embeddings (like BERT) are then employed to create rich, context-based representations of these posts.   ","Our focus is on the health domain, particularly in identifying posts related to eating disorders (e.g., anorexia, bulimia) to aid healthcare providers in early diagnosis.","We tested our approach on a dataset of 2,000 tweets about eating disorders, finding that merging word embeddings with knowledge graph information enhances the predictive models' reliability.","This methodology aims to assist health experts in spotting patterns indicative of mental disorders, thereby improving early detection and accurate diagnosis for personalized medicine."],"url":"http://arxiv.org/abs/2402.05536v1","category":"cs.LG"}
{"created":"2024-02-08 10:13:52","title":"On Optimizing Deterministic Concurrent Scheduling for Smart Contracts and Blockchains","abstract":"Executing smart contracts is a compute and storage-intensive task, which currently dominates modern blockchain's performance. Given that computers are becoming increasingly multicore, concurrency is an attractive approach to improve programs' execution runtime. A unique challenge of blockchains is that all replicas (minors or validators) must execute all smart contracts in the same logical order to maintain the semantics of State Machine Replication (SMR). While non-conflicting transactions can be executed in any actual order, replicas need to enforce a unique logical order among all pairs of conflicting transactions.   In this work, we formally study the maximal level of parallelism obtainable when focusing on the conflict graphs between transactions packaged in the same block, rather than relying on the total ordering order. To that end, we describe a generic framework for Active State Machine Replication (ASMR) that is strictly serializable. The generic framework allows for shifting our focus to developing efficient execution engines for transactions without introducing non-deterministic results.   Then, we suggest the concept of graph scheduling, and the minimal latency scheduling problem, which we prove to be NP-Hard. We show that the restricted version of the problem for homogeneous transactions is equivalent to the classic Graph Vertex Coloring Problem, yet the heterogenous case is more complex. We discuss practical implications of these results.","sentences":["Executing smart contracts is a compute and storage-intensive task, which currently dominates modern blockchain's performance.","Given that computers are becoming increasingly multicore, concurrency is an attractive approach to improve programs' execution runtime.","A unique challenge of blockchains is that all replicas (minors or validators) must execute all smart contracts in the same logical order to maintain the semantics of State Machine Replication (SMR).","While non-conflicting transactions can be executed in any actual order, replicas need to enforce a unique logical order among all pairs of conflicting transactions.   ","In this work, we formally study the maximal level of parallelism obtainable when focusing on the conflict graphs between transactions packaged in the same block, rather than relying on the total ordering order.","To that end, we describe a generic framework for Active State Machine Replication (ASMR) that is strictly serializable.","The generic framework allows for shifting our focus to developing efficient execution engines for transactions without introducing non-deterministic results.   ","Then, we suggest the concept of graph scheduling, and the minimal latency scheduling problem, which we prove to be NP-Hard.","We show that the restricted version of the problem for homogeneous transactions is equivalent to the classic Graph Vertex Coloring Problem, yet the heterogenous case is more complex.","We discuss practical implications of these results."],"url":"http://arxiv.org/abs/2402.05535v1","category":"cs.DC"}
{"created":"2024-02-08 10:05:11","title":"Differentially Private Model-Based Offline Reinforcement Learning","abstract":"We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset. To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees. A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level. Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data. We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting.","sentences":["We address offline reinforcement learning with privacy guarantees, where the goal is to train a policy that is differentially private with respect to individual trajectories in the dataset.","To achieve this, we introduce DP-MORL, an MBRL algorithm coming with differential privacy guarantees.","A private model of the environment is first learned from offline data using DP-FedAvg, a training method for neural networks that provides differential privacy guarantees at the trajectory level.","Then, we use model-based policy optimization to derive a policy from the (penalized) private model, without any further interaction with the system or access to the input data.","We empirically show that DP-MORL enables the training of private RL agents from offline data and we furthermore outline the price of privacy in this setting."],"url":"http://arxiv.org/abs/2402.05525v1","category":"cs.LG"}
{"created":"2024-02-08 10:01:29","title":"Linearizing Models for Efficient yet Robust Private Inference","abstract":"The growing concern about data privacy has led to the development of private inference (PI) frameworks in client-server applications which protects both data privacy and model IP. However, the cryptographic primitives required yield significant latency overhead which limits its wide-spread application. At the same time, changing environments demand the PI service to be robust against various naturally occurring and gradient-based perturbations. Despite several works focused on the development of latency-efficient models suitable for PI, the impact of these models on robustness has remained unexplored. Towards this goal, this paper presents RLNet, a class of robust linearized networks that can yield latency improvement via reduction of high-latency ReLU operations while improving the model performance on both clean and corrupted images. In particular, RLNet models provide a \"triple win ticket\" of improved classification accuracy on clean, naturally perturbed, and gradient-based perturbed images using a shared-mask shared-weight architecture with over an order of magnitude fewer ReLUs than baseline models. To demonstrate the efficacy of RLNet, we perform extensive experiments with ResNet and WRN model variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Our experimental evaluations show that RLNet can yield models with up to 11.14x fewer ReLUs, with accuracy close to the all-ReLU models, on clean, naturally perturbed, and gradient-based perturbed images. Compared with the SoTA non-robust linearized models at similar ReLU budgets, RLNet achieves an improvement in adversarial accuracy of up to ~47%, naturally perturbed accuracy up to ~16.4%, while improving clean image accuracy up to ~1.5%.","sentences":["The growing concern about data privacy has led to the development of private inference (PI) frameworks in client-server applications which protects both data privacy and model IP.","However, the cryptographic primitives required yield significant latency overhead which limits its wide-spread application.","At the same time, changing environments demand the PI service to be robust against various naturally occurring and gradient-based perturbations.","Despite several works focused on the development of latency-efficient models suitable for PI, the impact of these models on robustness has remained unexplored.","Towards this goal, this paper presents RLNet, a class of robust linearized networks that can yield latency improvement via reduction of high-latency ReLU operations while improving the model performance on both clean and corrupted images.","In particular, RLNet models provide a \"triple win ticket\" of improved classification accuracy on clean, naturally perturbed, and gradient-based perturbed images using a shared-mask shared-weight architecture with over an order of magnitude fewer ReLUs than baseline models.","To demonstrate the efficacy of RLNet, we perform extensive experiments with ResNet and WRN model variants on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets.","Our experimental evaluations show that RLNet can yield models with up to 11.14x fewer ReLUs, with accuracy close to the all-ReLU models, on clean, naturally perturbed, and gradient-based perturbed images.","Compared with the SoTA non-robust linearized models at similar ReLU budgets, RLNet achieves an improvement in adversarial accuracy of up to ~47%, naturally perturbed accuracy up to ~16.4%, while improving clean image accuracy up to ~1.5%."],"url":"http://arxiv.org/abs/2402.05521v1","category":"cs.LG"}
{"created":"2024-02-08 10:00:40","title":"Can ChatGPT evaluate research quality?","abstract":"Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task. Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT. This was applied to 51 of my own articles and compared against my own quality judgements. Findings: ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria. Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0). In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509. Thus, averaging scores from multiple ChatGPT-4 rounds seems more effective than individual scores. The positive correlation may be due to ChatGPT being able to extract the author's significance, rigour, and originality claims from inside each paper. If my weakest articles are removed, then the correlation with average scores (r=0.200) falls below statistical significance, suggesting that ChatGPT struggles to make fine-grained evaluations. Research limitations: The data is self-evaluations of a convenience sample of articles from one academic in one field. Practical implications: Overall, ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks. Research evaluators, including journal editors, should therefore take steps to control its use. Originality/value: This is the first published attempt at post-publication expert review accuracy testing for ChatGPT.","sentences":["Purpose: Assess whether ChatGPT 4.0 is accurate enough to perform research evaluations on journal articles to automate this time-consuming task.","Design/methodology/approach: Test the extent to which ChatGPT-4 can assess the quality of journal articles using a case study of the published scoring guidelines of the UK Research Excellence Framework (REF) 2021 to create a research evaluation ChatGPT.","This was applied to 51 of my own articles and compared against my own quality judgements.","Findings: ChatGPT-4 can produce plausible document summaries and quality evaluation rationales that match the REF criteria.","Its overall scores have weak correlations with my self-evaluation scores of the same documents (averaging r=0.281 over 15 iterations, with 8 being statistically significantly different from 0).","In contrast, the average scores from the 15 iterations produced a statistically significant positive correlation of 0.509.","Thus, averaging scores from multiple ChatGPT-4 rounds seems more effective than individual scores.","The positive correlation may be due to ChatGPT being able to extract the author's significance, rigour, and originality claims from inside each paper.","If my weakest articles are removed, then the correlation with average scores (r=0.200) falls below statistical significance, suggesting that ChatGPT struggles to make fine-grained evaluations.","Research limitations: The data is self-evaluations of a convenience sample of articles from one academic in one field.","Practical implications: Overall, ChatGPT does not yet seem to be accurate enough to be trusted for any formal or informal research quality evaluation tasks.","Research evaluators, including journal editors, should therefore take steps to control its use.","Originality/value: This is the first published attempt at post-publication expert review accuracy testing for ChatGPT."],"url":"http://arxiv.org/abs/2402.05519v1","category":"cs.DL"}
{"created":"2024-02-08 09:57:32","title":"Rigidity of Lyapunov Exponents for Geodesic Flows","abstract":"In this paper, we study rigidity problems between Lyapunov exponents along periodic orbits and geometric structures. More specifically, we prove that for a surface M without focal points, if the value of the Lyapunov exponents is constant over all periodic orbits, then $M$ is the flat 2-torus or a surface of constant negative curvature. We obtain the same result for the case of Anosov geodesic flow for surface, which generalizes Butler's result in dimension two. Using completely different techniques, we also prove an extension of Butler's result to the finite volume case, where the value of the Lyapunov exponents along all periodic orbits is constant, being the maximum or minimum possible.","sentences":["In this paper, we study rigidity problems between Lyapunov exponents along periodic orbits and geometric structures.","More specifically, we prove that for a surface M without focal points, if the value of the Lyapunov exponents is constant over all periodic orbits, then $M$ is the flat 2-torus or a surface of constant negative curvature.","We obtain the same result for the case of Anosov geodesic flow for surface, which generalizes Butler's result in dimension two.","Using completely different techniques, we also prove an extension of Butler's result to the finite volume case, where the value of the Lyapunov exponents along all periodic orbits is constant, being the maximum or minimum possible."],"url":"http://arxiv.org/abs/2402.05518v1","category":"math.DS"}
{"created":"2024-02-08 09:57:11","title":"Memory in quantum processes with indefinite time direction and causal order","abstract":"We examine the emergence of dynamical memory effects in quantum processes having indefinite time direction and causal order. In particular, we focus on the class of phase-covariant qubit channels, which encompasses some of the most significant paradigmatic open quantum system models. In order to assess the memory in the time evolution of the system, we utilize the trace distance and the entanglement based measures of non-Markovianity in relation to the property of CP-indivisibility. While the indefinite time direction is obtained through the quantum time flip operation that realizes a coherent superposition of forward and backward processes, the indefinite causal order is achieved via the quantum switch map, which implements two quantum processes in a coherent superposition of their two possible orders. Considering various different families of phase-covariant qubit channels, we demonstrate that, when implemented on memoryless quantum processes, both the quantum time flip and the quantum switch operations can generate memory effects in the dynamics according to the trace distance based measure under certain conditions. On the other hand, with respect to the entanglement based measure, we show that neither the quantum time flip nor the quantum switch could induce dynamical memory for any of the considered phase-covariant channels.","sentences":["We examine the emergence of dynamical memory effects in quantum processes having indefinite time direction and causal order.","In particular, we focus on the class of phase-covariant qubit channels, which encompasses some of the most significant paradigmatic open quantum system models.","In order to assess the memory in the time evolution of the system, we utilize the trace distance and the entanglement based measures of non-Markovianity in relation to the property of CP-indivisibility.","While the indefinite time direction is obtained through the quantum time flip operation that realizes a coherent superposition of forward and backward processes, the indefinite causal order is achieved via the quantum switch map, which implements two quantum processes in a coherent superposition of their two possible orders.","Considering various different families of phase-covariant qubit channels, we demonstrate that, when implemented on memoryless quantum processes, both the quantum time flip and the quantum switch operations can generate memory effects in the dynamics according to the trace distance based measure under certain conditions.","On the other hand, with respect to the entanglement based measure, we show that neither the quantum time flip nor the quantum switch could induce dynamical memory for any of the considered phase-covariant channels."],"url":"http://arxiv.org/abs/2402.05517v1","category":"quant-ph"}
{"created":"2024-02-08 09:48:02","title":"NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning","abstract":"In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence. Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs. In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration. Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions. Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence. Therefore, we believe that NoisyICL is an effective calibration of ICL. Our experimental code is uploaded to Github.","sentences":["In-Context Learning (ICL) is suffering from unsatisfactory performance and under-calibration due to high prior bias and unfaithful confidence.","Some previous works fine-tuned language models for better ICL performance with enormous datasets and computing costs.","In this paper, we propose NoisyICL, simply perturbing the model parameters by random noises to strive for better performance and calibration.","Our experiments on 2 models and 12 downstream datasets show that NoisyICL can help ICL produce more accurate predictions.","Our further analysis indicates that NoisyICL enables the model to provide more fair predictions, and also with less unfaithful confidence.","Therefore, we believe that NoisyICL is an effective calibration of ICL.","Our experimental code is uploaded to Github."],"url":"http://arxiv.org/abs/2402.05515v1","category":"cs.CL"}
{"created":"2024-02-08 09:44:02","title":"GPTs Are Multilingual Annotators for Sequence Generation Tasks","abstract":"Data annotation is an essential step for constructing new datasets. However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive. In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers. To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance. Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation. Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study. We have opened our source code for further study and reproducibility.","sentences":["Data annotation is an essential step for constructing new datasets.","However, the conventional approach of data annotation through crowdsourcing is both time-consuming and expensive.","In addition, the complexity of this process increases when dealing with low-resource languages owing to the difference in the language pool of crowdworkers.","To address these issues, this study proposes an autonomous annotation method by utilizing large language models, which have been recently demonstrated to exhibit remarkable performance.","Through our experiments, we demonstrate that the proposed method is not just cost-efficient but also applicable for low-resource language annotation.","Additionally, we constructed an image captioning dataset using our approach and are committed to open this dataset for future study.","We have opened our source code for further study and reproducibility."],"url":"http://arxiv.org/abs/2402.05512v1","category":"cs.CL"}
{"created":"2024-02-08 09:43:37","title":"Topological closure of formal power series ideals and application to topological rewriting theory","abstract":"We investigate formal power series ideals and their relationship to topological rewriting theory. Since commutative formal power series algebras are Zariski rings, their ideals are closed for the adic topology defined by the maximal ideal generated by the indeterminates. We provide a constructive proof of this result which, given a formal power series in the topological closure of an ideal, consists in computing a cofactor representation of the series with respect to a standard basis of the ideal. We apply this result to topological rewriting theory. In this context, two natural notions of confluence arise: topological confluence and infinitary confluence. We give explicit examples illustrating that in general, infinitary confluence is a strictly stronger notion that topological confluence. Using topological closure of ideals, we finally show that in the context of rewriting theory on commutative formal power series, infinitary and topological confluence are equivalent.","sentences":["We investigate formal power series ideals and their relationship to topological rewriting theory.","Since commutative formal power series algebras are Zariski rings, their ideals are closed for the adic topology defined by the maximal ideal generated by the indeterminates.","We provide a constructive proof of this result which, given a formal power series in the topological closure of an ideal, consists in computing a cofactor representation of the series with respect to a standard basis of the ideal.","We apply this result to topological rewriting theory.","In this context, two natural notions of confluence arise: topological confluence and infinitary confluence.","We give explicit examples illustrating that in general, infinitary confluence is a strictly stronger notion that topological confluence.","Using topological closure of ideals, we finally show that in the context of rewriting theory on commutative formal power series, infinitary and topological confluence are equivalent."],"url":"http://arxiv.org/abs/2402.05511v1","category":"math.AC"}
{"created":"2024-02-08 09:39:42","title":"The Impact of Cometary 'impacts' on the Chemistry, Climate, and Spectra of Hot Jupiter Atmospheres","abstract":"Impacts from icy and rocky bodies have helped shape the composition of solar system objects, for example the Earth-Moon system, or the recent impact of comet Shoemaker-Levy 9 with Jupiter. It is likely that such impacts also shape the composition of exoplanetary systems. Here we investigate how cometary impacts might affect the atmospheric composition/chemistry of hot Jupiters, which are prime targets for characterisation. We introduce a parametrised cometary impact model that includes thermal ablation and pressure driven breakup, which we couple with the 1D `radiative-convective' atmospheric model ATMO, including disequilibrium chemistry. We use this model to investigate a wide range of impactor masses and compositions, including those based on observations of Solar System comets, and interstellar ices (with JWST). We find that even a small impactor (R = 2.5 km) can lead to significant short-term changes in the atmospheric chemistry, including a factor $>10$ enhancement in H$_2$O, CO, CO$_2$ abundances, and atmospheric opacity more generally, and the near complete removal of observable hydrocarbons, such as CH$_4$, from the upper atmosphere. These effects scale with the change in atmospheric C/O ratio and metallicity. Potentially observable changes are possible for a body that has undergone significant/continuous bombardment, such that the global atmospheric chemistry has been impacted. Our works reveals that cometary impacts can significantly alter or pollute the atmospheric composition/chemistry of hot Jupiters. These changes have the potential to mute/break the proposed link between atmospheric C/O ratio and planet formation location relative to key snowlines in the natal protoplanetary disc.","sentences":["Impacts from icy and rocky bodies have helped shape the composition of solar system objects, for example the Earth-Moon system, or the recent impact of comet Shoemaker-Levy 9 with Jupiter.","It is likely that such impacts also shape the composition of exoplanetary systems.","Here we investigate how cometary impacts might affect the atmospheric composition/chemistry of hot Jupiters, which are prime targets for characterisation.","We introduce a parametrised cometary impact model that includes thermal ablation and pressure driven breakup, which we couple with the 1D `radiative-convective' atmospheric model ATMO, including disequilibrium chemistry.","We use this model to investigate a wide range of impactor masses and compositions, including those based on observations of Solar System comets, and interstellar ices (with JWST).","We find that even a small impactor (R = 2.5 km) can lead to significant short-term changes in the atmospheric chemistry, including a factor $>10$ enhancement in H$_2$O, CO, CO$_2$ abundances, and atmospheric opacity more generally, and the near complete removal of observable hydrocarbons, such as CH$_4$, from the upper atmosphere.","These effects scale with the change in atmospheric C/O ratio and metallicity.","Potentially observable changes are possible for a body that has undergone significant/continuous bombardment, such that the global atmospheric chemistry has been impacted.","Our works reveals that cometary impacts can significantly alter or pollute the atmospheric composition/chemistry of hot Jupiters.","These changes have the potential to mute/break the proposed link between atmospheric C/O ratio and planet formation location relative to key snowlines in the natal protoplanetary disc."],"url":"http://arxiv.org/abs/2402.05509v1","category":"astro-ph.EP"}
{"created":"2024-02-08 09:21:29","title":"On the type of ill-posedness of generalized Hilbert matrices and related operators","abstract":"We consider infinite-dimensional generalized Hilbert matrices of the form $H_{i,j} = \\frac{d_i d_j}{x_i + x_j}$, where $d_i$ are nonnegative weights and $x_i$ are pairwise disjoint positive numbers. We state sufficient and, for monotonically rearrangeable $x_i$, also necessary conditions for $d_i$, $x_i$ such that the induced operator from $\\ell^2 \\to \\ell^2$ and related operators are well-defined, bounded, or compact. Furthermore, we give conditions, when this operator is injective and ill-posed.","sentences":["We consider infinite-dimensional generalized Hilbert matrices of the form $H_{i,j} = \\frac{d_i d_j}{x_i + x_j}$, where $d_i$ are nonnegative weights and $x_i$ are pairwise disjoint positive numbers.","We state sufficient and, for monotonically rearrangeable $x_i$, also necessary conditions for $d_i$, $x_i$ such that the induced operator from $\\ell^2 \\to \\ell^2$ and related operators are well-defined, bounded, or compact.","Furthermore, we give conditions, when this operator is injective and ill-posed."],"url":"http://arxiv.org/abs/2402.05503v1","category":"math.FA"}
{"created":"2024-02-08 09:07:38","title":"Heart disease risk prediction using deep learning techniques with feature augmentation","abstract":"Cardiovascular diseases state as one of the greatest risks of death for the general population. Late detection in heart diseases highly conditions the chances of survival for patients. Age, sex, cholesterol level, sugar level, heart rate, among other factors, are known to have an influence on life-threatening heart problems, but, due to the high amount of variables, it is often difficult for an expert to evaluate each patient taking this information into account. In this manuscript, the authors propose using deep learning methods, combined with feature augmentation techniques for evaluating whether patients are at risk of suffering cardiovascular disease. The results of the proposed methods outperform other state of the art methods by 4.4%, leading to a precision of a 90%, which presents a significant improvement, even more so when it comes to an affliction that affects a large population.","sentences":["Cardiovascular diseases state as one of the greatest risks of death for the general population.","Late detection in heart diseases highly conditions the chances of survival for patients.","Age, sex, cholesterol level, sugar level, heart rate, among other factors, are known to have an influence on life-threatening heart problems, but, due to the high amount of variables, it is often difficult for an expert to evaluate each patient taking this information into account.","In this manuscript, the authors propose using deep learning methods, combined with feature augmentation techniques for evaluating whether patients are at risk of suffering cardiovascular disease.","The results of the proposed methods outperform other state of the art methods by 4.4%, leading to a precision of a 90%, which presents a significant improvement, even more so when it comes to an affliction that affects a large population."],"url":"http://arxiv.org/abs/2402.05495v1","category":"cs.LG"}
{"created":"2024-02-08 18:56:18","title":"Understanding Social Immunity in Ants: A Markovian Approach to Collective Cleaning Strategies","abstract":"Understanding social immunity mechanisms in ant colonies remains crucial to comprehending the evolution of defense strategies in eusocial organisms. This study assumes the absence of the role of memory in the ants' defense strategy, considering that they can make a new exploration of collective cleaning behaviors. We investigate how worker interactions and previous behaviors influence the evolution of social immunity strategies in response to the presence of pathogens and vulnerable colony members. In this context, with the application of Markov transition models it was possible to describe changes in cleaning behavior over time influenced by the presence of vulnerable members and treatment with an entomopathogenic fungus. We found a significant effect on ant behavior when exposed to \\it{Metarhizium anisopliae} when a fungus garden fragment and one larva were present. Our findings confirm a link between prophylactic cleaning and the presence of vulnerable members. Remarkably, distinct behaviors and transition times vary among treatments, revealing workers' adaptability to threats. Allogrooming shows adaptive changes when exposed to pathogens, potentially affecting pathogen transmission. In addition, our study elucidates the intricate interaction between internal and external factors shaping worker behavior. The influence of environmental context on decision-making principles emerges, emphasizing the importance of both intrinsic colony organization and external threats. By using a Markov model to understand ant hygiene behavior, we offer insights into social immunity mechanisms in eusocial organisms. Deciphering collective cleaning strategies can aid in understanding disease dynamics and decision-making processes in complex societies.","sentences":["Understanding social immunity mechanisms in ant colonies remains crucial to comprehending the evolution of defense strategies in eusocial organisms.","This study assumes the absence of the role of memory in the ants' defense strategy, considering that they can make a new exploration of collective cleaning behaviors.","We investigate how worker interactions and previous behaviors influence the evolution of social immunity strategies in response to the presence of pathogens and vulnerable colony members.","In this context, with the application of Markov transition models it was possible to describe changes in cleaning behavior over time influenced by the presence of vulnerable members and treatment with an entomopathogenic fungus.","We found a significant effect on ant behavior when exposed to \\it{Metarhizium anisopliae} when a fungus garden fragment and one larva were present.","Our findings confirm a link between prophylactic cleaning and the presence of vulnerable members.","Remarkably, distinct behaviors and transition times vary among treatments, revealing workers' adaptability to threats.","Allogrooming shows adaptive changes when exposed to pathogens, potentially affecting pathogen transmission.","In addition, our study elucidates the intricate interaction between internal and external factors shaping worker behavior.","The influence of environmental context on decision-making principles emerges, emphasizing the importance of both intrinsic colony organization and external threats.","By using a Markov model to understand ant hygiene behavior, we offer insights into social immunity mechanisms in eusocial organisms.","Deciphering collective cleaning strategies can aid in understanding disease dynamics and decision-making processes in complex societies."],"url":"http://arxiv.org/abs/2402.05924v1","category":"q-bio.QM"}
{"created":"2024-02-08 18:09:17","title":"Federated Offline Reinforcement Learning: Collaborative Single-Policy Coverage Suffices","abstract":"Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive. This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents. Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL. FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term. Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual agents, as long as the local datasets collectively cover the state-action space visited by the optimal policy, highlighting the power of collaboration in the federated setting. In fact, the sample complexity almost matches that of the single-agent counterpart, as if all the data are stored at a central location, up to polynomial factors of the horizon length. Furthermore, FedLCB-Q is communication-efficient, where the number of communication rounds is only linear with respect to the horizon length up to logarithmic factors.","sentences":["Offline reinforcement learning (RL), which seeks to learn an optimal policy using offline data, has garnered significant interest due to its potential in critical applications where online data collection is infeasible or expensive.","This work explores the benefit of federated learning for offline RL, aiming at collaboratively leveraging offline datasets at multiple agents.","Focusing on finite-horizon episodic tabular Markov decision processes (MDPs), we design FedLCB-Q, a variant of the popular model-free Q-learning algorithm tailored for federated offline RL.","FedLCB-Q updates local Q-functions at agents with novel learning rate schedules and aggregates them at a central server using importance averaging and a carefully designed pessimistic penalty term.","Our sample complexity analysis reveals that, with appropriately chosen parameters and synchronization schedules, FedLCB-Q achieves linear speedup in terms of the number of agents without requiring high-quality datasets at individual agents, as long as the local datasets collectively cover the state-action space visited by the optimal policy, highlighting the power of collaboration in the federated setting.","In fact, the sample complexity almost matches that of the single-agent counterpart, as if all the data are stored at a central location, up to polynomial factors of the horizon length.","Furthermore, FedLCB-Q is communication-efficient, where the number of communication rounds is only linear with respect to the horizon length up to logarithmic factors."],"url":"http://arxiv.org/abs/2402.05876v1","category":"cs.LG"}
{"created":"2024-02-08 17:42:34","title":"The performance of missing transverse momentum reconstruction and its significance with the ATLAS detector using 140 fb$^{-1}$ of $\\sqrt{s}=13$ TeV $pp$ collisions","abstract":"This paper presents the reconstruction of missing transverse momentum ($p_{\\text{T}}^{\\text{miss}}$) in proton-proton collisions, at a center-of-mass energy of 13 TeV. This is a challenging task involving many detector inputs, combining fully calibrated electrons, muons, photons, hadronically decaying $\\tau$-leptons, hadronic jets, and soft activity from remaining tracks. Possible double counting of momentum is avoided by applying a signal ambiguity resolution procedure which rejects detector inputs that have already been used. Several $p_{\\text{T}}^{\\text{miss}}$ `working points' are defined with varying stringency of selections, the tightest improving the resolution at high pile-up by up to 30% compared to the loosest. The $p_{\\text{T}}^{\\text{miss}}$ performance is evaluated using data and Monte Carlo simulation, with an emphasis on understanding the impact of pile-up, primarily using events consistent with leptonic $Z$ decays. The studies use $140~\\text{fb}^{-1}$ of data, collected by the ATLAS experiment at the Large Hadron Collider between 2015 and 2018. The results demonstrate that $p_{\\text{T}}^{\\text{miss}}$ reconstruction, and its associated significance, are well understood and reliably modelled by simulation. Finally, the systematic uncertainties on the soft $p_{\\text{T}}^{\\text{miss}}$ component are calculated. After various improvements the scale and resolution uncertainties are reduced by up to 76% and 51%, respectively, compared to the previous calculation at a lower luminosity.","sentences":["This paper presents the reconstruction of missing transverse momentum ($p_{\\text{T}}^{\\text{miss}}$) in proton-proton collisions, at a center-of-mass energy of 13 TeV. This is a challenging task involving many detector inputs, combining fully calibrated electrons, muons, photons, hadronically decaying $\\tau$-leptons, hadronic jets, and soft activity from remaining tracks.","Possible double counting of momentum is avoided by applying a signal ambiguity resolution procedure which rejects detector inputs that have already been used.","Several $p_{\\text{T}}^{\\text{miss}}$ `working points' are defined with varying stringency of selections, the tightest improving the resolution at high pile-up by up to 30% compared to the loosest.","The $p_{\\text{T}}^{\\text{miss}}$ performance is evaluated using data and Monte Carlo simulation, with an emphasis on understanding the impact of pile-up, primarily using events consistent with leptonic $Z$ decays.","The studies use $140~\\text{fb}^{-1}$ of data, collected by the ATLAS experiment at the Large Hadron Collider between 2015 and 2018.","The results demonstrate that $p_{\\text{T}}^{\\text{miss}}$ reconstruction, and its associated significance, are well understood and reliably modelled by simulation.","Finally, the systematic uncertainties on the soft $p_{\\text{T}}^{\\text{miss}}$ component are calculated.","After various improvements the scale and resolution uncertainties are reduced by up to 76% and 51%, respectively, compared to the previous calculation at a lower luminosity."],"url":"http://arxiv.org/abs/2402.05858v1","category":"hep-ex"}
{"created":"2024-02-08 17:04:04","title":"Loss induced collective mode in one-dimensional Bose gases","abstract":"We show that two-body losses induce a collective excitation in a harmonically trapped one-dimensional Bose gas, even in the absence of a quench in the trap or any other external perturbation. Focusing on the dissipatively fermionized regime, we perform an exact mode expansion of the rapidity distribution function and characterize the emergence of the collective motion. We find clear coherent oscillations in both the potential and kinetic energies as well as in the phase space quadrupole mode of the gas. We also discuss how this loss induced collective mode differs from the well known breathing mode studied in the absence of dissipation.","sentences":["We show that two-body losses induce a collective excitation in a harmonically trapped one-dimensional Bose gas, even in the absence of a quench in the trap or any other external perturbation.","Focusing on the dissipatively fermionized regime, we perform an exact mode expansion of the rapidity distribution function and characterize the emergence of the collective motion.","We find clear coherent oscillations in both the potential and kinetic energies as well as in the phase space quadrupole mode of the gas.","We also discuss how this loss induced collective mode differs from the well known breathing mode studied in the absence of dissipation."],"url":"http://arxiv.org/abs/2402.05824v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-08 15:48:58","title":"OGLE GD-CEP-0516: the most metal-poor lithium-rich Galactic Cepheid","abstract":"Classical Cepheids (DCEPs) are important astrophysical objects not only as standard candles for the determination of the cosmic distance ladder but also as a test-bed for the stellar evolution theory, thanks to the connection between their pulsation (periods, amplitudes) and stellar (luminosity, mass, effective temperature, metallicity) parameters. We aim to unveil the nature of the Galactic DCEP OGLE-GD-CEP-0516 and other DCEPs showing an enhanced abundance of lithium in their atmospheres. We have collected a high-resolution spectrum for OGLE-GD-CEP-0516 with UVES@VLT. Accurate stellar parameters: effective temperature, gravity, micro- and macro-turbulence, radial velocity, and metal abundances were measured for this star, using spectral synthesis techniques based on LTE plane-parallel atmospheric model. We measured a chemical pattern with most elements under-abundant compared with the Sun, i.e. [Fe/H]\\,=\\,$-$0.54\\,$\\pm$\\,0.16~dex, [C/H]\\,=\\,$-$0.45\\,$\\pm$\\,0.05~dex, or [Mg/H]\\,=\\,$-$0.40\\,$\\pm$\\,0.16~dex among others. In particular, we measured a lithium abundance A(Li)\\,=\\,3.06\\,$\\pm$\\,0.10~dex for OGLE-GD-CEP-0516, which makes this object the sixth Li-rich object among the Milky Way DCEPs. Our results favour the scenario in which the six Galactic Li-rich DCEPs are first-crossing the instability strip having had slowly-rotating progenitors during their main sequence phase. This study explored the link between lithium abundance and the pulsation period in classical Cepheids. It found that brighter Cepheids, indicative of higher mass, show enhanced lithium abundance, contrary to predictions from evolutionary models considering rotation. Additionally, an analysis of lithium abundance versus [Fe/H] revealed a lack of significant correlation, contradicting expectations from galactic chemical evolution (GCE) models.","sentences":["Classical Cepheids (DCEPs) are important astrophysical objects not only as standard candles for the determination of the cosmic distance ladder but also as a test-bed for the stellar evolution theory, thanks to the connection between their pulsation (periods, amplitudes) and stellar (luminosity, mass, effective temperature, metallicity) parameters.","We aim to unveil the nature of the Galactic DCEP OGLE-GD-CEP-0516 and other DCEPs showing an enhanced abundance of lithium in their atmospheres.","We have collected a high-resolution spectrum for OGLE-GD-CEP-0516 with UVES@VLT.","Accurate stellar parameters: effective temperature, gravity, micro- and macro-turbulence, radial velocity, and metal abundances were measured for this star, using spectral synthesis techniques based on LTE plane-parallel atmospheric model.","We measured a chemical pattern with most elements under-abundant compared with the Sun, i.e. [Fe/H]\\,=\\,$-$0.54\\,$\\pm$\\,0.16~dex, [C/H]\\,=\\,$-$0.45\\,$\\pm$\\,0.05~dex, or [Mg/H]\\,=\\,$-$0.40\\,$\\pm$\\,0.16~dex among others.","In particular, we measured a lithium abundance A(Li)\\,=\\,3.06\\,$\\pm$\\,0.10~dex for OGLE-GD-CEP-0516, which makes this object the sixth Li-rich object among the Milky Way DCEPs.","Our results favour the scenario in which the six Galactic Li-rich DCEPs are first-crossing the instability strip having had slowly-rotating progenitors during their main sequence phase.","This study explored the link between lithium abundance and the pulsation period in classical Cepheids.","It found that brighter Cepheids, indicative of higher mass, show enhanced lithium abundance, contrary to predictions from evolutionary models considering rotation.","Additionally, an analysis of lithium abundance versus [Fe/H] revealed a lack of significant correlation, contradicting expectations from galactic chemical evolution (GCE) models."],"url":"http://arxiv.org/abs/2402.05762v1","category":"astro-ph.SR"}
{"created":"2024-02-08 15:09:46","title":"First operation of a multi-channel Q-Pix prototype: measuring transverse electron diffusion in a gas time projection chamber","abstract":"We report measurements of the transverse diffusion of electrons in P-10 gas (90% Ar, 10% CH4) in a laboratory-scale time projection chamber (TPC) utilizing a novel pixelated signal capture and digitization technique known as Q-Pix. The Q-Pix method incorporates a precision switched integrating transimpedance amplifier whose output is compared to a threshold voltage. Upon reaching the threshold, a comparator sends a 'reset' signal, initiating a discharge of the integrating capacitor. The time difference between successive resets is inversely proportional to the average current at the pixel in that time interval, and the number of resets is directly proportional to the total collected charge. We developed a 16-channel Q-Pix prototype fabricated from commercial off-the-shelf components and coupled them to 16 concentric annular anode electrodes to measure the spatial extent of the electron swarm that reaches the anode after drifting through the uniform field of the TPC. The swarm is produced at a gold photocathode using pulsed UV light. The measured transverse diffusion agrees with simulations in PyBoltz across a range of operating pressures (200-1500 Torr). These results demonstrate that a Q-Pix readout can successfully reconstruct the ionization topology in a TPC.","sentences":["We report measurements of the transverse diffusion of electrons in P-10 gas (90% Ar, 10% CH4) in a laboratory-scale time projection chamber (TPC) utilizing a novel pixelated signal capture and digitization technique known as Q-Pix.","The Q-Pix method incorporates a precision switched integrating transimpedance amplifier whose output is compared to a threshold voltage.","Upon reaching the threshold, a comparator sends a 'reset' signal, initiating a discharge of the integrating capacitor.","The time difference between successive resets is inversely proportional to the average current at the pixel in that time interval, and the number of resets is directly proportional to the total collected charge.","We developed a 16-channel Q-Pix prototype fabricated from commercial off-the-shelf components and coupled them to 16 concentric annular anode electrodes to measure the spatial extent of the electron swarm that reaches the anode after drifting through the uniform field of the TPC.","The swarm is produced at a gold photocathode using pulsed UV light.","The measured transverse diffusion agrees with simulations in PyBoltz across a range of operating pressures (200-1500 Torr).","These results demonstrate that a Q-Pix readout can successfully reconstruct the ionization topology in a TPC."],"url":"http://arxiv.org/abs/2402.05734v1","category":"hep-ex"}
{"created":"2024-02-08 12:11:24","title":"An Implementation for Dynamic Application Allocation in Shared Sensor Networks","abstract":"We present a system architecture implementation to perform dynamic application allocation in shared sensor networks, where highly integrated wireless sensor systems are used to support multiple applications. The architecture is based on a central controller that collects the received data from the sensor nodes, dynamically decides which applications must be simultaneously deployed in each node and, accordingly, over-the-air reprograms the sensor nodes. Waspmote devices are used as sensor nodes that communicate with the controller using ZigBee protocol. Experimental results show the viability of the proposal.","sentences":["We present a system architecture implementation to perform dynamic application allocation in shared sensor networks, where highly integrated wireless sensor systems are used to support multiple applications.","The architecture is based on a central controller that collects the received data from the sensor nodes, dynamically decides which applications must be simultaneously deployed in each node and, accordingly, over-the-air reprograms the sensor nodes.","Waspmote devices are used as sensor nodes that communicate with the controller using ZigBee protocol.","Experimental results show the viability of the proposal."],"url":"http://arxiv.org/abs/2402.05611v1","category":"cs.NI"}
{"created":"2024-02-08 12:08:51","title":"Optimization of a portable liquid scintillation counting device for determining 222Rn in water","abstract":"The new EU Council Directive 2013/51/Euratom of 22 October 2013 introduced limits for the content of 222Rn in drinking water. Radon analysis in water requires a lengthy task of collection, storage, transport and subsequent measurement in a laboratory. A portable liquid scintillation counting device allows rapid sampling with significant savings of time, space, and cost compared with the commonly used techniques of gamma spectrometry or methods based on the desorption of radon dissolved in water. In this study, we describe a calibration procedure for a portable liquid scintillation counting device that allows measurements of 222Rn in water by the direct method, and we also consider the case of 226Ra being present in the sample. The results obtained with this portable device are compared with those obtained by standard laboratory techniques (gamma spectrometry with a high-purity Ge detector, gamma spectrometry with a NaI detector, and desorption followed by ionization chamber detection).","sentences":["The new EU Council Directive 2013/51/Euratom of 22 October 2013 introduced limits for the content of 222Rn in drinking water.","Radon analysis in water requires a lengthy task of collection, storage, transport and subsequent measurement in a laboratory.","A portable liquid scintillation counting device allows rapid sampling with significant savings of time, space, and cost compared with the commonly used techniques of gamma spectrometry or methods based on the desorption of radon dissolved in water.","In this study, we describe a calibration procedure for a portable liquid scintillation counting device that allows measurements of 222Rn in water by the direct method, and we also consider the case of 226Ra being present in the sample.","The results obtained with this portable device are compared with those obtained by standard laboratory techniques (gamma spectrometry with a high-purity Ge detector, gamma spectrometry with a NaI detector, and desorption followed by ionization chamber detection)."],"url":"http://arxiv.org/abs/2402.05609v1","category":"physics.ins-det"}
{"created":"2024-02-08 12:04:59","title":"A Learning-based Model Predictive Control Scheme with Application to Temperature Control Units","abstract":"Temperature control is a complex task due to its often unknown dynamics and disturbances. This paper explores the use of Neural Nonlinear AutoRegressive eXogenous (NNARX) models for nonlinear system identification and model predictive control of a temperature control unit. First, the NNARX model is identified from input-output data collected from the real plant, and a state-space representation with known measurable states consisting of past input and output variables is formulated. Second, a tailored model predictive controller is designed based on the trained NNARX network. The proposed control architecture is experimentally tested on the temperature control units manufactured by Tool-Temp AG. The results achieved are compared with those obtained using a PI controller and a linear MPC. The findings illustrate that the proposed scheme achieves satisfactory tracking performance while incurring the lowest energy cost among the compared controllers.","sentences":["Temperature control is a complex task due to its often unknown dynamics and disturbances.","This paper explores the use of Neural Nonlinear AutoRegressive eXogenous (NNARX) models for nonlinear system identification and model predictive control of a temperature control unit.","First, the NNARX model is identified from input-output data collected from the real plant, and a state-space representation with known measurable states consisting of past input and output variables is formulated.","Second, a tailored model predictive controller is designed based on the trained NNARX network.","The proposed control architecture is experimentally tested on the temperature control units manufactured by Tool-Temp AG.","The results achieved are compared with those obtained using a PI controller and a linear MPC.","The findings illustrate that the proposed scheme achieves satisfactory tracking performance while incurring the lowest energy cost among the compared controllers."],"url":"http://arxiv.org/abs/2402.05606v1","category":"eess.SY"}
{"created":"2024-02-08 11:51:24","title":"Neural operators meet conjugate gradients: The FCG-NO method for efficient PDE solving","abstract":"Deep learning solvers for partial differential equations typically have limited accuracy. We propose to overcome this problem by using them as preconditioners. More specifically, we apply discretization-invariant neural operators to learn preconditioners for the flexible conjugate gradient method (FCG). Architecture paired with novel loss function and training scheme allows for learning efficient preconditioners that can be used across different resolutions. On the theoretical side, FCG theory allows us to safely use nonlinear preconditioners that can be applied in $O(N)$ operations without constraining the form of the preconditioners matrix. To justify learning scheme components (the loss function and the way training data is collected) we perform several ablation studies. Numerical results indicate that our approach favorably compares with classical preconditioners and allows to reuse of preconditioners learned for lower resolution to the higher resolution data.","sentences":["Deep learning solvers for partial differential equations typically have limited accuracy.","We propose to overcome this problem by using them as preconditioners.","More specifically, we apply discretization-invariant neural operators to learn preconditioners for the flexible conjugate gradient method (FCG).","Architecture paired with novel loss function and training scheme allows for learning efficient preconditioners that can be used across different resolutions.","On the theoretical side, FCG theory allows us to safely use nonlinear preconditioners that can be applied in $O(N)$ operations without constraining the form of the preconditioners matrix.","To justify learning scheme components (the loss function and the way training data is collected) we perform several ablation studies.","Numerical results indicate that our approach favorably compares with classical preconditioners and allows to reuse of preconditioners learned for lower resolution to the higher resolution data."],"url":"http://arxiv.org/abs/2402.05598v1","category":"math.NA"}
{"created":"2024-02-08 11:16:13","title":"Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)-Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study","abstract":"Background: Eating disorders are increasingly prevalent, and social networks offer valuable information.   Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders.   Methods: Over three months, we collected tweets about eating disorders. A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content. Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time.   Results: From 1,058,957 collected tweets, transformer-based bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories.   Conclusions: Transformer-based models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources.","sentences":["Background: Eating disorders are increasingly prevalent, and social networks offer valuable information.   ","Objective: Our goal was to identify efficient machine learning models for categorizing tweets related to eating disorders.   ","Methods: Over three months, we collected tweets about eating disorders.","A 2,000-tweet subset was labeled for: (1) being written by individuals with eating disorders, (2) promoting eating disorders, (3) informativeness, and (4) scientific content.","Both traditional machine learning and deep learning models were employed for classification, assessing accuracy, F1 score, and computational time.   ","Results: From 1,058,957 collected tweets, transformer-based bidirectional encoder representations achieved the highest F1 scores (71.1%-86.4%) across all four categories.   ","Conclusions: Transformer-based models outperform traditional techniques in classifying eating disorder-related tweets, though they require more computational resources."],"url":"http://arxiv.org/abs/2402.05571v1","category":"cs.CL"}
{"created":"2024-02-08 10:43:55","title":"One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning","abstract":"Objective: Ultrasound (US) examination has unique advantages in diagnosing carpal tunnel syndrome (CTS) while identifying the median nerve (MN) and diagnosing CTS depends heavily on the expertise of examiners. To alleviate this problem, we aimed to develop a one-stop automated CTS diagnosis system (OSA-CTSD) and evaluate its effectiveness as a computer-aided diagnostic tool. Methods: We combined real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis into a unified framework, called OSA-CTSD. We collected a total of 32,301 static images from US videos of 90 normal wrists and 40 CTS wrists for evaluation using a simplified scanning protocol. Results: The proposed model showed better segmentation and measurement performance than competing methods, reporting that HD95 score of 7.21px, ASSD score of 2.64px, Dice score of 85.78%, and IoU score of 76.00%, respectively. In the reader study, it demonstrated comparable performance with the average performance of the experienced in classifying the CTS, while outperformed that of the inexperienced radiologists in terms of classification metrics (e.g., accuracy score of 3.59% higher and F1 score of 5.85% higher). Conclusion: The OSA-CTSD demonstrated promising diagnostic performance with the advantages of real-time, automation, and clinical interpretability. The application of such a tool can not only reduce reliance on the expertise of examiners, but also can help to promote the future standardization of the CTS diagnosis process, benefiting both patients and radiologists.","sentences":["Objective: Ultrasound (US) examination has unique advantages in diagnosing carpal tunnel syndrome (CTS) while identifying the median nerve (MN) and diagnosing CTS depends heavily on the expertise of examiners.","To alleviate this problem, we aimed to develop a one-stop automated CTS diagnosis system (OSA-CTSD) and evaluate its effectiveness as a computer-aided diagnostic tool.","Methods: We combined real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis into a unified framework, called OSA-CTSD.","We collected a total of 32,301 static images from US videos of 90 normal wrists and 40 CTS wrists for evaluation using a simplified scanning protocol.","Results:","The proposed model showed better segmentation and measurement performance than competing methods, reporting that HD95 score of 7.21px, ASSD score of 2.64px, Dice score of 85.78%, and IoU score of 76.00%, respectively.","In the reader study, it demonstrated comparable performance with the average performance of the experienced in classifying the CTS, while outperformed that of the inexperienced radiologists in terms of classification metrics (e.g., accuracy score of 3.59% higher and F1 score of 5.85% higher).","Conclusion: The OSA-CTSD demonstrated promising diagnostic performance with the advantages of real-time, automation, and clinical interpretability.","The application of such a tool can not only reduce reliance on the expertise of examiners, but also can help to promote the future standardization of the CTS diagnosis process, benefiting both patients and radiologists."],"url":"http://arxiv.org/abs/2402.05554v1","category":"eess.IV"}
{"created":"2024-02-08 10:06:45","title":"Measurement of the Branching Fraction of $B^{0} \\rightarrow J/\u03c8 \u03c0^{0}$ Decays","abstract":"The ratio of branching fractions between $B^{0} \\rightarrow J/\\psi \\pi^{0}$ and $B^{+} \\rightarrow J/\\psi K^{*+}$ decays is measured with proton-proton collision data collected by the LHCb experiment, corresponding to an integrated luminosity of 9 fb$^{-1}$. The measured value is $\\frac{\\mathcal{B}_{B^{0} \\rightarrow J/\\psi \\pi^{0}}}{\\mathcal{B}_{B^{+} \\rightarrow J/\\psi K^{*+}}} = (1.153 \\pm 0.053 \\pm 0.048 ) \\times 10^{-2}$, where the first uncertainty is statistical and the second is systematic. The branching fraction for $B^{0} \\rightarrow J/\\psi \\pi^{0}$ decays is determined using the branching fraction of the normalisation channel, resulting in $\\mathcal{B}_{B^{0} \\rightarrow J/\\psi \\pi^{0}} = (1.670 \\pm 0.077 \\pm 0.069 \\pm 0.095) \\times 10^{-5}$, where the last uncertainty corresponds to that of the external input. This result is consistent with the current world average value and competitive with the most precise single measurement to date.","sentences":["The ratio of branching fractions between $B^{0} \\rightarrow J/\\psi \\pi^{0}$ and $B^{+} \\rightarrow J/\\psi K^{*+}$ decays is measured with proton-proton collision data collected by the LHCb experiment, corresponding to an integrated luminosity of 9 fb$^{-1}$. The measured value is $\\frac{\\mathcal{B}_{B^{0} \\rightarrow J/\\psi \\pi^{0}}}{\\mathcal{B}_{B^{+} \\rightarrow J/\\psi K^{*+}}} = (1.153 \\pm 0.053 \\pm 0.048 )","\\times 10^{-2}$, where the first uncertainty is statistical and the second is systematic.","The branching fraction for $B^{0} \\rightarrow J/\\psi \\pi^{0}$ decays is determined using the branching fraction of the normalisation channel, resulting in $\\mathcal{B}_{B^{0} \\rightarrow J/\\psi \\pi^{0}} = (1.670 \\pm 0.077 \\pm 0.069 \\pm 0.095)","\\times 10^{-5}$, where the last uncertainty corresponds to that of the external input.","This result is consistent with the current world average value and competitive with the most precise single measurement to date."],"url":"http://arxiv.org/abs/2402.05528v1","category":"hep-ex"}
{"created":"2024-02-08 10:03:38","title":"Observation of the $B_c^+ \\to J/\u03c8\u03c0^+ \u03c0^0$ decay","abstract":"The first observation of the $B_c^+ \\to J/\\psi \\pi^+ \\pi^0$ decay is reported with high significance using proton-proton collision data, corresponding to an integrated luminosity of 9fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of 7, 8, and 13 TeV. The ratio of its branching fraction relative to the $B_c^+ \\to J/\\psi \\pi^+$ channel is measured to be   $$   \\frac{ {\\cal{B}}( B_c^+ \\to J/\\psi \\pi^+\\pi^0 ) }   { {\\cal{B}}( B_c^+ \\to J/\\psi \\pi^+ ) }   = 2.80 \\pm 0.15 \\pm 0.11 \\pm 0.16 \\,,   $$ where the first uncertainty is statistical, the second systematic and the third related to imprecise knowledge of the branching fractions for $B^+ \\to J/\\psi K^{*+}$ and $B^+ \\to J/\\psi K^+$ decays, which are used to determine the $\\pi^0$ detection efficiency. The $\\pi^+\\pi^0$ mass spectrum is found to be consistent with the dominance of an intermediate $\\rho^+$ contribution in accordance with a model based on QCD factorisation.","sentences":["The first observation of the $B_c^+ \\to J/\\psi \\pi^+ \\pi^0$ decay is reported with high significance using proton-proton collision data, corresponding to an integrated luminosity of 9fb$^{-1}$, collected with the LHCb detector at centre-of-mass energies of 7, 8, and 13 TeV. The ratio of its branching fraction relative to the $B_c^+ \\to J/\\psi \\pi^+$ channel is measured to be   $$   \\frac{ {\\cal{B}}( B_c^+ \\to J/\\psi \\pi^+\\pi^0 ) }   { {\\cal{B}}( B_c^+ \\to J/\\psi \\pi^+ ) }   = 2.80 \\pm 0.15 \\pm 0.11 \\pm 0.16 \\,,   $$ where the first uncertainty is statistical, the second systematic and the third related to imprecise knowledge of the branching fractions for $B^+ \\to J/\\psi K^{*+}$ and $B^+ \\to J/\\psi K^+$ decays, which are used to determine the $\\pi^0$ detection efficiency.","The $\\pi^+\\pi^0$ mass spectrum is found to be consistent with the dominance of an intermediate $\\rho^+$ contribution in accordance with a model based on QCD factorisation."],"url":"http://arxiv.org/abs/2402.05523v1","category":"hep-ex"}
{"created":"2024-02-08 09:09:03","title":"A Solution for Commercializing, Decentralizing and Storing Electronic Medical Records by Integrating Proxy Re-Encryption, IPFS, and Blockchain","abstract":"The rapid expansion of user medical records across global systems presents not only opportunities but also new challenges in maintaining effective application models that ensure user privacy, controllability, and the ability to commercialize patient medical records. Moreover, the proliferation of data analysis models in healthcare institutions necessitates the decentralization and restorability of medical record data. It is imperative that user medical data collected from these systems can be easily analyzed and utilized even years after collection, without the risk of data loss due to numerous factors. Additionally, medical information must be authorized by the data owner, granting patients the right to accept or decline data usage requests from medical research agencies. In response, we propose an innovative solution for implementing a decentralized system utilizing an EVM-compatible blockchain and IPFS for decentralized storage. To ensure privacy and control, we employ Proxy Re-Encryption (PRE), a cryptographic authorized method, within the medical data marketplace. Our proposed architecture significantly reduces costs associated with granting read access to healthcare research agencies by minimizing the encryption and decryption time of stored records. Furthermore, it empowers users with enhanced control over their health data through tamperproof blockchain smart contracts and IPFS, safeguarding the integrity and privacy of their medical records.","sentences":["The rapid expansion of user medical records across global systems presents not only opportunities but also new challenges in maintaining effective application models that ensure user privacy, controllability, and the ability to commercialize patient medical records.","Moreover, the proliferation of data analysis models in healthcare institutions necessitates the decentralization and restorability of medical record data.","It is imperative that user medical data collected from these systems can be easily analyzed and utilized even years after collection, without the risk of data loss due to numerous factors.","Additionally, medical information must be authorized by the data owner, granting patients the right to accept or decline data usage requests from medical research agencies.","In response, we propose an innovative solution for implementing a decentralized system utilizing an EVM-compatible blockchain and IPFS for decentralized storage.","To ensure privacy and control, we employ Proxy Re-Encryption (PRE), a cryptographic authorized method, within the medical data marketplace.","Our proposed architecture significantly reduces costs associated with granting read access to healthcare research agencies by minimizing the encryption and decryption time of stored records.","Furthermore, it empowers users with enhanced control over their health data through tamperproof blockchain smart contracts and IPFS, safeguarding the integrity and privacy of their medical records."],"url":"http://arxiv.org/abs/2402.05498v1","category":"cs.CR"}
{"created":"2024-02-08 09:03:17","title":"Investigating White-Box Attacks for On-Device Models","abstract":"Numerous mobile apps have leveraged deep learning capabilities. However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps. Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies. This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms. Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks. To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies? We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model. Specifically, REOM first transforms compiled on-device models into Open Neural Network Exchange format, then removes the non-debuggable parts, and converts them to the debuggable DL models format that allows attackers to exploit in a white-box setting. Our experimental results show that our approach is effective in achieving automated transformation among 244 TFLite models. Compared with previous attacks using surrogate models, REOM enables attackers to achieve higher attack success rates with a hundred times smaller attack perturbations. In addition, because the ONNX platform has plenty of tools for model format exchanging, the proposed method based on the ONNX platform can be adapted to other model formats. Our findings emphasize the need for developers to carefully consider their model deployment strategies, and use white-box methods to evaluate the vulnerability of on-device models.","sentences":["Numerous mobile apps have leveraged deep learning capabilities.","However, on-device models are vulnerable to attacks as they can be easily extracted from their corresponding mobile apps.","Existing on-device attacking approaches only generate black-box attacks, which are far less effective and efficient than white-box strategies.","This is because mobile deep learning frameworks like TFLite do not support gradient computing, which is necessary for white-box attacking algorithms.","Thus, we argue that existing findings may underestimate the harmfulness of on-device attacks.","To this end, we conduct a study to answer this research question: Can on-device models be directly attacked via white-box strategies?","We first systematically analyze the difficulties of transforming the on-device model to its debuggable version, and propose a Reverse Engineering framework for On-device Models (REOM), which automatically reverses the compiled on-device TFLite model to the debuggable model.","Specifically, REOM first transforms compiled on-device models into Open Neural Network Exchange format, then removes the non-debuggable parts, and converts them to the debuggable DL models format that allows attackers to exploit in a white-box setting.","Our experimental results show that our approach is effective in achieving automated transformation among 244 TFLite models.","Compared with previous attacks using surrogate models, REOM enables attackers to achieve higher attack success rates with a hundred times smaller attack perturbations.","In addition, because the ONNX platform has plenty of tools for model format exchanging, the proposed method based on the ONNX platform can be adapted to other model formats.","Our findings emphasize the need for developers to carefully consider their model deployment strategies, and use white-box methods to evaluate the vulnerability of on-device models."],"url":"http://arxiv.org/abs/2402.05493v1","category":"cs.SE"}
{"created":"2024-02-08 08:46:50","title":"Multispecies bird sound recognition using a fully convolutional neural network","abstract":"This study proposes a method based on fully convolutional neural networks (FCNs) to identify migratory birds from their songs, with the objective of recognizing which birds pass through certain areas and at what time. To determine the best FCN architecture, extensive experimentation was conducted through a grid search, exploring the optimal depth, width, and activation function of the network. The results showed that the optimal number of filters is 400 in the widest layer, with 4 convolutional blocks with maxpooling and an adaptive activation function. The proposed FCN offers a significant advantage over other techniques, as it can recognize the sound of a bird in audio of any length with an accuracy greater than 85%. Furthermore, due to its architecture, the network can detect more than one species from audio and can carry out near-real-time sound recognition. Additionally, the proposed method is lightweight, making it ideal for deployment and use in IoT devices. The study also presents a comparative analysis of the proposed method against other techniques, demonstrating an improvement of over 67% in the best-case scenario. These findings contribute to advancing the field of bird sound recognition and provide valuable insights into the practical application of FCNs in real-world scenarios.","sentences":["This study proposes a method based on fully convolutional neural networks (FCNs) to identify migratory birds from their songs, with the objective of recognizing which birds pass through certain areas and at what time.","To determine the best FCN architecture, extensive experimentation was conducted through a grid search, exploring the optimal depth, width, and activation function of the network.","The results showed that the optimal number of filters is 400 in the widest layer, with 4 convolutional blocks with maxpooling and an adaptive activation function.","The proposed FCN offers a significant advantage over other techniques, as it can recognize the sound of a bird in audio of any length with an accuracy greater than 85%.","Furthermore, due to its architecture, the network can detect more than one species from audio and can carry out near-real-time sound recognition.","Additionally, the proposed method is lightweight, making it ideal for deployment and use in IoT devices.","The study also presents a comparative analysis of the proposed method against other techniques, demonstrating an improvement of over 67% in the best-case scenario.","These findings contribute to advancing the field of bird sound recognition and provide valuable insights into the practical application of FCNs in real-world scenarios."],"url":"http://arxiv.org/abs/2402.05489v1","category":"cs.SD"}
{"created":"2024-02-08 08:25:41","title":"Leveraging AI for Enhanced Software Effort Estimation: A Comprehensive Study and Framework Proposal","abstract":"This paper presents an extensive study on the application of AI techniques for software effort estimation in the past five years from 2017 to 2023. By overcoming the limitations of traditional methods, the study aims to improve accuracy and reliability. Through performance evaluation and comparison with diverse Machine Learning models, including Artificial Neural Network (ANN), Support Vector Machine (SVM), Linear Regression, Random Forest and other techniques, the most effective method is identified. The proposed AI-based framework holds the potential to enhance project planning and resource allocation, contributing to the research area of software project effort estimation.","sentences":["This paper presents an extensive study on the application of AI techniques for software effort estimation in the past five years from 2017 to 2023.","By overcoming the limitations of traditional methods, the study aims to improve accuracy and reliability.","Through performance evaluation and comparison with diverse Machine Learning models, including Artificial Neural Network (ANN), Support Vector Machine (SVM), Linear Regression, Random Forest and other techniques, the most effective method is identified.","The proposed AI-based framework holds the potential to enhance project planning and resource allocation, contributing to the research area of software project effort estimation."],"url":"http://arxiv.org/abs/2402.05484v1","category":"cs.SE"}
{"created":"2024-02-08 08:09:14","title":"Measurable entanglement criterion for extended Bose-Hubbard model","abstract":"We present an experimentally--accessible entanglement criterion witnessing collective entanglement among lattice sites. It can be measured through Raman scattering or time-of-flight expansion experiments. It witnesses, for instance, Mott insulator--superfluid and Mott insulator--charge density wave phase transitions impressively.","sentences":["We present an experimentally--accessible entanglement criterion witnessing collective entanglement among lattice sites.","It can be measured through Raman scattering or time-of-flight expansion experiments.","It witnesses, for instance, Mott insulator--superfluid and Mott insulator--charge density wave phase transitions impressively."],"url":"http://arxiv.org/abs/2402.05477v1","category":"quant-ph"}
{"created":"2024-02-08 08:04:14","title":"Assessing the atomic moment picture of spin dynamics: the perspective of \\textit{ab initio} magnon wavefunction","abstract":"Our understanding of collective spin fluctuation in materials relies largely on Heisenberg-type spin Hamiltonians. Implicit in these spin models is the atomic moment picture that in transverse spin dynamics the magnetization around an atom undergoes precessional motion as a rigid moment, which has been challenged by emerging theoretical and experimental advances.To assess the validity of the atomic moment picture in spin dynamics, however, necessitates magnon wavefunctions from \\textit{ab initio} methods without \\textit{a priori} spin models.To this end, we develop an efficient model-free {\\it ab initio} method for computing magnon spectrum and wavefunctions. Niu-Kleinman's adiabatic spin-wave dynamics is reformulated using linear perturbation theory into a generalized eigenvalue problem, which can be solved to produce magnon spectrum and wavefunctions without assuming atomic moments. We have implemented this method in the framework of density functional perturbation theory (DFPT). A dynamical extension of Niu-Kleinman equation of motion is proposed to improve inaccurate predicted magnon energies due to imperfect adiabaticity at higher energies. Based on so-obtained {\\it ab initio} magnon wavefunctions, we find the atomic moment picture to be valid in typical ferromagnets and antiferromagnets, but fails in the molecular orbital crystal Na$_2$IrO$_3$. Our results suggest that the usual spin Hamiltonian approach should be taken with a grain of salt, and possible experimental ramification on the issue is discussed.","sentences":["Our understanding of collective spin fluctuation in materials relies largely on Heisenberg-type spin Hamiltonians.","Implicit in these spin models is the atomic moment picture that in transverse spin dynamics the magnetization around an atom undergoes precessional motion as a rigid moment, which has been challenged by emerging theoretical and experimental advances.","To assess the validity of the atomic moment picture in spin dynamics, however, necessitates magnon wavefunctions from \\textit{ab initio} methods without \\textit{a priori} spin models.","To this end, we develop an efficient model-free {\\it ab initio} method for computing magnon spectrum and wavefunctions.","Niu-Kleinman's adiabatic spin-wave dynamics is reformulated using linear perturbation theory into a generalized eigenvalue problem, which can be solved to produce magnon spectrum and wavefunctions without assuming atomic moments.","We have implemented this method in the framework of density functional perturbation theory (DFPT).","A dynamical extension of Niu-Kleinman equation of motion is proposed to improve inaccurate predicted magnon energies due to imperfect adiabaticity at higher energies.","Based on so-obtained {\\it ab initio} magnon wavefunctions, we find the atomic moment picture to be valid in typical ferromagnets and antiferromagnets, but fails in the molecular orbital crystal Na$_2$IrO$_3$.","Our results suggest that the usual spin Hamiltonian approach should be taken with a grain of salt, and possible experimental ramification on the issue is discussed."],"url":"http://arxiv.org/abs/2402.05473v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-08 08:00:29","title":"Fast Transition-Aware Reconfiguration of Liquid Crystal-based RISs","abstract":"Liquid crystal (LC) technology offers a cost-effective, scalable, energy-efficient, and continuous phase tunable realization of extremely large reconfigurable intelligent surfaces (RISs). However, LC response time to achieve a desired differential phase is significantly higher compared to competing silicon-based technologies (RF switches, PIN diodes, etc). The slow response time can be the performance bottleneck for applications where frequent reconfiguration of the RIS (e.g., to serve different users) is needed. In this paper, we develop an RIS phase-shift design that is aware of the transition behavior and aims to minimize the time to switch among multiple RIS configurations each serving a mobile user in a time-division multiple-access (TDMA) protocol. Our simulation results confirm that the proposed algorithm significantly reduces the time required for the users to achieve a threshold signal quality. This leads to a considerable improvement in the achievable throughput for applications, where the length of the TDMA time intervals is comparable with the RIS reconfiguration time.","sentences":["Liquid crystal (LC) technology offers a cost-effective, scalable, energy-efficient, and continuous phase tunable realization of extremely large reconfigurable intelligent surfaces (RISs).","However, LC response time to achieve a desired differential phase is significantly higher compared to competing silicon-based technologies (RF switches, PIN diodes, etc).","The slow response time can be the performance bottleneck for applications where frequent reconfiguration of the RIS (e.g., to serve different users) is needed.","In this paper, we develop an RIS phase-shift design that is aware of the transition behavior and aims to minimize the time to switch among multiple RIS configurations each serving a mobile user in a time-division multiple-access (TDMA) protocol.","Our simulation results confirm that the proposed algorithm significantly reduces the time required for the users to achieve a threshold signal quality.","This leads to a considerable improvement in the achievable throughput for applications, where the length of the TDMA time intervals is comparable with the RIS reconfiguration time."],"url":"http://arxiv.org/abs/2402.05469v1","category":"eess.SP"}
{"created":"2024-02-08 07:56:49","title":"Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia","abstract":"Large Language Models (LLMs) have become prevalent across diverse sectors, transforming human life with their extraordinary reasoning and comprehension abilities. As they find increased use in sensitive tasks, safety concerns have gained widespread attention. Extensive efforts have been dedicated to aligning LLMs with human moral principles to ensure their safe deployment. Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content. The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle. In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the processes of the mind that occur without conscious awareness and the involuntary mimicry of actions, respectively. Evaluations across 6 open-source LLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success Rate of 91.5\\%, outperforming five current methods by up to 47.0\\% with an 8x reduction in overhead. Furthermore, it displays significant transferability and stealth, successfully evading established detection mechanisms. The code of our work is available at \\url{https://github.com/SolidShen/RIPPLE_official/tree/official}","sentences":["Large Language Models (LLMs) have become prevalent across diverse sectors, transforming human life with their extraordinary reasoning and comprehension abilities.","As they find increased use in sensitive tasks, safety concerns have gained widespread attention.","Extensive efforts have been dedicated to aligning LLMs with human moral principles to ensure their safe deployment.","Despite their potential, recent research indicates aligned LLMs are prone to specialized jailbreaking prompts that bypass safety measures to elicit violent and harmful content.","The intrinsic discrete nature and substantial scale of contemporary LLMs pose significant challenges in automatically generating diverse, efficient, and potent jailbreaking prompts, representing a continuous obstacle.","In this paper, we introduce RIPPLE (Rapid Optimization via Subconscious Exploitation and Echopraxia), a novel optimization-based method inspired by two psychological concepts: subconsciousness and echopraxia, which describe the processes of the mind that occur without conscious awareness and the involuntary mimicry of actions, respectively.","Evaluations across 6 open-source LLMs and 4 commercial LLM APIs show RIPPLE achieves an average Attack Success Rate of 91.5\\%, outperforming five current methods by up to 47.0\\% with an 8x reduction in overhead.","Furthermore, it displays significant transferability and stealth, successfully evading established detection mechanisms.","The code of our work is available at \\url{https://github.com/SolidShen/RIPPLE_official/tree/official}"],"url":"http://arxiv.org/abs/2402.05467v1","category":"cs.AI"}
{"created":"2024-02-08 07:55:57","title":"Engineering End-to-End Remote Labs using IoT-based Retrofitting","abstract":"Remote labs are a groundbreaking development in the education industry, providing students with access to laboratory education anytime, anywhere. However, most remote labs are costly and difficult to scale, especially in developing countries. With this as a motivation, this paper proposes a new remote labs (RLabs) solution that includes two use case experiments: Vanishing Rod and Focal Length. The hardware experiments are built at a low-cost by retrofitting Internet of Things (IoT) components. They are also made portable by designing miniaturised and modular setups. The software architecture designed as part of the solution seamlessly supports the scalability of the experiments, offering compatibility with a wide range of hardware devices and IoT platforms. Additionally, it can live-stream remote experiments without needing dedicated server space for the stream. The software architecture also includes an automation suite that periodically checks the status of the experiments using computer vision (CV). RLabs is qualitatively evaluated against seven non-functional attributes - affordability, portability, scalability, compatibility, maintainability, usability, and universality. Finally, user feedback was collected from a group of students, and the scores indicate a positive response to the students' learning and the platform's usability.","sentences":["Remote labs are a groundbreaking development in the education industry, providing students with access to laboratory education anytime, anywhere.","However, most remote labs are costly and difficult to scale, especially in developing countries.","With this as a motivation, this paper proposes a new remote labs (RLabs) solution that includes two use case experiments: Vanishing Rod and Focal Length.","The hardware experiments are built at a low-cost by retrofitting Internet of Things (IoT) components.","They are also made portable by designing miniaturised and modular setups.","The software architecture designed as part of the solution seamlessly supports the scalability of the experiments, offering compatibility with a wide range of hardware devices and IoT platforms.","Additionally, it can live-stream remote experiments without needing dedicated server space for the stream.","The software architecture also includes an automation suite that periodically checks the status of the experiments using computer vision (CV).","RLabs is qualitatively evaluated against seven non-functional attributes - affordability, portability, scalability, compatibility, maintainability, usability, and universality.","Finally, user feedback was collected from a group of students, and the scores indicate a positive response to the students' learning and the platform's usability."],"url":"http://arxiv.org/abs/2402.05466v1","category":"cs.HC"}
{"created":"2024-02-08 07:21:45","title":"It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition","abstract":"Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the acoustic modality. Experimental evidence collected from various ASR tasks shows that UADF surpasses existing fusion mechanisms in several ways. It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in LLM and addressing the poor generalization relied with sole modality during fusion. We also demonstrate that UADF seamlessly adapts to audio-visual speech recognition.","sentences":["Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output.","Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription.","However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal.","In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF).","UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the acoustic modality.","Experimental evidence collected from various ASR tasks shows that UADF surpasses existing fusion mechanisms in several ways.","It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in LLM and addressing the poor generalization relied with sole modality during fusion.","We also demonstrate that UADF seamlessly adapts to audio-visual speech recognition."],"url":"http://arxiv.org/abs/2402.05457v1","category":"cs.CL"}
{"created":"2024-02-08 07:17:53","title":"Inflation and Isotropization in Quintom Cosmology","abstract":"This paper studies inflation and isotropization in the quintom model in the Bianchi I, Bianchi III, and Kantowski-Sachs backgrounds. First, we investigate inherent properties and generalize Heusler's proposition. Then by the use of the dynamical system approach, we consider the system in multiplicative and collective modes of potentials. The conclusions of Collins and Hawking and also Burd and Barrow are discussed.","sentences":["This paper studies inflation and isotropization in the quintom model in the Bianchi I, Bianchi III, and Kantowski-Sachs backgrounds.","First, we investigate inherent properties and generalize Heusler's proposition.","Then by the use of the dynamical system approach, we consider the system in multiplicative and collective modes of potentials.","The conclusions of Collins and Hawking and also Burd and Barrow are discussed."],"url":"http://arxiv.org/abs/2402.05454v1","category":"gr-qc"}
{"created":"2024-02-08 07:01:00","title":"Minecraft-ify: Minecraft Style Image Generation with Text-guided Image Editing for In-Game Application","abstract":"In this paper, we first present the character texture generation system \\textit{Minecraft-ify}, specified to Minecraft video game toward in-game application. Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold. While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution. Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP. These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool. Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/","sentences":["In this paper, we first present the character texture generation system \\textit{Minecraft-ify}, specified to Minecraft video game toward in-game application.","Ours can generate face-focused image for texture mapping tailored to 3D virtual character having cube manifold.","While existing projects or works only generate texture, proposed system can inverse the user-provided real image, or generate average/random appearance from learned distribution.","Moreover, it can be manipulated with text-guidance using StyleGAN and StyleCLIP.","These features provide a more extended user experience with enlarged freedom as a user-friendly AI-tool.","Project page can be found at https://gh-bumsookim.github.io/Minecraft-ify/"],"url":"http://arxiv.org/abs/2402.05448v1","category":"cs.CV"}
{"created":"2024-02-08 06:35:06","title":"Spiking Neural Network Enhanced Hand Gesture Recognition Using Low-Cost Single-photon Avalanche Diode Array","abstract":"We present a compact spiking convolutional neural network (SCNN) and spiking multilayer perceptron (SMLP) to recognize ten different gestures in dark and bright light environments, using a $9.6 single-photon avalanche diode (SPAD) array. In our hand gesture recognition (HGR) system, photon intensity data was leveraged to train and test the network. A vanilla convolutional neural network (CNN) was also implemented to compare the performance of SCNN with the same network topologies and training strategies. Our SCNN was trained from scratch instead of being converted from the CNN. We tested the three models in dark and ambient light (AL)-corrupted environments. The results indicate that SCNN achieves comparable accuracy (90.8%) to CNN (92.9%) and exhibits lower floating operations with only 8 timesteps. SMLP also presents a trade-off between computational workload and accuracy. The code and collected datasets of this work are available at https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN.","sentences":["We present a compact spiking convolutional neural network (SCNN) and spiking multilayer perceptron (SMLP) to recognize ten different gestures in dark and bright light environments, using a $9.6 single-photon avalanche diode (SPAD) array.","In our hand gesture recognition (HGR) system, photon intensity data was leveraged to train and test the network.","A vanilla convolutional neural network (CNN) was also implemented to compare the performance of SCNN with the same network topologies and training strategies.","Our SCNN was trained from scratch instead of being converted from the CNN.","We tested the three models in dark and ambient light (AL)-corrupted environments.","The results indicate that SCNN achieves comparable accuracy (90.8%) to CNN (92.9%) and exhibits lower floating operations with only 8 timesteps.","SMLP also presents a trade-off between computational workload and accuracy.","The code and collected datasets of this work are available at https://github.com/zzy666666zzy/TinyLiDAR_NET_SNN."],"url":"http://arxiv.org/abs/2402.05441v1","category":"cs.CV"}
{"created":"2024-02-08 06:34:11","title":"Improving Agent Interactions in Virtual Environments with Language Models","abstract":"Enhancing AI systems with efficient communication skills for effective human assistance necessitates proactive initiatives from the system side to discern specific circumstances and interact aptly. This research focuses on a collective building assignment in the Minecraft dataset, employing language modeling to enhance task understanding through state-of-the-art methods. These models focus on grounding multi-modal understanding and task-oriented dialogue comprehension tasks, providing insights into their interpretative and responsive capabilities. Our experimental results showcase a substantial improvement over existing methods, indicating a promising direction for future research in this domain.","sentences":["Enhancing AI systems with efficient communication skills for effective human assistance necessitates proactive initiatives from the system side to discern specific circumstances and interact aptly.","This research focuses on a collective building assignment in the Minecraft dataset, employing language modeling to enhance task understanding through state-of-the-art methods.","These models focus on grounding multi-modal understanding and task-oriented dialogue comprehension tasks, providing insights into their interpretative and responsive capabilities.","Our experimental results showcase a substantial improvement over existing methods, indicating a promising direction for future research in this domain."],"url":"http://arxiv.org/abs/2402.05440v1","category":"cs.CL"}
{"created":"2024-02-08 06:20:01","title":"GPT-4 Generated Narratives of Life Events using a Structured Narrative Prompt: A Validation Study","abstract":"Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form. In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4. From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events. Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt. To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets. Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives. All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid. Our findings not only advance the study of LLM capabilities, limitations, and validity but also offer practical insights for narrative generation and natural language processing applications.","sentences":["Large Language Models (LLMs) play a pivotal role in generating vast arrays of narratives, facilitating a systematic exploration of their effectiveness for communicating life events in narrative form.","In this study, we employ a zero-shot structured narrative prompt to generate 24,000 narratives using OpenAI's GPT-4.","From this dataset, we manually classify 2,880 narratives and evaluate their validity in conveying birth, death, hiring, and firing events.","Remarkably, 87.43% of the narratives sufficiently convey the intention of the structured prompt.","To automate the identification of valid and invalid narratives, we train and validate nine Machine Learning models on the classified datasets.","Leveraging these models, we extend our analysis to predict the classifications of the remaining 21,120 narratives.","All the ML models excelled at classifying valid narratives as valid, but experienced challenges at simultaneously classifying invalid narratives as invalid.","Our findings not only advance the study of LLM capabilities, limitations, and validity but also offer practical insights for narrative generation and natural language processing applications."],"url":"http://arxiv.org/abs/2402.05435v1","category":"cs.CL"}
{"created":"2024-02-08 06:02:00","title":"Dynamical quantum state tomography with time-dependent channels","abstract":"In this paper, we establish a dynamical quantum state tomography framework. Under this framework, it is feasible to obtain complete knowledge of any unknown state of a $d$-level system via only an arbitrary operator of certain types of IC-POVMs in dimension $d$. We show that under the time-dependent average channel, we can acquire a collection of projective operators that is informationally complete (IC) and thus obtain the corresponding IC-POVMs. We show that under certain condition, it is possible to obtain infinite families of projective operators that are IC, and obtain infinite families of corresponding IC-POVMs; otherwise, the Zauner's conjecture is incorrect. We also show how to simulate a SIC-POVM on any unknown quantum state by using the time-dependent average channel.","sentences":["In this paper, we establish a dynamical quantum state tomography framework.","Under this framework, it is feasible to obtain complete knowledge of any unknown state of a $d$-level system via only an arbitrary operator of certain types of IC-POVMs in dimension $d$. We show that under the time-dependent average channel, we can acquire a collection of projective operators that is informationally complete (IC) and thus obtain the corresponding IC-POVMs.","We show that under certain condition, it is possible to obtain infinite families of projective operators that are IC, and obtain infinite families of corresponding IC-POVMs; otherwise, the Zauner's conjecture is incorrect.","We also show how to simulate a SIC-POVM on any unknown quantum state by using the time-dependent average channel."],"url":"http://arxiv.org/abs/2402.05431v1","category":"quant-ph"}
{"created":"2024-02-08 05:54:08","title":"Mixture Density Networks for Classification with an Application to Product Bundling","abstract":"While mixture density networks (MDNs) have been extensively used for regression tasks, they have not been used much for classification tasks. One reason for this is that the usability of MDNs for classification is not clear and straightforward. In this paper, we propose two MDN-based models for classification tasks. Both models fit mixtures of Gaussians to the the data and use the fitted distributions to classify a given sample by evaluating the learnt cumulative distribution function for the given input features. While the proposed MDN-based models perform slightly better than, or on par with, five baseline classification models on three publicly available datasets, the real utility of our models comes out through a real-world product bundling application. Specifically, we use our MDN-based models to learn the willingness-to-pay (WTP) distributions for two products from synthetic sales data of the individual products. The Gaussian mixture representation of the learnt WTP distributions is then exploited to obtain the WTP distribution of the bundle consisting of both the products. The proposed MDN-based models are able to approximate the true WTP distributions of both products and the bundle well.","sentences":["While mixture density networks (MDNs) have been extensively used for regression tasks, they have not been used much for classification tasks.","One reason for this is that the usability of MDNs for classification is not clear and straightforward.","In this paper, we propose two MDN-based models for classification tasks.","Both models fit mixtures of Gaussians to the the data and use the fitted distributions to classify a given sample by evaluating the learnt cumulative distribution function for the given input features.","While the proposed MDN-based models perform slightly better than, or on par with, five baseline classification models on three publicly available datasets, the real utility of our models comes out through a real-world product bundling application.","Specifically, we use our MDN-based models to learn the willingness-to-pay (WTP) distributions for two products from synthetic sales data of the individual products.","The Gaussian mixture representation of the learnt WTP distributions is then exploited to obtain the WTP distribution of the bundle consisting of both the products.","The proposed MDN-based models are able to approximate the true WTP distributions of both products and the bundle well."],"url":"http://arxiv.org/abs/2402.05428v1","category":"cs.LG"}
{"created":"2024-02-08 05:26:40","title":"DiffTOP: Differentiable Trajectory Optimization for Deep Reinforcement and Imitation Learning","abstract":"This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning. Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function. The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization. As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end. DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process. We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feed-forward policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15 model-based RL tasks and 13 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTOP outperforms prior state-of-the-art methods in both domains.","sentences":["This paper introduces DiffTOP, which utilizes Differentiable Trajectory OPtimization as the policy representation to generate actions for deep reinforcement and imitation learning.","Trajectory optimization is a powerful and widely used algorithm in control, parameterized by a cost and a dynamics function.","The key to our approach is to leverage the recent progress in differentiable trajectory optimization, which enables computing the gradients of the loss with respect to the parameters of trajectory optimization.","As a result, the cost and dynamics functions of trajectory optimization can be learned end-to-end.","DiffTOP addresses the ``objective mismatch'' issue of prior model-based RL algorithms, as the dynamics model in DiffTOP is learned to directly maximize task performance by differentiating the policy gradient loss through the trajectory optimization process.","We further benchmark DiffTOP for imitation learning on standard robotic manipulation task suites with high-dimensional sensory observations and compare our method to feed-forward policy classes as well as Energy-Based Models (EBM) and Diffusion.","Across 15 model-based RL tasks and 13 imitation learning tasks with high-dimensional image and point cloud inputs, DiffTOP outperforms prior state-of-the-art methods in both domains."],"url":"http://arxiv.org/abs/2402.05421v1","category":"cs.LG"}
{"created":"2024-02-08 04:42:29","title":"In-Context Principle Learning from Mistakes","abstract":"In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific \"principles\" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.","sentences":["In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples.","Nonetheless, all ICL-based approaches only learn from correct input-output pairs.","In this paper, we revisit this paradigm, by learning more from the few given input-output examples.","We introduce Learning Principles (LEAP):","First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific \"principles\" from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles.","We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and Claude-2.1.","For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA.","Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings."],"url":"http://arxiv.org/abs/2402.05403v1","category":"cs.CL"}
{"created":"2024-02-08 04:27:14","title":"CURE: Simulation-Augmented Auto-Tuning in Robotics","abstract":"Robotic systems are typically composed of various subsystems, such as localization and navigation, each encompassing numerous configurable components (e.g., selecting different planning algorithms). Once an algorithm has been selected for a component, its associated configuration options must be set to the appropriate values. Configuration options across the system stack interact non-trivially. Finding optimal configurations for highly configurable robots to achieve desired performance poses a significant challenge due to the interactions between configuration options across software and hardware that result in an exponentially large and complex configuration space. These challenges are further compounded by the need for transferability between different environments and robotic platforms. Data efficient optimization algorithms (e.g., Bayesian optimization) have been increasingly employed to automate the tuning of configurable parameters in cyber-physical systems. However, such optimization algorithms converge at later stages, often after exhausting the allocated budget (e.g., optimization steps, allotted time) and lacking transferability. This paper proposes CURE -- a method that identifies causally relevant configuration options, enabling the optimization process to operate in a reduced search space, thereby enabling faster optimization of robot performance. CURE abstracts the causal relationships between various configuration options and robot performance objectives by learning a causal model in the source (a low-cost environment such as the Gazebo simulator) and applying the learned knowledge to perform optimization in the target (e.g., Turtlebot 3 physical robot). We demonstrate the effectiveness and transferability of CURE by conducting experiments that involve varying degrees of deployment changes in both physical robots and simulation.","sentences":["Robotic systems are typically composed of various subsystems, such as localization and navigation, each encompassing numerous configurable components (e.g., selecting different planning algorithms).","Once an algorithm has been selected for a component, its associated configuration options must be set to the appropriate values.","Configuration options across the system stack interact non","-trivially.","Finding optimal configurations for highly configurable robots to achieve desired performance poses a significant challenge due to the interactions between configuration options across software and hardware that result in an exponentially large and complex configuration space.","These challenges are further compounded by the need for transferability between different environments and robotic platforms.","Data efficient optimization algorithms (e.g., Bayesian optimization) have been increasingly employed to automate the tuning of configurable parameters in cyber-physical systems.","However, such optimization algorithms converge at later stages, often after exhausting the allocated budget (e.g., optimization steps, allotted time) and lacking transferability.","This paper proposes CURE -- a method that identifies causally relevant configuration options, enabling the optimization process to operate in a reduced search space, thereby enabling faster optimization of robot performance.","CURE abstracts the causal relationships between various configuration options and robot performance objectives by learning a causal model in the source (a low-cost environment such as the Gazebo simulator) and applying the learned knowledge to perform optimization in the target (e.g., Turtlebot 3 physical robot).","We demonstrate the effectiveness and transferability of CURE by conducting experiments that involve varying degrees of deployment changes in both physical robots and simulation."],"url":"http://arxiv.org/abs/2402.05399v1","category":"cs.RO"}
{"created":"2024-02-08 04:16:35","title":"TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning","abstract":"Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and scalability. TASER adapts its mini-batch selection based on training dynamics and temporal neighbor selection based on the contextual, structural, and temporal properties of past interactions. To alleviate the bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal neighbor finder and a dedicated GPU feature cache. We evaluate the performance of TASER using two state-of-the-art backbone TGNNs. On five popular datasets, TASER outperforms the corresponding baselines by an average of 2.3% in Mean Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training time.","sentences":["Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation.","Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution.","The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages.","However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node.","In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors.","We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling.","In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and scalability.","TASER adapts its mini-batch selection based on training dynamics and temporal neighbor selection based on the contextual, structural, and temporal properties of past interactions.","To alleviate the bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal neighbor finder and a dedicated GPU feature cache.","We evaluate the performance of TASER using two state-of-the-art backbone TGNNs.","On five popular datasets, TASER outperforms the corresponding baselines by an average of 2.3% in Mean Reciprocal Rank (MRR) while achieving an average of 5.1x speedup in training time."],"url":"http://arxiv.org/abs/2402.05396v1","category":"cs.LG"}
{"created":"2024-02-08 04:04:36","title":"Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey","abstract":"Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work.","sentences":["Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation.","In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm.","We begin by defining KGs and MMKGs, then explore their construction progress.","Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories.","For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research.","Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies.","This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work."],"url":"http://arxiv.org/abs/2402.05391v1","category":"cs.AI"}
{"created":"2024-02-08 04:03:47","title":"Integrated Sensing and Communication Driven Digital Twin for Intelligent Machine Network","abstract":"Intelligent machines (IMs), including industrial machines, unmanned aerial vehicles (UAVs), and unmanned vehicles, etc., could perform effective cooperation in complex environment when they form IM network. The efficient environment sensing and communication are crucial for IM network, enabling the real-time and stable control of IMs. With the emergence of integrated sensing and communication (ISAC) technology, IM network is empowered with ubiquitous sensing capabilities, which is helpful in improving the efficiency of communication and sensing with the mutual benefit of them. However, the massive amount of sensing information brings challenges for the processing, storage and application of sensing information. In this article, ISAC driven digital twin (DT) is proposed for IM network, and the architecture and enabling technologies are revealed. ISAC driven DT structurally stores the sensing information, which is further applied to optimize communication, networking and control schemes of IMs, promoting the widespread applications of IMs.","sentences":["Intelligent machines (IMs), including industrial machines, unmanned aerial vehicles (UAVs), and unmanned vehicles, etc., could perform effective cooperation in complex environment when they form IM network.","The efficient environment sensing and communication are crucial for IM network, enabling the real-time and stable control of IMs.","With the emergence of integrated sensing and communication (ISAC) technology, IM network is empowered with ubiquitous sensing capabilities, which is helpful in improving the efficiency of communication and sensing with the mutual benefit of them.","However, the massive amount of sensing information brings challenges for the processing, storage and application of sensing information.","In this article, ISAC driven digital twin (DT) is proposed for IM network, and the architecture and enabling technologies are revealed.","ISAC driven DT structurally stores the sensing information, which is further applied to optimize communication, networking and control schemes of IMs, promoting the widespread applications of IMs."],"url":"http://arxiv.org/abs/2402.05390v1","category":"cs.NI"}
{"created":"2024-02-08 03:46:32","title":"Task-customized Masked AutoEncoder via Mixture of Cluster-conditional Experts","abstract":"Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training. However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability. To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks. Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates. Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data. Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\\% on average. It also obtains new state-of-the-art self-supervised learning results on detection and segmentation.","sentences":["Masked Autoencoder~(MAE) is a prevailing self-supervised learning method that achieves promising results in model pre-training.","However, when the various downstream tasks have data distributions different from the pre-training data, the semantically irrelevant pre-training information might result in negative transfer, impeding MAE's scalability.","To address this issue, we propose a novel MAE-based pre-training paradigm, Mixture of Cluster-conditional Experts (MoCE), which can be trained once but provides customized pre-training models for diverse downstream tasks.","Different from the mixture of experts (MoE), our MoCE trains each expert only with semantically relevant images by using cluster-conditional gates.","Thus, each downstream task can be allocated to its customized model pre-trained with data most similar to the downstream data.","Experiments on a collection of 11 downstream tasks show that MoCE outperforms the vanilla MAE by 2.45\\% on average.","It also obtains new state-of-the-art self-supervised learning results on detection and segmentation."],"url":"http://arxiv.org/abs/2402.05382v1","category":"cs.CV"}
{"created":"2024-02-08 03:22:12","title":"Graph Neural Networks for Physical-Layer Security in Multi-User Flexible-Duplex Networks","abstract":"This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD) networks, considering scenarios involving eavesdroppers. Our investigation revolves around the intricacies of the sum secrecy rate maximization problem, particularly when faced with coordinated and distributed eavesdroppers employing a Minimum Mean Square Error (MMSE) receiver. Our contributions include an iterative classical optimization solution and an unsupervised learning strategy based on Graph Neural Networks (GNNs). To the best of our knowledge, this work marks the initial exploration of GNNs for PLS applications. Additionally, we extend the GNN approach to address the absence of eavesdroppers' channel knowledge. Extensive numerical simulations highlight FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's superiority over the classical method in both performance and time complexity.","sentences":["This paper explores Physical-Layer Security (PLS) in Flexible Duplex (FlexD) networks, considering scenarios involving eavesdroppers.","Our investigation revolves around the intricacies of the sum secrecy rate maximization problem, particularly when faced with coordinated and distributed eavesdroppers employing a Minimum Mean Square Error (MMSE) receiver.","Our contributions include an iterative classical optimization solution and an unsupervised learning strategy based on Graph Neural Networks (GNNs).","To the best of our knowledge, this work marks the initial exploration of GNNs for PLS applications.","Additionally, we extend the GNN approach to address the absence of eavesdroppers' channel knowledge.","Extensive numerical simulations highlight FlexD's superiority over Half-Duplex (HD) communications and the GNN approach's superiority over the classical method in both performance and time complexity."],"url":"http://arxiv.org/abs/2402.05378v1","category":"eess.SP"}
{"created":"2024-02-08 03:12:25","title":"CIC: A framework for Culturally-aware Image Captioning","abstract":"Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 different cultural groups with a high understanding of the corresponding culture shows that our proposed framework generates more culturally descriptive captions when compared to the image captioning baseline based on VLPs. Our code and dataset will be made publicly available upon acceptance.","sentences":["Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly.","However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups.","In this paper, we propose a new framework, \\textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures.","Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts.","Our human evaluation conducted on 45 participants from 4 different cultural groups with a high understanding of the corresponding culture shows that our proposed framework generates more culturally descriptive captions when compared to the image captioning baseline based on VLPs.","Our code and dataset will be made publicly available upon acceptance."],"url":"http://arxiv.org/abs/2402.05374v1","category":"cs.CV"}
{"created":"2024-02-08 03:00:50","title":"Attention as Robust Representation for Time Series Forecasting","abstract":"Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV. Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role. Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges. Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy. Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution. Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core neural network architecture. It serves as a versatile component that can readily replace recent patching based embedding schemes in transformer-based models, boosting their performance.","sentences":["Time series forecasting is essential for many practical applications, with the adoption of transformer-based models on the rise due to their impressive performance in NLP and CV.","Transformers' key feature, the attention mechanism, dynamically fusing embeddings to enhance data representation, often relegating attention weights to a byproduct role.","Yet, time series data, characterized by noise and non-stationarity, poses significant forecasting challenges.","Our approach elevates attention weights as the primary representation for time series, capitalizing on the temporal relationships among data points to improve forecasting accuracy.","Our study shows that an attention map, structured using global landmarks and local windows, acts as a robust kernel representation for data points, withstanding noise and shifts in distribution.","Our method outperforms state-of-the-art models, reducing mean squared error (MSE) in multivariate time series forecasting by a notable 3.6% without altering the core neural network architecture.","It serves as a versatile component that can readily replace recent patching based embedding schemes in transformer-based models, boosting their performance."],"url":"http://arxiv.org/abs/2402.05370v1","category":"cs.LG"}
{"created":"2024-02-08 02:37:30","title":"Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving","abstract":"Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achieve better performance than typical prompting strategies in tasks bothered by intermediate errors and deceptive contents, such as large integer multiplication, hallucination detection and misinformation detection.","sentences":["Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications.","Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas.","However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination.","To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process.","Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer.","Experiments indicate that our proposed method can achieve better performance than typical prompting strategies in tasks bothered by intermediate errors and deceptive contents, such as large integer multiplication, hallucination detection and misinformation detection."],"url":"http://arxiv.org/abs/2402.05359v1","category":"cs.AI"}
{"created":"2024-02-08 02:27:13","title":"A Survey on Safe Multi-Modal Learning System","abstract":"With the wide deployment of multimodal learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent. The absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns. Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development. Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research.","sentences":["With the wide deployment of multimodal learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent.","The absence of systematic research into their safety is a significant barrier to progress in this field.","To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns.","Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development.","Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research."],"url":"http://arxiv.org/abs/2402.05355v1","category":"cs.CY"}
{"created":"2024-02-08 02:21:33","title":"Revisiting Early-Learning Regularization When Federated Learning Meets Noisy Labels","abstract":"In the evolving landscape of federated learning (FL), addressing label noise presents unique challenges due to the decentralized and diverse nature of data collection across clients. Traditional centralized learning approaches to mitigate label noise are constrained in FL by privacy concerns and the heterogeneity of client data. This paper revisits early-learning regularization, introducing an innovative strategy, Federated Label-mixture Regularization (FLR). FLR adeptly adapts to FL's complexities by generating new pseudo labels, blending local and global model predictions. This method not only enhances the accuracy of the global model in both i.i.d. and non-i.i.d. settings but also effectively counters the memorization of noisy labels. Demonstrating compatibility with existing label noise and FL techniques, FLR paves the way for improved generalization in FL environments fraught with label inaccuracies.","sentences":["In the evolving landscape of federated learning (FL), addressing label noise presents unique challenges due to the decentralized and diverse nature of data collection across clients.","Traditional centralized learning approaches to mitigate label noise are constrained in FL by privacy concerns and the heterogeneity of client data.","This paper revisits early-learning regularization, introducing an innovative strategy, Federated Label-mixture Regularization (FLR).","FLR adeptly adapts to FL's complexities by generating new pseudo labels, blending local and global model predictions.","This method not only enhances the accuracy of the global model in both i.i.d. and non-i.i.d. settings but also effectively counters the memorization of noisy labels.","Demonstrating compatibility with existing label noise and FL techniques, FLR paves the way for improved generalization in FL environments fraught with label inaccuracies."],"url":"http://arxiv.org/abs/2402.05353v1","category":"cs.LG"}
{"created":"2024-02-08 02:11:52","title":"Descanning: From Scanned to the Original Images with a Color Correction Diffusion Model","abstract":"A significant volume of analog information, i.e., documents and images, have been digitized in the form of scanned copies for storing, sharing, and/or analyzing in the digital world. However, the quality of such contents is severely degraded by various distortions caused by printing, storing, and scanning processes in the physical world. Although restoring high-quality content from scanned copies has become an indispensable task for many products, it has not been systematically explored, and to the best of our knowledge, no public datasets are available. In this paper, we define this problem as Descanning and introduce a new high-quality and large-scale dataset named DESCAN-18K. It contains 18K pairs of original and scanned images collected in the wild containing multiple complex degradations. In order to eliminate such complex degradations, we propose a new image restoration model called DescanDiffusion consisting of a color encoder that corrects the global color degradation and a conditional denoising diffusion probabilistic model (DDPM) that removes local degradations. To further improve the generalization ability of DescanDiffusion, we also design a synthetic data generation scheme by reproducing prominent degradations in scanned images. We demonstrate that our DescanDiffusion outperforms other baselines including commercial restoration products, objectively and subjectively, via comprehensive experiments and analyses.","sentences":["A significant volume of analog information, i.e., documents and images, have been digitized in the form of scanned copies for storing, sharing, and/or analyzing in the digital world.","However, the quality of such contents is severely degraded by various distortions caused by printing, storing, and scanning processes in the physical world.","Although restoring high-quality content from scanned copies has become an indispensable task for many products, it has not been systematically explored, and to the best of our knowledge, no public datasets are available.","In this paper, we define this problem as Descanning and introduce a new high-quality and large-scale dataset named DESCAN-18K. It contains 18K pairs of original and scanned images collected in the wild containing multiple complex degradations.","In order to eliminate such complex degradations, we propose a new image restoration model called DescanDiffusion consisting of a color encoder that corrects the global color degradation and a conditional denoising diffusion probabilistic model (DDPM) that removes local degradations.","To further improve the generalization ability of DescanDiffusion, we also design a synthetic data generation scheme by reproducing prominent degradations in scanned images.","We demonstrate that our DescanDiffusion outperforms other baselines including commercial restoration products, objectively and subjectively, via comprehensive experiments and analyses."],"url":"http://arxiv.org/abs/2402.05350v1","category":"cs.CV"}
{"created":"2024-02-08 01:41:28","title":"KIX: A Metacognitive Generalization Framework","abstract":"Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.","sentences":["Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time.","But artificial agents are more of a specialist, lacking such generalist behaviors.","Artificial agents will require understanding and exploiting critical structured knowledge representations.","We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization.","It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems."],"url":"http://arxiv.org/abs/2402.05346v1","category":"cs.AI"}
{"created":"2024-02-08 01:05:16","title":"POLARIS: A framework to guide the development of Trustworthy AI systems","abstract":"In the ever-expanding landscape of Artificial Intelligence (AI), where innovation thrives and new products and services are continuously being delivered, ensuring that AI systems are designed and developed responsibly throughout their entire lifecycle is crucial. To this end, several AI ethics principles and guidelines have been issued to which AI systems should conform. Nevertheless, relying solely on high-level AI ethics principles is far from sufficient to ensure the responsible engineering of AI systems. In this field, AI professionals often navigate by sight. Indeed, while recommendations promoting Trustworthy AI (TAI) exist, these are often high-level statements that are difficult to translate into concrete implementation strategies. There is a significant gap between high-level AI ethics principles and low-level concrete practices for AI professionals. To address this challenge, our work presents an experience report where we develop a novel holistic framework for Trustworthy AI - designed to bridge the gap between theory and practice - and report insights from its application in an industrial case study. The framework is built on the result of a systematic review of the state of the practice, a survey, and think-aloud interviews with 34 AI practitioners. The framework, unlike most of those already in the literature, is designed to provide actionable guidelines and tools to support different types of stakeholders throughout the entire Software Development Life Cycle (SDLC). Our goal is to empower AI professionals to confidently navigate the ethical dimensions of TAI through practical insights, ensuring that the vast potential of AI is exploited responsibly for the benefit of society as a whole.","sentences":["In the ever-expanding landscape of Artificial Intelligence (AI), where innovation thrives and new products and services are continuously being delivered, ensuring that AI systems are designed and developed responsibly throughout their entire lifecycle is crucial.","To this end, several AI ethics principles and guidelines have been issued to which AI systems should conform.","Nevertheless, relying solely on high-level AI ethics principles is far from sufficient to ensure the responsible engineering of AI systems.","In this field, AI professionals often navigate by sight.","Indeed, while recommendations promoting Trustworthy AI (TAI) exist, these are often high-level statements that are difficult to translate into concrete implementation strategies.","There is a significant gap between high-level AI ethics principles and low-level concrete practices for AI professionals.","To address this challenge, our work presents an experience report where we develop a novel holistic framework for Trustworthy AI - designed to bridge the gap between theory and practice - and report insights from its application in an industrial case study.","The framework is built on the result of a systematic review of the state of the practice, a survey, and think-aloud interviews with 34 AI practitioners.","The framework, unlike most of those already in the literature, is designed to provide actionable guidelines and tools to support different types of stakeholders throughout the entire Software Development Life Cycle (SDLC).","Our goal is to empower AI professionals to confidently navigate the ethical dimensions of TAI through practical insights, ensuring that the vast potential of AI is exploited responsibly for the benefit of society as a whole."],"url":"http://arxiv.org/abs/2402.05340v1","category":"cs.SE"}
{"created":"2024-02-07 23:50:00","title":"Learning on Multimodal Graphs: A Survey","abstract":"Multimodal data pervades various domains, including healthcare, social media, and transportation, where multimodal graphs play a pivotal role. Machine learning on multimodal graphs, referred to as multimodal graph learning (MGL), is essential for successful artificial intelligence (AI) applications. The burgeoning research in this field encompasses diverse graph data types and modalities, learning techniques, and application scenarios. This survey paper conducts a comparative analysis of existing works in multimodal graph learning, elucidating how multimodal learning is achieved across different graph types and exploring the characteristics of prevalent learning techniques. Additionally, we delineate significant applications of multimodal graph learning and offer insights into future directions in this domain. Consequently, this paper serves as a foundational resource for researchers seeking to comprehend existing MGL techniques and their applicability across diverse scenarios.","sentences":["Multimodal data pervades various domains, including healthcare, social media, and transportation, where multimodal graphs play a pivotal role.","Machine learning on multimodal graphs, referred to as multimodal graph learning (MGL), is essential for successful artificial intelligence (AI) applications.","The burgeoning research in this field encompasses diverse graph data types and modalities, learning techniques, and application scenarios.","This survey paper conducts a comparative analysis of existing works in multimodal graph learning, elucidating how multimodal learning is achieved across different graph types and exploring the characteristics of prevalent learning techniques.","Additionally, we delineate significant applications of multimodal graph learning and offer insights into future directions in this domain.","Consequently, this paper serves as a foundational resource for researchers seeking to comprehend existing MGL techniques and their applicability across diverse scenarios."],"url":"http://arxiv.org/abs/2402.05322v1","category":"cs.LG"}
{"created":"2024-02-07 23:00:24","title":"Three Pathways to Neurosymbolic Reinforcement Learning with Interpretable Model and Policy Networks","abstract":"Neurosymbolic AI combines the interpretability, parsimony, and explicit reasoning of classical symbolic approaches with the statistical learning of data-driven neural approaches. Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage. This paper demonstrates three pathways to implementing such models and policies in a real-world reinforcement learning setting. Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture. We reveal and highlight both the potential and the essential difficulties of combining logic, simulation, and learning. One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable. The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable. Another lesson is that using logic in the context of a numerical simulation involves a non-trivial mapping from raw (e.g., real-valued time series) simulation data to logical predicates. Some open questions this note exposes include: What are the limits of rule-based controllers, and how learnable are they? Do the differentiable interpretable approaches discussed here scale to large, complex, uncertain systems? Can we truly achieve interpretability? We highlight these and other themes across the three approaches.","sentences":["Neurosymbolic AI combines the interpretability, parsimony, and explicit reasoning of classical symbolic approaches with the statistical learning of data-driven neural approaches.","Models and policies that are simultaneously differentiable and interpretable may be key enablers of this marriage.","This paper demonstrates three pathways to implementing such models and policies in a real-world reinforcement learning setting.","Specifically, we study a broad class of neural networks that build interpretable semantics directly into their architecture.","We reveal and highlight both the potential and the essential difficulties of combining logic, simulation, and learning.","One lesson is that learning benefits from continuity and differentiability, but classical logic is discrete and non-differentiable.","The relaxation to real-valued, differentiable representations presents a trade-off; the more learnable, the less interpretable.","Another lesson is that using logic in the context of a numerical simulation involves a non-trivial mapping from raw (e.g., real-valued time series) simulation data to logical predicates.","Some open questions this note exposes include: What are the limits of rule-based controllers, and how learnable are they?","Do the differentiable interpretable approaches discussed here scale to large, complex, uncertain systems?","Can we truly achieve interpretability?","We highlight these and other themes across the three approaches."],"url":"http://arxiv.org/abs/2402.05307v1","category":"cs.AI"}
{"created":"2024-02-07 22:53:54","title":"Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making","abstract":"Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexity of expression trees and perform precise step-wise updates significantly enhances flexibility and efficiency. Our results demonstrate that Sym-Q excels not only in recovering underlying mathematical structures but also uniquely learns to efficiently refine the output expression based on reward signals, thereby discovering underlying expressions. Sym-Q paves the way for more intuitive and impactful discoveries in physical science, marking a substantial advancement in the field of symbolic regression.","sentences":["Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data.","While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability.","Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression.","This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships.","Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task.","Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision.","Its distinctive ability to manage the complexity of expression trees and perform precise step-wise updates significantly enhances flexibility and efficiency.","Our results demonstrate that Sym-Q excels not only in recovering underlying mathematical structures but also uniquely learns to efficiently refine the output expression based on reward signals, thereby discovering underlying expressions.","Sym-Q paves the way for more intuitive and impactful discoveries in physical science, marking a substantial advancement in the field of symbolic regression."],"url":"http://arxiv.org/abs/2402.05306v1","category":"cs.LG"}
{"created":"2024-02-07 22:37:16","title":"BIKED++: A Multimodal Dataset of 1.4 Million Bicycle Image and Parametric CAD Designs","abstract":"This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images. The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs. This rendering engine is discussed in the paper and also released publicly alongside the dataset. Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations. For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly. This allows similarity relations to be established between parametric bicycle designs and text strings or reference images. Trained predictive models are also made public. The dataset joins the BIKED dataset family which includes thousands of mixed-representation human-designed bicycle models and several datasets quantifying design performance. The code and dataset can be found at: https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main","sentences":["This paper introduces a public dataset of 1.4 million procedurally-generated bicycle designs represented parametrically, as JSON files, and as rasterized images.","The dataset is created through the use of a rendering engine which harnesses the BikeCAD software to generate vector graphics from parametric designs.","This rendering engine is discussed in the paper and also released publicly alongside the dataset.","Though this dataset has numerous applications, a principal motivation is the need to train cross-modal predictive models between parametric and image-based design representations.","For example, we demonstrate that a predictive model can be trained to accurately estimate Contrastive Language-Image Pretraining (CLIP) embeddings from a parametric representation directly.","This allows similarity relations to be established between parametric bicycle designs and text strings or reference images.","Trained predictive models are also made public.","The dataset joins the BIKED dataset family which includes thousands of mixed-representation human-designed bicycle models and several datasets quantifying design performance.","The code and dataset can be found at: https://github.com/Lyleregenwetter/BIKED_multimodal/tree/main"],"url":"http://arxiv.org/abs/2402.05301v1","category":"cs.CV"}
{"created":"2024-02-07 22:36:49","title":"Multi-Player Resource-Sharing Games with Fair Reward Allocation","abstract":"This paper considers a multi-player resource-sharing game with a fair reward allocation model. Multiple players choose from a collection of resources. Each resource brings a random reward equally divided among the players who choose it. We consider two settings. The first setting is a one-slot game where the mean rewards of the resources are known to all the players, and the objective of player 1 is to maximize their worst-case expected utility. Certain special cases of this setting have explicit solutions. These cases provide interesting yet non-intuitive insights into the problem. The second setting is an online setting, where the game is played over a finite time horizon, where the mean rewards are unknown to the first player. Instead, the first player receives, as feedback, the rewards of the resources they chose after the action. We develop a novel Upper Confidence Bound (UCB) algorithm that minimizes the worst-case regret of the first player using the feedback received.","sentences":["This paper considers a multi-player resource-sharing game with a fair reward allocation model.","Multiple players choose from a collection of resources.","Each resource brings a random reward equally divided among the players who choose it.","We consider two settings.","The first setting is a one-slot game where the mean rewards of the resources are known to all the players, and the objective of player 1 is to maximize their worst-case expected utility.","Certain special cases of this setting have explicit solutions.","These cases provide interesting yet non-intuitive insights into the problem.","The second setting is an online setting, where the game is played over a finite time horizon, where the mean rewards are unknown to the first player.","Instead, the first player receives, as feedback, the rewards of the resources they chose after the action.","We develop a novel Upper Confidence Bound (UCB) algorithm that minimizes the worst-case regret of the first player using the feedback received."],"url":"http://arxiv.org/abs/2402.05300v1","category":"cs.GT"}
{"created":"2024-02-07 22:19:08","title":"Classifying spam emails using agglomerative hierarchical clustering and a topic-based approach","abstract":"Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware, phishing or hoaxes. Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective. Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories. We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative hierarchical clustering into 11 classes. We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words, Word2Vec and BERT- and four classifiers: Support Vector Machine, N\\\"aive Bayes, Random Forest and Logistic Regression. Experimental results show that the highest performance is achieved with TF-IDF and LR for the English dataset, with a F1 score of 0.953 and an accuracy of 94.6%, and while for the Spanish dataset, TF-IDF with NB yields a F1 score of 0.945 and 98.5% accuracy. Regarding the processing time, TF-IDF with LR leads to the fastest classification, processing an English and Spanish spam email in and on average, respectively.","sentences":["Spam emails are unsolicited, annoying and sometimes harmful messages which may contain malware, phishing or hoaxes.","Unlike most studies that address the design of efficient anti-spam filters, we approach the spam email problem from a different and novel perspective.","Focusing on the needs of cybersecurity units, we follow a topic-based approach for addressing the classification of spam email into multiple categories.","We propose SPEMC-15K-E and SPEMC-15K-S, two novel datasets with approximately 15K emails each in English and Spanish, respectively, and we label them using agglomerative hierarchical clustering into 11 classes.","We evaluate 16 pipelines, combining four text representation techniques -Term Frequency-Inverse Document Frequency (TF-IDF), Bag of Words, Word2Vec and BERT- and four classifiers: Support Vector Machine, N\\\"aive Bayes, Random Forest and Logistic Regression.","Experimental results show that the highest performance is achieved with TF-IDF and LR for the English dataset, with a F1 score of 0.953 and an accuracy of 94.6%, and while for the Spanish dataset, TF-IDF with NB yields a F1 score of 0.945 and 98.5% accuracy.","Regarding the processing time, TF-IDF with LR leads to the fastest classification, processing an English and Spanish spam email in and on average, respectively."],"url":"http://arxiv.org/abs/2402.05296v1","category":"cs.LG"}
{"created":"2024-02-07 22:17:37","title":"An information theoretic approach to quantify the stability of feature selection and ranking algorithms","abstract":"Feature selection is a key step when dealing with high dimensional data. In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features. A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable. Thus, small variations in the data may yield very different feature rankings. Assessing the stability of these methods becomes an important issue in the previously mentioned situations. We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness. Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists. This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and being able to give more importance to the disagreements that appear at the top of the list. Moreover, it possesses desirable properties including correction for change, upper lower bounds and conditions for a deterministic selection. We illustrate the use of this stability metric with data generated in a fully controlled way and compare it with popular metrics including the Spearmans rank correlation and the Kunchevas index on feature ranking and selection outcomes, respectively. Additionally, experimental validation of the proposed approach is carried out on a real-world problem of food quality assessment showing its potential to quantify stability from different perspectives.","sentences":["Feature selection is a key step when dealing with high dimensional data.","In particular, these techniques simplify the process of knowledge discovery from the data by selecting the most relevant features out of the noisy, redundant and irrelevant features.","A problem that arises in many of these practical applications is that the outcome of the feature selection algorithm is not stable.","Thus, small variations in the data may yield very different feature rankings.","Assessing the stability of these methods becomes an important issue in the previously mentioned situations.","We propose an information theoretic approach based on the Jensen Shannon divergence to quantify this robustness.","Unlike other stability measures, this metric is suitable for different algorithm outcomes: full ranked lists, feature subsets as well as the lesser studied partial ranked lists.","This generalized metric quantifies the difference among a whole set of lists with the same size, following a probabilistic approach and being able to give more importance to the disagreements that appear at the top of the list.","Moreover, it possesses desirable properties including correction for change, upper lower bounds and conditions for a deterministic selection.","We illustrate the use of this stability metric with data generated in a fully controlled way and compare it with popular metrics including the Spearmans rank correlation and the Kunchevas index on feature ranking and selection outcomes, respectively.","Additionally, experimental validation of the proposed approach is carried out on a real-world problem of food quality assessment showing its potential to quantify stability from different perspectives."],"url":"http://arxiv.org/abs/2402.05295v1","category":"cs.LG"}
{"created":"2024-02-07 22:16:53","title":"Examining Modality Incongruity in Multimodal Federated Learning for Medical Vision and Language-based Disease Detection","abstract":"Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart. However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked. This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients. We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL. Furthermore, we examine three potential routes of addressing this issue. Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL. Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem. Thirdly, we assess the capability of client-level and server-level regularization techniques towards mitigating modality incongruity effects. Experiments are conducted under several MMFL settings on two publicly available real-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology reports.","sentences":["Multimodal Federated Learning (MMFL) utilizes multiple modalities in each client to build a more powerful Federated Learning (FL) model than its unimodal counterpart.","However, the impact of missing modality in different clients, also called modality incongruity, has been greatly overlooked.","This paper, for the first time, analyses the impact of modality incongruity and reveals its connection with data heterogeneity across participating clients.","We particularly inspect whether incongruent MMFL with unimodal and multimodal clients is more beneficial than unimodal FL.","Furthermore, we examine three potential routes of addressing this issue.","Firstly, we study the effectiveness of various self-attention mechanisms towards incongruity-agnostic information fusion in MMFL.","Secondly, we introduce a modality imputation network (MIN) pre-trained in a multimodal client for modality translation in unimodal clients and investigate its potential towards mitigating the missing modality problem.","Thirdly, we assess the capability of client-level and server-level regularization techniques towards mitigating modality incongruity effects.","Experiments are conducted under several MMFL settings on two publicly available real-world datasets, MIMIC-CXR and Open-I, with Chest X-Ray and radiology reports."],"url":"http://arxiv.org/abs/2402.05294v1","category":"cs.LG"}
{"created":"2024-02-07 22:14:14","title":"A comparative study on feature selection for a risk prediction model for colorectal cancer","abstract":"Background and objective   Risk prediction models aim at identifying people at higher risk of developing a target disease. Feature selection is particularly important to improve the prediction model performance avoiding overfitting and to identify the leading cancer risk (and protective) factors. Assessing the stability of feature selection/ranking algorithms becomes an important issue when the aim is to analyze the features with more prediction power. Methods   This work is focused on colorectal cancer, assessing several feature ranking algorithms in terms of performance for a set of risk prediction models (Neural Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest Neighbors and Boosted Trees). Additionally, their robustness is evaluated following a conventional approach with scalar stability metrics and a visual approach proposed in this work to study both similarity among feature ranking techniques as well as their individual stability. A comparative analysis is carried out between the most relevant features found out in this study and features provided by the experts according to the state-of-the-art knowledge. Results   The two best performance results in terms of Area Under the ROC Curve (AUC) are achieved with a SVM classifier using the top-41 features selected by the SVM wrapper approach (AUC=0.693) and Logistic Regression with the top-40 features selected by the Pearson (AUC=0.689). Experiments showed that performing feature selection contributes to classification performance with a 3.9% and 1.9% improvement in AUC for the SVM and Logistic Regression classifier, respectively, with respect to the results using the full feature set. The visual approach proposed in this work allows to see that the Neural Network-based wrapper ranking is the most unstable while the Random Forest is the most stable.","sentences":["Background and objective   Risk prediction models aim at identifying people at higher risk of developing a target disease.","Feature selection is particularly important to improve the prediction model performance avoiding overfitting and to identify the leading cancer risk (and protective) factors.","Assessing the stability of feature selection/ranking algorithms becomes an important issue when the aim is to analyze the features with more prediction power.","Methods   This work is focused on colorectal cancer, assessing several feature ranking algorithms in terms of performance for a set of risk prediction models (Neural Networks, Support Vector Machines (SVM), Logistic Regression, k-Nearest Neighbors and Boosted Trees).","Additionally, their robustness is evaluated following a conventional approach with scalar stability metrics and a visual approach proposed in this work to study both similarity among feature ranking techniques as well as their individual stability.","A comparative analysis is carried out between the most relevant features found out in this study and features provided by the experts according to the state-of-the-art knowledge.","Results   The two best performance results in terms of Area Under the ROC Curve (AUC) are achieved with a SVM classifier using the top-41 features selected by the SVM wrapper approach (AUC=0.693) and Logistic Regression with the top-40 features selected by the Pearson (AUC=0.689).","Experiments showed that performing feature selection contributes to classification performance with a 3.9% and 1.9% improvement in AUC for the SVM and Logistic Regression classifier, respectively, with respect to the results using the full feature set.","The visual approach proposed in this work allows to see that the Neural Network-based wrapper ranking is the most unstable while the Random Forest is the most stable."],"url":"http://arxiv.org/abs/2402.05293v1","category":"cs.LG"}
{"created":"2024-02-07 22:09:46","title":"Do Transformer World Models Give Better Policy Gradients?","abstract":"A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy. However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes. Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem? Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients. To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation. We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent. We demonstrate that AWMs can generate optimization landscapes that are easier to navigate even when compared to those from the simulator itself. This property allows transformer AWMs to produce better policies than competitive baselines in realistic long-horizon tasks.","sentences":["A natural approach for reinforcement learning is to predict future rewards by unrolling a neural network world model, and to backpropagate through the resulting computational graph to learn a policy.","However, this method often becomes impractical for long horizons since typical world models induce hard-to-optimize loss landscapes.","Transformers are known to efficiently propagate gradients overlong horizons: could they be the solution to this problem?","Surprisingly, we show that commonly-used transformer world models produce circuitous gradient paths, which can be detrimental to long-range policy gradients.","To tackle this challenge, we propose a class of world models called Actions World Models (AWMs), designed to provide more direct routes for gradient propagation.","We integrate such AWMs into a policy gradient framework that underscores the relationship between network architectures and the policy gradient updates they inherently represent.","We demonstrate that AWMs can generate optimization landscapes that are easier to navigate even when compared to those from the simulator itself.","This property allows transformer AWMs to produce better policies than competitive baselines in realistic long-horizon tasks."],"url":"http://arxiv.org/abs/2402.05290v1","category":"cs.LG"}
{"created":"2024-02-07 21:56:10","title":"Carousel phase retrieval algorithm for 3D coherent X-ray diffraction imaging","abstract":"Coherent X-ray Diffraction Imaging (CXDI) is a unique technique that allows reconstructing 2D and 3D objects at nanoscale resolution by performing computational phase reconstruction procedures based on measured scattered intensity maps. The reconstruction procedures can have a high computational complexity and typically cannot be performed in real time during experiments. We present a carousel phase retrieval algorithm (CPRA) that represents the 3D reconstruction problem as a set of 2D reconstructions of projected images corresponding to different collected angles based on the Fourier slice theorem. To maintain the consistency between the 2D reconstructions, we introduce an iterative procedure, in which each 2D reconstruction is based on the adjacent 2D reconstructions in a periodic (carousel) manner. The use of 2D reconstructions results in major reductions of the computational time and memory consumption. We show CPRA implementation on CPU and GPU computing architectures for a test problem of a complex biological cell of various spatial sizes. CPRA exhibits speed-of 300 time on GPUs and 120 times on CPUs as compared to conventional CXDI reconstruction algorithms. CPRA also can achieve a higher reconstruction quality. The achieved speed allows for a high-resolution reconstruction of computationally large objects in real time.","sentences":["Coherent X-ray Diffraction Imaging (CXDI) is a unique technique that allows reconstructing 2D and 3D objects at nanoscale resolution by performing computational phase reconstruction procedures based on measured scattered intensity maps.","The reconstruction procedures can have a high computational complexity and typically cannot be performed in real time during experiments.","We present a carousel phase retrieval algorithm (CPRA) that represents the 3D reconstruction problem as a set of 2D reconstructions of projected images corresponding to different collected angles based on the Fourier slice theorem.","To maintain the consistency between the 2D reconstructions, we introduce an iterative procedure, in which each 2D reconstruction is based on the adjacent 2D reconstructions in a periodic (carousel) manner.","The use of 2D reconstructions results in major reductions of the computational time and memory consumption.","We show CPRA implementation on CPU and GPU computing architectures for a test problem of a complex biological cell of various spatial sizes.","CPRA exhibits speed-of 300 time on GPUs and 120 times on CPUs as compared to conventional CXDI reconstruction algorithms.","CPRA also can achieve a higher reconstruction quality.","The achieved speed allows for a high-resolution reconstruction of computationally large objects in real time."],"url":"http://arxiv.org/abs/2402.05283v1","category":"physics.comp-ph"}
{"created":"2024-02-07 21:31:53","title":"Gradient descent induces alignment between weights and the empirical NTK for deep non-linear networks","abstract":"Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning. Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA). However, the reason these quantities become correlated during training is poorly understood. In this work, we explain the emergence of this correlation. We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights. We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment. We show that the speed of NFA development can be predicted analytically at early training times in terms of simple statistics of the inputs and labels. Finally, we introduce a simple intervention to increase NFA correlation at any given layer, which dramatically improves the quality of features learned.","sentences":["Understanding the mechanisms through which neural networks extract statistics from input-label pairs is one of the most important unsolved problems in supervised learning.","Prior works have identified that the gram matrices of the weights in trained neural networks of general architectures are proportional to the average gradient outer product of the model, in a statement known as the Neural Feature Ansatz (NFA).","However, the reason these quantities become correlated during training is poorly understood.","In this work, we explain the emergence of this correlation.","We identify that the NFA is equivalent to alignment between the left singular structure of the weight matrices and a significant component of the empirical neural tangent kernels associated with those weights.","We establish that the NFA introduced in prior works is driven by a centered NFA that isolates this alignment.","We show that the speed of NFA development can be predicted analytically at early training times in terms of simple statistics of the inputs and labels.","Finally, we introduce a simple intervention to increase NFA correlation at any given layer, which dramatically improves the quality of features learned."],"url":"http://arxiv.org/abs/2402.05271v1","category":"stat.ML"}
{"created":"2024-02-07 21:31:11","title":"Review and Analysis of Recent Advances in Intelligent Network Softwarization for the Internet of Things","abstract":"The Internet of Things (IoT) is an emerging technology that aims to connect heterogeneous and constrained objects to each other and to the Internet. It has grown significantly in a wide variety of applications such as smart homes, smart cities, smart vehicles, etc. The huge number of connected devices increases the challenges, as IoT provides diverse and complex network services with different requirements on a common infrastructure. Network Softwarization is the latest network paradigm that transforms traditional network processes to the separation of hardware and software by using some enabling network technologies such as Software Defined Networking (SDN) and Network Function Virtualization (NFV). Machine Learning (ML) plays an essential role in creating smarter IoT networks, as it has shown remarkable results in various domains. Given that the network softwarization allows it to be easily integrated, ML can play a crucial role in efficient and self-adaptive IoT networks. In this paper, we provide a detailed overview of the concepts of IoT, network softwarization, and ML, and we study and discuss the state of the art of intelligent ML-enabled network softwarization for IoT. We also identify the most prominent future research directions to be considered.","sentences":["The Internet of Things (IoT) is an emerging technology that aims to connect heterogeneous and constrained objects to each other and to the Internet.","It has grown significantly in a wide variety of applications such as smart homes, smart cities, smart vehicles, etc.","The huge number of connected devices increases the challenges, as IoT provides diverse and complex network services with different requirements on a common infrastructure.","Network Softwarization is the latest network paradigm that transforms traditional network processes to the separation of hardware and software by using some enabling network technologies such as Software Defined Networking (SDN) and Network Function Virtualization (NFV).","Machine Learning (ML) plays an essential role in creating smarter IoT networks, as it has shown remarkable results in various domains.","Given that the network softwarization allows it to be easily integrated, ML can play a crucial role in efficient and self-adaptive IoT networks.","In this paper, we provide a detailed overview of the concepts of IoT, network softwarization, and ML, and we study and discuss the state of the art of intelligent ML-enabled network softwarization for IoT.","We also identify the most prominent future research directions to be considered."],"url":"http://arxiv.org/abs/2402.05270v1","category":"cs.NI"}
{"created":"2024-02-07 21:07:36","title":"Enabling Architecture for Distributed Intelligent Network Softwarization for the Internet of Things","abstract":"The Internet of Things (IoT) is becoming a part of everyday life through its various sensing devices that collect valuable information. The huge number of interconnected heterogeneous IoT devices poses immense challenges, and network softwarization techniques are an adequate solution to these concerns. Software Defined Networking (SDN) and Network Function Virtualization (NFV) are two key softwarization techniques that enable the realization of efficient, agile IoT networks, especially when combined with Machine Learning (ML), mainly Federated Learning (FL). Unfortunately, existing solutions do not take advantage of such a combination to strengthen IoT networks in terms of efficiency and scalability. In this paper, we propose a novel architecture to achieve distributed intelligent network softwarization for IoT, in which SDN, NFV, and ML combine forces to enhance IoT constrained networks.","sentences":["The Internet of Things (IoT) is becoming a part of everyday life through its various sensing devices that collect valuable information.","The huge number of interconnected heterogeneous IoT devices poses immense challenges, and network softwarization techniques are an adequate solution to these concerns.","Software Defined Networking (SDN) and Network Function Virtualization (NFV) are two key softwarization techniques that enable the realization of efficient, agile IoT networks, especially when combined with Machine Learning (ML), mainly Federated Learning (FL).","Unfortunately, existing solutions do not take advantage of such a combination to strengthen IoT networks in terms of efficiency and scalability.","In this paper, we propose a novel architecture to achieve distributed intelligent network softwarization for IoT, in which SDN, NFV, and ML combine forces to enhance IoT constrained networks."],"url":"http://arxiv.org/abs/2402.05259v1","category":"cs.NI"}
{"created":"2024-02-07 21:00:59","title":"An Overview of Machine Learning-Enabled Network Softwarization for the Internet of Things","abstract":"The Internet of Things (IoT) has evolved from a novel technology to an integral part of our everyday lives. It encompasses a multitude of heterogeneous devices that collect valuable data through various sensors. The sheer volume of these interconnected devices poses significant challenges as IoT provides complex network services with diverse requirements on a shared infrastructure. Network softwarization could help address these issues as it has emerged as a paradigm that enhances traditional networking by decoupling hardware from software and leveraging enabling technologies such as Software Defined Networking (SDN) and Network Function Virtualization (NFV). In networking, Machine Learning (ML) has demonstrated impressive results across multiple domains. By smoothly integrating with network softwarization, ML plays a pivotal role in building efficient and intelligent IoT networks. This paper explores the fundamentals of IoT, network softwarization, and ML, while reviewing the latest advances in ML-enabled network softwarization for IoT.","sentences":["The Internet of Things (IoT) has evolved from a novel technology to an integral part of our everyday lives.","It encompasses a multitude of heterogeneous devices that collect valuable data through various sensors.","The sheer volume of these interconnected devices poses significant challenges as IoT provides complex network services with diverse requirements on a shared infrastructure.","Network softwarization could help address these issues as it has emerged as a paradigm that enhances traditional networking by decoupling hardware from software and leveraging enabling technologies such as Software Defined Networking (SDN) and Network Function Virtualization (NFV).","In networking, Machine Learning (ML) has demonstrated impressive results across multiple domains.","By smoothly integrating with network softwarization, ML plays a pivotal role in building efficient and intelligent IoT networks.","This paper explores the fundamentals of IoT, network softwarization, and ML, while reviewing the latest advances in ML-enabled network softwarization for IoT."],"url":"http://arxiv.org/abs/2402.05255v1","category":"cs.NI"}
{"created":"2024-02-07 20:53:53","title":"Learning Fair Ranking Policies via Differentiable Optimization of Ordered Weighted Averages","abstract":"Learning to Rank (LTR) is one of the most widely used machine learning applications. It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds. Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance. However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms. This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency. In particular, this paper is the first to show how to backpropagate through constrained optimizations of OWA objectives, enabling their use in integrated prediction and decision models.","sentences":["Learning to Rank (LTR) is one of the most widely used machine learning applications.","It is a key component in platforms with profound societal impacts, including job search, healthcare information retrieval, and social media content feeds.","Conventional LTR models have been shown to produce biases results, stimulating a discourse on how to address the disparities introduced by ranking systems that solely prioritize user relevance.","However, while several models of fair learning to rank have been proposed, they suffer from deficiencies either in accuracy or efficiency, thus limiting their applicability to real-world ranking platforms.","This paper shows how efficiently-solvable fair ranking models, based on the optimization of Ordered Weighted Average (OWA) functions, can be integrated into the training loop of an LTR model to achieve favorable balances between fairness, user utility, and runtime efficiency.","In particular, this paper is the first to show how to backpropagate through constrained optimizations of OWA objectives, enabling their use in integrated prediction and decision models."],"url":"http://arxiv.org/abs/2402.05252v1","category":"cs.LG"}
{"created":"2024-02-07 20:52:54","title":"A multiscale sensorimotor model of experience-dependent behavior in a minimal organism","abstract":"To survive in ever-changing environments, living organisms need to continuously combine the ongoing external inputs they receive, representing present conditions, with their dynamical internal state, which includes influences of past experiences. It is still unclear in general, however, (i) how this happens at the molecular and cellular levels, and (ii) how the corresponding molecular and cellular processes are integrated with the behavioral responses of the organism. Here we address these issues by modeling mathematically a particular behavioral paradigm in a minimal model organism, namely chemotaxis in the nematode C. elegans. Specifically, we use a long-standing collection of elegant experiments on salt chemotaxis in this animal, in which the migration direction varies depending on its previous experience. Our model integrates the molecular, cellular and organismal levels to reproduce the experimentally observed experience-dependent behavior. The model proposes specific molecular mechanisms for the encoding of current conditions and past experiences in key neurons associated with this response, predicting the behavior of various mutants associated with those molecular circuits.","sentences":["To survive in ever-changing environments, living organisms need to continuously combine the ongoing external inputs they receive, representing present conditions, with their dynamical internal state, which includes influences of past experiences.","It is still unclear in general, however, (i) how this happens at the molecular and cellular levels, and (ii) how the corresponding molecular and cellular processes are integrated with the behavioral responses of the organism.","Here we address these issues by modeling mathematically a particular behavioral paradigm in a minimal model organism, namely chemotaxis in the nematode C. elegans.","Specifically, we use a long-standing collection of elegant experiments on salt chemotaxis in this animal, in which the migration direction varies depending on its previous experience.","Our model integrates the molecular, cellular and organismal levels to reproduce the experimentally observed experience-dependent behavior.","The model proposes specific molecular mechanisms for the encoding of current conditions and past experiences in key neurons associated with this response, predicting the behavior of various mutants associated with those molecular circuits."],"url":"http://arxiv.org/abs/2402.05251v1","category":"q-bio.NC"}
{"created":"2024-02-07 20:12:27","title":"Universal Neural Functionals","abstract":"A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network. Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks. However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections. This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space. Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models. Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the weight space they optimize. We open-source our library for constructing UNFs at https://github.com/AllanYangZhou/universal_neural_functional.","sentences":["A challenging problem in many modern machine learning tasks is to process weight-space features, i.e., to transform or extract information from the weights and gradients of a neural network.","Recent works have developed promising weight-space models that are equivariant to the permutation symmetries of simple feedforward networks.","However, they are not applicable to general architectures, since the permutation symmetries of a weight space can be complicated by recurrence or residual connections.","This work proposes an algorithm that automatically constructs permutation equivariant models, which we refer to as universal neural functionals (UNFs), for any weight space.","Among other applications, we demonstrate how UNFs can be substituted into existing learned optimizer designs, and find promising improvements over prior methods when optimizing small image classifiers and language models.","Our results suggest that learned optimizers can benefit from considering the (symmetry) structure of the weight space they optimize.","We open-source our library for constructing UNFs at https://github.com/AllanYangZhou/universal_neural_functional."],"url":"http://arxiv.org/abs/2402.05232v1","category":"cs.LG"}
{"created":"2024-02-07 20:02:31","title":"FLARE: field line analysis and reconstruction for 3D plasma boundary modeling","abstract":"The FLARE code is a magnetic mesh generator that is integrated within a suite of tools for the analysis of the magnetic geometry in toroidal fusion devices. A magnetic mesh is constructed from field line segments and permits fast reconstruction of field lines in 3D plasma boundary codes such as EMC3-EIRENE. Both intrinsically non-axisymmetric configurations (stellarators) and those with symmetry breaking perturbations of an axisymmetric equilibrium (tokamaks) are supported. The code itself is written in Modern Fortran with MPI support for parallel computing, and it incorporates object-oriented programming for the definition of the magnetic field and the material surface geometry. Extended derived types for a number of different magnetohydrodynamic (MHD) equilibrium and plasma response models are implemented. The core element of FLARE is a field line tracer with adaptive step-size control, and this is integrated into tools for the construction of Poincar\\'e maps and invariant manifolds of X-points. A collection of high-level procedures that generate output files for visualization is build on top of that. The analysis modules are build with Python frontends that facilitate customization of tasks and/or scripting of parameter scans.","sentences":["The FLARE code is a magnetic mesh generator that is integrated within a suite of tools for the analysis of the magnetic geometry in toroidal fusion devices.","A magnetic mesh is constructed from field line segments and permits fast reconstruction of field lines in 3D plasma boundary codes such as EMC3-EIRENE.","Both intrinsically non-axisymmetric configurations (stellarators) and those with symmetry breaking perturbations of an axisymmetric equilibrium (tokamaks) are supported.","The code itself is written in Modern Fortran with MPI support for parallel computing, and it incorporates object-oriented programming for the definition of the magnetic field and the material surface geometry.","Extended derived types for a number of different magnetohydrodynamic (MHD) equilibrium and plasma response models are implemented.","The core element of FLARE is a field line tracer with adaptive step-size control, and this is integrated into tools for the construction of Poincar\\'e maps and invariant manifolds of X-points.","A collection of high-level procedures that generate output files for visualization is build on top of that.","The analysis modules are build with Python frontends that facilitate customization of tasks and/or scripting of parameter scans."],"url":"http://arxiv.org/abs/2402.05225v1","category":"physics.plasm-ph"}
{"created":"2024-02-07 20:02:09","title":"VerAs: Verify then Assess STEM Lab Reports","abstract":"With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills. A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations. Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills. Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes. While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports. We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answering (OpenQA). VerAs first verifies whether a report contains any content relevant to a given rubric dimension, and if so, assesses the relevant sentences. On the lab reports, VerAs outperforms multiple baselines based on OpenQA systems or Automated Essay Scoring (AES). VerAs also performs well on an analytic rubric for middle school physics essays.","sentences":["With an increasing focus in STEM education on critical thinking skills, science writing plays an ever more important role in curricula that stress inquiry skills.","A recently published dataset of two sets of college level lab reports from an inquiry-based physics curriculum relies on analytic assessment rubrics that utilize multiple dimensions, specifying subject matter knowledge and general components of good explanations.","Each analytic dimension is assessed on a 6-point scale, to provide detailed feedback to students that can help them improve their science writing skills.","Manual assessment can be slow, and difficult to calibrate for consistency across all students in large classes.","While much work exists on automated assessment of open-ended questions in STEM subjects, there has been far less work on long-form writing such as lab reports.","We present an end-to-end neural architecture that has separate verifier and assessment modules, inspired by approaches to Open Domain Question Answering (OpenQA).","VerAs first verifies whether a report contains any content relevant to a given rubric dimension, and if so, assesses the relevant sentences.","On the lab reports, VerAs outperforms multiple baselines based on OpenQA systems or Automated Essay Scoring (AES).","VerAs also performs well on an analytic rubric for middle school physics essays."],"url":"http://arxiv.org/abs/2402.05224v1","category":"cs.CL"}
{"created":"2024-02-07 20:01:41","title":"Taming Timeout Flakiness: An Empirical Study of SAP HANA","abstract":"Regression testing aims to prevent code changes from breaking existing features. Flaky tests negatively affect regression testing because they result in test failures that are not necessarily caused by code changes, thus providing an ambiguous signal. Test timeouts are one contributing factor to such flaky test failures. With the goal of reducing test flakiness in SAP HANA, we empirically study the impact of test timeouts on flakiness in system tests. We evaluate different approaches to automatically adjust timeout values, assessing their suitability for reducing execution time costs and improving build turnaround times. We collect metadata on SAP HANA's test executions by repeatedly executing tests on the same code revision over a period of six months. We analyze the test flakiness rate, investigate the evolution of test timeout values, and evaluate different approaches for optimizing timeout values. The test flakiness rate ranges from 49% to 70%, depending on the number of repeated test executions. Test timeouts account for 70% of flaky test failures. Developers typically react to flaky timeouts by manually increasing timeout values or splitting long-running tests. However, manually adjusting timeout values is a tedious task. Our approach for timeout optimization reduces timeout-related flaky failures by 80% and reduces the overall median timeout value by 25%, i.e., blocked tests are identified faster. Test timeouts are a major contributing factor to flakiness in system tests. It is challenging for developers to effectively mitigate this problem manually. Our technique for optimizing timeout values reduces flaky failures while minimizing test costs. Practitioners working on large-scale industrial software systems can use our findings to increase the effectiveness of their system tests while reducing the burden on developers to manually maintain appropriate timeout values.","sentences":["Regression testing aims to prevent code changes from breaking existing features.","Flaky tests negatively affect regression testing because they result in test failures that are not necessarily caused by code changes, thus providing an ambiguous signal.","Test timeouts are one contributing factor to such flaky test failures.","With the goal of reducing test flakiness in SAP HANA, we empirically study the impact of test timeouts on flakiness in system tests.","We evaluate different approaches to automatically adjust timeout values, assessing their suitability for reducing execution time costs and improving build turnaround times.","We collect metadata on SAP HANA's test executions by repeatedly executing tests on the same code revision over a period of six months.","We analyze the test flakiness rate, investigate the evolution of test timeout values, and evaluate different approaches for optimizing timeout values.","The test flakiness rate ranges from 49% to 70%, depending on the number of repeated test executions.","Test timeouts account for 70% of flaky test failures.","Developers typically react to flaky timeouts by manually increasing timeout values or splitting long-running tests.","However, manually adjusting timeout values is a tedious task.","Our approach for timeout optimization reduces timeout-related flaky failures by 80% and reduces the overall median timeout value by 25%, i.e., blocked tests are identified faster.","Test timeouts are a major contributing factor to flakiness in system tests.","It is challenging for developers to effectively mitigate this problem manually.","Our technique for optimizing timeout values reduces flaky failures while minimizing test costs.","Practitioners working on large-scale industrial software systems can use our findings to increase the effectiveness of their system tests while reducing the burden on developers to manually maintain appropriate timeout values."],"url":"http://arxiv.org/abs/2402.05223v1","category":"cs.SE"}
{"created":"2024-02-07 19:20:22","title":"Giving Robots a Voice: Human-in-the-Loop Voice Creation and open-ended Labeling","abstract":"Speech is a natural interface for humans to interact with robots. Yet, aligning a robot's voice to its appearance is challenging due to the rich vocabulary of both modalities. Previous research has explored a few labels to describe robots and tested them on a limited number of robots and existing voices. Here, we develop a robot-voice creation tool followed by large-scale behavioral human experiments (N=2,505). First, participants collectively tune robotic voices to match 175 robot images using an adaptive human-in-the-loop pipeline. Then, participants describe their impression of the robot or their matched voice using another human-in-the-loop paradigm for open-ended labeling. The elicited taxonomy is then used to rate robot attributes and to predict the best voice for an unseen robot. We offer a web interface to aid engineers in customizing robot voices, demonstrating the synergy between cognitive science and machine learning for engineering tools.","sentences":["Speech is a natural interface for humans to interact with robots.","Yet, aligning a robot's voice to its appearance is challenging due to the rich vocabulary of both modalities.","Previous research has explored a few labels to describe robots and tested them on a limited number of robots and existing voices.","Here, we develop a robot-voice creation tool followed by large-scale behavioral human experiments (N=2,505).","First, participants collectively tune robotic voices to match 175 robot images using an adaptive human-in-the-loop pipeline.","Then, participants describe their impression of the robot or their matched voice using another human-in-the-loop paradigm for open-ended labeling.","The elicited taxonomy is then used to rate robot attributes and to predict the best voice for an unseen robot.","We offer a web interface to aid engineers in customizing robot voices, demonstrating the synergy between cognitive science and machine learning for engineering tools."],"url":"http://arxiv.org/abs/2402.05206v1","category":"cs.HC"}
{"created":"2024-02-07 19:16:26","title":"Electron beam emittance at operational intensity in fourth-generation synchrotron light sources","abstract":"For synchrotron light sources, the brightness of user X-ray beams is primarily determined by the electron beam emittance and energy spread at operational intensity. A common feature of fourth-generation synchrotrons is the short length of electron bunches combined with a very small transverse beam size. Consequently, the particle density is much higher than in machines of previous generations, leading to strong collective effects that significantly increase the emittance and limit the achievable brightness at operational beam intensity. In this article, we summarize our studies of the emittance scaled with the beam energy and intensity, taking into account the effects of intrabeam scattering, beam-impedance interaction, and bunch lengthening provided by higher-harmonic RF systems, to identify optimal combinations of machine and beam parameters.","sentences":["For synchrotron light sources, the brightness of user X-ray beams is primarily determined by the electron beam emittance and energy spread at operational intensity.","A common feature of fourth-generation synchrotrons is the short length of electron bunches combined with a very small transverse beam size.","Consequently, the particle density is much higher than in machines of previous generations, leading to strong collective effects that significantly increase the emittance and limit the achievable brightness at operational beam intensity.","In this article, we summarize our studies of the emittance scaled with the beam energy and intensity, taking into account the effects of intrabeam scattering, beam-impedance interaction, and bunch lengthening provided by higher-harmonic RF systems, to identify optimal combinations of machine and beam parameters."],"url":"http://arxiv.org/abs/2402.05204v1","category":"physics.acc-ph"}
{"created":"2024-02-07 19:15:30","title":"UEyes: An Eye-Tracking Dataset across User Interface Types","abstract":"Different types of user interfaces differ significantly in the number of elements and how they are displayed. To examine how such differences affect the way users look at UIs, we collected and analyzed a large eye-tracking-based dataset, UEyes (62 participants, 1,980 UI screenshots, near 20K eye movement sequences), covering four major UI types: webpage, desktop UI, mobile UI, and poster. Furthermore, we analyze and discuss the differences in important factors, such as color, location, and gaze direction across UI types, individual viewing strategies and potential future directions. This position paper is a derivative of our recent paper with a particular focus on the UEyes dataset.","sentences":["Different types of user interfaces differ significantly in the number of elements and how they are displayed.","To examine how such differences affect the way users look at UIs, we collected and analyzed a large eye-tracking-based dataset, UEyes (62 participants, 1,980 UI screenshots, near 20K eye movement sequences), covering four major UI types: webpage, desktop UI, mobile UI, and poster.","Furthermore, we analyze and discuss the differences in important factors, such as color, location, and gaze direction across UI types, individual viewing strategies and potential future directions.","This position paper is a derivative of our recent paper with a particular focus on the UEyes dataset."],"url":"http://arxiv.org/abs/2402.05202v1","category":"cs.HC"}
{"created":"2024-02-07 19:11:23","title":"The Effect of Sampling Temperature on Problem Solving in Large Language Models","abstract":"In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks. We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks. Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0. Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks. In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain. All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature.","sentences":["In this research study, we empirically investigate the effect of sampling temperature on the performance of Large Language Models (LLMs) on various problem-solving tasks.","We created a multiple-choice question-and-answer (MCQA) exam by randomly sampling problems from standard LLM benchmarks.","Then, we used four popular LLMs with five prompt-engineering techniques to solve the MCQA problems while increasing the sampling temperature from 0.0 to 1.0.","Despite anecdotal reports to the contrary, our empirical results indicate that changes in temperature in the range 0.0 to 1.0 do not have a statistically significant impact on LLM performance for problem-solving tasks.","In addition, these results appear to hold regardless of the LLM, the prompt-engineering technique, or the problem domain.","All code, data, and supplemental materials are available on GitHub at: https://github.com/matthewrenze/jhu-llm-temperature."],"url":"http://arxiv.org/abs/2402.05201v1","category":"cs.CL"}
{"created":"2024-02-07 19:10:36","title":"Are LLMs Ready for Real-World Materials Discovery?","abstract":"Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science. While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools. In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge. Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing. The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist. As such, we describe key materials science information extraction challenges which need to be overcome in order to build large-scale, multi-modal datasets that capture valuable materials science knowledge. Finally, we outline a roadmap for applying future MatSci-LLMs for real-world materials discovery via: 1. Automated Knowledge Base Generation; 2. Automated In-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials Laboratories.","sentences":["Large Language Models (LLMs) create exciting possibilities for powerful language processing tools to accelerate research in materials science.","While LLMs have great potential to accelerate materials understanding and discovery, they currently fall short in being practical materials science tools.","In this position paper, we show relevant failure cases of LLMs in materials science that reveal current limitations of LLMs related to comprehending and reasoning over complex, interconnected materials science knowledge.","Given those shortcomings, we outline a framework for developing Materials Science LLMs (MatSci-LLMs) that are grounded in materials science knowledge and hypothesis generation followed by hypothesis testing.","The path to attaining performant MatSci-LLMs rests in large part on building high-quality, multi-modal datasets sourced from scientific literature where various information extraction challenges persist.","As such, we describe key materials science information extraction challenges which need to be overcome in order to build large-scale, multi-modal datasets that capture valuable materials science knowledge.","Finally, we outline a roadmap for applying future MatSci-LLMs for real-world materials discovery via: 1. Automated Knowledge Base Generation;","2. Automated In-Silico Material Design; and 3.","MatSci-LLM Integrated Self-Driving Materials Laboratories."],"url":"http://arxiv.org/abs/2402.05200v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 19:05:37","title":"Multi-class classification of biomechanical data: A functional LDA approach based on multi-class penalized functional PLS","abstract":"A functional linear discriminant analysis approach to classify a set of kinematic data (human movement curves of individuals performing different physical activities) is performed. Kinematic data, usually collected in linear acceleration or angular rotation format, can be identified with functions in a continuous domain (time, percentage of gait cycle, etc.). Since kinematic curves are measured in the same sample of individuals performing different activities, they are a clear example of functional data with repeated measures. On the other hand, the sample curves are observed with noise. Then, a roughness penalty might be necessary in order to provide a smooth estimation of the discriminant functions, which would make them more interpretable. Moreover, because of the infinite dimension of functional data, a reduction dimension technique should be considered. To solve these problems, we propose a multi-class approach for penalized functional partial least squares (FPLS) regression. Then linear discriminant analysis (LDA) will be performed on the estimated FPLS components. This methodology is motivated by two case studies. The first study considers the linear acceleration recorded every two seconds in 30 subjects, related to three different activities (walking, climbing stairs and down stairs). The second study works with the triaxial angular rotation, for each joint, in 51 children when they completed a cycle walking under three conditions (walking, carrying a backpack and pulling a trolley). A simulation study is also developed for comparing the performance of the proposed functional LDA with respect to the corresponding multivariate and non-penalized approaches.","sentences":["A functional linear discriminant analysis approach to classify a set of kinematic data (human movement curves of individuals performing different physical activities) is performed.","Kinematic data, usually collected in linear acceleration or angular rotation format, can be identified with functions in a continuous domain (time, percentage of gait cycle, etc.).","Since kinematic curves are measured in the same sample of individuals performing different activities, they are a clear example of functional data with repeated measures.","On the other hand, the sample curves are observed with noise.","Then, a roughness penalty might be necessary in order to provide a smooth estimation of the discriminant functions, which would make them more interpretable.","Moreover, because of the infinite dimension of functional data, a reduction dimension technique should be considered.","To solve these problems, we propose a multi-class approach for penalized functional partial least squares (FPLS) regression.","Then linear discriminant analysis (LDA) will be performed on the estimated FPLS components.","This methodology is motivated by two case studies.","The first study considers the linear acceleration recorded every two seconds in 30 subjects, related to three different activities (walking, climbing stairs and down stairs).","The second study works with the triaxial angular rotation, for each joint, in 51 children when they completed a cycle walking under three conditions (walking, carrying a backpack and pulling a trolley).","A simulation study is also developed for comparing the performance of the proposed functional LDA with respect to the corresponding multivariate and non-penalized approaches."],"url":"http://arxiv.org/abs/2402.05194v1","category":"stat.ME"}
{"created":"2024-02-07 19:01:11","title":"InCoRo: In-Context Learning for Robotics Control with Feedback Loops","abstract":"One of the challenges in robotics is to enable robotic units with the reasoning capability that would be robust enough to execute complex tasks in dynamic environments. Recent advances in LLMs have positioned them as go-to tools for simple reasoning tasks, motivating the pioneering work of Liang et al. [35] that uses an LLM to translate natural language commands into low-level static execution plans for robotic units. Using LLMs inside robotics systems brings their generalization to a new level, enabling zero-shot generalization to new tasks. This paper extends this prior work to dynamic environments. We propose InCoRo, a system that uses a classical robotic feedback loop composed of an LLM controller, a scene understanding unit, and a robot. Our system continuously analyzes the state of the environment and provides adapted execution commands, enabling the robot to adjust to changing environmental conditions and correcting for controller errors. Our system does not require any iterative optimization to learn to accomplish a task as it leverages in-context learning with an off-the-shelf LLM model. Through an extensive validation process involving two standardized industrial robotic units -- SCARA and DELTA types -- we contribute knowledge about these robots, not popular in the community, thereby enriching it. We highlight the generalization capabilities of our system and show that (1) in-context learning in combination with the current state-of-the-art LLMs is an effective way to implement a robotic controller; (2) in static environments, InCoRo surpasses the prior art in terms of the success rate; (3) in dynamic environments, we establish new state-of-the-art for the SCARA and DELTA units, respectively. This research paves the way towards building reliable, efficient, intelligent autonomous systems that adapt to dynamic environments.","sentences":["One of the challenges in robotics is to enable robotic units with the reasoning capability that would be robust enough to execute complex tasks in dynamic environments.","Recent advances in LLMs have positioned them as go-to tools for simple reasoning tasks, motivating the pioneering work of Liang et al.","[35] that uses an LLM to translate natural language commands into low-level static execution plans for robotic units.","Using LLMs inside robotics systems brings their generalization to a new level, enabling zero-shot generalization to new tasks.","This paper extends this prior work to dynamic environments.","We propose InCoRo, a system that uses a classical robotic feedback loop composed of an LLM controller, a scene understanding unit, and a robot.","Our system continuously analyzes the state of the environment and provides adapted execution commands, enabling the robot to adjust to changing environmental conditions and correcting for controller errors.","Our system does not require any iterative optimization to learn to accomplish a task as it leverages in-context learning with an off-the-shelf LLM model.","Through an extensive validation process involving two standardized industrial robotic units -- SCARA and DELTA types -- we contribute knowledge about these robots, not popular in the community, thereby enriching it.","We highlight the generalization capabilities of our system and show that (1) in-context learning in combination with the current state-of-the-art LLMs is an effective way to implement a robotic controller; (2) in static environments, InCoRo surpasses the prior art in terms of the success rate; (3) in dynamic environments, we establish new state-of-the-art for the SCARA and DELTA units, respectively.","This research paves the way towards building reliable, efficient, intelligent autonomous systems that adapt to dynamic environments."],"url":"http://arxiv.org/abs/2402.05188v1","category":"cs.RO"}
{"created":"2024-02-07 19:00:02","title":"cecilia: A Machine Learning-Based Pipeline for Measuring Metal Abundances of Helium-rich Polluted White Dwarfs","abstract":"Over the past several decades, conventional spectral analysis techniques of polluted white dwarfs have become powerful tools to learn about the geology and chemistry of extrasolar bodies. Despite their proven capabilities and extensive legacy of scientific discoveries, these techniques are however still limited by their manual, time-intensive, and iterative nature. As a result, they are susceptible to human errors and are difficult to scale up to population-wide studies of metal pollution. This paper seeks to address this problem by presenting cecilia, the first Machine Learning (ML)-powered spectral modeling code designed to measure the metal abundances of intermediate-temperature (10,000$\\leq T_{\\rm eff} \\leq$20,000 K), Helium-rich polluted white dwarfs. Trained with more than 22,000 randomly drawn atmosphere models and stellar parameters, our pipeline aims to overcome the limitations of classical methods by replacing the generation of synthetic spectra from computationally expensive codes and uniformly spaced model grids, with a fast, automated, and efficient neural-network-based interpolator. More specifically, cecilia combines state-of-the-art atmosphere models, powerful artificial intelligence tools, and robust statistical techniques to rapidly generate synthetic spectra of polluted white dwarfs in high-dimensional space, and enable accurate ($\\lesssim$0.1 dex) and simultaneous measurements of 14 stellar parameters -- including 11 elemental abundances -- from real spectroscopic observations. As massively multiplexed astronomical surveys begin scientific operations, cecilia's performance has the potential to unlock large-scale studies of extrasolar geochemistry and propel the field of white dwarf science into the era of Big Data. In doing so, we aspire to uncover new statistical insights that were previously impractical with traditional white dwarf characterisation techniques.","sentences":["Over the past several decades, conventional spectral analysis techniques of polluted white dwarfs have become powerful tools to learn about the geology and chemistry of extrasolar bodies.","Despite their proven capabilities and extensive legacy of scientific discoveries, these techniques are however still limited by their manual, time-intensive, and iterative nature.","As a result, they are susceptible to human errors and are difficult to scale up to population-wide studies of metal pollution.","This paper seeks to address this problem by presenting cecilia, the first Machine Learning (ML)-powered spectral modeling code designed to measure the metal abundances of intermediate-temperature (10,000$\\leq T_{\\rm eff} \\leq$20,000 K), Helium-rich polluted white dwarfs.","Trained with more than 22,000 randomly drawn atmosphere models and stellar parameters, our pipeline aims to overcome the limitations of classical methods by replacing the generation of synthetic spectra from computationally expensive codes and uniformly spaced model grids, with a fast, automated, and efficient neural-network-based interpolator.","More specifically, cecilia combines state-of-the-art atmosphere models, powerful artificial intelligence tools, and robust statistical techniques to rapidly generate synthetic spectra of polluted white dwarfs in high-dimensional space, and enable accurate ($\\lesssim$0.1 dex) and simultaneous measurements of 14 stellar parameters -- including 11 elemental abundances -- from real spectroscopic observations.","As massively multiplexed astronomical surveys begin scientific operations, cecilia's performance has the potential to unlock large-scale studies of extrasolar geochemistry and propel the field of white dwarf science into the era of Big Data.","In doing so, we aspire to uncover new statistical insights that were previously impractical with traditional white dwarf characterisation techniques."],"url":"http://arxiv.org/abs/2402.05176v1","category":"astro-ph.IM"}
{"created":"2024-02-07 18:58:18","title":"A Resource Model For Neural Scaling Law","abstract":"Neural scaling laws characterize how model performance improves as the model size scales up. Inspired by empirical observations, we introduce a resource model of neural scaling. A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks). On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons. (2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants. We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556. We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural networks.","sentences":["Neural scaling laws characterize how model performance improves as the model size scales up.","Inspired by empirical observations, we introduce a resource model of neural scaling.","A task is usually composite hence can be decomposed into many subtasks, which compete for resources (measured by the number of neurons allocated to subtasks).","On toy problems, we empirically find that: (1) The loss of a subtask is inversely proportional to its allocated neurons.","(2) When multiple subtasks are present in a composite task, the resources acquired by each subtask uniformly grow as models get larger, keeping the ratios of acquired resources constants.","We hypothesize these findings to be generally true and build a model to predict neural scaling laws for general composite tasks, which successfully replicates the neural scaling law of Chinchilla models reported in arXiv:2203.15556.","We believe that the notion of resource used in this paper will be a useful tool for characterizing and diagnosing neural networks."],"url":"http://arxiv.org/abs/2402.05164v1","category":"cs.LG"}
{"created":"2024-02-07 18:34:38","title":"Assessing the Brittleness of Safety Alignment via Pruning and Low-Rank Modifications","abstract":"Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning. This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications. We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels. Surprisingly, the isolated regions we find are sparse, comprising about $3\\%$ at the parameter level and $2.5\\%$ at the rank level. Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms. Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted. These findings underscore the urgent need for more robust safety strategies in LLMs.","sentences":["Large language models (LLMs) show inherent brittleness in their safety mechanisms, as evidenced by their susceptibility to jailbreaking and even non-malicious fine-tuning.","This study explores this brittleness of safety alignment by leveraging pruning and low-rank modifications.","We develop methods to identify critical regions that are vital for safety guardrails, and that are disentangled from utility-relevant regions at both the neuron and rank levels.","Surprisingly, the isolated regions we find are sparse, comprising about $3\\%$ at the parameter level and $2.5\\%$ at the rank level.","Removing these regions compromises safety without significantly impacting utility, corroborating the inherent brittleness of the model's safety mechanisms.","Moreover, we show that LLMs remain vulnerable to low-cost fine-tuning attacks even when modifications to the safety-critical regions are restricted.","These findings underscore the urgent need for more robust safety strategies in LLMs."],"url":"http://arxiv.org/abs/2402.05162v1","category":"cs.LG"}
{"created":"2024-02-07 18:04:32","title":"What's documented in AI? Systematic Analysis of 32K AI Model Cards","abstract":"The rapid proliferation of AI models has underscored the importance of thorough documentation, as it enables users to understand, trust, and effectively utilize these models in various applications. Although developers are encouraged to produce model cards, it's not clear how much information or what information these cards contain. In this study, we conduct a comprehensive analysis of 32,111 AI model documentations on Hugging Face, a leading platform for distributing and deploying AI models. Our investigation sheds light on the prevailing model card documentation practices. Most of the AI models with substantial downloads provide model cards, though the cards have uneven informativeness. We find that sections addressing environmental impact, limitations, and evaluation exhibit the lowest filled-out rates, while the training section is the most consistently filled-out. We analyze the content of each section to characterize practitioners' priorities. Interestingly, there are substantial discussions of data, sometimes with equal or even greater emphasis than the model itself. To evaluate the impact of model cards, we conducted an intervention study by adding detailed model cards to 42 popular models which had no or sparse model cards previously. We find that adding model cards is moderately correlated with an increase weekly download rates. Our study opens up a new perspective for analyzing community norms and practices for model documentation through large-scale data science and linguistics analysis.","sentences":["The rapid proliferation of AI models has underscored the importance of thorough documentation, as it enables users to understand, trust, and effectively utilize these models in various applications.","Although developers are encouraged to produce model cards, it's not clear how much information or what information these cards contain.","In this study, we conduct a comprehensive analysis of 32,111 AI model documentations on Hugging Face, a leading platform for distributing and deploying AI models.","Our investigation sheds light on the prevailing model card documentation practices.","Most of the AI models with substantial downloads provide model cards, though the cards have uneven informativeness.","We find that sections addressing environmental impact, limitations, and evaluation exhibit the lowest filled-out rates, while the training section is the most consistently filled-out.","We analyze the content of each section to characterize practitioners' priorities.","Interestingly, there are substantial discussions of data, sometimes with equal or even greater emphasis than the model itself.","To evaluate the impact of model cards, we conducted an intervention study by adding detailed model cards to 42 popular models which had no or sparse model cards previously.","We find that adding model cards is moderately correlated with an increase weekly download rates.","Our study opens up a new perspective for analyzing community norms and practices for model documentation through large-scale data science and linguistics analysis."],"url":"http://arxiv.org/abs/2402.05160v1","category":"cs.SE"}
{"created":"2024-02-07 18:02:33","title":"Enhancement of Bengali OCR by Specialized Models and Advanced Techniques for Diverse Document Types","abstract":"This research paper presents a unique Bengali OCR system with some capabilities. The system excels in reconstructing document layouts while preserving structure, alignment, and images. It incorporates advanced image and signature detection for accurate extraction. Specialized models for word segmentation cater to diverse document types, including computer-composed, letterpress, typewriter, and handwritten documents. The system handles static and dynamic handwritten inputs, recognizing various writing styles. Furthermore, it has the ability to recognize compound characters in Bengali. Extensive data collection efforts provide a diverse corpus, while advanced technical components optimize character and word recognition. Additional contributions include image, logo, signature and table recognition, perspective correction, layout reconstruction, and a queuing module for efficient and scalable processing. The system demonstrates outstanding performance in efficient and accurate text extraction and analysis.","sentences":["This research paper presents a unique Bengali OCR system with some capabilities.","The system excels in reconstructing document layouts while preserving structure, alignment, and images.","It incorporates advanced image and signature detection for accurate extraction.","Specialized models for word segmentation cater to diverse document types, including computer-composed, letterpress, typewriter, and handwritten documents.","The system handles static and dynamic handwritten inputs, recognizing various writing styles.","Furthermore, it has the ability to recognize compound characters in Bengali.","Extensive data collection efforts provide a diverse corpus, while advanced technical components optimize character and word recognition.","Additional contributions include image, logo, signature and table recognition, perspective correction, layout reconstruction, and a queuing module for efficient and scalable processing.","The system demonstrates outstanding performance in efficient and accurate text extraction and analysis."],"url":"http://arxiv.org/abs/2402.05158v1","category":"cs.CV"}
{"created":"2024-02-07 16:31:58","title":"What About the Data? A Mapping Study on Data Engineering for AI Systems","abstract":"AI systems cannot exist without data. Now that AI models (data science and AI) have matured and are readily available to apply in practice, most organizations struggle with the data infrastructure to do so. There is a growing need for data engineers that know how to prepare data for AI systems or that can setup enterprise-wide data architectures for analytical projects. But until now, the data engineering part of AI engineering has not been getting much attention, in favor of discussing the modeling part. In this paper we aim to change this by perform a mapping study on data engineering for AI systems, i.e., AI data engineering. We found 25 relevant papers between January 2019 and June 2023, explaining AI data engineering activities. We identify which life cycle phases are covered, which technical solutions or architectures are proposed and which lessons learned are presented. We end by an overall discussion of the papers with implications for practitioners and researchers. This paper creates an overview of the body of knowledge on data engineering for AI. This overview is useful for practitioners to identify solutions and best practices as well as for researchers to identify gaps.","sentences":["AI systems cannot exist without data.","Now that AI models (data science and AI) have matured and are readily available to apply in practice, most organizations struggle with the data infrastructure to do so.","There is a growing need for data engineers that know how to prepare data for AI systems or that can setup enterprise-wide data architectures for analytical projects.","But until now, the data engineering part of AI engineering has not been getting much attention, in favor of discussing the modeling part.","In this paper we aim to change this by perform a mapping study on data engineering for AI systems, i.e., AI data engineering.","We found 25 relevant papers between January 2019 and June 2023, explaining AI data engineering activities.","We identify which life cycle phases are covered, which technical solutions or architectures are proposed and which lessons learned are presented.","We end by an overall discussion of the papers with implications for practitioners and researchers.","This paper creates an overview of the body of knowledge on data engineering for AI.","This overview is useful for practitioners to identify solutions and best practices as well as for researchers to identify gaps."],"url":"http://arxiv.org/abs/2402.05156v1","category":"cs.DL"}
{"created":"2024-02-07 15:21:18","title":"Adaptive Hypergraph Network for Trust Prediction","abstract":"Trust plays an essential role in an individual's decision-making. Traditional trust prediction models rely on pairwise correlations to infer potential relationships between users. However, in the real world, interactions between users are usually complicated rather than pairwise only. Hypergraphs offer a flexible approach to modeling these complex high-order correlations (not just pairwise connections), since hypergraphs can leverage hyperedeges to link more than two nodes. However, most hypergraph-based methods are generic and cannot be well applied to the trust prediction task. In this paper, we propose an Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that improves trust prediction accuracy by using higher-order correlations. AHNTP utilizes Motif-based PageRank to capture high-order social influence information. In addition, it constructs hypergroups from both node-level and structure-level attributes to incorporate complex correlation information. Furthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network (GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user embeddings, facilitating trust relationship prediction. To enhance model generalization and robustness, we introduce a novel supervised contrastive learning loss for optimization. Extensive experiments demonstrate the superiority of our model over the state-of-the-art approaches in terms of trust prediction accuracy. The source code of this work can be accessed via https://github.com/Sherry-XU1995/AHNTP.","sentences":["Trust plays an essential role in an individual's decision-making.","Traditional trust prediction models rely on pairwise correlations to infer potential relationships between users.","However, in the real world, interactions between users are usually complicated rather than pairwise only.","Hypergraphs offer a flexible approach to modeling these complex high-order correlations (not just pairwise connections), since hypergraphs can leverage hyperedeges to link more than two nodes.","However, most hypergraph-based methods are generic and cannot be well applied to the trust prediction task.","In this paper, we propose an Adaptive Hypergraph Network for Trust Prediction (AHNTP), a novel approach that improves trust prediction accuracy by using higher-order correlations.","AHNTP utilizes Motif-based PageRank to capture high-order social influence information.","In addition, it constructs hypergroups from both node-level and structure-level attributes to incorporate complex correlation information.","Furthermore, AHNTP leverages adaptive hypergraph Graph Convolutional Network (GCN) layers and multilayer perceptrons (MLPs) to generate comprehensive user embeddings, facilitating trust relationship prediction.","To enhance model generalization and robustness, we introduce a novel supervised contrastive learning loss for optimization.","Extensive experiments demonstrate the superiority of our model over the state-of-the-art approaches in terms of trust prediction accuracy.","The source code of this work can be accessed via https://github.com/Sherry-XU1995/AHNTP."],"url":"http://arxiv.org/abs/2402.05154v1","category":"cs.SI"}
{"created":"2024-02-07 13:51:33","title":"Estimating On-road Transportation Carbon Emissions from Open Data of Road Network and Origin-destination Flow Data","abstract":"Accounting for over 20% of the total carbon emissions, the precise estimation of on-road transportation carbon emissions is crucial for carbon emission monitoring and efficient mitigation policy formulation. However, existing estimation methods typically depend on hard-to-collect individual statistics of vehicle miles traveled to calculate emissions, thereby suffering from high data collection difficulty. To relieve this issue by utilizing the strong pattern recognition of artificial intelligence, we incorporate two sources of open data representative of the transportation demand and capacity factors, the origin-destination (OD) flow data and the road network data, to build a hierarchical heterogeneous graph learning method for on-road carbon emission estimation (HENCE). Specifically, a hierarchical graph consisting of the road network level, community level, and region level is constructed to model the multi-scale road network-based connectivity and travel connection between spatial areas. Heterogeneous graphs consisting of OD links and spatial links are further built at both the community level and region level to capture the intrinsic interactions between travel demand and road network accessibility. Extensive experiments on two large-scale real-world datasets demonstrate HENCE's effectiveness and superiority with R-squared exceeding 0.75 and outperforming baselines by 9.60% on average, validating its success in pioneering the use of artificial intelligence to empower carbon emission management and sustainability development. The implementation codes are available at this link: https://github.com/tsinghua-fib-lab/HENCE.","sentences":["Accounting for over 20% of the total carbon emissions, the precise estimation of on-road transportation carbon emissions is crucial for carbon emission monitoring and efficient mitigation policy formulation.","However, existing estimation methods typically depend on hard-to-collect individual statistics of vehicle miles traveled to calculate emissions, thereby suffering from high data collection difficulty.","To relieve this issue by utilizing the strong pattern recognition of artificial intelligence, we incorporate two sources of open data representative of the transportation demand and capacity factors, the origin-destination (OD) flow data and the road network data, to build a hierarchical heterogeneous graph learning method for on-road carbon emission estimation (HENCE).","Specifically, a hierarchical graph consisting of the road network level, community level, and region level is constructed to model the multi-scale road network-based connectivity and travel connection between spatial areas.","Heterogeneous graphs consisting of OD links and spatial links are further built at both the community level and region level to capture the intrinsic interactions between travel demand and road network accessibility.","Extensive experiments on two large-scale real-world datasets demonstrate HENCE's effectiveness and superiority with R-squared exceeding 0.75 and outperforming baselines by 9.60% on average, validating its success in pioneering the use of artificial intelligence to empower carbon emission management and sustainability development.","The implementation codes are available at this link: https://github.com/tsinghua-fib-lab/HENCE."],"url":"http://arxiv.org/abs/2402.05153v1","category":"cs.LG"}
{"created":"2024-02-08 18:56:08","title":"Mean field control of droplet dynamics with high order finite element computations","abstract":"Liquid droplet dynamics are widely used in biological and engineering applications, which contain complex interfacial instabilities and pattern formulation such as droplet merging, splitting, and transport. This paper studies a class of mean field control formulation towards these droplet dynamics. They are used to control and maintain the manipulation of droplets in applications. We first formulate the droplet dynamics as gradient flows of free energies in modified optimal transport metrics with nonlinear mobilities. We then design an optimal control problem for these gradient flows. We lastly apply the primal-dual hybrid gradient algorithm with high-order finite element methods to simulate the proposed mean field control problems. Numerical examples, including droplet formation, bead-up/spreading, transport, and merging/splitting on a two-dimensional spatial domain, demonstrate the effectiveness of the proposed mean field control mechanism.","sentences":["Liquid droplet dynamics are widely used in biological and engineering applications, which contain complex interfacial instabilities and pattern formulation such as droplet merging, splitting, and transport.","This paper studies a class of mean field control formulation towards these droplet dynamics.","They are used to control and maintain the manipulation of droplets in applications.","We first formulate the droplet dynamics as gradient flows of free energies in modified optimal transport metrics with nonlinear mobilities.","We then design an optimal control problem for these gradient flows.","We lastly apply the primal-dual hybrid gradient algorithm with high-order finite element methods to simulate the proposed mean field control problems.","Numerical examples, including droplet formation, bead-up/spreading, transport, and merging/splitting on a two-dimensional spatial domain, demonstrate the effectiveness of the proposed mean field control mechanism."],"url":"http://arxiv.org/abs/2402.05923v1","category":"math.OC"}
{"created":"2024-02-08 18:52:35","title":"Consensus-driven Deviated Pursuit for Guaranteed Simultaneous Interception of Moving Targets","abstract":"This work proposes a cooperative strategy that employs deviated pursuit guidance to simultaneously intercept a moving (but not manoeuvring) target. As opposed to many existing cooperative guidance strategies which use estimates of time-to-go, based on proportional-navigation guidance, the proposed strategy uses an exact expression for time-to-go to ensure simultaneous interception. The guidance design considers nonlinear engagement kinematics, allowing the proposed strategy to remain effective over a large operating regime. Unlike existing strategies on simultaneous interception that achieve interception at the average value of their initial time-to-go estimates, this work provides flexibility in the choice of impact time. By judiciously choosing the edge weights of the communication network, a weighted consensus in time-to-go can be achieved. It has been shown that by allowing an edge weight to be negative, consensus in time-to-go can even be achieved for an impact time that lies outside the convex hull of the set of initial time-to-go values of the individual interceptors. The bounds on such negative weights have been analysed for some special graphs, using Nyquist criterion. Simulations are provided to vindicate the efficacy of the proposed strategy.","sentences":["This work proposes a cooperative strategy that employs deviated pursuit guidance to simultaneously intercept a moving (but not manoeuvring) target.","As opposed to many existing cooperative guidance strategies which use estimates of time-to-go, based on proportional-navigation guidance, the proposed strategy uses an exact expression for time-to-go to ensure simultaneous interception.","The guidance design considers nonlinear engagement kinematics, allowing the proposed strategy to remain effective over a large operating regime.","Unlike existing strategies on simultaneous interception that achieve interception at the average value of their initial time-to-go estimates, this work provides flexibility in the choice of impact time.","By judiciously choosing the edge weights of the communication network, a weighted consensus in time-to-go can be achieved.","It has been shown that by allowing an edge weight to be negative, consensus in time-to-go can even be achieved for an impact time that lies outside the convex hull of the set of initial time-to-go values of the individual interceptors.","The bounds on such negative weights have been analysed for some special graphs, using Nyquist criterion.","Simulations are provided to vindicate the efficacy of the proposed strategy."],"url":"http://arxiv.org/abs/2402.05918v1","category":"eess.SY"}
{"created":"2024-02-08 18:50:01","title":"Triangular phase-shift detector for drone precise vertical landing RF systems","abstract":"This paper presents a circuit for precise vertical landing of drones based on a three phase-shifts detection of a single frequency transmitted from the landing point. The circuit can be considered as a new navigation sensor that assists in guidance corrections for landing at a specific point. The circuit has three inputs to which the signal transmitted from an oscillator located at the landing point arrives with different delays. The input signals are combined in pairs in each of the three analog phase detectors, after having passed through 3 dB@90 o hybrid couplers that guarantee a theoretical non-ambiguous phase-shift range of +-90 degree. Each output has a voltage that is proportional to the phase-shift between each of the input signals, which in turn depend on the position relative to the landing point. A simple landing algorithm based on phase-shift values is proposed, which could be integrated into the same flight control platform, thus avoiding the need to add additional processing components. To demonstrate the feasibility of the proposed design, a triangular phase-shift detector prototype has been implemented using commercial devices. Calibration and measurements at 2.46 GHz show a dynamic range of 30 dB and a non-ambiguous detection range of +-80 degree in the worst cases. Those specs let us to track the drone during the landing maneuver in an inverted cone formed by a surface with a +-4.19 m radius at 10m high and the landing point.","sentences":["This paper presents a circuit for precise vertical landing of drones based on a three phase-shifts detection of a single frequency transmitted from the landing point.","The circuit can be considered as a new navigation sensor that assists in guidance corrections for landing at a specific point.","The circuit has three inputs to which the signal transmitted from an oscillator located at the landing point arrives with different delays.","The input signals are combined in pairs in each of the three analog phase detectors, after having passed through 3 dB@90 o hybrid couplers that guarantee a theoretical non-ambiguous phase-shift range of +-90 degree.","Each output has a voltage that is proportional to the phase-shift between each of the input signals, which in turn depend on the position relative to the landing point.","A simple landing algorithm based on phase-shift values is proposed, which could be integrated into the same flight control platform, thus avoiding the need to add additional processing components.","To demonstrate the feasibility of the proposed design, a triangular phase-shift detector prototype has been implemented using commercial devices.","Calibration and measurements at 2.46 GHz show a dynamic range of 30 dB and a non-ambiguous detection range of +-80 degree in the worst cases.","Those specs let us to track the drone during the landing maneuver in an inverted cone formed by a surface with a +-4.19 m radius at 10m high and the landing point."],"url":"http://arxiv.org/abs/2402.05914v1","category":"eess.SY"}
{"created":"2024-02-08 18:49:09","title":"Efficient Stagewise Pretraining via Progressive Subnetworks","abstract":"Recent developments in large language models have sparked interest in efficient pretraining methods. A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)). While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages. In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step. We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages. RaPTr achieves better pre-training loss for BERT and UL2 language models while requiring 20-33% fewer FLOPs compared to standard training, and is competitive or better than other efficient training methods. Furthermore, RaPTr shows better downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5% compared to standard training and stacking. Finally, we provide a theoretical basis for RaPTr to justify (a) the increasing complexity of subnetworks in stages, and (b) the stability in loss across stage transitions due to residual connections and layer norm.","sentences":["Recent developments in large language models have sparked interest in efficient pretraining methods.","A recent effective paradigm is to perform stage-wise training, where the size of the model is gradually increased over the course of training (e.g. gradual stacking (Reddi et al., 2023)).","While the resource and wall-time savings are appealing, it has limitations, particularly the inability to evaluate the full model during earlier stages, and degradation in model quality due to smaller model capacity in the initial stages.","In this work, we propose an alternative framework, progressive subnetwork training, that maintains the full model throughout training, but only trains subnetworks within the model in each step.","We focus on a simple instantiation of this framework, Random Path Training (RaPTr) that only trains a sub-path of layers in each step, progressively increasing the path lengths in stages.","RaPTr","achieves better pre-training loss for BERT and UL2 language models while requiring 20-33% fewer FLOPs compared to standard training, and is competitive or better than other efficient training methods.","Furthermore, RaPTr shows better downstream performance on UL2, improving QA tasks and SuperGLUE by 1-5% compared to standard training and stacking.","Finally, we provide a theoretical basis for RaPTr to justify (a) the increasing complexity of subnetworks in stages, and (b) the stability in loss across stage transitions due to residual connections and layer norm."],"url":"http://arxiv.org/abs/2402.05913v1","category":"cs.CL"}
{"created":"2024-02-08 18:44:01","title":"A Survey on Detection, Classification, and Tracking of Aerial Threats using Radar and Communications Systems","abstract":"The use of unmanned aerial vehicles (UAVs) for a variety of commercial, civilian, and defense applications has increased many folds in recent years. While UAVs are expected to transform future air operations, there are instances where they can be used for malicious purposes. In this context, the detection, classification, and tracking (DCT) of UAVs (DCT-U) for safety and surveillance of national air space is a challenging task when compared to DCT of manned aerial vehicles. In this survey, we discuss the threats and challenges from malicious UAVs and we subsequently study three radio frequency (RF)-based systems for DCT-U. These RF-based systems include radars, communication systems, and RF analyzers. Radar systems are further divided into conventional and modern radar systems, while communication systems can be used for joint communications and sensing (JC&S) in active mode and act as a source of illumination to passive radars for DCT-U. The limitations of the three RF-based systems are also provided. The survey briefly discusses non-RF systems for DCT-U and their limitations. Future directions based on the lessons learned are provided at the end of the survey.","sentences":["The use of unmanned aerial vehicles (UAVs) for a variety of commercial, civilian, and defense applications has increased many folds in recent years.","While UAVs are expected to transform future air operations, there are instances where they can be used for malicious purposes.","In this context, the detection, classification, and tracking (DCT) of UAVs (DCT-U) for safety and surveillance of national air space is a challenging task when compared to DCT of manned aerial vehicles.","In this survey, we discuss the threats and challenges from malicious UAVs and we subsequently study three radio frequency (RF)-based systems for DCT-U. These RF-based systems include radars, communication systems, and RF analyzers.","Radar systems are further divided into conventional and modern radar systems, while communication systems can be used for joint communications and sensing (JC&S) in active mode and act as a source of illumination to passive radars for DCT-U. The limitations of the three RF-based systems are also provided.","The survey briefly discusses non-RF systems for DCT-U and their limitations.","Future directions based on the lessons learned are provided at the end of the survey."],"url":"http://arxiv.org/abs/2402.05909v1","category":"eess.SP"}
{"created":"2024-02-08 18:43:05","title":"FACT-GPT: Fact-Checking Augmentation via Claim Matching with LLMs","abstract":"Our society is facing rampant misinformation harming public health and trust. To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking. FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims. Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment. This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field.","sentences":["Our society is facing rampant misinformation harming public health and trust.","To address the societal challenge, we introduce FACT-GPT, a system leveraging Large Language Models (LLMs) to automate the claim matching stage of fact-checking.","FACT-GPT, trained on a synthetic dataset, identifies social media content that aligns with, contradicts, or is irrelevant to previously debunked claims.","Our evaluation shows that our specialized LLMs can match the accuracy of larger models in identifying related claims, closely mirroring human judgment.","This research provides an automated solution for efficient claim matching, demonstrates the potential of LLMs in supporting fact-checkers, and offers valuable resources for further research in the field."],"url":"http://arxiv.org/abs/2402.05904v1","category":"cs.CL"}
{"created":"2024-02-08 18:39:43","title":"Dielectric and refractive index measurements for the systems 1-pentanol + octane, or + dibutyl ether or for dibutyl ether + octane at different temperatures","abstract":"Relative permittivities, $\\varepsilon_{r}$, and refractive indixes, $n_{D}$, have been measured at (288.15-308.15) K and (293.15-303.15) K, respectively, for the mixtures 1-pentanol + octane, or + dibutyl ether and dibutyl ether + octane. These data have been used, together with density measurements available in the literature, to determine the correlation factor, $g_{K}$, for the studied systems according to the Kirkwood-Fr\\\"ohlich equations. Results show that the existence of cyclic species of 1-pentanol are predominant at low concentrations of this alkanol when is mixed with octane. These species are broken in large extent by the more active molecules of oxaalkane in the dibutyl ether mixtures, which is in agreement with calorimetric data. The dibutyl ether + octane system does not show meaningful structure. These conclusions are confirmed by values of the molar polarization and by the temperature dependence of $\\varepsilon_{r}$. The empirical expressions of Lorentz-Lorenz, Wiener, Heller, Gladstone-Dale and Newton correlate well the $n_{D}$ data.","sentences":["Relative permittivities, $\\varepsilon_{r}$, and refractive indixes, $n_{D}$, have been measured at (288.15-308.15) K and (293.15-303.15) K, respectively, for the mixtures 1-pentanol + octane, or + dibutyl ether and dibutyl ether + octane.","These data have been used, together with density measurements available in the literature, to determine the correlation factor, $g_{K}$, for the studied systems according to the Kirkwood-Fr\\\"ohlich equations.","Results show that the existence of cyclic species of 1-pentanol are predominant at low concentrations of this alkanol when is mixed with octane.","These species are broken in large extent by the more active molecules of oxaalkane in the dibutyl ether mixtures, which is in agreement with calorimetric data.","The dibutyl ether + octane system does not show meaningful structure.","These conclusions are confirmed by values of the molar polarization and by the temperature dependence of $\\varepsilon_{r}$. The empirical expressions of Lorentz-Lorenz, Wiener, Heller, Gladstone-Dale and Newton correlate well the $n_{D}$ data."],"url":"http://arxiv.org/abs/2402.05899v1","category":"physics.chem-ph"}
{"created":"2024-02-08 18:27:47","title":"Thermodynamics of ketone + amine mixtures: Part II. Volumetric and speed of sound data at (293.15, 298.15 and 303.15) K for 2-propanone + dipropylamine, + dibutylamine or + triethylamine systems","abstract":"Densities, $\\rho$ , and speeds of sound, $u$, of 2-propanone + dipropylamine,+ dibutylamine or + triethylamine systems have been measured at (293.15, 298.15 and 303.15) K and atmospheric pressure using a vibrating tube densimeter and sound analyser Anton Paar model DSA-5000. The $\\rho$ and $u$ values were used to calculate excess molar volumes, $V^{E}$, and the excess functions at 298.15 K for the thermal expansion coefficient, $\\alpha_{p}^{E}$, and for the isentropic compressibility, $\\kappa_{S}^{E}$ at 298.15 K. $V^{E}$, $\\kappa_{S}^{E}$ and $\\alpha_{p}^{E}$ are positive magnitudes. When replacing dipropylamine by dibutylamine or triethylamine in the studied mixtures, the excess functions increase. This may be ascribed to the interactions between unlike molecules are more important in the former solutions. From the comparison with similar data obtained for 2-propanone + aniline, + N-methylaniline, or + pyridine systems, it is concluded that interactions between unlike molecules are stronger in mixtures containing aromatic amines. Free volume effects are present in solutions with dipropyl or dibutylamine as the $V^{E}$ curves are shifted towards higher mole fractions of 2-propanone.","sentences":["Densities, $\\rho$ , and speeds of sound, $u$, of 2-propanone + dipropylamine,+ dibutylamine or + triethylamine systems have been measured at (293.15, 298.15 and 303.15) K and atmospheric pressure using a vibrating tube densimeter and sound analyser Anton Paar model DSA-5000.","The $\\rho$ and $u$ values were used to calculate excess molar volumes, $V^{E}$, and the excess functions at 298.15 K for the thermal expansion coefficient, $\\alpha_{p}^{E}$, and for the isentropic compressibility, $\\kappa_{S}^{E}$ at 298.15 K. $V^{E}$, $\\kappa_{S}^{E}$ and $\\alpha_{p}^{E}$ are positive magnitudes.","When replacing dipropylamine by dibutylamine or triethylamine in the studied mixtures, the excess functions increase.","This may be ascribed to the interactions between unlike molecules are more important in the former solutions.","From the comparison with similar data obtained for 2-propanone + aniline, + N-methylaniline, or + pyridine systems, it is concluded that interactions between unlike molecules are stronger in mixtures containing aromatic amines.","Free volume effects are present in solutions with dipropyl or dibutylamine as the $V^{E}$ curves are shifted towards higher mole fractions of 2-propanone."],"url":"http://arxiv.org/abs/2402.05890v1","category":"physics.chem-ph"}
{"created":"2024-02-08 18:23:50","title":"Sandwiched Compression: Repurposing Standard Codecs with Neural Network Wrappers","abstract":"We propose sandwiching standard image and video codecs between pre- and post-processing neural networks. The networks are jointly trained through a differentiable codec proxy to minimize a given rate-distortion loss. This sandwich architecture not only improves the standard codec's performance on its intended content, it can effectively adapt the codec to other types of image/video content and to other distortion measures. Essentially, the sandwich learns to transmit ``neural code images'' that optimize overall rate-distortion performance even when the overall problem is well outside the scope of the codec's design. Through a variety of examples, we apply the sandwich architecture to sources with different numbers of channels, higher resolution, higher dynamic range, and perceptual distortion measures. The results demonstrate substantial improvements (up to 9 dB gains or up to 30\\% bitrate reductions) compared to alternative adaptations. We derive VQ equivalents for the sandwich, establish optimality properties, and design differentiable codec proxies approximating current standard codecs. We further analyze model complexity, visual quality under perceptual metrics, as well as sandwich configurations that offer interesting potentials in image/video compression and streaming.","sentences":["We propose sandwiching standard image and video codecs between pre- and post-processing neural networks.","The networks are jointly trained through a differentiable codec proxy to minimize a given rate-distortion loss.","This sandwich architecture not only improves the standard codec's performance on its intended content, it can effectively adapt the codec to other types of image/video content and to other distortion measures.","Essentially, the sandwich learns to transmit ``neural code images'' that optimize overall rate-distortion performance even when the overall problem is well outside the scope of the codec's design.","Through a variety of examples, we apply the sandwich architecture to sources with different numbers of channels, higher resolution, higher dynamic range, and perceptual distortion measures.","The results demonstrate substantial improvements (up to 9 dB gains or up to 30\\% bitrate reductions) compared to alternative adaptations.","We derive VQ equivalents for the sandwich, establish optimality properties, and design differentiable codec proxies approximating current standard codecs.","We further analyze model complexity, visual quality under perceptual metrics, as well as sandwich configurations that offer interesting potentials in image/video compression and streaming."],"url":"http://arxiv.org/abs/2402.05887v1","category":"eess.IV"}
{"created":"2024-02-08 18:23:18","title":"The impact of high frequency-based stability on the onset of action potentials in neuron models","abstract":"This paper studies the phenomenon of conduction block in model neurons using high-frequency biphasic stimulation (HFBS). The focus is investigating the triggering of undesired onset action potentials when the HFBS is turned on. The approach analyzes the transient behavior of an averaged system corresponding to the FitzHugh-Nagumo neuron model using Lyapunov and quasi-static methods. The first result provides a more comprehensive understanding of the onset activation through a mathematical proof of how to avoid it using a ramp in the amplitude of the oscillatory source. The second result tests the response of the blocked system to a piecewise linear stimulus, providing a quantitative description of how the HFBS strength translates into conduction block robustness. The results of this work can provide insights for the design of electrical neurostimulation therapies.","sentences":["This paper studies the phenomenon of conduction block in model neurons using high-frequency biphasic stimulation (HFBS).","The focus is investigating the triggering of undesired onset action potentials when the HFBS is turned on.","The approach analyzes the transient behavior of an averaged system corresponding to the FitzHugh-Nagumo neuron model using Lyapunov and quasi-static methods.","The first result provides a more comprehensive understanding of the onset activation through a mathematical proof of how to avoid it using a ramp in the amplitude of the oscillatory source.","The second result tests the response of the blocked system to a piecewise linear stimulus, providing a quantitative description of how the HFBS strength translates into conduction block robustness.","The results of this work can provide insights for the design of electrical neurostimulation therapies."],"url":"http://arxiv.org/abs/2402.05886v1","category":"q-bio.NC"}
{"created":"2024-02-08 18:18:29","title":"Thermodynamics of Mixtures Containing a Strongly Polar Compound. 9. Liquid-Liquid Equilibria for $\u03b5$-CAPROLACTAM + Selected Alkanes","abstract":"The coexistence curves of the liquid-liquid equilibria (LLE) for systems of -caprolactam with heptane, octane, nonane, decane or 2,2,4-trimethylpentane have been determined by the method of the critical opalescence using a laser scattering technique. All the curves show an upper critical solution temperature (UCST), have a rather horizontal top and their symmetry depends on the size of the alkane. The UCST increases almost linearly with the chain length of the n-alkane. For the octane mixture, the UCST is lower than for the solution including 2,2,4-trimethylpentane.","sentences":["The coexistence curves of the liquid-liquid equilibria (LLE) for systems of -caprolactam with heptane, octane, nonane, decane or 2,2,4-trimethylpentane have been determined by the method of the critical opalescence using a laser scattering technique.","All the curves show an upper critical solution temperature (UCST), have a rather horizontal top and their symmetry depends on the size of the alkane.","The UCST increases almost linearly with the chain length of the n-alkane.","For the octane mixture, the UCST is lower than for the solution including 2,2,4-trimethylpentane."],"url":"http://arxiv.org/abs/2402.05883v1","category":"physics.chem-ph"}
{"created":"2024-02-08 18:18:29","title":"Cutsets and EF1 Fair Division of Graphs","abstract":"In fair division of a connected graph $G = (V, E)$, each of $n$ agents receives a share of $G$'s vertex set $V$. These shares partition $V$, with each share required to induce a connected subgraph. Agents use their own valuation functions to determine the non-negative numerical values of the shares, which determine whether the allocation is fair in some specified sense. We introduce forbidden substructures called graph cutsets, which block divisions that are fair in the EF1 (envy-free up to one item) sense by cutting the graph into \"too many pieces\". Two parameters - gap and valence - determine blocked values of $n$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations that are CA (common and additive), then $G$ contains no elementary cutset of gap $k \\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations in the broader CM (common and monotone) class, then $G$ contains no cutset of gap $k \\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$. These results rule out the existence of connected EF1 allocations in a variety of situations. For some graphs $G$ we can, with help from some new positive results, pin down $G$'s spectrum - the list of exactly which values of $n$ do/do not guarantee connected EF1 allocations. Examples suggest a conjectured common spectral pattern for all graphs. Further, we show that it is NP-hard to determine whether a graph admits a cutset. We also provide an example of a (non-traceable) graph on eight vertices that has no cutsets of gap $\\ge 2$ at all, yet fails to guarantee connected EF1 allocations for three agents with CA preferences.","sentences":["In fair division of a connected graph $G = (V, E)$, each of $n$ agents receives a share of $G$'s vertex set $V$. These shares partition $V$, with each share required to induce a connected subgraph.","Agents use their own valuation functions to determine the non-negative numerical values of the shares, which determine whether the allocation is fair in some specified sense.","We introduce forbidden substructures called graph cutsets, which block divisions that are fair in the EF1 (envy-free up to one item) sense by cutting the graph into \"too many pieces\".","Two parameters - gap and valence - determine blocked values of $n$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations that are CA (common and additive), then $G$ contains no elementary cutset of gap $k \\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$. If $G$ guarantees connected EF1 allocations for $n$ agents with valuations in the broader CM (common and monotone) class, then $G$ contains no cutset of gap $k \\ge 2$ and valence in the interval $\\[n - k + 1, n - 1\\]$.","These results rule out the existence of connected EF1 allocations in a variety of situations.","For some graphs $G$ we can, with help from some new positive results, pin down $G$'s spectrum - the list of exactly which values of $n$ do/do not guarantee connected EF1 allocations.","Examples suggest a conjectured common spectral pattern for all graphs.","Further, we show that it is NP-hard to determine whether a graph admits a cutset.","We also provide an example of a (non-traceable) graph on eight vertices that has no cutsets of gap $\\ge 2$ at all, yet fails to guarantee connected EF1 allocations for three agents with CA preferences."],"url":"http://arxiv.org/abs/2402.05884v1","category":"cs.GT"}
{"created":"2024-02-08 18:13:55","title":"Optical Shubnikov - de Haas oscillations in 2D electron systems","abstract":"We report on dynamic Shubnikov - de Haas (SdH) oscillations that are measured in the optical response, sub - terahertz transmittance of two-dimensional systems, and reveal two distinct types of oscillation nodes: \"universal\" nodes at integer ratios of radiation and cyclotron frequencies and \"tunable\" nodes at positions sensitive to all parameters of the structure. The nodes in both real and imaginary parts of the measured complex transmittance are analyzed using a dynamic version of the static Lifshitz-Kosevich formula. These results demonstrate that the node structure of the dynamic SdH oscillations provides an all-optical access to quantization- and interaction-induced renormalization effects, in addition to parameters one can obtain from the static SdH oscillations.","sentences":["We report on dynamic Shubnikov - de Haas (SdH) oscillations that are measured in the optical response, sub - terahertz transmittance of two-dimensional systems, and reveal two distinct types of oscillation nodes: \"universal\" nodes at integer ratios of radiation and cyclotron frequencies and \"tunable\" nodes at positions sensitive to all parameters of the structure.","The nodes in both real and imaginary parts of the measured complex transmittance are analyzed using a dynamic version of the static Lifshitz-Kosevich formula.","These results demonstrate that the node structure of the dynamic SdH oscillations provides an all-optical access to quantization- and interaction-induced renormalization effects, in addition to parameters one can obtain from the static SdH oscillations."],"url":"http://arxiv.org/abs/2402.05879v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-08 18:01:19","title":"You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for Semantic and Property Prediction","abstract":"Robots must be able to understand their surroundings to perform complex tasks in challenging environments and many of these complex tasks require estimates of physical properties such as friction or weight. Estimating such properties using learning is challenging due to the large amounts of labelled data required for training and the difficulty of updating these learned models online at run time. To overcome these challenges, this paper introduces a novel, multi-modal approach for representing semantic predictions and physical property estimates jointly in a probabilistic manner. By using conjugate pairs, the proposed method enables closed-form Bayesian updates given visual and tactile measurements without requiring additional training data. The efficacy of the proposed algorithm is demonstrated through several hardware experiments. In particular, this paper illustrates that by conditioning semantic classifications on physical properties, the proposed method quantitatively outperforms state-of-the-art semantic classification methods that rely on vision alone. To further illustrate its utility, the proposed method is used in several applications including to represent affordance-based properties probabilistically and a challenging terrain traversal task using a legged robot. In the latter task, the proposed method represents the coefficient of friction of the terrain probabilistically, which enables the use of an on-line risk-aware planner that switches the legged robot from a dynamic gait to a static, stable gait when the expected value of the coefficient of friction falls below a given threshold. Videos of these case studies are presented in the multimedia attachment. The proposed framework includes an open-source C++ and ROS interface.","sentences":["Robots must be able to understand their surroundings to perform complex tasks in challenging environments and many of these complex tasks require estimates of physical properties such as friction or weight.","Estimating such properties using learning is challenging due to the large amounts of labelled data required for training and the difficulty of updating these learned models online at run time.","To overcome these challenges, this paper introduces a novel, multi-modal approach for representing semantic predictions and physical property estimates jointly in a probabilistic manner.","By using conjugate pairs, the proposed method enables closed-form Bayesian updates given visual and tactile measurements without requiring additional training data.","The efficacy of the proposed algorithm is demonstrated through several hardware experiments.","In particular, this paper illustrates that by conditioning semantic classifications on physical properties, the proposed method quantitatively outperforms state-of-the-art semantic classification methods that rely on vision alone.","To further illustrate its utility, the proposed method is used in several applications including to represent affordance-based properties probabilistically and a challenging terrain traversal task using a legged robot.","In the latter task, the proposed method represents the coefficient of friction of the terrain probabilistically, which enables the use of an on-line risk-aware planner that switches the legged robot from a dynamic gait to a static, stable gait when the expected value of the coefficient of friction falls below a given threshold.","Videos of these case studies are presented in the multimedia attachment.","The proposed framework includes an open-source C++ and ROS interface."],"url":"http://arxiv.org/abs/2402.05872v1","category":"cs.RO"}
{"created":"2024-02-08 17:50:22","title":"Memory Consolidation Enables Long-Context Video Understanding","abstract":"Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters.","sentences":["Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity.","While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity.","We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations.","By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos.","In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters."],"url":"http://arxiv.org/abs/2402.05861v1","category":"cs.CV"}
{"created":"2024-02-08 17:35:36","title":"A central limit theorem for the matching number of a sparse random graph","abstract":"In 1981, Karp and Sipser proved a law of large numbers for the matching number of a sparse Erd\\H{o}s-R\\'enyi random graph, in an influential paper pioneering the so-called differential equation method for analysis of random graph processes. Strengthening this classical result, and answering a question of Aronson, Frieze and Pittel, we prove a central limit theorem in the same setting: the fluctuations in the matching number of a sparse random graph are asymptotically Gaussian.   Our new contribution is to prove this central limit theorem in the subcritical and critical regimes, according to a celebrated algorithmic phase transition first observed by Karp and Sipser. Indeed, in the supercritical regime, a central limit theorem has recently been proved in the PhD thesis of Krea\\v{c}i\\'c, using a stochastic generalisation of the differential equation method (comparing the so-called Karp-Sipser process to a system of stochastic differential equations). Our proof builds on these methods, and introduces new techniques to handle certain degeneracies present in the subcritical and critical cases. Curiously, our new techniques lead to a non-constructive result: we are able to characterise the fluctuations of the matching number around its mean, despite these fluctuations being much smaller than the error terms in our best estimates of the mean.   We also prove a central limit theorem for the rank of the adjacency matrix of a sparse random graph.","sentences":["In 1981, Karp and Sipser proved a law of large numbers for the matching number of a sparse Erd\\H{o}s-R\\'enyi random graph, in an influential paper pioneering the so-called differential equation method for analysis of random graph processes.","Strengthening this classical result, and answering a question of Aronson, Frieze and Pittel, we prove a central limit theorem in the same setting: the fluctuations in the matching number of a sparse random graph are asymptotically Gaussian.   ","Our new contribution is to prove this central limit theorem in the subcritical and critical regimes, according to a celebrated algorithmic phase transition first observed by Karp and Sipser.","Indeed, in the supercritical regime, a central limit theorem has recently been proved in the PhD thesis of Krea\\v{c}i\\'c, using a stochastic generalisation of the differential equation method (comparing the so-called Karp-Sipser process to a system of stochastic differential equations).","Our proof builds on these methods, and introduces new techniques to handle certain degeneracies present in the subcritical and critical cases.","Curiously, our new techniques lead to a non-constructive result: we are able to characterise the fluctuations of the matching number around its mean, despite these fluctuations being much smaller than the error terms in our best estimates of the mean.   ","We also prove a central limit theorem for the rank of the adjacency matrix of a sparse random graph."],"url":"http://arxiv.org/abs/2402.05851v1","category":"math.CO"}
{"created":"2024-02-08 17:28:04","title":"Quantum Anomalous Hall Effect in $d$-Electron Kagome Systems: Chern Insulating States from Transverse Spin-Orbit Coupling","abstract":"Inspired by the discovery of metal-organic frameworks, the possibility of quantum anomalous Hall effect (QAHE) in two-dimensional kagome systems with $d$-orbital electrons is studied within a multi-orbital tight-binding model. In the absence of exchange-type spin-orbit coupling, isotropic Slater-Koster integrals give a band structure with relativistic (Dirac) and quadratic band crossing points at high symmetry spots in the Brillouin zone. A quantized topological invariant requires a flux-creating spin-orbit coupling, giving Chern number (per spin sector) $C=1$ not only from the familiar Dirac points at the six corners of the Brillouin zone, but also from the quadratic band crossing point at the center $\\Gamma$. Surprisingly, this QAHE comes from the nontrivial effective flux induced by the transverse part of the spin-orbit coupling, exhibited by electrons in the $d$-orbital state with $m_l=0$ ($d_{z^2}$ orbital), in stark contrast to the more familiar form of QAHE due to the $d$-orbitals with $m_l \\neq 0$, driven by the Ising part of spin-orbit coupling. The $C=1$ Chern plateau (per spin sector) due to Dirac point extends over a smaller region of Fermi energy than that due to quadratic band crossing. Our result hints at the promising potential of kagome metal-organic frameworks as a platform for dissipationless electronics by virtue of its unique QAHE.","sentences":["Inspired by the discovery of metal-organic frameworks, the possibility of quantum anomalous Hall effect (QAHE) in two-dimensional kagome systems with $d$-orbital electrons is studied within a multi-orbital tight-binding model.","In the absence of exchange-type spin-orbit coupling, isotropic Slater-Koster integrals give a band structure with relativistic (Dirac) and quadratic band crossing points at high symmetry spots in the Brillouin zone.","A quantized topological invariant requires a flux-creating spin-orbit coupling, giving Chern number (per spin sector) $C=1$ not only from the familiar Dirac points at the six corners of the Brillouin zone, but also from the quadratic band crossing point at the center $\\Gamma$. Surprisingly, this QAHE comes from the nontrivial effective flux induced by the transverse part of the spin-orbit coupling, exhibited by electrons in the $d$-orbital state with $m_l=0$ ($d_{z^2}$ orbital), in stark contrast to the more familiar form of QAHE due to the $d$-orbitals with $m_l \\neq 0$, driven by the Ising part of spin-orbit coupling.","The $C=1$ Chern plateau (per spin sector) due to Dirac point extends over a smaller region of Fermi energy than that due to quadratic band crossing.","Our result hints at the promising potential of kagome metal-organic frameworks as a platform for dissipationless electronics by virtue of its unique QAHE."],"url":"http://arxiv.org/abs/2402.05845v1","category":"cond-mat.str-el"}
{"created":"2024-02-08 17:06:17","title":"Microscopic models for the large-scale spread of SARS-CoV-2 virus: A Statistical Mechanics approach","abstract":"In this work, we derive a system of Boltzmann-type equations to describe the spread of SARS-CoV-2 virus at the microscopic scale, that is by modeling the human-to-human mechanisms of transmission. To this end, we consider two populations, characterized by specific distribution functions, made up of individuals without symptoms (population $1$) and infected people with symptoms (population $2$). The Boltzmann operators model the interactions between individuals within the same population and among different populations with a probability of transition from one to the other due to contagion or, vice versa, to recovery. In addition, the influence of innate and adaptive immune systems is taken into account. Then, starting from the Boltzmann microscopic description we derive a set of evolution equations for the size and mean state of each population considered. Mathematical properties of such macroscopic equations, as equilibria and their stability, are investigated and some numerical simulations are performed in order to analyze the ability of our model to reproduce the characteristic features of Covid-19.","sentences":["In this work, we derive a system of Boltzmann-type equations to describe the spread of SARS-CoV-2 virus at the microscopic scale, that is by modeling the human-to-human mechanisms of transmission.","To this end, we consider two populations, characterized by specific distribution functions, made up of individuals without symptoms (population $1$) and infected people with symptoms (population $2$).","The Boltzmann operators model the interactions between individuals within the same population and among different populations with a probability of transition from one to the other due to contagion or, vice versa, to recovery.","In addition, the influence of innate and adaptive immune systems is taken into account.","Then, starting from the Boltzmann microscopic description we derive a set of evolution equations for the size and mean state of each population considered.","Mathematical properties of such macroscopic equations, as equilibria and their stability, are investigated and some numerical simulations are performed in order to analyze the ability of our model to reproduce the characteristic features of Covid-19."],"url":"http://arxiv.org/abs/2402.05826v1","category":"q-bio.PE"}
{"created":"2024-02-08 16:52:31","title":"Effects of kappa-opioid agonist U-50488 and p38 MAPK inhibitor SB203580 on the spike activity of pyramidal neurons in the basolateral amygdala","abstract":"Introduction: Kappa-opioid receptor (KOR) signaling in the basolateral amygdala (BLA) underlies KOR agonist-induced aversion. In this study, we aimed to understand the individual and combined effects of KOR agonist U-50488 and p38 MAPK inhibitor SB203580 on the spiking activity of pyramidal neurons in the BLA to shed light on the complex interplay between KORs, the p38 MAPK, and neuronal excitability. Materials and Methods: Electrophysiological experiments were performed using the patch-clamp technique in the whole-cell configuration. Rat brain slices containing the amygdala were prepared, and pyramidal neurons within the BLA were visually patched and recorded in the current clamp mode. The neurons were identified by their accommodation properties and neural activity signals were amplified and analyzed. Using local perfusion, we obtained three dose-response curves for: a) U-50488 (0.001-10 {\\mu}M); b) U-50488 (0.001-10 {\\mu}M) in the presence of SB203580 (1 {\\mu}M); and c) U-50488 (0.01-10 {\\mu}M) in the presence of SB203580 (5 {\\mu}M). Results: After the application of U-50488, pyramidal neurons had a higher action potential firing rate in response to a current injection than control neurons (p<0.001). The dose-dependent curves we obtained indicate that the combination of U-50488 and SB203580 results in non-competitive antagonism. This conclusion is supported by the observed change in the curve`s slope with reduction in the maximum effect of U-50488. Thus, it can be assumed that the increase in spike activity of pyramidal neurons of the amygdala is mediated through the beta-arrestin pathway. When this pathway is blocked, the spike activity reverts to its baseline level. Conclusion: Our study found that the KOR agonist-induced spiking activity of the BLA pyramidal neurons is mediated by the beta-arrestin pathway and can be suppressed by the application of the p38 MAPK inhibitor SB203580.","sentences":["Introduction: Kappa-opioid receptor (KOR) signaling in the basolateral amygdala (BLA) underlies KOR agonist-induced aversion.","In this study, we aimed to understand the individual and combined effects of KOR agonist U-50488 and p38 MAPK inhibitor SB203580 on the spiking activity of pyramidal neurons in the BLA to shed light on the complex interplay between KORs, the p38 MAPK, and neuronal excitability.","Materials and Methods: Electrophysiological experiments were performed using the patch-clamp technique in the whole-cell configuration.","Rat brain slices containing the amygdala were prepared, and pyramidal neurons within the BLA were visually patched and recorded in the current clamp mode.","The neurons were identified by their accommodation properties and neural activity signals were amplified and analyzed.","Using local perfusion, we obtained three dose-response curves for: a) U-50488 (0.001-10 {\\mu}M); b) U-50488 (0.001-10 {\\mu}M) in the presence of SB203580 (1 {\\mu}M); and c) U-50488 (0.01-10 {\\mu}M) in the presence of SB203580 (5 {\\mu}M).","Results:","After the application of U-50488, pyramidal neurons had a higher action potential firing rate in response to a current injection than control neurons (p<0.001).","The dose-dependent curves we obtained indicate that the combination of U-50488 and SB203580 results in non-competitive antagonism.","This conclusion is supported by the observed change in the curve`s slope with reduction in the maximum effect of U-50488.","Thus, it can be assumed that the increase in spike activity of pyramidal neurons of the amygdala is mediated through the beta-arrestin pathway.","When this pathway is blocked, the spike activity reverts to its baseline level.","Conclusion: Our study found that the KOR agonist-induced spiking activity of the BLA pyramidal neurons is mediated by the beta-arrestin pathway and can be suppressed by the application of the p38 MAPK inhibitor SB203580."],"url":"http://arxiv.org/abs/2402.05815v1","category":"q-bio.NC"}
{"created":"2024-02-08 16:47:58","title":"High-Q Cavity Interface for Color Centers in Thin Film Diamond","abstract":"Quantum information technology offers the potential to realize unprecedented computational resources via secure channels capable of distributing entanglement between quantum computers. Diamond, as a host to atom-like defects with optically-accessible spin qubits, is a leading platform to realize quantum memory nodes needed to extend the reach of quantum links. Photonic crystal (PhC) cavities enhance light-matter interaction and are essential ingredients of an efficient interface between spins and photons that are used to store and communicate quantum information respectively. Despite great effort, however, the realization of visible PhC cavities with high quality factor (Q) and design flexibility is challenging in diamond. Here, we demonstrate one- and two-dimensional PhC cavities fabricated in recently developed thin-film diamonds, featuring Q-factors of 1.8x10$^5$ and 1.6x10$^5$, respectively, the highest Qs for visible PhC cavities realized in any material. Importantly, our fabrication process is simple and high-yield, based on conventional planar fabrication techniques, in contrast to previous approaches that rely on complex undercut methods. We also demonstrate fiber-coupled 1D PhC cavities with high photon extraction efficiency, and optical coupling between a single SiV center and such a cavity at 4K achieving a Purcell factor of 13. The demonstrated diamond thin-film photonic platform will improve the performance and scalability of quantum nodes and expand the range of quantum technologies.","sentences":["Quantum information technology offers the potential to realize unprecedented computational resources via secure channels capable of distributing entanglement between quantum computers.","Diamond, as a host to atom-like defects with optically-accessible spin qubits, is a leading platform to realize quantum memory nodes needed to extend the reach of quantum links.","Photonic crystal (PhC) cavities enhance light-matter interaction and are essential ingredients of an efficient interface between spins and photons that are used to store and communicate quantum information respectively.","Despite great effort, however, the realization of visible PhC cavities with high quality factor (Q) and design flexibility is challenging in diamond.","Here, we demonstrate one-","and two-dimensional PhC cavities fabricated in recently developed thin-film diamonds, featuring Q-factors of 1.8x10$^5$ and 1.6x10$^5$, respectively, the highest Qs for visible PhC cavities realized in any material.","Importantly, our fabrication process is simple and high-yield, based on conventional planar fabrication techniques, in contrast to previous approaches that rely on complex undercut methods.","We also demonstrate fiber-coupled 1D PhC cavities with high photon extraction efficiency, and optical coupling between a single SiV center and such a cavity at 4K achieving a Purcell factor of 13.","The demonstrated diamond thin-film photonic platform will improve the performance and scalability of quantum nodes and expand the range of quantum technologies."],"url":"http://arxiv.org/abs/2402.05811v1","category":"quant-ph"}
{"created":"2024-02-08 16:47:54","title":"Natural Language User Profiles for Transparent and Scrutable Recommendations","abstract":"Current state-of-the-art recommender systems predominantly rely on either implicit or explicit feedback from users to suggest new items. While effective in recommending novel options, these conventional systems often use uninterpretable embeddings. This lack of transparency not only limits user understanding of why certain items are suggested but also reduces the user's ability to easily scrutinize and edit their preferences. For example, if a user has a change in interests, they would need to make significant changes to their interaction history to adjust the model's recommendations. To address these limitations, we introduce a novel method that utilizes user reviews to craft personalized, natural language profiles describing users' preferences. Through these descriptive profiles, our system provides transparent recommendations in natural language. Our evaluations show that this novel approach maintains a performance level on par with established recommender systems, but with the added benefits of transparency and user control. By enabling users to scrutinize why certain items are recommended, they can more easily verify, adjust, and have greater autonomy over their recommendations.","sentences":["Current state-of-the-art recommender systems predominantly rely on either implicit or explicit feedback from users to suggest new items.","While effective in recommending novel options, these conventional systems often use uninterpretable embeddings.","This lack of transparency not only limits user understanding of why certain items are suggested but also reduces the user's ability to easily scrutinize and edit their preferences.","For example, if a user has a change in interests, they would need to make significant changes to their interaction history to adjust the model's recommendations.","To address these limitations, we introduce a novel method that utilizes user reviews to craft personalized, natural language profiles describing users' preferences.","Through these descriptive profiles, our system provides transparent recommendations in natural language.","Our evaluations show that this novel approach maintains a performance level on par with established recommender systems, but with the added benefits of transparency and user control.","By enabling users to scrutinize why certain items are recommended, they can more easily verify, adjust, and have greater autonomy over their recommendations."],"url":"http://arxiv.org/abs/2402.05810v1","category":"cs.IR"}
{"created":"2024-02-08 16:45:51","title":"Rigid currents in birational geometry","abstract":"A rigid current on a compact complex manifold is a closed positive current whose cohomology class contains only one closed positive current. Rigid currents occur in complex dynamics, algebraic and differential geometry. The goals of the present paper are: (a) to give a systematic treatment of rigid currents, (b) to demonstrate how they appear within the Minimal Model Program, and (c) to give many new examples of rigid currents.","sentences":["A rigid current on a compact complex manifold is a closed positive current whose cohomology class contains only one closed positive current.","Rigid currents occur in complex dynamics, algebraic and differential geometry.","The goals of the present paper are: (a) to give a systematic treatment of rigid currents, (b) to demonstrate how they appear within the Minimal Model Program, and (c) to give many new examples of rigid currents."],"url":"http://arxiv.org/abs/2402.05807v1","category":"math.AG"}
{"created":"2024-02-08 16:31:02","title":"Underwater MEMS Gyrocompassing: A Virtual Testing Ground","abstract":"In underwater navigation, accurate heading information is crucial for accurately and continuously tracking trajectories, especially during extended missions beneath the waves. In order to determine the initial heading, a gyrocompassing procedure must be employed. As unmanned underwater vehicles (UUV) are susceptible to ocean currents and other disturbances, the model-based gyrocompassing procedure may experience degraded performance. To cope with such situations, this paper introduces a dedicated learning framework aimed at mitigating environmental effects and offering precise underwater gyrocompassing. Through the analysis of the dynamic UUV signature obtained from inertial measurements, our proposed framework learns to refine disturbed signals, enabling a focused examination of the earth's rotation rate vector. Leveraging recent machine learning advancements, empirical simulations assess the framework's adaptability to challenging underwater conditions. Ultimately, its contribution lies in providing a resilient gyrocompassing solution for UUVs.","sentences":["In underwater navigation, accurate heading information is crucial for accurately and continuously tracking trajectories, especially during extended missions beneath the waves.","In order to determine the initial heading, a gyrocompassing procedure must be employed.","As unmanned underwater vehicles (UUV) are susceptible to ocean currents and other disturbances, the model-based gyrocompassing procedure may experience degraded performance.","To cope with such situations, this paper introduces a dedicated learning framework aimed at mitigating environmental effects and offering precise underwater gyrocompassing.","Through the analysis of the dynamic UUV signature obtained from inertial measurements, our proposed framework learns to refine disturbed signals, enabling a focused examination of the earth's rotation rate vector.","Leveraging recent machine learning advancements, empirical simulations assess the framework's adaptability to challenging underwater conditions.","Ultimately, its contribution lies in providing a resilient gyrocompassing solution for UUVs."],"url":"http://arxiv.org/abs/2402.05790v1","category":"eess.SY"}
{"created":"2024-02-08 16:09:06","title":"The $\u03c6^n$ trajectory bootstrap","abstract":"The Green's functions $G_n=\\langle\\phi^n\\rangle$ and their self-consistent equations admit analytic continuations to complex $n$. The indeterminacy of bootstrap problems can be resolved by the principle of minimal singularity. We use the harmonic oscillator to illustrate various aspects of the bootstrap analysis, such as the large $n$ expansion, matching conditions, exact quantization condition, and high energy asymptotic behavior. For the Hermitian quartic and non-Hermitian cubic oscillators, we revisit the $\\phi^n$ trajectories at non-integer $n$ by the standard wave function formulation. The results are in agreement with the minimally singular solutions. Using the matching procedure, we obtain accurate solutions for anharmonic oscillators with higher powers. In particular, the existence of $G_n$ with non-integer $n$ allows us to bootstrap the $\\mathcal{PT}$ invariant oscillators with non-integer powers.","sentences":["The Green's functions $G_n=\\langle\\phi^n\\rangle$ and their self-consistent equations admit analytic continuations to complex $n$. The indeterminacy of bootstrap problems can be resolved by the principle of minimal singularity.","We use the harmonic oscillator to illustrate various aspects of the bootstrap analysis, such as the large $n$ expansion, matching conditions, exact quantization condition, and high energy asymptotic behavior.","For the Hermitian quartic and non-Hermitian cubic oscillators, we revisit the $\\phi^n$ trajectories at non-integer $n$ by the standard wave function formulation.","The results are in agreement with the minimally singular solutions.","Using the matching procedure, we obtain accurate solutions for anharmonic oscillators with higher powers.","In particular, the existence of $G_n$ with non-integer $n$ allows us to bootstrap the $\\mathcal{PT}$ invariant oscillators with non-integer powers."],"url":"http://arxiv.org/abs/2402.05778v1","category":"hep-th"}
{"created":"2024-02-08 16:02:31","title":"Relations between closed string amplitudes and mixed string amplitudes at tree-level","abstract":"This paper investigates the relationships between closed and mixed string amplitudes at the tree level in string theory. Through the analytic continuation of complex variables, we establish a factorization of closed string amplitudes into those involving ($n-2$) open strings and a single closed string. Expressions for four-, five-, and six-point amplitudes are provided, along with systematic formulations for $n$ strings. The paper addresses possible correction terms arising from integration along infinite tubes during the Wick rotation process. In the field theory limit, the correction terms become negligible due to being of subleading order.","sentences":["This paper investigates the relationships between closed and mixed string amplitudes at the tree level in string theory.","Through the analytic continuation of complex variables, we establish a factorization of closed string amplitudes into those involving ($n-2$) open strings and a single closed string.","Expressions for four-, five-, and six-point amplitudes are provided, along with systematic formulations for $n$ strings.","The paper addresses possible correction terms arising from integration along infinite tubes during the Wick rotation process.","In the field theory limit, the correction terms become negligible due to being of subleading order."],"url":"http://arxiv.org/abs/2402.05775v1","category":"hep-th"}
{"created":"2024-02-08 15:42:38","title":"The non-collinear phase of the antiferromagnetic sawtooth chain","abstract":"The antiferromagnetic sawtooth chain is a prototypical example of a frustrated spin system with vertex-sharing triangles, giving rise to complex quantum states. Depending on the interaction parameters, this system has three phases, of which the gapless non-collinear phase (for strongly coupled basal spins and loosely attached apical spins) has received little theoretical attention so far. In this work, we comprehensively investigate the properties of the non-collinear phase using large-scale tensor network computations which exploit the full SU(2) symmetry of the underlying Heisenberg model. We study the ground state both for finite systems using the density-matrix renormalization group (DMRG) as well as for infinite chains via the variational uniform matrix-product state (VUMPS) formalism. Finite temperatures and correlation functions are tackled via imaginary- or real time evolutions, which we implement using the time-dependent variational principle (TDVP). We find that the non-collinear phase is characterized by a double-Q structure for the apex-apex correlations. Deep into the phase, two peaks merge into a single one indicating a 90-degree spiral. The apical spins are soft and highly susceptible to external perturbations; they form a large number of gapless magnetic states that are polarized by weak fields and cause a long low-temperature tail in the specific heat. The dynamic spin-structure factor exhibits additive contributions from a two-spinon continuum (excitations of the basal chain) and a gapless peak at $k=\\pi/2$ (excitations of the apical spins). Small temperatures excite the gapless states and smear the spectral weight of the $k=\\pi/2$ peak out into a homogeneous flat-band structure. Our results are relevant, e.g., for the material atacamite Cu$_2$Cl(OH)$_3$ in high magnetic fields.","sentences":["The antiferromagnetic sawtooth chain is a prototypical example of a frustrated spin system with vertex-sharing triangles, giving rise to complex quantum states.","Depending on the interaction parameters, this system has three phases, of which the gapless non-collinear phase (for strongly coupled basal spins and loosely attached apical spins) has received little theoretical attention so far.","In this work, we comprehensively investigate the properties of the non-collinear phase using large-scale tensor network computations which exploit the full SU(2) symmetry of the underlying Heisenberg model.","We study the ground state both for finite systems using the density-matrix renormalization group (DMRG) as well as for infinite chains via the variational uniform matrix-product state (VUMPS) formalism.","Finite temperatures and correlation functions are tackled via imaginary- or real time evolutions, which we implement using the time-dependent variational principle (TDVP).","We find that the non-collinear phase is characterized by a double-Q structure for the apex-apex correlations.","Deep into the phase, two peaks merge into a single one indicating a 90-degree spiral.","The apical spins are soft and highly susceptible to external perturbations; they form a large number of gapless magnetic states that are polarized by weak fields and cause a long low-temperature tail in the specific heat.","The dynamic spin-structure factor exhibits additive contributions from a two-spinon continuum (excitations of the basal chain) and a gapless peak at $k=\\pi/2$ (excitations of the apical spins).","Small temperatures excite the gapless states and smear the spectral weight of the $k=\\pi/2$ peak out into a homogeneous flat-band structure.","Our results are relevant, e.g., for the material atacamite Cu$_2$Cl(OH)$_3$ in high magnetic fields."],"url":"http://arxiv.org/abs/2402.05759v1","category":"cond-mat.str-el"}
{"created":"2024-02-08 15:33:41","title":"Metamodeling and Control of Medical Digital Twins","abstract":"The vision of personalized medicine is to identify interventions that maintain or restore a person's health based on their individual biology. Medical digital twins, computational models that integrate a wide range of health-related data about a person and can be dynamically updated, are a key technology that can help guide medical decisions. Such medical digital twin models can be high-dimensional, multi-scale, and stochastic. To be practical for healthcare applications, they need to be simplified into low-dimensional metamodels that can be used for forecasting and optimal design of interventions. This paper introduces metamodeling algorithms for the purpose of optimal control applications. It uses agent-based models as a use case, a common model type in biomedicine for which there are no readily available optimal control algorithms. With systems of ordinary differential equations as metamodels, optimal control methods can be applied to the metamodels, and results can be lifted to the agent-based model.","sentences":["The vision of personalized medicine is to identify interventions that maintain or restore a person's health based on their individual biology.","Medical digital twins, computational models that integrate a wide range of health-related data about a person and can be dynamically updated, are a key technology that can help guide medical decisions.","Such medical digital twin models can be high-dimensional, multi-scale, and stochastic.","To be practical for healthcare applications, they need to be simplified into low-dimensional metamodels that can be used for forecasting and optimal design of interventions.","This paper introduces metamodeling algorithms for the purpose of optimal control applications.","It uses agent-based models as a use case, a common model type in biomedicine for which there are no readily available optimal control algorithms.","With systems of ordinary differential equations as metamodels, optimal control methods can be applied to the metamodels, and results can be lifted to the agent-based model."],"url":"http://arxiv.org/abs/2402.05750v1","category":"q-bio.QM"}
{"created":"2024-02-08 15:26:13","title":"All dielectric integrable optical isolators","abstract":"On-chip optical isolators, functioning as unidirectional gates for light, play a crucial role in maintaining signal integrity, preventing laser destabilization, and fortifying the overall performance of optical systems. In this paper, we propose a five-layered heterostructure consisting of a magneto-optic material sandwiched between parallel dielectric slab waveguides. Under TMOKE configuration, the coupled optical modes undergo an electromagnetic profile transformation that can be harnessed to confine the input mode in one waveguide during forward propagation and in the other during backward propagation. Together with radiative subwavelength gratings, such a system can provide a 20dB isolation ratio with negligible insertion losses.","sentences":["On-chip optical isolators, functioning as unidirectional gates for light, play a crucial role in maintaining signal integrity, preventing laser destabilization, and fortifying the overall performance of optical systems.","In this paper, we propose a five-layered heterostructure consisting of a magneto-optic material sandwiched between parallel dielectric slab waveguides.","Under TMOKE configuration, the coupled optical modes undergo an electromagnetic profile transformation that can be harnessed to confine the input mode in one waveguide during forward propagation and in the other during backward propagation.","Together with radiative subwavelength gratings, such a system can provide a 20dB isolation ratio with negligible insertion losses."],"url":"http://arxiv.org/abs/2402.05745v1","category":"physics.optics"}
{"created":"2024-02-08 15:16:44","title":"Critical mobility in policy making for epidemic containment","abstract":"When considering airborne epidemic spreading in social systems, a natural connection arises between mobility and epidemic contacts. As individuals travel, possibilities to encounter new people either at the final destination or during the transportation process appear. Such contacts can lead to new contagion events. In fact, mobility has been a crucial target for early non-pharmaceutical containment measures against the recent COVID-19 pandemic, with a degree of intensity ranging from public transportation line closures to regional, city or even home confinements. Nonetheless, quantitative knowledge on the relationship between mobility-contagions and, consequently, on the efficiency of containment measures remains elusive. Here we introduce an agent-based model with a simple interaction between mobility and contacts. Despite its simplicity our model shows the emergence of a critical mobility level, inducing major outbreaks when surpassed. We explore the interplay between mobility restrictions and the infection in recent intervention policies seen across many countries, and how interventions in the form of closures triggered by incidence rates can guide the epidemic into an oscillatory regime with recurrent waves. We consider how the different interventions impact societal well-being, the economy and the population. Finally, we propose a mitigation framework based on the critical nature of mobility in an epidemic, able to suppress incidence and oscillations at will, preventing extreme incidence peaks with potential to saturate health care resources.","sentences":["When considering airborne epidemic spreading in social systems, a natural connection arises between mobility and epidemic contacts.","As individuals travel, possibilities to encounter new people either at the final destination or during the transportation process appear.","Such contacts can lead to new contagion events.","In fact, mobility has been a crucial target for early non-pharmaceutical containment measures against the recent COVID-19 pandemic, with a degree of intensity ranging from public transportation line closures to regional, city or even home confinements.","Nonetheless, quantitative knowledge on the relationship between mobility-contagions and, consequently, on the efficiency of containment measures remains elusive.","Here we introduce an agent-based model with a simple interaction between mobility and contacts.","Despite its simplicity our model shows the emergence of a critical mobility level, inducing major outbreaks when surpassed.","We explore the interplay between mobility restrictions and the infection in recent intervention policies seen across many countries, and how interventions in the form of closures triggered by incidence rates can guide the epidemic into an oscillatory regime with recurrent waves.","We consider how the different interventions impact societal well-being, the economy and the population.","Finally, we propose a mitigation framework based on the critical nature of mobility in an epidemic, able to suppress incidence and oscillations at will, preventing extreme incidence peaks with potential to saturate health care resources."],"url":"http://arxiv.org/abs/2402.05739v1","category":"physics.soc-ph"}
{"created":"2024-02-08 15:14:39","title":"Blockchain Based Residential Smart Rent","abstract":"The real estate market includes complex and inefficient mediation processes. Renting a property envolves multiple entities with different responsibilities and interests. Therefore it is imperative to establish a trustful relationship between parties through intermediaries such as notaries, banks or real estate agencies to avoid eventual disputes. Although an intermediary ensures trust, the current process still has some drawbacks concerning efficiency, costs, transparency, bureaucracy and data security. The blockchain technology aims to reduce this issues by providing transparent and secure real estate transactions. We propose a GDPR compliant blockchain-based residential smart rental platform, designed to allow both landlords and tenants to establish rental contracts and make rental payments securely.","sentences":["The real estate market includes complex and inefficient mediation processes.","Renting a property envolves multiple entities with different responsibilities and interests.","Therefore it is imperative to establish a trustful relationship between parties through intermediaries such as notaries, banks or real estate agencies to avoid eventual disputes.","Although an intermediary ensures trust, the current process still has some drawbacks concerning efficiency, costs, transparency, bureaucracy and data security.","The blockchain technology aims to reduce this issues by providing transparent and secure real estate transactions.","We propose a GDPR compliant blockchain-based residential smart rental platform, designed to allow both landlords and tenants to establish rental contracts and make rental payments securely."],"url":"http://arxiv.org/abs/2402.05737v1","category":"cs.CR"}
{"created":"2024-02-08 15:11:03","title":"A numerical study of the Bose-Einstein condensates in a double-well trap using finite differences","abstract":"Bose-Einstein condensates in a double-well potential contain the essential ingredients to study many-body systems within a rich classical phase-space that includes an unstable point and a separatrix. Employing a selfconsistent finite difference method, we study some of their quantum properties and their dependency on the strength of the boson-boson interaction. We observe a deviation in the critical parameters associated with a behavior change in both the energy distribution and the eigenstates of the system. We also examine the trends of the nonclassicality via the Wigner function, the tunneling transmission coefficient, and the nonorthogonality of eigenstates associated with the nonlinearity aspects of the Gross-Pitaevskii equation.","sentences":["Bose-Einstein condensates in a double-well potential contain the essential ingredients to study many-body systems within a rich classical phase-space that includes an unstable point and a separatrix.","Employing a selfconsistent finite difference method, we study some of their quantum properties and their dependency on the strength of the boson-boson interaction.","We observe a deviation in the critical parameters associated with a behavior change in both the energy distribution and the eigenstates of the system.","We also examine the trends of the nonclassicality via the Wigner function, the tunneling transmission coefficient, and the nonorthogonality of eigenstates associated with the nonlinearity aspects of the Gross-Pitaevskii equation."],"url":"http://arxiv.org/abs/2402.05735v1","category":"quant-ph"}
{"created":"2024-02-08 15:08:57","title":"TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation","abstract":"Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activities, and laboratory work. We conduct extensive experiments with various state-of-the-art LLMs using TimeArena. Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.","sentences":["Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time.","To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios.","In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time.","We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment.","TimeArena grounds to 30 real-world tasks in cooking, household activities, and laboratory work.","We conduct extensive experiments with various state-of-the-art LLMs using TimeArena.","Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents."],"url":"http://arxiv.org/abs/2402.05733v1","category":"cs.CL"}
{"created":"2024-02-08 14:53:33","title":"Physical Layer Security over Fluid Antenna Systems","abstract":"This paper investigates the performance of physical layer security (PLS) in fluid antenna-aided communication systems under arbitrary correlated fading channels. In particular, it is considered that a single fixed-antenna transmitter aims to send confidential information to a legitimate receiver equipped with a planar fluid antenna system (FAS), while an eavesdropper, also taking advantage of a planar FAS, attempts to decode the desired message. For this scenario, we first present analytical expressions of the equivalent channel distributions at the legitimate user and eavesdropper by using copula, so that the obtained analytical results are valid for any arbitrarily correlated fading distributions. Then, with the help of Gauss-Laguerre quadrature, we derive compact analytical expressions for the average secrecy capacity (ASC), the secrecy outage probability (SOP), and the secrecy energy efficiency (SEE) for the FAS wiretap channel. Moreover, for exemplary purposes, we also obtain the compact expression of ASC, SOP, and SEE by utilizing the Gaussian copula under correlated Rayleigh fading channels as a special case. Eventually, numerical results indicate that applying the fluid antenna with only one active port to PLS can guarantee more secure and reliable transmission, when compared to traditional antenna systems (TAS) exploiting maximal ratio combining (MRC).","sentences":["This paper investigates the performance of physical layer security (PLS) in fluid antenna-aided communication systems under arbitrary correlated fading channels.","In particular, it is considered that a single fixed-antenna transmitter aims to send confidential information to a legitimate receiver equipped with a planar fluid antenna system (FAS), while an eavesdropper, also taking advantage of a planar FAS, attempts to decode the desired message.","For this scenario, we first present analytical expressions of the equivalent channel distributions at the legitimate user and eavesdropper by using copula, so that the obtained analytical results are valid for any arbitrarily correlated fading distributions.","Then, with the help of Gauss-Laguerre quadrature, we derive compact analytical expressions for the average secrecy capacity (ASC), the secrecy outage probability (SOP), and the secrecy energy efficiency (SEE) for the FAS wiretap channel.","Moreover, for exemplary purposes, we also obtain the compact expression of ASC, SOP, and SEE by utilizing the Gaussian copula under correlated Rayleigh fading channels as a special case.","Eventually, numerical results indicate that applying the fluid antenna with only one active port to PLS can guarantee more secure and reliable transmission, when compared to traditional antenna systems (TAS) exploiting maximal ratio combining (MRC)."],"url":"http://arxiv.org/abs/2402.05722v1","category":"cs.IT"}
{"created":"2024-02-08 14:45:45","title":"Phase stability of Ni-Al nanoparticles","abstract":"The phase stability of NiAl clusters of nanometer size was studied by using the embedded atom model and Monte Carlo simulation techniques. For temperatures of 500 and 1000 K and for a range of compositions below 70 at.% Al, the equilibrium structures of the system were determined and compared with the bulk results. We found that the bulk NiAl (B2) and Ni3Al (L12) phases were stable phases in the nanoparticle system; however, for deviations from ideal composition, the analysis revealed that, because of the surface effect, the composition of the clusters was not uniform. There was a core region in which the structure was ordered, B2 or L12, with a composition very close to the ideal, and a chemically disordered mantle region that allocated the deviations from ideal stoichiometries; in this way, a larger phase field appeared, indicating trends similar to those found in experiments on nanocrystalline NiAl powder [S.K. Pabi and B.S. Murty, Mater. Sci. Eng. A214, 146 (1996)]. For concentrations between 37 and 51 at.% Al, an intermediate phase, similar to the tetragonal L10 martensite, appeared.","sentences":["The phase stability of NiAl clusters of nanometer size was studied by using the embedded atom model and Monte Carlo simulation techniques.","For temperatures of 500 and 1000 K and for a range of compositions below 70 at.% Al, the equilibrium structures of the system were determined and compared with the bulk results.","We found that the bulk NiAl (B2) and Ni3Al (L12) phases were stable phases in the nanoparticle system; however, for deviations from ideal composition, the analysis revealed that, because of the surface effect, the composition of the clusters was not uniform.","There was a core region in which the structure was ordered, B2 or L12, with a composition very close to the ideal, and a chemically disordered mantle region that allocated the deviations from ideal stoichiometries; in this way, a larger phase field appeared, indicating trends similar to those found in experiments on nanocrystalline NiAl powder [S.K. Pabi and B.S. Murty, Mater.","Sci.","Eng. A214, 146 (1996)].","For concentrations between 37 and 51 at.% Al, an intermediate phase, similar to the tetragonal L10 martensite, appeared."],"url":"http://arxiv.org/abs/2402.05717v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-08 14:38:29","title":"Using Changeset Descriptions as a Data Source to Assist Feature Location","abstract":"Feature location attempts to assist developers in discovering functionality in source code. Many textual feature location techniques utilize information retrieval and rely on comments and identifiers of source code to describe software entities. An interesting alternative would be to employ the changeset descriptions of the code altered in that changeset as a data source to describe such software entities. To investigate this we implement a technique utilizing changeset descriptions and conduct an empirical study to observe this technique's overall performance. Moreover, we study how the granularity (i.e. file or method level of software entities) and changeset range inclusion (i.e. most recent or all historical changesets) affect such an approach. The results of a preliminary study with Rhino and Mylyn.Tasks systems suggest that the approach could lead to a potentially efficient feature location technique. They also suggest that it is advantageous in terms of the effort to configure the technique at method level granularity and that older changesets from older systems may reduce the effectiveness of the technique.","sentences":["Feature location attempts to assist developers in discovering functionality in source code.","Many textual feature location techniques utilize information retrieval and rely on comments and identifiers of source code to describe software entities.","An interesting alternative would be to employ the changeset descriptions of the code altered in that changeset as a data source to describe such software entities.","To investigate this we implement a technique utilizing changeset descriptions and conduct an empirical study to observe this technique's overall performance.","Moreover, we study how the granularity (i.e. file or method level of software entities) and changeset range inclusion (i.e. most recent or all historical changesets) affect such an approach.","The results of a preliminary study with Rhino and Mylyn.","Tasks systems suggest that the approach could lead to a potentially efficient feature location technique.","They also suggest that it is advantageous in terms of the effort to configure the technique at method level granularity and that older changesets from older systems may reduce the effectiveness of the technique."],"url":"http://arxiv.org/abs/2402.05711v1","category":"cs.SE"}
{"created":"2024-02-08 14:38:08","title":"Vanishing Immanants","abstract":"We classify all the irreducible characters of a symmetric group such that the induced immanant function $d_{\\chi}$ vanishes identically on alternate matrices with the entries in the complex field.","sentences":["We classify all the irreducible characters of a symmetric group such that the induced immanant function $d_{\\chi}$ vanishes identically on alternate matrices with the entries in the complex field."],"url":"http://arxiv.org/abs/2402.05710v1","category":"math.RT"}
{"created":"2024-02-08 14:37:58","title":"Exploring the Nostr Ecosystem: A Study of Decentralization and Resilience","abstract":"Nostr is an open decentralized social network launched in 2022. From a user's perspective, it is similar to a micro-blogging service like Twitter. However, the underlying infrastructure is very different, and Nostr boasts a range of unique features that set it apart. Nostr introduces the concept of relays, which act as open storage servers that receive, store, and distribute user posts. Each user is uniquely identified by a public key, ensuring authenticity of posts through digital signatures. Consequently, users are able to securely send and receive posts through various relays, which frees them from single-server reliance and enhances post availability (e.g., making it more censorship resistant). The Nostr ecosystem has garnered significant attention, boasting 4 million users and 60 million posts in just 2 years. To understand its characteristics and challenges, we conduct the first large-scale measurement of the Nostr ecosystem, spanning from July 1, 2023, to December 31, 2023. Our study focuses on two key aspects: Nostr relays and post replication strategies. We find that Nostr achieves superior decentralization compared to traditional Fediverse applications. However, relay availability remains a challenge, where financial sustainability (particularly for free-to-use relays) emerges as a contributing factor. We also find that the replication of posts across relays enhances post availability but introduces significant overhead. To address this, we propose two design innovations. One to control the number of post replications, and another to reduce the overhead during post retrieval. Via data-driven evaluations, we demonstrate their effectiveness without negatively impacting the system.","sentences":["Nostr is an open decentralized social network launched in 2022.","From a user's perspective, it is similar to a micro-blogging service like Twitter.","However, the underlying infrastructure is very different, and Nostr boasts a range of unique features that set it apart.","Nostr introduces the concept of relays, which act as open storage servers that receive, store, and distribute user posts.","Each user is uniquely identified by a public key, ensuring authenticity of posts through digital signatures.","Consequently, users are able to securely send and receive posts through various relays, which frees them from single-server reliance and enhances post availability (e.g., making it more censorship resistant).","The Nostr ecosystem has garnered significant attention, boasting 4 million users and 60 million posts in just 2 years.","To understand its characteristics and challenges, we conduct the first large-scale measurement of the Nostr ecosystem, spanning from July 1, 2023, to December 31, 2023.","Our study focuses on two key aspects: Nostr relays and post replication strategies.","We find that Nostr achieves superior decentralization compared to traditional Fediverse applications.","However, relay availability remains a challenge, where financial sustainability (particularly for free-to-use relays) emerges as a contributing factor.","We also find that the replication of posts across relays enhances post availability but introduces significant overhead.","To address this, we propose two design innovations.","One to control the number of post replications, and another to reduce the overhead during post retrieval.","Via data-driven evaluations, we demonstrate their effectiveness without negatively impacting the system."],"url":"http://arxiv.org/abs/2402.05709v1","category":"cs.SI"}
{"created":"2024-02-08 14:27:13","title":"Shellable slices of hyperbolic polynomials and the degree principle","abstract":"We study a natural stratification of certain affine slices of univariate hyperbolic polynomials. We look into which posets of strata can be realized and show that the dual of the poset of strata is a shellable simplicial complex and in particular a combinatorial sphere. From this we obtain a g-theorem and an upper bound theorem on the number of strata. We use these results to design smaller test sets to improve upon Timofte's degree principle and give bounds on how much the degree principle can be improved.","sentences":["We study a natural stratification of certain affine slices of univariate hyperbolic polynomials.","We look into which posets of strata can be realized and show that the dual of the poset of strata is a shellable simplicial complex and in particular a combinatorial sphere.","From this we obtain a g-theorem and an upper bound theorem on the number of strata.","We use these results to design smaller test sets to improve upon Timofte's degree principle and give bounds on how much the degree principle can be improved."],"url":"http://arxiv.org/abs/2402.05702v1","category":"math.AG"}
{"created":"2024-02-08 14:19:35","title":"Global Solution of the Inverse Spectral Problem for Differential Operators on a Finite Interval with Complex Weights","abstract":"Non-self-adjoint second-order ordinary differential operators on a finite interval with complex weights are studied. Properties of spectral characteristics are established and the inverse problem of recovering operators from their spectral characteristics are investigated. For this class of nonlinear inverse problems an algorithm for constructing the global solution is obtained. To study this class of inverse problems, we develop ideas of the method of spectral mappings.","sentences":["Non-self-adjoint second-order ordinary differential operators on a finite interval with complex weights are studied.","Properties of spectral characteristics are established and the inverse problem of recovering operators from their spectral characteristics are investigated.","For this class of nonlinear inverse problems an algorithm for constructing the global solution is obtained.","To study this class of inverse problems, we develop ideas of the method of spectral mappings."],"url":"http://arxiv.org/abs/2402.05697v1","category":"math.SP"}
{"created":"2024-02-08 14:16:58","title":"Continuity of the natural dimension of piecewise linear iterated function systems","abstract":"We consider iterated function systems on the real line that consist of continuous, piecewise linear functions. We show that typically the natural dimension of these systems changes continuously with respect to the parameters that define the system. As an application of this property, we prove a result on the positivity of the Lebesgue measure of the attractor.","sentences":["We consider iterated function systems on the real line that consist of continuous, piecewise linear functions.","We show that typically the natural dimension of these systems changes continuously with respect to the parameters that define the system.","As an application of this property, we prove a result on the positivity of the Lebesgue measure of the attractor."],"url":"http://arxiv.org/abs/2402.05695v1","category":"math.DS"}
{"created":"2024-02-08 14:10:21","title":"What is the best simulation approach for measuring local density fluctuations near solvo/hydrophobes?","abstract":"Measurements of local density fluctuations are crucial to characterizing the interfacial properties of equilibrium fluids. A specific case that has been well-explored involves the heightened compressibility of water near hydrophobic entities. Commonly, a spatial profile of local fluctuation strength is constructed from measurements of the mean and variance of solvent particle number fluctuations in a set of contiguous sub-volumes of the system adjacent to the solvo/hydrophobe. An alternative measure proposed by Evans and Stewart (J. Phys.: Condens. Matter 27, 194111 (2015)) defines a local compressibility profile in terms of the chemical potential derivative of the spatial number density profile. Using Grand Canonical Monte Carlo simulation, we compare and contrast the efficacy of these two approaches for a Lennard-Jones solvent at spherical and planar solvophobic interfaces, and SPC/E water at a hydrophobic spherical solute. Our principal findings are that: (i) the local compressibility profile $\\chi({\\bf r})$ of Evans and Stewart is considerably more sensitive to variations in the strength of local density fluctuations than the spatial fluctuation profile $F({\\bf r})$ and can resolve much more detailed structure; (ii) while the local compressibility profile is essentially independent of the choice of spatial discretization used to construct the profile, the spatial fluctuation profile exhibits strong systematic dependence on the size of the subvolumes on which the profile is defined. We clarify the origin and nature of this finite-size effect.","sentences":["Measurements of local density fluctuations are crucial to characterizing the interfacial properties of equilibrium fluids.","A specific case that has been well-explored involves the heightened compressibility of water near hydrophobic entities.","Commonly, a spatial profile of local fluctuation strength is constructed from measurements of the mean and variance of solvent particle number fluctuations in a set of contiguous sub-volumes of the system adjacent to the solvo/hydrophobe.","An alternative measure proposed by Evans and Stewart (J. Phys.: Condens.","Matter 27, 194111 (2015)) defines a local compressibility profile in terms of the chemical potential derivative of the spatial number density profile.","Using Grand Canonical Monte Carlo simulation, we compare and contrast the efficacy of these two approaches for a Lennard-Jones solvent at spherical and planar solvophobic interfaces, and SPC/E water at a hydrophobic spherical solute.","Our principal findings are that: (i) the local compressibility profile $\\chi({\\bf r})$ of Evans and Stewart is considerably more sensitive to variations in the strength of local density fluctuations than the spatial fluctuation profile $F({\\bf r})$ and can resolve much more detailed structure; (ii) while the local compressibility profile is essentially independent of the choice of spatial discretization used to construct the profile, the spatial fluctuation profile exhibits strong systematic dependence on the size of the subvolumes on which the profile is defined.","We clarify the origin and nature of this finite-size effect."],"url":"http://arxiv.org/abs/2402.05692v1","category":"cond-mat.soft"}
{"created":"2024-02-08 14:06:11","title":"Derivative-free sample-and-hold control with prescribed performance","abstract":"A feedback controller is proposed to perform output reference tracking with prescribed performance for unknown nonlinear continuous-time systems. The controller is of sampled-data type, i.e., measurements are available only at sampling times - a typical situation in real systems when sensors are involved. Furthermore, only output information is available, i.e., neither the full state nor derivatives can be used for feedback. A sufficient uniform sampling rate is derived and the control consists of piecewise constant signals on the sampling intervals, i.e., zero-order hold. Feasibility of the controller and satisfaction of the control objective are rigorously proven. The controller is illustrated by a numerical example.","sentences":["A feedback controller is proposed to perform output reference tracking with prescribed performance for unknown nonlinear continuous-time systems.","The controller is of sampled-data type, i.e., measurements are available only at sampling times - a typical situation in real systems when sensors are involved.","Furthermore, only output information is available, i.e., neither the full state nor derivatives can be used for feedback.","A sufficient uniform sampling rate is derived and the control consists of piecewise constant signals on the sampling intervals, i.e., zero-order hold.","Feasibility of the controller and satisfaction of the control objective are rigorously proven.","The controller is illustrated by a numerical example."],"url":"http://arxiv.org/abs/2402.05688v1","category":"math.OC"}
{"created":"2024-02-08 14:06:01","title":"Assessment of the Sparsity-Diversity Trade-offs in Active Users Detection for mMTC","abstract":"Wireless communication systems must increasingly support a multitude of machine-type communications (MTC) devices, thus calling for advanced strategies for active user detection (AUD). Recent literature has delved into AUD techniques based on compressed sensing, highlighting the critical role of signal sparsity. This study investigates the relationship between frequency diversity and signal sparsity in the AUD problem. Single-antenna users transmit multiple copies of non-orthogonal pilots across multiple frequency channels and the base station independently performs AUD in each channel using the orthogonal matching pursuit algorithm. We note that, although frequency diversity may improve the likelihood of successful reception of the signals, it may also damage the channel sparsity level, leading to important trade-offs. We show that a sparser signal significantly benefits AUD, surpassing the advantages brought by frequency diversity in scenarios with limited temporal resources and/or high numbers of receive antennas. Conversely, with longer pilots and fewer receive antennas, investing in frequency diversity becomes more impactful, resulting in a tenfold AUD performance improvement.","sentences":["Wireless communication systems must increasingly support a multitude of machine-type communications (MTC) devices, thus calling for advanced strategies for active user detection (AUD).","Recent literature has delved into AUD techniques based on compressed sensing, highlighting the critical role of signal sparsity.","This study investigates the relationship between frequency diversity and signal sparsity in the AUD problem.","Single-antenna users transmit multiple copies of non-orthogonal pilots across multiple frequency channels and the base station independently performs AUD in each channel using the orthogonal matching pursuit algorithm.","We note that, although frequency diversity may improve the likelihood of successful reception of the signals, it may also damage the channel sparsity level, leading to important trade-offs.","We show that a sparser signal significantly benefits AUD, surpassing the advantages brought by frequency diversity in scenarios with limited temporal resources and/or high numbers of receive antennas.","Conversely, with longer pilots and fewer receive antennas, investing in frequency diversity becomes more impactful, resulting in a tenfold AUD performance improvement."],"url":"http://arxiv.org/abs/2402.05687v1","category":"cs.IT"}
{"created":"2024-02-08 14:00:17","title":"Optimum dimensional synthesis of planar mechanisms with geometric constraints","abstract":"The deformed energy method has shown to be a good option for dimensional synthesis of mechanisms. In this paper the introduction of some new features to such approach is proposed. First, constraints fixing dimensions of certain links are introduced in the error function of the synthesis problem. Second, requirements on distances between determinate nodes are included in the error function for the analysis of the deformed position problem. Both the overall synthesis error function and the inner analysis error function are optimized using a Sequential Quadratic Problem (SQP) approach. This also reduces the probability of branch or circuit defects. In the case of the inner function analytical derivatives are used, while in the synthesis optimization approximate derivatives have been introduced. Furthermore, constraints are analyzed under two formulations, the Euclidean distance and an alternative approach that uses the previous raised to the power of two. The latter approach is often used in kinematics, and simplifies the computation of derivatives. Some examples are provided to show the convergence order of the error function and the fulfilment of the constraints in both formulations studied under different topological situations or achieved energy levels.","sentences":["The deformed energy method has shown to be a good option for dimensional synthesis of mechanisms.","In this paper the introduction of some new features to such approach is proposed.","First, constraints fixing dimensions of certain links are introduced in the error function of the synthesis problem.","Second, requirements on distances between determinate nodes are included in the error function for the analysis of the deformed position problem.","Both the overall synthesis error function and the inner analysis error function are optimized using a Sequential Quadratic Problem (SQP) approach.","This also reduces the probability of branch or circuit defects.","In the case of the inner function analytical derivatives are used, while in the synthesis optimization approximate derivatives have been introduced.","Furthermore, constraints are analyzed under two formulations, the Euclidean distance and an alternative approach that uses the previous raised to the power of two.","The latter approach is often used in kinematics, and simplifies the computation of derivatives.","Some examples are provided to show the convergence order of the error function and the fulfilment of the constraints in both formulations studied under different topological situations or achieved energy levels."],"url":"http://arxiv.org/abs/2402.05684v1","category":"math.NA"}
{"created":"2024-02-08 13:59:27","title":"The Cellular Homology of Digraphs","abstract":"In \\cite{TY}, we consider the minimal paths in the path complex associated to a digraph $G$ under the strongly regular condition. In this paper, first, based on our new definitions of admissible pair and admissible relations among the set of minimal paths, we will give the definitions of cellular chain complex associated to $G$ and prove the well-definedness. Then we will study several properties of such cellular homologies. Finally, we give several interesting examples as well as some important observations.","sentences":["In \\cite{TY}, we consider the minimal paths in the path complex associated to a digraph $G$ under the strongly regular condition.","In this paper, first, based on our new definitions of admissible pair and admissible relations among the set of minimal paths, we will give the definitions of cellular chain complex associated to $G$ and prove the well-definedness.","Then we will study several properties of such cellular homologies.","Finally, we give several interesting examples as well as some important observations."],"url":"http://arxiv.org/abs/2402.05682v1","category":"math.CO"}
{"created":"2024-02-08 13:47:27","title":"Representation of the Terrestrial Carbon Cycle in CMIP6","abstract":"Improvements in the representation of the land carbon cycle in Earth system models participating in the Coupled Model Intercomparison Project Phase 6 (CMIP6) include interactive treatment of both the carbon and nitrogen cycles, improved photosynthesis, and soil hydrology. To assess the impact of these model developments on aspects of the global carbon cycle, the Earth System Model Evaluation Tool is expanded to compare CO2 concentration and emission-driven historical simulations from CMIP5 and CMIP6 to observational data sets. Overestimations of photosynthesis (GPP) in CMIP5 were largely resolved in CMIP6 for participating models with an interactive nitrogen cycle, but remaining for models without one. This points to the importance of including nutrient limitation. Simulating the leaf area index (LAI) remains challenging with a large model spread in both CMIP5 and CMIP6. In ESMs, global mean land carbon uptake (NBP) is well reproduced in the CMIP5 and CMIP6 multi-model means. However, this is the result of an underestimation of NBP in the northern hemisphere, which is compensated by an overestimation in the southern hemisphere and the tropics. Overall, a slight improvement in the simulation of land carbon cycle parameters is found in CMIP6 compared to CMIP5, but with many biases remaining, further improvements of models in particular for LAI and NBP is required. Emission-driven simulations perform just as well as concentration driven models despite the added process-realism. Due to this we recommend ESMs in future CMIP phases to perform emission-driven simulations as the standard so that climate-carbon cycle feedbacks are fully active. The inclusion of nitrogen limitation led to a large improvement in photosynthesis compared to models not including this process, suggesting the need to view the nitrogen cycle as a necessary part of all future carbon cycle models.","sentences":["Improvements in the representation of the land carbon cycle in Earth system models participating in the Coupled Model Intercomparison Project Phase 6 (CMIP6) include interactive treatment of both the carbon and nitrogen cycles, improved photosynthesis, and soil hydrology.","To assess the impact of these model developments on aspects of the global carbon cycle, the Earth System Model Evaluation Tool is expanded to compare CO2 concentration and emission-driven historical simulations from CMIP5 and CMIP6 to observational data sets.","Overestimations of photosynthesis (GPP) in CMIP5 were largely resolved in CMIP6 for participating models with an interactive nitrogen cycle, but remaining for models without one.","This points to the importance of including nutrient limitation.","Simulating the leaf area index (LAI) remains challenging with a large model spread in both CMIP5 and CMIP6.","In ESMs, global mean land carbon uptake (NBP) is well reproduced in the CMIP5 and CMIP6 multi-model means.","However, this is the result of an underestimation of NBP in the northern hemisphere, which is compensated by an overestimation in the southern hemisphere and the tropics.","Overall, a slight improvement in the simulation of land carbon cycle parameters is found in CMIP6 compared to CMIP5, but with many biases remaining, further improvements of models in particular for LAI and NBP is required.","Emission-driven simulations perform just as well as concentration driven models despite the added process-realism.","Due to this we recommend ESMs in future CMIP phases to perform emission-driven simulations as the standard so that climate-carbon cycle feedbacks are fully active.","The inclusion of nitrogen limitation led to a large improvement in photosynthesis compared to models not including this process, suggesting the need to view the nitrogen cycle as a necessary part of all future carbon cycle models."],"url":"http://arxiv.org/abs/2402.05671v1","category":"physics.ao-ph"}
{"created":"2024-02-08 13:46:42","title":"New Properties of Holomorphic Sobolev-Hardy Spaces","abstract":"We give new characterizations of the optimal data space for the $L^p(bD,\\sigma)$-Neumann boundary value problem for the $\\bar{\\partial}$ operator associated to a bounded, Lipschitz domain $D\\subset\\mathbb{C}$. We show that the solution space is embedded (as a Banach space) in the Dirichlet space and that for $p=2$, the solution space is a reproducing kernel Hilbert space.","sentences":["We give new characterizations of the optimal data space for the $L^p(bD,\\sigma)$-Neumann boundary value problem for the $\\bar{\\partial}$ operator associated to a bounded, Lipschitz domain $D\\subset\\mathbb{C}$. We show that the solution space is embedded (as a Banach space) in the Dirichlet space and that for $p=2$, the solution space is a reproducing kernel Hilbert space."],"url":"http://arxiv.org/abs/2402.05670v1","category":"math.CV"}
{"created":"2024-02-08 13:38:23","title":"S$\u03a9$I: Score-based O-INFORMATION Estimation","abstract":"The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables. Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions. Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables. One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems. However, its practical application is limited to simplified cases. In this work, we introduce S$\\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system. Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$\\Omega$I in the context of a real-world use case.","sentences":["The analysis of scientific data and complex multivariate systems requires information quantities that capture relationships among multiple random variables.","Recently, new information-theoretic measures have been developed to overcome the shortcomings of classical ones, such as mutual information, that are restricted to considering pairwise interactions.","Among them, the concept of information synergy and redundancy is crucial for understanding the high-order dependencies between variables.","One of the most prominent and versatile measures based on this concept is O-information, which provides a clear and scalable way to quantify the synergy-redundancy balance in multivariate systems.","However, its practical application is limited to simplified cases.","In this work, we introduce S$\\Omega$I, which allows for the first time to compute O-information without restrictive assumptions about the system.","Our experiments validate our approach on synthetic data, and demonstrate the effectiveness of S$\\Omega$I in the context of a real-world use case."],"url":"http://arxiv.org/abs/2402.05667v1","category":"cs.LG"}
{"created":"2024-02-08 13:11:55","title":"Bias induced circular current in a loop nanojunction with AAH modulation: Role of hopping dimerization","abstract":"In this work, we put forward, for the first time, the interplay between correlated disorder and hopping dimerization on bias driven circular current in a loop conductor that is clamped between two electrodes. The correlated disorder is introduced in site energies of the ring in the form of Aubry-Andr\\'e-Harper (AAH) model. Simulating the quantum system within a tight-binding framework all the results are worked out based on the standard wave-guide theory. Unlike transport current, commonly referred to drain current, circular current in the loop conductor can get enhanced with increasing disorder strength. This enhancement becomes much effective when hopping dimerization is included which is taken following the Su-Schrieffer-Heeger (SSH) model. The characteristic features of bias driven circular current are studied under different input conditions and we find the results are robust for wide range of physical parameters. Our analysis may provide a new insight in analyzing transport behavior in different disordered lattices in presence of additional restrictions in hopping integrals.","sentences":["In this work, we put forward, for the first time, the interplay between correlated disorder and hopping dimerization on bias driven circular current in a loop conductor that is clamped between two electrodes.","The correlated disorder is introduced in site energies of the ring in the form of Aubry-Andr\\'e-Harper (AAH) model.","Simulating the quantum system within a tight-binding framework all the results are worked out based on the standard wave-guide theory.","Unlike transport current, commonly referred to drain current, circular current in the loop conductor can get enhanced with increasing disorder strength.","This enhancement becomes much effective when hopping dimerization is included which is taken following the Su-Schrieffer-Heeger (SSH) model.","The characteristic features of bias driven circular current are studied under different input conditions and we find the results are robust for wide range of physical parameters.","Our analysis may provide a new insight in analyzing transport behavior in different disordered lattices in presence of additional restrictions in hopping integrals."],"url":"http://arxiv.org/abs/2402.05654v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-08 13:10:14","title":"Flat-histogram algorithms: optimal parameters and extended application","abstract":"We provide analysis of the convergence properties and applicability extensions of flat-histogram algorithms, with a particular focus on the Wang-Landau algorithms (exemplified by converging stochastic approximation Monte Carlo (SAMC)) and multicanonical (MUCA) algorithms. Our investigation reveals that the optimal decay rate of the modification factor in SAMC algorithms is influenced by the number of energy bins rather than the width of the energy range. Despite the frequent naming of these algorithms based on the histogram flatness, our findings indicate that flatness demonstrates a limited correlation with estimation accuracy. We explore the implications of integrating the importance sampling technique with flat-histogram algorithms, demonstrating that this combination yields comparable or better accuracy in density of states estimations, almost independent of specific algorithmic parameters within certain bounds. Furthermore, our research extends the possibilities of the flat-histogram and importance sampling combination for investigating a range of underlying system parameters simultaneously within a single simulation. These system parameters could both originate from the potential, e.g., various relative contributions of different energy terms or characteristic interaction range, and characterize the accessible configurations, e.g., through the size of the simulation box.","sentences":["We provide analysis of the convergence properties and applicability extensions of flat-histogram algorithms, with a particular focus on the Wang-Landau algorithms (exemplified by converging stochastic approximation Monte Carlo (SAMC)) and multicanonical (MUCA) algorithms.","Our investigation reveals that the optimal decay rate of the modification factor in SAMC algorithms is influenced by the number of energy bins rather than the width of the energy range.","Despite the frequent naming of these algorithms based on the histogram flatness, our findings indicate that flatness demonstrates a limited correlation with estimation accuracy.","We explore the implications of integrating the importance sampling technique with flat-histogram algorithms, demonstrating that this combination yields comparable or better accuracy in density of states estimations, almost independent of specific algorithmic parameters within certain bounds.","Furthermore, our research extends the possibilities of the flat-histogram and importance sampling combination for investigating a range of underlying system parameters simultaneously within a single simulation.","These system parameters could both originate from the potential, e.g., various relative contributions of different energy terms or characteristic interaction range, and characterize the accessible configurations, e.g., through the size of the simulation box."],"url":"http://arxiv.org/abs/2402.05653v1","category":"physics.comp-ph"}
{"created":"2024-02-08 13:07:56","title":"Su-Schrieffer-Heeger quasicrystal: Topology, localization, and mobility edge","abstract":"In this paper we discussed the topological transition between trivial and nontrivial phases of a quasi-periodic (Aubry-Andr\\'e like) mechanical Su-Schrieffer-Heeger (SSH) model. We find that there exists a nontrivial boundary separating the two topological phases and an analytical expression for this boundary is found. We discuss the localization of the vibrational modes using the calculation of the inverse participation ratio (IPR) and access the localization nature of the states of the system. We find three different regimes: extended, localized, and critical, depending on the intensity of the Aubry-Andr\\'e spring. We further study the energy dependent mobility edge (ME) separating localized from extended eigenstates and find its analytical expression for both commensurate and incommensurate modulation wavelengths, thus enlarging the library of models possessing analytical expressions for the ME. Our results extend previous results for the theory of fermionic topological insulators and localization theory in quantum matter to the classical realm.","sentences":["In this paper we discussed the topological transition between trivial and nontrivial phases of a quasi-periodic (Aubry-Andr\\'e like) mechanical Su-Schrieffer-Heeger (SSH) model.","We find that there exists a nontrivial boundary separating the two topological phases and an analytical expression for this boundary is found.","We discuss the localization of the vibrational modes using the calculation of the inverse participation ratio (IPR) and access the localization nature of the states of the system.","We find three different regimes: extended, localized, and critical, depending on the intensity of the Aubry-Andr\\'e spring.","We further study the energy dependent mobility edge (ME) separating localized from extended eigenstates and find its analytical expression for both commensurate and incommensurate modulation wavelengths, thus enlarging the library of models possessing analytical expressions for the ME.","Our results extend previous results for the theory of fermionic topological insulators and localization theory in quantum matter to the classical realm."],"url":"http://arxiv.org/abs/2402.05651v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-08 13:00:18","title":"Investigating Reproducibility in Deep Learning-Based Software Fault Prediction","abstract":"Over the past few years, deep learning methods have been applied for a wide range of Software Engineering (SE) tasks, including in particular for the important task of automatically predicting and localizing faults in software. With the rapid adoption of increasingly complex machine learning models, it however becomes more and more difficult for scholars to reproduce the results that are reported in the literature. This is in particular the case when the applied deep learning models and the evaluation methodology are not properly documented and when code and data are not shared. Given some recent -- and very worrying -- findings regarding reproducibility and progress in other areas of applied machine learning, the goal of this work is to analyze to what extent the field of software engineering, in particular in the area of software fault prediction, is plagued by similar problems. We have therefore conducted a systematic review of the current literature and examined the level of reproducibility of 56 research articles that were published between 2019 and 2022 in top-tier software engineering conferences. Our analysis revealed that scholars are apparently largely aware of the reproducibility problem, and about two thirds of the papers provide code for their proposed deep learning models. However, it turned out that in the vast majority of cases, crucial elements for reproducibility are missing, such as the code of the compared baselines, code for data pre-processing or code for hyperparameter tuning. In these cases, it therefore remains challenging to exactly reproduce the results in the current research literature. Overall, our meta-analysis therefore calls for improved research practices to ensure the reproducibility of machine-learning based research.","sentences":["Over the past few years, deep learning methods have been applied for a wide range of Software Engineering (SE) tasks, including in particular for the important task of automatically predicting and localizing faults in software.","With the rapid adoption of increasingly complex machine learning models, it however becomes more and more difficult for scholars to reproduce the results that are reported in the literature.","This is in particular the case when the applied deep learning models and the evaluation methodology are not properly documented and when code and data are not shared.","Given some recent -- and very worrying -- findings regarding reproducibility and progress in other areas of applied machine learning, the goal of this work is to analyze to what extent the field of software engineering, in particular in the area of software fault prediction, is plagued by similar problems.","We have therefore conducted a systematic review of the current literature and examined the level of reproducibility of 56 research articles that were published between 2019 and 2022 in top-tier software engineering conferences.","Our analysis revealed that scholars are apparently largely aware of the reproducibility problem, and about two thirds of the papers provide code for their proposed deep learning models.","However, it turned out that in the vast majority of cases, crucial elements for reproducibility are missing, such as the code of the compared baselines, code for data pre-processing or code for hyperparameter tuning.","In these cases, it therefore remains challenging to exactly reproduce the results in the current research literature.","Overall, our meta-analysis therefore calls for improved research practices to ensure the reproducibility of machine-learning based research."],"url":"http://arxiv.org/abs/2402.05645v1","category":"cs.SE"}
{"created":"2024-02-08 12:36:35","title":"Strassen's algorithm is not optimally accurate","abstract":"We propose a non-commutative algorithm for multiplying 2x2 matrices using 7 coefficient products. This algorithm reaches simultaneously a better accuracy in practice compared to previously known such fast algorithms, and a time complexity bound with the best currently known leading term (obtained via alternate basis sparsification). To build this algorithm, we consider matrix and tensor norms bounds governing the stability and accuracy of numerical matrix multiplication. First, we reduce those bounds by minimizing a growth factor along the unique orbit of Strassen's 2x2-matrix multiplication tensor decomposition. Second, we develop heuristics for minimizing the number of operations required to realize a given bilinear formula, while further improving its accuracy. Third, we perform an alternate basis sparsification that improves on the time complexity constant and mostly preserves the overall accuracy.","sentences":["We propose a non-commutative algorithm for multiplying 2x2 matrices using 7 coefficient products.","This algorithm reaches simultaneously a better accuracy in practice compared to previously known such fast algorithms, and a time complexity bound with the best currently known leading term (obtained via alternate basis sparsification).","To build this algorithm, we consider matrix and tensor norms bounds governing the stability and accuracy of numerical matrix multiplication.","First, we reduce those bounds by minimizing a growth factor along the unique orbit of Strassen's 2x2-matrix multiplication tensor decomposition.","Second, we develop heuristics for minimizing the number of operations required to realize a given bilinear formula, while further improving its accuracy.","Third, we perform an alternate basis sparsification that improves on the time complexity constant and mostly preserves the overall accuracy."],"url":"http://arxiv.org/abs/2402.05630v1","category":"math.NA"}
{"created":"2024-02-08 12:35:41","title":"RepQuant: Towards Accurate Post-Training Quantization of Large Transformer Models via Scale Reparameterization","abstract":"Large transformer models have demonstrated remarkable success. Post-training quantization (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models. Regrettably, existing PTQ methods typically exhibit non-trivial performance loss. We find that the performance bottleneck stems from over-consideration of hardware compatibility in the quantization process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy. With the above insights, we propose RepQuant, a novel PTQ framework with quantization-inference decoupling paradigm to address the above issues. RepQuant employs complex quantizers in the quantization process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through quantization scale reparameterization, thus ensuring both accurate quantization and efficient inference. More specifically, we focus on two components with extreme distributions: LayerNorm activations and Softmax activations. Initially, we apply channel-wise quantization and log$\\sqrt{2}$ quantization, respectively, which are tailored to their distributions. In particular, for the former, we introduce a learnable per-channel dual clipping scheme, which is designed to efficiently identify outliers in the unbalanced activations with fine granularity. Then, we reparameterize the scales to hardware-friendly layer-wise quantization and log2 quantization for inference. Moreover, quantized weight reconstruction is seamlessly integrated into the above procedure to further push the performance limits. Extensive experiments are performed on different large-scale transformer variants on multiple tasks, including vision, language, and multi-modal transformers, and RepQuant encouragingly demonstrates significant performance advantages.","sentences":["Large transformer models have demonstrated remarkable success.","Post-training quantization (PTQ), which requires only a small dataset for calibration and avoids end-to-end retraining, is a promising solution for compressing these large models.","Regrettably, existing PTQ methods typically exhibit non-trivial performance loss.","We find that the performance bottleneck stems from over-consideration of hardware compatibility in the quantization process, compelling them to reluctantly employ simple quantizers, albeit at the expense of accuracy.","With the above insights, we propose RepQuant, a novel PTQ framework with quantization-inference decoupling paradigm to address the above issues.","RepQuant employs complex quantizers in the quantization process and simplified quantizers in the inference process, and performs mathematically equivalent transformations between the two through quantization scale reparameterization, thus ensuring both accurate quantization and efficient inference.","More specifically, we focus on two components with extreme distributions: LayerNorm activations and Softmax activations.","Initially, we apply channel-wise quantization and log$\\sqrt{2}$ quantization, respectively, which are tailored to their distributions.","In particular, for the former, we introduce a learnable per-channel dual clipping scheme, which is designed to efficiently identify outliers in the unbalanced activations with fine granularity.","Then, we reparameterize the scales to hardware-friendly layer-wise quantization and log2 quantization for inference.","Moreover, quantized weight reconstruction is seamlessly integrated into the above procedure to further push the performance limits.","Extensive experiments are performed on different large-scale transformer variants on multiple tasks, including vision, language, and multi-modal transformers, and RepQuant encouragingly demonstrates significant performance advantages."],"url":"http://arxiv.org/abs/2402.05628v1","category":"cs.LG"}
{"created":"2024-02-08 12:25:08","title":"Optimized Denial-of-Service Threats on the Scalability of LT Coded Blockchains","abstract":"Coded blockchains have acquired prominence in the recent past as a promising approach to slash the storage costs as well as to facilitate scalability. Within this class, Luby Transform (LT) coded blockchains are an appealing choice for scalability in heterogeneous networks owing to the availability of a wide range of low-complexity LT decoders. While these architectures have been studied from the aspects of storage savings and scalability, not much is known in terms of their security vulnerabilities. Pointing at this research gap, in this work, we present novel denial-of-service (DoS) threats on LT coded blockchains that target nodes with specific decoding capabilities, thereby preventing them from joining the network. Our proposed threats are non-oblivious in nature, wherein adversaries gain access to the archived blocks, and choose to execute their threat on a subset of them based on underlying coding scheme. We show that our optimized threats can achieve the same level of damage as that of blind attacks, however, with limited amount of resources. This is the first work of its kind that opens up new questions on designing coded blockchains to jointly provide storage savings, scalability and resilience to optimized threats.","sentences":["Coded blockchains have acquired prominence in the recent past as a promising approach to slash the storage costs as well as to facilitate scalability.","Within this class, Luby Transform (LT) coded blockchains are an appealing choice for scalability in heterogeneous networks owing to the availability of a wide range of low-complexity LT decoders.","While these architectures have been studied from the aspects of storage savings and scalability, not much is known in terms of their security vulnerabilities.","Pointing at this research gap, in this work, we present novel denial-of-service (DoS) threats on LT coded blockchains that target nodes with specific decoding capabilities, thereby preventing them from joining the network.","Our proposed threats are non-oblivious in nature, wherein adversaries gain access to the archived blocks, and choose to execute their threat on a subset of them based on underlying coding scheme.","We show that our optimized threats can achieve the same level of damage as that of blind attacks, however, with limited amount of resources.","This is the first work of its kind that opens up new questions on designing coded blockchains to jointly provide storage savings, scalability and resilience to optimized threats."],"url":"http://arxiv.org/abs/2402.05620v1","category":"cs.IT"}
{"created":"2024-02-08 12:05:49","title":"Internal Model Control design for systems learned by Control Affine Neural Nonlinear Autoregressive Exogenous Models","abstract":"This paper explores the use of Control Affine Neural Nonlinear AutoRegressive eXogenous (CA-NNARX) models for nonlinear system identification and model-based control design. The idea behind this architecture is to match the known control-affine structure of the system to achieve improved performance. Coherently with recent literature of neural networks for data-driven control, we first analyze the stability properties of CA-NNARX models, devising sufficient conditions for their incremental Input-to-State Stability ($\\delta$ISS) that can be enforced at the model training stage. The model's stability property is then leveraged to design a stable Internal Model Control (IMC) architecture. The proposed control scheme is tested on a simulated Quadruple Tank benchmark system to address the output reference tracking problem. The results achieved show that (i) the modeling accuracy of CA-NNARX is superior to the one of a standard NNARX model for given weight size and training epochs, and (ii) the proposed IMC law provides performance comparable to the ones of a standard Model Predictive Controller (MPC) at a significantly lower computational burden.","sentences":["This paper explores the use of Control Affine Neural Nonlinear AutoRegressive eXogenous (CA-NNARX) models for nonlinear system identification and model-based control design.","The idea behind this architecture is to match the known control-affine structure of the system to achieve improved performance.","Coherently with recent literature of neural networks for data-driven control, we first analyze the stability properties of CA-NNARX models, devising sufficient conditions for their incremental Input-to-State Stability ($\\delta$ISS) that can be enforced at the model training stage.","The model's stability property is then leveraged to design a stable Internal Model Control (IMC) architecture.","The proposed control scheme is tested on a simulated Quadruple Tank benchmark system to address the output reference tracking problem.","The results achieved show that (i) the modeling accuracy of CA-NNARX is superior to the one of a standard NNARX model for given weight size and training epochs, and (ii) the proposed IMC law provides performance comparable to the ones of a standard Model Predictive Controller (MPC) at a significantly lower computational burden."],"url":"http://arxiv.org/abs/2402.05607v1","category":"eess.SY"}
{"created":"2024-02-08 11:46:03","title":"MERP: Metaverse Extended Realtiy Portal","abstract":"A standardized control system called Metaverse Extended Reality Portal (MERP) is presented as a solution to the issues with conventional VR eyewear. The MERP system improves user awareness of the physical world while offering an immersive 3D view of the metaverse by using a shouldermounted projector to display a Heads-Up Display (HUD) in a designated Metaverse Experience Room. To provide natural and secure interaction inside the metaverse, a compass module and gyroscope integration enable accurate mapping of real-world motions to avatar actions. Through user tests and research, the MERP system shows that it may reduce mishaps brought on by poor spatial awareness, offering an improved metaverse experience and laying the groundwork for future developments in virtual reality technology. MERP, which is compared with existing Virtual Reality (VR) glasses used to traverse the metaverse, is projected to become a seamless, novel and better alternative. Existing VR headsets and AR glasses have well-known drawbacks that making them ineffective for prolonged usage as it causes harm to the eyes.","sentences":["A standardized control system called Metaverse Extended Reality Portal (MERP) is presented as a solution to the issues with conventional VR eyewear.","The MERP system improves user awareness of the physical world while offering an immersive 3D view of the metaverse by using a shouldermounted projector to display a Heads-Up Display (HUD) in a designated Metaverse Experience Room.","To provide natural and secure interaction inside the metaverse, a compass module and gyroscope integration enable accurate mapping of real-world motions to avatar actions.","Through user tests and research, the MERP system shows that it may reduce mishaps brought on by poor spatial awareness, offering an improved metaverse experience and laying the groundwork for future developments in virtual reality technology.","MERP, which is compared with existing Virtual Reality (VR) glasses used to traverse the metaverse, is projected to become a seamless, novel and better alternative.","Existing VR headsets and AR glasses have well-known drawbacks that making them ineffective for prolonged usage as it causes harm to the eyes."],"url":"http://arxiv.org/abs/2402.05592v1","category":"cs.HC"}
{"created":"2024-02-08 11:37:22","title":"Fundamental physics with ESPRESSO: a new determination of the D/H ratio towards PKS1937-101","abstract":"Primordial abundances of light elements are sensitive to the physics of the early Universe and can directly constrain cosmological quantities, such as the baryon-to-photon ratio $\\eta_{10}$, the baryon density and the number of neutrino families. Deuterium is especially suited for these studies: its primordial abundance is sensitive and monotonically dependent on $\\eta_{10}$, allowing an independent measurement of the cosmic baryon density that can be compared, for instance, against the Planck satellite data. The primordial deuterium abundance can be measured in high $H_I$ column density absorption systems towards distant quasars. We report here a new measurement, based on high-resolution ESPRESSO data, of the primordial $D_I$ abundance of a system at redshift $z \\sim 3.572$, towards PKS1937-101. Using only ESPRESSO data, we find a D/H ratio of $2.638\\pm0.128 \\times 10^{-5}$, while including the available UVES data improves the precision, leading to a ratio of $2.608 \\pm 0.102 \\times 10^{-5}$. The results of this analysis agree with those of the most precise existing measurements. We find that the relatively low column density of this system ($\\log{N_{\\rm H_I}/ {\\rm cm}^{-2}}\\sim18 $) introduces modelling uncertainties, which become the main contributor to the error budget.","sentences":["Primordial abundances of light elements are sensitive to the physics of the early Universe and can directly constrain cosmological quantities, such as the baryon-to-photon ratio $\\eta_{10}$, the baryon density and the number of neutrino families.","Deuterium is especially suited for these studies: its primordial abundance is sensitive and monotonically dependent on $\\eta_{10}$, allowing an independent measurement of the cosmic baryon density that can be compared, for instance, against the Planck satellite data.","The primordial deuterium abundance can be measured in high $H_I$ column density absorption systems towards distant quasars.","We report here a new measurement, based on high-resolution ESPRESSO data, of the primordial $D_I$ abundance of a system at redshift $z \\sim 3.572$, towards PKS1937-101.","Using only ESPRESSO data, we find a D/H ratio of $2.638\\pm0.128 \\times 10^{-5}$, while including the available UVES data improves the precision, leading to a ratio of $2.608 \\pm 0.102 \\times 10^{-5}$.","The results of this analysis agree with those of the most precise existing measurements.","We find that the relatively low column density of this system ($\\log{N_{\\rm H_I}/ {\\rm cm}^{-2}}\\sim18 $) introduces modelling uncertainties, which become the main contributor to the error budget."],"url":"http://arxiv.org/abs/2402.05586v1","category":"astro-ph.CO"}
{"created":"2024-02-08 11:35:10","title":"On the Spectral Efficiency of Indoor Wireless Networks with a Rotary Uniform Linear Array","abstract":"Contemporary wireless communication systems rely on Multi-User Multiple-Input Multiple-Output (MU-MIMO) techniques. In such systems, each Access Point (AP) is equipped with multiple antenna elements and serves multiple devices simultaneously. Notably, traditional systems utilize fixed antennas, i.e., antennas without any movement capabilities, while the idea of movable antennas has recently gained traction among the research community. By moving in a confined region, movable antennas are able to exploit the wireless channel variation in the continuous domain. This additional degree of freedom may enhance the quality of the wireless links, and consequently the communication performance. However, movable antennas for MU-MIMO proposed in the literature are complex, bulky, expensive and present a high power consumption. In this paper, we propose an alternative to such systems that has lower complexity and lower cost. More specifically, we propose the incorporation of rotation capabilities to APs equipped with Uniform Linear Arrays (ULAs) of antennas. We consider the uplink of an indoor scenario where the AP serves multiple devices simultaneously. The optimal rotation of the ULA is computed based on estimates of the positions of the active devices and aiming at maximizing the per-user mean achievable Spectral Efficiency (SE). Adopting a spatially correlated Rician channel model, our numerical results show that the rotation capabilities of the AP can bring substantial improvements in the SE in scenarios where the line-of-sight component of the channel vectors is strong. Moreover, our proposed system is robust against imperfect positioning estimates.","sentences":["Contemporary wireless communication systems rely on Multi-User Multiple-Input Multiple-Output (MU-MIMO) techniques.","In such systems, each Access Point (AP) is equipped with multiple antenna elements and serves multiple devices simultaneously.","Notably, traditional systems utilize fixed antennas, i.e., antennas without any movement capabilities, while the idea of movable antennas has recently gained traction among the research community.","By moving in a confined region, movable antennas are able to exploit the wireless channel variation in the continuous domain.","This additional degree of freedom may enhance the quality of the wireless links, and consequently the communication performance.","However, movable antennas for MU-MIMO proposed in the literature are complex, bulky, expensive and present a high power consumption.","In this paper, we propose an alternative to such systems that has lower complexity and lower cost.","More specifically, we propose the incorporation of rotation capabilities to APs equipped with Uniform Linear Arrays (ULAs) of antennas.","We consider the uplink of an indoor scenario where the AP serves multiple devices simultaneously.","The optimal rotation of the ULA is computed based on estimates of the positions of the active devices and aiming at maximizing the per-user mean achievable Spectral Efficiency (SE).","Adopting a spatially correlated Rician channel model, our numerical results show that the rotation capabilities of the AP can bring substantial improvements in the SE in scenarios where the line-of-sight component of the channel vectors is strong.","Moreover, our proposed system is robust against imperfect positioning estimates."],"url":"http://arxiv.org/abs/2402.05583v1","category":"cs.IT"}
{"created":"2024-02-08 11:25:04","title":"Comment on \"A new universality class describes Vicsek's flocking phase in physical dimensions''","abstract":"In a recent preprint, ``A new universality class describes Vicsek's flocking phase in physical dimensions'', Patrick Jentsch and Chiu Fan Lee have computed the critical exponents of the Vicsek model in the ordered phase by means of functional renormalization group methods. In this note, we compare their results with our previous theoretical predictions for the Vicsek model, which is expected to be exact in $d=2$ dimensions. We point out that the critical exponents predicted by the two theories are extremely close in both $d=2$ and $3$ dimensions. We found that both theories fit the current numerical data equally well. Extensive numerical simulations for larger system sizes are thus highly desirable to judge which theory is correct.","sentences":["In a recent preprint, ``A new universality class describes Vicsek's flocking phase in physical dimensions'', Patrick Jentsch and Chiu Fan Lee have computed the critical exponents of the Vicsek model in the ordered phase by means of functional renormalization group methods.","In this note, we compare their results with our previous theoretical predictions for the Vicsek model, which is expected to be exact in $d=2$ dimensions.","We point out that the critical exponents predicted by the two theories are extremely close in both $d=2$ and $3$ dimensions.","We found that both theories fit the current numerical data equally well.","Extensive numerical simulations for larger system sizes are thus highly desirable to judge which theory is correct."],"url":"http://arxiv.org/abs/2402.05577v1","category":"cond-mat.soft"}
{"created":"2024-02-08 11:18:50","title":"Research on the evolution of domestic multi-functional meter technology","abstract":"The technical evolution of domestic multi-functional electricity meter is deeply discussed. With the rapid development of the domestic power market and the continuous innovation of technology, the domestic multi-functional electricity meters have experienced the transformation from simple billing to complex multi-functional, from a single application to a wide range of fields. This transformation has not only driven the rapid development of electricity meter technology, but also met the increasing power demand and management requirements. This paper expounds the concept of multi-function meter, the working principle and algorithm of digital multiplier, the initiation and evolution of multi-function electricity meter standard, and the initiation and evolution of domestic multi-function electricity meter products. Although the domestic independent production of multi-functional meter has made great achievements in performance, but in the reliability and key process technology still need to be improved. In addition, the development of communication technology also provides a new opportunity for the progress of electricity meter technology. The application of the new technology provides a more convenient and efficient way for the data transmission and remote management of electricity meters. Domestic multi-functional electricity meters have made remarkable achievements in technology evolution and application and expansion, but they still face some challenges and opportunities. In the future, with the continuous development of the power market and the promotion of smart grid construction, domestic multi-functional electricity meters need to continue to strengthen technological innovation and product research and development, improve the reliability and competitiveness of products, in order to meet higher application needs and market requirements.","sentences":["The technical evolution of domestic multi-functional electricity meter is deeply discussed.","With the rapid development of the domestic power market and the continuous innovation of technology, the domestic multi-functional electricity meters have experienced the transformation from simple billing to complex multi-functional, from a single application to a wide range of fields.","This transformation has not only driven the rapid development of electricity meter technology, but also met the increasing power demand and management requirements.","This paper expounds the concept of multi-function meter, the working principle and algorithm of digital multiplier, the initiation and evolution of multi-function electricity meter standard, and the initiation and evolution of domestic multi-function electricity meter products.","Although the domestic independent production of multi-functional meter has made great achievements in performance, but in the reliability and key process technology still need to be improved.","In addition, the development of communication technology also provides a new opportunity for the progress of electricity meter technology.","The application of the new technology provides a more convenient and efficient way for the data transmission and remote management of electricity meters.","Domestic multi-functional electricity meters have made remarkable achievements in technology evolution and application and expansion, but they still face some challenges and opportunities.","In the future, with the continuous development of the power market and the promotion of smart grid construction, domestic multi-functional electricity meters need to continue to strengthen technological innovation and product research and development, improve the reliability and competitiveness of products, in order to meet higher application needs and market requirements."],"url":"http://arxiv.org/abs/2402.05573v1","category":"physics.ins-det"}
{"created":"2024-02-08 11:04:11","title":"Succint Interaction-Aware Explanations","abstract":"SHAP is a popular approach to explain black-box models by revealing the importance of individual features. As it ignores feature interactions, SHAP explanations can be confusing up to misleading. NSHAP, on the other hand, reports the additive importance for all subsets of features. While this does include all interacting sets of features, it also leads to an exponentially sized, difficult to interpret explanation. In this paper, we propose to combine the best of these two worlds, by partitioning the features into parts that significantly interact, and use these parts to compose a succinct, interpretable, additive explanation. We derive a criterion by which to measure the representativeness of such a partition for a models behavior, traded off against the complexity of the resulting explanation. To efficiently find the best partition out of super-exponentially many, we show how to prune sub-optimal solutions using a statistical test, which not only improves runtime but also helps to detect spurious interactions. Experiments on synthetic and real world data show that our explanations are both more accurate resp. more easily interpretable than those of SHAP and NSHAP.","sentences":["SHAP is a popular approach to explain black-box models by revealing the importance of individual features.","As it ignores feature interactions, SHAP explanations can be confusing up to misleading.","NSHAP, on the other hand, reports the additive importance for all subsets of features.","While this does include all interacting sets of features, it also leads to an exponentially sized, difficult to interpret explanation.","In this paper, we propose to combine the best of these two worlds, by partitioning the features into parts that significantly interact, and use these parts to compose a succinct, interpretable, additive explanation.","We derive a criterion by which to measure the representativeness of such a partition for a models behavior, traded off against the complexity of the resulting explanation.","To efficiently find the best partition out of super-exponentially many, we show how to prune sub-optimal solutions using a statistical test, which not only improves runtime but also helps to detect spurious interactions.","Experiments on synthetic and real world data show that our explanations are both more accurate resp.","more easily interpretable than those of SHAP and NSHAP."],"url":"http://arxiv.org/abs/2402.05566v1","category":"cs.LG"}
{"created":"2024-02-08 11:02:26","title":"A Game-Theoretical Approach for Optimal Supervisory Control of Discrete Event Systems under Energy Constraints","abstract":"In this paper, we investigate the problem of optimal supervisory control for the discrete event systems under energy constraints. We consider that the execution of events consumes energy and the energy can be replenished at specific reload states. When the energy level drops below zero, the system will be crashed. To capture the above scenario, we introduce a new model, called consumption discrete event system (cDES). Our objective is to find the minimal initial energy value and synthesize an optimal supervisor ensuring that the energy will never be exhausted. To solve this problem, we propose a game-theoretical approach by converting the cDES as a consumption two-player graph game (cTPG) and reformulate the optimal supervisory control problem in game theory. In particular, we demonstrate that the converted game can be decomposed into independent reachability games related to reload vertices, which can be solved by a fixed point iterative algorithm proposed in this paper. Through iteratively removing unsafe reload vertices and solving reachability games for the remaining reload vertices, a solution can be found.","sentences":["In this paper, we investigate the problem of optimal supervisory control for the discrete event systems under energy constraints.","We consider that the execution of events consumes energy and the energy can be replenished at specific reload states.","When the energy level drops below zero, the system will be crashed.","To capture the above scenario, we introduce a new model, called consumption discrete event system (cDES).","Our objective is to find the minimal initial energy value and synthesize an optimal supervisor ensuring that the energy will never be exhausted.","To solve this problem, we propose a game-theoretical approach by converting the cDES as a consumption two-player graph game (cTPG) and reformulate the optimal supervisory control problem in game theory.","In particular, we demonstrate that the converted game can be decomposed into independent reachability games related to reload vertices, which can be solved by a fixed point iterative algorithm proposed in this paper.","Through iteratively removing unsafe reload vertices and solving reachability games for the remaining reload vertices, a solution can be found."],"url":"http://arxiv.org/abs/2402.05564v1","category":"eess.SY"}
{"created":"2024-02-08 10:59:40","title":"The dynamics of accretion flows near to the innermost stable circular orbit","abstract":"Accretion flows are fundamentally turbulent systems, yet are classically modelled with viscous theories only valid on length scales significantly greater than the typical size of turbulent eddies in the flow. We demonstrate that, while this will be a reasonable bulk description of the flow at large radii, this must break down as the flow approaches absorbing boundaries, such as the innermost stable circular orbit (ISCO) of a black hole disc. This is because in a turbulent flow large velocity fluctuations can carry a fluid element over the ISCO from a finite distance away, from which it will not return, a process without analogy in conventional models. This introduces a non-zero directional bias into the velocity fluctuations in the near-ISCO disc. By studying reduced random walk problems, we derive a number of implications of the presence of an absorbing boundary in an accretion context. In particular, we show that the average velocity with which a typical fluid element crosses the ISCO is much larger than is assumed in traditional theories. This enhanced velocity modifies the thermodynamic properties of black hole accretion flows on both sides of the ISCO. In particular, thermodynamic quantities for larger ISCO stresses no longer display pronounced cusps at the ISCO in this new formalism, a result with relevance for a number of observational probes of the intra-ISCO region. Finally, we demonstrate that these extended models reproduce the trans-ISCO behaviour observed in GRMHD simulations of thin discs.","sentences":["Accretion flows are fundamentally turbulent systems, yet are classically modelled with viscous theories only valid on length scales significantly greater than the typical size of turbulent eddies in the flow.","We demonstrate that, while this will be a reasonable bulk description of the flow at large radii, this must break down as the flow approaches absorbing boundaries, such as the innermost stable circular orbit (ISCO) of a black hole disc.","This is because in a turbulent flow large velocity fluctuations can carry a fluid element over the ISCO from a finite distance away, from which it will not return, a process without analogy in conventional models.","This introduces a non-zero directional bias into the velocity fluctuations in the near-ISCO disc.","By studying reduced random walk problems, we derive a number of implications of the presence of an absorbing boundary in an accretion context.","In particular, we show that the average velocity with which a typical fluid element crosses the ISCO is much larger than is assumed in traditional theories.","This enhanced velocity modifies the thermodynamic properties of black hole accretion flows on both sides of the ISCO.","In particular, thermodynamic quantities for larger ISCO stresses no longer display pronounced cusps at the ISCO in this new formalism, a result with relevance for a number of observational probes of the intra-ISCO region.","Finally, we demonstrate that these extended models reproduce the trans-ISCO behaviour observed in GRMHD simulations of thin discs."],"url":"http://arxiv.org/abs/2402.05561v1","category":"astro-ph.HE"}
{"created":"2024-02-08 10:53:00","title":"Automatizing Software Cognitive Complexity Reduction through Integer Linear Programming","abstract":"Reducing the cognitive complexity of a piece of code to a given threshold is not trivial. Recently, we modeled software cognitive complexity reduction as an optimization problem and we proposed an approach to assist developers on this task. This approach enumerates sequences of code extraction refactoring operations until a stopping criterion is met. As a result, it returns the minimal sequence of code extraction refactoring operations that is able to reduce the cognitive complexity of a code to the given threshold. However, exhaustive enumeration algorithms fail to scale with the code size. The number of refactoring plans can grow exponentially with the number of lines of code. In this paper, instead of enumerating sequences of code extraction refactoring operations, we model the cognitive complexity reduction as an Integer Linear Programming problem. This opens the door to the use of efficient solvers to find optimal solutions in large programs.","sentences":["Reducing the cognitive complexity of a piece of code to a given threshold is not trivial.","Recently, we modeled software cognitive complexity reduction as an optimization problem and we proposed an approach to assist developers on this task.","This approach enumerates sequences of code extraction refactoring operations until a stopping criterion is met.","As a result, it returns the minimal sequence of code extraction refactoring operations that is able to reduce the cognitive complexity of a code to the given threshold.","However, exhaustive enumeration algorithms fail to scale with the code size.","The number of refactoring plans can grow exponentially with the number of lines of code.","In this paper, instead of enumerating sequences of code extraction refactoring operations, we model the cognitive complexity reduction as an Integer Linear Programming problem.","This opens the door to the use of efficient solvers to find optimal solutions in large programs."],"url":"http://arxiv.org/abs/2402.05559v1","category":"cs.SE"}
{"created":"2024-02-08 10:47:49","title":"Slice regular holomorphic Cliffordian functions of order $k$","abstract":"Holomorphic Cliffordian functions of order $k$ are functions in the kernel of the differential operator $\\overline{\\partial}\\Delta^k$. When $\\overline{\\partial}\\Delta^k$ is applied to functions defined on the paravector space of some Clifford Algebra $\\mathbb{R}_m$ with an odd number of imaginary units, the Fueter-Sce construction establish a critical index $k=\\frac{m-1}{2}$ (sometimes called Fueter-Sce exponent) for which the class of slice regular functions is contained in the one of holomorphic Cliffordian functions of order $\\frac{m-1}{2}$. In this paper we analyze the case $k<\\frac{m-1}{2}$ and we find that the polynomials of degree at most $2k$ are the only slice regular holomorphic Cliffordian functions of order $k$.","sentences":["Holomorphic Cliffordian functions of order $k$ are functions in the kernel of the differential operator $\\overline{\\partial}\\Delta^k$. When $\\overline{\\partial}\\Delta^k$ is applied to functions defined on the paravector space of some Clifford Algebra $\\mathbb{R}_m$ with an odd number of imaginary units, the Fueter-Sce construction establish a critical index $k=\\frac{m-1}{2}$ (sometimes called Fueter-Sce exponent) for which the class of slice regular functions is contained in the one of holomorphic Cliffordian functions of order $\\frac{m-1}{2}$.","In this paper we analyze the case $k<\\frac{m-1}{2}$ and we find that the polynomials of degree at most $2k$ are the only slice regular holomorphic Cliffordian functions of order $k$."],"url":"http://arxiv.org/abs/2402.05556v1","category":"math.CV"}
{"created":"2024-02-08 10:43:47","title":"Non-thermal emission in M31 and M33","abstract":"Spiral galaxies M31 and M33 are Fermi/LAT-detected gamma-ray sources. We model the broadband non-thermal (NT) emission of the central region of M31 (R < 5.5 kpc) and of the disk of M33 (R ~ 9 kpc). For either galaxy, we self-consistently model the broadband SED of the diffuse NT emission based on published radio and gamma-ray data. All relevant radiative processes involving relativistic and thermal electrons (synchrotron, Compton scattering, bremsstrahlung, and free-free emission and absorption), along with relativistic protons (neutral-pion decay following interaction with thermal protons), are considered, using exact emissivity formulae. We also use the Fermi/LAT validated gamma-ray emissivities for pulsars. We find that, in both sources, the radio emission is composed of primary and secondary electron synchrotron and thermal bremsstrahlung. The M33 gamma-ray emission appears to be mainly hadronic, similar to the Magellanic Clouds (Persic & Rephaeli 2022). In contrast, we find suggestions of a more complex situation in the central region of M31, whose emission could be a mix of pulsar emission and hadronic emission, with the latter possibly originating from both the disk and the vicinity of the nuclear black hole. The alternative modelling of the spectra of M31 and M33 is motivated by the different hydrogen distribution in the two galaxies: the hydrogen deficiency in the central region of M31 partially unveils emissions from the nuclear BH and the pulsar population in the bulge and inner disk. If this were to be the case in M33 as well, these emissions would be outshined by diffuse pionic emission originating within the flat central-peak gas distribution in M33.","sentences":["Spiral galaxies M31 and M33 are Fermi/LAT-detected gamma-ray sources.","We model the broadband non-thermal (NT) emission of the central region of M31 (R < 5.5 kpc) and of the disk of M33 (R ~ 9 kpc).","For either galaxy, we self-consistently model the broadband SED of the diffuse NT emission based on published radio and gamma-ray data.","All relevant radiative processes involving relativistic and thermal electrons (synchrotron, Compton scattering, bremsstrahlung, and free-free emission and absorption), along with relativistic protons (neutral-pion decay following interaction with thermal protons), are considered, using exact emissivity formulae.","We also use the Fermi/LAT validated gamma-ray emissivities for pulsars.","We find that, in both sources, the radio emission is composed of primary and secondary electron synchrotron and thermal bremsstrahlung.","The M33 gamma-ray emission appears to be mainly hadronic, similar to the Magellanic Clouds (Persic & Rephaeli 2022).","In contrast, we find suggestions of a more complex situation in the central region of M31, whose emission could be a mix of pulsar emission and hadronic emission, with the latter possibly originating from both the disk and the vicinity of the nuclear black hole.","The alternative modelling of the spectra of M31 and M33 is motivated by the different hydrogen distribution in the two galaxies: the hydrogen deficiency in the central region of M31 partially unveils emissions from the nuclear BH and the pulsar population in the bulge and inner disk.","If this were to be the case in M33 as well, these emissions would be outshined by diffuse pionic emission originating within the flat central-peak gas distribution in M33."],"url":"http://arxiv.org/abs/2402.05553v1","category":"astro-ph.HE"}
{"created":"2024-02-08 10:42:47","title":"Learning quantum Hamiltonians at any temperature in polynomial time with Chebyshev and bit complexity","abstract":"We consider the problem of learning local quantum Hamiltonians given copies of their Gibbs state at a known inverse temperature, following Haah et al. [2108.04842] and Bakshi et al. [arXiv:2310.02243]. Our main technical contribution is a new flat polynomial approximation of the exponential function based on the Chebyshev expansion, which enables the formulation of learning quantum Hamiltonians as a polynomial optimization problem. This, in turn, can benefit from the use of moment/SOS relaxations, whose polynomial bit complexity requires careful analysis [O'Donnell, ITCS 2017]. Finally, we show that learning a $k$-local Hamiltonian, whose dual interaction graph is of bounded degree, runs in polynomial time under mild assumptions.","sentences":["We consider the problem of learning local quantum Hamiltonians given copies of their Gibbs state at a known inverse temperature, following Haah et al.","[2108.04842] and Bakshi et al.","[arXiv:2310.02243].","Our main technical contribution is a new flat polynomial approximation of the exponential function based on the Chebyshev expansion, which enables the formulation of learning quantum Hamiltonians as a polynomial optimization problem.","This, in turn, can benefit from the use of moment/SOS relaxations, whose polynomial bit complexity requires careful analysis [O'Donnell, ITCS 2017].","Finally, we show that learning a $k$-local Hamiltonian, whose dual interaction graph is of bounded degree, runs in polynomial time under mild assumptions."],"url":"http://arxiv.org/abs/2402.05552v1","category":"quant-ph"}
{"created":"2024-02-08 10:38:56","title":"Towards a Thermodynamical Deep-Learning-Vision-Based Flexible Robotic Cell for Circular Healthcare","abstract":"The dependence on finite reserves of raw materials and the production of waste are two unsolved problems of the traditional linear economy. Healthcare, as a major sector of any nation, is currently facing them. Hence, in this paper, we report theoretical and practical advances of robotic reprocessing of small medical devices. Specifically, on the theory, we combine compartmental dynamical thermodynamics with the mechanics of robots to integrate robotics into a system-level perspective, and then, propose graph-based circularity indicators by leveraging our thermodynamic framework. Our thermodynamic framework is also a step forward in defining the theoretical foundations of circular material flow designs as it improves material flow analysis (MFA) by adding dynamical energy balances to the usual mass balances. On the practice, we report on the on-going design of a flexible robotic cell enabled by deep-learning vision for resources mapping and quantification, disassembly, and waste sorting of small medical devices.","sentences":["The dependence on finite reserves of raw materials and the production of waste are two unsolved problems of the traditional linear economy.","Healthcare, as a major sector of any nation, is currently facing them.","Hence, in this paper, we report theoretical and practical advances of robotic reprocessing of small medical devices.","Specifically, on the theory, we combine compartmental dynamical thermodynamics with the mechanics of robots to integrate robotics into a system-level perspective, and then, propose graph-based circularity indicators by leveraging our thermodynamic framework.","Our thermodynamic framework is also a step forward in defining the theoretical foundations of circular material flow designs as it improves material flow analysis (MFA) by adding dynamical energy balances to the usual mass balances.","On the practice, we report on the on-going design of a flexible robotic cell enabled by deep-learning vision for resources mapping and quantification, disassembly, and waste sorting of small medical devices."],"url":"http://arxiv.org/abs/2402.05551v1","category":"cs.RO"}
{"created":"2024-02-08 10:33:07","title":"Efficient Expression Neutrality Estimation with Application to Face Recognition Utility Prediction","abstract":"The recognition performance of biometric systems strongly depends on the quality of the compared biometric samples. Motivated by the goal of establishing a common understanding of face image quality and enabling system interoperability, the committee draft of ISO/IEC 29794-5 introduces expression neutrality as one of many component quality elements affecting recognition performance. In this study, we train classifiers to assess facial expression neutrality using seven datasets. We conduct extensive performance benchmarking to evaluate their classification and face recognition utility prediction abilities. Our experiments reveal significant differences in how each classifier distinguishes \"neutral\" from \"non-neutral\" expressions. While Random Forests and AdaBoost classifiers are most suitable for distinguishing neutral from non-neutral facial expressions with high accuracy, they underperform compared to Support Vector Machines in predicting face recognition utility.","sentences":["The recognition performance of biometric systems strongly depends on the quality of the compared biometric samples.","Motivated by the goal of establishing a common understanding of face image quality and enabling system interoperability, the committee draft of ISO/IEC 29794-5 introduces expression neutrality as one of many component quality elements affecting recognition performance.","In this study, we train classifiers to assess facial expression neutrality using seven datasets.","We conduct extensive performance benchmarking to evaluate their classification and face recognition utility prediction abilities.","Our experiments reveal significant differences in how each classifier distinguishes \"neutral\" from \"non-neutral\" expressions.","While Random Forests and AdaBoost classifiers are most suitable for distinguishing neutral from non-neutral facial expressions with high accuracy, they underperform compared to Support Vector Machines in predicting face recognition utility."],"url":"http://arxiv.org/abs/2402.05548v1","category":"cs.CV"}
{"created":"2024-02-08 10:21:51","title":"Tightly Coupled Range Inertial Localization on a 3D Prior Map Based on Sliding Window Factor Graph Optimization","abstract":"This paper presents a range inertial localization algorithm for a 3D prior map. The proposed algorithm tightly couples scan-to-scan and scan-to-map point cloud registration factors along with IMU factors on a sliding window factor graph. The tight coupling of the scan-to-scan and scan-to-map registration factors enables a smooth fusion of sensor ego-motion estimation and map-based trajectory correction that results in robust tracking of the sensor pose under severe point cloud degeneration and defective regions in a map. We also propose an initial sensor state estimation algorithm that robustly estimates the gravity direction and IMU state and helps perform global localization in 3- or 4-DoF for system initialization without prior position information. Experimental results show that the proposed method outperforms existing state-of-the-art methods in extremely severe situations where the point cloud data becomes degenerate, there are momentary sensor interruptions, or the sensor moves along the map boundary or into unmapped regions.","sentences":["This paper presents a range inertial localization algorithm for a 3D prior map.","The proposed algorithm tightly couples scan-to-scan and scan-to-map point cloud registration factors along with IMU factors on a sliding window factor graph.","The tight coupling of the scan-to-scan and scan-to-map registration factors enables a smooth fusion of sensor ego-motion estimation and map-based trajectory correction that results in robust tracking of the sensor pose under severe point cloud degeneration and defective regions in a map.","We also propose an initial sensor state estimation algorithm that robustly estimates the gravity direction and IMU state and helps perform global localization in 3- or 4-DoF for system initialization without prior position information.","Experimental results show that the proposed method outperforms existing state-of-the-art methods in extremely severe situations where the point cloud data becomes degenerate, there are momentary sensor interruptions, or the sensor moves along the map boundary or into unmapped regions."],"url":"http://arxiv.org/abs/2402.05540v1","category":"cs.RO"}
{"created":"2024-02-08 10:07:30","title":"Asynchronous Diffusion Learning with Agent Subsampling and Local Updates","abstract":"In this work, we examine a network of agents operating asynchronously, aiming to discover an ideal global model that suits individual local datasets. Our assumption is that each agent independently chooses when to participate throughout the algorithm and the specific subset of its neighbourhood with which it will cooperate at any given moment. When an agent chooses to take part, it undergoes multiple local updates before conveying its outcomes to the sub-sampled neighbourhood. Under this setup, we prove that the resulting asynchronous diffusion strategy is stable in the mean-square error sense and provide performance guarantees specifically for the federated learning setting. We illustrate the findings with numerical simulations.","sentences":["In this work, we examine a network of agents operating asynchronously, aiming to discover an ideal global model that suits individual local datasets.","Our assumption is that each agent independently chooses when to participate throughout the algorithm and the specific subset of its neighbourhood with which it will cooperate at any given moment.","When an agent chooses to take part, it undergoes multiple local updates before conveying its outcomes to the sub-sampled neighbourhood.","Under this setup, we prove that the resulting asynchronous diffusion strategy is stable in the mean-square error sense and provide performance guarantees specifically for the federated learning setting.","We illustrate the findings with numerical simulations."],"url":"http://arxiv.org/abs/2402.05529v1","category":"cs.LG"}
{"created":"2024-02-08 09:51:37","title":"Magnetic field effects on the Kitaev model coupled to environment","abstract":"Open quantum systems display unusual phenomena not seen in closed systems, such as new topological phases and unconventional phase transitions. An interesting example was studied for a quantum spin liquid in the Kitaev model [K. Yang, S. C. Morampudi, and E. J. Bergholtz, Phys. Rev. Lett. ${\\bf 126}$, 077201 (2021)]; an effective non-Hermitian Kitaev model, which incorporates dissipation effects, was shown to give rise to a gapless spin liquid state with exceptional points in the Majorana dispersions. Given that an external magnetic field induces a gapped Majorana topological state in the Hermitian case, the exceptional points may bring about intriguing quantum phenomena under a magnetic field. Here we investigate the non-Hermitian Kitaev model perturbed by the magnetic field. We show that the exceptional points remain gapless up to a finite critical magnetic field, in stark contrast to the Hermitian case where an infinitesimal field opens a gap. The gapless state is stable over a wide range of the magnetic field for some particular parameter sets, and in special cases, undergoes topological transitions to another gapless state with different winding number around the exceptional points without opening a gap. In addition, in the system with edges, we find that the non-Hermitian skin effect is induced by the magnetic field, even for the parameters where the skin effect is absent at zero field. The chirality of edge states is switched through the exceptional points, similarly to the surface Fermi arcs connected by the Weyl points in three-dimensional Weyl semimetals. Our results provide a new possible route to stabilize topological gapless quantum spin liquids under the magnetic field in the presence of dissipation.","sentences":["Open quantum systems display unusual phenomena not seen in closed systems, such as new topological phases and unconventional phase transitions.","An interesting example was studied for a quantum spin liquid in the Kitaev model [K. Yang, S. C. Morampudi, and E. J. Bergholtz, Phys.","Rev. Lett.","${\\bf 126}$, 077201 (2021)]; an effective non-Hermitian Kitaev model, which incorporates dissipation effects, was shown to give rise to a gapless spin liquid state with exceptional points in the Majorana dispersions.","Given that an external magnetic field induces a gapped Majorana topological state in the Hermitian case, the exceptional points may bring about intriguing quantum phenomena under a magnetic field.","Here we investigate the non-Hermitian Kitaev model perturbed by the magnetic field.","We show that the exceptional points remain gapless up to a finite critical magnetic field, in stark contrast to the Hermitian case where an infinitesimal field opens a gap.","The gapless state is stable over a wide range of the magnetic field for some particular parameter sets, and in special cases, undergoes topological transitions to another gapless state with different winding number around the exceptional points without opening a gap.","In addition, in the system with edges, we find that the non-Hermitian skin effect is induced by the magnetic field, even for the parameters where the skin effect is absent at zero field.","The chirality of edge states is switched through the exceptional points, similarly to the surface Fermi arcs connected by the Weyl points in three-dimensional Weyl semimetals.","Our results provide a new possible route to stabilize topological gapless quantum spin liquids under the magnetic field in the presence of dissipation."],"url":"http://arxiv.org/abs/2402.05516v1","category":"cond-mat.str-el"}
{"created":"2024-02-08 09:13:03","title":"The topology of spaces of holomorphic maps to projective space","abstract":"We show that the spaces of holomorphic and continuous maps from a smooth complex projective variety to a projective space have the same homology in a range depending on the degree of the maps.","sentences":["We show that the spaces of holomorphic and continuous maps from a smooth complex projective variety to a projective space have the same homology in a range depending on the degree of the maps."],"url":"http://arxiv.org/abs/2402.05500v1","category":"math.AT"}
{"created":"2024-02-08 09:08:57","title":"Thinking fast and slow and science education","abstract":"In recent years there has been growing evidence that even after teaching designed to address the learning difficulties dictated by literature, many physics learners fail to create the proper reasoning chains that connect the fundamental principles and lead to reasoned predictions. Even though students have the required knowledge and skills, they are often based on a variety of intuitive reasoning that leads them to wrong conclusions. This paper studies students' reasoning on science problems through heuristic - analytical thought processes (System 1 - System 2). System 1 operates automatically and quickly with little or no effort and no sense of voluntary control, while System 2 focuses on the demanding mental activities that require it and is slow based on rules.","sentences":["In recent years there has been growing evidence that even after teaching designed to address the learning difficulties dictated by literature, many physics learners fail to create the proper reasoning chains that connect the fundamental principles and lead to reasoned predictions.","Even though students have the required knowledge and skills, they are often based on a variety of intuitive reasoning that leads them to wrong conclusions.","This paper studies students' reasoning on science problems through heuristic - analytical thought processes (System 1 - System 2).","System 1 operates automatically and quickly with little or no effort and no sense of voluntary control, while System 2 focuses on the demanding mental activities that require it and is slow based on rules."],"url":"http://arxiv.org/abs/2402.05497v1","category":"physics.ed-ph"}
{"created":"2024-02-08 08:49:01","title":"Investigating and Controlling the Libration and Rotation Dynamics of Nanoparticles in an Optomechanical System","abstract":"In optomechanical systems, the libration and rotation of nanoparticles offer profound insights for ultrasensitive torque measurement and macroscopic quantum superpositions. Achievements include transitioning libration to rotation up to 6 GHz and cooling libration to millikelvin temperatures. It is undoubted that the libration and rotation are respectively driven by restoring and constant optical torques. The transition mechanisms between these two states, however, demand further exploration. In this perspective, it is demonstrated in this manuscript that monitoring lateral-scattered light allows real-time observation of libration/rotation transitions and associated hysteresis as ellipticities of trapping laser fields vary. By calculating optical torques and solving the Langevin equation, transitions are linked to the balance between anisotropic-polarization-induced sinusoidal optical torques and constant ones, with absorption identified as the main contributor to constant torques. These findings enable direct weak torque sensing and precise nanoparticle control in rotational degrees, paving the way for studying quantum effects like nonadiabatic phase shifts and macroscopic quantum superpositions, thereby enriching quantum optomechanics research.","sentences":["In optomechanical systems, the libration and rotation of nanoparticles offer profound insights for ultrasensitive torque measurement and macroscopic quantum superpositions.","Achievements include transitioning libration to rotation up to 6 GHz and cooling libration to millikelvin temperatures.","It is undoubted that the libration and rotation are respectively driven by restoring and constant optical torques.","The transition mechanisms between these two states, however, demand further exploration.","In this perspective, it is demonstrated in this manuscript that monitoring lateral-scattered light allows real-time observation of libration/rotation transitions and associated hysteresis as ellipticities of trapping laser fields vary.","By calculating optical torques and solving the Langevin equation, transitions are linked to the balance between anisotropic-polarization-induced sinusoidal optical torques and constant ones, with absorption identified as the main contributor to constant torques.","These findings enable direct weak torque sensing and precise nanoparticle control in rotational degrees, paving the way for studying quantum effects like nonadiabatic phase shifts and macroscopic quantum superpositions, thereby enriching quantum optomechanics research."],"url":"http://arxiv.org/abs/2402.05490v1","category":"physics.optics"}
{"created":"2024-02-08 08:25:01","title":"Reconsidering the performance of DEVS modeling and simulation environments using the DEVStone benchmark","abstract":"The Discrete Event System Specification formalism (DEVS), which supports hierarchical and modular model composition, has been widely used to understand, analyze and develop a variety of systems. DEVS has been implemented in various languages and platforms over the years. The DEVStone benchmark was conceived to generate a set of models with varied structure and behavior, and to automate the evaluation of the performance of DEVS-based simulators. However, DEVStone is still in a preliminar phase and more model analysis is required. In this paper, we revisit DEVStone introducing new equations to compute the number of events triggered. We also introduce a new benchmark, called HOmem, designed as an alternative version of HOmod, with similar CPU and memory requirements, but with an easier implementation and analytically more manageable. Finally, we compare both the performance and memory footprint of five different DEVS simulators in two different hardware platforms.","sentences":["The Discrete Event System Specification formalism (DEVS), which supports hierarchical and modular model composition, has been widely used to understand, analyze and develop a variety of systems.","DEVS has been implemented in various languages and platforms over the years.","The DEVStone benchmark was conceived to generate a set of models with varied structure and behavior, and to automate the evaluation of the performance of DEVS-based simulators.","However, DEVStone is still in a preliminar phase and more model analysis is required.","In this paper, we revisit DEVStone introducing new equations to compute the number of events triggered.","We also introduce a new benchmark, called HOmem, designed as an alternative version of HOmod, with similar CPU and memory requirements, but with an easier implementation and analytically more manageable.","Finally, we compare both the performance and memory footprint of five different DEVS simulators in two different hardware platforms."],"url":"http://arxiv.org/abs/2402.05483v1","category":"cs.PF"}
{"created":"2024-02-08 08:12:27","title":"A comparison of the effects of different methodologies on the statistics learning profiles of prospective primary education teachers from a gender perspective","abstract":"Over the last decades,it has been shown that teaching and learning statistics is complex, regardless of the teaching methodology. This research presents the different learning profiles identified in a group of future Primary Education (PE) teachers during the study of the Statistics blockdepending on the methodology used and gender, where the sample consists of 132 students in the third year of the PE undergraduate degree in theUniversity of the Basque Country(Universidad del Pa\\'is Vasco/Euskal Herriko Unibertsitatea, UPV/EHU). To determine the profiles, a cluster analysis technique has been used, where the main variables to determine them are, on the one hand, their statistical competence development and, on the other hand, the evolutionof their attitude towards statistics. In order to better understand the nature of the profiles obtained, the type of teaching methodology used to work on the Statistics block has been taken into account. This comparison is based on the fact that the sample is divided into two groups: one has worked with a Project Based Learning (PBL) methodology,while the other has worked with a methodology in which theoretical explanations and typically decontextualized exercises predominate. Among the results obtained,three differentiated profiles areobserved, highlighting the proportion of students with an advantageous profile in the group where PBL is included.With regard to gender, the results show that women's attitudes towardstatistics evolvedmore positively than men's after the sessions devoted to statistics in the PBL group.","sentences":["Over the last decades,it has been shown that teaching and learning statistics is complex, regardless of the teaching methodology.","This research presents the different learning profiles identified in a group of future Primary Education (PE) teachers during the study of the Statistics blockdepending on the methodology used and gender, where the sample consists of 132 students in the third year of the PE undergraduate degree in theUniversity of the Basque Country(Universidad del Pa\\'is Vasco/Euskal Herriko Unibertsitatea, UPV/EHU).","To determine the profiles, a cluster analysis technique has been used, where the main variables to determine them are, on the one hand, their statistical competence development and, on the other hand, the evolutionof their attitude towards statistics.","In order to better understand the nature of the profiles obtained, the type of teaching methodology used to work on the Statistics block has been taken into account.","This comparison is based on the fact that the sample is divided into two groups: one has worked with a Project Based Learning (PBL) methodology,while the other has worked with a methodology in which theoretical explanations and typically decontextualized exercises predominate.","Among the results obtained,three differentiated profiles areobserved, highlighting the proportion of students with an advantageous profile in the group where PBL is included.","With regard to gender, the results show that women's attitudes towardstatistics evolvedmore positively than men's after the sessions devoted to statistics in the PBL group."],"url":"http://arxiv.org/abs/2402.05479v1","category":"stat.ME"}
{"created":"2024-02-08 08:08:23","title":"Multi-Timescale Ensemble Q-learning for Markov Decision Process Policy Optimization","abstract":"Reinforcement learning (RL) is a classical tool to solve network control or policy optimization problems in unknown environments. The original Q-learning suffers from performance and complexity challenges across very large networks. Herein, a novel model-free ensemble reinforcement learning algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit Markov decision process (MDP) models. Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity. The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided. Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy error with up to 50% less runtime complexity than the state-of-the-art Q-learning algorithms. Numerical results validate assumptions made in the theoretical analysis.","sentences":["Reinforcement learning (RL) is a classical tool to solve network control or policy optimization problems in unknown environments.","The original Q-learning suffers from performance and complexity challenges across very large networks.","Herein, a novel model-free ensemble reinforcement learning algorithm which adapts the classical Q-learning is proposed to handle these challenges for networks which admit Markov decision process (MDP) models.","Multiple Q-learning algorithms are run on multiple, distinct, synthetically created and structurally related Markovian environments in parallel; the outputs are fused using an adaptive weighting mechanism based on the Jensen-Shannon divergence (JSD) to obtain an approximately optimal policy with low complexity.","The theoretical justification of the algorithm, including the convergence of key statistics and Q-functions are provided.","Numerical results across several network models show that the proposed algorithm can achieve up to 55% less average policy error with up to 50% less runtime complexity than the state-of-the-art Q-learning algorithms.","Numerical results validate assumptions made in the theoretical analysis."],"url":"http://arxiv.org/abs/2402.05476v1","category":"cs.LG"}
{"created":"2024-02-08 08:05:02","title":"Resources of the Quantum World","abstract":"This book delves into the burgeoning field of quantum resource theories, a novel and vibrant area of research within quantum information science that seeks to unify diverse quantum phenomena under a single framework. By recognizing various attributes of physical systems as \"resources,\" this approach offers a fresh perspective on quantum phenomena, transforming our understanding and application of concepts such as quantum entanglement, coherence, and more. With a focus on the pedagogical, the book aims to equip readers with the advanced mathematical tools and physical principles needed to navigate and contribute to this rapidly evolving field. It covers a wide range of topics, from the foundational aspects of quantum mechanics and quantum information to detailed explorations of specific resource theories, including entanglement, asymmetry, and thermodynamics. Through rigorous mathematical exposition and a unique axiomatic approach, the book provides deep insights into the operational and conceptual frameworks that underpin quantum resource theories, making it an invaluable resource for graduate students, early-career researchers, and anyone interested in the cutting-edge developments in quantum information science.","sentences":["This book delves into the burgeoning field of quantum resource theories, a novel and vibrant area of research within quantum information science that seeks to unify diverse quantum phenomena under a single framework.","By recognizing various attributes of physical systems as \"resources,\" this approach offers a fresh perspective on quantum phenomena, transforming our understanding and application of concepts such as quantum entanglement, coherence, and more.","With a focus on the pedagogical, the book aims to equip readers with the advanced mathematical tools and physical principles needed to navigate and contribute to this rapidly evolving field.","It covers a wide range of topics, from the foundational aspects of quantum mechanics and quantum information to detailed explorations of specific resource theories, including entanglement, asymmetry, and thermodynamics.","Through rigorous mathematical exposition and a unique axiomatic approach, the book provides deep insights into the operational and conceptual frameworks that underpin quantum resource theories, making it an invaluable resource for graduate students, early-career researchers, and anyone interested in the cutting-edge developments in quantum information science."],"url":"http://arxiv.org/abs/2402.05474v1","category":"quant-ph"}
{"created":"2024-02-08 08:01:49","title":"Impact of Ejecta Temperature and Mass on the Strength of Heavy Element Signature in Kilonova","abstract":"Kilonova, the electromagnetic emission produced by compact binary mergers, is formed through a delicate interplay of physical processes, involving r-process nucleosynthesis and interactions between heavy elements and photons through radiative transfer. This complexity makes it difficult to achieve a comprehensive understanding of the kilonova spectrum. In this study, we aim to enhance our understanding and establish connections between physical parameters and observables through the radiative-transfer simulation. Specifically, we investigate how the ejecta temperature and element mass influence the resulting kilonova spectrum. For each species, the strength of its line features in the kilonova spectrum depends on these parameters, leading to the formation of a distinct region in the parameter space, named as the Prominent Signature Region (PSR), where the line signature of that species is notably evident in the kilonova spectrum. We explore the origin and applications of PSR. Among explored r-process elements (31$\\leq$Z$\\leq$92), we find that four species -- Sr\\textsubscript{II}, Y\\textsubscript{II}, Ba\\textsubscript{II}, and Ce\\textsubscript{II} -- exhibit large and strong PSR, suggesting their significant contributions to the kilonova spectrum in specific wavelengths. In addition, we discuss potential challenges and future perspectives in observable heavy elements and their masses in the context of PSR.","sentences":["Kilonova, the electromagnetic emission produced by compact binary mergers, is formed through a delicate interplay of physical processes, involving r-process nucleosynthesis and interactions between heavy elements and photons through radiative transfer.","This complexity makes it difficult to achieve a comprehensive understanding of the kilonova spectrum.","In this study, we aim to enhance our understanding and establish connections between physical parameters and observables through the radiative-transfer simulation.","Specifically, we investigate how the ejecta temperature and element mass influence the resulting kilonova spectrum.","For each species, the strength of its line features in the kilonova spectrum depends on these parameters, leading to the formation of a distinct region in the parameter space, named as the Prominent Signature Region (PSR), where the line signature of that species is notably evident in the kilonova spectrum.","We explore the origin and applications of PSR.","Among explored r-process elements (31$\\leq$Z$\\leq$92), we find that four species -- Sr\\textsubscript{II}, Y\\textsubscript{II}, Ba\\textsubscript{II}, and Ce\\textsubscript{II} -- exhibit large and strong PSR, suggesting their significant contributions to the kilonova spectrum in specific wavelengths.","In addition, we discuss potential challenges and future perspectives in observable heavy elements and their masses in the context of PSR."],"url":"http://arxiv.org/abs/2402.05471v1","category":"astro-ph.HE"}
{"created":"2024-02-08 07:08:55","title":"Low-degree phase transitions for detecting a planted clique in sublinear time","abstract":"We consider the problem of detecting a planted clique of size $k$ in a random graph on $n$ vertices. When the size of the clique exceeds $\\Theta(\\sqrt{n})$, polynomial-time algorithms for detection proliferate. We study faster -- namely, sublinear time -- algorithms in the high-signal regime when $k = \\Theta(n^{1/2 + \\delta})$, for some $\\delta > 0$. To this end, we consider algorithms that non-adaptively query a subset $M$ of entries of the adjacency matrix and then compute a low-degree polynomial function of the revealed entries. We prove a computational phase transition for this class of non-adaptive low-degree algorithms: under the scaling $\\lvert M \\rvert = \\Theta(n^{\\gamma})$, the clique can be detected when $\\gamma > 3(1/2 - \\delta)$ but not when $\\gamma < 3(1/2 - \\delta)$. As a result, the best known runtime for detecting a planted clique, $\\widetilde{O}(n^{3(1/2-\\delta)})$, cannot be improved without looking beyond the non-adaptive low-degree class.   Our proof of the lower bound -- based on bounding the conditional low-degree likelihood ratio -- reveals further structure in non-adaptive detection of a planted clique. Using (a bound on) the conditional low-degree likelihood ratio as a potential function, we show that for every non-adaptive query pattern, there is a highly structured query pattern of the same size that is at least as effective.","sentences":["We consider the problem of detecting a planted clique of size $k$ in a random graph on $n$ vertices.","When the size of the clique exceeds $\\Theta(\\sqrt{n})$, polynomial-time algorithms for detection proliferate.","We study faster -- namely, sublinear time -- algorithms in the high-signal regime when $k = \\Theta(n^{1/2 + \\delta})$, for some $\\delta > 0$.","To this end, we consider algorithms that non-adaptively query a subset $M$ of entries of the adjacency matrix and then compute a low-degree polynomial function of the revealed entries.","We prove a computational phase transition for this class of non-adaptive low-degree algorithms: under the scaling $\\lvert M \\rvert = \\Theta(n^{\\gamma})$, the clique can be detected when $\\gamma > 3(1/2 - \\delta)$ but not when $\\gamma < 3(1/2 - \\delta)$. As a result, the best known runtime for detecting a planted clique, $\\widetilde{O}(n^{3(1/2-\\delta)})$, cannot be improved without looking beyond the non-adaptive low-degree class.   ","Our proof of the lower bound -- based on bounding the conditional low-degree likelihood ratio -- reveals further structure in non-adaptive detection of a planted clique.","Using (a bound on) the conditional low-degree likelihood ratio as a potential function, we show that for every non-adaptive query pattern, there is a highly structured query pattern of the same size that is at least as effective."],"url":"http://arxiv.org/abs/2402.05451v1","category":"cs.DS"}
{"created":"2024-02-08 07:07:22","title":"Causality and a possible interpretation of quantum mechanics","abstract":"From the ancient Einstein-Podolsky-Rosen paradox to the recent Sorkin-type impossible measurements problem, the contradictions between relativistic causality, quantum non-locality, and quantum measurement have persisted. Our work provides a framework based on quantum field theory to harmoniously integrate these three aspects. This framework consists of causality expressed by reduced density matrices and an interpretation of quantum mechanics that considers quantum mechanics to be complete. Specifically, we utilize reduced density matrices to characterize the local information of the quantum state and demonstrate that they cannot evolve superluminally. Unlike recent approaches focusing on causality, we do not introduce new operators or fields specifically to describe detectors; instead, everything (including detectors, environments, and humans) is composed of the same fundamental fields, leading to complex renormalization. It is precisely these renormalization that prompts us to question the validity of the derivation of quantum paradoxes and lead us to propose a very natural and relativistically compatible interpretation of quantum mechanics.","sentences":["From the ancient Einstein-Podolsky-Rosen paradox to the recent Sorkin-type impossible measurements problem, the contradictions between relativistic causality, quantum non-locality, and quantum measurement have persisted.","Our work provides a framework based on quantum field theory to harmoniously integrate these three aspects.","This framework consists of causality expressed by reduced density matrices and an interpretation of quantum mechanics that considers quantum mechanics to be complete.","Specifically, we utilize reduced density matrices to characterize the local information of the quantum state and demonstrate that they cannot evolve superluminally.","Unlike recent approaches focusing on causality, we do not introduce new operators or fields specifically to describe detectors; instead, everything (including detectors, environments, and humans) is composed of the same fundamental fields, leading to complex renormalization.","It is precisely these renormalization that prompts us to question the validity of the derivation of quantum paradoxes and lead us to propose a very natural and relativistically compatible interpretation of quantum mechanics."],"url":"http://arxiv.org/abs/2402.05450v1","category":"quant-ph"}
{"created":"2024-02-08 06:55:52","title":"Effective model and $s_\\pm$-wave superconductivity in trilayer nickelate La$_4$Ni$_3$O$_{10}$","abstract":"The recent discovery of bulk superconductivity in trilayer nickelate La$_4$Ni$_3$O$_{10}$ with the critical temperature $T_c$ near $30$K under high pressure is attracting a new wave of research interest, after the breakthrough of bilayer La$_3$Ni$_2$O$_7$ with $T_c$ near $80$K. The similarities and differences of electronic structure and superconducting mechanism in these two systems are urgent theoretical issues. In this Letter, we study the electronic band structure and construct a minimal trilayer tight-binding model for the high-pressure phase of La$_4$Ni$_3$O$_{10}$ in terms of the nickel $3d_{x^2-y^2}$ and $3d_{3z^2-r^2}$ orbitals, and study the superconducting mechanism due to local Coulomb interactions by the unbiased functional renormalization group. We find antiferromagnetic correlations between the outer layers instead of neighboring ones, apart from the inplane correlations. The effective interaction induces Cooper pairing with the $s_\\pm$-wave symmetry, which changes sign across the Fermi pockets. We find $T_c$ in La$_4$Ni$_3$O$_{10}$ is systematically lower than that in La$_3$Ni$_2$O$_7$.","sentences":["The recent discovery of bulk superconductivity in trilayer nickelate La$_4$Ni$_3$O$_{10}$ with the critical temperature $T_c$ near $30$K under high pressure is attracting a new wave of research interest, after the breakthrough of bilayer La$_3$Ni$_2$O$_7$ with $T_c$ near $80$K. The similarities and differences of electronic structure and superconducting mechanism in these two systems are urgent theoretical issues.","In this Letter, we study the electronic band structure and construct a minimal trilayer tight-binding model for the high-pressure phase of La$_4$Ni$_3$O$_{10}$ in terms of the nickel $3d_{x^2-y^2}$ and $3d_{3z^2-r^2}$ orbitals, and study the superconducting mechanism due to local Coulomb interactions by the unbiased functional renormalization group.","We find antiferromagnetic correlations between the outer layers instead of neighboring ones, apart from the inplane correlations.","The effective interaction induces Cooper pairing with the $s_\\pm$-wave symmetry, which changes sign across the Fermi pockets.","We find $T_c$ in La$_4$Ni$_3$O$_{10}$ is systematically lower than that in La$_3$Ni$_2$O$_7$."],"url":"http://arxiv.org/abs/2402.05447v1","category":"cond-mat.supr-con"}
{"created":"2024-02-08 06:45:03","title":"Scalable Wasserstein Gradient Flow for Generative Modeling through Unbalanced Optimal Transport","abstract":"Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space. WGF provides a promising approach for conducting optimization over the probability distributions. Numerically approximating the continuous WGF requires the time discretization method. The most well-known method for this is the JKO scheme. In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step. However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models. In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO). Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport. Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.19 on CelebA-HQ-256, which are comparable to state-of-the-art image generative models.","sentences":["Wasserstein Gradient Flow (WGF) describes the gradient dynamics of probability density within the Wasserstein space.","WGF provides a promising approach for conducting optimization over the probability distributions.","Numerically approximating the continuous WGF requires the time discretization method.","The most well-known method for this is the JKO scheme.","In this regard, previous WGF models employ the JKO scheme and parametrize transport map for each JKO step.","However, this approach results in quadratic training complexity $O(K^2)$ with the number of JKO step $K$. This severely limits the scalability of WGF models.","In this paper, we introduce a scalable WGF-based generative model, called Semi-dual JKO (S-JKO).","Our model is based on the semi-dual form of the JKO step, derived from the equivalence between the JKO step and the Unbalanced Optimal Transport.","Our approach reduces the training complexity to $O(K)$. We demonstrate that our model significantly outperforms existing WGF-based generative models, achieving FID scores of 2.62 on CIFAR-10 and 6.19 on CelebA-HQ-256, which are comparable to state-of-the-art image generative models."],"url":"http://arxiv.org/abs/2402.05443v1","category":"cs.LG"}
{"created":"2024-02-08 06:13:26","title":"Bivariate Bernstein Fractal Interpolation and Numerical Integration on Triangular Domains","abstract":"The fundamental aim of this paper is to provide the approximation and numerical integration of a discrete set of data points with Bernstein fractal approach. Using Bernstein polynomials in the iterated function system, the paper initially proposes the numerical integration formula for the data set corresponding to univariate functions. The proposed formula of integration is shown to be convergent by examining the data sets of certain weierstrass functions.   The paper then extends the Bernstein fractal approximation and numerical integration technique to two dimensional interpolating regions. Bernstein polynomials defined over triangular domain has been used for the purpose. The triangular domain has been partitioned and the newly generated points are assigned colors in a particular manner to maintain the chromatic number as 3. Following the above mentioned construction and approximation of bivariate Bernstein fractal interpolation functions, the paper introduces the numerical double integration formula using the constructed functions. The convergence of the double integration formula towards the actual integral value of the data sets is displayed with the help of some examples including the benchmark functions. Both the newly introduced iterated function systems are verified for their hyperbolicity and the resultant fractal interpolation functions are shown to be continuous.","sentences":["The fundamental aim of this paper is to provide the approximation and numerical integration of a discrete set of data points with Bernstein fractal approach.","Using Bernstein polynomials in the iterated function system, the paper initially proposes the numerical integration formula for the data set corresponding to univariate functions.","The proposed formula of integration is shown to be convergent by examining the data sets of certain weierstrass functions.   ","The paper then extends the Bernstein fractal approximation and numerical integration technique to two dimensional interpolating regions.","Bernstein polynomials defined over triangular domain has been used for the purpose.","The triangular domain has been partitioned and the newly generated points are assigned colors in a particular manner to maintain the chromatic number as 3.","Following the above mentioned construction and approximation of bivariate Bernstein fractal interpolation functions, the paper introduces the numerical double integration formula using the constructed functions.","The convergence of the double integration formula towards the actual integral value of the data sets is displayed with the help of some examples including the benchmark functions.","Both the newly introduced iterated function systems are verified for their hyperbolicity and the resultant fractal interpolation functions are shown to be continuous."],"url":"http://arxiv.org/abs/2402.05434v1","category":"math.GM"}
{"created":"2024-02-08 06:11:16","title":"A note on the hyperbolicity of the non-wandering sets of real quadratic maps","abstract":"The goal of this paper is to discuss about the hyperbolicity of the non-wandering set $\\mathcal{NW}(f_c)$ of real quadratic function $f_c(x)=x^2+c$ when $c\\in (-\\infty, -2]$. Even though the results we present here are not new, it is not easier to find the proofs of them. We present two different ways to prove the hyperbolicity of $\\mathcal{NW}(f_c)$ for the``considerably difficult case'' of when $c$ is closer to $-2$.","sentences":["The goal of this paper is to discuss about the hyperbolicity of the non-wandering set $\\mathcal{NW}(f_c)$ of real quadratic function $f_c(x)=x^2+c$ when $c\\in (-\\infty, -2]$. Even though the results we present here are not new, it is not easier to find the proofs of them.","We present two different ways to prove the hyperbolicity of $\\mathcal{NW}(f_c)$ for the``considerably difficult case'' of when $c$ is closer to $-2$."],"url":"http://arxiv.org/abs/2402.05433v1","category":"math.DS"}
{"created":"2024-02-08 05:56:23","title":"Modulation of magnetization in BiFeO$_3$ using circularly polarized light","abstract":"BiFeO$_3$ is a multiferroic material featuring ferroelectricity and noncollinear antiferromagnetism, the latter manifested as a cycloid of spin density. Definitive and efficient control of the characteristic spin texture of BiFeO$_3$ is attractive for emerging quantum devices. In this regard, crystal-field $d\\rightarrow d$ excitations localized on Fe atomic sites in BiFeO$_3$ provide an avenue for manipulation of the spin texture as they induce a complex interplay among the spin, charge and lattice degrees of freedom. In this work, the ab initio GW-BSE method is used to characterize these excitations within the exciton picture. We find that the $d-d$ transitions appear as strongly bound chiral spin-flip excitons deep within the electronic band gap as a result of the intricate competition between the lattice potential, the antiferromagnetic ordering, the spin-orbit coupling, and the electron-hole interaction. Most crucially, these excitons are composed of electron-hole pairs with opposite spins constituting their $\\pm \\hbar$ angular momentum. Excitons with a specific angular momentum can be selectively excited using circularly polarized light, consequently modulating the local magnetic moment to give rise to transient ferrimagnetism.","sentences":["BiFeO$_3$ is a multiferroic material featuring ferroelectricity and noncollinear antiferromagnetism, the latter manifested as a cycloid of spin density.","Definitive and efficient control of the characteristic spin texture of BiFeO$_3$ is attractive for emerging quantum devices.","In this regard, crystal-field $d\\rightarrow d$ excitations localized on Fe atomic sites in BiFeO$_3$ provide an avenue for manipulation of the spin texture as they induce a complex interplay among the spin, charge and lattice degrees of freedom.","In this work, the ab initio GW-BSE method is used to characterize these excitations within the exciton picture.","We find that the $d-d$ transitions appear as strongly bound chiral spin-flip excitons deep within the electronic band gap as a result of the intricate competition between the lattice potential, the antiferromagnetic ordering, the spin-orbit coupling, and the electron-hole interaction.","Most crucially, these excitons are composed of electron-hole pairs with opposite spins constituting their $\\pm \\hbar$ angular momentum.","Excitons with a specific angular momentum can be selectively excited using circularly polarized light, consequently modulating the local magnetic moment to give rise to transient ferrimagnetism."],"url":"http://arxiv.org/abs/2402.05430v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-08 05:52:45","title":"A Sampling Theory Perspective on Activations for Implicit Neural Representations","abstract":"Implicit Neural Representations (INRs) have gained popularity for encoding signals as compact, differentiable entities. While commonly using techniques like Fourier positional encodings or non-traditional activation functions (e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content, their properties lack exploration within a unified theoretical framework. Addressing this gap, we conduct a comprehensive analysis of these activations from a sampling theory perspective. Our investigation reveals that sinc activations, previously unused in conjunction with INRs, are theoretically optimal for signal encoding. Additionally, we establish a connection between dynamical systems and INRs, leveraging sampling theory to bridge these two paradigms.","sentences":["Implicit Neural Representations (INRs) have gained popularity for encoding signals as compact, differentiable entities.","While commonly using techniques like Fourier positional encodings or non-traditional activation functions (e.g., Gaussian, sinusoid, or wavelets) to capture high-frequency content, their properties lack exploration within a unified theoretical framework.","Addressing this gap, we conduct a comprehensive analysis of these activations from a sampling theory perspective.","Our investigation reveals that sinc activations, previously unused in conjunction with INRs, are theoretically optimal for signal encoding.","Additionally, we establish a connection between dynamical systems and INRs, leveraging sampling theory to bridge these two paradigms."],"url":"http://arxiv.org/abs/2402.05427v1","category":"cs.LG"}
{"created":"2024-02-08 05:42:13","title":"Neural Circuit Diagrams: Robust Diagrams for the Communication, Implementation, and Analysis of Deep Learning Architectures","abstract":"Diagrams matter. Unfortunately, the deep learning community has no standard method for diagramming architectures. The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail. However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances. I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures. Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations. A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve. Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation.   In this work, I introduce neural circuit diagrams for an audience of machine learning researchers. After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity. This includes the transformer architecture, convolution (and its difficult-to-explain extensions), residual networks, the U-Net, and the vision transformer. I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code. Finally, I examine backpropagation using neural circuit diagrams. I show their utility in providing mathematical insight and analyzing algorithms' time and space complexities.","sentences":["Diagrams matter.","Unfortunately, the deep learning community has no standard method for diagramming architectures.","The current combination of linear algebra notation and ad-hoc diagrams fails to offer the necessary precision to understand architectures in all their detail.","However, this detail is critical for faithful implementation, mathematical analysis, further innovation, and ethical assurances.","I present neural circuit diagrams, a graphical language tailored to the needs of communicating deep learning architectures.","Neural circuit diagrams naturally keep track of the changing arrangement of data, precisely show how operations are broadcast over axes, and display the critical parallel behavior of linear operations.","A lingering issue with existing diagramming methods is the inability to simultaneously express the detail of axes and the free arrangement of data, which neural circuit diagrams solve.","Their compositional structure is analogous to code, creating a close correspondence between diagrams and implementation.   ","In this work, I introduce neural circuit diagrams for an audience of machine learning researchers.","After introducing neural circuit diagrams, I cover a host of architectures to show their utility and breed familiarity.","This includes the transformer architecture, convolution (and its difficult-to-explain extensions), residual networks, the U-Net, and the vision transformer.","I include a Jupyter notebook that provides evidence for the close correspondence between diagrams and code.","Finally, I examine backpropagation using neural circuit diagrams.","I show their utility in providing mathematical insight and analyzing algorithms' time and space complexities."],"url":"http://arxiv.org/abs/2402.05424v1","category":"cs.LG"}
{"created":"2024-02-08 05:39:11","title":"MTSA-SNN: A Multi-modal Time Series Analysis Model Based on Spiking Neural Network","abstract":"Time series analysis and modelling constitute a crucial research area. Traditional artificial neural networks struggle with complex, non-stationary time series data due to high computational complexity, limited ability to capture temporal information, and difficulty in handling event-driven data. To address these challenges, we propose a Multi-modal Time Series Analysis Model Based on Spiking Neural Network (MTSA-SNN). The Pulse Encoder unifies the encoding of temporal images and sequential information in a common pulse-based representation. The Joint Learning Module employs a joint learning function and weight allocation mechanism to fuse information from multi-modal pulse signals complementary. Additionally, we incorporate wavelet transform operations to enhance the model's ability to analyze and evaluate temporal information. Experimental results demonstrate that our method achieved superior performance on three complex time-series tasks. This work provides an effective event-driven approach to overcome the challenges associated with analyzing intricate temporal information. Access to the source code is available at https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN","sentences":["Time series analysis and modelling constitute a crucial research area.","Traditional artificial neural networks struggle with complex, non-stationary time series data due to high computational complexity, limited ability to capture temporal information, and difficulty in handling event-driven data.","To address these challenges, we propose a Multi-modal Time Series Analysis Model Based on Spiking Neural Network (MTSA-SNN).","The Pulse Encoder unifies the encoding of temporal images and sequential information in a common pulse-based representation.","The Joint Learning Module employs a joint learning function and weight allocation mechanism to fuse information from multi-modal pulse signals complementary.","Additionally, we incorporate wavelet transform operations to enhance the model's ability to analyze and evaluate temporal information.","Experimental results demonstrate that our method achieved superior performance on three complex time-series tasks.","This work provides an effective event-driven approach to overcome the challenges associated with analyzing intricate temporal information.","Access to the source code is available at https://github.com/Chenngzz/MTSA-SNN}{https://github.com/Chenngzz/MTSA-SNN"],"url":"http://arxiv.org/abs/2402.05423v1","category":"cs.CV"}
{"created":"2024-02-08 05:20:22","title":"Significant noise improvement in a Kinetic Inductance Phonon-Mediated detector by use of a wideband parametric amplifier","abstract":"Microwave Kinetic Inductance Detectors (MKIDs) have been demonstrated as capable phonon sensors when coupled to crystalline substrates, and have been proposed as detectors for next-generation rare-event searches such as for the direct detection of dark matter. These Kinetic Inductance Phonon Mediated (KIPM) detector designs, favoring large superconducting absorber volumes and high readout powers, are oftentimes limited in their sensitivity by low temperature amplifier noise introduced in the signal readout chain. We report here an effort to couple a wideband Kinetic Inductance Travelling Wave Parametric Amplifier (KI-TWPA), operated near the Standard Quantum Limit of minimal added amplifier noise, to sensors spanning a 70 MHz bandwidth at 3.5 GHz. This results in a ~5x improvement in the inferred detector energy resolution in the best sensor and highlights the potential of constructing O(100) meV resolving phonon-mediated particle detectors. We detail limitations introduced by lossy passive components, degraded RF responsivity, and microphysical noise sources like two-level systems (TLS), in achieving ultimate quantum-limited system noise levels.","sentences":["Microwave Kinetic Inductance Detectors (MKIDs) have been demonstrated as capable phonon sensors when coupled to crystalline substrates, and have been proposed as detectors for next-generation rare-event searches such as for the direct detection of dark matter.","These Kinetic Inductance Phonon Mediated (KIPM) detector designs, favoring large superconducting absorber volumes and high readout powers, are oftentimes limited in their sensitivity by low temperature amplifier noise introduced in the signal readout chain.","We report here an effort to couple a wideband Kinetic Inductance Travelling Wave Parametric Amplifier (KI-TWPA), operated near the Standard Quantum Limit of minimal added amplifier noise, to sensors spanning a 70 MHz bandwidth at 3.5 GHz.","This results in a ~5x improvement in the inferred detector energy resolution in the best sensor and highlights the potential of constructing O(100)","meV resolving phonon-mediated particle detectors.","We detail limitations introduced by lossy passive components, degraded RF responsivity, and microphysical noise sources like two-level systems (TLS), in achieving ultimate quantum-limited system noise levels."],"url":"http://arxiv.org/abs/2402.05419v1","category":"physics.ins-det"}
{"created":"2024-02-08 05:18:11","title":"Segmentation-free Connectionist Temporal Classification loss based OCR Model for Text Captcha Classification","abstract":"Captcha are widely used to secure systems from automatic responses by distinguishing computer responses from human responses. Text, audio, video, picture picture-based Optical Character Recognition (OCR) are used for creating captcha. Text-based OCR captcha are the most often used captcha which faces issues namely, complex and distorted contents. There are attempts to build captcha detection and classification-based systems using machine learning and neural networks, which need to be tuned for accuracy. The existing systems face challenges in the recognition of distorted characters, handling variable-length captcha and finding sequential dependencies in captcha. In this work, we propose a segmentation-free OCR model for text captcha classification based on the connectionist temporal classification loss technique. The proposed model is trained and tested on a publicly available captcha dataset. The proposed model gives 99.80\\% character level accuracy, while 95\\% word level accuracy. The accuracy of the proposed model is compared with the state-of-the-art models and proves to be effective. The variable length complex captcha can be thus processed with the segmentation-free connectionist temporal classification loss technique with dependencies which will be massively used in securing the software systems.","sentences":["Captcha are widely used to secure systems from automatic responses by distinguishing computer responses from human responses.","Text, audio, video, picture picture-based Optical Character Recognition (OCR) are used for creating captcha.","Text-based OCR captcha are the most often used captcha which faces issues namely, complex and distorted contents.","There are attempts to build captcha detection and classification-based systems using machine learning and neural networks, which need to be tuned for accuracy.","The existing systems face challenges in the recognition of distorted characters, handling variable-length captcha and finding sequential dependencies in captcha.","In this work, we propose a segmentation-free OCR model for text captcha classification based on the connectionist temporal classification loss technique.","The proposed model is trained and tested on a publicly available captcha dataset.","The proposed model gives 99.80\\% character level accuracy, while 95\\% word level accuracy.","The accuracy of the proposed model is compared with the state-of-the-art models and proves to be effective.","The variable length complex captcha can be thus processed with the segmentation-free connectionist temporal classification loss technique with dependencies which will be massively used in securing the software systems."],"url":"http://arxiv.org/abs/2402.05417v1","category":"cs.CV"}
{"created":"2024-02-08 05:17:30","title":"From atomic to global connectivity in the structure of the SARS-CoV2-Human ACE2 receptor complex","abstract":"We investigate connectivity properties of the SARS-CoV2 spike protein-human ACE2-receptor complex employing a protein side chain-based network method that allows us to span a range from atomic to global protein scales. We analyze network topology in terms of clusters and cliques obtained from averaging over snapshots of MD simulations (from D.E. Shaw Research). We demonstrate that SARS-CoV2 forms a more dominant, robust connection with the ACE2-receptor as compared to the less virulent SARS-CoV1. Globally, this stronger connectivity is reflected by our percolation analysis where the interface cluster for the SARS-CoV2-ACE2 complex persists when restricted to stronger and stronger bonds, as compared to the SARSCoV1- ACE2 complex. At the atomic level, interface clique structure reflects a stronger connectivity in the former complex. We pinpoint key functional residues in SARSCoV2 that play important roles in establishing this higher connectivity. Thus, our studies provide an objective method to map spatial connectivity of atomic level non-covalent interactions to global connectivity between any two amino acids in the complex. We also analyze specific snapshots of the MD simulations to highlight prominent variations in network topology that explore diverse conformational landscapes. Finally, we demonstrate that a majority of mutations that occur in the SARSCoV2 spike protein in variants of concern/interest (including the currently circulating JN.1) have been observed at the interface with the ACE2 receptor. Our analyses highlight the importance of interface interactions and provide a rationale for designing receptor-like peptides and proteins to combat immunity-escaping variants.","sentences":["We investigate connectivity properties of the SARS-CoV2 spike protein-human ACE2-receptor complex employing a protein side chain-based network method that allows us to span a range from atomic to global protein scales.","We analyze network topology in terms of clusters and cliques obtained from averaging over snapshots of MD simulations (from D.E. Shaw Research).","We demonstrate that SARS-CoV2 forms a more dominant, robust connection with the ACE2-receptor as compared to the less virulent SARS-CoV1.","Globally, this stronger connectivity is reflected by our percolation analysis where the interface cluster for the SARS-CoV2-ACE2 complex persists when restricted to stronger and stronger bonds, as compared to the SARSCoV1- ACE2 complex.","At the atomic level, interface clique structure reflects a stronger connectivity in the former complex.","We pinpoint key functional residues in SARSCoV2 that play important roles in establishing this higher connectivity.","Thus, our studies provide an objective method to map spatial connectivity of atomic level non-covalent interactions to global connectivity between any two amino acids in the complex.","We also analyze specific snapshots of the MD simulations to highlight prominent variations in network topology that explore diverse conformational landscapes.","Finally, we demonstrate that a majority of mutations that occur in the SARSCoV2 spike protein in variants of concern/interest (including the currently circulating JN.1) have been observed at the interface with the ACE2 receptor.","Our analyses highlight the importance of interface interactions and provide a rationale for designing receptor-like peptides and proteins to combat immunity-escaping variants."],"url":"http://arxiv.org/abs/2402.05416v1","category":"physics.bio-ph"}
{"created":"2024-02-08 05:13:50","title":"Near-Optimal Convex Simple Bilevel Optimization with a Bisection Method","abstract":"This paper studies a class of simple bilevel optimization problems where we minimize a composite convex function at the upper-level subject to a composite convex lower-level problem. Existing methods either provide asymptotic guarantees for the upper-level objective or attain slow sublinear convergence rates. We propose a bisection algorithm to find a solution that is $\\epsilon_f$-optimal for the upper-level objective and $\\epsilon_g$-optimal for the lower-level objective. In each iteration, the binary search narrows the interval by assessing inequality system feasibility. Under mild conditions, we show that each iteration and initial bounds can be obtained in ${{\\mathcal{O}}}\\left(\\max\\{\\sqrt{L_{f_1}/\\epsilon_f},\\sqrt{L_{g_1}/\\epsilon_g}\\} \\right)$ iterations, where $L_{f_1}$ and $L_{g_1}$ are Lipschitz constants of the upper- and lower-level objectives' smooth components. Accounting for binary search iterations, our method's total complexity is ${\\tilde {\\mathcal{O}}}\\left(\\max\\{\\sqrt{L_{f_1}/\\epsilon_f},\\sqrt{L_{g_1}/\\epsilon_g} \\} \\right)$, where ${\\tilde {\\mathcal{O}}}$ hides logarithmic terms. Our approach achieves near-optimal rates, matching those in unconstrained smooth or composite convex optimization when disregarding logarithmic terms. Numerical experiments demonstrate the effectiveness of our method.","sentences":["This paper studies a class of simple bilevel optimization problems where we minimize a composite convex function at the upper-level subject to a composite convex lower-level problem.","Existing methods either provide asymptotic guarantees for the upper-level objective or attain slow sublinear convergence rates.","We propose a bisection algorithm to find a solution that is $\\epsilon_f$-optimal for the upper-level objective and $\\epsilon_g$-optimal for the lower-level objective.","In each iteration, the binary search narrows the interval by assessing inequality system feasibility.","Under mild conditions, we show that each iteration and initial bounds can be obtained in ${{\\mathcal{O}}}\\left(\\max\\{\\sqrt{L_{f_1}/\\epsilon_f},\\sqrt{L_{g_1}/\\epsilon_g}\\} \\right)$ iterations, where $L_{f_1}$ and $L_{g_1}$ are Lipschitz constants of the upper- and lower-level objectives' smooth components.","Accounting for binary search iterations, our method's total complexity is ${\\tilde {\\mathcal{O}}}\\left(\\max\\{\\sqrt{L_{f_1}/\\epsilon_f},\\sqrt{L_{g_1}/\\epsilon_g} \\} \\right)$, where ${\\tilde {\\mathcal{O}}}$ hides logarithmic terms.","Our approach achieves near-optimal rates, matching those in unconstrained smooth or composite convex optimization when disregarding logarithmic terms.","Numerical experiments demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.05415v1","category":"math.OC"}
{"created":"2024-02-08 05:11:07","title":"Splitting probabilities are optimal controllers of rare reactive events","abstract":"The committor constitutes the primary quantity of interest within chemical kinetics as it is understood to encode the ideal reaction coordinate for a rare reactive event. We show the generative utility of the committor, in that it can be used explicitly to produce a reactive trajectory ensemble that exhibits numerically exact statistics as that of the original transition path ensemble. This is done by relating a time-dependent analogue of the committor that solves a generalized bridge problem, to the splitting probability that solves a boundary value problem under a bistable assumption. By invoking stochastic optimal control and spectral theory, we derive a general form for the optimal controller of a bridge process that connects two metastable states expressed in terms of the splitting probability. This formalism offers an alternative perspective into the role of the committor and its gradients, in that they encode forcefields that guarantee reactivity, generating trajectories that are statistically identical to the way that a system would react autonomously.","sentences":["The committor constitutes the primary quantity of interest within chemical kinetics as it is understood to encode the ideal reaction coordinate for a rare reactive event.","We show the generative utility of the committor, in that it can be used explicitly to produce a reactive trajectory ensemble that exhibits numerically exact statistics as that of the original transition path ensemble.","This is done by relating a time-dependent analogue of the committor that solves a generalized bridge problem, to the splitting probability that solves a boundary value problem under a bistable assumption.","By invoking stochastic optimal control and spectral theory, we derive a general form for the optimal controller of a bridge process that connects two metastable states expressed in terms of the splitting probability.","This formalism offers an alternative perspective into the role of the committor and its gradients, in that they encode forcefields that guarantee reactivity, generating trajectories that are statistically identical to the way that a system would react autonomously."],"url":"http://arxiv.org/abs/2402.05414v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-08 05:10:52","title":"Non-parametric estimations for graphon mean-field particle systems","abstract":"We consider the graphon mean-field system introduced in the work of Bayraktar, Chakraborty, and Wu. It is the large-population limit of a heterogeneously interacting diffusive particle system, where the interaction is of mean-field type with weights characterized by an underlying graphon function. Observing continuous-time trajectories of a finite-population particle system, we build plug-in estimators of the particle densities, drift coefficients, and graphon interaction weights of the mean-field system. Our estimators for the densities and drifts are direct results of kernel interpolation on the empirical data, and a deconvolution method leads to an estimator of the underlying graphon function. We prove that the estimator converges to the true graphon function as the number of particles tends to infinity, when all other parameters are properly chosen. Besides, we also justify the pointwise optimality of the density estimator via a minimax analysis over a particular class of particle systems.","sentences":["We consider the graphon mean-field system introduced in the work of Bayraktar, Chakraborty, and Wu.","It is the large-population limit of a heterogeneously interacting diffusive particle system, where the interaction is of mean-field type with weights characterized by an underlying graphon function.","Observing continuous-time trajectories of a finite-population particle system, we build plug-in estimators of the particle densities, drift coefficients, and graphon interaction weights of the mean-field system.","Our estimators for the densities and drifts are direct results of kernel interpolation on the empirical data, and a deconvolution method leads to an estimator of the underlying graphon function.","We prove that the estimator converges to the true graphon function as the number of particles tends to infinity, when all other parameters are properly chosen.","Besides, we also justify the pointwise optimality of the density estimator via a minimax analysis over a particular class of particle systems."],"url":"http://arxiv.org/abs/2402.05413v1","category":"math.ST"}
{"created":"2024-02-08 05:10:29","title":"Multi-Network Constrained Operational Optimization in Community Integrated Energy Systems: A Safe Reinforcement Learning Approach","abstract":"The integrated community energy system (ICES) has emerged as a promising solution for enhancing the efficiency of the distribution system by effectively coordinating multiple energy sources. However, the operational optimization of ICES is hindered by the physical constraints of heterogeneous networks including electricity, natural gas, and heat. These challenges are difficult to address due to the non-linearity of network constraints and the high complexity of multi-network coordination. This paper, therefore, proposes a novel Safe Reinforcement Learning (SRL) algorithm to optimize the multi-network constrained operation problem of ICES. Firstly, a comprehensive ICES model is established considering integrated demand response (IDR), multiple energy devices, and network constraints. The multi-network operational optimization problem of ICES is then presented and reformulated as a constrained Markov Decision Process (C-MDP) accounting for violating physical network constraints. The proposed novel SRL algorithm, named Primal-Dual Twin Delayed Deep Deterministic Policy Gradient (PD-TD3), solves the C-MDP by employing a Lagrangian multiplier to penalize the multi-network constraint violation, ensuring that violations are within a tolerated range and avoid over-conservative strategy with a low reward at the same time. The proposed algorithm accurately estimates the cumulative reward and cost of the training process, thus achieving a fair balance between improving profits and reducing constraint violations in a privacy-protected environment with only partial information. A case study comparing the proposed algorithm with benchmark RL algorithms demonstrates the computational performance in increasing total profits and alleviating the network constraint violations.","sentences":["The integrated community energy system (ICES) has emerged as a promising solution for enhancing the efficiency of the distribution system by effectively coordinating multiple energy sources.","However, the operational optimization of ICES is hindered by the physical constraints of heterogeneous networks including electricity, natural gas, and heat.","These challenges are difficult to address due to the non-linearity of network constraints and the high complexity of multi-network coordination.","This paper, therefore, proposes a novel Safe Reinforcement Learning (SRL) algorithm to optimize the multi-network constrained operation problem of ICES.","Firstly, a comprehensive ICES model is established considering integrated demand response (IDR), multiple energy devices, and network constraints.","The multi-network operational optimization problem of ICES is then presented and reformulated as a constrained Markov Decision Process (C-MDP) accounting for violating physical network constraints.","The proposed novel SRL algorithm, named Primal-Dual Twin Delayed Deep Deterministic Policy Gradient (PD-TD3), solves the C-MDP by employing a Lagrangian multiplier to penalize the multi-network constraint violation, ensuring that violations are within a tolerated range and avoid over-conservative strategy with a low reward at the same time.","The proposed algorithm accurately estimates the cumulative reward and cost of the training process, thus achieving a fair balance between improving profits and reducing constraint violations in a privacy-protected environment with only partial information.","A case study comparing the proposed algorithm with benchmark RL algorithms demonstrates the computational performance in increasing total profits and alleviating the network constraint violations."],"url":"http://arxiv.org/abs/2402.05412v1","category":"eess.SY"}
{"created":"2024-02-08 05:04:39","title":"Dynamics of measurement-induced state transitions in superconducting qubits","abstract":"We have investigated temporal fluctuation of superconducting qubits via the time-resolved measurement for an IBM Quantum system. We found that the qubit error rate abruptly changes during specific time intervals. Each high error state persists for several tens of seconds, and exhibits an on-off behavior. The observed temporal instability can be attributed to qubit transitions induced by a measurement stimulus. Resonant transition between fluctuating dressed states of the qubits coupled with high-frequency resonators can be responsible for the error-rate change.","sentences":["We have investigated temporal fluctuation of superconducting qubits via the time-resolved measurement for an IBM Quantum system.","We found that the qubit error rate abruptly changes during specific time intervals.","Each high error state persists for several tens of seconds, and exhibits an on-off behavior.","The observed temporal instability can be attributed to qubit transitions induced by a measurement stimulus.","Resonant transition between fluctuating dressed states of the qubits coupled with high-frequency resonators can be responsible for the error-rate change."],"url":"http://arxiv.org/abs/2402.05409v1","category":"quant-ph"}
{"created":"2024-02-08 04:48:51","title":"Version age-based client scheduling policy for federated learning","abstract":"Federated Learning (FL) has emerged as a privacy-preserving machine learning paradigm facilitating collaborative training across multiple clients without sharing local data. Despite advancements in edge device capabilities, communication bottlenecks present challenges in aggregating a large number of clients; only a portion of the clients can update their parameters upon each global aggregation. This phenomenon introduces the critical challenge of stragglers in FL and the profound impact of client scheduling policies on global model convergence and stability. Existing scheduling strategies address staleness but predominantly focus on either timeliness or content. Motivated by this, we introduce the novel concept of Version Age of Information (VAoI) to FL. Unlike traditional Age of Information metrics, VAoI considers both timeliness and content staleness. Each client's version age is updated discretely, indicating the freshness of information. VAoI is incorporated into the client scheduling policy to minimize the average VAoI, mitigating the impact of outdated local updates and enhancing the stability of FL systems.","sentences":["Federated Learning (FL) has emerged as a privacy-preserving machine learning paradigm facilitating collaborative training across multiple clients without sharing local data.","Despite advancements in edge device capabilities, communication bottlenecks present challenges in aggregating a large number of clients; only a portion of the clients can update their parameters upon each global aggregation.","This phenomenon introduces the critical challenge of stragglers in FL and the profound impact of client scheduling policies on global model convergence and stability.","Existing scheduling strategies address staleness but predominantly focus on either timeliness or content.","Motivated by this, we introduce the novel concept of Version Age of Information (VAoI) to FL.","Unlike traditional Age of Information metrics, VAoI considers both timeliness and content staleness.","Each client's version age is updated discretely, indicating the freshness of information.","VAoI is incorporated into the client scheduling policy to minimize the average VAoI, mitigating the impact of outdated local updates and enhancing the stability of FL systems."],"url":"http://arxiv.org/abs/2402.05407v1","category":"cs.LG"}
{"created":"2024-02-08 04:46:56","title":"The Computation for Reversibility of Arbitrary One-dimensional Finite Cellular Automata Becomes Efficient","abstract":"In this paper, we completely solve the reversibility of one-dimensional finite cellular automata (FCA). This means that we will have an efficient method to determine the reversibility of any FCA with all numbers (n) of cells. The complexity of this algorithm is independent of n. We perform calculations on two new kinds of graphs and discover that the reversibility of any FCA exhibits periodicity as n increases. We successfully provide a method to compute the reversibility sequence that encompasses the reversibility of FCA with any number of cells. Additionally, the calculations in this paper are applicable to FCA with various types of boundaries.","sentences":["In this paper, we completely solve the reversibility of one-dimensional finite cellular automata (FCA).","This means that we will have an efficient method to determine the reversibility of any FCA with all numbers (n) of cells.","The complexity of this algorithm is independent of n.","We perform calculations on two new kinds of graphs and discover that the reversibility of any FCA exhibits periodicity as n increases.","We successfully provide a method to compute the reversibility sequence that encompasses the reversibility of FCA with any number of cells.","Additionally, the calculations in this paper are applicable to FCA with various types of boundaries."],"url":"http://arxiv.org/abs/2402.05404v1","category":"nlin.CG"}
{"created":"2024-02-08 04:42:29","title":"A State-of-the-art Survey on Full-duplex Network Design","abstract":"Full-duplex (FD) technology is gaining popularity for integration into a wide range of wireless networks due to its demonstrated potential in recent studies. In contrast to half-duplex (HD) technology, the implementation of FD in networks necessitates considering inter-node interference (INI) from various network perspectives. When deploying FD technology in networks, several critical factors must be taken into account. These include self-interference (SI) and the requisite SI cancellation (SIC) processes, as well as the selection of multiple user equipment (UE) per time slot. Additionally, inter-node interference (INI), including cross-link interference (CLI) and inter-cell interference (ICI), become crucial issues during concurrent uplink (UL) and downlink (DL) transmission and reception, similar to SI. Since most INI is challenging to eliminate, a comprehensive investigation that covers radio resource control (RRC), medium access control (MAC), and the physical layer (PHY) is essential in the context of FD network design, rather than focusing on individual network layers and types. This paper covers state-of-the-art studies, including protocols and documents from 3GPP for FD, MAC protocol, user scheduling, and CLI handling. The methods are also compared through a network-level system simulation based on 3D ray-tracing.","sentences":["Full-duplex (FD) technology is gaining popularity for integration into a wide range of wireless networks due to its demonstrated potential in recent studies.","In contrast to half-duplex (HD) technology, the implementation of FD in networks necessitates considering inter-node interference (INI) from various network perspectives.","When deploying FD technology in networks, several critical factors must be taken into account.","These include self-interference (SI) and the requisite SI cancellation (SIC) processes, as well as the selection of multiple user equipment (UE) per time slot.","Additionally, inter-node interference (INI), including cross-link interference (CLI) and inter-cell interference (ICI), become crucial issues during concurrent uplink (UL) and downlink (DL) transmission and reception, similar to SI.","Since most INI is challenging to eliminate, a comprehensive investigation that covers radio resource control (RRC), medium access control (MAC), and the physical layer (PHY) is essential in the context of FD network design, rather than focusing on individual network layers and types.","This paper covers state-of-the-art studies, including protocols and documents from 3GPP for FD, MAC protocol, user scheduling, and CLI handling.","The methods are also compared through a network-level system simulation based on 3D ray-tracing."],"url":"http://arxiv.org/abs/2402.05402v1","category":"cs.NI"}
{"created":"2024-02-08 04:21:30","title":"On the Effect of Image Resolution on Semantic Segmentation","abstract":"High-resolution semantic segmentation requires substantial computational resources. Traditional approaches in the field typically downscale the input images before processing and then upscale the low-resolution outputs back to their original dimensions. While this strategy effectively identifies broad regions, it often misses finer details. In this study, we demonstrate that a streamlined model capable of directly producing high-resolution segmentations can match the performance of more complex systems that generate lower-resolution results. By simplifying the network architecture, we enable the processing of images at their native resolution. Our approach leverages a bottom-up information propagation technique across various scales, which we have empirically shown to enhance segmentation accuracy. We have rigorously tested our method using leading-edge semantic segmentation datasets. Specifically, for the Cityscapes dataset, we further boost accuracy by applying the Noisy Student Training technique.","sentences":["High-resolution semantic segmentation requires substantial computational resources.","Traditional approaches in the field typically downscale the input images before processing and then upscale the low-resolution outputs back to their original dimensions.","While this strategy effectively identifies broad regions, it often misses finer details.","In this study, we demonstrate that a streamlined model capable of directly producing high-resolution segmentations can match the performance of more complex systems that generate lower-resolution results.","By simplifying the network architecture, we enable the processing of images at their native resolution.","Our approach leverages a bottom-up information propagation technique across various scales, which we have empirically shown to enhance segmentation accuracy.","We have rigorously tested our method using leading-edge semantic segmentation datasets.","Specifically, for the Cityscapes dataset, we further boost accuracy by applying the Noisy Student Training technique."],"url":"http://arxiv.org/abs/2402.05398v1","category":"cs.CV"}
{"created":"2024-02-08 04:05:36","title":"The bifurcation structure within robust chaos for two-dimensional piecewise-linear maps","abstract":"We study two-dimensional, two-piece, piecewise-linear maps having two saddle fixed points. Such maps reduce to a four-parameter family and are well known to have a chaotic attractor throughout open regions of parameter space. The purpose of this paper is to determine where and how this attractor undergoes bifurcations. We explore the bifurcation structure numerically by using Eckstein's greatest common divisor algorithm to estimate from sample orbits the number of connected components in the attractor. Where the map is orientation-preserving the numerical results agree with formal results obtained previously through renormalisation. Where the map is orientation-reversing or non-invertible the same renormalisation scheme appears to generate the bifurcation boundaries, but here we need to account for the possibility of some stable low-period solutions. Also the attractor can be destroyed in novel heteroclinic bifurcations (boundary crises) that do not correspond to simple algebraic constraints on the parameters. Overall the results reveal a broadly similar component-doubling bifurcation structure in the orientation-reversing and non-invertible settings, but with some additional complexities.","sentences":["We study two-dimensional, two-piece, piecewise-linear maps having two saddle fixed points.","Such maps reduce to a four-parameter family and are well known to have a chaotic attractor throughout open regions of parameter space.","The purpose of this paper is to determine where and how this attractor undergoes bifurcations.","We explore the bifurcation structure numerically by using Eckstein's greatest common divisor algorithm to estimate from sample orbits the number of connected components in the attractor.","Where the map is orientation-preserving the numerical results agree with formal results obtained previously through renormalisation.","Where the map is orientation-reversing or non-invertible the same renormalisation scheme appears to generate the bifurcation boundaries, but here we need to account for the possibility of some stable low-period solutions.","Also the attractor can be destroyed in novel heteroclinic bifurcations (boundary crises) that do not correspond to simple algebraic constraints on the parameters.","Overall the results reveal a broadly similar component-doubling bifurcation structure in the orientation-reversing and non-invertible settings, but with some additional complexities."],"url":"http://arxiv.org/abs/2402.05393v1","category":"nlin.CD"}
{"created":"2024-02-08 03:57:51","title":"Block Mott insulating state induced by next-nearest neighbor hopping in the S = 3/2 zigzag chain BaCoTe2O7","abstract":"Quasi-one-dimensional correlated electronic multi-orbital systems with either ladder or chain geometries continue attracting considerable interest due to their complex electronic phases arising from the interplay of the hopping matrix, the crystal-fields splitting, the electronic correlations, and strong quantum fluctuations. Recently, the intriguing cobalt zigzag chain system BaCoTe$_2$O$_7$, with electronic density $n = 7$, was prepared experimentally. Here, we systematically study the electronic and magnetic properties of this quasi-one-dimensional compound from the theory perspective. Based on first-principles density functional theory calculations, strongly anisotropic one-dimensional electronic Co $3d$ bands were found near the Fermi level. By evaluating the relevant hopping amplitudes, we provide the magnitude and origin of the nearest-neighbor (NN) and next nearest-neighbor (NNN) hopping matrices in BaCoTe$_2$O$_7$. With this information, we constructed a three-orbital electronic Hubbard model for this zigzag chain system, and studied two cases: with only a NN hopping matrix, and with NN plus NNN hopping matrices. Introducing the Hubbard and Hund couplings and studying the model via the density matrix renormalization group method, we constructed the ground-state phase diagram. A robust staggered antiferromagnetic (AFM) region was found when only the NN hopping matrix in the chain direction was employed. However, for the realistic case where the NNN hopping matrix is also included, the dominant state becomes instead a block AFM order, in agreement with experiments. The system displays Mott insulator characteristics with three half-filled orbitals, when the block AFM order is stable. Our results for BaCoTe$_2$O$_7$ provide guidance to experimentalists and theorists working on this zigzag one-dimensional chain and related materials.","sentences":["Quasi-one-dimensional correlated electronic multi-orbital systems with either ladder or chain geometries continue attracting considerable interest due to their complex electronic phases arising from the interplay of the hopping matrix, the crystal-fields splitting, the electronic correlations, and strong quantum fluctuations.","Recently, the intriguing cobalt zigzag chain system BaCoTe$_2$O$_7$, with electronic density $n = 7$, was prepared experimentally.","Here, we systematically study the electronic and magnetic properties of this quasi-one-dimensional compound from the theory perspective.","Based on first-principles density functional theory calculations, strongly anisotropic one-dimensional electronic Co $3d$ bands were found near the Fermi level.","By evaluating the relevant hopping amplitudes, we provide the magnitude and origin of the nearest-neighbor (NN) and next nearest-neighbor (NNN) hopping matrices in BaCoTe$_2$O$_7$. With this information, we constructed a three-orbital electronic Hubbard model for this zigzag chain system, and studied two cases: with only a NN hopping matrix, and with NN plus NNN hopping matrices.","Introducing the Hubbard and Hund couplings and studying the model via the density matrix renormalization group method, we constructed the ground-state phase diagram.","A robust staggered antiferromagnetic (AFM) region was found when only the NN hopping matrix in the chain direction was employed.","However, for the realistic case where the NNN hopping matrix is also included, the dominant state becomes instead a block AFM order, in agreement with experiments.","The system displays Mott insulator characteristics with three half-filled orbitals, when the block AFM order is stable.","Our results for BaCoTe$_2$O$_7$ provide guidance to experimentalists and theorists working on this zigzag one-dimensional chain and related materials."],"url":"http://arxiv.org/abs/2402.05389v1","category":"cond-mat.str-el"}
{"created":"2024-02-08 03:55:44","title":"Form-From: A Design Space of Social Media Systems","abstract":"Social media systems are as varied as they are pervasive. They have been almost universally adopted for a broad range of purposes including work, entertainment, activism, and decision making. As a result, they have also diversified, with many distinct designs differing in content type, organization, delivery mechanism, access control, and many other dimensions. In this work, we aim to characterize and then distill a concise design space of social media systems that can help us understand similarities and differences, recognize potential consequences of design choice, and identify spaces for innovation. Our model, which we call Form-From, characterizes social media based on (1) the form of the content, either threaded or flat, and (2) from where or from whom one might receive content, ranging from spaces to networks to the commons. We derive Form-From inductively from a larger set of 62 dimensions organized into 10 categories. To demonstrate the utility of our model, we trace the history of social media systems as they traverse the Form-From space over time, and we identify common design patterns within cells of the model.","sentences":["Social media systems are as varied as they are pervasive.","They have been almost universally adopted for a broad range of purposes including work, entertainment, activism, and decision making.","As a result, they have also diversified, with many distinct designs differing in content type, organization, delivery mechanism, access control, and many other dimensions.","In this work, we aim to characterize and then distill a concise design space of social media systems that can help us understand similarities and differences, recognize potential consequences of design choice, and identify spaces for innovation.","Our model, which we call Form-From, characterizes social media based on (1) the form of the content, either threaded or flat, and (2) from where or from whom one might receive content, ranging from spaces to networks to the commons.","We derive Form-From inductively from a larger set of 62 dimensions organized into 10 categories.","To demonstrate the utility of our model, we trace the history of social media systems as they traverse the Form-From space over time, and we identify common design patterns within cells of the model."],"url":"http://arxiv.org/abs/2402.05388v1","category":"cs.HC"}
{"created":"2024-02-08 03:55:16","title":"Can Channels be Fully Inferred Between Two Antenna Panels?","abstract":"This letter considers a two-panel massive multiple-input multiple-output (MIMO) communication system, where the base station (BS) is equipped with two antenna panels that may use different frequency bands for communication. By exploiting the geometric relationships between antenna panels, efficient channel inference methods across antenna panels are proposed to reduce the overhead of real-time channel estimation. Four scenarios are considered, namely far-field free-space, near-field free-space, multi-path sharing far-field scatterers, and multi-path sharing near-field scatterers. For both far-field and near-field free-space scenarios, we show that the channel of one panel can be fully inferred from that of the other panel, as long as the multi-path components (MPCs) composing the channel can be resolved. On the other hand, for the multi-path scenarios sharing far-field or near-field scatterers, only the angles or range of angles of the MPCs can be inferred, respectively. Simulation results based on commercial 3D ray-tracing software are presented to validate our developed channel inference techniques.","sentences":["This letter considers a two-panel massive multiple-input multiple-output (MIMO) communication system, where the base station (BS) is equipped with two antenna panels that may use different frequency bands for communication.","By exploiting the geometric relationships between antenna panels, efficient channel inference methods across antenna panels are proposed to reduce the overhead of real-time channel estimation.","Four scenarios are considered, namely far-field free-space, near-field free-space, multi-path sharing far-field scatterers, and multi-path sharing near-field scatterers.","For both far-field and near-field free-space scenarios, we show that the channel of one panel can be fully inferred from that of the other panel, as long as the multi-path components (MPCs) composing the channel can be resolved.","On the other hand, for the multi-path scenarios sharing far-field or near-field scatterers, only the angles or range of angles of the MPCs can be inferred, respectively.","Simulation results based on commercial 3D ray-tracing software are presented to validate our developed channel inference techniques."],"url":"http://arxiv.org/abs/2402.05387v1","category":"cs.IT"}
{"created":"2024-02-08 03:29:10","title":"Tradeoffs of Diagonal Fisher Information Matrix Estimators","abstract":"The Fisher information matrix characterizes the local geometry in the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two such estimators, whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in regression and classification networks. We navigate trade-offs of both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information.","sentences":["The Fisher information matrix characterizes the local geometry in the parameter space of neural networks.","It elucidates insightful theories and useful tools to understand and optimize neural networks.","Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries.","We examine two such estimators, whose accuracy and sample complexity depend on their associated variances.","We derive bounds of the variances and instantiate them in regression and classification networks.","We navigate trade-offs of both estimators based on analytical and numerical studies.","We find that the variance quantities depend on the non-linearity with respect to different parameter groups and should not be neglected when estimating the Fisher information."],"url":"http://arxiv.org/abs/2402.05379v1","category":"cs.LG"}
{"created":"2024-02-08 03:15:06","title":"Get What You Want, Not What You Don't: Image Content Suppression for Text-to-Image Diffusion Models","abstract":"The success of recent text-to-image diffusion models is largely due to their capacity to be guided by a complex text prompt, which enables users to precisely describe the desired content. However, these models struggle to effectively suppress the generation of undesired content, which is explicitly requested to be omitted from the generated image in the prompt. In this paper, we analyze how to manipulate the text embeddings and remove unwanted content from them. We introduce two contributions, which we refer to as $\\textit{soft-weighted regularization}$ and $\\textit{inference-time text embedding optimization}$. The first regularizes the text embedding matrix and effectively suppresses the undesired content. The second method aims to further suppress the unwanted content generation of the prompt, and encourages the generation of desired content. We evaluate our method quantitatively and qualitatively on extensive experiments, validating its effectiveness. Furthermore, our method is generalizability to both the pixel-space diffusion models (i.e. DeepFloyd-IF) and the latent-space diffusion models (i.e. Stable Diffusion).","sentences":["The success of recent text-to-image diffusion models is largely due to their capacity to be guided by a complex text prompt, which enables users to precisely describe the desired content.","However, these models struggle to effectively suppress the generation of undesired content, which is explicitly requested to be omitted from the generated image in the prompt.","In this paper, we analyze how to manipulate the text embeddings and remove unwanted content from them.","We introduce two contributions, which we refer to as $\\textit{soft-weighted regularization}$ and $\\textit{inference-time text embedding optimization}$.","The first regularizes the text embedding matrix and effectively suppresses the undesired content.","The second method aims to further suppress the unwanted content generation of the prompt, and encourages the generation of desired content.","We evaluate our method quantitatively and qualitatively on extensive experiments, validating its effectiveness.","Furthermore, our method is generalizability to both the pixel-space diffusion models (i.e. DeepFloyd-IF) and the latent-space diffusion models (i.e. Stable Diffusion)."],"url":"http://arxiv.org/abs/2402.05375v1","category":"cs.CV"}
{"created":"2024-02-08 02:44:24","title":"High-Performance Multi-Qubit System with Double-Transmon Couplers towards Scalable Superconducting Quantum Computers","abstract":"Tunable couplers in superconducting quantum computers have enabled fast and accurate two-qubit gates, with reported high fidelities over 0.99 in various architectures and gate implementation schemes. However, there are few tunable couplers whose performance in multi-qubit systems is clarified, except for the most widely used one: single-transmon coupler (STC). Achieving similar accuracy to isolated two-qubit systems remains challenging due to various undesirable couplings but is necessary for scalability. In this work, we numerically analyze a system of three fixed-frequency qubits coupled via two double-transmon couplers (DTCs) where nearest-neighbor qubits are highly detuned and also next nearest-neighbor ones are nearly resonant. The DTC is a recently proposed tunable coupler, which consists of two fixed-frequency transmons coupled through a common loop with an additional Josephson junction. We find that the DTC can not only reduce undesired residual couplings sufficiently, as well as in isolated two-qubits systems, but also enables implementations of 30-ns CZ gates and 10-ns $\\pi/2$ pulses with fidelities of 0.9999 or higher. For comparison, we also investigate the system where the DTCs are replaced by the STCs. The results show that the DTC outperforms the STC in terms of both residual coupling suppression and gate accuracy in the above systems. From these results, we expect that the DTC architecture is promising for realizing high-performance, scalable superconducting quantum computers.","sentences":["Tunable couplers in superconducting quantum computers have enabled fast and accurate two-qubit gates, with reported high fidelities over 0.99 in various architectures and gate implementation schemes.","However, there are few tunable couplers whose performance in multi-qubit systems is clarified, except for the most widely used one: single-transmon coupler (STC).","Achieving similar accuracy to isolated two-qubit systems remains challenging due to various undesirable couplings but is necessary for scalability.","In this work, we numerically analyze a system of three fixed-frequency qubits coupled via two double-transmon couplers (DTCs) where nearest-neighbor qubits are highly detuned and also next nearest-neighbor ones are nearly resonant.","The DTC is a recently proposed tunable coupler, which consists of two fixed-frequency transmons coupled through a common loop with an additional Josephson junction.","We find that the DTC can not only reduce undesired residual couplings sufficiently, as well as in isolated two-qubits systems, but also enables implementations of 30-ns CZ gates and 10-ns $\\pi/2$ pulses with fidelities of 0.9999 or higher.","For comparison, we also investigate the system where the DTCs are replaced by the STCs.","The results show that the DTC outperforms the STC in terms of both residual coupling suppression and gate accuracy in the above systems.","From these results, we expect that the DTC architecture is promising for realizing high-performance, scalable superconducting quantum computers."],"url":"http://arxiv.org/abs/2402.05361v1","category":"quant-ph"}
{"created":"2024-02-08 02:29:33","title":"Exploring Learning Complexity for Downstream Data Pruning","abstract":"The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources. An intuitive solution is to prune the less informative samples from the fine-tuning dataset. A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating. For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free. However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks. In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks. Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model. Then we preserve the diverse and easy samples for fine-tuning. Extensive experiments with vision datasets demonstrate the effectiveness and efficiency of the proposed scoring function for classification tasks. For the instruction fine-tuning of large language models, our method achieves state-of-the-art performance with stable convergence, outperforming the full training with only 10\\% of the instruction dataset.","sentences":["The over-parameterized pre-trained models pose a great challenge to fine-tuning with limited computation resources.","An intuitive solution is to prune the less informative samples from the fine-tuning dataset.","A series of training-based scoring functions are proposed to quantify the informativeness of the data subset but the pruning cost becomes non-negligible due to the heavy parameter updating.","For efficient pruning, it is viable to adapt the similarity scoring function of geometric-based methods from training-based to training-free.","However, we empirically show that such adaption distorts the original pruning and results in inferior performance on the downstream tasks.","In this paper, we propose to treat the learning complexity (LC) as the scoring function for classification and regression tasks.","Specifically, the learning complexity is defined as the average predicted confidence of subnets with different capacities, which encapsulates data processing within a converged model.","Then we preserve the diverse and easy samples for fine-tuning.","Extensive experiments with vision datasets demonstrate the effectiveness and efficiency of the proposed scoring function for classification tasks.","For the instruction fine-tuning of large language models, our method achieves state-of-the-art performance with stable convergence, outperforming the full training with only 10\\% of the instruction dataset."],"url":"http://arxiv.org/abs/2402.05356v1","category":"cs.LG"}
{"created":"2024-02-08 18:44:28","title":"Background independent tensor networks","abstract":"Conventional holographic tensor networks can be described as toy holographic maps constructed from many small linear maps acting in a spatially local way, all connected together with ``background entanglement'', i.e. links of a fixed state, often the maximally entangled state. However, these constructions fall short of modeling real holographic maps. One reason is that their ``areas'' are trivial, taking the same value for all states, unlike in gravity where the geometry is dynamical. Recently, new constructions have ameliorated this issue by adding degrees of freedom that ``live on the links''. This makes areas non-trivial, equal to the background entanglement piece plus a new positive piece that depends on the state of the link degrees of freedom. Nevertheless, this still has the downside that there is background entanglement, and hence it only models relatively limited code subspaces in which every area has a definite minimum value given by the background entanglement. In this note, we simply point out that a version of these constructions goes one step further: they can be background independent, with no background entanglement in the holographic map. This is advantageous because it allows tensor networks to model holographic maps for larger code subspaces. In addition to pointing this out, we address some subtleties involved in making it work and point out a nice connection it offers to recent discussions of random CFT data.","sentences":["Conventional holographic tensor networks can be described as toy holographic maps constructed from many small linear maps acting in a spatially local way, all connected together with ``background entanglement'', i.e. links of a fixed state, often the maximally entangled state.","However, these constructions fall short of modeling real holographic maps.","One reason is that their ``areas'' are trivial, taking the same value for all states, unlike in gravity where the geometry is dynamical.","Recently, new constructions have ameliorated this issue by adding degrees of freedom that ``live on the links''.","This makes areas non-trivial, equal to the background entanglement piece plus a new positive piece that depends on the state of the link degrees of freedom.","Nevertheless, this still has the downside that there is background entanglement, and hence it only models relatively limited code subspaces in which every area has a definite minimum value given by the background entanglement.","In this note, we simply point out that a version of these constructions goes one step further: they can be background independent, with no background entanglement in the holographic map.","This is advantageous because it allows tensor networks to model holographic maps for larger code subspaces.","In addition to pointing this out, we address some subtleties involved in making it work and point out a nice connection it offers to recent discussions of random CFT data."],"url":"http://arxiv.org/abs/2402.05910v1","category":"hep-th"}
{"created":"2024-02-08 18:42:04","title":"Neuronal functional connectivity graph estimation with the R package neurofuncon","abstract":"Researchers continue exploring neurons' intricate patterns of activity in the cerebral visual cortex in response to visual stimuli. The way neurons communicate and optimize their interactions with each other under different experimental conditions remains a topic of active investigation. Probabilistic Graphical Models are invaluable tools in neuroscience research, as they let us identify the functional connections, or conditional statistical dependencies, between neurons. Graphical models represent these connections as a graph, where nodes represent neurons and edges indicate the presence of functional connections between them. We developed the R package neurofuncon for the computation and visualization of functional connectivity graphs from large-scale data based on the Graphical lasso. We illustrate the use of this package with publicly available two-photon calcium microscopy imaging data from approximately 10000 neurons in a 1mm cubic section of a mouse visual cortex.","sentences":["Researchers continue exploring neurons' intricate patterns of activity in the cerebral visual cortex in response to visual stimuli.","The way neurons communicate and optimize their interactions with each other under different experimental conditions remains a topic of active investigation.","Probabilistic Graphical Models are invaluable tools in neuroscience research, as they let us identify the functional connections, or conditional statistical dependencies, between neurons.","Graphical models represent these connections as a graph, where nodes represent neurons and edges indicate the presence of functional connections between them.","We developed the R package neurofuncon for the computation and visualization of functional connectivity graphs from large-scale data based on the Graphical lasso.","We illustrate the use of this package with publicly available two-photon calcium microscopy imaging data from approximately 10000 neurons in a 1mm cubic section of a mouse visual cortex."],"url":"http://arxiv.org/abs/2402.05903v1","category":"q-bio.NC"}
{"created":"2024-02-08 18:30:44","title":"On benefits of cooperation under strategic power","abstract":"We introduce a new model involving TU-games and exogenous structures. Specifically, we consider that each player in a population can choose an element in a strategy set and that, for every possible strategy profile, a TU-game is associated with the population. This is what we call a TU-game with strategies. We propose and characterize the maxmin procedure to map every game with strategies to a TU-game. We also study whether or not the relevant properties of TU-games are transmitted by applying the maxmin procedure. Finally, we examine two relevant classes of TU-games with strategies: airport and simple games with strategies.","sentences":["We introduce a new model involving TU-games and exogenous structures.","Specifically, we consider that each player in a population can choose an element in a strategy set and that, for every possible strategy profile, a TU-game is associated with the population.","This is what we call a TU-game with strategies.","We propose and characterize the maxmin procedure to map every game with strategies to a TU-game.","We also study whether or not the relevant properties of TU-games are transmitted by applying the maxmin procedure.","Finally, we examine two relevant classes of TU-games with strategies: airport and simple games with strategies."],"url":"http://arxiv.org/abs/2402.05891v1","category":"cs.GT"}
{"created":"2024-02-08 18:12:12","title":"Well-posedness and inverse problems for semilinear nonlocal wave equations","abstract":"This article is devoted to forward and inverse problems associated with time-independent semilinear nonlocal wave equations. We first establish comprehensive well-posedness results for some semilinear nonlocal wave equations. The main challenge is due to the low regularity of the solutions of linear nonlocal wave equations. We then turn to an inverse problem of recovering the nonlinearity of the equation. More precisely, we show that the exterior Dirichlet-to-Neumann map uniquely determines homogeneous nonlinearities of the form $f(x,u)$ under certain growth conditions. On the other hand, we also prove that initial data can be determined by using passive measurements under certain nonlinearity conditions. The main tools used for the inverse problem are the unique continuation principle of the fractional Laplacian and a Runge approximation property. The results hold for any spatial dimension $n\\in \\N$.","sentences":["This article is devoted to forward and inverse problems associated with time-independent semilinear nonlocal wave equations.","We first establish comprehensive well-posedness results for some semilinear nonlocal wave equations.","The main challenge is due to the low regularity of the solutions of linear nonlocal wave equations.","We then turn to an inverse problem of recovering the nonlinearity of the equation.","More precisely, we show that the exterior Dirichlet-to-Neumann map uniquely determines homogeneous nonlinearities of the form $f(x,u)$ under certain growth conditions.","On the other hand, we also prove that initial data can be determined by using passive measurements under certain nonlinearity conditions.","The main tools used for the inverse problem are the unique continuation principle of the fractional Laplacian and a Runge approximation property.","The results hold for any spatial dimension $n\\in \\N$."],"url":"http://arxiv.org/abs/2402.05877v1","category":"math.AP"}
{"created":"2024-02-08 17:57:06","title":"A Groupoid Construction of Functional Integrals: Brownian Motion and Some TQFTs","abstract":"We formalize Feynman's construction of the quantum mechanical path integral. To do this, we shift the emphasis in differential geometry from the tangent bundle onto the pair groupoid. This allows us to use the van Est map and the piecewise linear structure of manifolds to develop a coordinate-free, partition of unity-free approach to integration of differential forms, etc. This framework makes sense for any field theory valued in a Lie algebroid. We apply it to define the Wiener measure, stochastic integrals and other observables in a coordinate-free way. We use it to reconstruct Chern-Simons with finite gauge group and to obtain some non-perturbative deformation quantizations via the Poisson sigma model on a disk.","sentences":["We formalize Feynman's construction of the quantum mechanical path integral.","To do this, we shift the emphasis in differential geometry from the tangent bundle onto the pair groupoid.","This allows us to use the van Est map and the piecewise linear structure of manifolds to develop a coordinate-free, partition of unity-free approach to integration of differential forms, etc.","This framework makes sense for any field theory valued in a Lie algebroid.","We apply it to define the Wiener measure, stochastic integrals and other observables in a coordinate-free way.","We use it to reconstruct Chern-Simons with finite gauge group and to obtain some non-perturbative deformation quantizations via the Poisson sigma model on a disk."],"url":"http://arxiv.org/abs/2402.05866v1","category":"math.DG"}
{"created":"2024-02-08 17:41:29","title":"Individual addressing and state readout of trapped ions utilizing rf micromotion","abstract":"Excess \"micromotion\" of trapped ions due to the residual radio frequency (rf) trapping field at their location is often undesirable and is usually carefully minimized. Here, we induce precise amounts of excess micromotion on individual ions by adjusting the local static electric field they experience. Micromotion modulates the coupling of an ion to laser fields, ideally tuning it from its maximum value to zero as the ion is moved away from the trap's rf null. We use tunable micromotion to vary the Rabi frequency of stimulated Raman transitions over two orders of magnitude, and to individually control the rates of resonant fluorescence from three ions under global laser illumination without any changes to the driving light fields. The technique is amenable to situations where addressing individual ions with focused laser beams is challenging, such as tightly packed linear ion strings or two-dimensional ion arrays illuminated from the side.","sentences":["Excess \"micromotion\" of trapped ions due to the residual radio frequency (rf) trapping field at their location is often undesirable and is usually carefully minimized.","Here, we induce precise amounts of excess micromotion on individual ions by adjusting the local static electric field they experience.","Micromotion modulates the coupling of an ion to laser fields, ideally tuning it from its maximum value to zero as the ion is moved away from the trap's rf null.","We use tunable micromotion to vary the Rabi frequency of stimulated Raman transitions over two orders of magnitude, and to individually control the rates of resonant fluorescence from three ions under global laser illumination without any changes to the driving light fields.","The technique is amenable to situations where addressing individual ions with focused laser beams is challenging, such as tightly packed linear ion strings or two-dimensional ion arrays illuminated from the side."],"url":"http://arxiv.org/abs/2402.05857v1","category":"physics.atom-ph"}
{"created":"2024-02-08 16:37:04","title":"TaE: Task-aware Expandable Representation for Long Tail Class Incremental Learning","abstract":"Class-incremental learning (CIL) aims to train classifiers that learn new classes without forgetting old ones. Most CIL methods focus on balanced data distribution for each task, overlooking real-world long-tailed distributions. Therefore, Long-Tailed Class-Incremental Learning (LT-CIL) has been introduced, which trains on data where head classes have more samples than tail classes. Existing methods mainly focus on preserving representative samples from previous classes to combat catastrophic forgetting. Recently, dynamic network algorithms frozen old network structures and expanded new ones, achieving significant performance. However, with the introduction of the long-tail problem, merely extending task-specific parameters can lead to miscalibrated predictions, while expanding the entire model results in an explosion of memory size. To address these issues, we introduce a novel Task-aware Expandable (TaE) framework, dynamically allocating and updating task-specific trainable parameters to learn diverse representations from each incremental task, while resisting forgetting through the majority of frozen model parameters. To further encourage the class-specific feature representation, we develop a Centroid-Enhanced (CEd) method to guide the update of these task-aware parameters. This approach is designed to adaptively minimize the distances between intra-class features while simultaneously maximizing the distances between inter-class features across all seen classes. The utility of this centroid-enhanced method extends to all \"training from scratch\" CIL algorithms. Extensive experiments were conducted on CIFAR-100 and ImageNet100 under different settings, which demonstrates that TaE achieves state-of-the-art performance.","sentences":["Class-incremental learning (CIL) aims to train classifiers that learn new classes without forgetting old ones.","Most CIL methods focus on balanced data distribution for each task, overlooking real-world long-tailed distributions.","Therefore, Long-Tailed Class-Incremental Learning (LT-CIL) has been introduced, which trains on data where head classes have more samples than tail classes.","Existing methods mainly focus on preserving representative samples from previous classes to combat catastrophic forgetting.","Recently, dynamic network algorithms frozen old network structures and expanded new ones, achieving significant performance.","However, with the introduction of the long-tail problem, merely extending task-specific parameters can lead to miscalibrated predictions, while expanding the entire model results in an explosion of memory size.","To address these issues, we introduce a novel Task-aware Expandable (TaE) framework, dynamically allocating and updating task-specific trainable parameters to learn diverse representations from each incremental task, while resisting forgetting through the majority of frozen model parameters.","To further encourage the class-specific feature representation, we develop a Centroid-Enhanced (CEd) method to guide the update of these task-aware parameters.","This approach is designed to adaptively minimize the distances between intra-class features while simultaneously maximizing the distances between inter-class features across all seen classes.","The utility of this centroid-enhanced method extends to all \"training from scratch\" CIL algorithms.","Extensive experiments were conducted on CIFAR-100 and ImageNet100 under different settings, which demonstrates that TaE achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.05797v1","category":"cs.CV"}
{"created":"2024-02-08 16:00:04","title":"The class of grim reapers in $\\mathbb{H}^2\\times\\mathbb{R}$","abstract":"We study translators of the mean curvature flow in the product space $\\h^2\\times\\r$, that is, self-similar solutions whose shape is invariant by translations of $\\h^2\\times\\r$. In $\\h^2\\times\\r$ there are three types of translations: vertical translations due to the factor $\\r$ and parabolic and hyperbolic translations from $\\h^2$. A grim reaper in $\\h^2\\times\\r$ is a translators invariant by a one-parameter group of translations. The variety of translators and translations in $\\h^2\\times\\r$ makes that the family of grim reapers particularly rich. In this paper we give a full classification of the grim reapers of $\\h^2\\times\\r$ with a description of their geometric properties. In some cases, we obtain explicit parametrizations of the surfaces.","sentences":["We study translators of the mean curvature flow in the product space $\\h^2\\times\\r$, that is, self-similar solutions whose shape is invariant by translations of $\\h^2\\times\\r$. In $\\h^2\\times\\r$ there are three types of translations: vertical translations due to the factor $\\r$ and parabolic and hyperbolic translations from $\\h^2$. A grim reaper in $\\h^2\\times\\r$ is a translators invariant by a one-parameter group of translations.","The variety of translators and translations in $\\h^2\\times\\r$ makes that the family of grim reapers particularly rich.","In this paper we give a full classification of the grim reapers of $\\h^2\\times\\r$ with a description of their geometric properties.","In some cases, we obtain explicit parametrizations of the surfaces."],"url":"http://arxiv.org/abs/2402.05772v1","category":"math.DG"}
{"created":"2024-02-08 15:49:58","title":"Datastringer: easy dataset monitoring for journalists","abstract":"We created a software enabling journalists to define a set of criteria they would like to see applied regularly to a constantly-updated dataset, sending them an alert when these criteria are met, thus signaling them that there may be a story to write. The main challenges were to keep the product scalable and powerful, while making sure that it could be used by journalists who would not possess all the technical knowledge to exploit it fully. In order to do so, we had to choose Javascript as our main language, as well as designing the code in such a way that it would allow re-usability and further improvements. This project is a proof of concept being tested in a real-life environment, and will be developed towards more and more accessibility.","sentences":["We created a software enabling journalists to define a set of criteria they would like to see applied regularly to a constantly-updated dataset, sending them an alert when these criteria are met, thus signaling them that there may be a story to write.","The main challenges were to keep the product scalable and powerful, while making sure that it could be used by journalists who would not possess all the technical knowledge to exploit it fully.","In order to do so, we had to choose Javascript as our main language, as well as designing the code in such a way that it would allow re-usability and further improvements.","This project is a proof of concept being tested in a real-life environment, and will be developed towards more and more accessibility."],"url":"http://arxiv.org/abs/2402.05764v1","category":"cs.CY"}
{"created":"2024-02-08 15:46:31","title":"Growth history and quasar bias evolution at z < 3 from Quaia","abstract":"We make use of the Gaia-Unwise quasar catalogue, Quaia, to constrain the growth history out to high redshifts from the clustering of quasars and their cross-correlation with maps of the Cosmic Microwave Background (CMB) lensing convergence. Considering three tomographic bins, centered at redshifts $\\bar{z}_i = [0.69, 1.59, 2.72]$, we reconstruct the evolution of the amplitude of matter fluctuations $\\sigma_8(z)$ over the last $\\sim12$ billion years of cosmic history. In particular, we make one of the highest-redshift measurements of $\\sigma_8$ ($\\sigma_8(z=2.72)=0.22\\pm 0.06$), finding it to be in good agreement (at the $\\sim1\\sigma$ level) with the value predicted by $\\Lambda$CDM using CMB data from Planck. We also used the data to study the evolution of the linear quasar bias for this sample, finding values similar to those of other quasar samples, although with a less steep evolution at high redshifts. Finally, we study the potential impact of foreground contamination in the CMB lensing maps and, although we find evidence of contamination in cross-correlations at $z\\sim1.7$ we are not able to clearly pinpoint its origin as being Galactic or extragalactic. Nevertheless, we determine that the impact of this contamination on our results is negligible.","sentences":["We make use of the Gaia-Unwise quasar catalogue, Quaia, to constrain the growth history out to high redshifts from the clustering of quasars and their cross-correlation with maps of the Cosmic Microwave Background (CMB) lensing convergence.","Considering three tomographic bins, centered at redshifts $\\bar{z}_i =","[0.69, 1.59, 2.72]$, we reconstruct the evolution of the amplitude of matter fluctuations $\\sigma_8(z)$ over the last $\\sim12$ billion years of cosmic history.","In particular, we make one of the highest-redshift measurements of $\\sigma_8$ ($\\sigma_8(z=2.72)=0.22\\pm 0.06$), finding it to be in good agreement (at the $\\sim1\\sigma$ level) with the value predicted by $\\Lambda$CDM using CMB data from Planck.","We also used the data to study the evolution of the linear quasar bias for this sample, finding values similar to those of other quasar samples, although with a less steep evolution at high redshifts.","Finally, we study the potential impact of foreground contamination in the CMB lensing maps and, although we find evidence of contamination in cross-correlations at $z\\sim1.7$ we are not able to clearly pinpoint its origin as being Galactic or extragalactic.","Nevertheless, we determine that the impact of this contamination on our results is negligible."],"url":"http://arxiv.org/abs/2402.05761v1","category":"astro-ph.CO"}
{"created":"2024-02-08 14:57:33","title":"Optimal probe states for single-mode quantum target detection in arbitrary object reflectivity","abstract":"Quantum target detection (QTD) utilizes nonclassical resources to enable radar-like detection for identifying reflecting objects in challenging environments, surpassing classical methods. To fully leverage the quantum advantage in QTD, determining the optimal probe states (OPSs) across various detection parameters and gaining a deeper understanding of their characteristics are crucial. In this study, we identified the single-mode continuous-variable OPSs for arbitrary object reflectivity using optimization algorithms. Our findings suggest that OPSs are non-Gaussian states in most reflectivity scenarios, with exceptions under specific conditions. Furthermore, we provide a comprehensive physical interpretation of the observed phenomena. This study offers a tool for identifying OPSs along with a clear physical interpretation. It also contributes to further advancements towards optimal multi-mode QTD, which has the potential for broad applications in quantum sensing and metrology.","sentences":["Quantum target detection (QTD) utilizes nonclassical resources to enable radar-like detection for identifying reflecting objects in challenging environments, surpassing classical methods.","To fully leverage the quantum advantage in QTD, determining the optimal probe states (OPSs) across various detection parameters and gaining a deeper understanding of their characteristics are crucial.","In this study, we identified the single-mode continuous-variable OPSs for arbitrary object reflectivity using optimization algorithms.","Our findings suggest that OPSs are non-Gaussian states in most reflectivity scenarios, with exceptions under specific conditions.","Furthermore, we provide a comprehensive physical interpretation of the observed phenomena.","This study offers a tool for identifying OPSs along with a clear physical interpretation.","It also contributes to further advancements towards optimal multi-mode QTD, which has the potential for broad applications in quantum sensing and metrology."],"url":"http://arxiv.org/abs/2402.05726v1","category":"quant-ph"}
{"created":"2024-02-08 14:53:05","title":"Classical Cepheid Pulsation properties in the Rubin-LSST filters","abstract":"Homogeneous multi-wavelength observations of classical Cepheids from the forthcoming Rubin-LSST have the potential to significantly contribute to our understanding of the evolutionary and pulsation properties of these pulsating stars. Updated pulsation models for Classical Cepheid stars have been computed under various assumptions about chemical compositions, including relatively low metallicity ($Z$ = $0.004$ with $Y$ =$0.25$ and $Z$=$0.008$ with $Y$ =$0.25$), solar metallicity ($Z$=$0.02$ with $Y$=$0.28$), and supersolar metallicity environments ($Z$ = $0.03$ with $Y$ = $0.28$).   From the predicted periods, intensity-weighted mean magnitudes, and colors, we have derived the first theoretical pulsation relations in the Rubin-LSST filters (ugrizy), including period-luminosity-color, period-Wesenheit, and period-age-color relations. We find that the coefficients of these relations are almost insensitive to the efficiency of superadiabatic convection but are significantly affected by the assumption of the mass-luminosity relation and the adopted chemical composition. Metal-dependent versions of these relations are also derived, representing valuable tools for individual distance determinations and correction for metallicity effects on the cosmic distance scale.","sentences":["Homogeneous multi-wavelength observations of classical Cepheids from the forthcoming Rubin-LSST have the potential to significantly contribute to our understanding of the evolutionary and pulsation properties of these pulsating stars.","Updated pulsation models for Classical Cepheid stars have been computed under various assumptions about chemical compositions, including relatively low metallicity ($Z$ = $0.004$ with $Y$ =$0.25$ and $Z$=$0.008$ with $Y$ =$0.25$), solar metallicity ($Z$=$0.02$ with $Y$=$0.28$), and supersolar metallicity environments ($Z$ = $0.03$ with $Y$ = $0.28$).   ","From the predicted periods, intensity-weighted mean magnitudes, and colors, we have derived the first theoretical pulsation relations in the Rubin-LSST filters (ugrizy), including period-luminosity-color, period-Wesenheit, and period-age-color relations.","We find that the coefficients of these relations are almost insensitive to the efficiency of superadiabatic convection but are significantly affected by the assumption of the mass-luminosity relation and the adopted chemical composition.","Metal-dependent versions of these relations are also derived, representing valuable tools for individual distance determinations and correction for metallicity effects on the cosmic distance scale."],"url":"http://arxiv.org/abs/2402.05721v1","category":"astro-ph.SR"}
{"created":"2024-02-08 14:43:56","title":"Collaborative non-parametric two-sample testing","abstract":"This paper addresses the multiple two-sample test problem in a graph-structured setting, which is a common scenario in fields such as Spatial Statistics and Neuroscience. Each node $v$ in fixed graph deals with a two-sample testing problem between two node-specific probability density functions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null hypothesis $p_v = q_v$ should be rejected, under the assumption that connected nodes would yield similar test outcomes. We propose the non-parametric collaborative two-sample testing (CTST) framework that efficiently leverages the graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our methodology integrates elements from f-divergence estimation, Kernel Methods, and Multitask Learning. We use synthetic experiments and a real sensor network detecting seismic activity to demonstrate that CTST outperforms state-of-the-art non-parametric statistical tests that apply at each node independently, hence disregard the geometry of the problem.","sentences":["This paper addresses the multiple two-sample test problem in a graph-structured setting, which is a common scenario in fields such as Spatial Statistics and Neuroscience.","Each node $v$ in fixed graph deals with a two-sample testing problem between two node-specific probability density functions (pdfs), $p_v$ and $q_v$. The goal is to identify nodes where the null hypothesis $p_v = q_v$ should be rejected, under the assumption that connected nodes would yield similar test outcomes.","We propose the non-parametric collaborative two-sample testing (CTST) framework that efficiently leverages the graph structure and minimizes the assumptions over $p_v$ and $q_v$. Our methodology integrates elements from f-divergence estimation, Kernel Methods, and Multitask Learning.","We use synthetic experiments and a real sensor network detecting seismic activity to demonstrate that CTST outperforms state-of-the-art non-parametric statistical tests that apply at each node independently, hence disregard the geometry of the problem."],"url":"http://arxiv.org/abs/2402.05715v1","category":"stat.ML"}
{"created":"2024-02-08 14:00:45","title":"An Ordinal Regression Framework for a Deep Learning Based Severity Assessment for Chest Radiographs","abstract":"This study investigates the application of ordinal regression methods for categorizing disease severity in chest radiographs. We propose a framework that divides the ordinal regression problem into three parts: a model, a target function, and a classification function. Different encoding methods, including one-hot, Gaussian, progress-bar, and our soft-progress-bar, are applied using ResNet50 and ViT-B-16 deep learning models. We show that the choice of encoding has a strong impact on performance and that the best encoding depends on the chosen weighting of Cohen's kappa and also on the model architecture used. We make our code publicly available on GitHub.","sentences":["This study investigates the application of ordinal regression methods for categorizing disease severity in chest radiographs.","We propose a framework that divides the ordinal regression problem into three parts: a model, a target function, and a classification function.","Different encoding methods, including one-hot, Gaussian, progress-bar, and our soft-progress-bar, are applied using ResNet50 and ViT-B-16 deep learning models.","We show that the choice of encoding has a strong impact on performance and that the best encoding depends on the chosen weighting of Cohen's kappa and also on the model architecture used.","We make our code publicly available on GitHub."],"url":"http://arxiv.org/abs/2402.05685v1","category":"cs.CV"}
{"created":"2024-02-08 12:46:37","title":"Noise through an additional variable for mean field games master equation on finite state space","abstract":"This paper provides a mathematical study of the well-posedness of master equation on finite state space involving terms modelling common noise. In this setting, the solution of the master equation depends on an additional variable modelling the value of a stochastic process impacting all players. Using technique from viscosity solutions, we give sufficient conditions for the existence of a Lipschitz continuous solution on any time interval. Under some structural assumptions, we are even able to treat cases in which the dynamics of this stochastic process depend on the state of the game.","sentences":["This paper provides a mathematical study of the well-posedness of master equation on finite state space involving terms modelling common noise.","In this setting, the solution of the master equation depends on an additional variable modelling the value of a stochastic process impacting all players.","Using technique from viscosity solutions, we give sufficient conditions for the existence of a Lipschitz continuous solution on any time interval.","Under some structural assumptions, we are even able to treat cases in which the dynamics of this stochastic process depend on the state of the game."],"url":"http://arxiv.org/abs/2402.05635v1","category":"math.AP"}
{"created":"2024-02-08 12:45:49","title":"Full Law Identification under Missing Data in the Categorical Colluder Model","abstract":"Missing data may be disastrous for the identifiability of causal and statistical estimands. In graphical missing data models, colluders are dependence structures that have a special importance for identification considerations. It has been shown that the presence of a colluder makes the full law, i.e., the joint distribution of variables and response indicators, non-parametrically non-identifiable. However, with additional mild assumptions regarding the variables involved with the colluder structure, identifiability is regained. We present a necessary and sufficient condition for the identification of the full law in the presence of a colluder structure with arbitrary categorical variables.","sentences":["Missing data may be disastrous for the identifiability of causal and statistical estimands.","In graphical missing data models, colluders are dependence structures that have a special importance for identification considerations.","It has been shown that the presence of a colluder makes the full law, i.e., the joint distribution of variables and response indicators, non-parametrically non-identifiable.","However, with additional mild assumptions regarding the variables involved with the colluder structure, identifiability is regained.","We present a necessary and sufficient condition for the identification of the full law in the presence of a colluder structure with arbitrary categorical variables."],"url":"http://arxiv.org/abs/2402.05633v1","category":"stat.ME"}
{"created":"2024-02-08 12:44:40","title":"Rates in the central limit theorem for random projections of Martingales","abstract":"In this paper, we consider partial sums of martingale differences weighted by random variables drawn uniformly on the sphere, and globally independent of the martingale differences. Combining Lindeberg's method and a series of arguments due to Bobkov, Chistyakov and G{\\\"o}tze, we show that the Kolmogorov distance between the distribution of these weighted sums and the limiting Gaussian is ''super-fast'' of order (log n)^2 /n, under conditions allowing us to control the higher-order conditional moments of the martingale differences. We give an application of this result to the least squares estimator of the slope in the linear model with Gaussian design.","sentences":["In this paper, we consider partial sums of martingale differences weighted by random variables drawn uniformly on the sphere, and globally independent of the martingale differences.","Combining Lindeberg's method and a series of arguments due to Bobkov, Chistyakov and G{\\\"o}tze, we show that the Kolmogorov distance between the distribution of these weighted sums and the limiting Gaussian is ''super-fast'' of order (log n)^2 /n, under conditions allowing us to control the higher-order conditional moments of the martingale differences.","We give an application of this result to the least squares estimator of the slope in the linear model with Gaussian design."],"url":"http://arxiv.org/abs/2402.05632v1","category":"math.PR"}
{"created":"2024-02-08 12:25:33","title":"Spin angular momentum and optical chirality of Poincar\u00e9 vector vortex beams","abstract":"The optical chirality and spin angular momentum of structured scalar vortex beams has been intensively studied in recent years. The pseudoscalar topological charge $\\ell$ of these beams is responsible for their unique properties. Constructed from a superposition of scalar vortex beams with topological charges $\\ell_\\text{A}$ and $\\ell_\\text{B}$, cylindrical vector vortex beams are higher-order Poincar\\'e modes which possess a spatially inhomogeneous polarization distribution. Here we highlight the highly tailorable and exotic spatial distributions of the optical spin and chirality densities of these higher-order structured beams under both paraxial (weak focusing) and non-paraxial (tight focusing) conditions. Our analytical theory can yield the spin angular momentum and optical chirality of each point on any higher-order or hybrid-order Poincar\\'e sphere. It is shown that the tunable Pancharatnam topological charge $\\ell_{\\text{P}} = (\\ell_\\text{A} + \\ell_\\text{B})/2$ and polarization index $m = (\\ell_\\text{B} -\\ell_\\text{A})/2$ of the vector vortex beam plays a decisive role in customizing their spin and chirality spatial distributions. We also provide the correct analytical equations to describe a focused, non-paraxial scalar Bessel beam.","sentences":["The optical chirality and spin angular momentum of structured scalar vortex beams has been intensively studied in recent years.","The pseudoscalar topological charge $\\ell$ of these beams is responsible for their unique properties.","Constructed from a superposition of scalar vortex beams with topological charges $\\ell_\\text{A}$ and $\\ell_\\text{B}$, cylindrical vector vortex beams are higher-order Poincar\\'e modes which possess a spatially inhomogeneous polarization distribution.","Here we highlight the highly tailorable and exotic spatial distributions of the optical spin and chirality densities of these higher-order structured beams under both paraxial (weak focusing) and non-paraxial (tight focusing) conditions.","Our analytical theory can yield the spin angular momentum and optical chirality of each point on any higher-order or hybrid-order Poincar\\'e sphere.","It is shown that the tunable Pancharatnam topological charge $\\ell_{\\text{P}} = (\\ell_\\text{A} + \\ell_\\text{B})/2$ and polarization index $m","= (\\ell_\\text{B} -\\ell_\\text{A})/2$ of the vector vortex beam plays a decisive role in customizing their spin and chirality spatial distributions.","We also provide the correct analytical equations to describe a focused, non-paraxial scalar Bessel beam."],"url":"http://arxiv.org/abs/2402.05621v1","category":"physics.optics"}
{"created":"2024-02-08 12:16:23","title":"Valley-dependent Multiple Quantum States and Topological Transitions in Germanene-based Ferromagnetic van der Waals Heterostructures","abstract":"Topological and valleytronic materials are promising for spintronic and quantum applications due to their unique properties. Using first principles calculations, we demonstrate that germanene (Ge)-based ferromagnetic heterostructures can exhibit multiple quantum states such as quantum anomalous Hall effect (QAHE) with Chern numbers of C=-1 or C=-2, quantum valley Hall effect (QVHE) with a valley Chern number of C$v$=2, valley-polarized quantum anomalous Hall effect (VP-QAHE) with two Chern numbers of C=-1 and C$v$=-1 as well as time-reversal symmetry broken quantum spin Hall effect (T-broken QSHE) with a spin Chern number of C$s$~1. Furthermore, we find that the transitions between different quantum states can occur by changing the magnetic orientation of ferromagnetic layers through applying a magnetic field. Our discovery provides new routes and novel material platforms with a unique combination of diverse properties that make it well suitable for applications in electronics, spintronics and valley electronics.","sentences":["Topological and valleytronic materials are promising for spintronic and quantum applications due to their unique properties.","Using first principles calculations, we demonstrate that germanene (Ge)-based ferromagnetic heterostructures can exhibit multiple quantum states such as quantum anomalous Hall effect (QAHE) with Chern numbers of C=-1 or C=-2, quantum valley Hall effect (QVHE) with a valley Chern number of C$v$=2, valley-polarized quantum anomalous Hall effect (VP-QAHE) with two Chern numbers of C=-1 and C$v$=-1 as well as time-reversal symmetry broken quantum spin Hall effect (T-broken QSHE) with a spin Chern number of C$s$~1.","Furthermore, we find that the transitions between different quantum states can occur by changing the magnetic orientation of ferromagnetic layers through applying a magnetic field.","Our discovery provides new routes and novel material platforms with a unique combination of diverse properties that make it well suitable for applications in electronics, spintronics and valley electronics."],"url":"http://arxiv.org/abs/2402.05613v1","category":"physics.comp-ph"}
{"created":"2024-02-08 12:03:38","title":"Rigorous relations for barrier transmittance and some physical corollaries","abstract":"Exactly solvable models are interesting for science and education, since they help in scientific search and in understanding of phenomena. Some exact solutions for simple quantum-mechanical models are considered. The models include two barriers, combinations of barrier pairs, three barriers, three wells etc. The model of two barriers can predict some interesting phenomena in the one-dimensional case. Clearing of wave and quantum-mechanical barriers (including reflection-free passage) is an important problem of physics. The rigorous equations for the transmission and reflection coefficients are derived. Barriers in substances are combined into associations, where the bond within each association is stronger than bonds between associations. Some properties of disordered media (the transparency of glasses, the conductivity of alloys or melts, the brittleness, or ductility, etc.) can be qualitatively understood from this viewpoint. The same material can exhibit various properties: transparent and opaque, metallic and non-metallic, ordered and disordered, and so on. Such transitions can occur under pressure. A model of the three-well potential can be applied to the phenomena under consideration. Some remarks on 3-D cases are made.","sentences":["Exactly solvable models are interesting for science and education, since they help in scientific search and in understanding of phenomena.","Some exact solutions for simple quantum-mechanical models are considered.","The models include two barriers, combinations of barrier pairs, three barriers, three wells etc.","The model of two barriers can predict some interesting phenomena in the one-dimensional case.","Clearing of wave and quantum-mechanical barriers (including reflection-free passage) is an important problem of physics.","The rigorous equations for the transmission and reflection coefficients are derived.","Barriers in substances are combined into associations, where the bond within each association is stronger than bonds between associations.","Some properties of disordered media (the transparency of glasses, the conductivity of alloys or melts, the brittleness, or ductility, etc.) can be qualitatively understood from this viewpoint.","The same material can exhibit various properties: transparent and opaque, metallic and non-metallic, ordered and disordered, and so on.","Such transitions can occur under pressure.","A model of the three-well potential can be applied to the phenomena under consideration.","Some remarks on 3-D cases are made."],"url":"http://arxiv.org/abs/2402.05603v1","category":"quant-ph"}
{"created":"2024-02-08 11:37:13","title":"Neural functional a posteriori error estimates","abstract":"We propose a new loss function for supervised and physics-informed training of neural networks and operators that incorporates a posteriori error estimate. More specifically, during the training stage, the neural network learns additional physical fields that lead to rigorous error majorants after a computationally cheap postprocessing stage. Theoretical results are based upon the theory of functional a posteriori error estimates, which allows for the systematic construction of such loss functions for a diverse class of practically relevant partial differential equations. From the numerical side, we demonstrate on a series of elliptic problems that for a variety of architectures and approaches (physics-informed neural networks, physics-informed neural operators, neural operators, and classical architectures in the regression and physics-informed settings), we can reach better or comparable accuracy and in addition to that cheaply recover high-quality upper bounds on the error after training.","sentences":["We propose a new loss function for supervised and physics-informed training of neural networks and operators that incorporates a posteriori error estimate.","More specifically, during the training stage, the neural network learns additional physical fields that lead to rigorous error majorants after a computationally cheap postprocessing stage.","Theoretical results are based upon the theory of functional a posteriori error estimates, which allows for the systematic construction of such loss functions for a diverse class of practically relevant partial differential equations.","From the numerical side, we demonstrate on a series of elliptic problems that for a variety of architectures and approaches (physics-informed neural networks, physics-informed neural operators, neural operators, and classical architectures in the regression and physics-informed settings), we can reach better or comparable accuracy and in addition to that cheaply recover high-quality upper bounds on the error after training."],"url":"http://arxiv.org/abs/2402.05585v1","category":"math.NA"}
{"created":"2024-02-08 11:31:23","title":"Establishing degrees of closeness between audio recordings along different dimensions using large-scale cross-lingual models","abstract":"In the highly constrained context of low-resource language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal. We propose a new unsupervised method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations. ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic. Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects. The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines. Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information. The method is fully unsupervised, potentially opening new research avenues for comparative work on under-documented languages.","sentences":["In the highly constrained context of low-resource language studies, we explore vector representations of speech from a pretrained model to determine their level of abstraction with regard to the audio signal.","We propose a new unsupervised method using ABX tests on audio recordings with carefully curated metadata to shed light on the type of information present in the representations.","ABX tests determine whether the representations computed by a multilingual speech model encode a given characteristic.","Three experiments are devised: one on room acoustics aspects, one on linguistic genre, and one on phonetic aspects.","The results confirm that the representations extracted from recordings with different linguistic/extra-linguistic characteristics differ along the same lines.","Embedding more audio signal in one vector better discriminates extra-linguistic characteristics, whereas shorter snippets are better to distinguish segmental information.","The method is fully unsupervised, potentially opening new research avenues for comparative work on under-documented languages."],"url":"http://arxiv.org/abs/2402.05581v1","category":"cs.CL"}
{"created":"2024-02-08 11:29:53","title":"Improved long time existence for the Willmore flow of surfaces of revolution with Dirichlet data","abstract":"To avoid possible singularities in the Willmore flow, one usually works under an energy threshold provided by the Li-Yau inequality. Here we improve this threshold by also considering parts outside of a possible singularity together with Dirichlet boundary data. We work in the class of surfaces of revolution.","sentences":["To avoid possible singularities in the Willmore flow, one usually works under an energy threshold provided by the Li-Yau inequality.","Here we improve this threshold by also considering parts outside of a possible singularity together with Dirichlet boundary data.","We work in the class of surfaces of revolution."],"url":"http://arxiv.org/abs/2402.05580v1","category":"math.AP"}
{"created":"2024-02-08 11:18:36","title":"Photo-Activated, Solid-State Introduction of Luminescent Oxygen Defects into Semiconducting Single-Walled Carbon Nanotubes","abstract":"Oxygen defects in semiconducting single-walled carbon nanotubes (SWCNTs) are localized disruptions in the carbon lattice caused by the formation of epoxy or ether groups, commonly through wet-chemical reactions. The associated modifications of the electronic structure can result in luminescent states with emission energies below those of pristine SWCNTs in the near-infrared range, which makes them promising candidates for applications in biosensing and as single-photon emitters. Here, we demonstrate the controlled introduction of luminescent oxygen defects into networks of monochiral (6,5) SWCNTs using a solid-state photocatalytic approach. UV irradiation of SWCNTs on the photoreactive surfaces of the transition metal oxides TiOx and ZnOx in the presence of trace amounts of water and oxygen results in the creation of reactive oxygen species that initiate radical reactions with the carbon lattice and the formation of oxygen defects. The created ether-d and epoxide-l defect configurations give rise to two distinct red-shifted emissive features. The chemical and dielectric properties of the photoactive oxides influence the final defect emission properties, with oxygen-functionalized SWCNTs on TiOx substrates being brighter than those on ZnOx or pristine SWCNTs on glass. The photoinduced functionalization of nanotubes is further employed to create lateral patterns of oxygen defects in (6,5) SWCNT networks with micrometer resolution and thus spatially controlled defect emission.","sentences":["Oxygen defects in semiconducting single-walled carbon nanotubes (SWCNTs) are localized disruptions in the carbon lattice caused by the formation of epoxy or ether groups, commonly through wet-chemical reactions.","The associated modifications of the electronic structure can result in luminescent states with emission energies below those of pristine SWCNTs in the near-infrared range, which makes them promising candidates for applications in biosensing and as single-photon emitters.","Here, we demonstrate the controlled introduction of luminescent oxygen defects into networks of monochiral (6,5) SWCNTs using a solid-state photocatalytic approach.","UV irradiation of SWCNTs on the photoreactive surfaces of the transition metal oxides TiOx and ZnOx in the presence of trace amounts of water and oxygen results in the creation of reactive oxygen species that initiate radical reactions with the carbon lattice and the formation of oxygen defects.","The created ether-d and epoxide-l defect configurations give rise to two distinct red-shifted emissive features.","The chemical and dielectric properties of the photoactive oxides influence the final defect emission properties, with oxygen-functionalized SWCNTs on TiOx substrates being brighter than those on ZnOx or pristine SWCNTs on glass.","The photoinduced functionalization of nanotubes is further employed to create lateral patterns of oxygen defects in (6,5) SWCNT networks with micrometer resolution and thus spatially controlled defect emission."],"url":"http://arxiv.org/abs/2402.05572v1","category":"physics.app-ph"}
{"created":"2024-02-08 11:01:32","title":"Uncertainty calibration for probabilistic projection methods","abstract":"Classical Krylov subspace projection methods for the solution of linear problem $Ax = b$ output an approximate solution $\\widetilde{x}\\simeq x$. Recently, it has been recognized that projection methods can be understood from a statistical perspective. These probabilistic projection methods return a distribution $p(\\widetilde{x})$ in place of a point estimate $\\widetilde{x}$. The resulting uncertainty, codified as a distribution, can, in theory, be meaningfully combined with other uncertainties, can be propagated through computational pipelines, and can be used in the framework of probabilistic decision theory. The problem we address is that the current probabilistic projection methods lead to the poorly calibrated posterior distribution. We improve the covariance matrix from previous works in a way that it does not contain such undesirable objects as $A^{-1}$ or $A^{-1}A^{-T}$, results in nontrivial uncertainty, and reproduces an arbitrary projection method as a mean of the posterior distribution. We also propose a variant that is numerically inexpensive in the case the uncertainty is calibrated a priori. Since it usually is not, we put forward a practical way to calibrate uncertainty that performs reasonably well, albeit at the expense of roughly doubling the numerical cost of the underlying projection method.","sentences":["Classical Krylov subspace projection methods for the solution of linear problem $Ax = b$ output an approximate solution $\\widetilde{x}\\simeq x$.","Recently, it has been recognized that projection methods can be understood from a statistical perspective.","These probabilistic projection methods return a distribution $p(\\widetilde{x})$ in place of a point estimate $\\widetilde{x}$. The resulting uncertainty, codified as a distribution, can, in theory, be meaningfully combined with other uncertainties, can be propagated through computational pipelines, and can be used in the framework of probabilistic decision theory.","The problem we address is that the current probabilistic projection methods lead to the poorly calibrated posterior distribution.","We improve the covariance matrix from previous works in a way that it does not contain such undesirable objects as $A^{-1}$ or $A^{-1}A^{-T}$, results in nontrivial uncertainty, and reproduces an arbitrary projection method as a mean of the posterior distribution.","We also propose a variant that is numerically inexpensive in the case the uncertainty is calibrated a priori.","Since it usually is not, we put forward a practical way to calibrate uncertainty that performs reasonably well, albeit at the expense of roughly doubling the numerical cost of the underlying projection method."],"url":"http://arxiv.org/abs/2402.05562v1","category":"math.NA"}
{"created":"2024-02-08 09:31:28","title":"SeAr PC: Sensitivity Enhanced Arbitrary Polynomial Chaos","abstract":"This paper presents a method for performing Uncertainty Quantification in high-dimensional uncertain spaces by combining arbitrary polynomial chaos with a recently proposed scheme for sensitivity enhancement (1). Including available sensitivity information offers a way to mitigate the curse of dimensionality in Polynomial Chaos Expansions (PCEs). Coupling the sensitivity enhancement to arbitrary Polynomial Chaos allows the formulation to be extended to a wide range of stochastic processes, including multi-modal, fat-tailed, and truncated probability distributions. In so doing, this work addresses two of the barriers to widespread industrial application of PCEs. The method is demonstrated for a number of synthetic test cases, including an uncertainty analysis of a Finite Element structure, determined using Topology Optimisation, with 306 uncertain inputs. We demonstrate that by exploiting sensitivity information, PCEs can feasibly be applied to such problems and through the Sobol sensitivity indices, can allow a designer to easily visualise the spatial distribution of the contributions to uncertainty in the structure.","sentences":["This paper presents a method for performing Uncertainty Quantification in high-dimensional uncertain spaces by combining arbitrary polynomial chaos with a recently proposed scheme for sensitivity enhancement (1).","Including available sensitivity information offers a way to mitigate the curse of dimensionality in Polynomial Chaos Expansions (PCEs).","Coupling the sensitivity enhancement to arbitrary Polynomial Chaos allows the formulation to be extended to a wide range of stochastic processes, including multi-modal, fat-tailed, and truncated probability distributions.","In so doing, this work addresses two of the barriers to widespread industrial application of PCEs.","The method is demonstrated for a number of synthetic test cases, including an uncertainty analysis of a Finite Element structure, determined using Topology Optimisation, with 306 uncertain inputs.","We demonstrate that by exploiting sensitivity information, PCEs can feasibly be applied to such problems and through the Sobol sensitivity indices, can allow a designer to easily visualise the spatial distribution of the contributions to uncertainty in the structure."],"url":"http://arxiv.org/abs/2402.05507v1","category":"math.NA"}
{"created":"2024-02-08 09:19:26","title":"Machine Learning Augmented Branch and Bound for Mixed Integer Linear Programming","abstract":"Mixed Integer Linear Programming (MILP) is a pillar of mathematical optimization that offers a powerful modeling language for a wide range of applications. During the past decades, enormous algorithmic progress has been made in solving MILPs, and many commercial and academic software packages exist. Nevertheless, the availability of data, both from problem instances and from solvers, and the desire to solve new problems and larger (real-life) instances, trigger the need for continuing algorithmic development. MILP solvers use branch and bound as their main component. In recent years, there has been an explosive development in the use of machine learning algorithms for enhancing all main tasks involved in the branch-and-bound algorithm, such as primal heuristics, branching, cutting planes, node selection and solver configuration decisions. This paper presents a survey of such approaches, addressing the vision of integration of machine learning and mathematical optimization as complementary technologies, and how this integration can benefit MILP solving. In particular, we give detailed attention to machine learning algorithms that automatically optimize some metric of branch-and-bound efficiency. We also address how to represent MILPs in the context of applying learning algorithms, MILP benchmarks and software.","sentences":["Mixed Integer Linear Programming (MILP) is a pillar of mathematical optimization that offers a powerful modeling language for a wide range of applications.","During the past decades, enormous algorithmic progress has been made in solving MILPs, and many commercial and academic software packages exist.","Nevertheless, the availability of data, both from problem instances and from solvers, and the desire to solve new problems and larger (real-life) instances, trigger the need for continuing algorithmic development.","MILP solvers use branch and bound as their main component.","In recent years, there has been an explosive development in the use of machine learning algorithms for enhancing all main tasks involved in the branch-and-bound algorithm, such as primal heuristics, branching, cutting planes, node selection and solver configuration decisions.","This paper presents a survey of such approaches, addressing the vision of integration of machine learning and mathematical optimization as complementary technologies, and how this integration can benefit MILP solving.","In particular, we give detailed attention to machine learning algorithms that automatically optimize some metric of branch-and-bound efficiency.","We also address how to represent MILPs in the context of applying learning algorithms, MILP benchmarks and software."],"url":"http://arxiv.org/abs/2402.05501v1","category":"math.OC"}
{"created":"2024-02-08 08:55:34","title":"Determining the severity of Parkinson's disease in patients using a multi task neural network","abstract":"Parkinson's disease is easy to diagnose when it is advanced, but it is very difficult to diagnose in its early stages. Early diagnosis is essential to be able to treat the symptoms. It impacts on daily activities and reduces the quality of life of both the patients and their families and it is also the second most prevalent neurodegenerative disorder after Alzheimer in people over the age of 60. Most current studies on the prediction of Parkinson's severity are carried out in advanced stages of the disease. In this work, the study analyzes a set of variables that can be easily extracted from voice analysis, making it a very non-intrusive technique. In this paper, a method based on different deep learning techniques is proposed with two purposes. On the one hand, to find out if a person has severe or non-severe Parkinson's disease, and on the other hand, to determine by means of regression techniques the degree of evolution of the disease in a given patient. The UPDRS (Unified Parkinson's Disease Rating Scale) has been used by taking into account both the motor and total labels, and the best results have been obtained using a mixed multi-layer perceptron (MLP) that classifies and regresses at the same time and the most important features of the data obtained are taken as input, using an autoencoder. A success rate of 99.15% has been achieved in the problem of predicting whether a person suffers from severe Parkinson's disease or non-severe Parkinson's disease. In the degree of disease involvement prediction problem case, a MSE (Mean Squared Error) of 0.15 has been obtained. Using a full deep learning pipeline for data preprocessing and classification has proven to be very promising in the field Parkinson's outperforming the state-of-the-art proposals.","sentences":["Parkinson's disease is easy to diagnose when it is advanced, but it is very difficult to diagnose in its early stages.","Early diagnosis is essential to be able to treat the symptoms.","It impacts on daily activities and reduces the quality of life of both the patients and their families and it is also the second most prevalent neurodegenerative disorder after Alzheimer in people over the age of 60.","Most current studies on the prediction of Parkinson's severity are carried out in advanced stages of the disease.","In this work, the study analyzes a set of variables that can be easily extracted from voice analysis, making it a very non-intrusive technique.","In this paper, a method based on different deep learning techniques is proposed with two purposes.","On the one hand, to find out if a person has severe or non-severe Parkinson's disease, and on the other hand, to determine by means of regression techniques the degree of evolution of the disease in a given patient.","The UPDRS (Unified Parkinson's Disease Rating Scale) has been used by taking into account both the motor and total labels, and the best results have been obtained using a mixed multi-layer perceptron (MLP) that classifies and regresses at the same time and the most important features of the data obtained are taken as input, using an autoencoder.","A success rate of 99.15% has been achieved in the problem of predicting whether a person suffers from severe Parkinson's disease or non-severe Parkinson's disease.","In the degree of disease involvement prediction problem case, a MSE (Mean Squared Error) of 0.15 has been obtained.","Using a full deep learning pipeline for data preprocessing and classification has proven to be very promising in the field Parkinson's outperforming the state-of-the-art proposals."],"url":"http://arxiv.org/abs/2402.05491v1","category":"cs.LG"}
{"created":"2024-02-08 08:41:31","title":"On decoupled standard random walks","abstract":"Let $S_{n}=\\sum_{k=1}^{n}\\xi_{k}$, $n\\in\\mathbb{N}$, be a standard random walk with i.i.d. nonnegative increments $\\xi_{1},\\xi_{2},\\ldots$ and associated renewal counting process $N(t)=\\sum_{n\\ge 1}1_{\\{S_{n}\\le t\\}}$, $t\\ge 0$. A decoupling of $(S_{n})_{n\\ge 1}$ is any sequence $\\hat{S}_{1}$, $\\hat{S}_{2},\\ldots$ of independent random variables such that, for each $n\\in\\mathbb{N}$, $\\hat{S}_{n}$ and $S_{n}$ have the same law. Under the assumption that the law of $\\hat{S}_{1}$ belongs to the domain of attraction of a stable law with finite mean, we prove a functional limit theorem for the \\emph{decoupled renewal counting process} $\\hat{N}(t)=\\sum_{n\\ge 1}1_{\\{\\hat{S}_{n}\\le t\\}}$, $t\\ge 0$, after proper scaling, centering and normalization. We also study the asymptotics of $\\log \\mathbb{P}\\{\\min_{n\\ge 1}\\hat{S}_{n}>t\\}$ as $t\\to\\infty$ under varying assumptions on the law of $\\hat{S}_{1}$. In particular, we recover the assertions which were previously known in the case when $\\hat{S}_{1}$ has an exponential law. These results, which were formulated in terms of an infinite Ginibre point process, served as an initial motivation for the present work. Finally, we prove strong law of large numbers type results for the sequence of decoupled maxima $M_{n}=\\max_{1\\le k\\le n}\\hat{S}_{k}$, $n\\in\\mathbb{N}$, and the related first passage time process $\\hat\\tau(t)=\\inf\\{n\\in\\mathbb{N}: M_{n}>t\\}$, $t\\ge 0$. In particular, we provide a tail condition on the law of $\\hat{S}_{1}$ in the case when the latter has finite mean but infinite variance that implies $\\lim_{t\\to\\infty}t^{-1}\\hat\\tau(t)=\\lim_{t\\to\\infty}t^{-1}\\mathbb{E}\\hat\\tau(t)=0$. In other words, $t^{-1}\\hat\\tau(t)$ may exhibit a different limit behavior than $t^{-1}\\tau(t)$, where $\\tau(t)$ denotes the level-$t$ first passage time of $(S_{n})_{n\\ge 1}$.","sentences":["Let $S_{n}=\\sum_{k=1}^{n}\\xi_{k}$, $n\\in\\mathbb{N}$, be a standard random walk with i.i.d.","nonnegative increments $\\xi_{1},\\xi_{2},\\ldots$ and associated renewal counting process $N(t)=\\sum_{n\\ge 1}1_{\\{S_{n}\\le t\\}}$, $t\\ge 0$.","A decoupling of $(S_{n})_{n\\ge 1}$ is any sequence $\\hat{S}_{1}$, $\\hat{S}_{2},\\ldots$ of independent random variables such that, for each $n\\in\\mathbb{N}$, $\\hat{S}_{n}$ and $S_{n}$ have the same law.","Under the assumption that the law of $\\hat{S}_{1}$ belongs to the domain of attraction of a stable law with finite mean, we prove a functional limit theorem for the \\emph{decoupled renewal counting process} $\\hat{N}(t)=\\sum_{n\\ge 1}1_{\\{\\hat{S}_{n}\\le t\\}}$, $t\\ge 0$, after proper scaling, centering and normalization.","We also study the asymptotics of $\\log \\mathbb{P}\\{\\min_{n\\ge 1}\\hat{S}_{n}>t\\}$ as $t\\to\\infty$ under varying assumptions on the law of $\\hat{S}_{1}$. In particular, we recover the assertions which were previously known in the case when $\\hat{S}_{1}$ has an exponential law.","These results, which were formulated in terms of an infinite Ginibre point process, served as an initial motivation for the present work.","Finally, we prove strong law of large numbers type results for the sequence of decoupled maxima $M_{n}=\\max_{1\\le k\\le n}\\hat{S}_{k}$, $n\\in\\mathbb{N}$, and the related first passage time process $\\hat\\tau(t)=\\inf\\{n\\in\\mathbb{N}: M_{n}>t\\}$, $t\\ge 0$.","In particular, we provide a tail condition on the law of $\\hat{S}_{1}$ in the case when the latter has finite mean but infinite variance that implies $\\lim_{t\\to\\infty}t^{-1}\\hat\\tau(t)=\\lim_{t\\to\\infty}t^{-1}\\mathbb{E}\\hat\\tau(t)=0$. In other words, $t^{-1}\\hat\\tau(t)$ may exhibit a different limit behavior than $t^{-1}\\tau(t)$, where $\\tau(t)$ denotes the level-$t$ first passage time of $(S_{n})_{n\\ge 1}$."],"url":"http://arxiv.org/abs/2402.05488v1","category":"math.PR"}
{"created":"2024-02-08 07:46:26","title":"Hot-wire based estimation of pressure fluctuations in the near field of a jet in the presence of a co-flow","abstract":"It is shown that the velocity fluctuation spectra measured using a hot-wire in the potential flow region of a turbulent jet near field in the presence of a co-flow can be converted into the spectra of pressure fluctuations. The proposed conversion method is based on the fact that the structure of instability waves, which make a decisive contribution to the jet near-field fluctuations, resembles homogeneous one-dimensional waves, which makes it possible to locally link the pressure fluctuations and the fluctuations of the streamwise velocity component measured by a hot-wire.","sentences":["It is shown that the velocity fluctuation spectra measured using a hot-wire in the potential flow region of a turbulent jet near field in the presence of a co-flow can be converted into the spectra of pressure fluctuations.","The proposed conversion method is based on the fact that the structure of instability waves, which make a decisive contribution to the jet near-field fluctuations, resembles homogeneous one-dimensional waves, which makes it possible to locally link the pressure fluctuations and the fluctuations of the streamwise velocity component measured by a hot-wire."],"url":"http://arxiv.org/abs/2402.05463v1","category":"physics.flu-dyn"}
{"created":"2024-02-08 06:53:31","title":"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention","abstract":"The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware. However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA. This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention. The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information. Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods. The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IRQLoRA. We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains. The code is available at https://github.com/htqin/ir-qlora.","sentences":["The LoRA-finetuning quantization of LLMs has been extensively studied to obtain accurate yet compact LLMs for deployment on resource-constrained hardware.","However, existing methods cause the quantized LLM to severely degrade and even fail to benefit from the finetuning of LoRA.","This paper proposes a novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate through information retention.","The proposed IR-QLoRA mainly relies on two technologies derived from the perspective of unified information: (1) statistics-based Information Calibration Quantization allows the quantized parameters of LLM to retain original information accurately; (2) finetuning-based Information Elastic Connection makes LoRA utilizes elastic representation transformation with diverse information.","Comprehensive experiments show that IR-QLoRA can significantly improve accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4% improvement on MMLU compared with the state-of-the-art methods.","The significant performance gain requires only a tiny 0.31% additional time consumption, revealing the satisfactory efficiency of our IRQLoRA.","We highlight that IR-QLoRA enjoys excellent versatility, compatible with various frameworks (e.g., NormalFloat and Integer quantization) and brings general accuracy gains.","The code is available at https://github.com/htqin/ir-qlora."],"url":"http://arxiv.org/abs/2402.05445v1","category":"cs.LG"}
{"created":"2024-02-08 06:32:06","title":"Learning Uncertainty-Aware Temporally-Extended Actions","abstract":"In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions. However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated. This issue often negates the advantages of action repetition. To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE). UTE employs ensemble methods to accurately measure uncertainty during action extension. This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs. We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments. Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhancing policy learning efficiency.","sentences":["In reinforcement learning, temporal abstraction in the action space, exemplified by action repetition, is a technique to facilitate policy learning through extended actions.","However, a primary limitation in previous studies of action repetition is its potential to degrade performance, particularly when sub-optimal actions are repeated.","This issue often negates the advantages of action repetition.","To address this, we propose a novel algorithm named Uncertainty-aware Temporal Extension (UTE).","UTE employs ensemble methods to accurately measure uncertainty during action extension.","This feature allows policies to strategically choose between emphasizing exploration or adopting an uncertainty-averse approach, tailored to their specific needs.","We demonstrate the effectiveness of UTE through experiments in Gridworld and Atari 2600 environments.","Our findings show that UTE outperforms existing action repetition algorithms, effectively mitigating their inherent limitations and significantly enhancing policy learning efficiency."],"url":"http://arxiv.org/abs/2402.05439v1","category":"cs.LG"}
{"created":"2024-02-08 05:29:04","title":"Memory-efficient deep end-to-end posterior network (DEEPEN) for inverse problems","abstract":"End-to-End (E2E) unrolled optimization frameworks show promise for Magnetic Resonance (MR) image recovery, but suffer from high memory usage during training. In addition, these deterministic approaches do not offer opportunities for sampling from the posterior distribution. In this paper, we introduce a memory-efficient approach for E2E learning of the posterior distribution. We represent this distribution as the combination of a data-consistency-induced likelihood term and an energy model for the prior, parameterized by a Convolutional Neural Network (CNN). The CNN weights are learned from training data in an E2E fashion using maximum likelihood optimization. The learned model enables the recovery of images from undersampled measurements using the Maximum A Posteriori (MAP) optimization. In addition, the posterior model can be sampled to derive uncertainty maps about the reconstruction. Experiments on parallel MR image reconstruction show that our approach performs comparable to the memory-intensive E2E unrolled algorithm, performs better than its memory-efficient counterpart, and can provide uncertainty maps. Our framework paves the way towards MR image reconstruction in 3D and higher dimensions","sentences":["End-to-End (E2E) unrolled optimization frameworks show promise for Magnetic Resonance (MR) image recovery, but suffer from high memory usage during training.","In addition, these deterministic approaches do not offer opportunities for sampling from the posterior distribution.","In this paper, we introduce a memory-efficient approach for E2E learning of the posterior distribution.","We represent this distribution as the combination of a data-consistency-induced likelihood term and an energy model for the prior, parameterized by a Convolutional Neural Network (CNN).","The CNN weights are learned from training data in an E2E fashion using maximum likelihood optimization.","The learned model enables the recovery of images from undersampled measurements using the Maximum A Posteriori (MAP) optimization.","In addition, the posterior model can be sampled to derive uncertainty maps about the reconstruction.","Experiments on parallel MR image reconstruction show that our approach performs comparable to the memory-intensive E2E unrolled algorithm, performs better than its memory-efficient counterpart, and can provide uncertainty maps.","Our framework paves the way towards MR image reconstruction in 3D and higher dimensions"],"url":"http://arxiv.org/abs/2402.05422v1","category":"eess.IV"}
{"created":"2024-02-08 05:06:23","title":"Linearised Second Law for Higher Curvature Gravity and Non-Minimally Coupled Vector Fields","abstract":"Expanding the work of arXiv:1504.08040, we show that black holes obey a second law for linear perturbations to bifurcate Killing horizons, in any covariant higher curvature gravity coupled to scalar and vector fields. The vector fields do not need to be gauged, and (like the scalars) can have arbitrary non-minimal couplings to the metric. The increasing entropy has a natural expression in covariant phase space language, which makes it manifestly invariant under JKM ambiguities. An explicit entropy formula is given for f(Riemann) gravity coupled to vectors, where at most one derivative acts on each vector. Besides the previously known curvature terms, there are three extra terms involving differentiating the Lagrangian by the symmetric vector derivative (which therefore vanish for gauge fields).","sentences":["Expanding the work of arXiv:1504.08040, we show that black holes obey a second law for linear perturbations to bifurcate Killing horizons, in any covariant higher curvature gravity coupled to scalar and vector fields.","The vector fields do not need to be gauged, and (like the scalars) can have arbitrary non-minimal couplings to the metric.","The increasing entropy has a natural expression in covariant phase space language, which makes it manifestly invariant under JKM ambiguities.","An explicit entropy formula is given for f(Riemann) gravity coupled to vectors, where at most one derivative acts on each vector.","Besides the previously known curvature terms, there are three extra terms involving differentiating the Lagrangian by the symmetric vector derivative (which therefore vanish for gauge fields)."],"url":"http://arxiv.org/abs/2402.05411v1","category":"gr-qc"}
{"created":"2024-02-08 04:48:26","title":"Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes","abstract":"Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning methods requiring comparable resources as Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000 that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM leaderboard.","sentences":["Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size.","Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target.","In this work, we explore the problem of structured pruning of LLMs using only forward passes.","We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference.","We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   ","We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning methods requiring comparable resources as Bonsai.","We also leverage Bonsai to produce a new sub-2B model using a single A6000 that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM leaderboard."],"url":"http://arxiv.org/abs/2402.05406v1","category":"cs.LG"}
{"created":"2024-02-08 04:35:09","title":"Adaptive Activation Functions for Predictive Modeling with Sparse Experimental Data","abstract":"A pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns. While the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability. This research aims to address these gaps by investigating the use of two types of adaptive activation functions. These functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances. Our investigation reveals that adaptive activation functions, such as Exponential Linear Unit (ELU) and Softplus, with individual trainable parameters, result in accurate and confident prediction models that outperform fixed-shape activation functions and the less flexible method of using identical trainable activation functions in a hidden layer. Therefore, this work presents an elegant way of facilitating the design of adaptive neural networks in scientific and engineering problems.","sentences":["A pivotal aspect in the design of neural networks lies in selecting activation functions, crucial for introducing nonlinear structures that capture intricate input-output patterns.","While the effectiveness of adaptive or trainable activation functions has been studied in domains with ample data, like image classification problems, significant gaps persist in understanding their influence on classification accuracy and predictive uncertainty in settings characterized by limited data availability.","This research aims to address these gaps by investigating the use of two types of adaptive activation functions.","These functions incorporate shared and individual trainable parameters per hidden layer and are examined in three testbeds derived from additive manufacturing problems containing fewer than one hundred training instances.","Our investigation reveals that adaptive activation functions, such as Exponential Linear Unit (ELU) and Softplus, with individual trainable parameters, result in accurate and confident prediction models that outperform fixed-shape activation functions and the less flexible method of using identical trainable activation functions in a hidden layer.","Therefore, this work presents an elegant way of facilitating the design of adaptive neural networks in scientific and engineering problems."],"url":"http://arxiv.org/abs/2402.05401v1","category":"cs.LG"}
{"created":"2024-02-08 04:31:21","title":"Optimizing for ROC Curves on Class-Imbalanced Data by Training over a Family of Loss Functions","abstract":"Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem. Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods. While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance. To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function. We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem. Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our method improves model performance and is more robust to hyperparameter choices. Code will be made available at: https://github.com/klieberman/roc_lct.","sentences":["Although binary classification is a well-studied problem in computer vision, training reliable classifiers under severe class imbalance remains a challenging problem.","Recent work has proposed techniques that mitigate the effects of training under imbalance by modifying the loss functions or optimization methods.","While this work has led to significant improvements in the overall accuracy in the multi-class case, we observe that slight changes in hyperparameter values of these methods can result in highly variable performance in terms of Receiver Operating Characteristic (ROC) curves on binary problems with severe imbalance.","To reduce the sensitivity to hyperparameter choices and train more general models, we propose training over a family of loss functions, instead of a single loss function.","We develop a method for applying Loss Conditional Training (LCT) to an imbalanced classification problem.","Extensive experiment results, on both CIFAR and Kaggle competition datasets, show that our method improves model performance and is more robust to hyperparameter choices.","Code will be made available at: https://github.com/klieberman/roc_lct."],"url":"http://arxiv.org/abs/2402.05400v1","category":"cs.LG"}
{"created":"2024-02-08 03:49:40","title":"Efficient Nonparametric Inference of Causal Mediation Effects with Nonignorable Missing Confounders","abstract":"We consider causal mediation analysis with confounders subject to nonignorable missingness in a nonparametric framework. Our approach relies on shadow variables that are associated with the missing confounders but independent of the missingness mechanism. The mediation effect of interest is shown to be a weighted average of an iterated conditional expectation, which motivates our Sieve-based Iterative Outward (SIO) estimator. We derive the rate of convergence and asymptotic normality of the SIO estimator, which do not suffer from the ill-posed inverse problem. Essentially, we show that the asymptotic normality is not affected by the slow convergence rate of nonparametric estimators of nuisance functions. Moreover, we demonstrate that our estimator is locally efficient and attains the semiparametric efficiency bound under certain conditions. We accurately depict the efficiency loss attributable to missingness and identify scenarios in which efficiency loss is absent. We also propose a stable and easy-to-implement approach to estimate asymptotic variance and construct confidence intervals for the mediation effects. Finally, we evaluate the finite-sample performance of our proposed approach through simulation studies, and apply it to the CFPS data to show its practical applicability.","sentences":["We consider causal mediation analysis with confounders subject to nonignorable missingness in a nonparametric framework.","Our approach relies on shadow variables that are associated with the missing confounders but independent of the missingness mechanism.","The mediation effect of interest is shown to be a weighted average of an iterated conditional expectation, which motivates our Sieve-based Iterative Outward (SIO) estimator.","We derive the rate of convergence and asymptotic normality of the SIO estimator, which do not suffer from the ill-posed inverse problem.","Essentially, we show that the asymptotic normality is not affected by the slow convergence rate of nonparametric estimators of nuisance functions.","Moreover, we demonstrate that our estimator is locally efficient and attains the semiparametric efficiency bound under certain conditions.","We accurately depict the efficiency loss attributable to missingness and identify scenarios in which efficiency loss is absent.","We also propose a stable and easy-to-implement approach to estimate asymptotic variance and construct confidence intervals for the mediation effects.","Finally, we evaluate the finite-sample performance of our proposed approach through simulation studies, and apply it to the CFPS data to show its practical applicability."],"url":"http://arxiv.org/abs/2402.05384v1","category":"stat.ME"}
{"created":"2024-02-08 03:02:59","title":"Reduced-order modeling of unsteady fluid flow using neural network ensembles","abstract":"The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models. Convolutional autoencoders (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations. When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables. Long short-term memory (LSTM) networks, a type of recurrent neural network useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction. When making predictions at unseen design points over long time horizons, error propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies. In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM framework referred to as the CAE-eLSTM ROM that uses CAEs for spatial reconstruction of the full-order model and LSTM ensembles for time-series prediction. When applied to two unsteady fluid dynamics problems, our results show that the presented framework effectively reduces error propagation and leads to more accurate time-series prediction of latent variables at unseen points.","sentences":["The use of deep learning has become increasingly popular in reduced-order models (ROMs) to obtain low-dimensional representations of full-order models.","Convolutional autoencoders (CAEs) are often used to this end as they are adept at handling data that are spatially distributed, including solutions to partial differential equations.","When applied to unsteady physics problems, ROMs also require a model for time-series prediction of the low-dimensional latent variables.","Long short-term memory (LSTM) networks, a type of recurrent neural network useful for modeling sequential data, are frequently employed in data-driven ROMs for autoregressive time-series prediction.","When making predictions at unseen design points over long time horizons, error propagation is a frequently encountered issue, where errors made early on can compound over time and lead to large inaccuracies.","In this work, we propose using bagging, a commonly used ensemble learning technique, to develop a fully data-driven ROM framework referred to as the CAE-eLSTM ROM that uses CAEs for spatial reconstruction of the full-order model and LSTM ensembles for time-series prediction.","When applied to two unsteady fluid dynamics problems, our results show that the presented framework effectively reduces error propagation and leads to more accurate time-series prediction of latent variables at unseen points."],"url":"http://arxiv.org/abs/2402.05372v1","category":"physics.flu-dyn"}
{"created":"2024-02-08 02:58:47","title":"Noise Contrastive Alignment of Language Models with Explicit Rewards","abstract":"User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs). Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given. In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations. Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data. Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories. By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction, while NCA optimizes absolute likelihood for each response. We apply our methods to align a 7B language model with a GPT-4 annotated reward dataset. Experimental results suggest that InfoNCA surpasses the DPO baseline in GPT-4 evaluations, while NCA enjoys better training stability with competitive performance.","sentences":["User intentions are typically formalized as evaluation rewards to be maximized when fine-tuning language models (LMs).","Existing alignment methods, such as Direct Preference Optimization (DPO), are mainly tailored for pairwise preference data where rewards are implicitly defined rather than explicitly given.","In this paper, we introduce a general framework for LM alignment, leveraging Noise Contrastive Estimation (NCE) to bridge the gap in handling reward datasets explicitly annotated with scalar evaluations.","Our framework comprises two parallel algorithms, NCA and InfoNCA, both enabling the direct extraction of an LM policy from reward data as well as preference data.","Notably, we show that the DPO loss is a special case of our proposed InfoNCA objective under pairwise preference settings, thereby integrating and extending current alignment theories.","By contrasting NCA and InfoNCA, we show that InfoNCA and DPO adjust relative likelihood across different responses to a single instruction, while NCA optimizes absolute likelihood for each response.","We apply our methods to align a 7B language model with a GPT-4 annotated reward dataset.","Experimental results suggest that InfoNCA surpasses the DPO baseline in GPT-4 evaluations, while NCA enjoys better training stability with competitive performance."],"url":"http://arxiv.org/abs/2402.05369v1","category":"cs.LG"}
{"created":"2024-02-08 02:45:57","title":"Low-Energy Theorems and Linearity Breaking in Anomalous Amplitudes","abstract":"This study seeks a better comprehension of anomalies by exploring (n+1)-point perturbative amplitudes in a 2n-dimensional framework. The involved structures combine axial and vector vertices into odd tensors. This configuration enables diverse expressions, considered identities at the integrand level. However, connecting them is not automatic after loop integration, as the divergent nature of amplitudes links to surface terms. The background to this subject is the conflict between the linearity of integration and the translational invariance observed in the context of anomalies. That makes it impossible to simultaneously satisfy all symmetry and linearity properties, constraints that arise through Ward identities and relations among Green functions. Using the method known as Implicit Regularization, we show that trace choices are a means to select the amount of anomaly contributions appearing in each symmetry relation. Such an idea appeared through recipes to take traces in recent works, but we introduce a more complete view. We also emphasize low-energy theorems of finite amplitudes as the source of these violations, proving that the total amount of anomaly remains fixed regardless of any choices.","sentences":["This study seeks a better comprehension of anomalies by exploring (n+1)-point perturbative amplitudes in a 2n-dimensional framework.","The involved structures combine axial and vector vertices into odd tensors.","This configuration enables diverse expressions, considered identities at the integrand level.","However, connecting them is not automatic after loop integration, as the divergent nature of amplitudes links to surface terms.","The background to this subject is the conflict between the linearity of integration and the translational invariance observed in the context of anomalies.","That makes it impossible to simultaneously satisfy all symmetry and linearity properties, constraints that arise through Ward identities and relations among Green functions.","Using the method known as Implicit Regularization, we show that trace choices are a means to select the amount of anomaly contributions appearing in each symmetry relation.","Such an idea appeared through recipes to take traces in recent works, but we introduce a more complete view.","We also emphasize low-energy theorems of finite amplitudes as the source of these violations, proving that the total amount of anomaly remains fixed regardless of any choices."],"url":"http://arxiv.org/abs/2402.05362v1","category":"hep-th"}
{"created":"2024-02-08 02:24:17","title":"Systematic investigation of trace anomaly contribution in nucleon mass","abstract":"In this work, under the framework of vector meson dominance model, the trace anomaly contribution value inside neutrons are extracted for the first time based on vector meson photoproduction data. Furthermore, we systematically compare and analyze the trace anomaly contributions of protons and neutrons. The results show that the trace anomaly contributions of protons and neutrons are close, which indirectly confirms that their internal structures and dynamic properties may have certain similarities. In addition, the main factors affecting the extraction of the trace anomaly contribution of nucleons are discussed in detail. This study not only provides a theoretical basis for us to better understand the source of nucleon mass, but also makes a useful exploration and discussion on how to extract the trace anomaly contribution of nucleon more accurately in the future.","sentences":["In this work, under the framework of vector meson dominance model, the trace anomaly contribution value inside neutrons are extracted for the first time based on vector meson photoproduction data.","Furthermore, we systematically compare and analyze the trace anomaly contributions of protons and neutrons.","The results show that the trace anomaly contributions of protons and neutrons are close, which indirectly confirms that their internal structures and dynamic properties may have certain similarities.","In addition, the main factors affecting the extraction of the trace anomaly contribution of nucleons are discussed in detail.","This study not only provides a theoretical basis for us to better understand the source of nucleon mass, but also makes a useful exploration and discussion on how to extract the trace anomaly contribution of nucleon more accurately in the future."],"url":"http://arxiv.org/abs/2402.05354v1","category":"hep-ph"}
{"created":"2024-02-08 02:18:07","title":"Absence of breakdown of ferrodark solitons exhibiting snake instability","abstract":"We investigate the dynamical stability and real time dynamics of the two-types of ferrodark solitons (FDSs) which occur as topological magnetic domain walls in the easy-plane phase of a quasi-two-dimensional (2D) ferromagnetic spin-1 Bose-Einstein condensate. The type-I FDS has positive inertial mass and exhibits a single dynamical instability that generates in plane spin winding, causing polar-core spin vortex dipoles. The positive inertial mass leads to the elastic oscillations of the soliton under transverse perturbations. The type-II FDS has negative inertial mass and exhibits a snake instability and a spin-twist instability, with the latter involving the generation of out of plane spin winding. Distinct from the normal dynamics of negative mass solitons under long wave length transverse perturbations, the snake instability does not lead to the type-II FDS breaking down. Instead, segments of the type-II FDS convert to type-I and mass vortex dipoles are produced. The resulting hybridized-chain of the two soliton types and vortices exhibits complex 2D soliton dynamics at long times while the vortices remain confined and the topological structure of a magnetic domain wall is preserved.","sentences":["We investigate the dynamical stability and real time dynamics of the two-types of ferrodark solitons (FDSs) which occur as topological magnetic domain walls in the easy-plane phase of a quasi-two-dimensional (2D) ferromagnetic spin-1","Bose-Einstein condensate.","The type-I FDS has positive inertial mass and exhibits a single dynamical instability that generates in plane spin winding, causing polar-core spin vortex dipoles.","The positive inertial mass leads to the elastic oscillations of the soliton under transverse perturbations.","The type-II FDS has negative inertial mass and exhibits a snake instability and a spin-twist instability, with the latter involving the generation of out of plane spin winding.","Distinct from the normal dynamics of negative mass solitons under long wave length transverse perturbations, the snake instability does not lead to the type-II FDS breaking down.","Instead, segments of the type-II FDS convert to type-I and mass vortex dipoles are produced.","The resulting hybridized-chain of the two soliton types and vortices exhibits complex 2D soliton dynamics at long times while the vortices remain confined and the topological structure of a magnetic domain wall is preserved."],"url":"http://arxiv.org/abs/2402.05351v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-08 02:01:36","title":"Scrapping The Web For Early Wildfire Detection","abstract":"Early wildfire detection is of the utmost importance to enable rapid response efforts, and thus minimize the negative impacts of wildfire spreads. To this end, we present \\Pyro, a web-scraping-based dataset composed of videos of wildfires from a network of cameras that were enhanced with manual bounding-box-level annotations. Our dataset was filtered based on a strategy to improve the quality and diversity of the data, reducing the final data to a set of 10,000 images. We ran experiments using a state-of-the-art object detection model and found out that the proposed dataset is challenging and its use in concordance with other public dataset helps to reach higher results overall. We will make our code and data publicly available.","sentences":["Early wildfire detection is of the utmost importance to enable rapid response efforts, and thus minimize the negative impacts of wildfire spreads.","To this end, we present \\Pyro, a web-scraping-based dataset composed of videos of wildfires from a network of cameras that were enhanced with manual bounding-box-level annotations.","Our dataset was filtered based on a strategy to improve the quality and diversity of the data, reducing the final data to a set of 10,000 images.","We ran experiments using a state-of-the-art object detection model and found out that the proposed dataset is challenging and its use in concordance with other public dataset helps to reach higher results overall.","We will make our code and data publicly available."],"url":"http://arxiv.org/abs/2402.05349v1","category":"cs.CV"}
{"created":"2024-02-08 01:50:59","title":"Are We Asking the Right Questions?: Designing for Community Stakeholders' Interactions with AI in Policing","abstract":"Research into recidivism risk prediction in the criminal legal system has garnered significant attention from HCI, critical algorithm studies, and the emerging field of human-AI decision-making. This study focuses on algorithmic crime mapping, a prevalent yet underexplored form of algorithmic decision support (ADS) in this context. We conducted experiments and follow-up interviews with 60 participants, including community members, technical experts, and law enforcement agents (LEAs), to explore how lived experiences, technical knowledge, and domain expertise shape interactions with the ADS, impacting human-AI decision-making. Surprisingly, we found that domain experts (LEAs) often exhibited anchoring bias, readily accepting and engaging with the first crime map presented to them. Conversely, community members and technical experts were more inclined to engage with the tool, adjust controls, and generate different maps. Our findings highlight that all three stakeholders were able to provide critical feedback regarding AI design and use - community members questioned the core motivation of the tool, technical experts drew attention to the elastic nature of data science practice, and LEAs suggested redesign pathways such that the tool could complement their domain expertise.","sentences":["Research into recidivism risk prediction in the criminal legal system has garnered significant attention from HCI, critical algorithm studies, and the emerging field of human-AI decision-making.","This study focuses on algorithmic crime mapping, a prevalent yet underexplored form of algorithmic decision support (ADS) in this context.","We conducted experiments and follow-up interviews with 60 participants, including community members, technical experts, and law enforcement agents (LEAs), to explore how lived experiences, technical knowledge, and domain expertise shape interactions with the ADS, impacting human-AI decision-making.","Surprisingly, we found that domain experts (LEAs) often exhibited anchoring bias, readily accepting and engaging with the first crime map presented to them.","Conversely, community members and technical experts were more inclined to engage with the tool, adjust controls, and generate different maps.","Our findings highlight that all three stakeholders were able to provide critical feedback regarding AI design and use - community members questioned the core motivation of the tool, technical experts drew attention to the elastic nature of data science practice, and LEAs suggested redesign pathways such that the tool could complement their domain expertise."],"url":"http://arxiv.org/abs/2402.05348v1","category":"cs.HC"}
{"created":"2024-02-08 01:46:58","title":"Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix Differential Equations","abstract":"In this work, we develop implicit rank-adaptive schemes for time-dependent matrix differential equations. The dynamic low rank approximation (DLRA) is a well-known technique to capture the dynamic low rank structure based on Dirac-Frenkel time-dependent variational principle. In recent years, it has attracted a lot of attention due to its wide applicability. Our schemes are inspired by the three-step procedure used in the rank adaptive version of the unconventional robust integrator (the so called BUG integrator) for DLRA. First, a prediction (basis update) step is made computing the approximate column and row spaces at the next time level. Second, a Galerkin evolution step is invoked using a base implicit solve for the small core matrix. Finally, a truncation is made according to a prescribed error threshold. Since the DLRA is evolving the differential equation projected on to the tangent space of the low rank manifold, the error estimate of the BUG integrator contains the tangent projection (modeling) error which cannot be easily controlled by mesh refinement. This can cause convergence issue for equations with cross terms.   To address this issue, we propose a simple modification, consisting of merging the row and column spaces from the explicit step truncation method together with the BUG spaces in the prediction step. In addition, we propose an adaptive strategy where the BUG spaces are only computed if the residual for the solution obtained from the prediction space by explicit step truncation method, is too large. We prove stability and estimate the local truncation error of the schemes under assumptions. We benchmark the schemes in several tests, such as anisotropic diffusion, solid body rotation and the combination of the two, to show robust convergence properties.","sentences":["In this work, we develop implicit rank-adaptive schemes for time-dependent matrix differential equations.","The dynamic low rank approximation (DLRA) is a well-known technique to capture the dynamic low rank structure based on Dirac-Frenkel time-dependent variational principle.","In recent years, it has attracted a lot of attention due to its wide applicability.","Our schemes are inspired by the three-step procedure used in the rank adaptive version of the unconventional robust integrator (the so called BUG integrator) for DLRA.","First, a prediction (basis update) step is made computing the approximate column and row spaces at the next time level.","Second, a Galerkin evolution step is invoked using a base implicit solve for the small core matrix.","Finally, a truncation is made according to a prescribed error threshold.","Since the DLRA is evolving the differential equation projected on to the tangent space of the low rank manifold, the error estimate of the BUG integrator contains the tangent projection (modeling) error which cannot be easily controlled by mesh refinement.","This can cause convergence issue for equations with cross terms.   ","To address this issue, we propose a simple modification, consisting of merging the row and column spaces from the explicit step truncation method together with the BUG spaces in the prediction step.","In addition, we propose an adaptive strategy where the BUG spaces are only computed if the residual for the solution obtained from the prediction space by explicit step truncation method, is too large.","We prove stability and estimate the local truncation error of the schemes under assumptions.","We benchmark the schemes in several tests, such as anisotropic diffusion, solid body rotation and the combination of the two, to show robust convergence properties."],"url":"http://arxiv.org/abs/2402.05347v1","category":"math.NA"}
{"created":"2024-02-08 01:30:06","title":"High-Precision Atmospheric Constraints for a Cool T Dwarf from JWST Spectroscopy","abstract":"We present observations of the T8 dwarf 2MASS 0415-0935 with JWST's NIRSpec spectrograph using the G395H grating ($\\sim$ 2.87 - 5.14 $\\mu$m). We perform the first atmospheric retrieval analysis at the maximum spectral resolution of NIRSpec (R$\\sim$2700) and combine the spectrum with previous observations to study the 0.9-20 $\\mu$m spectral energy distribution. We obtain precise constraints on chemical abundances ($\\sim$0.02 dex) for a number of species which complicate our understanding of disequilibrium chemistry, particularly for CO$_{2}$ and PH$_{3}$. Furthermore, we measure a $^{12}$CO/$^{13}$CO ratio of $\\sim 97^{+9}_{-8}$, making 2MASS 0415-0935 the coldest ($\\sim 760$ K) substellar object outside of our solar system with a measured $^{12}$CO/$^{13}$CO ratio. This work shows promise for similar observations with JWST to provide precise abundances of major chemical species as well as isotopologues, allowing for new tests of our understanding of the formation and atmospheres of substellar objects.","sentences":["We present observations of the T8 dwarf 2MASS 0415-0935 with JWST's NIRSpec spectrograph using the G395H grating ($\\sim$ 2.87 - 5.14 $\\mu$m).","We perform the first atmospheric retrieval analysis at the maximum spectral resolution of NIRSpec (R$\\sim$2700) and combine the spectrum with previous observations to study the 0.9-20","$\\mu$m spectral energy distribution.","We obtain precise constraints on chemical abundances ($\\sim$0.02 dex) for a number of species which complicate our understanding of disequilibrium chemistry, particularly for CO$_{2}$ and PH$_{3}$.","Furthermore, we measure a $^{12}$CO/$^{13}$CO ratio of $\\sim 97^{+9}_{-8}$, making 2MASS 0415-0935 the coldest ($\\sim 760$ K) substellar object outside of our solar system with a measured $^{12}$CO/$^{13}$CO ratio.","This work shows promise for similar observations with JWST to provide precise abundances of major chemical species as well as isotopologues, allowing for new tests of our understanding of the formation and atmospheres of substellar objects."],"url":"http://arxiv.org/abs/2402.05345v1","category":"astro-ph.SR"}
{"created":"2024-02-08 01:12:57","title":"Nonlinear Regression Analysis","abstract":"Nonlinear regression analysis is a popular and important tool for scientists and engineers. In this article, we introduce theories and methods of nonlinear regression and its statistical inferences using the frequentist and Bayesian statistical modeling and computation. Least squares with the Gauss-Newton method is the most widely used approach to parameters estimation. Under the assumption of normally distributed errors, maximum likelihood estimation is equivalent to least squares estimation. The Wald confidence regions for parameters in a nonlinear regression model are affected by the curvatures in the mean function. Furthermore, we introduce the Newton-Raphson method and the generalized least squares method to deal with variance heterogeneity. Examples of simulation data analysis are provided to illustrate important properties of confidence regions and the statistical inferences using the nonlinear least squares estimation and Bayesian inference.","sentences":["Nonlinear regression analysis is a popular and important tool for scientists and engineers.","In this article, we introduce theories and methods of nonlinear regression and its statistical inferences using the frequentist and Bayesian statistical modeling and computation.","Least squares with the Gauss-Newton method is the most widely used approach to parameters estimation.","Under the assumption of normally distributed errors, maximum likelihood estimation is equivalent to least squares estimation.","The Wald confidence regions for parameters in a nonlinear regression model are affected by the curvatures in the mean function.","Furthermore, we introduce the Newton-Raphson method and the generalized least squares method to deal with variance heterogeneity.","Examples of simulation data analysis are provided to illustrate important properties of confidence regions and the statistical inferences using the nonlinear least squares estimation and Bayesian inference."],"url":"http://arxiv.org/abs/2402.05342v1","category":"stat.ME"}
{"created":"2024-02-08 00:28:50","title":"A simple proof of existence of Lagrange multipliers","abstract":"In the seminal book M\\'echanique analitique, Lagrange, 1788, the notion of a Lagrange multiplier was first introduced in order to study a smooth minimization problem subject to equality constraints. The idea is that, under some regularity assumption, at a solution of the problem, one may associate a new variable (Lagrange multiplier) to each constraint such that an equilibrium equation is satisfied. This concept turned out to be central in studying more general constrained optimization problems and it has lead to the rapid development of nonlinear programming as a field of mathematics since the works of Karush and Kuhn-Tucker, who considered equality and inequality constraints. The usual proofs for the existence of Lagrange multipliers are somewhat cumbersome, relying on the implicit function theorem or duality theory. In the first section of this note we present an elementary proof of existence of Lagrange multipliers in the simplest context, which is easily accessible to a wide variety of readers. In addition, this proof is readily extended to the much more general context of conic constraints, which we present in the second section together with the background properties needed on the projection onto a closed and convex cone.","sentences":["In the seminal book M\\'echanique analitique, Lagrange, 1788, the notion of a Lagrange multiplier was first introduced in order to study a smooth minimization problem subject to equality constraints.","The idea is that, under some regularity assumption, at a solution of the problem, one may associate a new variable (Lagrange multiplier) to each constraint such that an equilibrium equation is satisfied.","This concept turned out to be central in studying more general constrained optimization problems and it has lead to the rapid development of nonlinear programming as a field of mathematics since the works of Karush and Kuhn-Tucker, who considered equality and inequality constraints.","The usual proofs for the existence of Lagrange multipliers are somewhat cumbersome, relying on the implicit function theorem or duality theory.","In the first section of this note we present an elementary proof of existence of Lagrange multipliers in the simplest context, which is easily accessible to a wide variety of readers.","In addition, this proof is readily extended to the much more general context of conic constraints, which we present in the second section together with the background properties needed on the projection onto a closed and convex cone."],"url":"http://arxiv.org/abs/2402.05335v1","category":"math.OC"}
{"created":"2024-02-08 00:25:30","title":"ML-Enabled Systems Model Deployment and Monitoring: Status Quo and Problems","abstract":"[Context] Systems incorporating Machine Learning (ML) models, often called ML-enabled systems, have become commonplace. However, empirical evidence on how ML-enabled systems are engineered in practice is still limited, especially for activities surrounding ML model dissemination. [Goal] We investigate contemporary industrial practices and problems related to ML model dissemination, focusing on the model deployment and the monitoring of ML life cycle phases. [Method] We conducted an international survey to gather practitioner insights on how ML-enabled systems are engineered. We gathered a total of 188 complete responses from 25 countries. We analyze the status quo and problems reported for the model deployment and monitoring phases. We analyzed contemporary practices using bootstrapping with confidence intervals and conducted qualitative analyses on the reported problems applying open and axial coding procedures. [Results] Practitioners perceive the model deployment and monitoring phases as relevant and difficult. With respect to model deployment, models are typically deployed as separate services, with limited adoption of MLOps principles. Reported problems include difficulties in designing the architecture of the infrastructure for production deployment and legacy application integration. Concerning model monitoring, many models in production are not monitored. The main monitored aspects are inputs, outputs, and decisions. Reported problems involve the absence of monitoring practices, the need to create custom monitoring tools, and the selection of suitable metrics. [Conclusion] Our results help provide a better understanding of the adopted practices and problems in practice and support guiding ML deployment and monitoring research in a problem-driven manner.","sentences":["[Context] Systems incorporating Machine Learning (ML) models, often called ML-enabled systems, have become commonplace.","However, empirical evidence on how ML-enabled systems are engineered in practice is still limited, especially for activities surrounding ML model dissemination.","[Goal] We investigate contemporary industrial practices and problems related to ML model dissemination, focusing on the model deployment and the monitoring of ML life cycle phases.","[Method] We conducted an international survey to gather practitioner insights on how ML-enabled systems are engineered.","We gathered a total of 188 complete responses from 25 countries.","We analyze the status quo and problems reported for the model deployment and monitoring phases.","We analyzed contemporary practices using bootstrapping with confidence intervals and conducted qualitative analyses on the reported problems applying open and axial coding procedures.","[Results] Practitioners perceive the model deployment and monitoring phases as relevant and difficult.","With respect to model deployment, models are typically deployed as separate services, with limited adoption of MLOps principles.","Reported problems include difficulties in designing the architecture of the infrastructure for production deployment and legacy application integration.","Concerning model monitoring, many models in production are not monitored.","The main monitored aspects are inputs, outputs, and decisions.","Reported problems involve the absence of monitoring practices, the need to create custom monitoring tools, and the selection of suitable metrics.","[Conclusion] Our results help provide a better understanding of the adopted practices and problems in practice and support guiding ML deployment and monitoring research in a problem-driven manner."],"url":"http://arxiv.org/abs/2402.05333v1","category":"cs.SE"}
{"created":"2024-02-08 00:12:18","title":"Classification under Nuisance Parameters and Generalized Label Shift in Likelihood-Free Inference","abstract":"An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data. We refer to this type of distributional shift as generalized label shift (GLS). Direct classification using observed data $\\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters. The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS. Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maintaining high power. We demonstrate its performance on two challenging scientific problems in biology and astroparticle physics with data from realistic mechanistic models.","sentences":["An open scientific challenge is how to classify events with reliable measures of uncertainty, when we have a mechanistic model of the data-generating process but the distribution over both labels and latent nuisance parameters is different between train and target data.","We refer to this type of distributional shift as generalized label shift (GLS).","Direct classification using observed data $\\mathbf{X}$ as covariates leads to biased predictions and invalid uncertainty estimates of labels $Y$. We overcome these biases by proposing a new method for robust uncertainty quantification that casts classification as a hypothesis testing problem under nuisance parameters.","The key idea is to estimate the classifier's receiver operating characteristic (ROC) across the entire nuisance parameter space, which allows us to devise cutoffs that are invariant under GLS.","Our method effectively endows a pre-trained classifier with domain adaptation capabilities and returns valid prediction sets while maintaining high power.","We demonstrate its performance on two challenging scientific problems in biology and astroparticle physics with data from realistic mechanistic models."],"url":"http://arxiv.org/abs/2402.05330v1","category":"stat.ML"}
{"created":"2024-02-08 00:10:05","title":"Selective linear segmentation for detecting relevant parameter changes","abstract":"Change-point processes are one flexible approach to model long time series. We propose a method to uncover which model parameter truly vary when a change-point is detected. Given a set of breakpoints, we use a penalized likelihood approach to select the best set of parameters that changes over time and we prove that the penalty function leads to a consistent selection of the true model. Estimation is carried out via the deterministic annealing expectation-maximization algorithm. Our method accounts for model selection uncertainty and associates a probability to all the possible time-varying parameter specifications. Monte Carlo simulations highlight that the method works well for many time series models including heteroskedastic processes. For a sample of 14 Hedge funds (HF) strategies, using an asset based style pricing model, we shed light on the promising ability of our method to detect the time-varying dynamics of risk exposures as well as to forecast HF returns.","sentences":["Change-point processes are one flexible approach to model long time series.","We propose a method to uncover which model parameter truly vary when a change-point is detected.","Given a set of breakpoints, we use a penalized likelihood approach to select the best set of parameters that changes over time and we prove that the penalty function leads to a consistent selection of the true model.","Estimation is carried out via the deterministic annealing expectation-maximization algorithm.","Our method accounts for model selection uncertainty and associates a probability to all the possible time-varying parameter specifications.","Monte Carlo simulations highlight that the method works well for many time series models including heteroskedastic processes.","For a sample of 14 Hedge funds (HF) strategies, using an asset based style pricing model, we shed light on the promising ability of our method to detect the time-varying dynamics of risk exposures as well as to forecast HF returns."],"url":"http://arxiv.org/abs/2402.05329v1","category":"econ.EM"}
{"created":"2024-02-08 00:07:15","title":"Two Simple Proofs of M\u00fcller's Theorem","abstract":"Due to M\\\"{u}ller's theorem, the Kolmogorov complexity of a string was shown to be equal to its quantum Kolmogorov complexity. Thus there are no benefits to using quantum mechanics to compress classical information. The quantitative amount of information in classical sources is invariant to the physical model used. These consequences make this theorem arguably the most important result in the intersection of algorithmic information theory and physics. The original proof is quite extensive. This paper contains two simple proofs of this theorem.","sentences":["Due to M\\\"{u}ller's theorem, the Kolmogorov complexity of a string was shown to be equal to its quantum Kolmogorov complexity.","Thus there are no benefits to using quantum mechanics to compress classical information.","The quantitative amount of information in classical sources is invariant to the physical model used.","These consequences make this theorem arguably the most important result in the intersection of algorithmic information theory and physics.","The original proof is quite extensive.","This paper contains two simple proofs of this theorem."],"url":"http://arxiv.org/abs/2402.05328v1","category":"cs.CC"}
{"created":"2024-02-07 23:58:39","title":"Structural modifications of GeO$_2$ glass under high pressure and high temperature","abstract":"Vitreous GeO$_2$ has been compressed at high temperature, to investigate the effect of thermal activation on the structural reorganization during compression. The measurements were performed in-situ using micro Raman spectroscopy under pressure up to 6 GPa and temperature up to 400$^\\circ$C. The evolution of the Raman shift of the main band (400-500 cm$^{-1}$) with temperature during compression evidences a pressure window around 3 GPa within which temperature has a remarkable influence on the structure, in particular the intermediate range order. We find that this change is well correlated with previous ex-situ density measurements from high pressure-high temperature densifications. Moreover, coordination changes from tetrahedrally (GeO$_4$) to octahedrally (GeO$_6$) coordinated GeO$_2$ are accelerated with the heating during the compression.","sentences":["Vitreous GeO$_2$ has been compressed at high temperature, to investigate the effect of thermal activation on the structural reorganization during compression.","The measurements were performed in-situ using micro Raman spectroscopy under pressure up to 6 GPa and temperature up to 400$^\\circ$C.","The evolution of the Raman shift of the main band (400-500 cm$^{-1}$) with temperature during compression evidences a pressure window around 3 GPa within which temperature has a remarkable influence on the structure, in particular the intermediate range order.","We find that this change is well correlated with previous ex-situ density measurements from high pressure-high temperature densifications.","Moreover, coordination changes from tetrahedrally (GeO$_4$) to octahedrally (GeO$_6$) coordinated GeO$_2$ are accelerated with the heating during the compression."],"url":"http://arxiv.org/abs/2402.05326v1","category":"physics.app-ph"}
{"created":"2024-02-07 23:44:42","title":"Assessment of models for nonlinear oscillatory flow through a hexagonal sphere pack","abstract":"We review models for unsteady porous media flow in the volume-averaging framework and we discuss the theoretical relations between the models and the definition of the model coefficients (and the uncertainty therein). The different models are compared against direct numerical simulations of oscillatory flow through a hexagonal sphere pack. The model constants are determined based on their definition in terms of the Stokes flow, the potential flow and steady nonlinear flow. Thus, the discrepancies between the model predictions and the simulation data can be attributed to shortcomings of the models' parametrisation.   We found that an extension of the dynamic permeability model of Pride et al. [Physical Review B 47(9), 1993] with a Forchheimer-type nonlinearity performs very well for linear flow and for nonlinear flow at low and medium frequencies, but the Forchheimer term with a coefficient obtained from the steady-state overpredicts the nonlinear drag at high frequencies. The model reduces to the unsteady Forchheimer equation with an acceleration coefficient based on the static viscous tortuosity for low frequencies.   The unsteady Forchheimer equation with an acceleration coefficient based on the high frequency limit of the dynamic tortuosity has large errors for linear flow at medium and high frequencies, but low errors for nonlinear flow at all frequencies. This is explained by an error cancellation between the inertial and the nonlinear drag.","sentences":["We review models for unsteady porous media flow in the volume-averaging framework and we discuss the theoretical relations between the models and the definition of the model coefficients (and the uncertainty therein).","The different models are compared against direct numerical simulations of oscillatory flow through a hexagonal sphere pack.","The model constants are determined based on their definition in terms of the Stokes flow, the potential flow and steady nonlinear flow.","Thus, the discrepancies between the model predictions and the simulation data can be attributed to shortcomings of the models' parametrisation.   ","We found that an extension of the dynamic permeability model of Pride et al.","[Physical Review B 47(9), 1993] with a Forchheimer-type nonlinearity performs very well for linear flow and for nonlinear flow at low and medium frequencies, but the Forchheimer term with a coefficient obtained from the steady-state overpredicts the nonlinear drag at high frequencies.","The model reduces to the unsteady Forchheimer equation with an acceleration coefficient based on the static viscous tortuosity for low frequencies.   ","The unsteady Forchheimer equation with an acceleration coefficient based on the high frequency limit of the dynamic tortuosity has large errors for linear flow at medium and high frequencies, but low errors for nonlinear flow at all frequencies.","This is explained by an error cancellation between the inertial and the nonlinear drag."],"url":"http://arxiv.org/abs/2402.05320v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 23:42:16","title":"Optimal energy-aware task scheduling for batteryless IoT devices","abstract":"Today's IoT devices rely on batteries, which offer stable energy storage but contain harmful chemicals. Having billions of IoT devices powered by batteries is not sustainable for the future. As an alternative, batteryless devices run on long-lived capacitors charged using energy harvesters. The small energy storage capacity of capacitors results in intermittent on-off behaviour. Traditional computing schedulers can not handle this intermittency, and in this paper we propose a first step towards an energy-aware task scheduler for constrained batteryless devices. We present a new energy-aware task scheduling algorithm that is able to optimally schedule application tasks to avoid power failures, and that will allow us to provide insights on the optimal look-ahead time for energy prediction. Our insights can be used as a basis for practical energy-aware scheduling and energy availability prediction algorithms. We formulate the scheduling problem as a Mixed Integer Linear Program. We evaluate its performance improvement when comparing it with state-of-the-art schedulers for batteryless IoT devices. Our results show that making the task scheduler energy aware avoids power failures and allows more tasks to successfully execute. Moreover, we conclude that a relatively short look-ahead energy prediction time of 8 future task executions is enough to achieve optimality.","sentences":["Today's IoT devices rely on batteries, which offer stable energy storage but contain harmful chemicals.","Having billions of IoT devices powered by batteries is not sustainable for the future.","As an alternative, batteryless devices run on long-lived capacitors charged using energy harvesters.","The small energy storage capacity of capacitors results in intermittent on-off behaviour.","Traditional computing schedulers can not handle this intermittency, and in this paper we propose a first step towards an energy-aware task scheduler for constrained batteryless devices.","We present a new energy-aware task scheduling algorithm that is able to optimally schedule application tasks to avoid power failures, and that will allow us to provide insights on the optimal look-ahead time for energy prediction.","Our insights can be used as a basis for practical energy-aware scheduling and energy availability prediction algorithms.","We formulate the scheduling problem as a Mixed Integer Linear Program.","We evaluate its performance improvement when comparing it with state-of-the-art schedulers for batteryless IoT devices.","Our results show that making the task scheduler energy aware avoids power failures and allows more tasks to successfully execute.","Moreover, we conclude that a relatively short look-ahead energy prediction time of 8 future task executions is enough to achieve optimality."],"url":"http://arxiv.org/abs/2402.05319v1","category":"cs.NI"}
{"created":"2024-02-07 23:11:00","title":"SplitSim: Large-Scale Simulations for Evaluating Network Systems Research","abstract":"When physical testbeds are out of reach for evaluating a networked system, we frequently turn to simulation. In today's datacenter networks, bottlenecks are rarely at the network protocol level, but instead in end-host software or hardware components, thus current protocol-level simulations are inadequate means of evaluation. End-to-end simulations covering these components on the other hand, simply cannot achieve the required scale with feasible simulation performance and computational resources.   In this paper, we address this with SplitSim, a simulation framework for end-to-end evaluation for large-scale network and distributed systems. To this end, SplitSim builds on prior work on modular end-to-end simulations and combines this with key elements to achieve scalability. First, mixed fidelity simulations judiciously reduce detail in simulation of parts of the system where this can be tolerated, while retaining the necessary detail elsewhere. SplitSim then parallelizes bottleneck simulators by decomposing them into multiple parallel but synchronized processes. Next, SplitSim provides a profiler to help users understand simulation performance and where the bottlenecks are, so users can adjust the configuration. Finally SplitSim provides abstractions to make it easy for users to build complex large-scale simulations. Our evaluation demonstrates SplitSim in multiple large-scale case studies.","sentences":["When physical testbeds are out of reach for evaluating a networked system, we frequently turn to simulation.","In today's datacenter networks, bottlenecks are rarely at the network protocol level, but instead in end-host software or hardware components, thus current protocol-level simulations are inadequate means of evaluation.","End-to-end simulations covering these components on the other hand, simply cannot achieve the required scale with feasible simulation performance and computational resources.   ","In this paper, we address this with SplitSim, a simulation framework for end-to-end evaluation for large-scale network and distributed systems.","To this end, SplitSim builds on prior work on modular end-to-end simulations and combines this with key elements to achieve scalability.","First, mixed fidelity simulations judiciously reduce detail in simulation of parts of the system where this can be tolerated, while retaining the necessary detail elsewhere.","SplitSim then parallelizes bottleneck simulators by decomposing them into multiple parallel but synchronized processes.","Next, SplitSim provides a profiler to help users understand simulation performance and where the bottlenecks are, so users can adjust the configuration.","Finally SplitSim provides abstractions to make it easy for users to build complex large-scale simulations.","Our evaluation demonstrates SplitSim in multiple large-scale case studies."],"url":"http://arxiv.org/abs/2402.05312v1","category":"cs.NI"}
{"created":"2024-02-07 22:12:30","title":"Hot Carriers from Intra- and Interband Transitions in Gold-Silver Alloy Nanoparticles","abstract":"Hot electrons and holes generated from the decay of localized surface plasmons in metallic nanoparticles can be harnessed for applications in solar energy conversion and sensing. In this paper, we study the generation of hot carriers in large spherical gold-silver alloy nanoparticles using a recently developed atomistic modelling approach that combines a solution of Maxwell's equations with large-scale tight-binding simulations. We find that hot-carrier properties depend sensitively on the alloy composition. Specifically, nanoparticles with a large gold fraction produce hot carriers under visible light illumination while nanoparticles with a large silver fraction require higher photon energies to produce hot carriers. Moreover, most hot carriers in nanoparticles with a large gold fraction originate from interband transitions which give rise to energetic holes and \"cold\" electrons near the Fermi level. Increasing the silver fraction enhances the generation rate of hot carriers from intraband transitions which produce energetic electrons and \"cold\" holes. These findings demonstrate that alloy composition is a powerful tuning parameter for the design of nanoparticles for applications in solar energy conversion and sensing that require precise control of hot-carrier properties.","sentences":["Hot electrons and holes generated from the decay of localized surface plasmons in metallic nanoparticles can be harnessed for applications in solar energy conversion and sensing.","In this paper, we study the generation of hot carriers in large spherical gold-silver alloy nanoparticles using a recently developed atomistic modelling approach that combines a solution of Maxwell's equations with large-scale tight-binding simulations.","We find that hot-carrier properties depend sensitively on the alloy composition.","Specifically, nanoparticles with a large gold fraction produce hot carriers under visible light illumination while nanoparticles with a large silver fraction require higher photon energies to produce hot carriers.","Moreover, most hot carriers in nanoparticles with a large gold fraction originate from interband transitions which give rise to energetic holes and \"cold\" electrons near the Fermi level.","Increasing the silver fraction enhances the generation rate of hot carriers from intraband transitions which produce energetic electrons and \"cold\" holes.","These findings demonstrate that alloy composition is a powerful tuning parameter for the design of nanoparticles for applications in solar energy conversion and sensing that require precise control of hot-carrier properties."],"url":"http://arxiv.org/abs/2402.05292v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-07 22:04:06","title":"Investigating Performance Trends of Simulated Real-time Solar Flare Predictions: The Impacts of Training Windows, Data Volumes, and the Solar Cycle","abstract":"This study explores the behavior of machine learning-based flare forecasting models deployed in a simulated operational environment. Using Georgia State University's Space Weather Analytics for Solar Flares benchmark dataset (Angryk et al. 2020a,b), we examine the impacts of training methodology and the solar cycle on decision tree, support vector machine, and multilayer perceptron performance. We implement our classifiers using three temporal training windows: stationary, rolling, and expanding. The stationary window trains models using a single set of data available before the first forecasting instance, which remains constant throughout the solar cycle. The rolling window trains models using data from a constant time interval before the forecasting instance, which moves with the solar cycle. Finally, the expanding window trains models using all available data before the forecasting instance. For each window, a number of input features (1, 5, 10, 25, 50, 120) and temporal sizes (5, 8, 11, 14, 17, 20 months) were tested. To our surprise, we found that for a 20-month window, skill scores were comparable regardless of the window type, feature count, and classifier selected. Furthermore, reducing the size of this window only marginally decreased stationary and rolling window performance. This implies that, given enough data, a stationary window can be chosen over other window types, eliminating the need for model retraining. Lastly, a moderately strong positive correlation was found to exist between a model's false positive rate and the solar X-ray background flux. This suggests that the solar cycle phase has a considerable influence on forecasting.","sentences":["This study explores the behavior of machine learning-based flare forecasting models deployed in a simulated operational environment.","Using Georgia State University's Space Weather Analytics for Solar Flares benchmark dataset (Angryk et al. 2020a,b), we examine the impacts of training methodology and the solar cycle on decision tree, support vector machine, and multilayer perceptron performance.","We implement our classifiers using three temporal training windows: stationary, rolling, and expanding.","The stationary window trains models using a single set of data available before the first forecasting instance, which remains constant throughout the solar cycle.","The rolling window trains models using data from a constant time interval before the forecasting instance, which moves with the solar cycle.","Finally, the expanding window trains models using all available data before the forecasting instance.","For each window, a number of input features (1, 5, 10, 25, 50, 120) and temporal sizes (5, 8, 11, 14, 17, 20 months) were tested.","To our surprise, we found that for a 20-month window, skill scores were comparable regardless of the window type, feature count, and classifier selected.","Furthermore, reducing the size of this window only marginally decreased stationary and rolling window performance.","This implies that, given enough data, a stationary window can be chosen over other window types, eliminating the need for model retraining.","Lastly, a moderately strong positive correlation was found to exist between a model's false positive rate and the solar X-ray background flux.","This suggests that the solar cycle phase has a considerable influence on forecasting."],"url":"http://arxiv.org/abs/2402.05288v1","category":"astro-ph.SR"}
{"created":"2024-02-07 21:58:53","title":"Prediction of $s^\\pm$-wave superconductivity enhanced by electronic doping in trilayer nickelates La$_4$Ni$_3$O$_{10}$ under pressure","abstract":"Motivated by the recently reported signatures of superconductivity in trilayer La$_4$Ni$_3$O$_{10}$ under pressure, we comprehensively study this system using {\\it ab initio} and random-phase approximation techniques. Without electronic interactions, the Ni $d_{3z^2-r^2}$ orbitals show a bonding-antibonding and nonbonding splitting behavior via the O $p_z$ orbital inducing a ``trimer'' lattice in La$_4$Ni$_3$O$_{10}$, analogous to the dimers of La$_3$Ni$_2$O$_{7}$. The Fermi surface consists of three electron sheets with mixed $e_g$ orbitals, and a hole and an electron pocket made up of the $d_{3z^2-r^2}$ orbital, suggesting a Ni two-orbital minimum model. In addition, we find that superconducting pairing is induced in the $s_{\\pm}$-wave channel due to partial nesting between the {\\bf M}=$(\\pi, \\pi)$ centered pockets and portions of the Fermi surface centered at the {\\bf $\\Gamma$}=$(0, 0)$ point. With changing electronic density $n$, the $s^\\pm$ instability remains leading and its pairing strength shows a dome-like behavior with a maximum around $n = 4.2$ ($\\sim 6.7\\%$ electron doping). The superconducting instability disappears at the same electronic density of La$_3$Ni$_2$O$_7$ correlated with the absence of the $\\gamma$ pocket, suggesting that the superconductivity of La$_3$Ni$_2$O$_7$ does not originate from trilayer- and single-layer structure. Furthermore, we predict an interesting spin-density-wave state in La$_4$Ni$_3$O$_{10}$ with an in-plane ($\\pi$, $\\pi$) order and antiferromagnetic coupling between the top and bottom Ni layers, while the middle layer has spin zero.","sentences":["Motivated by the recently reported signatures of superconductivity in trilayer La$_4$Ni$_3$O$_{10}$ under pressure, we comprehensively study this system using {\\it ab initio} and random-phase approximation techniques.","Without electronic interactions, the Ni $d_{3z^2-r^2}$ orbitals show a bonding-antibonding and nonbonding splitting behavior via the O $p_z$ orbital inducing a ``trimer'' lattice in La$_4$Ni$_3$O$_{10}$, analogous to the dimers of La$_3$Ni$_2$O$_{7}$. The Fermi surface consists of three electron sheets with mixed $e_g$ orbitals, and a hole and an electron pocket made up of the $d_{3z^2-r^2}$ orbital, suggesting a Ni two-orbital minimum model.","In addition, we find that superconducting pairing is induced in the $s_{\\pm}$-wave channel due to partial nesting between the {\\bf M}=$(\\pi, \\pi)$ centered pockets and portions of the Fermi surface centered at the {\\bf $\\Gamma$}=$(0, 0)$ point.","With changing electronic density $n$, the $s^\\pm$ instability remains leading and its pairing strength shows a dome-like behavior with a maximum around $n = 4.2$ ($\\sim 6.7\\%$ electron doping).","The superconducting instability disappears at the same electronic density of La$_3$Ni$_2$O$_7$ correlated with the absence of the $\\gamma$ pocket, suggesting that the superconductivity of La$_3$Ni$_2$O$_7$ does not originate from trilayer- and single-layer structure.","Furthermore, we predict an interesting spin-density-wave state in La$_4$Ni$_3$O$_{10}$ with an in-plane ($\\pi$, $\\pi$) order and antiferromagnetic coupling between the top and bottom Ni layers, while the middle layer has spin zero."],"url":"http://arxiv.org/abs/2402.05285v1","category":"cond-mat.supr-con"}
{"created":"2024-02-07 21:58:40","title":"Analyzing Adversarial Inputs in Deep Reinforcement Learning","abstract":"In recent years, Deep Reinforcement Learning (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems. However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns -- for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions. This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated. In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification. Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation. Our analysis empirically demonstrates how adversarial inputs can affect the safety of a given DRL system with respect to such perturbations. Moreover, we analyze the behavior of these configurations to suggest several useful practices and guidelines to help mitigate the vulnerability of trained DRL networks.","sentences":["In recent years, Deep Reinforcement Learning (DRL) has become a popular paradigm in machine learning due to its successful applications to real-world and complex systems.","However, even the state-of-the-art DRL models have been shown to suffer from reliability concerns -- for example, their susceptibility to adversarial inputs, i.e., small and abundant input perturbations that can fool the models into making unpredictable and potentially dangerous decisions.","This drawback limits the deployment of DRL systems in safety-critical contexts, where even a small error cannot be tolerated.","In this work, we present a comprehensive analysis of the characterization of adversarial inputs, through the lens of formal verification.","Specifically, we introduce a novel metric, the Adversarial Rate, to classify models based on their susceptibility to such perturbations, and present a set of tools and algorithms for its computation.","Our analysis empirically demonstrates how adversarial inputs can affect the safety of a given DRL system with respect to such perturbations.","Moreover, we analyze the behavior of these configurations to suggest several useful practices and guidelines to help mitigate the vulnerability of trained DRL networks."],"url":"http://arxiv.org/abs/2402.05284v1","category":"cs.LG"}
{"created":"2024-02-07 21:54:53","title":"TreeForm: End-to-end Annotation and Evaluation for Form Document Parsing","abstract":"Visually Rich Form Understanding (VRFU) poses a complex research problem due to the documents' highly structured nature and yet highly variable style and content. Current annotation schemes decompose form understanding and omit key hierarchical structure, making development and evaluation of end-to-end models difficult. In this paper, we propose a novel F1 metric to evaluate form parsers and describe a new content-agnostic, tree-based annotation scheme for VRFU: TreeForm. We provide methods to convert previous annotation schemes into TreeForm structures and evaluate TreeForm predictions using a modified version of the normalized tree-edit distance. We present initial baselines for our end-to-end performance metric and the TreeForm edit distance, averaged over the FUNSD and XFUND datasets, of 61.5 and 26.4 respectively. We hope that TreeForm encourages deeper research in annotating, modeling, and evaluating the complexities of form-like documents.","sentences":["Visually Rich Form Understanding (VRFU) poses a complex research problem due to the documents' highly structured nature and yet highly variable style and content.","Current annotation schemes decompose form understanding and omit key hierarchical structure, making development and evaluation of end-to-end models difficult.","In this paper, we propose a novel F1 metric to evaluate form parsers and describe a new content-agnostic, tree-based annotation scheme for VRFU: TreeForm.","We provide methods to convert previous annotation schemes into TreeForm structures and evaluate TreeForm predictions using a modified version of the normalized tree-edit distance.","We present initial baselines for our end-to-end performance metric and the TreeForm edit distance, averaged over the FUNSD and XFUND datasets, of 61.5 and 26.4 respectively.","We hope that TreeForm encourages deeper research in annotating, modeling, and evaluating the complexities of form-like documents."],"url":"http://arxiv.org/abs/2402.05282v1","category":"cs.CL"}
{"created":"2024-02-07 21:46:59","title":"Safe Human-UAS Collaboration Abstraction","abstract":"This paper studies the problem of safe humanuncrewed aerial system (UAS) collaboration in a shared work environment. By considering human and UAS as co-workers, we use Petri Nets to abstractly model evolution of shared tasks assigned to human and UAS co-workers. Particularly, the Petri Nets places represent work stations; therefore, the Petri Nets transitions can formally specify displacements between the work stations. The first objective is to incorporate uncertainty regarding the intentions of human co-workers into motion planning for UAS, when UAS co-workers closely interact with human co-workers. To this end, the proposed Petri Nets model uses conflict constructs to represent situations at which UAS deals with incomplete knowledge about human co-worker intention. The second objective is then to plan the motion of the UAS in a resilient and safe manner, in the presence of non-cooperative human co-workers. In order to achieve this objective, UAS equipped with onboard perception and decision-making capabilities are able to, through real-time processing of in-situ observation, predict human intention, quantify human distraction, and apply a non-stationary Markov Decision model to safely plan UAS motion in the presence of uncertainty.","sentences":["This paper studies the problem of safe humanuncrewed aerial system (UAS) collaboration in a shared work environment.","By considering human and UAS as co-workers, we use Petri Nets to abstractly model evolution of shared tasks assigned to human and UAS co-workers.","Particularly, the Petri Nets places represent work stations; therefore, the Petri Nets transitions can formally specify displacements between the work stations.","The first objective is to incorporate uncertainty regarding the intentions of human co-workers into motion planning for UAS, when UAS co-workers closely interact with human co-workers.","To this end, the proposed Petri Nets model uses conflict constructs to represent situations at which UAS deals with incomplete knowledge about human co-worker intention.","The second objective is then to plan the motion of the UAS in a resilient and safe manner, in the presence of non-cooperative human co-workers.","In order to achieve this objective, UAS equipped with onboard perception and decision-making capabilities are able to, through real-time processing of in-situ observation, predict human intention, quantify human distraction, and apply a non-stationary Markov Decision model to safely plan UAS motion in the presence of uncertainty."],"url":"http://arxiv.org/abs/2402.05277v1","category":"eess.SY"}
{"created":"2024-02-07 21:46:26","title":"Exploring Hierarchical Classification Performance for Time Series Data: Dissimilarity Measures and Classifier Comparisons","abstract":"The comparative performance of hierarchical classification (HC) and flat classification (FC) methodologies in the realm of time series data analysis is investigated in this study. Dissimilarity measures, including Jensen-Shannon Distance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance (CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF, and SVM. A subset of datasets from the UCR archive, focusing on multi-class cases comprising more than two classes, is employed for analysis. A significant trend is observed wherein HC demonstrates significant superiority over FC when paired with MINIROCKET utilizing TSD, diverging from conventional understandings. Conversely, FC exhibits consistent dominance across all configurations when employing alternative classifiers such as STSF and SVM. Moreover, TSD is found to consistently outperform both CBD and JSD across nearly all scenarios, except in instances involving the STSF classifier where CBD showcases superior performance. This discrepancy underscores the nuanced nature of dissimilarity measures and emphasizes the importance of their tailored selection based on the dataset and classifier employed. Valuable insights into the dynamic interplay between classification methodologies and dissimilarity measures in the realm of time series data analysis are provided by these findings. By elucidating the performance variations across different configurations, a foundation is laid for refining classification methodologies and dissimilarity measures to optimize performance in diverse analytical scenarios. Furthermore, the need for continued research aimed at elucidating the underlying mechanisms driving classification performance in time series data analysis is underscored, with implications for enhancing predictive modeling and decision-making in various domains.","sentences":["The comparative performance of hierarchical classification (HC) and flat classification (FC) methodologies in the realm of time series data analysis is investigated in this study.","Dissimilarity measures, including Jensen-Shannon Distance (JSD), Task Similarity Distance (TSD), and Classifier Based Distance (CBD), are leveraged alongside various classifiers such as MINIROCKET, STSF, and SVM.","A subset of datasets from the UCR archive, focusing on multi-class cases comprising more than two classes, is employed for analysis.","A significant trend is observed wherein HC demonstrates significant superiority over FC when paired with MINIROCKET utilizing TSD, diverging from conventional understandings.","Conversely, FC exhibits consistent dominance across all configurations when employing alternative classifiers such as STSF and SVM.","Moreover, TSD is found to consistently outperform both CBD and JSD across nearly all scenarios, except in instances involving the STSF classifier where CBD showcases superior performance.","This discrepancy underscores the nuanced nature of dissimilarity measures and emphasizes the importance of their tailored selection based on the dataset and classifier employed.","Valuable insights into the dynamic interplay between classification methodologies and dissimilarity measures in the realm of time series data analysis are provided by these findings.","By elucidating the performance variations across different configurations, a foundation is laid for refining classification methodologies and dissimilarity measures to optimize performance in diverse analytical scenarios.","Furthermore, the need for continued research aimed at elucidating the underlying mechanisms driving classification performance in time series data analysis is underscored, with implications for enhancing predictive modeling and decision-making in various domains."],"url":"http://arxiv.org/abs/2402.05275v1","category":"cs.LG"}
{"created":"2024-02-07 21:43:57","title":"Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes","abstract":"Infinite-state Markov Decision Processes (MDPs) are essential in modeling and optimizing a wide variety of engineering problems. In the reinforcement learning (RL) context, a variety of algorithms have been developed to learn and optimize these MDPs. At the heart of many popular policy-gradient based learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the Natural Policy Gradient (NPG) algorithm. Convergence results for these RL algorithms rest on convergence results for the NPG algorithm. However, all existing results on the convergence of the NPG algorithm are limited to finite-state settings.   We prove the first convergence rate bound for the NPG algorithm for infinite-state average-reward MDPs, proving a $O(1/\\sqrt{T})$ convergence rate, if the NPG algorithm is initialized with a good initial policy. Moreover, we show that in the context of a large class of queueing MDPs, the MaxWeight policy suffices to satisfy our initial-policy requirement and achieve a $O(1/\\sqrt{T})$ convergence rate. Key to our result are state-dependent bounds on the relative value function achieved by the iterate policies of the NPG algorithm.","sentences":["Infinite-state Markov Decision Processes (MDPs) are essential in modeling and optimizing a wide variety of engineering problems.","In the reinforcement learning (RL) context, a variety of algorithms have been developed to learn and optimize these MDPs.","At the heart of many popular policy-gradient based learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the Natural Policy Gradient (NPG) algorithm.","Convergence results for these RL algorithms rest on convergence results for the NPG algorithm.","However, all existing results on the convergence of the NPG algorithm are limited to finite-state settings.   ","We prove the first convergence rate bound for the NPG algorithm for infinite-state average-reward MDPs, proving a $O(1/\\sqrt{T})$ convergence rate, if the NPG algorithm is initialized with a good initial policy.","Moreover, we show that in the context of a large class of queueing MDPs, the MaxWeight policy suffices to satisfy our initial-policy requirement and achieve a $O(1/\\sqrt{T})$ convergence rate.","Key to our result are state-dependent bounds on the relative value function achieved by the iterate policies of the NPG algorithm."],"url":"http://arxiv.org/abs/2402.05274v1","category":"cs.LG"}
{"created":"2024-02-07 21:36:49","title":"Regime-Aware Asset Allocation: a Statistical Jump Model Approach","abstract":"This article investigates the impact of regime switching on asset allocation decisions, with a primary focus on comparing different regime identification models. In contrast to traditional Markov-switching models, we adopt the statistical jump model, a recently proposed robust model known for its ability to capture persistent market regimes by applying an explicit jump penalty. The feature set of our jump model comprises return and volatility features derived solely from the price series. We introduce a data-driven approach for selecting the jump penalty within a time-series cross-validation framework, which directly optimizes the performance metric of the regime-aware asset allocation strategy constructed following a comprehensive multi-step process. Through empirical analysis using daily return series from major US equity indices, we highlight the outperformance of employing jump models in comparison to both buy-and-hold strategies and Markov-switching asset allocation approaches. These results underline the enhanced robustness, interpretability, and realism inherent in asset allocation strategies guided by jump models, offering insights for portfolio managers.","sentences":["This article investigates the impact of regime switching on asset allocation decisions, with a primary focus on comparing different regime identification models.","In contrast to traditional Markov-switching models, we adopt the statistical jump model, a recently proposed robust model known for its ability to capture persistent market regimes by applying an explicit jump penalty.","The feature set of our jump model comprises return and volatility features derived solely from the price series.","We introduce a data-driven approach for selecting the jump penalty within a time-series cross-validation framework, which directly optimizes the performance metric of the regime-aware asset allocation strategy constructed following a comprehensive multi-step process.","Through empirical analysis using daily return series from major US equity indices, we highlight the outperformance of employing jump models in comparison to both buy-and-hold strategies and Markov-switching asset allocation approaches.","These results underline the enhanced robustness, interpretability, and realism inherent in asset allocation strategies guided by jump models, offering insights for portfolio managers."],"url":"http://arxiv.org/abs/2402.05272v1","category":"q-fin.PM"}
{"created":"2024-02-07 21:23:33","title":"Insights From Univalent Foundations: A Case Study Using Double Categories","abstract":"Category theory unifies mathematical concepts, aiding comparisons across structures by incorporating objects and morphisms, which capture their interactions. It has influenced areas of computer science such as automata theory, functional programming, and semantics. Certain objects naturally exhibit two classes of morphisms, leading to the concept of a double category, which has found applications in computing science (e.g., ornaments, profunctor optics, denotational semantics).   The emergence of diverse categorical structures motivated a unified framework for category theory. However, unlike other mathematical objects, classification of categorical structures faces challenges due to various relevant equivalences. This poses significant challenges when pursuing the formalization of categories and restricts the applicability of powerful techniques, such as transport along equivalences. This work contends that univalent foundations offers a suitable framework for classifying different categorical structures based on desired notions of equivalences, and remedy the challenges when formalizing categories. The richer notion of equality in univalent foundations makes the equivalence of a categorical structure an inherent part of its structure.   We concretely apply this analysis to double categorical structures. We characterize and formalize various definitions in Coq UniMath, including (pseudo) double categories and double bicategories, up to chosen equivalences. We also establish univalence principles, making chosen equivalences part of the double categorical structure, analyzing strict double setcategories (invariant under isomorphisms), pseudo double setcategories (invariant under isomorphisms), univalent pseudo double categories (invariant under vertical equivalences) and univalent double bicategories (invariant under gregarious equivalences).","sentences":["Category theory unifies mathematical concepts, aiding comparisons across structures by incorporating objects and morphisms, which capture their interactions.","It has influenced areas of computer science such as automata theory, functional programming, and semantics.","Certain objects naturally exhibit two classes of morphisms, leading to the concept of a double category, which has found applications in computing science (e.g., ornaments, profunctor optics, denotational semantics).   ","The emergence of diverse categorical structures motivated a unified framework for category theory.","However, unlike other mathematical objects, classification of categorical structures faces challenges due to various relevant equivalences.","This poses significant challenges when pursuing the formalization of categories and restricts the applicability of powerful techniques, such as transport along equivalences.","This work contends that univalent foundations offers a suitable framework for classifying different categorical structures based on desired notions of equivalences, and remedy the challenges when formalizing categories.","The richer notion of equality in univalent foundations makes the equivalence of a categorical structure an inherent part of its structure.   ","We concretely apply this analysis to double categorical structures.","We characterize and formalize various definitions in Coq UniMath, including (pseudo) double categories and double bicategories, up to chosen equivalences.","We also establish univalence principles, making chosen equivalences part of the double categorical structure, analyzing strict double setcategories (invariant under isomorphisms), pseudo double setcategories (invariant under isomorphisms), univalent pseudo double categories (invariant under vertical equivalences) and univalent double bicategories (invariant under gregarious equivalences)."],"url":"http://arxiv.org/abs/2402.05265v1","category":"math.CT"}
{"created":"2024-02-07 21:19:05","title":"AdaBatchGrad: Combining Adaptive Batch Size and Adaptive Step Size","abstract":"This paper presents a novel adaptation of the Stochastic Gradient Descent (SGD), termed AdaBatchGrad. This modification seamlessly integrates an adaptive step size with an adjustable batch size. An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of SGD and decrease its variance. A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step. Methods that utilized exact tests were observed to converge within $O(LR^2/\\varepsilon)$ iterations. Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance. To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method's robustness and stability. For exact tests, our approach converges in $O(LR^2/\\varepsilon)$ iterations, analogous to standard gradient descent. For inexact tests, it achieves convergence in $O(\\max\\lbrace LR^2/\\varepsilon, \\sigma^2 R^2/\\varepsilon^2 \\rbrace )$ iterations. This makes AdaBatchGrad markedly more robust and computationally efficient relative to prevailing methods. To substantiate the efficacy of our method, we experimentally show, how the introduction of adaptive step size and adaptive batch size gradually improves the performance of regular SGD. The results imply that AdaBatchGrad surpasses alternative methods, especially when applied to inexact tests.","sentences":["This paper presents a novel adaptation of the Stochastic Gradient Descent (SGD), termed AdaBatchGrad.","This modification seamlessly integrates an adaptive step size with an adjustable batch size.","An increase in batch size and a decrease in step size are well-known techniques to tighten the area of convergence of SGD and decrease its variance.","A range of studies by R. Byrd and J. Nocedal introduced various testing techniques to assess the quality of mini-batch gradient approximations and choose the appropriate batch sizes at every step.","Methods that utilized exact tests were observed to converge within $O(LR^2/\\varepsilon)$ iterations.","Conversely, inexact test implementations sometimes resulted in non-convergence and erratic performance.","To address these challenges, AdaBatchGrad incorporates both adaptive batch and step sizes, enhancing the method's robustness and stability.","For exact tests, our approach converges in $O(LR^2/\\varepsilon)$ iterations, analogous to standard gradient descent.","For inexact tests, it achieves convergence in $O(\\max\\lbrace LR^2/\\varepsilon, \\sigma^2 R^2/\\varepsilon^2 \\rbrace )$ iterations.","This makes AdaBatchGrad markedly more robust and computationally efficient relative to prevailing methods.","To substantiate the efficacy of our method, we experimentally show, how the introduction of adaptive step size and adaptive batch size gradually improves the performance of regular SGD.","The results imply that AdaBatchGrad surpasses alternative methods, especially when applied to inexact tests."],"url":"http://arxiv.org/abs/2402.05264v1","category":"cs.LG"}
{"created":"2024-02-07 21:07:21","title":"A Design Technique based on Equivalent Circuit and Coupler Theory for Broadband Linear to Circular Polarization Converters in Reflection or Transmission Mode","abstract":"A new approach to designing FSS-based LP-CP converters is presented. It is based on the use of FSSs which exhibit dual diagonal symmetry, and a novel 4-port equivalent circuit able to describe the electrical behavior of the cells for the two linear incident polarizations at the same time. The equivalent circuit allows the use of standardized branch line coupler theory to design LP-CP converters comprising a cascade of an arbitrary number of layers, whose synthesis includes the phase and makes it possible to achieve prescribed electrical conditions systematically. A full design procedure has been developed using the new approach and several designs in both transmission and reflection modes are presented and evaluated. It has been proven that single layer reflective converters exhibit large bandwidths as the two reflected field components are in quadrature independently of the frequency. One of these devices was designed and showed an AR<0.2 dB within the band from 21.5 to 28.5 GHz. The reflective LP-CP converter designed was also manufactured and tested, and the measurements were used to validate the design procedure.","sentences":["A new approach to designing FSS-based LP-CP converters is presented.","It is based on the use of FSSs which exhibit dual diagonal symmetry, and a novel 4-port equivalent circuit able to describe the electrical behavior of the cells for the two linear incident polarizations at the same time.","The equivalent circuit allows the use of standardized branch line coupler theory to design LP-CP converters comprising a cascade of an arbitrary number of layers, whose synthesis includes the phase and makes it possible to achieve prescribed electrical conditions systematically.","A full design procedure has been developed using the new approach and several designs in both transmission and reflection modes are presented and evaluated.","It has been proven that single layer reflective converters exhibit large bandwidths as the two reflected field components are in quadrature independently of the frequency.","One of these devices was designed and showed an AR<0.2 dB within the band from 21.5 to 28.5 GHz.","The reflective LP-CP converter designed was also manufactured and tested, and the measurements were used to validate the design procedure."],"url":"http://arxiv.org/abs/2402.05258v1","category":"physics.app-ph"}
{"created":"2024-02-07 20:56:40","title":"Generalized Bimode Equivalent Circuit of Arbitrary Planar Periodic Structures for Oblique Incidence","abstract":"This work presents, for the first time, a generalized bimode Fosters equivalent circuit for characterization of 2-D Planar Periodic Structures (PPSs) with arbitrary geometry at oblique incidence. It considers the interactions between the fundamental TE and TM modes without any restriction within the bimode bandwidth of the geometry. The proposed circuit is only composed of frequency-independent LC elements, which can be extracted systematically from electromagnetic (EM) simulations. The reactive immittances obtained in the process fulfill the Fosters theorem, enabling the design process of PPS-based devices using standardized synthesis techniques from circuit theory. To demonstrate its viability and general nature, equivalent circuits are extracted for different single- and multilayer PPS composed of rotated dipoles under oblique incidence theta=20 deg,phi=30 deg, and including dielectrics. Excellent agreement is found between the response of the circuit model and the EM simulation in all cases. Finally, to validate experimentally the proposed equivalent circuit and highlight its applicability, a 90 deg reflective LinearPolarization (LP) Rotator centered at 25 GHz and under oblique incidence, theta=30 deg, phi=0 deg (TE), is designed, manufactured, and tested. The agreement between the circuit response, the EM simulation and the measurement underlines the potential of the new equivalent circuit for PPS design under oblique incidence.","sentences":["This work presents, for the first time, a generalized bimode Fosters equivalent circuit for characterization of 2-D Planar Periodic Structures (PPSs) with arbitrary geometry at oblique incidence.","It considers the interactions between the fundamental TE and TM modes without any restriction within the bimode bandwidth of the geometry.","The proposed circuit is only composed of frequency-independent LC elements, which can be extracted systematically from electromagnetic (EM) simulations.","The reactive immittances obtained in the process fulfill the Fosters theorem, enabling the design process of PPS-based devices using standardized synthesis techniques from circuit theory.","To demonstrate its viability and general nature, equivalent circuits are extracted for different single- and multilayer PPS composed of rotated dipoles under oblique incidence theta=20 deg,phi=30 deg, and including dielectrics.","Excellent agreement is found between the response of the circuit model and the EM simulation in all cases.","Finally, to validate experimentally the proposed equivalent circuit and highlight its applicability, a 90 deg reflective LinearPolarization (LP) Rotator centered at 25 GHz and under oblique incidence, theta=30 deg, phi=0 deg (TE), is designed, manufactured, and tested.","The agreement between the circuit response, the EM simulation and the measurement underlines the potential of the new equivalent circuit for PPS design under oblique incidence."],"url":"http://arxiv.org/abs/2402.05253v1","category":"physics.app-ph"}
{"created":"2024-02-07 20:46:54","title":"On the Outcome Equivalence of Extensive-Form and Behavioral Correlated Equilibria","abstract":"We investigate two notions of correlated equilibrium for extensive-form games: extensive-form correlated equilibrium (EFCE) and behavioral correlated equilibrium (BCE). We show that the two are outcome-equivalent, in the sense that every outcome distribution achievable under one notion is achievable under the other. Our result implies, to our knowledge, the first polynomial-time algorithm for computing a BCE.","sentences":["We investigate two notions of correlated equilibrium for extensive-form games: extensive-form correlated equilibrium (EFCE) and behavioral correlated equilibrium (BCE).","We show that the two are outcome-equivalent, in the sense that every outcome distribution achievable under one notion is achievable under the other.","Our result implies, to our knowledge, the first polynomial-time algorithm for computing a BCE."],"url":"http://arxiv.org/abs/2402.05245v1","category":"cs.GT"}
{"created":"2024-02-07 20:31:07","title":"Automated Data-Driven Discovery of Material Models Based on Symbolic Regression: A Case Study on Human Brain Cortex","abstract":"We introduce a data-driven framework to automatically identify interpretable and physically meaningful hyperelastic constitutive models from sparse data. Leveraging symbolic regression, an algorithm based on genetic programming, our approach generates elegant hyperelastic models that achieve accurate data fitting through parsimonious mathematic formulae, while strictly adhering to hyperelasticity constraints such as polyconvexity. Our investigation spans three distinct hyperelastic models -- invariant-based, principal stretch-based, and normal strain-based -- and highlights the versatility of symbolic regression. We validate our new approach using synthetic data from five classic hyperelastic models and experimental data from the human brain to demonstrate algorithmic efficacy. Our results suggest that our symbolic regression robustly discovers accurate models with succinct mathematic expressions in invariant-based, stretch-based, and strain-based scenarios. Strikingly, the strain-based model exhibits superior accuracy, while both stretch- and strain-based models effectively capture the nonlinearity and tension-compression asymmetry inherent to human brain tissue. Polyconvexity examinations affirm the rigor of convexity within the training regime and demonstrate excellent extrapolation capabilities beyond this regime for all three models. However, the stretch-based models raise concerns regarding potential convexity loss under large deformations. Finally, robustness tests on noise-embedded data underscore the reliability of our symbolic regression algorithms. Our study confirms the applicability and accuracy of symbolic regression in the automated discovery of hyperelastic models for the human brain and gives rise to a wide variety of applications in other soft matter systems.","sentences":["We introduce a data-driven framework to automatically identify interpretable and physically meaningful hyperelastic constitutive models from sparse data.","Leveraging symbolic regression, an algorithm based on genetic programming, our approach generates elegant hyperelastic models that achieve accurate data fitting through parsimonious mathematic formulae, while strictly adhering to hyperelasticity constraints such as polyconvexity.","Our investigation spans three distinct hyperelastic models -- invariant-based, principal stretch-based, and normal strain-based -- and highlights the versatility of symbolic regression.","We validate our new approach using synthetic data from five classic hyperelastic models and experimental data from the human brain to demonstrate algorithmic efficacy.","Our results suggest that our symbolic regression robustly discovers accurate models with succinct mathematic expressions in invariant-based, stretch-based, and strain-based scenarios.","Strikingly, the strain-based model exhibits superior accuracy, while both stretch- and strain-based models effectively capture the nonlinearity and tension-compression asymmetry inherent to human brain tissue.","Polyconvexity examinations affirm the rigor of convexity within the training regime and demonstrate excellent extrapolation capabilities beyond this regime for all three models.","However, the stretch-based models raise concerns regarding potential convexity loss under large deformations.","Finally, robustness tests on noise-embedded data underscore the reliability of our symbolic regression algorithms.","Our study confirms the applicability and accuracy of symbolic regression in the automated discovery of hyperelastic models for the human brain and gives rise to a wide variety of applications in other soft matter systems."],"url":"http://arxiv.org/abs/2402.05238v1","category":"cs.SC"}
{"created":"2024-02-07 20:18:15","title":"Real-Time Line-Based Room Segmentation and Continuous Euclidean Distance Fields","abstract":"Continuous maps representations, as opposed to traditional discrete ones such as grid maps, have been gaining traction in the research community. However, current approaches still suffer from high computation costs, making them unable to be used in large environments without sacrificing precision. In this paper, a scalable method building upon Gaussian Process-based Euclidean Distance Fields (GP-EDFs) is proposed. By leveraging structure inherent to indoor environments, namely walls and rooms, we achieve an accurate continuous map representation that is fast enough to be updated and used in real-time. This is possible thanks to a novel line-based room segmentation algorithm, enabling the creation of smaller local GP-EDFs for each room, which in turn also use line segments as its shape priors, thus representing the map more efficiently with fewer data points. We evaluate this method in simulation experiments, and make the code available open-source.","sentences":["Continuous maps representations, as opposed to traditional discrete ones such as grid maps, have been gaining traction in the research community.","However, current approaches still suffer from high computation costs, making them unable to be used in large environments without sacrificing precision.","In this paper, a scalable method building upon Gaussian Process-based Euclidean Distance Fields (GP-EDFs) is proposed.","By leveraging structure inherent to indoor environments, namely walls and rooms, we achieve an accurate continuous map representation that is fast enough to be updated and used in real-time.","This is possible thanks to a novel line-based room segmentation algorithm, enabling the creation of smaller local GP-EDFs for each room, which in turn also use line segments as its shape priors, thus representing the map more efficiently with fewer data points.","We evaluate this method in simulation experiments, and make the code available open-source."],"url":"http://arxiv.org/abs/2402.05236v1","category":"cs.RO"}
{"created":"2024-02-07 20:14:22","title":"QGFN: Controllable Greediness with Action Values","abstract":"Generative Flow Networks (GFlowNets; GFNs) are a family of reward/energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples. However, biasing GFNs towards producing high-utility samples is non-trivial. In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter. We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity.","sentences":["Generative Flow Networks (GFlowNets; GFNs) are a family of reward/energy-based generative methods for combinatorial objects, capable of generating diverse and high-utility samples.","However, biasing GFNs towards producing high-utility samples is non-trivial.","In this work, we leverage connections between GFNs and reinforcement learning (RL) and propose to combine the GFN policy with an action-value estimate, $Q$, to create greedier sampling policies which can be controlled by a mixing parameter.","We show that several variants of the proposed method, QGFN, are able to improve on the number of high-reward samples generated in a variety of tasks without sacrificing diversity."],"url":"http://arxiv.org/abs/2402.05234v1","category":"cs.LG"}
{"created":"2024-02-07 20:11:38","title":"Estimating Fold Changes from Partially Observed Outcomes with Applications in Microbial Metagenomics","abstract":"We consider the problem of estimating fold-changes in the expected value of a multivariate outcome that is observed subject to unknown sample-specific and category-specific perturbations. We are motivated by high-throughput sequencing studies of the abundance of microbial taxa, in which microbes are systematically over- and under-detected relative to their true abundances. Our log-linear model admits a partially identifiable estimand, and we establish full identifiability by imposing interpretable parameter constraints. To reduce bias and guarantee the existence of parameter estimates in the presence of sparse observations, we apply an asymptotically negligible and constraint-invariant penalty to our estimating function. We develop a fast coordinate descent algorithm for estimation, and an augmented Lagrangian algorithm for estimation under null hypotheses. We construct a model-robust score test, and demonstrate valid inference even for small sample sizes and violated distributional assumptions. The flexibility of the approach and comparisons to related methods are illustrated via a meta-analysis of microbial associations with colorectal cancer.","sentences":["We consider the problem of estimating fold-changes in the expected value of a multivariate outcome that is observed subject to unknown sample-specific and category-specific perturbations.","We are motivated by high-throughput sequencing studies of the abundance of microbial taxa, in which microbes are systematically over- and under-detected relative to their true abundances.","Our log-linear model admits a partially identifiable estimand, and we establish full identifiability by imposing interpretable parameter constraints.","To reduce bias and guarantee the existence of parameter estimates in the presence of sparse observations, we apply an asymptotically negligible and constraint-invariant penalty to our estimating function.","We develop a fast coordinate descent algorithm for estimation, and an augmented Lagrangian algorithm for estimation under null hypotheses.","We construct a model-robust score test, and demonstrate valid inference even for small sample sizes and violated distributional assumptions.","The flexibility of the approach and comparisons to related methods are illustrated via a meta-analysis of microbial associations with colorectal cancer."],"url":"http://arxiv.org/abs/2402.05231v1","category":"stat.ME"}
{"created":"2024-02-07 20:04:54","title":"A Comprehensive Analysis of Secondary Coexistence in a Real-World CBRS Deployment","abstract":"The Federal Communications Commission (FCC) in the U.S. has made the Citizens Broadband Radio Service (CBRS) band (3.55 - 3.7 GHz) available for commercial wireless usage under a shared approach using a three-tier hierarchical architecture, where the federal incumbent is the highest priority Tier 1 user, Priority Access License (PAL) holders, who have paid for licenses, are Tier 2 users and Tier 3 users operate under General Authorized Access (GAA), without license fees or protection from higher priority users. The Spectrum Access System (SAS) ensures that higher priority users are protected from interference from lower priority users. However, the lowest priority GAA users are not given any protection from each other by the SAS and are expected to not cause any harmful interference to Tier 1 and Tier 2 users. As the deployments of GAA devices grow, the potential for secondary interference between GAA users increases, especially since the SAS architecture does not allow dynamic channel switching when faced with interference. In this paper, we present a first-of-its-kind extensive measurement campaign of a commercial CBRS network deployed in the city of South Bend, IN, that quantifies both co-channel interference (CCI) and adjacent channel interference (ACI) caused by competing GAA devices and C-band 5G, respectively. We (i) identify a particular CCI scenario and improve performance by changing the frequency allocation based on our study of other allocations in the vicinity and (ii) quantify ACI from 5G in C-band (3.7 GHz) on CBRS throughput. We conclude that (i) CCI and ACI for GAA users is not handled well by the SAS, (ii) proper frequency allocation for GAA requires additional analysis of interference from other GAA users followed by dynamical channel selection, and (iii) utilization of immediate adjacent channels by high power 5G deployments limits the performance of CBRS.","sentences":["The Federal Communications Commission (FCC) in the U.S. has made the Citizens Broadband Radio Service (CBRS) band (3.55 - 3.7 GHz) available for commercial wireless usage under a shared approach using a three-tier hierarchical architecture, where the federal incumbent is the highest priority Tier 1 user, Priority Access License (PAL) holders, who have paid for licenses, are Tier 2 users and Tier 3 users operate under General Authorized Access (GAA), without license fees or protection from higher priority users.","The Spectrum Access System (SAS) ensures that higher priority users are protected from interference from lower priority users.","However, the lowest priority GAA users are not given any protection from each other by the SAS and are expected to not cause any harmful interference to Tier 1 and Tier 2 users.","As the deployments of GAA devices grow, the potential for secondary interference between GAA users increases, especially since the SAS architecture does not allow dynamic channel switching when faced with interference.","In this paper, we present a first-of-its-kind extensive measurement campaign of a commercial CBRS network deployed in the city of South Bend, IN, that quantifies both co-channel interference (CCI) and adjacent channel interference (ACI) caused by competing GAA devices and C-band 5G, respectively.","We (i) identify a particular CCI scenario and improve performance by changing the frequency allocation based on our study of other allocations in the vicinity and (ii) quantify ACI from 5G in C-band (3.7 GHz) on CBRS throughput.","We conclude that (i) CCI and ACI for GAA users is not handled well by the SAS, (ii) proper frequency allocation for GAA requires additional analysis of interference from other GAA users followed by dynamical channel selection, and (iii) utilization of immediate adjacent channels by high power 5G deployments limits the performance of CBRS."],"url":"http://arxiv.org/abs/2402.05226v1","category":"cs.NI"}
{"created":"2024-02-07 20:01:14","title":"Validation Workflow for Machine Learning Interatomic Potentials for Complex Ceramics","abstract":"The number of published Machine Learning Interatomic Potentials (MLIPs) has increased significantly in recent years. These new data-driven potential energy approximations often lack the physics-based foundations that inform many traditionally-developed interatomic potentials and hence require robust validation methods for their applicability, accuracy, computational efficiency, and transferability to the intended applications. This work presents a sequential, three-stage workflow for MLIP validation: (i) preliminary validation, (ii) static property prediction, and (iii) dynamic property prediction. This material-agnostic procedure is demonstrated in a tutorial-approach for the development of a robust MLIP for boron carbide (B4C), a widely employed, structurally complex ceramic that undergoes a deleterious deformation mechanism called \"amorphization\" under high pressure loading. It is shown that the resulting B4C MLIP offers a more accurate prediction of properties and behaviors with increased computational efficiency compared to the available ReaxFF potential.","sentences":["The number of published Machine Learning Interatomic Potentials (MLIPs) has increased significantly in recent years.","These new data-driven potential energy approximations often lack the physics-based foundations that inform many traditionally-developed interatomic potentials and hence require robust validation methods for their applicability, accuracy, computational efficiency, and transferability to the intended applications.","This work presents a sequential, three-stage workflow for MLIP validation: (i) preliminary validation, (ii) static property prediction, and (iii) dynamic property prediction.","This material-agnostic procedure is demonstrated in a tutorial-approach for the development of a robust MLIP for boron carbide (B4C), a widely employed, structurally complex ceramic that undergoes a deleterious deformation mechanism called \"amorphization\" under high pressure loading.","It is shown that the resulting B4C MLIP offers a more accurate prediction of properties and behaviors with increased computational efficiency compared to the available ReaxFF potential."],"url":"http://arxiv.org/abs/2402.05222v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 19:58:51","title":"Higher Specht polynomials under the diagonal action","abstract":"We introduce higher Specht polynomials - analogs of Specht polynomials in higher degrees - in two sets of variables $x_1,\\ldots,x_n$ and $y_1,\\ldots,y_n$ under the diagonal action of the symmetric group $S_n$. This generalizes the classical Specht polynomial construction in one set of variables, as well as the higher Specht basis for the coinvariant ring $R_n$ due to Ariki, Terasoma, and Yamada, which has the advantage of respecting the decomposition into irreducibles.   As our main application of the general theory, we provide a higher Specht basis for the hook shape Garsia--Haiman modules. In the process, we obtain a new formula for their doubly graded Frobenius series in terms of new generalized cocharge statistics on tableaux.","sentences":["We introduce higher Specht polynomials - analogs of Specht polynomials in higher degrees - in two sets of variables $x_1,\\ldots,x_n$ and $y_1,\\ldots,y_n$ under the diagonal action of the symmetric group $S_n$. This generalizes the classical Specht polynomial construction in one set of variables, as well as the higher Specht basis for the coinvariant ring $R_n$ due to Ariki, Terasoma, and Yamada, which has the advantage of respecting the decomposition into irreducibles.   ","As our main application of the general theory, we provide a higher Specht basis for the hook shape Garsia--Haiman modules.","In the process, we obtain a new formula for their doubly graded Frobenius series in terms of new generalized cocharge statistics on tableaux."],"url":"http://arxiv.org/abs/2402.05221v1","category":"math.CO"}
{"created":"2024-02-07 19:26:33","title":"Stochastic modeling of Random Access Memories reset transitions","abstract":"Resistive Random Access Memories (RRAMs) are being studied by the industry and academia because it is widely accepted that they are promising candidates for the next generation of high density nonvolatile memories. Taking into account the stochastic nature of mechanisms behind resistive switching, a new technique based on the use of functional data analysis has been developed to accurately model resistive memory device characteristics. Functional principal component analysis (FPCA) based on Karhunen-Loeve expansion is applied to obtain an orthogonal decomposition of the reset process in terms of uncorrelated scalar random variables. Then, the device current has been accurately described making use of just one variable presenting a modeling approach that can be very attractive from the circuit simulation viewpoint. The new method allows a comprehensive description of the stochastic variability of these devices by introducing a probability distribution that allows the simulation of the main parameter that is employed for the model implementation. A rigorous description of the mathematical theory behind the technique is given and its application for a broad set of experimental measurements is explained.","sentences":["Resistive Random Access Memories (RRAMs) are being studied by the industry and academia because it is widely accepted that they are promising candidates for the next generation of high density nonvolatile memories.","Taking into account the stochastic nature of mechanisms behind resistive switching, a new technique based on the use of functional data analysis has been developed to accurately model resistive memory device characteristics.","Functional principal component analysis (FPCA) based on Karhunen-Loeve expansion is applied to obtain an orthogonal decomposition of the reset process in terms of uncorrelated scalar random variables.","Then, the device current has been accurately described making use of just one variable presenting a modeling approach that can be very attractive from the circuit simulation viewpoint.","The new method allows a comprehensive description of the stochastic variability of these devices by introducing a probability distribution that allows the simulation of the main parameter that is employed for the model implementation.","A rigorous description of the mathematical theory behind the technique is given and its application for a broad set of experimental measurements is explained."],"url":"http://arxiv.org/abs/2402.05209v1","category":"stat.ME"}
{"created":"2024-02-07 19:15:33","title":"Bellman Conformal Inference: Calibrating Prediction Intervals For Time Series","abstract":"We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals. Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step. In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP. We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts. We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods.","sentences":["We introduce Bellman Conformal Inference (BCI), a framework that wraps around any time series forecasting models and provides calibrated prediction intervals.","Unlike the existing methods, BCI is able to leverage multi-step ahead forecasts and explicitly optimize the average interval lengths by solving a one-dimensional stochastic control problem (SCP) at each time step.","In particular, we use the dynamic programming algorithm to find the optimal policy for the SCP.","We prove that BCI achieves long-term coverage under arbitrary distribution shifts and temporal dependence, even with poor multi-step ahead forecasts.","We find empirically that BCI avoids uninformative intervals that have infinite lengths and generates substantially shorter prediction intervals on volatility forecasting problems when compared with existing methods."],"url":"http://arxiv.org/abs/2402.05203v1","category":"cs.LG"}
{"created":"2024-02-07 19:08:25","title":"Analogs to Ramanujan's Master Theorem and Operational Methods","abstract":"In this paper, we utilize operational methods to obtain closed-form solutions for certain classes of integrals in the spirit of Ramanujan's Master Theorem and provide several analogs to it. Although the use of operational calculus makes the proofs formal in nature, they can still yield interesting and correct results and may stimulate further rigorous investigations in the future.","sentences":["In this paper, we utilize operational methods to obtain closed-form solutions for certain classes of integrals in the spirit of Ramanujan's Master Theorem and provide several analogs to it.","Although the use of operational calculus makes the proofs formal in nature, they can still yield interesting and correct results and may stimulate further rigorous investigations in the future."],"url":"http://arxiv.org/abs/2402.05199v1","category":"math.CA"}
{"created":"2024-02-07 19:03:53","title":"Anisotropy of Density Fluctuations in the Solar Wind at 1 au","abstract":"A well-known property of solar wind plasma turbulence is the observed anisotropy of the autocorrelations, or equivalently the spectra, of velocity and magnetic field fluctuations. Here we explore the related but apparently not well-studied issue of the anisotropy of plasma density fluctuations in the energy-containing and inertial ranges of solar wind turbulence. Using 10 years (1998-2008) of in situ data from the Advanced Composition Explorer (ACE) mission, we find that the density correlation scale is slightly larger in directions quasi-parallel to the large-scale mean magnetic field as compared to quasi-perpendicular directions. The effect is present in both fast and slow winds. The anisotropy as a function of the level of correlation is also explored. We find at small correlation levels, i.e., at energy-containing scales and larger, the density fluctuations are close to isotropy, but in fact slightly favor more rapid decorrelation in perpendicular directions. At relatively smaller turbulence inertial range scales where the correlation values are larger, the sense of anisotropy is reversed in all speed ranges, implying a more ``slab-like'' structure, especially prominent in the fast wind samples. We contrast this finding with published results on velocity and magnetic field correlations.","sentences":["A well-known property of solar wind plasma turbulence is the observed anisotropy of the autocorrelations, or equivalently the spectra, of velocity and magnetic field fluctuations.","Here we explore the related but apparently not well-studied issue of the anisotropy of plasma density fluctuations in the energy-containing and inertial ranges of solar wind turbulence.","Using 10 years (1998-2008) of in situ data from the Advanced Composition Explorer (ACE) mission, we find that the density correlation scale is slightly larger in directions quasi-parallel to the large-scale mean magnetic field as compared to quasi-perpendicular directions.","The effect is present in both fast and slow winds.","The anisotropy as a function of the level of correlation is also explored.","We find at small correlation levels, i.e., at energy-containing scales and larger, the density fluctuations are close to isotropy, but in fact slightly favor more rapid decorrelation in perpendicular directions.","At relatively smaller turbulence inertial range scales where the correlation values are larger, the sense of anisotropy is reversed in all speed ranges, implying a more ``slab-like'' structure, especially prominent in the fast wind samples.","We contrast this finding with published results on velocity and magnetic field correlations."],"url":"http://arxiv.org/abs/2402.05191v1","category":"physics.space-ph"}
{"created":"2024-02-07 19:00:03","title":"Superradiant Instability of Magnetic Black Holes","abstract":"Black hole superradiance has proven being very valuable in several realms of gravitational physics, and holds a promising discovery potential. In this paper, we show how it can sheed light on a long standing problem in physics, the quest for magnetic monopoles in the Universe. Placing them in the interior of primordial rotating black holes, which act as natural amplifiers, we show that massive charged bosonic fields in their vicinity exhibit a superradiant instability which surpasses significantly that of neutral Kerr black holes. Strikingly, this is true for black holes containing an order-one number of magnetic monopoles, or merely a single one, and possessing either low, moderate or large values of angular momentum. In particular, the instability is drastically faster than the radiative decay time of charged pions, thus making it physically relevant. Furthermore, our analysis identifies the most unstable modes as a class of monopole spheroidal harmonics, that we dub north and south monopole modes, whose morphology is markedly different from the usual superradiantly unstable modes since they extend along the rotational axis. We conclude by discussing implications of our results for primordial magnetic black holes, and their observational signatures as sources of cosmic rays and high-frequency gravitational waves.","sentences":["Black hole superradiance has proven being very valuable in several realms of gravitational physics, and holds a promising discovery potential.","In this paper, we show how it can sheed light on a long standing problem in physics, the quest for magnetic monopoles in the Universe.","Placing them in the interior of primordial rotating black holes, which act as natural amplifiers, we show that massive charged bosonic fields in their vicinity exhibit a superradiant instability which surpasses significantly that of neutral Kerr black holes.","Strikingly, this is true for black holes containing an order-one number of magnetic monopoles, or merely a single one, and possessing either low, moderate or large values of angular momentum.","In particular, the instability is drastically faster than the radiative decay time of charged pions, thus making it physically relevant.","Furthermore, our analysis identifies the most unstable modes as a class of monopole spheroidal harmonics, that we dub north and south monopole modes, whose morphology is markedly different from the usual superradiantly unstable modes since they extend along the rotational axis.","We conclude by discussing implications of our results for primordial magnetic black holes, and their observational signatures as sources of cosmic rays and high-frequency gravitational waves."],"url":"http://arxiv.org/abs/2402.05178v1","category":"gr-qc"}
{"created":"2024-02-07 19:00:02","title":"Electromagnetic signatures from accreting massive black hole binaries in time domain photometric surveys","abstract":"We study spectral and time variability of accreting massive black hole binaries (MBHBs) at milli-pc separations surrounded by a geometrically thin circumbinary disc. We present the first computed spectral energy distribution (SED) and light curves (LCs) from 3D hyper-Lagrangian resolution hydrodynamic simulations of these systems. We model binaries with mass of $10^6$ M$_\\odot$, eccentricities e=0, 0.9 and mass ratio q=0.1,1. The circumbinary disc has initial aspect ratio of 0.1, features an adiabatic equation of state, and evolves under the effect of viscous heating, black body cooling and self gravity. To compute the SED, we consider black body emission from each disc element and we add an X-ray corona with luminosity proportional to that of the mini-discs around each black hole. We find significant variability of the SED, especially at high energies, which translates into LCs displaying modulations of a factor of ~ 2 in optical and of ~ 10 in UV and X-rays. We focus on flux variability in the optical band which will be probed by the Vera Rubin Observatory (VRO). Modulations on the orbital period and half of the orbital period are evident in all systems. In equal mass binaries, we find another longer timescale modulation, linked to an over-density forming at the inner edge of the disc. Considering the VRO properties, we find that equal mass, circular binaries are unlikely to be identified, due to the lack of prominent peaks in their Fourier spectra. Conversely, unequal mass and/or eccentric binaries can be singled out up to z ~ 0.5 (for systems with $L_{\\rm bol}\\approx10^{42}$ erg s$^{-1}$) and z ~ 2 (for systems with $L_{\\rm bol}\\approx10^{44}$ erg s$^{-1}$). Identifying electromagnetic signatures of MBHBs at separations $\\sim 10^{-4}-10^{-2}$ pc is crucial for understanding the physics of the future Laser Interferometer Space Antenna sources and the origin of the GW background.","sentences":["We study spectral and time variability of accreting massive black hole binaries (MBHBs) at milli-pc separations surrounded by a geometrically thin circumbinary disc.","We present the first computed spectral energy distribution (SED) and light curves (LCs) from 3D hyper-Lagrangian resolution hydrodynamic simulations of these systems.","We model binaries with mass of $10^6$ M$_\\odot$, eccentricities e=0, 0.9 and mass ratio q=0.1,1.","The circumbinary disc has initial aspect ratio of 0.1, features an adiabatic equation of state, and evolves under the effect of viscous heating, black body cooling and self gravity.","To compute the SED, we consider black body emission from each disc element and we add an X-ray corona with luminosity proportional to that of the mini-discs around each black hole.","We find significant variability of the SED, especially at high energies, which translates into LCs displaying modulations of a factor of ~ 2 in optical and of ~ 10 in UV and X-rays.","We focus on flux variability in the optical band which will be probed by the Vera Rubin Observatory (VRO).","Modulations on the orbital period and half of the orbital period are evident in all systems.","In equal mass binaries, we find another longer timescale modulation, linked to an over-density forming at the inner edge of the disc.","Considering the VRO properties, we find that equal mass, circular binaries are unlikely to be identified, due to the lack of prominent peaks in their Fourier spectra.","Conversely, unequal mass and/or eccentric binaries can be singled out up to z ~ 0.5 (for systems with $L_{\\rm bol}\\approx10^{42}$ erg s$^{-1}$) and z ~ 2 (for systems with $L_{\\rm bol}\\approx10^{44}$ erg s$^{-1}$).","Identifying electromagnetic signatures of MBHBs at separations $\\sim 10^{-4}-10^{-2}$ pc is crucial for understanding the physics of the future Laser Interferometer Space Antenna sources and the origin of the GW background."],"url":"http://arxiv.org/abs/2402.05175v1","category":"astro-ph.HE"}
{"created":"2024-02-07 19:00:00","title":"Age uncertainties of red giants due to cumulative rotational mixing of progenitors calibrated by asteroseismology","abstract":"Galactic archaeology largely relies on precise ages of distant evolved stars in the Milky Way. Nowadays, asteroseismology can deliver ages for many red giants observed with high-cadence, high-precision photometric space missions. Our aim is to quantify age uncertainties of slowly-rotating red giants due to the cumulative effect of their fast rotation during core-hydrogen burning. Their rotation in earlier evolutionary phases caused mixing resulting in heavier helium cores and the prolongation of their main sequence. These rotational effects are usually ignored when age-dating red giants, despite our knowledge of fast rotation for stars with $M\\ge1.3\\,$M$_\\odot$. We use a sample of 490 $\\gamma$ Doradus pulsators with precise asteroseismic estimates of their internal rotation rate and with luminosity estimates from Gaia. For this sample, which includes stars rotating from nearly 0 to about 60% of the critical rate, we compute the cumulative effect on the age in their post-main sequence evolution caused by rotational mixing on the main sequence. We use stellar model grids with different physical prescriptions mimicking rotational mixing to assess systematic uncertainties on the age. With respect to non-rotating models, the sample of 490 stars, as red giant progenitors, reveals age differences up to 5% by the time they start hydrogen-shell burning when relying on the theory of rotationally induced diffusive mixing as included in the MIST isochrones. Using rotational mixing based on an advective-diffusive approach including meridional circulation leads to an age shift of 20% by the time of the TRGB. Age-dating of red giants is affected by the cumulative effect of rotational mixing during the main sequence. Such rotationally-induced age shifts should be taken into account in addition to other effects if the aim is to perform Galactic archaeological studies at the highest precision. (abridged)","sentences":["Galactic archaeology largely relies on precise ages of distant evolved stars in the Milky Way.","Nowadays, asteroseismology can deliver ages for many red giants observed with high-cadence, high-precision photometric space missions.","Our aim is to quantify age uncertainties of slowly-rotating red giants due to the cumulative effect of their fast rotation during core-hydrogen burning.","Their rotation in earlier evolutionary phases caused mixing resulting in heavier helium cores and the prolongation of their main sequence.","These rotational effects are usually ignored when age-dating red giants, despite our knowledge of fast rotation for stars with $M\\ge1.3\\,$M$_\\odot$. We use a sample of 490 $\\gamma$ Doradus pulsators with precise asteroseismic estimates of their internal rotation rate and with luminosity estimates from Gaia.","For this sample, which includes stars rotating from nearly 0 to about 60% of the critical rate, we compute the cumulative effect on the age in their post-main sequence evolution caused by rotational mixing on the main sequence.","We use stellar model grids with different physical prescriptions mimicking rotational mixing to assess systematic uncertainties on the age.","With respect to non-rotating models, the sample of 490 stars, as red giant progenitors, reveals age differences up to 5% by the time they start hydrogen-shell burning when relying on the theory of rotationally induced diffusive mixing as included in the MIST isochrones.","Using rotational mixing based on an advective-diffusive approach including meridional circulation leads to an age shift of 20% by the time of the TRGB.","Age-dating of red giants is affected by the cumulative effect of rotational mixing during the main sequence.","Such rotationally-induced age shifts should be taken into account in addition to other effects if the aim is to perform Galactic archaeological studies at the highest precision.","(abridged)"],"url":"http://arxiv.org/abs/2402.05168v1","category":"astro-ph.SR"}
{"created":"2024-02-07 16:53:24","title":"Stellar halo density with LAMOST K and M giants","abstract":"AIMS. We derive the morphology of the stellar component in the outer halo volume, and search for possible overdensities due to substructures therein.   METHODS. We made use of some of the data releases of the spectroscopic survey LAMOST DR8-DR9 in tandem with distance determinations for two subsamples, that is, of K-giants and M-giants, respectively, making up 60,000 stars. These distance are obtained through Bayesian techniques that derive absolute magnitudes as a function of measured spectroscopic parameters. Our calculation of the density from these catalogues requires: (1) derivation of the selection function; and (2) a correction for the convolution of the distance errors, which we carried out with Lucy's inversion of the corresponding integral equation.   RESULTS. The stellar density distribution of the outer halo (distance to the Galactic centre, $r_G$, of between 25 and 90 kpc) is a smooth monotonously decreasing function with a dependence of approximately $\\rho \\propto r_G^{-n}$, with $n=4.6\\pm 0.4$ for K-giants and $n=4.5\\pm 0.2$ for M-giants, and with a insignificant oblateness. The value of $n$ is independent of the angular distance to the Sagittarius tidal stream plane, which is what would be expected if such a stream did not exist in the anticenter positions or had a negligible imprint in the density distribution in the outer halo. Apart from random fluctuations or minor anomalies in some lines of sight, we do not see substructures superimposed in the outer halo volume within the resolution that we are using and limited by the error bars. This constrains the mass of over- and under-densities in the outer halo to be of $\\lesssim 10^3$ M$_\\odot $/deg$^2$, whereas the total mass of the stellar halo, including inner and outer parts, is $\\sim 7\\times 10^8$ M$_\\odot $.","sentences":["AIMS.","We derive the morphology of the stellar component in the outer halo volume, and search for possible overdensities due to substructures therein.   METHODS.","We made use of some of the data releases of the spectroscopic survey LAMOST DR8-DR9 in tandem with distance determinations for two subsamples, that is, of K-giants and M-giants, respectively, making up 60,000 stars.","These distance are obtained through Bayesian techniques that derive absolute magnitudes as a function of measured spectroscopic parameters.","Our calculation of the density from these catalogues requires: (1) derivation of the selection function; and (2) a correction for the convolution of the distance errors, which we carried out with Lucy's inversion of the corresponding integral equation.   RESULTS.","The stellar density distribution of the outer halo (distance to the Galactic centre, $r_G$, of between 25 and 90 kpc) is a smooth monotonously decreasing function with a dependence of approximately $\\rho \\propto r_G^{-n}$, with $n=4.6\\pm 0.4$ for K-giants and $n=4.5\\pm 0.2$ for M-giants, and with a insignificant oblateness.","The value of $n$ is independent of the angular distance to the Sagittarius tidal stream plane, which is what would be expected if such a stream did not exist in the anticenter positions or had a negligible imprint in the density distribution in the outer halo.","Apart from random fluctuations or minor anomalies in some lines of sight, we do not see substructures superimposed in the outer halo volume within the resolution that we are using and limited by the error bars.","This constrains the mass of over- and under-densities in the outer halo to be of $\\lesssim 10^3$ M$_\\odot $/deg$^2$, whereas the total mass of the stellar halo, including inner and outer parts, is $\\sim 7\\times 10^8$ M$_\\odot $."],"url":"http://arxiv.org/abs/2402.05157v1","category":"astro-ph.GA"}
{"created":"2024-02-08 17:57:03","title":"\"Can You Play Anything Else?\" Understanding Play Style Flexibility in League of Legends","abstract":"This study investigates the concept of flexibility within League of Legends, a popular online multiplayer game, focusing on the relationship between user adaptability and team success. Utilizing a dataset encompassing players of varying skill levels and play styles, we calculate two measures of flexibility for each player: overall flexibility and temporal flexibility. Our findings suggest that the flexibility of a user is dependent upon a user's preferred play style, and flexibility does impact match outcome. This work also shows that skill level not only indicates how willing a player is to adapt their play style but also how their adaptability changes over time. This paper highlights the the duality and balance of mastery versus flexibility, providing insights that can inform strategic planning, collaboration and resource allocation in competitive environments.","sentences":["This study investigates the concept of flexibility within League of Legends, a popular online multiplayer game, focusing on the relationship between user adaptability and team success.","Utilizing a dataset encompassing players of varying skill levels and play styles, we calculate two measures of flexibility for each player: overall flexibility and temporal flexibility.","Our findings suggest that the flexibility of a user is dependent upon a user's preferred play style, and flexibility does impact match outcome.","This work also shows that skill level not only indicates how willing a player is to adapt their play style but also how their adaptability changes over time.","This paper highlights the the duality and balance of mastery versus flexibility, providing insights that can inform strategic planning, collaboration and resource allocation in competitive environments."],"url":"http://arxiv.org/abs/2402.05865v1","category":"cs.HC"}
{"created":"2024-02-08 16:59:24","title":"Guided Evolution with Binary Discriminators for ML Program Search","abstract":"How to automatically design better machine learning programs is an open problem within AutoML. While evolution has been a popular tool to search for better ML programs, using learning itself to guide the search has been less successful and less understood on harder problems but has the promise to dramatically increase the speed and final performance of the optimization process. We propose guiding evolution with a binary discriminator, trained online to distinguish which program is better given a pair of programs. The discriminator selects better programs without having to perform a costly evaluation and thus speed up the convergence of evolution. Our method can encode a wide variety of ML components including symbolic optimizers, neural architectures, RL loss functions, and symbolic regression equations with the same directed acyclic graph representation. By combining this representation with modern GNNs and an adaptive mutation strategy, we demonstrate our method can speed up evolution across a set of diverse problems including a 3.7x speedup on the symbolic search for ML optimizers and a 4x speedup for RL loss functions.","sentences":["How to automatically design better machine learning programs is an open problem within AutoML.","While evolution has been a popular tool to search for better ML programs, using learning itself to guide the search has been less successful and less understood on harder problems but has the promise to dramatically increase the speed and final performance of the optimization process.","We propose guiding evolution with a binary discriminator, trained online to distinguish which program is better given a pair of programs.","The discriminator selects better programs without having to perform a costly evaluation and thus speed up the convergence of evolution.","Our method can encode a wide variety of ML components including symbolic optimizers, neural architectures, RL loss functions, and symbolic regression equations with the same directed acyclic graph representation.","By combining this representation with modern GNNs and an adaptive mutation strategy, we demonstrate our method can speed up evolution across a set of diverse problems including a 3.7x speedup on the symbolic search for ML optimizers and a 4x speedup for RL loss functions."],"url":"http://arxiv.org/abs/2402.05821v1","category":"cs.LG"}
{"created":"2024-02-08 16:45:12","title":"On Calibration and Conformal Prediction of Deep Classifiers","abstract":"In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive CP methods: it frequently leads to larger prediction sets. Then, we turn to theoretically analyze this behavior. We reveal several mathematical properties of the procedure, according to which we provide a reasoning for the phenomenon. Our study suggests that it may be worthwhile to utilize adaptive CP methods, chosen for their enhanced conditional coverage, based on softmax values prior to (or after canceling) temperature scaling calibration.","sentences":["In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication.","Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced.","While in practice both types of indications can be desired, so far the interplay between them has not been investigated.","Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods.","We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive CP methods: it frequently leads to larger prediction sets.","Then, we turn to theoretically analyze this behavior.","We reveal several mathematical properties of the procedure, according to which we provide a reasoning for the phenomenon.","Our study suggests that it may be worthwhile to utilize adaptive CP methods, chosen for their enhanced conditional coverage, based on softmax values prior to (or after canceling) temperature scaling calibration."],"url":"http://arxiv.org/abs/2402.05806v1","category":"cs.LG"}
{"created":"2024-02-08 15:21:48","title":"Learning Families of Algebraic Structures from Text","abstract":"We adapt the classical notion of learning from text to computable structure theory. Our main result is a model-theoretic characterization of the learnability from text for classes of structures. We show that a family of structures is learnable from text if and only if the structures can be distinguished in terms of their theories restricted to positive infinitary $\\Sigma_2$ sentences.","sentences":["We adapt the classical notion of learning from text to computable structure theory.","Our main result is a model-theoretic characterization of the learnability from text for classes of structures.","We show that a family of structures is learnable from text if and only if the structures can be distinguished in terms of their theories restricted to positive infinitary $\\Sigma_2$ sentences."],"url":"http://arxiv.org/abs/2402.05744v1","category":"math.LO"}
{"created":"2024-02-08 15:21:29","title":"Hydrogen abstraction from metal surfaces: When electron-hole pair excitations strongly affect hot-atom recombination","abstract":"Using molecular dynamics simulations, we predict that the inclusion of nonadiabatic electronic excitations influences the dynamics of preadsorbed hydrogen abstraction from the W(110) surface by hydrogen scattering. The hot-atom recombination, which involves hyperthermal diffusion of the impinging atom on the surface, is significantly affected by the dissipation of energy mediated by electron-hole pair excitations at low coverage and low incidence energy. This issue is of importance as this abstraction mechanism is thought to largely contribute to molecular hydrogen formation from metal surfaces.","sentences":["Using molecular dynamics simulations, we predict that the inclusion of nonadiabatic electronic excitations influences the dynamics of preadsorbed hydrogen abstraction from the W(110) surface by hydrogen scattering.","The hot-atom recombination, which involves hyperthermal diffusion of the impinging atom on the surface, is significantly affected by the dissipation of energy mediated by electron-hole pair excitations at low coverage and low incidence energy.","This issue is of importance as this abstraction mechanism is thought to largely contribute to molecular hydrogen formation from metal surfaces."],"url":"http://arxiv.org/abs/2402.05743v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-08 15:15:09","title":"Implicit Bias and Fast Convergence Rates for Self-attention","abstract":"Self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance. Towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (GD) in training a self-attention layer with fixed linear decoder in binary classification. Drawing inspiration from the study of GD in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects. Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape. Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in the attention map. Thirdly, through an analysis of normalized GD and Polyak step-size, we demonstrate analytically that adaptive step-size rules can accelerate the convergence of self-attention. Additionally, we remove the restriction of prior work on a fixed linear decoder. Our results reinforce the implicit-bias perspective of self-attention and strengthen its connections to implicit-bias in linear logistic regression, despite the intricate non-convex nature of the former.","sentences":["Self-attention, the core mechanism of transformers, distinguishes them from traditional neural networks and drives their outstanding performance.","Towards developing the fundamental optimization principles of self-attention, we investigate the implicit bias of gradient descent (GD) in training a self-attention layer with fixed linear decoder in binary classification.","Drawing inspiration from the study of GD in linear logistic regression over separable data, recent work demonstrates that as the number of iterations $t$ approaches infinity, the key-query matrix $W_t$ converges locally (with respect to the initialization direction) to a hard-margin SVM solution $W_{mm}$. Our work enhances this result in four aspects.","Firstly, we identify non-trivial data settings for which convergence is provably global, thus shedding light on the optimization landscape.","Secondly, we provide the first finite-time convergence rate for $W_t$ to $W_{mm}$, along with quantifying the rate of sparsification in the attention map.","Thirdly, through an analysis of normalized GD and Polyak step-size, we demonstrate analytically that adaptive step-size rules can accelerate the convergence of self-attention.","Additionally, we remove the restriction of prior work on a fixed linear decoder.","Our results reinforce the implicit-bias perspective of self-attention and strengthen its connections to implicit-bias in linear logistic regression, despite the intricate non-convex nature of the former."],"url":"http://arxiv.org/abs/2402.05738v1","category":"cs.LG"}
{"created":"2024-02-08 12:53:01","title":"Boosting Dynamic TDD in Small Cell Networks by the Multiplicative Weight Update Method","abstract":"We leverage the Multiplicative Weight Update (MWU) method to develop a decentralized algorithm that significantly improves the performance of dynamic time division duplexing (D-TDD) in small cell networks. The proposed algorithm adaptively adjusts the time portion allocated to uplink (UL) and downlink (DL) transmissions at every node during each scheduled time slot, aligning the packet transmissions toward the most appropriate link directions according to the feedback of signal-to-interference ratio information. Our simulation results reveal that compared to the (conventional) fixed configuration of UL/DL transmission probabilities in D-TDD, incorporating MWU into D-TDD brings about a two-fold improvement of mean packet throughput in the DL and a three-fold improvement of the same performance metric in the UL, resulting in the D-TDD even outperforming Static-TDD in the UL. It also shows that the proposed scheme maintains a consistent performance gain in the presence of an ascending traffic load, validating its effectiveness in boosting the network performance. This work also demonstrates an approach that accounts for algorithmic considerations at the forefront when solving stochastic problems.","sentences":["We leverage the Multiplicative Weight Update (MWU) method to develop a decentralized algorithm that significantly improves the performance of dynamic time division duplexing (D-TDD) in small cell networks.","The proposed algorithm adaptively adjusts the time portion allocated to uplink (UL) and downlink (DL) transmissions at every node during each scheduled time slot, aligning the packet transmissions toward the most appropriate link directions according to the feedback of signal-to-interference ratio information.","Our simulation results reveal that compared to the (conventional) fixed configuration of UL/DL transmission probabilities in D-TDD, incorporating MWU into D-TDD brings about a two-fold improvement of mean packet throughput in the DL and a three-fold improvement of the same performance metric in the UL, resulting in the D-TDD even outperforming Static-TDD in the UL.","It also shows that the proposed scheme maintains a consistent performance gain in the presence of an ascending traffic load, validating its effectiveness in boosting the network performance.","This work also demonstrates an approach that accounts for algorithmic considerations at the forefront when solving stochastic problems."],"url":"http://arxiv.org/abs/2402.05641v1","category":"cs.IT"}
{"created":"2024-02-08 12:38:12","title":"ShiftDTW: adapting the DTW metric for cyclic time series clustering","abstract":"The elasticity of the DTW metric provides a more flexible comparison between time series and is used in numerous machine learning domains such as classification or clustering. However, it does not align the measurements at the beginning and end of time series if they have a shift occurring right at the start of one series, with the omitted part appearing at the end of that series. Due to the cyclicity of such series - which lack a definite beginning or end - we rely on the Cyclic DTW approach to propose a less computationally expensive approximation of this calculation method. This approximation will then be employed in conjunction with the K-Means clustering method.","sentences":["The elasticity of the DTW metric provides a more flexible comparison between time series and is used in numerous machine learning domains such as classification or clustering.","However, it does not align the measurements at the beginning and end of time series if they have a shift occurring right at the start of one series, with the omitted part appearing at the end of that series.","Due to the cyclicity of such series - which lack a definite beginning or end - we rely on the Cyclic DTW approach to propose a less computationally expensive approximation of this calculation method.","This approximation will then be employed in conjunction with the K-Means clustering method."],"url":"http://arxiv.org/abs/2402.05631v1","category":"cs.DS"}
{"created":"2024-02-08 11:40:50","title":"RESMatch: Referring Expression Segmentation in a Semi-Supervised Manner","abstract":"Referring expression segmentation (RES), a task that involves localizing specific instance-level objects based on free-form linguistic descriptions, has emerged as a crucial frontier in human-AI interaction. It demands an intricate understanding of both visual and textual contexts and often requires extensive training data. This paper introduces RESMatch, the first semi-supervised learning (SSL) approach for RES, aimed at reducing reliance on exhaustive data annotation. Extensive validation on multiple RES datasets demonstrates that RESMatch significantly outperforms baseline approaches, establishing a new state-of-the-art. Although existing SSL techniques are effective in image segmentation, we find that they fall short in RES. Facing the challenges including the comprehension of free-form linguistic descriptions and the variability in object attributes, RESMatch introduces a trifecta of adaptations: revised strong perturbation, text augmentation, and adjustments for pseudo-label quality and strong-weak supervision. This pioneering work lays the groundwork for future research in semi-supervised learning for referring expression segmentation.","sentences":["Referring expression segmentation (RES), a task that involves localizing specific instance-level objects based on free-form linguistic descriptions, has emerged as a crucial frontier in human-AI interaction.","It demands an intricate understanding of both visual and textual contexts and often requires extensive training data.","This paper introduces RESMatch, the first semi-supervised learning (SSL) approach for RES, aimed at reducing reliance on exhaustive data annotation.","Extensive validation on multiple RES datasets demonstrates that RESMatch significantly outperforms baseline approaches, establishing a new state-of-the-art.","Although existing SSL techniques are effective in image segmentation, we find that they fall short in RES.","Facing the challenges including the comprehension of free-form linguistic descriptions and the variability in object attributes, RESMatch introduces a trifecta of adaptations: revised strong perturbation, text augmentation, and adjustments for pseudo-label quality and strong-weak supervision.","This pioneering work lays the groundwork for future research in semi-supervised learning for referring expression segmentation."],"url":"http://arxiv.org/abs/2402.05589v1","category":"cs.CV"}
{"created":"2024-02-08 11:09:27","title":"Neural Graphics Primitives-based Deformable Image Registration for On-the-fly Motion Extraction","abstract":"Intra-fraction motion in radiotherapy is commonly modeled using deformable image registration (DIR). However, existing methods often struggle to balance speed and accuracy, limiting their applicability in clinical scenarios. This study introduces a novel approach that harnesses Neural Graphics Primitives (NGP) to optimize the displacement vector field (DVF). Our method leverages learned primitives, processed as splats, and interpolates within space using a shallow neural network. Uniquely, it enables self-supervised optimization at an ultra-fast speed, negating the need for pre-training on extensive datasets and allowing seamless adaptation to new cases. We validated this approach on the 4D-CT lung dataset DIR-lab, achieving a target registration error (TRE) of 1.15\\pm1.15 mm within a remarkable time of 1.77 seconds. Notably, our method also addresses the sliding boundary problem, a common challenge in conventional DIR methods.","sentences":["Intra-fraction motion in radiotherapy is commonly modeled using deformable image registration (DIR).","However, existing methods often struggle to balance speed and accuracy, limiting their applicability in clinical scenarios.","This study introduces a novel approach that harnesses Neural Graphics Primitives (NGP) to optimize the displacement vector field (DVF).","Our method leverages learned primitives, processed as splats, and interpolates within space using a shallow neural network.","Uniquely, it enables self-supervised optimization at an ultra-fast speed, negating the need for pre-training on extensive datasets and allowing seamless adaptation to new cases.","We validated this approach on the 4D-CT lung dataset DIR-lab, achieving a target registration error (TRE) of 1.15\\pm1.15 mm within a remarkable time of 1.77 seconds.","Notably, our method also addresses the sliding boundary problem, a common challenge in conventional DIR methods."],"url":"http://arxiv.org/abs/2402.05568v1","category":"physics.med-ph"}
{"created":"2024-02-08 09:23:34","title":"The stability analysis based on viscous theory of Faraday waves in Hele-Shaw cells","abstract":"The linear instability of Faraday waves in Hele-Shaw cells is investigated with consideration of the viscosity of fluids after gap-averaging the governing equations due to the damping from two lateral walls and the dynamic behavior of contact angle. A new hydrodynamic model is thus derived and solved semi-analytically. The contribution of viscosity to critical acceleration amplitude is slight compared to other factors associated with dissipation, and the potential flow theory is sufficient to describe onset based on the present study, but the rotational component of velocity can change the timing of onset largely, which paradoxically comes from the viscosity. The model degenerates into a novel damped Mathieu equation if the viscosity is dropped with two damping terms referring to the gap-averaged damping and dissipation from dynamic contact angle, respectively. The former increases when the gap size decreases, and the latter grows as frequency rises. When it comes to the dispersion relation of Faraday waves, an unusual detuning emerges due to the imaginary part of the gap-averaged damping.","sentences":["The linear instability of Faraday waves in Hele-Shaw cells is investigated with consideration of the viscosity of fluids after gap-averaging the governing equations due to the damping from two lateral walls and the dynamic behavior of contact angle.","A new hydrodynamic model is thus derived and solved semi-analytically.","The contribution of viscosity to critical acceleration amplitude is slight compared to other factors associated with dissipation, and the potential flow theory is sufficient to describe onset based on the present study, but the rotational component of velocity can change the timing of onset largely, which paradoxically comes from the viscosity.","The model degenerates into a novel damped Mathieu equation if the viscosity is dropped with two damping terms referring to the gap-averaged damping and dissipation from dynamic contact angle, respectively.","The former increases when the gap size decreases, and the latter grows as frequency rises.","When it comes to the dispersion relation of Faraday waves, an unusual detuning emerges due to the imaginary part of the gap-averaged damping."],"url":"http://arxiv.org/abs/2402.05505v1","category":"physics.flu-dyn"}
{"created":"2024-02-08 08:23:33","title":"A Non-Intrusive Neural Quality Assessment Model for Surface Electromyography Signals","abstract":"In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals. To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals. QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy. Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively. The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth. These findings show the potential of QASE-net to substantially enhance the reliability and precision of sEMG quality assessment in practical applications.","sentences":["In practical scenarios involving the measurement of surface electromyography (sEMG) in muscles, particularly those areas near the heart, one of the primary sources of contamination is the presence of electrocardiogram (ECG) signals.","To assess the quality of real-world sEMG data more effectively, this study proposes QASE-net, a new non-intrusive model that predicts the SNR of sEMG signals.","QASE-net combines CNN-BLSTM with attention mechanisms and follows an end-to-end training strategy.","Our experimental framework utilizes real-world sEMG and ECG data from two open-access databases, the Non-Invasive Adaptive Prosthetics Database and the MIT-BIH Normal Sinus Rhythm Database, respectively.","The experimental results demonstrate the superiority of QASE-net over the previous assessment model, exhibiting significantly reduced prediction errors and notably higher linear correlations with the ground truth.","These findings show the potential of QASE-net to substantially enhance the reliability and precision of sEMG quality assessment in practical applications."],"url":"http://arxiv.org/abs/2402.05482v1","category":"eess.SP"}
{"created":"2024-02-08 03:17:38","title":"Zero-Shot Chain-of-Thought Reasoning Guided by Evolutionary Algorithms in Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting. However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal. In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically. Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem. Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem. Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of our proposed method compared to current zero-shot CoT prompting methods on GPT-3.5-turbo and GPT-4. Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various reasoning tasks.","sentences":["Large Language Models (LLMs) have demonstrated remarkable performance across diverse tasks and exhibited impressive reasoning abilities by applying zero-shot Chain-of-Thought (CoT) prompting.","However, due to the evolving nature of sentence prefixes during the pre-training phase, existing zero-shot CoT prompting methods that employ identical CoT prompting across all task instances may not be optimal.","In this paper, we introduce a novel zero-shot prompting method that leverages evolutionary algorithms to generate diverse promptings for LLMs dynamically.","Our approach involves initializing two CoT promptings, performing evolutionary operations based on LLMs to create a varied set, and utilizing the LLMs to select a suitable CoT prompting for a given problem.","Additionally, a rewriting operation, guided by the selected CoT prompting, enhances the understanding of the LLMs about the problem.","Extensive experiments conducted across ten reasoning datasets demonstrate the superior performance of our proposed method compared to current zero-shot CoT prompting methods on GPT-3.5-turbo and GPT-4.","Moreover, in-depth analytical experiments underscore the adaptability and effectiveness of our method in various reasoning tasks."],"url":"http://arxiv.org/abs/2402.05376v1","category":"cs.CL"}
{"created":"2024-02-08 02:58:02","title":"Bounded-Confidence Models of Opinion Dynamics with Neighborhood Effects","abstract":"As people's opinions change, their social networks typically coevolve with them. People are often more susceptible to influence by people with similar opinions than by people with dissimilar opinions. In a bounded-confidence model (BCM) of opinion dynamics, interacting individuals influence each other through dyadic influence if and only if their opinions are sufficiently similar to each other. We introduce `neighborhood BCMs' (NBCMs) that include both the usual dyadic influence and a transitive influence, which models the effect of friends of a friend when determining whether or not an interaction with a friend influences an individual. In this transitive influence, an individual's opinion is influenced by a neighbor when, on average, the opinions of the neighbor's neighbors are sufficiently similar to their own opinion. We formulate neighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK) BCMs. We simulate our NDW model on time-independent networks and observe interesting opinion states that cannot occur in an associated baseline DW model. We also simulate our NDW model on adaptive networks that coevolve with opinions by changing its structure through `transitive homophily'. An individual that breaks a tie to one of its neighbors and then rewires that tie to a new individual, with a preference for individuals with a mean neighbor opinion that is closer to that individual's opinion. We explore how the qualitative opinion dynamics and network properties of our time-independent and adaptive NDWM models change as we adjust the relative proportions of dyadic and transitive influence. Finally, we study a two-layer opinion--disease model in which we couple our NDW model with disease spread through a shared adaptive network that can change both on the opinion layer and on the disease layer and we examine how the opinion dynamics affect disease spread.","sentences":["As people's opinions change, their social networks typically coevolve with them.","People are often more susceptible to influence by people with similar opinions than by people with dissimilar opinions.","In a bounded-confidence model (BCM) of opinion dynamics, interacting individuals influence each other through dyadic influence if and only if their opinions are sufficiently similar to each other.","We introduce `neighborhood BCMs' (NBCMs) that include both the usual dyadic influence and a transitive influence, which models the effect of friends of a friend when determining whether or not an interaction with a friend influences an individual.","In this transitive influence, an individual's opinion is influenced by a neighbor when, on average, the opinions of the neighbor's neighbors are sufficiently similar to their own opinion.","We formulate neighborhood Deffuant--Weisbuch (NDW) and neighborhood Hegselmann--Krause (NHK) BCMs.","We simulate our NDW model on time-independent networks and observe interesting opinion states that cannot occur in an associated baseline DW model.","We also simulate our NDW model on adaptive networks that coevolve with opinions by changing its structure through `transitive homophily'.","An individual that breaks a tie to one of its neighbors and then rewires that tie to a new individual, with a preference for individuals with a mean neighbor opinion that is closer to that individual's opinion.","We explore how the qualitative opinion dynamics and network properties of our time-independent and adaptive NDWM models change as we adjust the relative proportions of dyadic and transitive influence.","Finally, we study a two-layer opinion--disease model in which we couple our NDW model with disease spread through a shared adaptive network that can change both on the opinion layer and on the disease layer and we examine how the opinion dynamics affect disease spread."],"url":"http://arxiv.org/abs/2402.05368v1","category":"physics.soc-ph"}
{"created":"2024-02-08 00:50:22","title":"Tidal Dissipation in Giant Planets","abstract":"Tidal interactions between moons and planets can have major effects on the orbits, spins, and thermal evolution of the moons. In the Saturn system, tidal dissipation in the planet transfers angular momentum from Saturn to the moons, causing them to migrate outwards. The rate of migration is determined by the mechanism of dissipation within the planet, which is closely tied to the planet's uncertain structure. We review current knowledge of giant planet internal structure and evolution, which has improved thanks to data from the \\textit{Juno} and \\textit{Cassini} missions. We discuss general principles of tidal dissipation, describing both equilibrium and dynamical tides, and how dissipation can occur in a solid core or a fluid envelope. Finally, we discuss the possibility of resonance locking, whereby a moon can lock into resonance with a planetary oscillation mode, producing enhanced tidal migration relative to classical theories, and possibly explaining recent measurements of moon migration rates.","sentences":["Tidal interactions between moons and planets can have major effects on the orbits, spins, and thermal evolution of the moons.","In the Saturn system, tidal dissipation in the planet transfers angular momentum from Saturn to the moons, causing them to migrate outwards.","The rate of migration is determined by the mechanism of dissipation within the planet, which is closely tied to the planet's uncertain structure.","We review current knowledge of giant planet internal structure and evolution, which has improved thanks to data from the \\textit{Juno} and \\textit{Cassini} missions.","We discuss general principles of tidal dissipation, describing both equilibrium and dynamical tides, and how dissipation can occur in a solid core or a fluid envelope.","Finally, we discuss the possibility of resonance locking, whereby a moon can lock into resonance with a planetary oscillation mode, producing enhanced tidal migration relative to classical theories, and possibly explaining recent measurements of moon migration rates."],"url":"http://arxiv.org/abs/2402.05338v1","category":"astro-ph.EP"}
{"created":"2024-02-08 00:23:42","title":"Domain-Agnostic Hardware Fingerprinting-Based Device Identifier for Zero-Trust IoT Security","abstract":"Next-generation networks aim for comprehensive connectivity, interconnecting humans, machines, devices, and systems seamlessly. This interconnectivity raises concerns about privacy and security, given the potential network-wide impact of a single compromise. To address this challenge, the Zero Trust (ZT) paradigm emerges as a key method for safeguarding network integrity and data confidentiality. This work introduces EPS-CNN, a novel deep-learning-based wireless device identification framework designed to serve as the device authentication layer within the ZT architecture, with a focus on resource-constrained IoT devices. At the core of EPS-CNN, a Convolutional Neural Network (CNN) is utilized to generate the device identity from a unique RF signal representation, known as the Double-Sided Envelope Power Spectrum (EPS), which effectively captures the device-specific hardware characteristics while ignoring device-unrelated information. Experimental evaluations show that the proposed framework achieves over 99%, 93%, and 95% testing accuracy when tested in same-domain (day, location, and channel), cross-day, and cross-location scenarios, respectively. Our findings demonstrate the superiority of the proposed framework in enhancing the accuracy, robustness, and adaptability of deep learning-based methods, thus offering a pioneering solution for enabling ZT IoT device identification.","sentences":["Next-generation networks aim for comprehensive connectivity, interconnecting humans, machines, devices, and systems seamlessly.","This interconnectivity raises concerns about privacy and security, given the potential network-wide impact of a single compromise.","To address this challenge, the Zero Trust (ZT) paradigm emerges as a key method for safeguarding network integrity and data confidentiality.","This work introduces EPS-CNN, a novel deep-learning-based wireless device identification framework designed to serve as the device authentication layer within the ZT architecture, with a focus on resource-constrained IoT devices.","At the core of EPS-CNN, a Convolutional Neural Network (CNN) is utilized to generate the device identity from a unique RF signal representation, known as the Double-Sided Envelope Power Spectrum (EPS), which effectively captures the device-specific hardware characteristics while ignoring device-unrelated information.","Experimental evaluations show that the proposed framework achieves over 99%, 93%, and 95% testing accuracy when tested in same-domain (day, location, and channel), cross-day, and cross-location scenarios, respectively.","Our findings demonstrate the superiority of the proposed framework in enhancing the accuracy, robustness, and adaptability of deep learning-based methods, thus offering a pioneering solution for enabling ZT IoT device identification."],"url":"http://arxiv.org/abs/2402.05332v1","category":"cs.CR"}
{"created":"2024-02-07 23:02:04","title":"Corotational modeling and NURBS-based kinematic constraint implementation in three-dimensional vehicle-track-structure interaction analysis","abstract":"An algorithm for three-dimensional dynamic vehicle-track-structure interaction (VTSI) analysis is described in this paper. The algorithm is described in terms of bridges and high-speed trains, but more generally applies to multibody systems coupled to deformable structures by time-varying kinematic constraints. Coupling is accomplished by a kinematic constraint/Lagrange multiplier approach, resulting in a system of index-3 Differential Algebraic Equations (DAE). Three main new concepts are developed. (i) A corotational approach is used to represent the vehicle (train) dynamics. Reference coordinate frames are fitted to the undeformed geometry of the bridge. While the displacements of the train can be large, deformations are taken to be small within these frames, resulting in linear (time-varying) rather than nonlinear dynamics. (ii) If conventional finite elements are used to discretize the track, the curvature is discontinuous across elements (and possibly rotation, too, for curved tracks). This results in spurious numerical oscillations in computed contact forces and accelerations, quantities of key interest in VTSI. A NURBS-based discretization is employed for the track to mitigate such oscillations. (iii) The higher order continuity due to using NURBS allows for alternative techniques for solving the VTSI system. First, enforcing constraints at the acceleration level reduces an index-3 DAE to an index-1 system that can be solved without numerical dissipation. Second, a constraint projection method is proposed to solve an index-3 DAE system without numerical dissipation by correcting wheel velocities and accelerations. Moreover, the modularity of the presented algorithm, resulting from a kinematic constraint/Lagrange multiplier formulation, enables ready integration of this VTSI approach in existing structural analysis and finite element software.","sentences":["An algorithm for three-dimensional dynamic vehicle-track-structure interaction (VTSI) analysis is described in this paper.","The algorithm is described in terms of bridges and high-speed trains, but more generally applies to multibody systems coupled to deformable structures by time-varying kinematic constraints.","Coupling is accomplished by a kinematic constraint/Lagrange multiplier approach, resulting in a system of index-3 Differential Algebraic Equations (DAE).","Three main new concepts are developed.","(i)","A corotational approach is used to represent the vehicle (train) dynamics.","Reference coordinate frames are fitted to the undeformed geometry of the bridge.","While the displacements of the train can be large, deformations are taken to be small within these frames, resulting in linear (time-varying) rather than nonlinear dynamics.","(ii) If conventional finite elements are used to discretize the track, the curvature is discontinuous across elements (and possibly rotation, too, for curved tracks).","This results in spurious numerical oscillations in computed contact forces and accelerations, quantities of key interest in VTSI.","A NURBS-based discretization is employed for the track to mitigate such oscillations.","(iii) The higher order continuity due to using NURBS allows for alternative techniques for solving the VTSI system.","First, enforcing constraints at the acceleration level reduces an index-3 DAE to an index-1 system that can be solved without numerical dissipation.","Second, a constraint projection method is proposed to solve an index-3 DAE system without numerical dissipation by correcting wheel velocities and accelerations.","Moreover, the modularity of the presented algorithm, resulting from a kinematic constraint/Lagrange multiplier formulation, enables ready integration of this VTSI approach in existing structural analysis and finite element software."],"url":"http://arxiv.org/abs/2402.05308v1","category":"math.NA"}
{"created":"2024-02-07 22:39:34","title":"Training DNN Models over Heterogeneous Clusters with Optimal Performance","abstract":"Adjusting batch sizes and adaptively tuning other hyperparameters can significantly speed up deep neural network (DNN) training. Despite the ubiquity of heterogeneous clusters, existing adaptive DNN training techniques solely consider homogeneous environments. Optimizing distributed DNN training over heterogeneous clusters is technically challenging, and directly adapting existing techniques results in low utilization and poor performance. To solve this problem, we introduce Cannikin -- a novel data-parallel distributed training system. Cannikin achieves efficient and near-optimal performance by accurately modeling the optimal system performance and predicting adaptive batch size training metrics for DNNs in heterogeneous clusters. We implemented Cannikin in PyTorch and conducted experiments over 16 GPUs in Chameleon. Empirical results show that Cannikin reduces DNN training in heterogeneous clusters by up to $52\\%$ compared to the state-of-the-art adaptive training system and up to $85\\%$ compared to native PyTorch DistributedDataParallel.","sentences":["Adjusting batch sizes and adaptively tuning other hyperparameters can significantly speed up deep neural network (DNN) training.","Despite the ubiquity of heterogeneous clusters, existing adaptive DNN training techniques solely consider homogeneous environments.","Optimizing distributed DNN training over heterogeneous clusters is technically challenging, and directly adapting existing techniques results in low utilization and poor performance.","To solve this problem, we introduce Cannikin -- a novel data-parallel distributed training system.","Cannikin achieves efficient and near-optimal performance by accurately modeling the optimal system performance and predicting adaptive batch size training metrics for DNNs in heterogeneous clusters.","We implemented Cannikin in PyTorch and conducted experiments over 16 GPUs in Chameleon.","Empirical results show that Cannikin reduces DNN training in heterogeneous clusters by up to $52\\%$ compared to the state-of-the-art adaptive training system and up to $85\\%$ compared to native PyTorch DistributedDataParallel."],"url":"http://arxiv.org/abs/2402.05302v1","category":"cs.DC"}
{"created":"2024-02-07 21:43:17","title":"ASCENT: A Context-Aware Spectrum Coexistence Design and Implementation Toolset for Policymakers in Satellite Bands","abstract":"This paper introduces ASCENT (context Aware Spectrum Coexistence Design and Implementation) toolset, an advanced context-aware terrestrial satellite spectrum sharing toolset designed for researchers, policymakers, and regulators. It serves two essential purposes (a) evaluating the potential for harmful interference to primary users in satellite bands and (b) facilitating the analysis, design, and implementation of diverse regulatory policies on spectrum usage and sharing. Notably, ASCENT implements a closed-loop feedback system that allows dynamic adaptation of policies according to a wide range of contextual factors (e.g., weather, buildings, summer/winter foliage, etc.) and feedback on the impact of these policies through realistic simulation. Specifically, ASCENT comprises the following components (i) interference evaluation tool for evaluating interference at the incumbents in a spectrum-sharing environment while taking the underlying contexts, (ii) dynamic spectrum access (DSA) framework for providing context-aware instructions to adapt networking parameters and control secondary terrestrial network's access to the shared spectrum band according to context aware prioritization, (iii) Context broker to acquire essential and relevant contexts from external context information providers; and (iv) DSA Database to store dynamic and static contexts and the regulator's policy information. The closed-loop feedback system of ASCENT is implemented by integrating these components in a modular software architecture. A case study of sharing the lower 12 GHz Ku band (12.2-12.7 GHz) with the 5G terrestrial cellular network is considered, and the usability of ASCENT is demonstrated by dynamically changing exclusion zone's radius in different weather conditions.","sentences":["This paper introduces ASCENT (context Aware Spectrum Coexistence Design and Implementation) toolset, an advanced context-aware terrestrial satellite spectrum sharing toolset designed for researchers, policymakers, and regulators.","It serves two essential purposes (a) evaluating the potential for harmful interference to primary users in satellite bands and (b) facilitating the analysis, design, and implementation of diverse regulatory policies on spectrum usage and sharing.","Notably, ASCENT implements a closed-loop feedback system that allows dynamic adaptation of policies according to a wide range of contextual factors (e.g., weather, buildings, summer/winter foliage, etc.) and feedback on the impact of these policies through realistic simulation.","Specifically, ASCENT comprises the following components (i) interference evaluation tool for evaluating interference at the incumbents in a spectrum-sharing environment while taking the underlying contexts, (ii) dynamic spectrum access (DSA) framework for providing context-aware instructions to adapt networking parameters and control secondary terrestrial network's access to the shared spectrum band according to context aware prioritization, (iii) Context broker to acquire essential and relevant contexts from external context information providers; and (iv) DSA Database to store dynamic and static contexts and the regulator's policy information.","The closed-loop feedback system of ASCENT is implemented by integrating these components in a modular software architecture.","A case study of sharing the lower 12 GHz Ku band (12.2-12.7 GHz) with the 5G terrestrial cellular network is considered, and the usability of ASCENT is demonstrated by dynamically changing exclusion zone's radius in different weather conditions."],"url":"http://arxiv.org/abs/2402.05273v1","category":"eess.SY"}
{"created":"2024-02-07 21:13:50","title":"Position and Speed Control of Brushless DC Motors Using Sensorless Techniques and Application Trends","abstract":"This paper provides a technical review of position and speed sensorless methods for controlling Brushless Direct Current (BLDC) motor drives, including the background analysis using sensors, limitations and advances. The performance and reliability of BLDC motor drivers have been improved because the conventional control and sensing techniques have been improved through sensorless technology. Then, in this paper sensorless advances are reviewed and recent developments in this area are introduced with their inherent advantages and drawbacks, including the analysis of practical implementation issues and applications. The study includes a deep overview of state-of-the-art back-EMF sensing methods, which includes Terminal Voltage Sensing, Third Harmonic Voltage Integration, Terminal Current Sensing, Back-EMF Integration and PWM strategies. Also, the most relevant techniques based on estimation and models are briefly analysed, such as Sliding-mode Observer, Extended Kalman Filter, Model Reference Adaptive System, Adaptive observers (Full-order and Pseudoreduced-order) and Artificial Neural Networks.","sentences":["This paper provides a technical review of position and speed sensorless methods for controlling Brushless Direct Current (BLDC) motor drives, including the background analysis using sensors, limitations and advances.","The performance and reliability of BLDC motor drivers have been improved because the conventional control and sensing techniques have been improved through sensorless technology.","Then, in this paper sensorless advances are reviewed and recent developments in this area are introduced with their inherent advantages and drawbacks, including the analysis of practical implementation issues and applications.","The study includes a deep overview of state-of-the-art back-EMF sensing methods, which includes Terminal Voltage Sensing, Third Harmonic Voltage Integration, Terminal Current Sensing, Back-EMF Integration and PWM strategies.","Also, the most relevant techniques based on estimation and models are briefly analysed, such as Sliding-mode Observer, Extended Kalman Filter, Model Reference Adaptive System, Adaptive observers (Full-order and Pseudoreduced-order) and Artificial Neural Networks."],"url":"http://arxiv.org/abs/2402.05263v1","category":"eess.SY"}
{"created":"2024-02-07 20:42:01","title":"Biological computation through recurrence","abstract":"One of the defining features of living systems is their adaptability to changing environmental conditions. This requires organisms to extract temporal and spatial features of their environment, and use that information to compute the appropriate response. In the last two decades, a growing body or work, mainly coming from the machine learning and computational neuroscience fields, has shown that such complex information processing can be performed by recurrent networks. In those networks, temporal computations emerge from the interaction between incoming stimuli and the internal dynamic state of the network. In this article we review our current understanding of how recurrent networks can be used by biological systems, from cells to brains, for complex information processing. Rather than focusing on sophisticated, artificial recurrent architectures such as long short-term memory (LSTM) networks, here we concentrate on simpler network structures and learning algorithms that can be expected to have been found by evolution. We also review studies showing evidence of naturally occurring recurrent networks in living organisms. Lastly, we discuss some relevant evolutionary aspects concerning the emergence of this natural computation paradigm.","sentences":["One of the defining features of living systems is their adaptability to changing environmental conditions.","This requires organisms to extract temporal and spatial features of their environment, and use that information to compute the appropriate response.","In the last two decades, a growing body or work, mainly coming from the machine learning and computational neuroscience fields, has shown that such complex information processing can be performed by recurrent networks.","In those networks, temporal computations emerge from the interaction between incoming stimuli and the internal dynamic state of the network.","In this article we review our current understanding of how recurrent networks can be used by biological systems, from cells to brains, for complex information processing.","Rather than focusing on sophisticated, artificial recurrent architectures such as long short-term memory (LSTM) networks, here we concentrate on simpler network structures and learning algorithms that can be expected to have been found by evolution.","We also review studies showing evidence of naturally occurring recurrent networks in living organisms.","Lastly, we discuss some relevant evolutionary aspects concerning the emergence of this natural computation paradigm."],"url":"http://arxiv.org/abs/2402.05243v1","category":"q-bio.NC"}
{"created":"2024-02-07 19:01:06","title":"Meta-learning the mirror map in policy mirror descent","abstract":"Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms. These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees. Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method. It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy. In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments. By applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along the training trajectory. We analyze the characteristics of these learned mirror maps and reveal shared traits among certain settings. Our results suggest that mirror maps have the potential to be adaptable across various environments, raising questions about how to best match a mirror map to an environment's structure and characteristics.","sentences":["Policy Mirror Descent (PMD) is a popular framework in reinforcement learning, serving as a unifying perspective that encompasses numerous algorithms.","These algorithms are derived through the selection of a mirror map and enjoy finite-time convergence guarantees.","Despite its popularity, the exploration of PMD's full potential is limited, with the majority of research focusing on a particular mirror map -- namely, the negative entropy -- which gives rise to the renowned Natural Policy Gradient (NPG) method.","It remains uncertain from existing theoretical studies whether the choice of mirror map significantly influences PMD's efficacy.","In our work, we conduct empirical investigations to show that the conventional mirror map choice (NPG) often yields less-than-optimal outcomes across several standard benchmark environments.","By applying a meta-learning approach, we identify more efficient mirror maps that enhance performance, both on average and in terms of best performance achieved along the training trajectory.","We analyze the characteristics of these learned mirror maps and reveal shared traits among certain settings.","Our results suggest that mirror maps have the potential to be adaptable across various environments, raising questions about how to best match a mirror map to an environment's structure and characteristics."],"url":"http://arxiv.org/abs/2402.05187v1","category":"stat.ML"}
{"created":"2024-02-07 09:41:39","title":"Cost Optimized Scheduling in Modular Electrolysis Plants","abstract":"In response to the global shift towards renewable energy resources, the production of green hydrogen through electrolysis is emerging as a promising solution. Modular electrolysis plants, designed for flexibility and scalability, offer a dynamic response to the increasing demand for hydrogen while accommodating the fluctuations inherent in renewable energy sources. However, optimizing their operation is challenging, especially when a large number of electrolysis modules needs to be coordinated, each with potentially different characteristics.   To address these challenges, this paper presents a decentralized scheduling model to optimize the operation of modular electrolysis plants using the Alternating Direction Method of Multipliers. The model aims to balance hydrogen production with fluctuating demand, to minimize the marginal Levelized Cost of Hydrogen (mLCOH), and to ensure adaptability to operational disturbances. A case study validates the accuracy of the model in calculating mLCOH values under nominal load conditions and demonstrates its responsiveness to dynamic changes, such as electrolyzer module malfunctions and scale-up scenarios.","sentences":["In response to the global shift towards renewable energy resources, the production of green hydrogen through electrolysis is emerging as a promising solution.","Modular electrolysis plants, designed for flexibility and scalability, offer a dynamic response to the increasing demand for hydrogen while accommodating the fluctuations inherent in renewable energy sources.","However, optimizing their operation is challenging, especially when a large number of electrolysis modules needs to be coordinated, each with potentially different characteristics.   ","To address these challenges, this paper presents a decentralized scheduling model to optimize the operation of modular electrolysis plants using the Alternating Direction Method of Multipliers.","The model aims to balance hydrogen production with fluctuating demand, to minimize the marginal Levelized Cost of Hydrogen (mLCOH), and to ensure adaptability to operational disturbances.","A case study validates the accuracy of the model in calculating mLCOH values under nominal load conditions and demonstrates its responsiveness to dynamic changes, such as electrolyzer module malfunctions and scale-up scenarios."],"url":"http://arxiv.org/abs/2402.05148v1","category":"cs.SY"}
{"created":"2024-02-07 08:15:30","title":"Online Learning Approach for Survival Analysis","abstract":"We introduce an online mathematical framework for survival analysis, allowing real time adaptation to dynamic environments and censored data. This framework enables the estimation of event time distributions through an optimal second order online convex optimization algorithm-Online Newton Step (ONS). This approach, previously unexplored, presents substantial advantages, including explicit algorithms with non-asymptotic convergence guarantees. Moreover, we analyze the selection of ONS hyperparameters, which depends on the exp-concavity property and has a significant influence on the regret bound. We propose a stochastic approach that guarantees logarithmic stochastic regret for ONS. Additionally, we introduce an adaptive aggregation method that ensures robustness in hyperparameter selection while maintaining fast regret bounds. The findings of this paper can extend beyond the survival analysis field, and are relevant for any case characterized by poor exp-concavity and unstable ONS. Finally, these assertions are illustrated by simulation experiments.","sentences":["We introduce an online mathematical framework for survival analysis, allowing real time adaptation to dynamic environments and censored data.","This framework enables the estimation of event time distributions through an optimal second order online convex optimization algorithm-Online Newton Step (ONS).","This approach, previously unexplored, presents substantial advantages, including explicit algorithms with non-asymptotic convergence guarantees.","Moreover, we analyze the selection of ONS hyperparameters, which depends on the exp-concavity property and has a significant influence on the regret bound.","We propose a stochastic approach that guarantees logarithmic stochastic regret for ONS.","Additionally, we introduce an adaptive aggregation method that ensures robustness in hyperparameter selection while maintaining fast regret bounds.","The findings of this paper can extend beyond the survival analysis field, and are relevant for any case characterized by poor exp-concavity and unstable ONS.","Finally, these assertions are illustrated by simulation experiments."],"url":"http://arxiv.org/abs/2402.05145v1","category":"cs.LG"}
{"created":"2024-02-07 01:45:14","title":"The Foundations of Computational Management: A Systematic Approach to Task Automation for the Integration of Artificial Intelligence into Existing Workflows","abstract":"Driven by the rapid ascent of artificial intelligence (AI), organizations are at the epicenter of a seismic shift, facing a crucial question: How can AI be successfully integrated into existing operations? To help answer it, manage expectations and mitigate frustration, this article introduces Computational Management, a systematic approach to task automation for enhancing the ability of organizations to harness AI's potential within existing workflows. Computational Management acts as a bridge between the strategic insights of management science with the analytical rigor of computational thinking. The article offers three easy step-by-step procedures to begin the process of implementing AI within a workflow. Such procedures focus on task (re)formulation, on the assessment of the automation potential of tasks, on the completion of task specification templates for AI selection and adaptation. Included in the article there are manual and automated methods, with prompt suggestions for publicly available LLMs, to complete these three procedures. The first procedure, task (re)formulation, focuses on breaking down work activities into basic units, so they can be completed by one agent, involve a single well-defined action, and produce a distinct outcome. The second, allows the assessment of the granular task and its suitability for automation, using the Task Automation Index to rank tasks based on whether they have standardized input, well-defined rules, repetitiveness, data dependency, and objective outputs. The third, focuses on a task specification template which details information on 16 critical components of tasks, and can be used as a checklist to select or adapt the most suitable AI solution for integration into existing workflows. Computational Management provides a roadmap and a toolkit for humans and AI to thrive together, while enhancing organizational efficiency and innovation.","sentences":["Driven by the rapid ascent of artificial intelligence (AI), organizations are at the epicenter of a seismic shift, facing a crucial question: How can AI be successfully integrated into existing operations?","To help answer it, manage expectations and mitigate frustration, this article introduces Computational Management, a systematic approach to task automation for enhancing the ability of organizations to harness AI's potential within existing workflows.","Computational Management acts as a bridge between the strategic insights of management science with the analytical rigor of computational thinking.","The article offers three easy step-by-step procedures to begin the process of implementing AI within a workflow.","Such procedures focus on task (re)formulation, on the assessment of the automation potential of tasks, on the completion of task specification templates for AI selection and adaptation.","Included in the article there are manual and automated methods, with prompt suggestions for publicly available LLMs, to complete these three procedures.","The first procedure, task (re)formulation, focuses on breaking down work activities into basic units, so they can be completed by one agent, involve a single well-defined action, and produce a distinct outcome.","The second, allows the assessment of the granular task and its suitability for automation, using the Task Automation Index to rank tasks based on whether they have standardized input, well-defined rules, repetitiveness, data dependency, and objective outputs.","The third, focuses on a task specification template which details information on 16 critical components of tasks, and can be used as a checklist to select or adapt the most suitable AI solution for integration into existing workflows.","Computational Management provides a roadmap and a toolkit for humans and AI to thrive together, while enhancing organizational efficiency and innovation."],"url":"http://arxiv.org/abs/2402.05142v1","category":"cs.SE"}
{"created":"2024-02-08 18:59:30","title":"Classifying Nodes in Graphs without GNNs","abstract":"Graph neural networks (GNNs) are the dominant paradigm for classifying nodes in a graph, but they have several undesirable attributes stemming from their message passing architecture. Recently, distillation methods succeeded in eliminating the use of GNNs at test time but they still require them during training. We perform a careful analysis of the role that GNNs play in distillation methods. This analysis leads us to propose a fully GNN-free approach for node classification, not requiring them at train or test time. Our method consists of three key components: smoothness constraints, pseudo-labeling iterations and neighborhood-label histograms. Our final approach can match the state-of-the-art accuracy on standard popular benchmarks such as citation and co-purchase networks, without training a GNN.","sentences":["Graph neural networks (GNNs) are the dominant paradigm for classifying nodes in a graph, but they have several undesirable attributes stemming from their message passing architecture.","Recently, distillation methods succeeded in eliminating the use of GNNs at test time but they still require them during training.","We perform a careful analysis of the role that GNNs play in distillation methods.","This analysis leads us to propose a fully GNN-free approach for node classification, not requiring them at train or test time.","Our method consists of three key components: smoothness constraints, pseudo-labeling iterations and neighborhood-label histograms.","Our final approach can match the state-of-the-art accuracy on standard popular benchmarks such as citation and co-purchase networks, without training a GNN."],"url":"http://arxiv.org/abs/2402.05934v1","category":"cs.LG"}
{"created":"2024-02-08 17:24:30","title":"The CATT SATT on the MATT: semiparametric inference for sample treatment effects on the treated","abstract":"We study variants of the average treatment effect on the treated with population parameters replaced by their sample counterparts. For each estimand, we derive the limiting distribution with respect to a semiparametric efficient estimator of the population effect and provide guidance on variance estimation. Included in our analysis is the well-known sample average treatment effect on the treated, for which we obtain some unexpected results. Unlike for the ordinary sample average treatment effect, we find that the asymptotic variance for the sample average treatment effect on the treated is point-identified and consistently estimable, but it potentially exceeds that of the population estimand. To address this shortcoming, we propose a modification that yields a new estimand, the mixed average treatment effect on the treated, which is always estimated more precisely than both the population and sample effects. We also introduce a second new estimand that arises from an alternative interpretation of the treatment effect on the treated with which all individuals are weighted by the propensity score.","sentences":["We study variants of the average treatment effect on the treated with population parameters replaced by their sample counterparts.","For each estimand, we derive the limiting distribution with respect to a semiparametric efficient estimator of the population effect and provide guidance on variance estimation.","Included in our analysis is the well-known sample average treatment effect on the treated, for which we obtain some unexpected results.","Unlike for the ordinary sample average treatment effect, we find that the asymptotic variance for the sample average treatment effect on the treated is point-identified and consistently estimable, but it potentially exceeds that of the population estimand.","To address this shortcoming, we propose a modification that yields a new estimand, the mixed average treatment effect on the treated, which is always estimated more precisely than both the population and sample effects.","We also introduce a second new estimand that arises from an alternative interpretation of the treatment effect on the treated with which all individuals are weighted by the propensity score."],"url":"http://arxiv.org/abs/2402.05844v1","category":"stat.ME"}
{"created":"2024-02-08 17:12:49","title":"How Much is Unseen Depends Chiefly on Information About the Seen","abstract":"It might seem counter-intuitive at first: We find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number $f_k$ of classes that do appear in the training data the same number of times. While in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage. However, our precise characterization of the dependency between $f_k$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators. Hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (MSE). In our experiments, our genetic algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator. This holds for over 96% of runs when there are at least as many samples as classes. Our estimators' MSE is roughly 80% of the Good-Turing estimator's.","sentences":["It might seem counter-intuitive at first: We find that, in expectation, the proportion of data points in an unknown population-that belong to classes that do not appear in the training data-is almost entirely determined by the number $f_k$ of classes that do appear in the training data the same number of times.","While in theory we show that the difference of the induced estimator decays exponentially in the size of the sample, in practice the high variance prevents us from using it directly for an estimator of the sample coverage.","However, our precise characterization of the dependency between $f_k$'s induces a large search space of different representations of the expected value, which can be deterministically instantiated as estimators.","Hence, we turn to optimization and develop a genetic algorithm that, given only the sample, searches for an estimator with minimal mean-squared error (MSE).","In our experiments, our genetic algorithm discovers estimators that have a substantially smaller MSE than the state-of-the-art Good-Turing estimator.","This holds for over 96% of runs when there are at least as many samples as classes.","Our estimators' MSE is roughly 80% of the Good-Turing estimator's."],"url":"http://arxiv.org/abs/2402.05835v1","category":"cs.LG"}
{"created":"2024-02-08 16:52:46","title":"Dynamical large deviations for boundary driven gradient symmetric exclusion processes in mild contact with reservoirs","abstract":"We consider a one-dimensional gradient symmetric exclusion process in mild contact with boundary reservoirs. The hydrodynamic limit of the empirical measure is given by a non-linear second-order parabolic equation with non-linear Robin boundary conditions. We prove the dynamical large deviations principle.","sentences":["We consider a one-dimensional gradient symmetric exclusion process in mild contact with boundary reservoirs.","The hydrodynamic limit of the empirical measure is given by a non-linear second-order parabolic equation with non-linear Robin boundary conditions.","We prove the dynamical large deviations principle."],"url":"http://arxiv.org/abs/2402.05816v1","category":"math.PR"}
{"created":"2024-02-08 16:45:05","title":"Revealing fingerprints of valence excitons in x-ray absorption spectra with the Bethe-Salpeter equation","abstract":"The Bethe-Salpeter equation (BSE) is a powerful theoretical approach that is capable to accurately treat electron-hole interactions in materials in an excited state. We developed an ab initio framework based on the BSE to describe a pump-probe experiment, in which an x-ray pulse probes solid-state valence excitons by means of x-ray absorption spectroscopy. Our theoretical framework is of relevance for an accurate modeling of pump-probe experiments of photo-excited materials that utilize novel capabilities offered by x-ray science.","sentences":["The Bethe-Salpeter equation (BSE) is a powerful theoretical approach that is capable to accurately treat electron-hole interactions in materials in an excited state.","We developed an ab initio framework based on the BSE to describe a pump-probe experiment, in which an x-ray pulse probes solid-state valence excitons by means of x-ray absorption spectroscopy.","Our theoretical framework is of relevance for an accurate modeling of pump-probe experiments of photo-excited materials that utilize novel capabilities offered by x-ray science."],"url":"http://arxiv.org/abs/2402.05805v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-08 16:34:13","title":"Spatially-Periodic Solutions for Evolution Anisotropic Variable-Coefficient Navier-Stokes Equations: I. Existence","abstract":"We consider evolution (non-stationary) space-periodic solutions to the $n$-dimensional non-linear Navier-Stokes equations of anisotropic fluids with the viscosity coefficient tensor variable in space and time and satisfying the relaxed ellipticity condition. Employing the Galerkin algorithm with the basis constituted by the eigenfunctions of the periodic Bessel-potential operator, we prove the existence of a global weak solution.","sentences":["We consider evolution (non-stationary) space-periodic solutions to the $n$-dimensional non-linear Navier-Stokes equations of anisotropic fluids with the viscosity coefficient tensor variable in space and time and satisfying the relaxed ellipticity condition.","Employing the Galerkin algorithm with the basis constituted by the eigenfunctions of the periodic Bessel-potential operator, we prove the existence of a global weak solution."],"url":"http://arxiv.org/abs/2402.05792v1","category":"math.AP"}
{"created":"2024-02-08 16:34:00","title":"Determining the significance and relative importance of parameters of a simulated quenching algorithm using statistical tools","abstract":"When search methods are being designed it is very important to know which parameters have the greatest influence on the behaviour and performance of the algorithm. To this end, algorithm parameters are commonly calibrated by means of either theoretic analysis or intensive experimentation. When undertaking a detailed statistical analysis of the influence of each parameter, the designer should pay attention mostly to the parameters that are statistically significant. In this paper the ANOVA (ANalysis Of the VAriance) method is used to carry out an exhaustive analysis of a simulated annealing based method and the different parameters it requires. Following this idea, the significance and relative importance of the parameters regarding the obtained results, as well as suitable values for each of these, were obtained using ANOVA and post-hoc Tukey HSD test, on four well known function optimization problems and the likelihood function that is used to estimate the parameters involved in the lognormal diffusion process. Through this statistical study we have verified the adequacy of parameter values available in the bibliography using parametric hypothesis tests.","sentences":["When search methods are being designed it is very important to know which parameters have the greatest influence on the behaviour and performance of the algorithm.","To this end, algorithm parameters are commonly calibrated by means of either theoretic analysis or intensive experimentation.","When undertaking a detailed statistical analysis of the influence of each parameter, the designer should pay attention mostly to the parameters that are statistically significant.","In this paper the ANOVA (ANalysis Of the VAriance) method is used to carry out an exhaustive analysis of a simulated annealing based method and the different parameters it requires.","Following this idea, the significance and relative importance of the parameters regarding the obtained results, as well as suitable values for each of these, were obtained using ANOVA and post-hoc Tukey HSD test, on four well known function optimization problems and the likelihood function that is used to estimate the parameters involved in the lognormal diffusion process.","Through this statistical study we have verified the adequacy of parameter values available in the bibliography using parametric hypothesis tests."],"url":"http://arxiv.org/abs/2402.05791v1","category":"cs.NE"}
{"created":"2024-02-08 15:57:10","title":"A non-damped stabilization algorithm for multibody dynamics","abstract":"The stability of integrators dealing with high order Differential Algebraic Equations (DAEs) is a major issue. The usual procedures give rise to instabilities that are not predicted by the usual linear analysis, rendering the common checks (developed for ODEs) unusable. The appearance of these difficult-toexplain and unexpected problems leads to methods that arise heavy numerical damping for avoiding them. This has the undesired consequences of lack of convergence of the methods, along with a need of smaller stepsizes. In this paper a new approach is presented. The algorithm presented here allows us to avoid the interference of the constraints in the integration, thus allowing the linear criteria to be applied. In order to do so, the integrator is applied to a set of instantaneous minimal coordinates that are obtained through the application of the null space. The new approach can be utilized along with any integration method. Some experiments using the Newmark method have been carried out, which validate the methodology and also show that the method behaves in a predictable way if one considers linear stability criteria.","sentences":["The stability of integrators dealing with high order Differential Algebraic Equations (DAEs) is a major issue.","The usual procedures give rise to instabilities that are not predicted by the usual linear analysis, rendering the common checks (developed for ODEs) unusable.","The appearance of these difficult-toexplain and unexpected problems leads to methods that arise heavy numerical damping for avoiding them.","This has the undesired consequences of lack of convergence of the methods, along with a need of smaller stepsizes.","In this paper a new approach is presented.","The algorithm presented here allows us to avoid the interference of the constraints in the integration, thus allowing the linear criteria to be applied.","In order to do so, the integrator is applied to a set of instantaneous minimal coordinates that are obtained through the application of the null space.","The new approach can be utilized along with any integration method.","Some experiments using the Newmark method have been carried out, which validate the methodology and also show that the method behaves in a predictable way if one considers linear stability criteria."],"url":"http://arxiv.org/abs/2402.05768v1","category":"math.NA"}
{"created":"2024-02-08 15:41:48","title":"Latent variable model for high-dimensional point process with structured missingness","abstract":"Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process. While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges. In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations. Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process. We construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training. We demonstrate competitive performance using both simulated and real datasets.","sentences":["Longitudinal data are important in numerous fields, such as healthcare, sociology and seismology, but real-world datasets present notable challenges for practitioners because they can be high-dimensional, contain structured missingness patterns, and measurement time points can be governed by an unknown stochastic process.","While various solutions have been suggested, the majority of them have been designed to account for only one of these challenges.","In this work, we propose a flexible and efficient latent-variable model that is capable of addressing all these limitations.","Our approach utilizes Gaussian processes to capture temporal correlations between samples and their associated missingness masks as well as to model the underlying point process.","We construct our model as a variational autoencoder together with deep neural network parameterised encoder and decoder models, and develop a scalable amortised variational inference approach for efficient model training.","We demonstrate competitive performance using both simulated and real datasets."],"url":"http://arxiv.org/abs/2402.05758v1","category":"cs.LG"}
{"created":"2024-02-08 15:20:07","title":"Interpretations of the ATLAS measurements of Higgs boson production and decay rates and differential cross-sections in $pp$ collisions at $\\sqrt{s}=13$ TeV","abstract":"Measurements of the Higgs boson production times decay rates and differential cross-sections have recently been performed by the ATLAS experiment in several decay channels using up to 139 fb$^{-1}$ of proton-proton collision data at $\\sqrt{s}=13$ TeV recorded at the Large Hadron Collider. This paper presents multiple interpretations of these Higgs boson measurements. Measurements of production-mode cross-sections, simplified template cross-sections and fiducial differential cross-sections in different decay channels are reparameterised in terms of the impact of Standard Model effective field theory operators, and constraints are reported on the corresponding Wilson coefficients. Production and decay rate measurements are interpreted in UV-complete extensions of the Standard Model, namely the two-Higgs-doublet model (2HDM) near thealignment limit and the Minimal Supersymmetric Standard Model (MSSM) for various MSSM benchmark scenarios. The constraints on the 2HDM parameters $(\\cos(\\beta-\\alpha), \\tan\\beta)$ and the MSSM parameters $(m_A, \\tan\\beta)$ are complementary to those obtained from direct searches for additional Higgs bosons.","sentences":["Measurements of the Higgs boson production times decay rates and differential cross-sections have recently been performed by the ATLAS experiment in several decay channels using up to 139 fb$^{-1}$ of proton-proton collision data at $\\sqrt{s}=13$ TeV recorded at the Large Hadron Collider.","This paper presents multiple interpretations of these Higgs boson measurements.","Measurements of production-mode cross-sections, simplified template cross-sections and fiducial differential cross-sections in different decay channels are reparameterised in terms of the impact of Standard Model effective field theory operators, and constraints are reported on the corresponding Wilson coefficients.","Production and decay rate measurements are interpreted in UV-complete extensions of the Standard Model, namely the two-Higgs-doublet model (2HDM) near thealignment limit and the Minimal Supersymmetric Standard Model (MSSM) for various MSSM benchmark scenarios.","The constraints on the 2HDM parameters $(\\cos(\\beta-\\alpha), \\tan\\beta)$ and the MSSM parameters $(m_A, \\tan\\beta)$ are complementary to those obtained from direct searches for additional Higgs bosons."],"url":"http://arxiv.org/abs/2402.05742v1","category":"hep-ex"}
{"created":"2024-02-08 14:02:07","title":"Bridging the gap between luminous red novae and common envelope evolution: the role of recombination energy and radiation force","abstract":"Luminous red novae (LRNe) and their connection to common envelope events (CEE) remain elusive in astrophysics. Here, we present a radiation hydrodynamic model capable of simulating the light curves of material ejected during a CEE. For the first time, the radiation hydrodynamic model incorporates complete recombination physics for hydrogen and helium. The radiation hydrodynamic equations are solved with Guangqi. With time-independent ejecta simulations, we show that the peaks in the light curves are attributed to radiation-dominated ejecta, while the extended plateaus are produced by matter-dominated ejecta. To showcase our model's capability, we fit the light curve of AT2019zhd. The central mass object of $6M_{\\odot}$ is assumed based on observations and scaling relations. Our model demonstrates that the ejecta mass of AT2019zhd falls within the range of $0.04M_{\\odot}$ to $0.1M_{\\odot}$. Additionally, we demonstrate that recombination energy and radiation force acceleration significantly impact the light curves, whereas dust formation has a limited effect during the peak and plateau phases.","sentences":["Luminous red novae (LRNe) and their connection to common envelope events (CEE) remain elusive in astrophysics.","Here, we present a radiation hydrodynamic model capable of simulating the light curves of material ejected during a CEE.","For the first time, the radiation hydrodynamic model incorporates complete recombination physics for hydrogen and helium.","The radiation hydrodynamic equations are solved with Guangqi.","With time-independent ejecta simulations, we show that the peaks in the light curves are attributed to radiation-dominated ejecta, while the extended plateaus are produced by matter-dominated ejecta.","To showcase our model's capability, we fit the light curve of AT2019zhd.","The central mass object of $6M_{\\odot}$ is assumed based on observations and scaling relations.","Our model demonstrates that the ejecta mass of AT2019zhd falls within the range of $0.04M_{\\odot}$ to $0.1M_{\\odot}$. Additionally, we demonstrate that recombination energy and radiation force acceleration significantly impact the light curves, whereas dust formation has a limited effect during the peak and plateau phases."],"url":"http://arxiv.org/abs/2402.05686v1","category":"astro-ph.SR"}
{"created":"2024-02-08 13:12:50","title":"Real-time Holistic Robot Pose Estimation with Unknown States","abstract":"Estimating robot pose from RGB images is a crucial problem in computer vision and robotics. While previous methods have achieved promising performance, most of them presume full knowledge of robot internal states, e.g. ground-truth robot joint angles, which are not always available in real-world scenarios. On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens and thus cannot support real-time applications. This work addresses the urgent need for efficient robot pose estimation with unknown states. We propose an end-to-end pipeline for real-time, holistic robot pose estimation from a single RGB image, even in the absence of known robot states. Our method decomposes the problem into estimating camera-to-robot rotation, robot state parameters, keypoint locations, and root depth. We further design a corresponding neural network module for each task. This approach allows for learning multi-facet representations and facilitates sim-to-real transfer through self-supervised learning. Notably, our method achieves inference with a single feedforward, eliminating the need for costly test-time iterative optimization. As a result, it delivers a 12-time speed boost with state-of-the-art accuracy, enabling real-time holistic robot pose estimation for the first time. Code is available at https://oliverbansk.github.io/Holistic-Robot-Pose/.","sentences":["Estimating robot pose from RGB images is a crucial problem in computer vision and robotics.","While previous methods have achieved promising performance, most of them presume full knowledge of robot internal states, e.g. ground-truth robot joint angles, which are not always available in real-world scenarios.","On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens and thus cannot support real-time applications.","This work addresses the urgent need for efficient robot pose estimation with unknown states.","We propose an end-to-end pipeline for real-time, holistic robot pose estimation from a single RGB image, even in the absence of known robot states.","Our method decomposes the problem into estimating camera-to-robot rotation, robot state parameters, keypoint locations, and root depth.","We further design a corresponding neural network module for each task.","This approach allows for learning multi-facet representations and facilitates sim-to-real transfer through self-supervised learning.","Notably, our method achieves inference with a single feedforward, eliminating the need for costly test-time iterative optimization.","As a result, it delivers a 12-time speed boost with state-of-the-art accuracy, enabling real-time holistic robot pose estimation for the first time.","Code is available at https://oliverbansk.github.io/Holistic-Robot-Pose/."],"url":"http://arxiv.org/abs/2402.05655v1","category":"cs.CV"}
{"created":"2024-02-08 12:51:56","title":"On the behaviour of harmonic functions on Riemannian cones","abstract":"We discuss the behavior of harmonic functions on Riemannian cones as defined below and Lioville's theorem.","sentences":["We discuss the behavior of harmonic functions on Riemannian cones as defined below and Lioville's theorem."],"url":"http://arxiv.org/abs/2402.05640v1","category":"math.DG"}
{"created":"2024-02-08 12:45:50","title":"LCAONet: Message-passing with physically optimized atomic basis functions","abstract":"A Model capable of handling various elemental species and substances is essential for discovering new materials in the vast phase and compound space. Message-passing neural networks (MPNNs) are promising as such models, in which various vector operations model the atomic interaction with its neighbors. However, conventional MPNNs tend to overlook the importance of physicochemical information for each node atom, relying solely on the geometric features of the material graph. We propose the new three-body MPNN architecture with a message-passing layer that utilizes optimized basis functions based on the electronic structure of the node elemental species. This enables conveying the message that includes physical information and better represents the interaction for each elemental species. Inspired by the LCAO (linear combination of atomic orbitals) method, a classical method for calculating the orbital interactions, the linear combination of atomic basis constructed based on the wave function of hydrogen-like atoms is used for the present message-passing. Our model achieved higher prediction accuracy with smaller parameters than the state-of-the-art models on the challenging crystalline dataset containing many elemental species, including heavy elements. Ablation studies have also shown that the proposed method is effective in improving accuracy. Our implementation is available online.","sentences":["A Model capable of handling various elemental species and substances is essential for discovering new materials in the vast phase and compound space.","Message-passing neural networks (MPNNs) are promising as such models, in which various vector operations model the atomic interaction with its neighbors.","However, conventional MPNNs tend to overlook the importance of physicochemical information for each node atom, relying solely on the geometric features of the material graph.","We propose the new three-body MPNN architecture with a message-passing layer that utilizes optimized basis functions based on the electronic structure of the node elemental species.","This enables conveying the message that includes physical information and better represents the interaction for each elemental species.","Inspired by the LCAO (linear combination of atomic orbitals) method, a classical method for calculating the orbital interactions, the linear combination of atomic basis constructed based on the wave function of hydrogen-like atoms is used for the present message-passing.","Our model achieved higher prediction accuracy with smaller parameters than the state-of-the-art models on the challenging crystalline dataset containing many elemental species, including heavy elements.","Ablation studies have also shown that the proposed method is effective in improving accuracy.","Our implementation is available online."],"url":"http://arxiv.org/abs/2402.05634v1","category":"physics.comp-ph"}
{"created":"2024-02-08 12:30:29","title":"The Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escaping, and Network Embedding","abstract":"In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss. As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points. We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases. Additionally, we show that, if a stationary point does not contain \"escape neurons\", which are defined with first-order conditions, then it must be a local minimum. Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum. Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons. Moreover, we are also able to fully discuss how network embedding, which is to instantiate a narrower network within a wider network, reshapes the stationary points.","sentences":["In this paper, we investigate the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions trained with the empirical squared loss.","As the activation function is non-differentiable, it is so far unclear how to completely characterize the stationary points.","We propose the conditions for stationarity that apply to both non-differentiable and differentiable cases.","Additionally, we show that, if a stationary point does not contain \"escape neurons\", which are defined with first-order conditions, then it must be a local minimum.","Moreover, for the scalar-output case, the presence of an escape neuron guarantees that the stationary point is not a local minimum.","Our results refine the description of the saddle-to-saddle training process starting from infinitesimally small (vanishing) initialization for shallow ReLU-like networks, linking saddle escaping directly with the parameter changes of escape neurons.","Moreover, we are also able to fully discuss how network embedding, which is to instantiate a narrower network within a wider network, reshapes the stationary points."],"url":"http://arxiv.org/abs/2402.05626v1","category":"cs.LG"}
{"created":"2024-02-08 10:26:30","title":"A priori bounds for 2-d generalised Parabolic Anderson Model","abstract":"We show a priori bounds for solutions to $(\\partial_t - \\Delta) u = \\sigma (u) \\xi$ in finite volume in the framework of Hairer's Regularity Structures [Invent Math 198:269--504, 2014]. We assume $\\sigma \\in C_b^2 (\\mathbb{R})$ and that $\\xi$ is of negative H\\\"older regularity of order $- 1 - \\kappa$ where $\\kappa < \\bar{\\kappa}$ for an explicit $\\bar{\\kappa}< 1/3$, and that it can be lifted to a model in the sense of Regularity Structures. Our main results guarantee non-explosion of the solution in finite time and a growth which is at most polynomial in $t > 0$. Our estimates imply global well-posedness for the 2-d generalised parabolic Anderson model on the torus, as well as for the parabolic quantisation of the Sine-Gordon Euclidean Quantum Field Theory (EQFT) on the torus in the regime $\\beta^2 \\in (4 \\pi, (1 + \\bar{\\kappa}) 4 \\pi)$. We also consider the parabolic quantisation of a massive Sine-Gordon EQFT and derive estimates that imply the existence of the measure for the same range of $\\beta$. Finally, our estimates apply to It\\^o SPDEs in the sense of Da Prato-Zabczyk [Stochastic Equations in Infinite Dimensions, Enc. Math. App., Cambridge Univ. Press, 1992] and imply existence of a stochastic flow beyond the trace-class regime.","sentences":["We show a priori bounds for solutions to $(\\partial_t - \\Delta)","u = \\sigma (u) \\xi$ in finite volume in the framework of Hairer's Regularity Structures","[Invent Math 198:269--504, 2014].","We assume $\\sigma \\in C_b^2 (\\mathbb{R})$ and that $\\xi$ is of negative H\\\"older regularity of order $- 1 - \\kappa$ where $\\kappa < \\bar{\\kappa}$ for an explicit $\\bar{\\kappa}< 1/3$, and that it can be lifted to a model in the sense of Regularity Structures.","Our main results guarantee non-explosion of the solution in finite time and a growth which is at most polynomial in $t > 0$.","Our estimates imply global well-posedness for the 2-d generalised parabolic Anderson model on the torus, as well as for the parabolic quantisation of the Sine-Gordon Euclidean Quantum Field Theory (EQFT) on the torus in the regime $\\beta^2 \\in (4 \\pi, (1 + \\bar{\\kappa})","4 \\pi)$. We also consider the parabolic quantisation of a massive Sine-Gordon EQFT and derive estimates that imply the existence of the measure for the same range of $\\beta$. Finally, our estimates apply to It\\^o SPDEs in the sense of Da Prato-Zabczyk [Stochastic Equations in Infinite Dimensions, Enc.","Math.","App., Cambridge Univ.","Press, 1992] and imply existence of a stochastic flow beyond the trace-class regime."],"url":"http://arxiv.org/abs/2402.05544v1","category":"math.AP"}
{"created":"2024-02-08 10:16:15","title":"Anisotropic surface polaritons at isotropic--uniaxial interface: an algebraic solution","abstract":"Surface polaritons in an anisotropic media posses a strong dependence of the wavevector on the propagation direction. This can lead to the fact that polariton propagation will be possible in a limited range of angles in the boundary plane. Notable examples are Dyakonov surface waves and hyperbolic plasmons in a hyperbolic metamaterial. Exact solutions of the dispersion equation in closed form are known only in the case of weakly anisotropic medium and for highly symmetric directions. This work provides an exact algebraic solution for surface polariton at the boundary of an isotropic and anisotropic medium when the optic axis is parallel to the boundary. It was used to analyze the shapes of isofrequency contours and classify them. The aim of the work is to summarize the known results of theoretical researches on surface polaritons in this case.","sentences":["Surface polaritons in an anisotropic media posses a strong dependence of the wavevector on the propagation direction.","This can lead to the fact that polariton propagation will be possible in a limited range of angles in the boundary plane.","Notable examples are Dyakonov surface waves and hyperbolic plasmons in a hyperbolic metamaterial.","Exact solutions of the dispersion equation in closed form are known only in the case of weakly anisotropic medium and for highly symmetric directions.","This work provides an exact algebraic solution for surface polariton at the boundary of an isotropic and anisotropic medium when the optic axis is parallel to the boundary.","It was used to analyze the shapes of isofrequency contours and classify them.","The aim of the work is to summarize the known results of theoretical researches on surface polaritons in this case."],"url":"http://arxiv.org/abs/2402.05537v1","category":"physics.optics"}
{"created":"2024-02-08 10:10:44","title":"A new family of translating solitons in hyperbolic space","abstract":"If $\\xi$ is a Killing vector field of the hyperbolic space $\\h^3$ whose flow are parabolic isometries, a surface $\\Sigma\\subset\\h^3$ is a $\\xi$-translator if its mean curvature $H$ satisfies $H=\\langle N,\\xi\\rangle$, where $N$ is the unit normal of $\\Sigma$. We classify all $\\xi$-translators invariant by a one-parameter group of rotations of $\\h^3$, exhibiting the existence of a new family of grim reapers. We use these grim reapers to prove the non-existence of closed $\\xi$-translators.","sentences":["If $\\xi$ is a Killing vector field of the hyperbolic space $\\h^3$ whose flow are parabolic isometries, a surface $\\Sigma\\subset\\h^3$ is a $\\xi$-translator if its mean curvature $H$ satisfies $H=\\langle N,\\xi\\rangle$, where $N$ is the unit normal of $\\Sigma$. We classify all $\\xi$-translators invariant by a one-parameter group of rotations of $\\h^3$, exhibiting the existence of a new family of grim reapers.","We use these grim reapers to prove the non-existence of closed $\\xi$-translators."],"url":"http://arxiv.org/abs/2402.05533v1","category":"math.DG"}
{"created":"2024-02-08 10:09:12","title":"NCRF: Neural Contact Radiance Fields for Free-Viewpoint Rendering of Hand-Object Interaction","abstract":"Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision. Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation. To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos. In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object. (b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences. We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis. Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy.","sentences":["Modeling hand-object interactions is a fundamentally challenging task in 3D computer vision.","Despite remarkable progress that has been achieved in this field, existing methods still fail to synthesize the hand-object interaction photo-realistically, suffering from degraded rendering quality caused by the heavy mutual occlusions between the hand and the object, and inaccurate hand-object pose estimation.","To tackle these challenges, we present a novel free-viewpoint rendering framework, Neural Contact Radiance Field (NCRF), to reconstruct hand-object interactions from a sparse set of videos.","In particular, the proposed NCRF framework consists of two key components: (a) A contact optimization field that predicts an accurate contact field from 3D query points for achieving desirable contact between the hand and the object.","(b) A hand-object neural radiance field to learn an implicit hand-object representation in a static canonical space, in concert with the specifically designed hand-object motion field to produce observation-to-canonical correspondences.","We jointly learn these key components where they mutually help and regularize each other with visual and geometric constraints, producing a high-quality hand-object reconstruction that achieves photo-realistic novel view synthesis.","Extensive experiments on HO3D and DexYCB datasets show that our approach outperforms the current state-of-the-art in terms of both rendering quality and pose estimation accuracy."],"url":"http://arxiv.org/abs/2402.05532v1","category":"cs.CV"}
{"created":"2024-02-08 10:05:35","title":"Horo-shrinkers in the hyperbolic space","abstract":"A surface $\\Sigma$ in the hyperbolic space $\\h^3$ is called a horo-shrinker if its mean curvature $H$ satisfies $H=\\langle N,\\partial_z\\rangle$, where $(x,y,z)$ are the coordinates of $\\h^3$ in the upper half-space model and $N$ is the unit normal of $\\Sigma$. In this paper we study horo-shrinkers invariant by one-parameter groups of isometries of $\\h^3$ depending if these isometries are hyperbolic, parabolic or spherical. We characterize totally geodesic planes as the only horo-shrinkers invariant by a one-parameter group of hyperbolic translations. The grim reapers are defined as the horo-shrinkers invariant by a one-parameter group of parabolic translations. We describe the geometry of the grim reapers proving that they are periodic surfaces. In the last part of the paper, we give a complete classification of horo-shrinkers invariant by spherical rotations, distinguishing if the surfaces intersect or not the rotation axis.","sentences":["A surface $\\Sigma$ in the hyperbolic space $\\h^3$ is called a horo-shrinker if its mean curvature $H$ satisfies $H=\\langle N,\\partial_z\\rangle$, where $(x,y,z)$ are the coordinates of $\\h^3$ in the upper half-space model and $N$ is the unit normal of $\\Sigma$. In this paper we study horo-shrinkers invariant by one-parameter groups of isometries of $\\h^3$ depending if these isometries are hyperbolic, parabolic or spherical.","We characterize totally geodesic planes as the only horo-shrinkers invariant by a one-parameter group of hyperbolic translations.","The grim reapers are defined as the horo-shrinkers invariant by a one-parameter group of parabolic translations.","We describe the geometry of the grim reapers proving that they are periodic surfaces.","In the last part of the paper, we give a complete classification of horo-shrinkers invariant by spherical rotations, distinguishing if the surfaces intersect or not the rotation axis."],"url":"http://arxiv.org/abs/2402.05527v1","category":"math.DG"}
{"created":"2024-02-08 09:45:33","title":"The Neumann condition for the superposition of fractional Laplacians","abstract":"We present a new functional setting for Neumann conditions related to the superposition of (possibly infinitely many) fractional Laplace operators. We will introduce some bespoke functional framework and present minimization properties, existence and uniqueness results, asymptotic formulas, spectral analyses, rigidity results, integration by parts formulas, superpositions of fractional perimeters, as well as a study of the associated heat equation.","sentences":["We present a new functional setting for Neumann conditions related to the superposition of (possibly infinitely many) fractional Laplace operators.","We will introduce some bespoke functional framework and present minimization properties, existence and uniqueness results, asymptotic formulas, spectral analyses, rigidity results, integration by parts formulas, superpositions of fractional perimeters, as well as a study of the associated heat equation."],"url":"http://arxiv.org/abs/2402.05514v1","category":"math.AP"}
{"created":"2024-02-08 09:37:12","title":"Performance Evaluation of Associative Watermarking Using Statistical Neurodynamics","abstract":"We theoretically evaluated the performance of our proposed associative watermarking method in which the watermark is not embedded directly into the image. We previously proposed a watermarking method that extends the zero-watermarking model by applying associative memory models. In this model, the hetero-associative memory model is introduced to the mapping process between image features and watermarks, and the auto-associative memory model is applied to correct watermark errors. We herein show that the associative watermarking model outperforms the zero-watermarking model through computer simulations using actual images. In this paper, we describe how we derive the macroscopic state equation for the associative watermarking model using the Okada theory. The theoretical results obtained by the fourth-order theory were in good agreement with those obtained by computer simulations. Furthermore, the performance of the associative watermarking model was evaluated using the bit error rate of the watermark, both theoretically and using computer simulations.","sentences":["We theoretically evaluated the performance of our proposed associative watermarking method in which the watermark is not embedded directly into the image.","We previously proposed a watermarking method that extends the zero-watermarking model by applying associative memory models.","In this model, the hetero-associative memory model is introduced to the mapping process between image features and watermarks, and the auto-associative memory model is applied to correct watermark errors.","We herein show that the associative watermarking model outperforms the zero-watermarking model through computer simulations using actual images.","In this paper, we describe how we derive the macroscopic state equation for the associative watermarking model using the Okada theory.","The theoretical results obtained by the fourth-order theory were in good agreement with those obtained by computer simulations.","Furthermore, the performance of the associative watermarking model was evaluated using the bit error rate of the watermark, both theoretically and using computer simulations."],"url":"http://arxiv.org/abs/2402.05508v1","category":"cs.MM"}
{"created":"2024-02-08 08:34:59","title":"Gravitating kinks with asymptotically flat metrics","abstract":"In this work, we consider a two-dimensional (2D) dilaton gravity model where the dilaton kinetic term $\\mathcal{X}$ is modified by an additional derivative coupling term $\\alpha\\mathcal{X}^2$. In the case with a canonical scalar matter field, the field equations of this model have a simple first-order formalism, from which exact static kink solutions can be constructed. The novelty of these solutions is that the corresponding metric can be asymptotically flat rather than asymptotically anti de Sitter. The linear stability and the localization of scalar matter fields are also studied. It was found that the solutions are stable against small linear perturbations, and the localization of scalar matter fields can be realized by introducing scalar-kink interactions.","sentences":["In this work, we consider a two-dimensional (2D) dilaton gravity model where the dilaton kinetic term $\\mathcal{X}$ is modified by an additional derivative coupling term $\\alpha\\mathcal{X}^2$. In the case with a canonical scalar matter field, the field equations of this model have a simple first-order formalism, from which exact static kink solutions can be constructed.","The novelty of these solutions is that the corresponding metric can be asymptotically flat rather than asymptotically anti de Sitter.","The linear stability and the localization of scalar matter fields are also studied.","It was found that the solutions are stable against small linear perturbations, and the localization of scalar matter fields can be realized by introducing scalar-kink interactions."],"url":"http://arxiv.org/abs/2402.05486v1","category":"hep-th"}
{"created":"2024-02-08 08:13:14","title":"Infrared Characterisation of Jupiter's Equatorial Disturbance Cycle","abstract":"We use an infrared dataset captured between 1984 and 2017 using several instruments and observatories to report five rare equatorial disturbances that completely altered the appearance of Jupiter's Equatorial Zone (EZ): the clearance of tropospheric clouds revealed a new 5-$\\mu$m-bright band encircling the planet at the equator, accompanied by large 5-$\\mu$m-bright filaments. Three events were observed in ground-based images in 1973, 1979 and 1992. We report and characterize for the first time the entire evolution of two new episodes of this unusual EZ state that presented their maximum 5-$\\mu$m-brightness in December 1999 and February 2007, coinciding with a brown coloration south of the equator and with large bluish filaments and white plumes in the northern EZ at visible wavelengths. We characterize their typical infrared-bright lifetimes of 12-18 months, with possible periodicities of 6-8 or 13-14 years. We predict that a full-scale equatorial disturbance could occur in 2019-21.","sentences":["We use an infrared dataset captured between 1984 and 2017 using several instruments and observatories to report five rare equatorial disturbances that completely altered the appearance of Jupiter's Equatorial Zone (EZ): the clearance of tropospheric clouds revealed a new 5-$\\mu$m-bright band encircling the planet at the equator, accompanied by large 5-$\\mu$m-bright filaments.","Three events were observed in ground-based images in 1973, 1979 and 1992.","We report and characterize for the first time the entire evolution of two new episodes of this unusual EZ state that presented their maximum 5-$\\mu$m-brightness in December 1999 and February 2007, coinciding with a brown coloration south of the equator and with large bluish filaments and white plumes in the northern EZ at visible wavelengths.","We characterize their typical infrared-bright lifetimes of 12-18 months, with possible periodicities of 6-8 or 13-14 years.","We predict that a full-scale equatorial disturbance could occur in 2019-21."],"url":"http://arxiv.org/abs/2402.05481v1","category":"astro-ph.EP"}
{"created":"2024-02-08 08:01:39","title":"Complete immersions with constant scalar curvature and flat normal bundle","abstract":"Let $M$ be a complete Riemannian manifold with constant scalar curvature $R$ and dimension $n\\geq 3,$ and let $f: M\\rightarrow M^{n+k}(c)$ be a proper isometric immersion with flat normal bundle and type $(1, n-1)$ in a complete simply-connected Riemannian space form $M^{n+k}(c)$ of constant sectional curvature $c$ and dimension $n+k.$ Our first result is global and states that $R\\geq 0$ if $c= 0,\\ R> \\frac{(n-1)(n-2)}{2}c$ if $c> 0,$ and $R\\geq \\frac{n(n-1)}{2}c$ if $c< 0.$ Our second result is of a local character and states that, if in addition we assume that $c\\geq 0$ and the mean curvature field of the immersion is parallel in the normal bundle, then $M$ has non-negative sectional curvature.","sentences":["Let $M$ be a complete Riemannian manifold with constant scalar curvature $R$ and dimension $n\\geq 3,$ and let $f: M\\rightarrow M^{n+k}(c)$ be a proper isometric immersion with flat normal bundle and type $(1, n-1)$ in a complete simply-connected Riemannian space form $M^{n+k}(c)$ of constant sectional curvature $c$ and dimension $n+k.$ Our first result is global and states that $R\\geq 0$ if $c= 0,\\ R> \\frac{(n-1)(n-2)}{2}c$ if $c> 0,$ and $R\\geq \\frac{n(n-1)}{2}c$ if $c< 0.$ Our second result is of a local character and states that, if in addition we assume that $c\\geq 0$ and the mean curvature field of the immersion is parallel in the normal bundle, then $M$ has non-negative sectional curvature."],"url":"http://arxiv.org/abs/2402.05470v1","category":"math.DG"}
{"created":"2024-02-08 08:00:11","title":"Implicit Diffusion: Efficient Optimization through Stochastic Sampling","abstract":"We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings.","sentences":["We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions.","Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters.","We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps.","This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions.","We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings."],"url":"http://arxiv.org/abs/2402.05468v1","category":"cs.LG"}
{"created":"2024-02-08 07:42:44","title":"Anisotropic star with a linear equation of state (EOS)","abstract":"A family of solutions defining the interior of a static, spherically symmetric, compact anisotropic star is described by considering a new form of the equation of state (EOS). The analytic solution is derived by using the Finch and Skea ansatz for the metric potential g_rr, which has a clear geometric interpretation for the related background spacetime. The model parameters are fixed by smooth matching of the interior solution to the Schwarzschild exterior metric over the bounding surface of the compact star, together with the requirement that the radial pressure vanishes at the boundary. Data available for the pulsar 4U1802030 has been utilized to analyze the physical viability of the developed model. The model is shown to be stable.","sentences":["A family of solutions defining the interior of a static, spherically symmetric, compact anisotropic star is described by considering a new form of the equation of state (EOS).","The analytic solution is derived by using the Finch and Skea ansatz for the metric potential g_rr, which has a clear geometric interpretation for the related background spacetime.","The model parameters are fixed by smooth matching of the interior solution to the Schwarzschild exterior metric over the bounding surface of the compact star, together with the requirement that the radial pressure vanishes at the boundary.","Data available for the pulsar 4U1802030 has been utilized to analyze the physical viability of the developed model.","The model is shown to be stable."],"url":"http://arxiv.org/abs/2402.05461v1","category":"gr-qc"}
{"created":"2024-02-08 07:41:20","title":"I-FENN with Temporal Convolutional Networks: expediting the load-history analysis of non-local gradient damage propagation","abstract":"In this paper, we demonstrate for the first time how the Integrated Finite Element Neural Network (I-FENN) framework, previously proposed by the authors, can efficiently simulate the entire loading history of non-local gradient damage propagation. To achieve this goal, we first adopt a Temporal Convolutional Network (TCN) as the neural network of choice to capture the history-dependent evolution of the non-local strain in a coarsely meshed domain. The quality of the network predictions governs the computational performance of I-FENN, and therefore we perform an extended investigation aimed at enhancing them. We explore a data-driven vs. physics-informed TCN setup to arrive at an optimum network training, evaluating the network based on a coherent set of relevant performance metrics. We address the crucial issue of training a physics-informed network with input data that span vastly different length scales by proposing a systematic way of input normalization and output un-normalization. We then integrate the trained TCN within the nonlinear iterative FEM solver and apply I-FENN to simulate the damage propagation analysis. I-FENN is always applied in mesh idealizations different from the one used for the TCN training, showcasing the framework's ability to be used at progressively refined mesh resolutions. We illustrate several cases that I-FENN completes the simulation using either a modified or a full Newton-Raphson scheme, and we showcase its computational savings compared to both the classical monolithic and staggered FEM solvers. We underline that we satisfy very strict convergence criteria for every increment across the entire simulation, providing clear evidence of the robustness and accuracy of I-FENN. All the code and data used in this work will be made publicly available upon publication of the article.","sentences":["In this paper, we demonstrate for the first time how the Integrated Finite Element Neural Network (I-FENN) framework, previously proposed by the authors, can efficiently simulate the entire loading history of non-local gradient damage propagation.","To achieve this goal, we first adopt a Temporal Convolutional Network (TCN) as the neural network of choice to capture the history-dependent evolution of the non-local strain in a coarsely meshed domain.","The quality of the network predictions governs the computational performance of I-FENN, and therefore we perform an extended investigation aimed at enhancing them.","We explore a data-driven vs. physics-informed TCN setup to arrive at an optimum network training, evaluating the network based on a coherent set of relevant performance metrics.","We address the crucial issue of training a physics-informed network with input data that span vastly different length scales by proposing a systematic way of input normalization and output un-normalization.","We then integrate the trained TCN within the nonlinear iterative FEM solver and apply I-FENN to simulate the damage propagation analysis.","I-FENN is always applied in mesh idealizations different from the one used for the TCN training, showcasing the framework's ability to be used at progressively refined mesh resolutions.","We illustrate several cases that I-FENN completes the simulation using either a modified or a full Newton-Raphson scheme, and we showcase its computational savings compared to both the classical monolithic and staggered FEM solvers.","We underline that we satisfy very strict convergence criteria for every increment across the entire simulation, providing clear evidence of the robustness and accuracy of I-FENN.","All the code and data used in this work will be made publicly available upon publication of the article."],"url":"http://arxiv.org/abs/2402.05460v1","category":"cs.CE"}
{"created":"2024-02-08 07:14:17","title":"Mitigating Privacy Risk in Membership Inference by Convex-Concave Loss","abstract":"Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set. Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk. However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance. In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent. Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training. Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term. Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs. Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance in the privacy-utility trade-off.","sentences":["Machine learning models are susceptible to membership inference attacks (MIAs), which aim to infer whether a sample is in the training set.","Existing work utilizes gradient ascent to enlarge the loss variance of training data, alleviating the privacy risk.","However, optimizing toward a reverse direction may cause the model parameters to oscillate near local minima, leading to instability and suboptimal performance.","In this work, we propose a novel method -- Convex-Concave Loss, which enables a high variance of training loss distribution by gradient descent.","Our method is motivated by the theoretical analysis that convex losses tend to decrease the loss variance during training.","Thus, our key idea behind CCL is to reduce the convexity of loss functions with a concave term.","Trained with CCL, neural networks produce losses with high variance for training data, reinforcing the defense against MIAs.","Extensive experiments demonstrate the superiority of CCL, achieving state-of-the-art balance in the privacy-utility trade-off."],"url":"http://arxiv.org/abs/2402.05453v1","category":"cs.LG"}
{"created":"2024-02-08 07:08:55","title":"Gauge Independent Logarithms from Inflationary Gravitons","abstract":"Dependence on the graviton gauge enters the conventional effective field equations because they fail to account for quantum gravitational correlations with the source which excites the effective field and with the observer who measures it. Including these correlations has been shown to eliminate gauge dependence in flat space background. We generalize the technique to de Sitter background for the case of the 1-loop graviton corrections to the exchange potential of a massless, minimally coupled scalar.","sentences":["Dependence on the graviton gauge enters the conventional effective field equations because they fail to account for quantum gravitational correlations with the source which excites the effective field and with the observer who measures it.","Including these correlations has been shown to eliminate gauge dependence in flat space background.","We generalize the technique to de Sitter background for the case of the 1-loop graviton corrections to the exchange potential of a massless, minimally coupled scalar."],"url":"http://arxiv.org/abs/2402.05452v1","category":"hep-th"}
{"created":"2024-02-08 06:41:53","title":"Triangular solutions to the reflection equation for $U_q(\\widehat{sl_n})$","abstract":"We study solutions of the reflection equation related to the quantum affine algebra $U_q(\\widehat{sl_n})$. First, we explain how to construct a family of stochastic integrable vertex models with fixed boundary conditions. Then, we construct upper- and lower-triangular solutions of the reflection equation related to the symmetric tensor representations of $U_q(\\widehat{sl_n})$ with arbitrary spin. We also prove the star-star relation for the Boltzmann weights of the Ising-type model, conjectured by Bazhanov and Sergeev, and use it to verify certain properties of the obtained solutions.","sentences":["We study solutions of the reflection equation related to the quantum affine algebra $U_q(\\widehat{sl_n})$. First, we explain how to construct a family of stochastic integrable vertex models with fixed boundary conditions.","Then, we construct upper- and lower-triangular solutions of the reflection equation related to the symmetric tensor representations of $U_q(\\widehat{sl_n})$ with arbitrary spin.","We also prove the star-star relation for the Boltzmann weights of the Ising-type model, conjectured by Bazhanov and Sergeev, and use it to verify certain properties of the obtained solutions."],"url":"http://arxiv.org/abs/2402.05442v1","category":"math-ph"}
{"created":"2024-02-08 06:21:28","title":"One-loop QCD corrections to heavy quark angular distributions in DIS","abstract":"In this paper we calculate the differential cross sections for one heavy quark production in deep-inelastic scattering. We construct proper projection operators to give all possible azimuthal angle distributions of the heavy quark for unpolarized and longitudinally polarized scatterings. These projection operators are expressed in terms of momenta of incoming hadron, virtual photon and detected heavy quark. The azimuthal angle distributions are calculated to next-to-leading order of $\\alpha_s$, i.e.,$O(\\alpha_s^2)$, in a unified way. Analytic expressions of the hard coefficients are given. Numerical results on future electron-ion colliders are also given. It is found that at least three azimuthal angle asymmetries can be more than $1\\%$ in typical kinematical regions of these colliders.","sentences":["In this paper we calculate the differential cross sections for one heavy quark production in deep-inelastic scattering.","We construct proper projection operators to give all possible azimuthal angle distributions of the heavy quark for unpolarized and longitudinally polarized scatterings.","These projection operators are expressed in terms of momenta of incoming hadron, virtual photon and detected heavy quark.","The azimuthal angle distributions are calculated to next-to-leading order of $\\alpha_s$, i.e.,$O(\\alpha_s^2)$, in a unified way.","Analytic expressions of the hard coefficients are given.","Numerical results on future electron-ion colliders are also given.","It is found that at least three azimuthal angle asymmetries can be more than $1\\%$ in typical kinematical regions of these colliders."],"url":"http://arxiv.org/abs/2402.05436v1","category":"hep-ph"}
{"created":"2024-02-08 05:56:20","title":"The isoperimetric inequality","abstract":"We discuss several classical and recent proofs of the isoperimetric inequality and the Sobolev inequality.","sentences":["We discuss several classical and recent proofs of the isoperimetric inequality and the Sobolev inequality."],"url":"http://arxiv.org/abs/2402.05429v1","category":"math.DG"}
{"created":"2024-02-08 02:21:23","title":"Nonlinear functionals of master equation unravelings","abstract":"Unravelings provide a probabilistic representation of solutions of master equations and a method of computation of the density operator dynamics. The trajectories generated by unravelings may also be treated as real -- as in the stochastic collapse models. While averages of linear functionals of the unraveling trajectories can be calculated from the master equation, the situation is different for nonlinear functionals, thanks to the corrections with nonzero expected values, coming from the It\\^o formula. Two types of nonlinear functionals are considered here: variance, and entropy. The corrections are calculated explicitly for two types of unravelings, based on Poisson and Wiener processes. In the case of entropy, these corrections are shown to be negative, expressing the localization introduced by the Lindblad operators.","sentences":["Unravelings provide a probabilistic representation of solutions of master equations and a method of computation of the density operator dynamics.","The trajectories generated by unravelings may also be treated as real -- as in the stochastic collapse models.","While averages of linear functionals of the unraveling trajectories can be calculated from the master equation, the situation is different for nonlinear functionals, thanks to the corrections with nonzero expected values, coming from the It\\^o formula.","Two types of nonlinear functionals are considered here: variance, and entropy.","The corrections are calculated explicitly for two types of unravelings, based on Poisson and Wiener processes.","In the case of entropy, these corrections are shown to be negative, expressing the localization introduced by the Lindblad operators."],"url":"http://arxiv.org/abs/2402.05352v1","category":"quant-ph"}
{"created":"2024-02-08 01:06:36","title":"Conservation laws for a generalized seventh order KdV equation","abstract":"In this paper, by applying the multiplier method we obtain a complete classification of low-order local conservation laws for a generalized seventh-order KdV equation depending on seven arbitrary nonzero parameters. We apply the Lie method in order to classify all point symmetries admitted by the equation in terms of the arbitrary parameters. We find that there are no special cases of the parameters for which the equation admits extra symmetries, other than those that can be found by inspection (scaling symmetry and space and time translation symmetries). We consider the reduced ordinary differential equations and we determined all integrating factors of the reduced equation from the combined $x$- and $t$- translation symmetries. Finally, we observe that all integrating factors arise by reduction of the low-order multipliers of the generalized seventh-order KdV equation.","sentences":["In this paper, by applying the multiplier method we obtain a complete classification of low-order local conservation laws for a generalized seventh-order KdV equation depending on seven arbitrary nonzero parameters.","We apply the Lie method in order to classify all point symmetries admitted by the equation in terms of the arbitrary parameters.","We find that there are no special cases of the parameters for which the equation admits extra symmetries, other than those that can be found by inspection (scaling symmetry and space and time translation symmetries).","We consider the reduced ordinary differential equations and we determined all integrating factors of the reduced equation from the combined $x$- and $t$- translation symmetries.","Finally, we observe that all integrating factors arise by reduction of the low-order multipliers of the generalized seventh-order KdV equation."],"url":"http://arxiv.org/abs/2402.05341v1","category":"nlin.SI"}
{"created":"2024-02-07 23:57:08","title":"Phonon Modal Analysis of Thermal Transport in ThO2 with Point Defects using Equilibrium Molecular Dynamics","abstract":"In this study, we investigate how point defects in ThO2, an advanced nuclear fuel as well as a surrogate for other fluorite structured materials, affect phonon mode-resolved thermal transport based on extensive equilibrium molecular dynamics. By incorporating the normal modes from lattice dynamics, we decompose the trajectory and heat flux to normal mode space and extract key phonon properties, including phonon relaxation times and their contributions to thermal conductivity. Two types of theories are applied: one obtains phonon relaxation times and spectral conductivity via the Boltzmann transport equation and normal mode analysis, while the other applies the Green-Kubo theory for modal analysis. Both approaches enable us to evaluate the effects of four types of point defects. The strongest impact to a reduction in thermal conductivity is from Th interstitial, followed by Th vacancy, and O defects of similar magnitude, consistent with previous theoretical calculations.","sentences":["In this study, we investigate how point defects in ThO2, an advanced nuclear fuel as well as a surrogate for other fluorite structured materials, affect phonon mode-resolved thermal transport based on extensive equilibrium molecular dynamics.","By incorporating the normal modes from lattice dynamics, we decompose the trajectory and heat flux to normal mode space and extract key phonon properties, including phonon relaxation times and their contributions to thermal conductivity.","Two types of theories are applied: one obtains phonon relaxation times and spectral conductivity via the Boltzmann transport equation and normal mode analysis, while the other applies the Green-Kubo theory for modal analysis.","Both approaches enable us to evaluate the effects of four types of point defects.","The strongest impact to a reduction in thermal conductivity is from Th interstitial, followed by Th vacancy, and O defects of similar magnitude, consistent with previous theoretical calculations."],"url":"http://arxiv.org/abs/2402.05325v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-07 23:02:53","title":"Investigating Generalization Behaviours of Generative Flow Networks","abstract":"Generative Flow Networks (GFlowNets, GFNs) are a generative framework for learning unnormalized probability mass functions over discrete spaces. Since their inception, GFlowNets have proven to be useful for learning generative models in applications where the majority of the discrete space is unvisited during training. This has inspired some to hypothesize that GFlowNets, when paired with deep neural networks (DNNs), have favourable generalization properties. In this work, we empirically verify some of the hypothesized mechanisms of generalization of GFlowNets. In particular, we find that the functions that GFlowNets learn to approximate have an implicit underlying structure which facilitate generalization. We also find that GFlowNets are sensitive to being trained offline and off-policy; however, the reward implicitly learned by GFlowNets is robust to changes in the training distribution.","sentences":["Generative Flow Networks (GFlowNets, GFNs) are a generative framework for learning unnormalized probability mass functions over discrete spaces.","Since their inception, GFlowNets have proven to be useful for learning generative models in applications where the majority of the discrete space is unvisited during training.","This has inspired some to hypothesize that GFlowNets, when paired with deep neural networks (DNNs), have favourable generalization properties.","In this work, we empirically verify some of the hypothesized mechanisms of generalization of GFlowNets.","In particular, we find that the functions that GFlowNets learn to approximate have an implicit underlying structure which facilitate generalization.","We also find that GFlowNets are sensitive to being trained offline and off-policy; however, the reward implicitly learned by GFlowNets is robust to changes in the training distribution."],"url":"http://arxiv.org/abs/2402.05309v1","category":"cs.LG"}
{"created":"2024-02-07 22:10:36","title":"Graph Neural Networks as Fast and High-fidelity Emulators for Finite-Element Ice Sheet Modeling","abstract":"Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU). In this study, we develop graph neural networks (GNN) as fast surrogate models to preserve the finite element structure of ISSM. Using the 20-year transient simulations in the Pine Island Glacier (PIG), we train and test three GNNs: graph convolutional network (GCN), graph attention network (GAT), and equivariant graph convolutional network (EGCN). These GNNs reproduce ice thickness and velocity with better accuracy than the classic convolutional neural network (CNN) and multi-layer perception (MLP). In particular, GNNs successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG. When our GNN emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster computational time than the CPU-based ISSM simulation.","sentences":["Although the finite element approach of the Ice-sheet and Sea-level System Model (ISSM) solves ice dynamics problems governed by Stokes equations quickly and accurately, such numerical modeling requires intensive computation on central processing units (CPU).","In this study, we develop graph neural networks (GNN) as fast surrogate models to preserve the finite element structure of ISSM.","Using the 20-year transient simulations in the Pine Island Glacier (PIG), we train and test three GNNs: graph convolutional network (GCN), graph attention network (GAT), and equivariant graph convolutional network (EGCN).","These GNNs reproduce ice thickness and velocity with better accuracy than the classic convolutional neural network (CNN) and multi-layer perception (MLP).","In particular, GNNs successfully capture the ice mass loss and acceleration induced by higher basal melting rates in the PIG.","When our GNN emulators are implemented on graphic processing units (GPUs), they show up to 50 times faster computational time than the CPU-based ISSM simulation."],"url":"http://arxiv.org/abs/2402.05291v1","category":"cs.LG"}
{"created":"2024-02-07 22:02:20","title":"Triadic Resonance in Columnar Vortices","abstract":"Employing a poloidal-toroidal projection technique, a multi-scale analysis of resonant wave triads in columnar vortices is performed to obtain the governing equations of the triadically coupled wave amplitudes. For inviscid flows, we establish that resonance between neutral, smooth waves is conservative, and the temporal evolution of wave amplitudes is either bounded or explosively unstable based on the signs of the triad's interaction coefficients. Assessing the onset of weakly nonlinear instabilities through the pseudoenergy criterion introduced by Cairns (1979, J. Fluid Mech., vol. 92), we use the large-axial-wavenumber asymptotic approach by Le Dizes and Lacaze (2005, J. Fluid Mech., vol. 542) to evaluate each triad member's pseudoenergy and argue against the possibility of explosive conservative three-wave resonance involving only regular Kelvin waves. Additionally, extending our investigation to specific vortices, such as the Lamb-Oseen vortex and the Batchelor vortex, we find that triadic resonance among their neutral modes consistently results in bounded behaviour.","sentences":["Employing a poloidal-toroidal projection technique, a multi-scale analysis of resonant wave triads in columnar vortices is performed to obtain the governing equations of the triadically coupled wave amplitudes.","For inviscid flows, we establish that resonance between neutral, smooth waves is conservative, and the temporal evolution of wave amplitudes is either bounded or explosively unstable based on the signs of the triad's interaction coefficients.","Assessing the onset of weakly nonlinear instabilities through the pseudoenergy criterion introduced by Cairns (1979, J. Fluid Mech., vol. 92), we use the large-axial-wavenumber asymptotic approach by Le Dizes and Lacaze (2005, J. Fluid Mech., vol. 542) to evaluate each triad member's pseudoenergy and argue against the possibility of explosive conservative three-wave resonance involving only regular Kelvin waves.","Additionally, extending our investigation to specific vortices, such as the Lamb-Oseen vortex and the Batchelor vortex, we find that triadic resonance among their neutral modes consistently results in bounded behaviour."],"url":"http://arxiv.org/abs/2402.05287v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 21:53:28","title":"Physics Informed and Data Driven Simulation of Underwater Images via Residual Learning","abstract":"In general, underwater images suffer from color distortion and low contrast, because light is attenuated and backscattered as it propagates through water (differently depending on wavelength and on the properties of the water body). An existing simple degradation model (similar to atmospheric image \"hazing\" effects), though helpful, is not sufficient to properly represent the underwater image degradation because there are unaccounted for and non-measurable factors e.g. scattering of light due to turbidity of water, reflective characteristics of turbid medium etc. We propose a deep learning-based architecture to automatically simulate the underwater effects where only a dehazing-like image formation equation is known to the network, and the additional degradation due to the other unknown factors if inferred in a data-driven way. We only use RGB images (because in real-time scenario depth image is not available) to estimate the depth image. For testing, we have proposed (due to the lack of real underwater image datasets) a complex image formation model/equation to manually generate images that resemble real underwater images (used as ground truth). However, only the classical image formation equation (the one used for image dehazing) is informed to the network. This mimics the fact that in a real scenario, the physics are never completely known and only simplified models are known. Thanks to the ground truth, generated by a complex image formation equation, we could successfully perform a qualitative and quantitative evaluation of proposed technique, compared to other purely data driven approaches","sentences":["In general, underwater images suffer from color distortion and low contrast, because light is attenuated and backscattered as it propagates through water (differently depending on wavelength and on the properties of the water body).","An existing simple degradation model (similar to atmospheric image \"hazing\" effects), though helpful, is not sufficient to properly represent the underwater image degradation because there are unaccounted for and non-measurable factors e.g. scattering of light due to turbidity of water, reflective characteristics of turbid medium etc.","We propose a deep learning-based architecture to automatically simulate the underwater effects where only a dehazing-like image formation equation is known to the network, and the additional degradation due to the other unknown factors if inferred in a data-driven way.","We only use RGB images (because in real-time scenario depth image is not available) to estimate the depth image.","For testing, we have proposed (due to the lack of real underwater image datasets) a complex image formation model/equation to manually generate images that resemble real underwater images (used as ground truth).","However, only the classical image formation equation (the one used for image dehazing) is informed to the network.","This mimics the fact that in a real scenario, the physics are never completely known and only simplified models are known.","Thanks to the ground truth, generated by a complex image formation equation, we could successfully perform a qualitative and quantitative evaluation of proposed technique, compared to other purely data driven approaches"],"url":"http://arxiv.org/abs/2402.05281v1","category":"cs.CV"}
{"created":"2024-02-07 21:28:58","title":"Global existence of a classical solution for the isentropic nozzle flow","abstract":"Our goal in this paper is to prove the global existence of a classical solution for the isentropic nozzle flow. Regarding this problem, there exist some global existence theorems of weak solutions. However, that of classical solutions does not have much attention until now. When we consider the present problem, the main difficulty is to obtain the uniform bound of solutions and their derivatives. To solve this, we introduce an invariant region depending on the space variable and a functional satisfying the Riccati equation along the characteristic lines.","sentences":["Our goal in this paper is to prove the global existence of a classical solution for the isentropic nozzle flow.","Regarding this problem, there exist some global existence theorems of weak solutions.","However, that of classical solutions does not have much attention until now.","When we consider the present problem, the main difficulty is to obtain the uniform bound of solutions and their derivatives.","To solve this, we introduce an invariant region depending on the space variable and a functional satisfying the Riccati equation along the characteristic lines."],"url":"http://arxiv.org/abs/2402.05268v1","category":"math.AP"}
{"created":"2024-02-07 21:23:47","title":"A computational approach to visual ecology with deep reinforcement learning","abstract":"Animal vision is thought to optimize various objectives from metabolic efficiency to discrimination performance, yet its ultimate objective is to facilitate the survival of the animal within its ecological niche. However, modeling animal behavior in complex environments has been challenging. To study how environments shape and constrain visual processing, we developed a deep reinforcement learning framework in which an agent moves through a 3-d environment that it perceives through a vision model, where its only goal is to survive. Within this framework we developed a foraging task where the agent must gather food that sustains it, and avoid food that harms it. We first established that the complexity of the vision model required for survival on this task scaled with the variety and visual complexity of the food in the environment. Moreover, we showed that a recurrent network architecture was necessary to fully exploit complex vision models on the most visually demanding tasks. Finally, we showed how different network architectures learned distinct representations of the environment and task, and lead the agent to exhibit distinct behavioural strategies. In summary, this paper lays the foundation for a computational approach to visual ecology, provides extensive benchmarks for future work, and demonstrates how representations and behaviour emerge from an agent's drive for survival.","sentences":["Animal vision is thought to optimize various objectives from metabolic efficiency to discrimination performance, yet its ultimate objective is to facilitate the survival of the animal within its ecological niche.","However, modeling animal behavior in complex environments has been challenging.","To study how environments shape and constrain visual processing, we developed a deep reinforcement learning framework in which an agent moves through a 3-d environment that it perceives through a vision model, where its only goal is to survive.","Within this framework we developed a foraging task where the agent must gather food that sustains it, and avoid food that harms it.","We first established that the complexity of the vision model required for survival on this task scaled with the variety and visual complexity of the food in the environment.","Moreover, we showed that a recurrent network architecture was necessary to fully exploit complex vision models on the most visually demanding tasks.","Finally, we showed how different network architectures learned distinct representations of the environment and task, and lead the agent to exhibit distinct behavioural strategies.","In summary, this paper lays the foundation for a computational approach to visual ecology, provides extensive benchmarks for future work, and demonstrates how representations and behaviour emerge from an agent's drive for survival."],"url":"http://arxiv.org/abs/2402.05266v1","category":"cs.NE"}
{"created":"2024-02-07 20:51:49","title":"Time-fractional Allen-Cahn equations versus powers of the mean curvature","abstract":"We show by a formal asymptotic expansion that level sets of solutions of a time-fractional Allen-Cahn equation evolve by a geometric flow whose normal velocity is a positive power of the mean curvature.   This connection is quite intriguing, since the original equation is nonlocal and the evolution of its solutions depends on all previous states, but the associated geometric flow is of purely local type, with no memory effect involved.","sentences":["We show by a formal asymptotic expansion that level sets of solutions of a time-fractional Allen-Cahn equation evolve by a geometric flow whose normal velocity is a positive power of the mean curvature.   ","This connection is quite intriguing, since the original equation is nonlocal and the evolution of its solutions depends on all previous states, but the associated geometric flow is of purely local type, with no memory effect involved."],"url":"http://arxiv.org/abs/2402.05250v1","category":"math.AP"}
{"created":"2024-02-07 20:34:40","title":"Cholla-MHD: An Exascale-Capable Magnetohydrodynamic Extension to the Cholla Astrophysical Simulation Code","abstract":"We present an extension of the massively parallel, GPU native, astrophysical hydrodynamics code Cholla to magnetohydrodynamics (MHD). Cholla solves the ideal MHD equations in their Eulerian form on a static Cartesian mesh utilizing the Van Leer + Constrained Transport integrator, the HLLD Riemann solver, and reconstruction methods at second and third order. Cholla's MHD module can perform over 200 million cell updates per GPU-second while using the HLLD Riemann solver and second order reconstruction. The inherently parallel nature of GPUs combined with increased memory in new hardware allows Cholla's MHD module to perform simulation with resolutions of $>450^3$ cells on a single GPU. We employ GPU direct MPI to attain nearly perfect weak scaling on the exascale supercomputer \\textit{Frontier}, while using up to 74,000 GPUs and simulating a total grid size of over 1.2 trillion cells. A suite of test problems highlights the accuracy of Cholla's MHD module and demonstrates that zero magnetic divergence in solutions is maintained to round off error. We also present new testing and continuous integration tools using GoogleTest, GitHub Actions, and Jenkins that have made development more robust and accurate and ensure reliability in the future.","sentences":["We present an extension of the massively parallel, GPU native, astrophysical hydrodynamics code Cholla to magnetohydrodynamics (MHD).","Cholla solves the ideal MHD equations in their Eulerian form on a static Cartesian mesh utilizing the Van Leer + Constrained Transport integrator, the HLLD Riemann solver, and reconstruction methods at second and third order.","Cholla's MHD module can perform over 200 million cell updates per GPU-second while using the HLLD Riemann solver and second order reconstruction.","The inherently parallel nature of GPUs combined with increased memory in new hardware allows Cholla's MHD module to perform simulation with resolutions of $>450^3$ cells on a single GPU.","We employ GPU direct MPI to attain nearly perfect weak scaling on the exascale supercomputer \\textit{Frontier}, while using up to 74,000 GPUs and simulating a total grid size of over 1.2 trillion cells.","A suite of test problems highlights the accuracy of Cholla's MHD module and demonstrates that zero magnetic divergence in solutions is maintained to round off error.","We also present new testing and continuous integration tools using GoogleTest, GitHub Actions, and Jenkins that have made development more robust and accurate and ensure reliability in the future."],"url":"http://arxiv.org/abs/2402.05240v1","category":"astro-ph.GA"}
{"created":"2024-02-07 20:09:59","title":"Mittag-Leffler functions in the Fourier space","abstract":"Let $\\alpha \\in (0,2)$ and let $\\beta>0$. Fix $-\\pi<\\varphi\\leq \\pi$ such that $|\\varphi|>\\alpha \\pi/2$. We determine the precise asymptotic behaviour of the Fourier transform of $E_{\\alpha,\\beta}(e^{\\dot{\\imath} \\varphi} |\\cdot|^{\\sigma})$ whenever $\\sigma>(n-1)/2$. Remarkably, this asymptotic behaviour turns out to be independent of $\\alpha$ and $\\beta$. This helps us determine the values of the Lebesgue exponent $p=p(\\sigma)$, $\\sigma>(n-1)/2$, for which $\\mathcal{F} \\left(E_{\\alpha,\\beta}(e^{\\dot{\\imath} \\varphi} |\\cdot|^{\\sigma})\\right)$ is in $L^{p}(\\mathbb{R}^{n})$. These values cannot be obtained via the Hausdorff-Young inequality. This problem arises in the study of space-time fractional equations. Our approach provides an effective alternative to the asymptotic analysis of the Fox $H-$ functions recently applied to the case $\\alpha \\in (0,1)$, $\\beta=\\alpha, 1$, $\\varphi=-\\pi/2,\\pi$. We rather rely on an appropriate integral representative that represents $E_{\\alpha,\\beta}$ continuously up to the origin, and we develop an extended asymptotic expansion for the Bessel function whose coefficients and remainder term are obtained explicitly.","sentences":["Let $\\alpha \\in (0,2)$ and let $\\beta>0$. Fix $-\\pi<\\varphi\\leq \\pi$","such that $|\\varphi|>\\alpha \\pi/2$. We determine the precise asymptotic behaviour of the Fourier transform of $E_{\\alpha,\\beta}(e^{\\dot{\\imath} \\varphi} |\\cdot|^{\\sigma})$ whenever $\\sigma>(n-1)/2$. Remarkably, this asymptotic behaviour turns out to be independent of $\\alpha$ and $\\beta$. This helps us determine the values of the Lebesgue exponent $p=p(\\sigma)$, $\\sigma>(n-1)/2$, for which $\\mathcal{F} \\left(E_{\\alpha,\\beta}(e^{\\dot{\\imath} \\varphi} |\\cdot|^{\\sigma})\\right)$ is in $L^{p}(\\mathbb{R}^{n})$. These values cannot be obtained via the Hausdorff-Young inequality.","This problem arises in the study of space-time fractional equations.","Our approach provides an effective alternative to the asymptotic analysis of the Fox $H-$ functions recently applied to the case $\\alpha \\in (0,1)$, $\\beta=\\alpha, 1$, $\\varphi=-\\pi/2,\\pi$. We rather rely on an appropriate integral representative that represents $E_{\\alpha,\\beta}$ continuously up to the origin, and we develop an extended asymptotic expansion for the Bessel function whose coefficients and remainder term are obtained explicitly."],"url":"http://arxiv.org/abs/2402.05230v1","category":"math.CA"}
{"created":"2024-02-07 20:09:11","title":"Long time numerical stability of implicit schemes for stochastic heat equations","abstract":"This paper studies the long time stability of both stochastic heat equations on a bounded domain driven by a correlated noise and their approximations. It is popular for researchers to prove the intermittency of the solution which means that the moments of solution to stochastic heat equation usually grow exponentially to infinite and this hints that the solution to stochastic heat equation is generally not stable in long time. However, quite surprisingly in this paper we show that when the domain is bounded and when the noise is not singular in spatial variables, the system can be long time stable and we also prove that we can approximate the solution by its finite dimensional spectral approximation which is also long time stable. The idea is to use eigenfunction expansion of the Laplacian on bounded domain. We also present numerical experiments which are consistent with our theoretical results.","sentences":["This paper studies the long time stability of both stochastic heat equations on a bounded domain driven by a correlated noise and their approximations.","It is popular for researchers to prove the intermittency of the solution which means that the moments of solution to stochastic heat equation usually grow exponentially to infinite and this hints that the solution to stochastic heat equation is generally not stable in long time.","However, quite surprisingly in this paper we show that when the domain is bounded and when the noise is not singular in spatial variables, the system can be long time stable and we also prove that we can approximate the solution by its finite dimensional spectral approximation which is also long time stable.","The idea is to use eigenfunction expansion of the Laplacian on bounded domain.","We also present numerical experiments which are consistent with our theoretical results."],"url":"http://arxiv.org/abs/2402.05229v1","category":"math.NA"}
{"created":"2024-02-07 19:52:14","title":"Photochemically-induced acousto-optics in gases","abstract":"Acousto-optics consists of launching acoustic waves in a medium (usually a crystal) in order to modulate its refractive index and create a tunable optical grating. In this article, we present the theoretical basis of a new scheme to generate acousto-optics in a gas, where the acoustic waves are initiated by the localized absorption (and thus gas heating) of spatially-modulated UV light, as was demonstrated in Y. Michine and H. Yoneda, Commun. Phys. 3, 24 (2020). We identify the chemical reactions initiated by the absorption of UV light via the photodissociation of ozone molecules present in the gas, and calculate the resulting temperature increase in the gas as a function of space and time. Solving the Euler fluid equations shows that the modulated, isochoric heating initiates a mixed acoustic/entropy wave in the gas, whose high-amplitude density (and thus refractive index) modulation can be used to manipulate a high-power laser. We calculate that diffraction efficiencies near 100 percent can be obtained using only a few millimeters of gas containing a few percent ozone fraction at room temperature, with UV fluences of less than 100 mJ/cm2, consistent with the experimental measurements by Michine and Yoneda. Gases have optics damage thresholds two to three times beyond those of solids; these optical elements should therefore be able to manipulate kJ-class lasers. Our analysis suggest possible ways to optimize the diffraction efficiency by changing the buffer gas composition.","sentences":["Acousto-optics consists of launching acoustic waves in a medium (usually a crystal) in order to modulate its refractive index and create a tunable optical grating.","In this article, we present the theoretical basis of a new scheme to generate acousto-optics in a gas, where the acoustic waves are initiated by the localized absorption (and thus gas heating) of spatially-modulated UV light, as was demonstrated in Y. Michine and H. Yoneda, Commun.","Phys. 3, 24 (2020).","We identify the chemical reactions initiated by the absorption of UV light via the photodissociation of ozone molecules present in the gas, and calculate the resulting temperature increase in the gas as a function of space and time.","Solving the Euler fluid equations shows that the modulated, isochoric heating initiates a mixed acoustic/entropy wave in the gas, whose high-amplitude density (and thus refractive index) modulation can be used to manipulate a high-power laser.","We calculate that diffraction efficiencies near 100 percent can be obtained using only a few millimeters of gas containing a few percent ozone fraction at room temperature, with UV fluences of less than 100 mJ/cm2, consistent with the experimental measurements by Michine and Yoneda.","Gases have optics damage thresholds two to three times beyond those of solids; these optical elements should therefore be able to manipulate kJ-class lasers.","Our analysis suggest possible ways to optimize the diffraction efficiency by changing the buffer gas composition."],"url":"http://arxiv.org/abs/2402.05219v1","category":"physics.optics"}
{"created":"2024-02-07 19:43:35","title":"Classical and Quantum Theory of Fluctuations for Many-Particle Systems out of Equilibrium","abstract":"Correlated classical and quantum many-particle systems out of equilibrium are of high interest in many fields, including dense plasmas, correlated solids, and ultracold atoms. Accurate theoretical description of these systems is challenging both, conceptionally and with respect to computational resources. While for classical systems, in principle, exact simulations are possible via molecular dynamics, this is not the case for quantum systems. Alternatively, one can use many-particle approaches such as hydrodynamics, kinetic theory or nonequilibrium Green functions (NEGF). However, NEGF exhibit a very unfavorable cubic scaling of the CPU time with the number of time steps. An alternative is the G1--G2 scheme [N. Schl\\\"unzen et al., Phys. Rev. Lett. \\textbf{124}, 076601 (2020)] which allows for NEGF simulations with time linear scaling, however, at the cost of large memory consumption. The reason is the need to store the two-particle correlation function. This problem can be overcome for a number of approximations by reformulating the kinetic equations in terms of fluctuations -- an approach that was developed, for classical systems, by Yu.L. Klimontovich [JETP \\textbf{33}, 982 (1957)]. Here we present an overview of his ideas and extend them to quantum systems. In particular, we demonstrate that this quantum fluctuations approach can reproduce the nonequilibrium $GW$ approximation [E. Schroedter \\textit{et al.}, Cond. Matt. Phys. \\textbf{25}, 23401 (2022)] promising high accuracy at low computational cost which arises from an effective semiclassical stochastic sampling procedure. We also demonstrate how to extend the approach to the two-time exchange-correlation functions and the density response properties. [E. Schroedter \\textit{et al.}, Phys. Rev. B \\textbf{108}, 205109 (2023)].","sentences":["Correlated classical and quantum many-particle systems out of equilibrium are of high interest in many fields, including dense plasmas, correlated solids, and ultracold atoms.","Accurate theoretical description of these systems is challenging both, conceptionally and with respect to computational resources.","While for classical systems, in principle, exact simulations are possible via molecular dynamics, this is not the case for quantum systems.","Alternatively, one can use many-particle approaches such as hydrodynamics, kinetic theory or nonequilibrium Green functions (NEGF).","However, NEGF exhibit a very unfavorable cubic scaling of the CPU time with the number of time steps.","An alternative is the G1--G2 scheme [N. Schl\\\"unzen et al., Phys.","Rev. Lett.","\\textbf{124}, 076601 (2020)] which allows for NEGF simulations with time linear scaling, however, at the cost of large memory consumption.","The reason is the need to store the two-particle correlation function.","This problem can be overcome for a number of approximations by reformulating the kinetic equations in terms of fluctuations -- an approach that was developed, for classical systems, by Yu.","L. Klimontovich","[JETP \\textbf{33}, 982 (1957)].","Here we present an overview of his ideas and extend them to quantum systems.","In particular, we demonstrate that this quantum fluctuations approach can reproduce the nonequilibrium $GW$ approximation","[E. Schroedter \\textit{et al.}, Cond.","Matt.","Phys. \\textbf{25}, 23401 (2022)] promising high accuracy at low computational cost which arises from an effective semiclassical stochastic sampling procedure.","We also demonstrate how to extend the approach to the two-time exchange-correlation functions and the density response properties.","[E. Schroedter \\textit{et al.}, Phys. Rev. B \\textbf{108}, 205109 (2023)]."],"url":"http://arxiv.org/abs/2402.05214v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-07 19:07:32","title":"Geometric Slosh-Free Tracking for Robotic Manipulators","abstract":"This work focuses on the agile transportation of liquids with robotic manipulators. In contrast to existing methods that are either computationally heavy, system/container specific or dependant on a singularity-prone pendulum model, we present a real-time slosh-free tracking technique. This method solely requires the reference trajectory and the robot's kinematic constraints to output kinematically feasible joint space commands. The crucial element underlying this approach consists on mimicking the end-effector's motion through a virtual quadrotor, which is inherently slosh-free and differentially flat, thereby allowing us to calculate a slosh-free reference orientation. Through the utilization of a cascaded proportional-derivative (PD) controller, this slosh-free reference is transformed into task space acceleration commands, which, following the resolution of a Quadratic Program (QP) based on Resolved Acceleration Control (RAC), are translated into a feasible joint configuration. The validity of the proposed approach is demonstrated by simulated and real-world experiments on a 7 DoF Franka Emika Panda robot.   Code: https://github.com/jonarriza96/gsft Video: https://youtu.be/4kitqYVS9n8","sentences":["This work focuses on the agile transportation of liquids with robotic manipulators.","In contrast to existing methods that are either computationally heavy, system/container specific or dependant on a singularity-prone pendulum model, we present a real-time slosh-free tracking technique.","This method solely requires the reference trajectory and the robot's kinematic constraints to output kinematically feasible joint space commands.","The crucial element underlying this approach consists on mimicking the end-effector's motion through a virtual quadrotor, which is inherently slosh-free and differentially flat, thereby allowing us to calculate a slosh-free reference orientation.","Through the utilization of a cascaded proportional-derivative (PD) controller, this slosh-free reference is transformed into task space acceleration commands, which, following the resolution of a Quadratic Program (QP) based on Resolved Acceleration Control (RAC), are translated into a feasible joint configuration.","The validity of the proposed approach is demonstrated by simulated and real-world experiments on a 7 DoF Franka Emika Panda robot.   ","Code: https://github.com/jonarriza96/gsft Video: https://youtu.be/4kitqYVS9n8"],"url":"http://arxiv.org/abs/2402.05197v1","category":"cs.RO"}
{"created":"2024-02-07 19:05:27","title":"JAX-Fluids 2.0: Towards HPC for Differentiable CFD of Compressible Two-phase Flows","abstract":"In our effort to facilitate machine learning-assisted computational fluid dynamics (CFD), we introduce the second iteration of JAX-Fluids. JAX-Fluids is a Python-based fully-differentiable CFD solver designed for compressible single- and two-phase flows. In this work, the first version is extended to incorporate high-performance computing (HPC) capabilities. We introduce a parallelization strategy utilizing JAX primitive operations that scales efficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024 TPU v3 cores) HPC systems. We further demonstrate the stable parallel computation of automatic differentiation gradients across extended integration trajectories. The new code version offers enhanced two-phase flow modeling capabilities. In particular, a five-equation diffuse-interface model is incorporated which complements the level-set sharp-interface model. Additional algorithmic improvements include positivity-preserving limiters for increased robustness, support for stretched Cartesian meshes, refactored I/O handling, comprehensive post-processing routines, and an updated list of state-of-the-art high-order numerical discretization schemes. We verify newly added numerical models by showcasing simulation results for single- and two-phase flows, including turbulent boundary layer and channel flows, air-helium shock bubble interactions, and air-water shock drop interactions.","sentences":["In our effort to facilitate machine learning-assisted computational fluid dynamics (CFD), we introduce the second iteration of JAX-Fluids.","JAX-Fluids is a Python-based fully-differentiable CFD solver designed for compressible single- and two-phase flows.","In this work, the first version is extended to incorporate high-performance computing (HPC) capabilities.","We introduce a parallelization strategy utilizing JAX primitive operations that scales efficiently on GPU (up to 512 NVIDIA A100 graphics cards) and TPU (up to 1024 TPU v3 cores) HPC systems.","We further demonstrate the stable parallel computation of automatic differentiation gradients across extended integration trajectories.","The new code version offers enhanced two-phase flow modeling capabilities.","In particular, a five-equation diffuse-interface model is incorporated which complements the level-set sharp-interface model.","Additional algorithmic improvements include positivity-preserving limiters for increased robustness, support for stretched Cartesian meshes, refactored I/O handling, comprehensive post-processing routines, and an updated list of state-of-the-art high-order numerical discretization schemes.","We verify newly added numerical models by showcasing simulation results for single- and two-phase flows, including turbulent boundary layer and channel flows, air-helium shock bubble interactions, and air-water shock drop interactions."],"url":"http://arxiv.org/abs/2402.05193v1","category":"physics.flu-dyn"}
{"created":"2024-02-07 19:00:01","title":"Towards Understanding Inductive Bias in Transformers: A View From Infinity","abstract":"We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space. We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens. We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs. We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length. Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry.","sentences":["We study inductive bias in Transformers in the infinitely over-parameterized Gaussian process limit and argue transformers tend to be biased towards more permutation symmetric functions in sequence space.","We show that the representation theory of the symmetric group can be used to give quantitative analytical predictions when the dataset is symmetric to permutations between tokens.","We present a simplified transformer block and solve the model at the limit, including accurate predictions for the learning curves and network outputs.","We show that in common setups, one can derive tight bounds in the form of a scaling law for the learnability as a function of the context length.","Finally, we argue WikiText dataset, does indeed possess a degree of permutation symmetry."],"url":"http://arxiv.org/abs/2402.05173v1","category":"cs.LG"}
{"created":"2024-02-07 16:14:04","title":"Non-convergence to global minimizers for Adam and stochastic gradient descent optimization and constructions of local minimizers in the training of artificial neural networks","abstract":"Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs). Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs. In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem. Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers. In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such ANNs that SGD methods (such as the plain vanilla SGD, the momentum SGD, the AdaGrad, the RMSprop, and the Adam optimizers) can find a global minimizer with high probability. Even stronger, we reveal in the training of such ANNs that SGD methods do with high probability fail to converge to global minimizers in the optimization landscape. The findings of this work do, however, not disprove that SGD methods succeed to train ANNs since they do not exclude the possibility that SGD methods find good local minimizers whose risk values are close to the risk values of the global minimizers. In this context, another key contribution of this work is to establish the existence of a hierarchical structure of local minimizers with distinct risk values in the optimization landscape of ANN training problems with ReLU and related activations.","sentences":["Stochastic gradient descent (SGD) optimization methods such as the plain vanilla SGD method and the popular Adam optimizer are nowadays the method of choice in the training of artificial neural networks (ANNs).","Despite the remarkable success of SGD methods in the ANN training in numerical simulations, it remains in essentially all practical relevant scenarios an open problem to rigorously explain why SGD methods seem to succeed to train ANNs.","In particular, in most practically relevant supervised learning problems, it seems that SGD methods do with high probability not converge to global minimizers in the optimization landscape of the ANN training problem.","Nevertheless, it remains an open problem of research to disprove the convergence of SGD methods to global minimizers.","In this work we solve this research problem in the situation of shallow ANNs with the rectified linear unit (ReLU) and related activations with the standard mean square error loss by disproving in the training of such ANNs that SGD methods (such as the plain vanilla SGD, the momentum SGD, the AdaGrad, the RMSprop, and the Adam optimizers) can find a global minimizer with high probability.","Even stronger, we reveal in the training of such ANNs that SGD methods do with high probability fail to converge to global minimizers in the optimization landscape.","The findings of this work do, however, not disprove that SGD methods succeed to train ANNs since they do not exclude the possibility that SGD methods find good local minimizers whose risk values are close to the risk values of the global minimizers.","In this context, another key contribution of this work is to establish the existence of a hierarchical structure of local minimizers with distinct risk values in the optimization landscape of ANN training problems with ReLU and related activations."],"url":"http://arxiv.org/abs/2402.05155v1","category":"math.OC"}
{"created":"2024-02-08 18:56:40","title":"On the Convergence of Zeroth-Order Federated Tuning in Large Language Models","abstract":"The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing. However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources. To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO. Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies. Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also significantly reduces GPU memory usage during training to levels comparable to those during inference. Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction. We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs and facilitate further development and research.","sentences":["The confluence of Federated Learning (FL) and Large Language Models (LLMs) is ushering in a new era in privacy-preserving natural language processing.","However, the intensive memory requirements for fine-tuning LLMs pose significant challenges, especially when deploying on edge devices with limited computational resources.","To circumvent this, we explore the novel integration of Memory-efficient Zeroth-Order Optimization within a federated setting, a synergy we denote as FedMeZO.","Our study is the first to examine the theoretical underpinnings of FedMeZO in the context of LLMs, tackling key questions regarding the influence of large parameter spaces on optimization behavior, the establishment of convergence properties, and the identification of critical parameters for convergence to inform personalized federated strategies.","Our extensive empirical evidence supports the theory, showing that FedMeZO not only converges faster than traditional first-order methods such as SGD but also significantly reduces GPU memory usage during training to levels comparable to those during inference.","Moreover, the proposed personalized FL strategy that is built upon the theoretical insights to customize the client-wise learning rate can effectively accelerate loss reduction.","We hope our work can help to bridge theoretical and practical aspects of federated fine-tuning for LLMs and facilitate further development and research."],"url":"http://arxiv.org/abs/2402.05926v1","category":"cs.LG"}
{"created":"2024-02-08 18:13:26","title":"Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits","abstract":"We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios.","sentences":["We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits.","We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment.","We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI.","Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods.","We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings.","Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios."],"url":"http://arxiv.org/abs/2402.05878v1","category":"stat.ML"}
{"created":"2024-02-08 16:55:21","title":"Integrating Self-supervised Speech Model with Pseudo Word-level Targets from Visually-grounded Speech Model","abstract":"Recent advances in self-supervised speech models have shown significant improvement in many downstream tasks. However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension. Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting. To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data. Our experimental results on four spoken language understanding (SLU) benchmarks suggest the superiority of our model in capturing semantic information.","sentences":["Recent advances in self-supervised speech models have shown significant improvement in many downstream tasks.","However, these models predominantly centered on frame-level training objectives, which can fall short in spoken language understanding tasks that require semantic comprehension.","Existing works often rely on additional speech-text data as intermediate targets, which is costly in the real-world setting.","To address this challenge, we propose Pseudo-Word HuBERT (PW-HuBERT), a framework that integrates pseudo word-level targets into the training process, where the targets are derived from a visually-ground speech model, notably eliminating the need for speech-text paired data.","Our experimental results on four spoken language understanding (SLU) benchmarks suggest the superiority of our model in capturing semantic information."],"url":"http://arxiv.org/abs/2402.05819v1","category":"eess.AS"}
{"created":"2024-02-08 16:41:03","title":"Unsupervised Discovery of Clinical Disease Signatures Using Probabilistic Independence","abstract":"Insufficiently precise diagnosis of clinical disease is likely responsible for many treatment failures, even for common conditions and treatments. With a large enough dataset, it may be possible to use unsupervised machine learning to define clinical disease patterns more precisely. We present an approach to learning these patterns by using probabilistic independence to disentangle the imprint on the medical record of causal latent sources of disease. We inferred a broad set of 2000 clinical signatures of latent sources from 9195 variables in 269,099 Electronic Health Records. The learned signatures produced better discrimination than the original variables in a lung cancer prediction task unknown to the inference algorithm, predicting 3-year malignancy in patients with no history of cancer before a solitary lung nodule was discovered. More importantly, the signatures' greater explanatory power identified pre-nodule signatures of apparently undiagnosed cancer in many of those patients.","sentences":["Insufficiently precise diagnosis of clinical disease is likely responsible for many treatment failures, even for common conditions and treatments.","With a large enough dataset, it may be possible to use unsupervised machine learning to define clinical disease patterns more precisely.","We present an approach to learning these patterns by using probabilistic independence to disentangle the imprint on the medical record of causal latent sources of disease.","We inferred a broad set of 2000 clinical signatures of latent sources from 9195 variables in 269,099 Electronic Health Records.","The learned signatures produced better discrimination than the original variables in a lung cancer prediction task unknown to the inference algorithm, predicting 3-year malignancy in patients with no history of cancer before a solitary lung nodule was discovered.","More importantly, the signatures' greater explanatory power identified pre-nodule signatures of apparently undiagnosed cancer in many of those patients."],"url":"http://arxiv.org/abs/2402.05802v1","category":"cs.LG"}
{"created":"2024-02-08 15:51:50","title":"Off-policy Distributional Q($\u03bb$): Distributional RL without Importance Sampling","abstract":"We introduce off-policy distributional Q($\\lambda$), a new addition to the family of off-policy distributional evaluation algorithms. Off-policy distributional Q($\\lambda$) does not apply importance sampling for off-policy learning, which introduces intriguing interactions with signed measures. Such unique properties distributional Q($\\lambda$) from other existing alternatives such as distributional Retrace. We characterize the algorithmic properties of distributional Q($\\lambda$) and validate theoretical insights with tabular experiments. We show how distributional Q($\\lambda$)-C51, a combination of Q($\\lambda$) with the C51 agent, exhibits promising results on deep RL benchmarks.","sentences":["We introduce off-policy distributional Q($\\lambda$), a new addition to the family of off-policy distributional evaluation algorithms.","Off-policy distributional Q($\\lambda$) does not apply importance sampling for off-policy learning, which introduces intriguing interactions with signed measures.","Such unique properties distributional Q($\\lambda$) from other existing alternatives such as distributional Retrace.","We characterize the algorithmic properties of distributional Q($\\lambda$) and validate theoretical insights with tabular experiments.","We show how distributional Q($\\lambda$)-C51, a combination of Q($\\lambda$) with the C51 agent, exhibits promising results on deep RL benchmarks."],"url":"http://arxiv.org/abs/2402.05766v1","category":"cs.LG"}
{"created":"2024-02-08 15:39:32","title":"SpiRit-LM: Interleaved Spoken and Written Language Model","abstract":"We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech. Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units. Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus. SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units. For both versions, the text is encoded with subword BPE tokens. The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models. Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification).","sentences":["We introduce SPIRIT-LM, a foundation multimodal language model that freely mixes text and speech.","Our model is based on a pretrained text language model that we extend to the speech modality by continuously training it on text and speech units.","Speech and text sequences are concatenated as a single set of tokens, and trained with a word-level interleaving method using a small automatically-curated speech-text parallel corpus.","SPIRIT-LM comes in two versions: a BASE version that uses speech semantic units and an EXPRESSIVE version that models expressivity using pitch and style units in addition to the semantic units.","For both versions, the text is encoded with subword BPE tokens.","The resulting model displays both the semantic abilities of text models and the expressive abilities of speech models.","Additionally, we demonstrate that SPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities (i.e. ASR, TTS, Speech Classification)."],"url":"http://arxiv.org/abs/2402.05755v1","category":"cs.CL"}
{"created":"2024-02-08 14:07:20","title":"Unichain and Aperiodicity are Sufficient for Asymptotic Optimality of Average-Reward Restless Bandits","abstract":"We consider the infinite-horizon, average-reward restless bandit problem in discrete time. We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution. We show that our policies are asymptotically optimal with an $O(1/\\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic. Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA).","sentences":["We consider the infinite-horizon, average-reward restless bandit problem in discrete time.","We propose a new class of policies that are designed to drive a progressively larger subset of arms toward the optimal distribution.","We show that our policies are asymptotically optimal with an $O(1/\\sqrt{N})$ optimality gap for an $N$-armed problem, provided that the single-armed relaxed problem is unichain and aperiodic.","Our approach departs from most existing work that focuses on index or priority policies, which rely on the Uniform Global Attractor Property (UGAP) to guarantee convergence to the optimum, or a recently developed simulation-based policy, which requires a Synchronization Assumption (SA)."],"url":"http://arxiv.org/abs/2402.05689v1","category":"cs.LG"}
{"created":"2024-02-08 12:50:38","title":"Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients","abstract":"This paper proposes SAGD-IV, a novel framework for conducting nonparametric instrumental variable (NPIV) regression by employing stochastic approximate gradients to minimize the projected populational risk. Instrumental Variables (IVs) are widely used in econometrics to address estimation problems in the presence of unobservable confounders, and the Machine Learning community has devoted significant effort to improving existing methods and devising new ones in the NPIV setting, which is known to be an ill-posed linear inverse problem. We provide theoretical support for our algorithm and further exemplify its competitive performance through empirical experiments. Furthermore, we address, with promising results, the case of binary outcomes, which has not received as much attention from the community as its continuous counterpart.","sentences":["This paper proposes SAGD-IV, a novel framework for conducting nonparametric instrumental variable (NPIV) regression by employing stochastic approximate gradients to minimize the projected populational risk.","Instrumental Variables (IVs) are widely used in econometrics to address estimation problems in the presence of unobservable confounders, and the Machine Learning community has devoted significant effort to improving existing methods and devising new ones in the NPIV setting, which is known to be an ill-posed linear inverse problem.","We provide theoretical support for our algorithm and further exemplify its competitive performance through empirical experiments.","Furthermore, we address, with promising results, the case of binary outcomes, which has not received as much attention from the community as its continuous counterpart."],"url":"http://arxiv.org/abs/2402.05639v1","category":"stat.ML"}
{"created":"2024-02-08 12:49:46","title":"Learning pseudo-contractive denoisers for inverse problems","abstract":"Deep denoisers have shown excellent performance in solving inverse problems in signal and image processing. In order to guarantee the convergence, the denoiser needs to satisfy some Lipschitz conditions like non-expansiveness. However, enforcing such constraints inevitably compromises recovery performance. This paper introduces a novel training strategy that enforces a weaker constraint on the deep denoiser called pseudo-contractiveness. By studying the spectrum of the Jacobian matrix, relationships between different denoiser assumptions are revealed. Effective algorithms based on gradient descent and Ishikawa process are derived, and further assumptions of strict pseudo-contractiveness yield efficient algorithms using half-quadratic splitting and forward-backward splitting. The proposed algorithms theoretically converge strongly to a fixed point. A training strategy based on holomorphic transformation and functional calculi is proposed to enforce the pseudo-contractive denoiser assumption. Extensive experiments demonstrate superior performance of the pseudo-contractive denoiser compared to related denoisers. The proposed methods are competitive in terms of visual effects and quantitative values.","sentences":["Deep denoisers have shown excellent performance in solving inverse problems in signal and image processing.","In order to guarantee the convergence, the denoiser needs to satisfy some Lipschitz conditions like non-expansiveness.","However, enforcing such constraints inevitably compromises recovery performance.","This paper introduces a novel training strategy that enforces a weaker constraint on the deep denoiser called pseudo-contractiveness.","By studying the spectrum of the Jacobian matrix, relationships between different denoiser assumptions are revealed.","Effective algorithms based on gradient descent and Ishikawa process are derived, and further assumptions of strict pseudo-contractiveness yield efficient algorithms using half-quadratic splitting and forward-backward splitting.","The proposed algorithms theoretically converge strongly to a fixed point.","A training strategy based on holomorphic transformation and functional calculi is proposed to enforce the pseudo-contractive denoiser assumption.","Extensive experiments demonstrate superior performance of the pseudo-contractive denoiser compared to related denoisers.","The proposed methods are competitive in terms of visual effects and quantitative values."],"url":"http://arxiv.org/abs/2402.05637v1","category":"cs.CV"}
{"created":"2024-02-08 12:20:28","title":"Deep Learning-based Computational Job Market Analysis: A Survey on Skill Extraction and Classification from Job Postings","abstract":"Recent years have brought significant advances to Natural Language Processing (NLP), which enabled fast progress in the field of computational job market analysis. Core tasks in this application domain are skill extraction and classification from job postings. Because of its quick growth and its interdisciplinary nature, there is no exhaustive assessment of this emerging field. This survey aims to fill this gap by providing a comprehensive overview of deep learning methodologies, datasets, and terminologies specific to NLP-driven skill extraction and classification. Our comprehensive cataloging of publicly available datasets addresses the lack of consolidated information on dataset creation and characteristics. Finally, the focus on terminology addresses the current lack of consistent definitions for important concepts, such as hard and soft skills, and terms relating to skill extraction and classification.","sentences":["Recent years have brought significant advances to Natural Language Processing (NLP), which enabled fast progress in the field of computational job market analysis.","Core tasks in this application domain are skill extraction and classification from job postings.","Because of its quick growth and its interdisciplinary nature, there is no exhaustive assessment of this emerging field.","This survey aims to fill this gap by providing a comprehensive overview of deep learning methodologies, datasets, and terminologies specific to NLP-driven skill extraction and classification.","Our comprehensive cataloging of publicly available datasets addresses the lack of consolidated information on dataset creation and characteristics.","Finally, the focus on terminology addresses the current lack of consistent definitions for important concepts, such as hard and soft skills, and terms relating to skill extraction and classification."],"url":"http://arxiv.org/abs/2402.05617v1","category":"cs.CL"}
{"created":"2024-02-08 11:38:30","title":"Quark-hadron duality and the determination of $\u03b1_s$ from hadronic $\u03c4$ decay: facts vs. myths","abstract":"Non-perturbative effects have a small but non-trivial impact on the determination of the strong coupling from hadronic $\\tau$ decay data. Several approaches have been proposed to take these into account, the two most important of which are the ``truncated OPE'' approach and ``DV-model'' approach. Recently, Pich and Rodr\\'iguez-S\\'anchez have raised a number of criticisms of the latter approach, including, most notably, claims of the existence of (i) a supposed instability with respect to variations of the model for incorporating quark-hadron duality violations, and (ii) an alleged redundancy in the fitting strategy employed in the DV-model approach. In this paper, we address these criticisms one by one, showing they fail to survive more detailed scrutiny of the mathematical or numerical arguments that underpin them. We also show that, while the redundancy claim does not apply to the DV-model approach, it does, in fact, apply to the truncated OPE approach. This leads to the conclusion that a revision of the conventional understanding of what is learned from truncated OPE analyses is necessary and that only very limited self-consistency checks are possible within this framework. These observations raise new, non-trivial issues for the truncated OPE approach.","sentences":["Non-perturbative effects have a small but non-trivial impact on the determination of the strong coupling from hadronic $\\tau$ decay data.","Several approaches have been proposed to take these into account, the two most important of which are the ``truncated OPE'' approach and ``DV-model'' approach.","Recently, Pich and Rodr\\'iguez-S\\'anchez have raised a number of criticisms of the latter approach, including, most notably, claims of the existence of (i) a supposed instability with respect to variations of the model for incorporating quark-hadron duality violations, and (ii) an alleged redundancy in the fitting strategy employed in the DV-model approach.","In this paper, we address these criticisms one by one, showing they fail to survive more detailed scrutiny of the mathematical or numerical arguments that underpin them.","We also show that, while the redundancy claim does not apply to the DV-model approach, it does, in fact, apply to the truncated OPE approach.","This leads to the conclusion that a revision of the conventional understanding of what is learned from truncated OPE analyses is necessary and that only very limited self-consistency checks are possible within this framework.","These observations raise new, non-trivial issues for the truncated OPE approach."],"url":"http://arxiv.org/abs/2402.05588v1","category":"hep-ph"}
{"created":"2024-02-08 10:22:45","title":"Machine learning applied to omics data","abstract":"In this chapter we illustrate the use of some Machine Learning techniques in the context of omics data. More precisely, we review and evaluate the use of Random Forest and Penalized Multinomial Logistic Regression for integrative analysis of genomics and immunomics in pancreatic cancer. Furthermore, we propose the use of association rules with predictive purposes to overcome the low predictive power of the previously mentioned models. Finally, we apply the reviewed methods to a real data set from TCGA made of 107 tumoral pancreatic samples and 117,486 germline SNPs, showing the good performance of the proposed methods to predict the immunological infiltration in pancreatic cancer.","sentences":["In this chapter we illustrate the use of some Machine Learning techniques in the context of omics data.","More precisely, we review and evaluate the use of Random Forest and Penalized Multinomial Logistic Regression for integrative analysis of genomics and immunomics in pancreatic cancer.","Furthermore, we propose the use of association rules with predictive purposes to overcome the low predictive power of the previously mentioned models.","Finally, we apply the reviewed methods to a real data set from TCGA made of 107 tumoral pancreatic samples and 117,486 germline SNPs, showing the good performance of the proposed methods to predict the immunological infiltration in pancreatic cancer."],"url":"http://arxiv.org/abs/2402.05543v1","category":"q-bio.GN"}
{"created":"2024-02-08 10:05:28","title":"Buffer Overflow in Mixture of Experts","abstract":"Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady. We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks. Malicious queries can be sent to a model and can affect a model's output on other benign queries if they are grouped in the same batch. We demonstrate this via a proof-of-concept attack in a toy experimental setting.","sentences":["Mixture of Experts (MoE) has become a key ingredient for scaling large foundation models while keeping inference costs steady.","We show that expert routing strategies that have cross-batch dependencies are vulnerable to attacks.","Malicious queries can be sent to a model and can affect a model's output on other benign queries if they are grouped in the same batch.","We demonstrate this via a proof-of-concept attack in a toy experimental setting."],"url":"http://arxiv.org/abs/2402.05526v1","category":"cs.CR"}
{"created":"2024-02-08 05:06:14","title":"SpirDet: Towards Efficient, Accurate and Lightweight Infrared Small Target Detector","abstract":"In recent years, the detection of infrared small targets using deep learning methods has garnered substantial attention due to notable advancements. To improve the detection capability of small targets, these methods commonly maintain a pathway that preserves high-resolution features of sparse and tiny targets. However, it can result in redundant and expensive computations. To tackle this challenge, we propose SpirDet, a novel approach for efficient detection of infrared small targets. Specifically, to cope with the computational redundancy issue, we employ a new dual-branch sparse decoder to restore the feature map. Firstly, the fast branch directly predicts a sparse map indicating potential small target locations (occupying only 0.5\\% area of the map). Secondly, the slow branch conducts fine-grained adjustments at the positions indicated by the sparse map. Additionally, we design an lightweight DO-RepEncoder based on reparameterization with the Downsampling Orthogonality, which can effectively reduce memory consumption and inference latency. Extensive experiments show that the proposed SpirDet significantly outperforms state-of-the-art models while achieving faster inference speed and fewer parameters. For example, on the IRSTD-1K dataset, SpirDet improves $MIoU$ by 4.7 and has a $7\\times$ $FPS$ acceleration compared to the previous state-of-the-art model. The code will be open to the public.","sentences":["In recent years, the detection of infrared small targets using deep learning methods has garnered substantial attention due to notable advancements.","To improve the detection capability of small targets, these methods commonly maintain a pathway that preserves high-resolution features of sparse and tiny targets.","However, it can result in redundant and expensive computations.","To tackle this challenge, we propose SpirDet, a novel approach for efficient detection of infrared small targets.","Specifically, to cope with the computational redundancy issue, we employ a new dual-branch sparse decoder to restore the feature map.","Firstly, the fast branch directly predicts a sparse map indicating potential small target locations (occupying only 0.5\\% area of the map).","Secondly, the slow branch conducts fine-grained adjustments at the positions indicated by the sparse map.","Additionally, we design an lightweight DO-RepEncoder based on reparameterization with the Downsampling Orthogonality, which can effectively reduce memory consumption and inference latency.","Extensive experiments show that the proposed SpirDet significantly outperforms state-of-the-art models while achieving faster inference speed and fewer parameters.","For example, on the IRSTD-1K dataset, SpirDet improves $MIoU$ by 4.7 and has a $7\\times$ $FPS$ acceleration compared to the previous state-of-the-art model.","The code will be open to the public."],"url":"http://arxiv.org/abs/2402.05410v1","category":"cs.CV"}
{"created":"2024-02-08 04:07:38","title":"Enhancing Zero-shot Counting via Language-guided Exemplar Learning","abstract":"Recently, Class-Agnostic Counting (CAC) problem has garnered increasing attention owing to its intriguing generality and superior efficiency compared to Category-Specific Counting (CSC). This paper proposes a novel ExpressCount to enhance zero-shot object counting by delving deeply into language-guided exemplar learning. Specifically, the ExpressCount is comprised of an innovative Language-oriented Exemplar Perceptron and a downstream visual Zero-shot Counting pipeline. Thereinto, the perceptron hammers at exploiting accurate exemplar cues from collaborative language-vision signals by inheriting rich semantic priors from the prevailing pre-trained Large Language Models (LLMs), whereas the counting pipeline excels in mining fine-grained features through dual-branch and cross-attention schemes, contributing to the high-quality similarity learning. Apart from building a bridge between the LLM in vogue and the visual counting tasks, expression-guided exemplar estimation significantly advances zero-shot learning capabilities for counting instances with arbitrary classes. Moreover, devising a FSC-147-Express with annotations of meticulous linguistic expressions pioneers a new venue for developing and validating language-based counting models. Extensive experiments demonstrate the state-of-the-art performance of our ExpressCount, even showcasing the accuracy on par with partial CSC models.","sentences":["Recently, Class-Agnostic Counting (CAC) problem has garnered increasing attention owing to its intriguing generality and superior efficiency compared to Category-Specific Counting (CSC).","This paper proposes a novel ExpressCount to enhance zero-shot object counting by delving deeply into language-guided exemplar learning.","Specifically, the ExpressCount is comprised of an innovative Language-oriented Exemplar Perceptron and a downstream visual Zero-shot Counting pipeline.","Thereinto, the perceptron hammers at exploiting accurate exemplar cues from collaborative language-vision signals by inheriting rich semantic priors from the prevailing pre-trained Large Language Models (LLMs), whereas the counting pipeline excels in mining fine-grained features through dual-branch and cross-attention schemes, contributing to the high-quality similarity learning.","Apart from building a bridge between the LLM in vogue and the visual counting tasks, expression-guided exemplar estimation significantly advances zero-shot learning capabilities for counting instances with arbitrary classes.","Moreover, devising a FSC-147-Express with annotations of meticulous linguistic expressions pioneers a new venue for developing and validating language-based counting models.","Extensive experiments demonstrate the state-of-the-art performance of our ExpressCount, even showcasing the accuracy on par with partial CSC models."],"url":"http://arxiv.org/abs/2402.05394v1","category":"cs.CV"}
{"created":"2024-02-08 03:01:51","title":"Learning to Control Emulated Muscles in Real Robots: Towards Exploiting Bio-Inspired Actuator Morphology","abstract":"Recent studies have demonstrated the immense potential of exploiting muscle actuator morphology for natural and robust movement -- in simulation. A validation on real robotic hardware is yet missing. In this study, we emulate muscle actuator properties on hardware in real-time, taking advantage of modern and affordable electric motors. We demonstrate that our setup can emulate a simplified muscle model on a real robot while being controlled by a learned policy. We improve upon an existing muscle model by deriving a damping rule that ensures that the model is not only performant and stable but also tuneable for the real hardware. Our policies are trained by reinforcement learning entirely in simulation, where we show that previously reported benefits of muscles extend to the case of quadruped locomotion and hopping: the learned policies are more robust and exhibit more regular gaits. Finally, we confirm that the learned policies can be executed on real hardware and show that sim-to-real transfer with real-time emulated muscles on a quadruped robot is possible. These results show that artificial muscles can be highly beneficial actuators for future generations of robust legged robots.","sentences":["Recent studies have demonstrated the immense potential of exploiting muscle actuator morphology for natural and robust movement -- in simulation.","A validation on real robotic hardware is yet missing.","In this study, we emulate muscle actuator properties on hardware in real-time, taking advantage of modern and affordable electric motors.","We demonstrate that our setup can emulate a simplified muscle model on a real robot while being controlled by a learned policy.","We improve upon an existing muscle model by deriving a damping rule that ensures that the model is not only performant and stable but also tuneable for the real hardware.","Our policies are trained by reinforcement learning entirely in simulation, where we show that previously reported benefits of muscles extend to the case of quadruped locomotion and hopping: the learned policies are more robust and exhibit more regular gaits.","Finally, we confirm that the learned policies can be executed on real hardware and show that sim-to-real transfer with real-time emulated muscles on a quadruped robot is possible.","These results show that artificial muscles can be highly beneficial actuators for future generations of robust legged robots."],"url":"http://arxiv.org/abs/2402.05371v1","category":"cs.RO"}
{"created":"2024-02-08 02:57:47","title":"Principled Preferential Bayesian Optimization","abstract":"We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions. Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback. An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the cumulative regret, a first-of-its-kind for preferential BO. This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate. Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence.","sentences":["We study the problem of preferential Bayesian optimization (BO), where we aim to optimize a black-box function with only preference feedback over a pair of candidate solutions.","Inspired by the likelihood ratio idea, we construct a confidence set of the black-box function using only the preference feedback.","An optimistic algorithm with an efficient computational method is then developed to solve the problem, which enjoys an information-theoretic bound on the cumulative regret, a first-of-its-kind for preferential BO.","This bound further allows us to design a scheme to report an estimated best solution, with a guaranteed convergence rate.","Experimental results on sampled instances from Gaussian processes, standard test functions, and a thermal comfort optimization problem all show that our method stably achieves better or competitive performance as compared to the existing state-of-the-art heuristics, which, however, do not have theoretical guarantees on regret bounds or convergence."],"url":"http://arxiv.org/abs/2402.05367v1","category":"cs.LG"}
{"created":"2024-02-08 00:44:45","title":"Investigating the Impact of SOLID Design Principles on Machine Learning Code Understanding","abstract":"[Context] Applying design principles has long been acknowledged as beneficial for understanding and maintainability in traditional software projects. These benefits may similarly hold for Machine Learning (ML) projects, which involve iterative experimentation with data, models, and algorithms. However, ML components are often developed by data scientists with diverse educational backgrounds, potentially resulting in code that doesn't adhere to software design best practices. [Goal] In order to better understand this phenomenon, we investigated the impact of the SOLID design principles on ML code understanding. [Method] We conducted a controlled experiment with three independent trials involving 100 data scientists. We restructured real industrial ML code that did not use SOLID principles. Within each trial, one group was presented with the original ML code, while the other was presented with ML code incorporating SOLID principles. Participants of both groups were asked to analyze the code and fill out a questionnaire that included both open-ended and closed-ended questions on their understanding. [Results] The study results provide statistically significant evidence that the adoption of the SOLID design principles can improve code understanding within the realm of ML projects. [Conclusion] We put forward that software engineering design principles should be spread within the data science community and considered for enhancing the maintainability of ML code.","sentences":["[Context] Applying design principles has long been acknowledged as beneficial for understanding and maintainability in traditional software projects.","These benefits may similarly hold for Machine Learning (ML) projects, which involve iterative experimentation with data, models, and algorithms.","However, ML components are often developed by data scientists with diverse educational backgrounds, potentially resulting in code that doesn't adhere to software design best practices.","[Goal] In order to better understand this phenomenon, we investigated the impact of the SOLID design principles on ML code understanding.","[Method] We conducted a controlled experiment with three independent trials involving 100 data scientists.","We restructured real industrial ML code that did not use SOLID principles.","Within each trial, one group was presented with the original ML code, while the other was presented with ML code incorporating SOLID principles.","Participants of both groups were asked to analyze the code and fill out a questionnaire that included both open-ended and closed-ended questions on their understanding.","[Results]","The study results provide statistically significant evidence that the adoption of the SOLID design principles can improve code understanding within the realm of ML projects.","[Conclusion] We put forward that software engineering design principles should be spread within the data science community and considered for enhancing the maintainability of ML code."],"url":"http://arxiv.org/abs/2402.05337v1","category":"cs.SE"}
{"created":"2024-02-08 00:27:56","title":"On the Interaction between Software Engineers and Data Scientists when building Machine Learning-Enabled Systems","abstract":"In recent years, Machine Learning (ML) components have been increasingly integrated into the core systems of organizations. Engineering such systems presents various challenges from both a theoretical and practical perspective. One of the key challenges is the effective interaction between actors with different backgrounds who need to work closely together, such as software engineers and data scientists. This paper presents an exploratory case study to understand the current interaction and collaboration dynamics between these roles in ML projects. We conducted semi-structured interviews with four practitioners with experience in software engineering and data science of a large ML-enabled system project and analyzed the data using reflexive thematic analysis. Our findings reveal several challenges that can hinder collaboration between software engineers and data scientists, including differences in technical expertise, unclear definitions of each role's duties, and the lack of documents that support the specification of the ML-enabled system. We also indicate potential solutions to address these challenges, such as fostering a collaborative culture, encouraging team communication, and producing concise system documentation. This study contributes to understanding the complex dynamics between software engineers and data scientists in ML projects and provides insights for improving collaboration and communication in this context. We encourage future studies investigating this interaction in other projects.","sentences":["In recent years, Machine Learning (ML) components have been increasingly integrated into the core systems of organizations.","Engineering such systems presents various challenges from both a theoretical and practical perspective.","One of the key challenges is the effective interaction between actors with different backgrounds who need to work closely together, such as software engineers and data scientists.","This paper presents an exploratory case study to understand the current interaction and collaboration dynamics between these roles in ML projects.","We conducted semi-structured interviews with four practitioners with experience in software engineering and data science of a large ML-enabled system project and analyzed the data using reflexive thematic analysis.","Our findings reveal several challenges that can hinder collaboration between software engineers and data scientists, including differences in technical expertise, unclear definitions of each role's duties, and the lack of documents that support the specification of the ML-enabled system.","We also indicate potential solutions to address these challenges, such as fostering a collaborative culture, encouraging team communication, and producing concise system documentation.","This study contributes to understanding the complex dynamics between software engineers and data scientists in ML projects and provides insights for improving collaboration and communication in this context.","We encourage future studies investigating this interaction in other projects."],"url":"http://arxiv.org/abs/2402.05334v1","category":"cs.SE"}
{"created":"2024-02-07 23:39:40","title":"Navigating the Knowledge Sea: Planet-scale answer retrieval using LLMs","abstract":"Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines. This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval. The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems. This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries. Through this exploration, we seek to illuminate the technological milestones that have shaped this journey and the potential future directions in this rapidly changing field.","sentences":["Information retrieval is a rapidly evolving field of information retrieval, which is characterized by a continuous refinement of techniques and technologies, from basic hyperlink-based navigation to sophisticated algorithm-driven search engines.","This paper aims to provide a comprehensive overview of the evolution of Information Retrieval Technology, with a particular focus on the role of Large Language Models (LLMs) in bridging the gap between traditional search methods and the emerging paradigm of answer retrieval.","The integration of LLMs in the realms of response retrieval and indexing signifies a paradigm shift in how users interact with information systems.","This paradigm shift is driven by the integration of large language models (LLMs) like GPT-4, which are capable of understanding and generating human-like text, thus enabling them to provide more direct and contextually relevant answers to user queries.","Through this exploration, we seek to illuminate the technological milestones that have shaped this journey and the potential future directions in this rapidly changing field."],"url":"http://arxiv.org/abs/2402.05318v1","category":"cs.IR"}
{"created":"2024-02-07 23:39:20","title":"Emerging Results on Automated Support for Searching and Selecting Evidence for Systematic Literature Review Updates","abstract":"Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates. However, searching and selecting evidence for SLR updates demands significant effort from SE researchers. Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE. Method: We developed an automated tool prototype to perform the snowballing search technique and support selecting relevant studies for SLR updates using Machine Learning (ML) algorithms. We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update. Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI). The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine, with approximately 74% recall and 15% precision. Using such algorithms with conservative thresholds to minimize the risk of missing papers can significantly reduce evidence selection efforts. Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE.","sentences":["Context: The constant growth of primary evidence and Systematic Literature Reviews (SLRs) publications in the Software Engineering (SE) field leads to the need for SLR Updates.","However, searching and selecting evidence for SLR updates demands significant effort from SE researchers.","Objective: We present emerging results on an automated approach to support searching and selecting studies for SLR updates in SE.","Method: We developed an automated tool prototype to perform the snowballing search technique and support selecting relevant studies for SLR updates using Machine Learning (ML) algorithms.","We evaluated our automation proposition through a small-scale evaluation with a reliable dataset from an SLR replication and its update.","Results: Effectively automating snowballing-based search strategies showed feasibility with minor losses, specifically related to papers without Digital Object Identifier (DOI).","The ML algorithm giving the highest performance to select studies for SLR updates was Linear Support Vector Machine, with approximately 74% recall and 15% precision.","Using such algorithms with conservative thresholds to minimize the risk of missing papers can significantly reduce evidence selection efforts.","Conclusion: The preliminary results of our evaluation point in promising directions, indicating the potential of automating snowballing search efforts and of reducing the number of papers to be manually analyzed by about 2.5 times when selecting evidence for updating SLRs in SE."],"url":"http://arxiv.org/abs/2402.05317v1","category":"cs.SE"}
{"created":"2024-02-07 23:05:30","title":"Dual-disentangled Deep Multiple Clustering","abstract":"Multiple clustering has gathered significant attention in recent years due to its potential to reveal multiple hidden structures of the data from different perspectives. Most of multiple clustering methods first derive feature representations by controlling the dissimilarity among them, subsequently employing traditional clustering methods (e.g., k-means) to achieve the final multiple clustering outcomes. However, the learned feature representations can exhibit a weak relevance to the ultimate goal of distinct clustering. Moreover, these features are often not explicitly learned for the purpose of clustering. Therefore, in this paper, we propose a novel Dual-Disentangled deep Multiple Clustering method named DDMC by learning disentangled representations. Specifically, DDMC is achieved by a variational Expectation-Maximization (EM) framework. In the E-step, the disentanglement learning module employs coarse-grained and fine-grained disentangled representations to obtain a more diverse set of latent factors from the data. In the M-step, the cluster assignment module utilizes a cluster objective function to augment the effectiveness of the cluster output. Our extensive experiments demonstrate that DDMC consistently outperforms state-of-the-art methods across seven commonly used tasks. Our code is available at https://github.com/Alexander-Yao/DDMC.","sentences":["Multiple clustering has gathered significant attention in recent years due to its potential to reveal multiple hidden structures of the data from different perspectives.","Most of multiple clustering methods first derive feature representations by controlling the dissimilarity among them, subsequently employing traditional clustering methods (e.g., k-means) to achieve the final multiple clustering outcomes.","However, the learned feature representations can exhibit a weak relevance to the ultimate goal of distinct clustering.","Moreover, these features are often not explicitly learned for the purpose of clustering.","Therefore, in this paper, we propose a novel Dual-Disentangled deep Multiple Clustering method named DDMC by learning disentangled representations.","Specifically, DDMC is achieved by a variational Expectation-Maximization (EM) framework.","In the E-step, the disentanglement learning module employs coarse-grained and fine-grained disentangled representations to obtain a more diverse set of latent factors from the data.","In the M-step, the cluster assignment module utilizes a cluster objective function to augment the effectiveness of the cluster output.","Our extensive experiments demonstrate that DDMC consistently outperforms state-of-the-art methods across seven commonly used tasks.","Our code is available at https://github.com/Alexander-Yao/DDMC."],"url":"http://arxiv.org/abs/2402.05310v1","category":"cs.CV"}
{"created":"2024-02-07 22:50:47","title":"Knowledge Distillation for Road Detection based on cross-model Semi-Supervised Learning","abstract":"The advancement of knowledge distillation has played a crucial role in enabling the transfer of knowledge from larger teacher models to smaller and more efficient student models, and is particularly beneficial for online and resource-constrained applications. The effectiveness of the student model heavily relies on the quality of the distilled knowledge received from the teacher. Given the accessibility of unlabelled remote sensing data, semi-supervised learning has become a prevalent strategy for enhancing model performance. However, relying solely on semi-supervised learning with smaller models may be insufficient due to their limited capacity for feature extraction. This limitation restricts their ability to exploit training data. To address this issue, we propose an integrated approach that combines knowledge distillation and semi-supervised learning methods. This hybrid approach leverages the robust capabilities of large models to effectively utilise large unlabelled data whilst subsequently providing the small student model with rich and informative features for enhancement. The proposed semi-supervised learning-based knowledge distillation (SSLKD) approach demonstrates a notable improvement in the performance of the student model, in the application of road segmentation, surpassing the effectiveness of traditional semi-supervised learning methods.","sentences":["The advancement of knowledge distillation has played a crucial role in enabling the transfer of knowledge from larger teacher models to smaller and more efficient student models, and is particularly beneficial for online and resource-constrained applications.","The effectiveness of the student model heavily relies on the quality of the distilled knowledge received from the teacher.","Given the accessibility of unlabelled remote sensing data, semi-supervised learning has become a prevalent strategy for enhancing model performance.","However, relying solely on semi-supervised learning with smaller models may be insufficient due to their limited capacity for feature extraction.","This limitation restricts their ability to exploit training data.","To address this issue, we propose an integrated approach that combines knowledge distillation and semi-supervised learning methods.","This hybrid approach leverages the robust capabilities of large models to effectively utilise large unlabelled data whilst subsequently providing the small student model with rich and informative features for enhancement.","The proposed semi-supervised learning-based knowledge distillation (SSLKD) approach demonstrates a notable improvement in the performance of the student model, in the application of road segmentation, surpassing the effectiveness of traditional semi-supervised learning methods."],"url":"http://arxiv.org/abs/2402.05305v1","category":"cs.CV"}
{"created":"2024-02-07 21:53:01","title":"No Dimensional Sampling Coresets for Classification","abstract":"We refine and generalize what is known about coresets for classification problems via the sensitivity sampling framework. Such coresets seek the smallest possible subsets of input data, so one can optimize a loss function on the coreset and ensure approximation guarantees with respect to the original data. Our analysis provides the first no dimensional coresets, so the size does not depend on the dimension. Moreover, our results are general, apply for distributional input and can use iid samples, so provide sample complexity bounds, and work for a variety of loss functions. A key tool we develop is a Radamacher complexity version of the main sensitivity sampling approach, which can be of independent interest.","sentences":["We refine and generalize what is known about coresets for classification problems via the sensitivity sampling framework.","Such coresets seek the smallest possible subsets of input data, so one can optimize a loss function on the coreset and ensure approximation guarantees with respect to the original data.","Our analysis provides the first no dimensional coresets, so the size does not depend on the dimension.","Moreover, our results are general, apply for distributional input and can use iid samples, so provide sample complexity bounds, and work for a variety of loss functions.","A key tool we develop is a Radamacher complexity version of the main sensitivity sampling approach, which can be of independent interest."],"url":"http://arxiv.org/abs/2402.05280v1","category":"cs.LG"}
{"created":"2024-02-08 18:24:22","title":"Safeguarding Oscillators and Qudits with Distributed Two-Mode Squeezing","abstract":"Recent advancements in multimode Gottesman-Kitaev-Preskill (GKP) codes have shown great promise in enhancing the protection of both discrete and analog quantum information. This broadened range of protection brings opportunities beyond quantum computing to benefit quantum sensing by safeguarding squeezing -- the essential resource in many quantum metrology protocols. However, it is less explored how quantum sensing can benefit quantum error correction. In this work, we provide a unique example where techniques from quantum sensing can be applied to improve multimode GKP codes. Inspired by distributed quantum sensing, we propose the distributed two-mode squeezing (dtms) GKP codes that offer benefits in error correction with minimal active encoding operations. In fact, the proposed codes rely on a single (active) two-mode squeezing element and an array of beamsplitters that effectively distributes continuous-variable correlations to many GKP ancillae, similar to continuous-variable distributed quantum sensing. Despite this simple construction, the code distance achievable with dtms-GKP qubit codes is comparable to previous results obtained through brute-force numerical search [PRX Quantum 4, 040334 (2023)]. Moreover, these codes enable analog noise suppression beyond that of the best-known two-mode codes [Phys. Rev. Lett. 125, 080503 (2020)] without requiring an additional squeezer. We also provide a simple two-stage decoder for the proposed codes, which appears near-optimal for the case of two modes and permits analytical evaluation.","sentences":["Recent advancements in multimode Gottesman-Kitaev-Preskill (GKP) codes have shown great promise in enhancing the protection of both discrete and analog quantum information.","This broadened range of protection brings opportunities beyond quantum computing to benefit quantum sensing by safeguarding squeezing -- the essential resource in many quantum metrology protocols.","However, it is less explored how quantum sensing can benefit quantum error correction.","In this work, we provide a unique example where techniques from quantum sensing can be applied to improve multimode GKP codes.","Inspired by distributed quantum sensing, we propose the distributed two-mode squeezing (dtms) GKP codes that offer benefits in error correction with minimal active encoding operations.","In fact, the proposed codes rely on a single (active) two-mode squeezing element and an array of beamsplitters that effectively distributes continuous-variable correlations to many GKP ancillae, similar to continuous-variable distributed quantum sensing.","Despite this simple construction, the code distance achievable with dtms-GKP qubit codes is comparable to previous results obtained through brute-force numerical search [PRX Quantum 4, 040334 (2023)].","Moreover, these codes enable analog noise suppression beyond that of the best-known two-mode codes [Phys.","Rev. Lett.","125, 080503 (2020)] without requiring an additional squeezer.","We also provide a simple two-stage decoder for the proposed codes, which appears near-optimal for the case of two modes and permits analytical evaluation."],"url":"http://arxiv.org/abs/2402.05888v1","category":"quant-ph"}
{"created":"2024-02-08 16:39:43","title":"Predicting the photodynamics of cyclobutanone triggered by a laser pulse at 200 nm and its MeV-UED signals -- a trajectory surface hopping and XMS-CASPT2 perspective","abstract":"This work is part of a prediction challenge that invited theoretical/computational chemists to predict the photochemistry of cyclobutanone in the gas phase, excited at 200 nm by a laser pulse, and the expected signal that will be recorded during a time-resolved megaelectronvolt ultrafast electron diffraction (MeV-UED). We present here our theoretical predictions based on a combination of trajectory surface hopping with XMS-CASPT2 (for the nonadiabatic molecular dynamics) and Born-Oppenheimer molecular dynamics (BOMD) with MP2 (for the athermal ground-state dynamics following internal conversion), coined (NA+BO)MD. The initial conditions were sampled from BOMD coupled to a quantum thermostat. Our simulations indicate that the main photoproducts after 2 ps of dynamics are CO + cyclopropane (50%), CO + propene (10%), and ethene and ketene (34%). The photoexcited cyclobutanone in its second excited electronic state S$_2$ can follow two pathways for its nonradiative decay: (i) a ring-opening in S$_2$ and a subsequent rapid decay to the ground electronic state, where the photoproducts are formed, or (ii) a transfer through a closed-ring conical intersection to S$_1$, where cyclobutanone ring opens and then funnels to the ground state. Lifetimes for the photoproduct and electronic populations were determined. We calculated a stationary MeV-UED signal [difference pair distribution function - $\\Delta$PDF$(r)$] for each (interpolated) pathway as well as a time-resolved signal [$\\Delta$PDF$(r,t)$ and $\\Delta I/I(s,t)$] for the full swarm of (NA+BO)MD trajectories. Furthermore, our analysis provides time-independent basis functions that can be used to fit the time-dependent experimental UED signals and potentially recover the population of photoproducts. We also offer a detailed analysis of the limitations of our model and their potential impact on the predicted experimental signals.","sentences":["This work is part of a prediction challenge that invited theoretical/computational chemists to predict the photochemistry of cyclobutanone in the gas phase, excited at 200 nm by a laser pulse, and the expected signal that will be recorded during a time-resolved megaelectronvolt ultrafast electron diffraction (MeV-UED).","We present here our theoretical predictions based on a combination of trajectory surface hopping with XMS-CASPT2 (for the nonadiabatic molecular dynamics) and Born-Oppenheimer molecular dynamics (BOMD) with MP2 (for the athermal ground-state dynamics following internal conversion), coined (NA+BO)MD.","The initial conditions were sampled from BOMD coupled to a quantum thermostat.","Our simulations indicate that the main photoproducts after 2 ps of dynamics are CO + cyclopropane (50%), CO + propene (10%), and ethene and ketene (34%).","The photoexcited cyclobutanone in its second excited electronic state S$_2$ can follow two pathways for its nonradiative decay: (i) a ring-opening in S$_2$ and a subsequent rapid decay to the ground electronic state, where the photoproducts are formed, or (ii) a transfer through a closed-ring conical intersection to S$_1$, where cyclobutanone ring opens and then funnels to the ground state.","Lifetimes for the photoproduct and electronic populations were determined.","We calculated a stationary MeV-UED signal [difference pair distribution function - $\\Delta$PDF$(r)$] for each (interpolated) pathway as well as a time-resolved signal [$\\Delta$PDF$(r,t)$ and $\\Delta I/I(s,t)$] for the full swarm of (NA+BO)MD trajectories.","Furthermore, our analysis provides time-independent basis functions that can be used to fit the time-dependent experimental UED signals and potentially recover the population of photoproducts.","We also offer a detailed analysis of the limitations of our model and their potential impact on the predicted experimental signals."],"url":"http://arxiv.org/abs/2402.05801v1","category":"physics.chem-ph"}
{"created":"2024-02-08 15:33:09","title":"Mixed Integer Linear Programming Solver Using Benders Decomposition Assisted by Neutral Atom Quantum Processor","abstract":"This paper presents a new hybrid classical-quantum approach to solve Mixed Integer Linear Programming (MILP) using neutral atom quantum computations. We apply Benders decomposition (BD) to segment MILPs into a master problem (MP) and a subproblem (SP), where the MP is addressed using a neutral-atom device, after being transformed into a Quadratic Unconstrained Binary Optimization (QUBO) model. To solve the QUBO, we develop a heuristic for atom register embedding and apply Quantum Approximate Optimization Algorithm (QAOA) for pulse shaping. In addition, we implement a Proof of Concept (PoC) that outperforms existing solutions. We also conduct preliminary numerical results: in a series of small MILP instances our algorithm identifies over 95\\% of feasible solutions of high quality, outperforming classical BD approaches where the MP is solved using simulated annealing. To the best of our knowledge, this work is the first to utilize a neutral atom quantum processor in developing an automated, problem-agnostic framework for solving MILPs through BD.","sentences":["This paper presents a new hybrid classical-quantum approach to solve Mixed Integer Linear Programming (MILP) using neutral atom quantum computations.","We apply Benders decomposition (BD) to segment MILPs into a master problem (MP) and a subproblem (SP), where the MP is addressed using a neutral-atom device, after being transformed into a Quadratic Unconstrained Binary Optimization (QUBO) model.","To solve the QUBO, we develop a heuristic for atom register embedding and apply Quantum Approximate Optimization Algorithm (QAOA) for pulse shaping.","In addition, we implement a Proof of Concept (PoC) that outperforms existing solutions.","We also conduct preliminary numerical results: in a series of small MILP instances our algorithm identifies over 95\\% of feasible solutions of high quality, outperforming classical BD approaches where the MP is solved using simulated annealing.","To the best of our knowledge, this work is the first to utilize a neutral atom quantum processor in developing an automated, problem-agnostic framework for solving MILPs through BD."],"url":"http://arxiv.org/abs/2402.05748v1","category":"quant-ph"}
{"created":"2024-02-08 14:24:01","title":"RF Energy Absorption in Human Bodies Due to Wearable Antennas in the 2.4 GHz Frequency Band","abstract":"Human exposure to electromagnetic fields produced by two wearable antennas operating in the 2.4 GHz frequency band was assessed by computational tools. Both antennas were designed to be attached to the skin, but they were intended for different applications. The first antenna was designed for off-body applications, i.e. to communicate with a device placed outside the body, while the second antenna model was optimized to communicate with a device located inside the body. The power absorption in human tissues was determined at several locations of adult male and female body models. The maximum specific absorption rate (SAR) value obtained with the off-body antenna was found on the torso of the woman model and was equal to 0.037 W/kg at 2.45 GHz. SAR levels increased significantly for the antenna transmitting inside the body. In this case, SAR values ranged between 0.23 and 0.45 W/kg at the same body location. The power absorbed in different body tissues and total power absorbed in the body were also calculated; the maximum total power absorbed was equal to 5.2 mW for an antenna input power equal to 10 mW.","sentences":["Human exposure to electromagnetic fields produced by two wearable antennas operating in the 2.4 GHz frequency band was assessed by computational tools.","Both antennas were designed to be attached to the skin, but they were intended for different applications.","The first antenna was designed for off-body applications, i.e. to communicate with a device placed outside the body, while the second antenna model was optimized to communicate with a device located inside the body.","The power absorption in human tissues was determined at several locations of adult male and female body models.","The maximum specific absorption rate (SAR) value obtained with the off-body antenna was found on the torso of the woman model and was equal to 0.037 W/kg at 2.45 GHz.","SAR levels increased significantly for the antenna transmitting inside the body.","In this case, SAR values ranged between 0.23 and 0.45 W/kg at the same body location.","The power absorbed in different body tissues and total power absorbed in the body were also calculated; the maximum total power absorbed was equal to 5.2 mW for an antenna input power equal to 10 mW."],"url":"http://arxiv.org/abs/2402.05700v1","category":"eess.SP"}
{"created":"2024-02-08 13:46:17","title":"$q$-Bass martingales","abstract":"An intriguing question in martingale optimal transport is to characterize the martingale with prescribed initial and terminal marginals whose transition kernel is as Gaussian as possible. In this work we address an extension of this question, in which the role of the Gaussian distribution is replaced by an arbitrary reference measure $q$. Our first main result is a dual formulation of the corresponding martingale optimization problem in terms of convex functions.   In the well-studied case when $q$ is Gaussian, the careful analysis of the solution to the above-mentioned optimization problem is a crucial building block in the construction of Bass martingales, i.e., Brownian martingales induced by gradients of convex functions with possibly non-degenerate starting laws. In our second main result we extend this concept beyond the Gaussian case by introducing the notion of $q$-Bass martingales in discrete time, and give sufficient conditions for their existence.","sentences":["An intriguing question in martingale optimal transport is to characterize the martingale with prescribed initial and terminal marginals whose transition kernel is as Gaussian as possible.","In this work we address an extension of this question, in which the role of the Gaussian distribution is replaced by an arbitrary reference measure $q$. Our first main result is a dual formulation of the corresponding martingale optimization problem in terms of convex functions.   ","In the well-studied case when $q$ is Gaussian, the careful analysis of the solution to the above-mentioned optimization problem is a crucial building block in the construction of Bass martingales, i.e., Brownian martingales induced by gradients of convex functions with possibly non-degenerate starting laws.","In our second main result we extend this concept beyond the Gaussian case by introducing the notion of $q$-Bass martingales in discrete time, and give sufficient conditions for their existence."],"url":"http://arxiv.org/abs/2402.05669v1","category":"math.PR"}
{"created":"2024-02-08 11:27:58","title":"Quantifier Elimination for Normal Cone Computations","abstract":"We present effective procedures to calculate regular normal cones and other related objects using quantifier elimination. This method of normal cone calculations is complementary to computing Lagrangians and it works best at points where the constraint qualifications fail and extra work for other methods becomes inevitable. This method also serves as a tool to calculate the regular co-derivative for semismooth* Newton methods. We list algorithms and their demonstrations of different use cases for this approach.","sentences":["We present effective procedures to calculate regular normal cones and other related objects using quantifier elimination.","This method of normal cone calculations is complementary to computing Lagrangians and it works best at points where the constraint qualifications fail and extra work for other methods becomes inevitable.","This method also serves as a tool to calculate the regular co-derivative for semismooth* Newton methods.","We list algorithms and their demonstrations of different use cases for this approach."],"url":"http://arxiv.org/abs/2402.05579v1","category":"math.OC"}
{"created":"2024-02-08 10:18:10","title":"Inertial active harmonic particle with memory escape induced by viscoelastic suspension","abstract":"We investigate the self-propulsion of an inertial active particle confined in a two-dimensional harmonic trap. The particle is suspended in a non-Newtonian or viscoelastic suspension with a friction kernel that decays exponentially with a time constant characterizing the memory timescale or transient elasticity of the medium. By solving the associated non-Markovian dynamics, we identify two regimes in parameter space distinguishing the oscillatory and non-oscillatory behavior of the particle motion. By simulating the particle trajectories and exactly calculating the steady state probability distribution functions and mean square displacement, interestingly, we observe that with an increase in the memory time scale, the elastic bound of suspension dominates over the influence of harmonic trap. As a consequence, the particle can escape out of the trap without approaching steady state. On the other hand, with an increase in the duration of the activity, the particle becomes trapped by the harmonic confinement.","sentences":["We investigate the self-propulsion of an inertial active particle confined in a two-dimensional harmonic trap.","The particle is suspended in a non-Newtonian or viscoelastic suspension with a friction kernel that decays exponentially with a time constant characterizing the memory timescale or transient elasticity of the medium.","By solving the associated non-Markovian dynamics, we identify two regimes in parameter space distinguishing the oscillatory and non-oscillatory behavior of the particle motion.","By simulating the particle trajectories and exactly calculating the steady state probability distribution functions and mean square displacement, interestingly, we observe that with an increase in the memory time scale, the elastic bound of suspension dominates over the influence of harmonic trap.","As a consequence, the particle can escape out of the trap without approaching steady state.","On the other hand, with an increase in the duration of the activity, the particle becomes trapped by the harmonic confinement."],"url":"http://arxiv.org/abs/2402.05538v1","category":"cond-mat.soft"}
{"created":"2024-02-08 09:20:06","title":"An Optimal Control Formulation of Tool Affordance Applied to Impact Tasks","abstract":"Humans use tools to complete impact-aware tasks such as hammering a nail or playing tennis. The postures adopted to use these tools can significantly influence the performance of these tasks, where the force or velocity of the hand holding a tool plays a crucial role. The underlying motion planning challenge consists of grabbing the tool in preparation for the use of this tool with an optimal body posture. Directional manipulability describes the dexterity of force and velocity in a joint configuration along a specific direction. In order to take directional manipulability and tool affordances into account, we apply an optimal control method combining iterative linear quadratic regulator(iLQR) with the alternating direction method of multipliers(ADMM). Our approach considers the notion of tool affordances to solve motion planning problems, by introducing a cost based on directional velocity manipulability. The proposed approach is applied to impact tasks in simulation and on a real 7-axis robot, specifically in a nail-hammering task with the assistance of a pilot hole. Our comparison study demonstrates the importance of maximizing directional manipulability in impact-aware tasks.","sentences":["Humans use tools to complete impact-aware tasks such as hammering a nail or playing tennis.","The postures adopted to use these tools can significantly influence the performance of these tasks, where the force or velocity of the hand holding a tool plays a crucial role.","The underlying motion planning challenge consists of grabbing the tool in preparation for the use of this tool with an optimal body posture.","Directional manipulability describes the dexterity of force and velocity in a joint configuration along a specific direction.","In order to take directional manipulability and tool affordances into account, we apply an optimal control method combining iterative linear quadratic regulator(iLQR) with the alternating direction method of multipliers(ADMM).","Our approach considers the notion of tool affordances to solve motion planning problems, by introducing a cost based on directional velocity manipulability.","The proposed approach is applied to impact tasks in simulation and on a real 7-axis robot, specifically in a nail-hammering task with the assistance of a pilot hole.","Our comparison study demonstrates the importance of maximizing directional manipulability in impact-aware tasks."],"url":"http://arxiv.org/abs/2402.05502v1","category":"cs.RO"}
{"created":"2024-02-08 09:12:11","title":"Sustainable allocation of greenhouse gas emission permits for firms with Leontief technologies","abstract":"In this paper we deal with production situations where a cap or limit to the amount of greenhouse gas emissions permitted is imposed. Fixing a tax for each ton of pollutant emitted is also considered. We use bankruptcy rules to define cooperative games with externalities associated with these situations and analyze the existence of coalitionally stable allocations of the emission permits. We prove that the constrained equal awards ( CEA ) rule provides stable allocations and as a direct mechanism, it is incentive compatible. These two facts have interesting managerial implications to control pollution emissions.","sentences":["In this paper we deal with production situations where a cap or limit to the amount of greenhouse gas emissions permitted is imposed.","Fixing a tax for each ton of pollutant emitted is also considered.","We use bankruptcy rules to define cooperative games with externalities associated with these situations and analyze the existence of coalitionally stable allocations of the emission permits.","We prove that the constrained equal awards ( CEA ) rule provides stable allocations and as a direct mechanism, it is incentive compatible.","These two facts have interesting managerial implications to control pollution emissions."],"url":"http://arxiv.org/abs/2402.05499v1","category":"cs.GT"}
{"created":"2024-02-08 08:05:44","title":"Optimal linear approximation and isometric extensions","abstract":"Let $X$ be a Banach space with the unit ball $B(X)$ and $A\\subset X$ be a convex origin-symmetric compact in $X$. Let $\\mathrm{j}:X\\rightarrow \\widetilde{X}$ be an isometric extension of $X$. It is well-known that linear widths $\\lambda _{n}\\left( \\mathrm{j}\\left( A\\right) \\text{,}% \\widetilde{X}\\right) $ may decrease in order when compared with $\\lambda _{n}\\left( A\\text{,}X\\right) $ and absolute widths $\\Lambda \\left( A,% \\widehat{X}\\right) =\\inf_{\\mathrm{j}}\\left( \\mathrm{j}\\left( A\\right) ,% \\widetilde{X}\\right) $ are realized in the space $\\widehat{X}$ which is the Banach space of bounded functions $f:B\\left( X^{\\ast }\\right) \\rightarrow \\mathbb{R}$ on the unit ball $B\\left( X^{\\ast }\\right) $ of the conjugate space $X^{\\ast }$. We show that it is sufficient to use just $n$-dimensional extensions of $X$ to attain absolute linear widths. This unexpected fact significantly reduces the space $\\ \\widehat{X}$. This allows us to introduce the notion of preabsolute widths. We give the respective optimal extensions explicitly and establish order estimates for preabsolute widths of a wide range of sets of smooth functions considered in \\cite{C11}. In particular, in the case of super-small and super-high smoothness considered in \\cite{C11} the orders of preabsolute linear widths coincide with the orders of absolute linear widths. In the intermediate cases of finite and infinite smoothness the respective orders are different.","sentences":["Let $X$ be a Banach space with the unit ball $B(X)$ and $A\\subset X$ be a convex origin-symmetric compact in $X$. Let $\\mathrm{j}:X\\rightarrow \\widetilde{X}$ be an isometric extension of $X$. It is well-known that linear widths $\\lambda _{n}\\left( \\mathrm{j}\\left( A\\right) \\text{,}% \\widetilde{X}\\right) $ may decrease in order when compared with $\\lambda _{n}\\left( A\\text{,}X\\right) $ and absolute widths $\\Lambda \\left( A,% \\widehat{X}\\right) =\\inf_{\\mathrm{j}}\\left( \\mathrm{j}\\left( A\\right) ,% \\widetilde{X}\\right) $ are realized in the space $\\widehat{X}$ which is the Banach space of bounded functions $f:B\\left( X^{\\ast }\\right) \\rightarrow \\mathbb{R}$ on the unit ball $B\\left( X^{\\ast }\\right) $ of the conjugate space $X^{\\ast }$.","We show that it is sufficient to use just $n$-dimensional extensions of $X$ to attain absolute linear widths.","This unexpected fact significantly reduces the space $\\ \\widehat{X}$. This allows us to introduce the notion of preabsolute widths.","We give the respective optimal extensions explicitly and establish order estimates for preabsolute widths of a wide range of sets of smooth functions considered in \\cite{C11}.","In particular, in the case of super-small and super-high smoothness considered in \\cite{C11} the orders of preabsolute linear widths coincide with the orders of absolute linear widths.","In the intermediate cases of finite and infinite smoothness the respective orders are different."],"url":"http://arxiv.org/abs/2402.05475v1","category":"math.FA"}
{"created":"2024-02-08 08:03:39","title":"Question Aware Vision Transformer for Multimodal Reasoning","abstract":"Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning. These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space. Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions. Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image. To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder. This integration results in dynamic visual features focusing on relevant image aspects to the posed question. QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture. Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding.","sentences":["Vision-Language (VL) models have gained significant research focus, enabling remarkable advances in multimodal reasoning.","These architectures typically comprise a vision encoder, a Large Language Model (LLM), and a projection module that aligns visual features with the LLM's representation space.","Despite their success, a critical limitation persists: the vision encoding process remains decoupled from user queries, often in the form of image-related questions.","Consequently, the resulting visual features may not be optimally attuned to the query-specific elements of the image.","To address this, we introduce QA-ViT, a Question Aware Vision Transformer approach for multimodal reasoning, which embeds question awareness directly within the vision encoder.","This integration results in dynamic visual features focusing on relevant image aspects to the posed question.","QA-ViT is model-agnostic and can be incorporated efficiently into any VL architecture.","Extensive experiments demonstrate the effectiveness of applying our method to various multimodal architectures, leading to consistent improvement across diverse tasks and showcasing its potential for enhancing visual and scene-text understanding."],"url":"http://arxiv.org/abs/2402.05472v1","category":"cs.CV"}
{"created":"2024-02-08 07:43:51","title":"Random Methods for Variational Inequalities","abstract":"This paper considers a variational inequality (VI) problem arising from a game among multiple agents, where each agent aims to minimize its own cost function subject to its constrained set represented as the intersection of a (possibly infinite) number of convex functional level sets. A direct projection-based approach or Lagrangian-based techniques for such a problem can be computationally expensive if not impossible to implement. To deal with the problem, we consider randomized methods that avoid the projection step on the whole constraint set by employing random feasibility updates. In particular, we propose and analyze such random methods for solving VIs based on the projection method, Korpelevich method, and Popov method. We establish the almost sure convergence of the methods and, also, provide their convergence rate guarantees. We illustrate the performance of the methods in simulations for two-agent games.","sentences":["This paper considers a variational inequality (VI) problem arising from a game among multiple agents, where each agent aims to minimize its own cost function subject to its constrained set represented as the intersection of a (possibly infinite) number of convex functional level sets.","A direct projection-based approach or Lagrangian-based techniques for such a problem can be computationally expensive if not impossible to implement.","To deal with the problem, we consider randomized methods that avoid the projection step on the whole constraint set by employing random feasibility updates.","In particular, we propose and analyze such random methods for solving VIs based on the projection method, Korpelevich method, and Popov method.","We establish the almost sure convergence of the methods and, also, provide their convergence rate guarantees.","We illustrate the performance of the methods in simulations for two-agent games."],"url":"http://arxiv.org/abs/2402.05462v1","category":"math.OC"}
{"created":"2024-02-08 07:05:00","title":"Robust inference of the Galactic centre gamma-ray excess spatial properties","abstract":"The gamma-ray Fermi-LAT Galactic centre excess (GCE) has puzzled scientists for over 15 years. Despite ongoing debates about its properties, and especially its spatial distribution, its nature remains elusive. We scrutinize how the estimated spatial morphology of this excess depends on models for the Galactic diffuse emission, focusing particularly on the extent to which the Galactic plane and point sources are masked. Our main aim is to compare a spherically symmetric morphology - potentially arising from the annihilation of dark matter (DM) particles - with a boxy morphology - expected if faint unresolved sources in the Galactic bulge dominate the excess emission. Recent claims favouring a DM-motivated template for the GCE are shown to rely on a specific Galactic bulge template, which performs worse than other templates for the Galactic bulge. We find that a non-parametric model of the Galactic bulge derived from the VVV survey results in a significantly better fit for the GCE than DM-motivated templates. This result is independent of whether a GALPROP-based model or a more non-parametric ring-based model is used to describe the diffuse Galactic emission. This conclusion remains true even when additional freedom is added in the background models, allowing for non-parametric modulation of the model components and substantially improving the fit quality. When adopted, optimized background models provide robust results in terms of preference for a boxy bulge morphology of the GCE, regardless of the mask applied to the Galactic plane.","sentences":["The gamma-ray Fermi-LAT Galactic centre excess (GCE) has puzzled scientists for over 15 years.","Despite ongoing debates about its properties, and especially its spatial distribution, its nature remains elusive.","We scrutinize how the estimated spatial morphology of this excess depends on models for the Galactic diffuse emission, focusing particularly on the extent to which the Galactic plane and point sources are masked.","Our main aim is to compare a spherically symmetric morphology - potentially arising from the annihilation of dark matter (DM) particles - with a boxy morphology - expected if faint unresolved sources in the Galactic bulge dominate the excess emission.","Recent claims favouring a DM-motivated template for the GCE are shown to rely on a specific Galactic bulge template, which performs worse than other templates for the Galactic bulge.","We find that a non-parametric model of the Galactic bulge derived from the VVV survey results in a significantly better fit for the GCE than DM-motivated templates.","This result is independent of whether a GALPROP-based model or a more non-parametric ring-based model is used to describe the diffuse Galactic emission.","This conclusion remains true even when additional freedom is added in the background models, allowing for non-parametric modulation of the model components and substantially improving the fit quality.","When adopted, optimized background models provide robust results in terms of preference for a boxy bulge morphology of the GCE, regardless of the mask applied to the Galactic plane."],"url":"http://arxiv.org/abs/2402.05449v1","category":"astro-ph.GA"}
{"created":"2024-02-08 06:51:28","title":"Refined TSSOS","abstract":"The moment-sum of squares hierarchy by Lasserre has become an established technique for solving polynomial optimization problems. It provides a monotonically increasing series of tight bounds, but has well-known scalability limitations. For structured optimization problems, the term-sparsity SOS (TSSOS) approach scales much better due to block-diagonal matrices, obtained by completing the connected components of adjacency graphs. This block structure can be exploited by semidefinite programming solvers, for which the overall runtime then depends heavily on the size of the largest block. However, already the first step of the TSSOS hierarchy may result in large diagonal blocks. We suggest a new approach that refines TSSOS iterations using combinatorial optimization and results in block-diagonal matrices with reduced maximum block sizes. Numerical results on a benchmark library show the large potential for computational speedup for unconstrained and constrained polynomial optimization problems, while obtaining almost identical bounds in comparison to established methods.","sentences":["The moment-sum of squares hierarchy by Lasserre has become an established technique for solving polynomial optimization problems.","It provides a monotonically increasing series of tight bounds, but has well-known scalability limitations.","For structured optimization problems, the term-sparsity SOS (TSSOS) approach scales much better due to block-diagonal matrices, obtained by completing the connected components of adjacency graphs.","This block structure can be exploited by semidefinite programming solvers, for which the overall runtime then depends heavily on the size of the largest block.","However, already the first step of the TSSOS hierarchy may result in large diagonal blocks.","We suggest a new approach that refines TSSOS iterations using combinatorial optimization and results in block-diagonal matrices with reduced maximum block sizes.","Numerical results on a benchmark library show the large potential for computational speedup for unconstrained and constrained polynomial optimization problems, while obtaining almost identical bounds in comparison to established methods."],"url":"http://arxiv.org/abs/2402.05444v1","category":"math.OC"}
{"created":"2024-02-08 06:28:11","title":"Penalized spline estimation of principal components for sparse functional data: rates of convergence","abstract":"This paper gives a comprehensive treatment of the convergence rates of penalized spline estimators for simultaneously estimating several leading principal component functions, when the functional data is sparsely observed. The penalized spline estimators are defined as the solution of a penalized empirical risk minimization problem, where the loss function belongs to a general class of loss functions motivated by the matrix Bregman divergence, and the penalty term is the integrated squared derivative. The theory reveals that the asymptotic behavior of penalized spline estimators depends on the interesting interplay between several factors, i.e., the smoothness of the unknown functions, the spline degree, the spline knot number, the penalty order, and the penalty parameter. The theory also classifies the asymptotic behavior into seven scenarios and characterizes whether and how the minimax optimal rates of convergence are achievable in each scenario.","sentences":["This paper gives a comprehensive treatment of the convergence rates of penalized spline estimators for simultaneously estimating several leading principal component functions, when the functional data is sparsely observed.","The penalized spline estimators are defined as the solution of a penalized empirical risk minimization problem, where the loss function belongs to a general class of loss functions motivated by the matrix Bregman divergence, and the penalty term is the integrated squared derivative.","The theory reveals that the asymptotic behavior of penalized spline estimators depends on the interesting interplay between several factors, i.e., the smoothness of the unknown functions, the spline degree, the spline knot number, the penalty order, and the penalty parameter.","The theory also classifies the asymptotic behavior into seven scenarios and characterizes whether and how the minimax optimal rates of convergence are achievable in each scenario."],"url":"http://arxiv.org/abs/2402.05438v1","category":"math.ST"}
{"created":"2024-02-08 05:25:40","title":"Optimizing Visibility-based Search in Polygonal Domains","abstract":"Given a geometric domain $P$, visibility-based search problems seek routes for one or more mobile agents (``watchmen'') to move within $P$ in order to be able to see a portion (or all) of $P$, while optimizing objectives, such as the length(s) of the route(s), the size (e.g., area or volume) of the portion seen, the probability of detecting a target distributed within $P$ according to a prior distribution, etc. The classic watchman route problem seeks a shortest route for an observer, with omnidirectional vision, to see all of $P$. In this paper we study bicriteria optimization problems for a single mobile agent within a polygonal domain $P$ in the plane, with the criteria of route length and area seen. Specifically, we address the problem of computing a minimum length route that sees at least a specified area of $P$ (minimum length, for a given area quota). We also study the problem of computing a length-constrained route that sees as much area as possible. We provide hardness results and approximation algorithms. In particular, for a simple polygon $P$ we provide the first fully polynomial-time approximation scheme for the problem of computing a shortest route seeing an area quota, as well as a (slightly more efficient) polynomial dual approximation. We also consider polygonal domains $P$ (with holes) and the special case of a planar domain consisting of a union of lines. Our results yield the first approximation algorithms for computing a time-optimal search route in $P$ to guarantee some specified probability of detection of a static target within $P$, randomly distributed in $P$ according to a given prior distribution.","sentences":["Given a geometric domain $P$, visibility-based search problems seek routes for one or more mobile agents (``watchmen'') to move within $P$ in order to be able to see a portion (or all) of $P$, while optimizing objectives, such as the length(s) of the route(s), the size (e.g., area or volume) of the portion seen, the probability of detecting a target distributed within $P$ according to a prior distribution, etc.","The classic watchman route problem seeks a shortest route for an observer, with omnidirectional vision, to see all of $P$. In this paper we study bicriteria optimization problems for a single mobile agent within a polygonal domain $P$ in the plane, with the criteria of route length and area seen.","Specifically, we address the problem of computing a minimum length route that sees at least a specified area of $P$ (minimum length, for a given area quota).","We also study the problem of computing a length-constrained route that sees as much area as possible.","We provide hardness results and approximation algorithms.","In particular, for a simple polygon $P$ we provide the first fully polynomial-time approximation scheme for the problem of computing a shortest route seeing an area quota, as well as a (slightly more efficient) polynomial dual approximation.","We also consider polygonal domains $P$ (with holes) and the special case of a planar domain consisting of a union of lines.","Our results yield the first approximation algorithms for computing a time-optimal search route in $P$ to guarantee some specified probability of detection of a static target within $P$, randomly distributed in $P$ according to a given prior distribution."],"url":"http://arxiv.org/abs/2402.05420v1","category":"cs.CG"}
{"created":"2024-02-08 04:48:20","title":"Why Do Weak-Binding M-N-C Single-Atom Catalysts Still Possess Anomalously High Oxygen Reduction Activity in Alkaline Media?","abstract":"Single-atom catalysts (SACs) with metal-nitrogen-carbon (M-N-C) structures are promising candidates for oxygen reduction reactions (ORR). Based on the adsorption strength, theory predicts that an optimal catalyst should possess moderate binding strength (e.g., M=Fe/Co). However, the considerable ORR activity observed from weak-binding M-N-C catalysts (M=Ni/Cu/Zn) in alkaline electrolytes contradicts theoretical prediction, challenging the well-established Sabatier principle and urging for identifying new underlying mechanisms. This study reveals a new bridge-adsorbed oxygen mechanism in weak-binding SACs by incorporating a pH-field coupled microkinetic model and experiments. We found that the O* favours bridge adsorption in weak-binding SACs that differs significantly from the typical atop-O* in terms of scaling relations, electric field responses, and solvation effects. These variations further impact the pH dependence and the kinetic barriers of HOO* activation, for which a unified scaling relation model with kinetics incorporated has been developed. This new model shows improved alignment with experimental data, offering a design principle for high-performance SACs by leveraging weak-binding atoms to modify reaction pathways, potentially advancing SAC development for ORR and beyond.","sentences":["Single-atom catalysts (SACs) with metal-nitrogen-carbon (M-N-C) structures are promising candidates for oxygen reduction reactions (ORR).","Based on the adsorption strength, theory predicts that an optimal catalyst should possess moderate binding strength (e.g., M=Fe/Co).","However, the considerable ORR activity observed from weak-binding M-N-C catalysts (M=Ni/Cu/Zn) in alkaline electrolytes contradicts theoretical prediction, challenging the well-established Sabatier principle and urging for identifying new underlying mechanisms.","This study reveals a new bridge-adsorbed oxygen mechanism in weak-binding SACs by incorporating a pH-field coupled microkinetic model and experiments.","We found that the O* favours bridge adsorption in weak-binding SACs that differs significantly from the typical atop-O* in terms of scaling relations, electric field responses, and solvation effects.","These variations further impact the pH dependence and the kinetic barriers of HOO* activation, for which a unified scaling relation model with kinetics incorporated has been developed.","This new model shows improved alignment with experimental data, offering a design principle for high-performance SACs by leveraging weak-binding atoms to modify reaction pathways, potentially advancing SAC development for ORR and beyond."],"url":"http://arxiv.org/abs/2402.05405v1","category":"physics.chem-ph"}
{"created":"2024-02-08 03:52:12","title":"Zeroth-order Low-rank Hessian Estimation via Matrix Recovery","abstract":"A zeroth-order Hessian estimator aims to recover the Hessian matrix of an objective function at any given point, using minimal finite-difference computations. This paper studies zeroth-order Hessian estimation for low-rank Hessians, from a matrix recovery perspective. Our challenge lies in the fact that traditional matrix recovery techniques are not directly suitable for our scenario. They either demand incoherence assumptions (or its variants), or require an impractical number of finite-difference computations in our setting. To overcome these hurdles, we employ zeroth-order Hessian estimations aligned with proper matrix measurements, and prove new recovery guarantees for these estimators. More specifically, we prove that for a Hessian matrix $H \\in \\mathbb{R}^{n \\times n}$ of rank $r$, $ \\mathcal{O}(nr^2 \\log^2 n ) $ proper zeroth-order finite-difference computations ensures a highly probable exact recovery of $H$. Compared to existing methods, our method can greatly reduce the number of finite-difference computations, and does not require any incoherence assumptions.","sentences":["A zeroth-order Hessian estimator aims to recover the Hessian matrix of an objective function at any given point, using minimal finite-difference computations.","This paper studies zeroth-order Hessian estimation for low-rank Hessians, from a matrix recovery perspective.","Our challenge lies in the fact that traditional matrix recovery techniques are not directly suitable for our scenario.","They either demand incoherence assumptions (or its variants), or require an impractical number of finite-difference computations in our setting.","To overcome these hurdles, we employ zeroth-order Hessian estimations aligned with proper matrix measurements, and prove new recovery guarantees for these estimators.","More specifically, we prove that for a Hessian matrix $H \\in \\mathbb{R}^{n \\times n}$ of rank $r$, $ \\mathcal{O}(nr^2 \\log^2 n ) $ proper zeroth-order finite-difference computations ensures a highly probable exact recovery of $H$. Compared to existing methods, our method can greatly reduce the number of finite-difference computations, and does not require any incoherence assumptions."],"url":"http://arxiv.org/abs/2402.05385v1","category":"math.OC"}
{"created":"2024-02-07 22:47:25","title":"Control of AC-AC interlinking converters for multi-grids","abstract":"This paper considers the control of AC-AC inter-linking converters (ILCs) in a multi-grid network. We overview the control schemes in the literature and propose a passivity framework for the stabilization of multi-grid networks, considering both AC grid-following and AC grid-forming behavior for the ILC connections. We then analyze a range of AC/AC interlinking converter control methods derived from the literature and propose suitable controllers for this purpose including both AC grid-forming and grid-following behavior. The controller we propose is partially grid-forming; in particular, it is based on a combination of a grid-following and a grid-forming converter to improve the stability properties of the network. Simulation results and theoretical analysis confirm that the proposed ILC control designs are appropriate for the multi-grid network.","sentences":["This paper considers the control of AC-AC inter-linking converters (ILCs) in a multi-grid network.","We overview the control schemes in the literature and propose a passivity framework for the stabilization of multi-grid networks, considering both AC grid-following and AC grid-forming behavior for the ILC connections.","We then analyze a range of AC/AC interlinking converter control methods derived from the literature and propose suitable controllers for this purpose including both AC grid-forming and grid-following behavior.","The controller we propose is partially grid-forming; in particular, it is based on a combination of a grid-following and a grid-forming converter to improve the stability properties of the network.","Simulation results and theoretical analysis confirm that the proposed ILC control designs are appropriate for the multi-grid network."],"url":"http://arxiv.org/abs/2402.05303v1","category":"eess.SY"}
{"created":"2024-02-07 22:24:12","title":"Asymptotic Quantum State Discrimination for Mixtures of Unitarily Related States","abstract":"Given a mixture of states, finding a way to optimally discriminate its elements is a prominent problem in quantum communication theory. In this paper, we will address mixtures of density operators that are unitarily equivalent via elements of a one-parameter unitary group, and the corresponding quantum state discrimination (QSD) problems. We will be particularly interested in QSD as time goes to infinity. We first present an approach to QSD in the case of countable mixtures and address the respective asymptotic QSD optimization problems, proving necessary and sufficient conditions for minimal error to be obtained in the asymptotic regime (we say that in such a case QSD is fully solvable). We then outline an analogous approach to uncountable mixtures, presenting some conjectures that mirror the results presented for the cases of countable mixtures. As a technical tool, we prove and use an infinite dimensional version of the well-known Barnum-Knill bound.","sentences":["Given a mixture of states, finding a way to optimally discriminate its elements is a prominent problem in quantum communication theory.","In this paper, we will address mixtures of density operators that are unitarily equivalent via elements of a one-parameter unitary group, and the corresponding quantum state discrimination (QSD) problems.","We will be particularly interested in QSD as time goes to infinity.","We first present an approach to QSD in the case of countable mixtures and address the respective asymptotic QSD optimization problems, proving necessary and sufficient conditions for minimal error to be obtained in the asymptotic regime (we say that in such a case QSD is fully solvable).","We then outline an analogous approach to uncountable mixtures, presenting some conjectures that mirror the results presented for the cases of countable mixtures.","As a technical tool, we prove and use an infinite dimensional version of the well-known Barnum-Knill bound."],"url":"http://arxiv.org/abs/2402.05297v1","category":"math-ph"}
{"created":"2024-02-07 20:13:01","title":"Astronomical Image Processing Benchmark Study for Various Telescope Aperture Shapes","abstract":"We explore the impact of different telescope apertures on the image simulation and deconvolution processes within the context of a synthetic star field. Using HCIPy and Python programming, we modelled six telescope apertures namely Circular, Hexagonal, Elliptical (with horizontal and vertical major axes), segmented hexagonal (JWST), and obstructed circular (HST). We calculated Point Spread Functions (PSFs) for each aperture, incorporating surface shape-induced wavefront aberrations, convolved them with a synthetic star field spanning a range of brightness magnitudes, and introduced photon and detector noise layers to simulate realistic imaging conditions. Subsequent deconvolution using the Richardson-Lucy algorithm allowed for an analysis of deconvolution accuracy based on parameters like average distance between stars and differences in the number of stars between original and deconvolved images. Results indicate that the choice of telescope aperture significantly influences both simulated images and deconvolution outcomes, with brightness magnitude also playing a crucial role. The study highlights the necessity of optimizing image processing pipelines and Deconvolution algorithms tailored to each aperture shapes and their corresponding PSFs, emphasizing the pivotal role of aperture selection and optimization in achieving accurate astronomical imaging performance.","sentences":["We explore the impact of different telescope apertures on the image simulation and deconvolution processes within the context of a synthetic star field.","Using HCIPy and Python programming, we modelled six telescope apertures namely Circular, Hexagonal, Elliptical (with horizontal and vertical major axes), segmented hexagonal (JWST), and obstructed circular (HST).","We calculated Point Spread Functions (PSFs) for each aperture, incorporating surface shape-induced wavefront aberrations, convolved them with a synthetic star field spanning a range of brightness magnitudes, and introduced photon and detector noise layers to simulate realistic imaging conditions.","Subsequent deconvolution using the Richardson-Lucy algorithm allowed for an analysis of deconvolution accuracy based on parameters like average distance between stars and differences in the number of stars between original and deconvolved images.","Results indicate that the choice of telescope aperture significantly influences both simulated images and deconvolution outcomes, with brightness magnitude also playing a crucial role.","The study highlights the necessity of optimizing image processing pipelines and Deconvolution algorithms tailored to each aperture shapes and their corresponding PSFs, emphasizing the pivotal role of aperture selection and optimization in achieving accurate astronomical imaging performance."],"url":"http://arxiv.org/abs/2402.05233v1","category":"astro-ph.IM"}
{"created":"2024-02-07 20:06:29","title":"The promising path of evolutionary optimization to avoid barren plateaus","abstract":"Variational quantum algorithms are viewed as promising candidates for demonstrating quantum advantage on near-term devices. These approaches typically involve the training of parameterized quantum circuits through a classical optimization loop. However, they often encounter challenges attributed to the exponentially diminishing gradient components, known as the barren plateau (BP) problem. This work introduces a novel optimization method designed to alleviate the adverse effects of BPs during circuit training. Our approach to select the optimization search direction relies on the distant features of the cost-function landscape. This enables the optimization path to navigate around barren plateaus without the need for external control mechanisms. We have successfully applied our optimization strategy to quantum circuits comprising $16$ qubits and $15000$ entangling gates, demonstrating robust resistance against BPs. Additionally, we have extended our optimization strategy by incorporating an evolutionary selection framework, enhancing its ability to avoid local minima in the landscape. The modified algorithm has been successfully utilized in quantum gate synthesis applications, showcasing a significantly improved efficiency in generating highly compressed quantum circuits compared to traditional gradient-based optimization approaches.","sentences":["Variational quantum algorithms are viewed as promising candidates for demonstrating quantum advantage on near-term devices.","These approaches typically involve the training of parameterized quantum circuits through a classical optimization loop.","However, they often encounter challenges attributed to the exponentially diminishing gradient components, known as the barren plateau (BP) problem.","This work introduces a novel optimization method designed to alleviate the adverse effects of BPs during circuit training.","Our approach to select the optimization search direction relies on the distant features of the cost-function landscape.","This enables the optimization path to navigate around barren plateaus without the need for external control mechanisms.","We have successfully applied our optimization strategy to quantum circuits comprising $16$ qubits and $15000$ entangling gates, demonstrating robust resistance against BPs.","Additionally, we have extended our optimization strategy by incorporating an evolutionary selection framework, enhancing its ability to avoid local minima in the landscape.","The modified algorithm has been successfully utilized in quantum gate synthesis applications, showcasing a significantly improved efficiency in generating highly compressed quantum circuits compared to traditional gradient-based optimization approaches."],"url":"http://arxiv.org/abs/2402.05227v1","category":"quant-ph"}
{"created":"2024-02-07 19:44:00","title":"Geometric characterizations of Lipschitz stability for convex optimization problems","abstract":"In this paper, we mainly study tilt stability and Lipschitz stability of convex optimization problems. Our characterizations are geometric and fully computable in many important cases. As a result, we apply our theory to the group Lasso problem and the nuclear norm minimization problem and reveal that the Lipschitz stability of the solution mapping in these problems is automatic whenever the solution mapping is single-valued.","sentences":["In this paper, we mainly study tilt stability and Lipschitz stability of convex optimization problems.","Our characterizations are geometric and fully computable in many important cases.","As a result, we apply our theory to the group Lasso problem and the nuclear norm minimization problem and reveal that the Lipschitz stability of the solution mapping in these problems is automatic whenever the solution mapping is single-valued."],"url":"http://arxiv.org/abs/2402.05215v1","category":"math.OC"}
{"created":"2024-02-07 19:39:27","title":"Non-Monotonicity of Branching Rules with respect to Linear Relaxations","abstract":"Modern mixed-integer programming solvers use the branch-and-cut framework, where cutting planes are added to improve the tightness of the linear programming (LP) relaxation, with the expectation that the tighter formulation would produce smaller branch-and-bound trees. In this work, we consider the question of whether adding cuts will always lead to smaller trees for a given fixed branching rule. We formally call such a property of a branching rule monotonicity. We prove that any branching rule which exclusively branches on fractional variables in the LP solution is non-monotonic. Moreover, we present a family of instances where adding a single cut leads to an exponential increase in the size of full strong branching trees, despite improving the LP bound. Finally, we empirically attempt to estimate the prevalence of non-monotonicity in practice while using full strong branching. We consider randomly generated multi-dimensional knapsacks tightened by cover cuts as well as instances from the MIPLIB 2017 benchmark set for the computational experiments. Our main insight from these experiments is that if the gap closed by cuts is small, change in tree size is difficult to predict, and often increases, possibly due to inherent non-monotonicity. However, when a sufficiently large gap is closed, a significant decrease in tree size may be expected.","sentences":["Modern mixed-integer programming solvers use the branch-and-cut framework, where cutting planes are added to improve the tightness of the linear programming (LP) relaxation, with the expectation that the tighter formulation would produce smaller branch-and-bound trees.","In this work, we consider the question of whether adding cuts will always lead to smaller trees for a given fixed branching rule.","We formally call such a property of a branching rule monotonicity.","We prove that any branching rule which exclusively branches on fractional variables in the LP solution is non-monotonic.","Moreover, we present a family of instances where adding a single cut leads to an exponential increase in the size of full strong branching trees, despite improving the LP bound.","Finally, we empirically attempt to estimate the prevalence of non-monotonicity in practice while using full strong branching.","We consider randomly generated multi-dimensional knapsacks tightened by cover cuts as well as instances from the MIPLIB 2017 benchmark set for the computational experiments.","Our main insight from these experiments is that if the gap closed by cuts is small, change in tree size is difficult to predict, and often increases, possibly due to inherent non-monotonicity.","However, when a sufficiently large gap is closed, a significant decrease in tree size may be expected."],"url":"http://arxiv.org/abs/2402.05213v1","category":"math.OC"}
{"created":"2024-02-07 19:36:36","title":"A Maturity Model for Urban Dataset Meta-data","abstract":"In the current environment of data generation and publication, there is an ever-growing number of datasets available for download. This growth precipitates an existing challenge: sourcing and integrating relevant datasets for analysis is becoming more complex. Despite efforts by open data platforms, obstacles remain, predominantly rooted in inadequate metadata, unsuitable data presentation, complications in pinpointing desired data, and data integration. This paper delves into the intricacies of dataset retrieval, emphasizing the pivotal role of metadata in aligning datasets with user queries. Through an exploration of existing literature, it underscores prevailing issues such as the identification of valuable metadata and the development of tools to maintain and annotate them effectively. The central contribution of this research is the proposition of a dataset metadata maturity model. Deriving inspiration from software engineering maturity models, this framework delineates a progression from rudimentary metadata documentation to advanced levels, aiding dataset creators in their documentation efforts. The model encompasses seven pivotal dimensions, spanning content to quality information, each stratified across six maturity levels to guide the optimal documentation of datasets, ensuring ease of discovery, relevance assessment, and comprehensive dataset understanding. This paper also incorporates the maturity model into a data cataloguing tool called CKAN through a custom plugin, CKANext-udc. The plugin introduces custom fields based on different maturity levels, allows for user interface customisation, and integrates with a graph database, converting catalogue data into a knowledge graph based on the Maturity Model ontology.","sentences":["In the current environment of data generation and publication, there is an ever-growing number of datasets available for download.","This growth precipitates an existing challenge: sourcing and integrating relevant datasets for analysis is becoming more complex.","Despite efforts by open data platforms, obstacles remain, predominantly rooted in inadequate metadata, unsuitable data presentation, complications in pinpointing desired data, and data integration.","This paper delves into the intricacies of dataset retrieval, emphasizing the pivotal role of metadata in aligning datasets with user queries.","Through an exploration of existing literature, it underscores prevailing issues such as the identification of valuable metadata and the development of tools to maintain and annotate them effectively.","The central contribution of this research is the proposition of a dataset metadata maturity model.","Deriving inspiration from software engineering maturity models, this framework delineates a progression from rudimentary metadata documentation to advanced levels, aiding dataset creators in their documentation efforts.","The model encompasses seven pivotal dimensions, spanning content to quality information, each stratified across six maturity levels to guide the optimal documentation of datasets, ensuring ease of discovery, relevance assessment, and comprehensive dataset understanding.","This paper also incorporates the maturity model into a data cataloguing tool called CKAN through a custom plugin, CKANext-udc.","The plugin introduces custom fields based on different maturity levels, allows for user interface customisation, and integrates with a graph database, converting catalogue data into a knowledge graph based on the Maturity Model ontology."],"url":"http://arxiv.org/abs/2402.05211v1","category":"cs.DL"}
{"created":"2024-02-07 19:26:03","title":"Measurement Methodology for Determining the Optimal Frequency Domain Configuration to Accurately Record WiFi Exposure Levels","abstract":"Radiofrequency fields are usually measured in order to be compared with electromagnetic exposure limits defined by international standardization organizations with the aim of preserving the human health. However, in the case of WiFi technology, accurate measurement of the radiation coming from user terminals and access points is a great challenge due to the nature of these emissions, which are noncontinuous signals transmitted in the form of pulses of short duration. Most of the methodologies defined up to now for determining WiFi exposure levels use or take as reference exposimeters, broadband probes, and spectrum analyzers without taking into account that WiFi signals are not continuously transmitted. This leads to an overestimation of the radiation level that cannot be considered negligible when data of the actual exposure are needed. To avoid this, other procedures apply empirical weighting factors that account for the actual duration of burst transmissions. However, this implies the implementation of additional measurements for calculating the weighting factors, and thus, increases the complexity of the work. According to this, it was still necessary to define the frequency domain measurement setup that is optimal for obtaining realistic WiFi signal values, without requiring the performance of additional recordings. Thus, the definition of an appropriate methodology to achieve this goal was established as the main objective of this paper. The set of tasks carried out to identify such a configuration, as well as the limitations obtained for other measurement settings, are deeply explained in this paper.","sentences":["Radiofrequency fields are usually measured in order to be compared with electromagnetic exposure limits defined by international standardization organizations with the aim of preserving the human health.","However, in the case of WiFi technology, accurate measurement of the radiation coming from user terminals and access points is a great challenge due to the nature of these emissions, which are noncontinuous signals transmitted in the form of pulses of short duration.","Most of the methodologies defined up to now for determining WiFi exposure levels use or take as reference exposimeters, broadband probes, and spectrum analyzers without taking into account that WiFi signals are not continuously transmitted.","This leads to an overestimation of the radiation level that cannot be considered negligible when data of the actual exposure are needed.","To avoid this, other procedures apply empirical weighting factors that account for the actual duration of burst transmissions.","However, this implies the implementation of additional measurements for calculating the weighting factors, and thus, increases the complexity of the work.","According to this, it was still necessary to define the frequency domain measurement setup that is optimal for obtaining realistic WiFi signal values, without requiring the performance of additional recordings.","Thus, the definition of an appropriate methodology to achieve this goal was established as the main objective of this paper.","The set of tasks carried out to identify such a configuration, as well as the limitations obtained for other measurement settings, are deeply explained in this paper."],"url":"http://arxiv.org/abs/2402.05208v1","category":"eess.SP"}
{"created":"2024-02-07 14:49:10","title":"Two Trades is not Baffled: Condensing Graph via Crafting Rational Gradient Matching","abstract":"Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns. As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set. Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories. Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs. In light of this, we propose a novel graph condensation method named \\textbf{C}raf\\textbf{T}ing \\textbf{R}ationa\\textbf{L} trajectory (\\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for gradient matching. Theoretically, CTRL can effectively neutralize the impact of accumulated errors on the performance of condensed graphs. We provide extensive experiments on various graph datasets and downstream tasks to support the effectiveness of CTRL. Code is released at https://github.com/NUS-HPC-AI-Lab/CTRL.","sentences":["Training on large-scale graphs has achieved remarkable results in graph representation learning, but its cost and storage have raised growing concerns.","As one of the most promising directions, graph condensation methods address these issues by employing gradient matching, aiming to condense the full graph into a more concise yet information-rich synthetic set.","Though encouraging, these strategies primarily emphasize matching directions of the gradients, which leads to deviations in the training trajectories.","Such deviations are further magnified by the differences between the condensation and evaluation phases, culminating in accumulated errors, which detrimentally affect the performance of the condensed graphs.","In light of this, we propose a novel graph condensation method named \\textbf{C}raf\\textbf{T}ing \\textbf{R}ationa\\textbf{L} trajectory (\\textbf{CTRL}), which offers an optimized starting point closer to the original dataset's feature distribution and a more refined strategy for gradient matching.","Theoretically, CTRL can effectively neutralize the impact of accumulated errors on the performance of condensed graphs.","We provide extensive experiments on various graph datasets and downstream tasks to support the effectiveness of CTRL.","Code is released at https://github.com/NUS-HPC-AI-Lab/CTRL."],"url":"http://arxiv.org/abs/2402.04924v2","category":"cs.LG"}
