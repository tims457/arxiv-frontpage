{"created":"2024-03-28 17:59:50","title":"GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling","abstract":"3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed. However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling. To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling. We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport. The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs. Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation.","sentences":["3D Gaussian Splatting (GS) have achieved considerable improvement over Neural Radiance Fields in terms of 3D fitting fidelity and rendering speed.","However, this unstructured representation with scattered Gaussians poses a significant challenge for generative modeling.","To address the problem, we introduce GaussianCube, a structured GS representation that is both powerful and efficient for generative modeling.","We achieve this by first proposing a modified densification-constrained GS fitting algorithm which can yield high-quality fitting results using a fixed number of free Gaussians, and then re-arranging the Gaussians into a predefined voxel grid via Optimal Transport.","The structured grid representation allows us to use standard 3D U-Net as our backbone in diffusion generative modeling without elaborate designs.","Extensive experiments conducted on ShapeNet and OmniObject3D show that our model achieves state-of-the-art generation results both qualitatively and quantitatively, underscoring the potential of GaussianCube as a powerful and versatile 3D representation."],"url":"http://arxiv.org/abs/2403.19655v1","category":"cs.CV"}
{"created":"2024-03-28 17:59:42","title":"Detecting Image Attribution for Text-to-Image Diffusion Models in RGB and Beyond","abstract":"Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity. These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task. In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible. Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent. We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure. Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images. Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored.","sentences":["Modern text-to-image (T2I) diffusion models can generate images with remarkable realism and creativity.","These advancements have sparked research in fake image detection and attribution, yet prior studies have not fully explored the practical and scientific dimensions of this task.","In addition to attributing images to 12 state-of-the-art T2I generators, we provide extensive analyses on what inference stage hyperparameters and image modifications are discernible.","Our experiments reveal that initialization seeds are highly detectable, along with other subtle variations in the image generation process to some extent.","We further investigate what visual traces are leveraged in image attribution by perturbing high-frequency details and employing mid-level representations of image style and structure.","Notably, altering high-frequency information causes only slight reductions in accuracy, and training an attributor on style representations outperforms training on RGB images.","Our analyses underscore that fake images are detectable and attributable at various levels of visual granularity than previously explored."],"url":"http://arxiv.org/abs/2403.19653v1","category":"cs.CV"}
{"created":"2024-03-28 17:59:30","title":"InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction","abstract":"Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations. However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions. This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data. Our key insight in achieving this is that interaction semantics and dynamics can be decoupled. Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model. While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics. To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion. By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner. We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives.","sentences":["Text-conditioned human motion generation has experienced significant advancements with diffusion models trained on extensive motion capture data and corresponding textual annotations.","However, extending such success to 3D dynamic human-object interaction (HOI) generation faces notable challenges, primarily due to the lack of large-scale interaction data and comprehensive descriptions that align with these interactions.","This paper takes the initiative and showcases the potential of generating human-object interactions without direct training on text-interaction pair data.","Our key insight in achieving this is that interaction semantics and dynamics can be decoupled.","Being unable to learn interaction semantics through supervised training, we instead leverage pre-trained large models, synergizing knowledge from a large language model and a text-to-motion model.","While such knowledge offers high-level control over interaction semantics, it cannot grasp the intricacies of low-level interaction dynamics.","To overcome this issue, we further introduce a world model designed to comprehend simple physics, modeling how human actions influence object motion.","By integrating these components, our novel framework, InterDreamer, is able to generate text-aligned 3D HOI sequences in a zero-shot manner.","We apply InterDreamer to the BEHAVE and CHAIRS datasets, and our comprehensive experimental analysis demonstrates its capability to generate realistic and coherent interaction sequences that seamlessly align with the text directives."],"url":"http://arxiv.org/abs/2403.19652v1","category":"cs.CV"}
{"created":"2024-03-28 17:59:20","title":"MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions","abstract":"Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures. Recent work leverages text instructions to allow users to more freely express their search intents. However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations. The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity. To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions. MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via large multimodal models (LMMs) and large language models (LLMs). Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves comparable or better results on eight benchmarks of various image retrieval tasks than prior state-of-the-art (SOTA) methods. Remarkably, it outperforms previous SOTA but with a 50X smaller model size on multiple benchmarks. Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens.","sentences":["Image retrieval, i.e., finding desired images given a reference image, inherently encompasses rich, multi-faceted search intents that are difficult to capture solely using image-based measures.","Recent work leverages text instructions to allow users to more freely express their search intents.","However, existing work primarily focuses on image pairs that are visually similar and/or can be characterized by a small set of pre-defined relations.","The core thesis of this paper is that text instructions can enable retrieving images with richer relations beyond visual similarity.","To show this, we introduce MagicLens, a series of self-supervised image retrieval models that support open-ended instructions.","MagicLens is built on a key novel insight: image pairs that naturally occur on the same web pages contain a wide range of implicit relations (e.g., inside view of), and we can bring those implicit relations explicit by synthesizing instructions via large multimodal models (LMMs) and large language models (LLMs).","Trained on 36.7M (query image, instruction, target image) triplets with rich semantic relations mined from the web, MagicLens achieves comparable or better results on eight benchmarks of various image retrieval tasks than prior state-of-the-art (SOTA) methods.","Remarkably, it outperforms previous SOTA but with a 50X smaller model size on multiple benchmarks.","Additional human analyses on a 1.4M-image unseen corpus further demonstrate the diversity of search intents supported by MagicLens."],"url":"http://arxiv.org/abs/2403.19651v1","category":"cs.CV"}
{"created":"2024-03-28 17:58:01","title":"Consistency of JWST Black Hole Observations with NANOGrav Gravitational Wave Measurements","abstract":"JWST observations have opened a new chapter in studies of supermassive black holes (SMBHs), stimulating discussion of two puzzles: the abundance of SMBHs in the early Universe and the fraction of dual AGNs. In this paper we argue that the answers to these puzzles may be linked to an interpretation of the data on the nHz gravitational wave (GWs) discovered by NANOGrav and other Pulsar Timing Arrays (PTAs) in terms of SMBH binaries losing energy by interactions with their environments as well as by GW emission. According to this interpretation, the SMBHs in low-$z$ AGNs are the tip of the iceberg of the local SMBH population, which are mainly in inactive galaxies. This interpretation would favour the observability of GW signals from BH binaries in LISA and deciHz GW detectors.","sentences":["JWST observations have opened a new chapter in studies of supermassive black holes (SMBHs), stimulating discussion of two puzzles: the abundance of SMBHs in the early Universe and the fraction of dual AGNs.","In this paper we argue that the answers to these puzzles may be linked to an interpretation of the data on the nHz gravitational wave (GWs) discovered by NANOGrav and other Pulsar Timing Arrays (PTAs) in terms of SMBH binaries losing energy by interactions with their environments as well as by GW emission.","According to this interpretation, the SMBHs in low-$z$ AGNs are the tip of the iceberg of the local SMBH population, which are mainly in inactive galaxies.","This interpretation would favour the observability of GW signals from BH binaries in LISA and deciHz GW detectors."],"url":"http://arxiv.org/abs/2403.19650v1","category":"astro-ph.CO"}
{"created":"2024-03-28 17:57:27","title":"GraspXL: Generating Grasping Motions for Diverse Objects at Scale","abstract":"Human hands possess the dexterity to interact with diverse objects such as grasping specific parts of the objects and/or approaching them from desired directions. More importantly, humans can grasp objects of any shape without object-specific skills. Recent works synthesize grasping motions following single objectives such as a desired approach heading direction or a grasping area. Moreover, they usually rely on expensive 3D hand-object data during training and inference, which limits their capability to synthesize grasping motions for unseen objects at scale. In this paper, we unify the generation of hand-object grasping motions across multiple motion objectives, diverse object shapes and dexterous hand morphologies in a policy learning framework GraspXL. The objectives are composed of the graspable area, heading direction during approach, wrist rotation, and hand position. Without requiring any 3D hand-object interaction data, our policy trained with 58 objects can robustly synthesize diverse grasping motions for more than 500k unseen objects with a success rate of 82.2%. At the same time, the policy adheres to objectives, which enables the generation of diverse grasps per object. Moreover, we show that our framework can be deployed to different dexterous hands and work with reconstructed or generated objects. We quantitatively and qualitatively evaluate our method to show the efficacy of our approach. Our model and code will be available.","sentences":["Human hands possess the dexterity to interact with diverse objects such as grasping specific parts of the objects and/or approaching them from desired directions.","More importantly, humans can grasp objects of any shape without object-specific skills.","Recent works synthesize grasping motions following single objectives such as a desired approach heading direction or a grasping area.","Moreover, they usually rely on expensive 3D hand-object data during training and inference, which limits their capability to synthesize grasping motions for unseen objects at scale.","In this paper, we unify the generation of hand-object grasping motions across multiple motion objectives, diverse object shapes and dexterous hand morphologies in a policy learning framework GraspXL.","The objectives are composed of the graspable area, heading direction during approach, wrist rotation, and hand position.","Without requiring any 3D hand-object interaction data, our policy trained with 58 objects can robustly synthesize diverse grasping motions for more than 500k unseen objects with a success rate of 82.2%.","At the same time, the policy adheres to objectives, which enables the generation of diverse grasps per object.","Moreover, we show that our framework can be deployed to different dexterous hands and work with reconstructed or generated objects.","We quantitatively and qualitatively evaluate our method to show the efficacy of our approach.","Our model and code will be available."],"url":"http://arxiv.org/abs/2403.19649v1","category":"cs.RO"}
{"created":"2024-03-28 17:56:56","title":"Human-compatible driving partners through data-regularized self-play reinforcement learning","abstract":"A central challenge for autonomous vehicles is coordinating with humans. Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation. Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving. However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting. To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy. In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations. We evaluate agents in a large set of multi-agent traffic scenes. Results show our HR-PPO agents are highly effective in achieving goals, with a success rate of 93%, an off-road rate of 3.5%, and a collision rate of 3%. At the same time, the agents drive in a human-like manner, as measured by their similarity to existing human driving logs. We also find that HR-PPO agents show considerable improvements on proxy measures for coordination with human driving, particularly in highly interactive scenarios. We open-source our code and trained agents at https://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent behaviors at https://sites.google.com/view/driving-partners.","sentences":["A central challenge for autonomous vehicles is coordinating with humans.","Therefore, incorporating realistic human agents is essential for scalable training and evaluation of autonomous driving systems in simulation.","Simulation agents are typically developed by imitating large-scale, high-quality datasets of human driving.","However, pure imitation learning agents empirically have high collision rates when executed in a multi-agent closed-loop setting.","To build agents that are realistic and effective in closed-loop settings, we propose Human-Regularized PPO (HR-PPO), a multi-agent algorithm where agents are trained through self-play with a small penalty for deviating from a human reference policy.","In contrast to prior work, our approach is RL-first and only uses 30 minutes of imperfect human demonstrations.","We evaluate agents in a large set of multi-agent traffic scenes.","Results show our HR-PPO agents are highly effective in achieving goals, with a success rate of 93%, an off-road rate of 3.5%, and a collision rate of 3%.","At the same time, the agents drive in a human-like manner, as measured by their similarity to existing human driving logs.","We also find that HR-PPO agents show considerable improvements on proxy measures for coordination with human driving, particularly in highly interactive scenarios.","We open-source our code and trained agents at https://github.com/Emerge-Lab/nocturne_lab and provide demonstrations of agent behaviors at https://sites.google.com/view/driving-partners."],"url":"http://arxiv.org/abs/2403.19648v1","category":"cs.RO"}
{"created":"2024-03-28 17:56:07","title":"Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models","abstract":"We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.","sentences":["We introduce methods for discovering and applying sparse feature circuits.","These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors.","Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications.","In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms.","Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant.","Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors."],"url":"http://arxiv.org/abs/2403.19647v1","category":"cs.LG"}
{"created":"2024-03-28 17:55:42","title":"Change-Agent: Towards Interactive Comprehensive Change Interpretation and Analysis from Change Detection and Change Captioning","abstract":"Monitoring changes in the Earth's surface is crucial for understanding natural processes and human impacts, necessitating precise and comprehensive interpretation methodologies. Remote sensing satellite imagery offers a unique perspective for monitoring these changes, leading to the emergence of remote sensing image change interpretation (RSICI) as a significant research focus. Current RSICI technology encompasses change detection and change captioning, each with its limitations in providing comprehensive interpretation. To address this, we propose an interactive Change-Agent which integrates a multi-level change interpretation (MCI) model as eyes and a large language model (LLM) as the brain. Our Change-Agent can follow user instructions to achieve comprehensive change interpretation and insightful analysis according to user instructions, such as change detection and change captioning, change object counting, change cause analysis, etc. Our proposed MCI model contains two branches of pixel-level change detection and semantic-level change captioning, in which multiple BI-temporal Iterative Interaction (BI3) layers utilize Local Perception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA) modules to enhance the model's discriminative feature representation capabilities. To train the MCI model, we build the LEVIR-MCI dataset with change masks and captions of bi-temporal images. Extensive experiments demonstrate the effectiveness of the proposed change interpretation model and highlight the promising potential of our Change-Agent in facilitating comprehensive and intelligent interpretation of surface changes. We will make our dataset and codebase of the change interpretation model and Change-Agent publicly available to facilitate future research at https://github.com/Chen-Yang-Liu/Change-Agent","sentences":["Monitoring changes in the Earth's surface is crucial for understanding natural processes and human impacts, necessitating precise and comprehensive interpretation methodologies.","Remote sensing satellite imagery offers a unique perspective for monitoring these changes, leading to the emergence of remote sensing image change interpretation (RSICI) as a significant research focus.","Current RSICI technology encompasses change detection and change captioning, each with its limitations in providing comprehensive interpretation.","To address this, we propose an interactive Change-Agent which integrates a multi-level change interpretation (MCI) model as eyes and a large language model (LLM) as the brain.","Our Change-Agent can follow user instructions to achieve comprehensive change interpretation and insightful analysis according to user instructions, such as change detection and change captioning, change object counting, change cause analysis, etc.","Our proposed MCI model contains two branches of pixel-level change detection and semantic-level change captioning, in which multiple BI-temporal Iterative Interaction (BI3) layers utilize Local Perception Enhancement (LPE) and the Global Difference Fusion Attention (GDFA) modules to enhance the model's discriminative feature representation capabilities.","To train the MCI model, we build the LEVIR-MCI dataset with change masks and captions of bi-temporal images.","Extensive experiments demonstrate the effectiveness of the proposed change interpretation model and highlight the promising potential of our Change-Agent in facilitating comprehensive and intelligent interpretation of surface changes.","We will make our dataset and codebase of the change interpretation model and Change-Agent publicly available to facilitate future research at https://github.com/Chen-Yang-Liu/Change-Agent"],"url":"http://arxiv.org/abs/2403.19646v1","category":"cs.CV"}
{"created":"2024-03-28 17:55:16","title":"GANTASTIC: GAN-based Transfer of Interpretable Directions for Disentangled Image Editing in Text-to-Image Diffusion Models","abstract":"The rapid advancement in image generation models has predominantly been driven by diffusion models, which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual prompts. Despite their success, diffusion models encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched. In contrast, Generative Adversarial Networks (GANs) have been recognized for their success in disentangled edits through their interpretable latent spaces. We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained GAN models-representative of specific, controllable attributes-and transfers these directions into diffusion-based models. This novel approach not only maintains the generative quality and diversity that diffusion models are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds.","sentences":["The rapid advancement in image generation models has predominantly been driven by diffusion models, which have demonstrated unparalleled success in generating high-fidelity, diverse images from textual prompts.","Despite their success, diffusion models encounter substantial challenges in the domain of image editing, particularly in executing disentangled edits-changes that target specific attributes of an image while leaving irrelevant parts untouched.","In contrast, Generative Adversarial Networks (GANs) have been recognized for their success in disentangled edits through their interpretable latent spaces.","We introduce GANTASTIC, a novel framework that takes existing directions from pre-trained GAN models-representative of specific, controllable attributes-and transfers these directions into diffusion-based models.","This novel approach not only maintains the generative quality and diversity that diffusion models are known for but also significantly enhances their capability to perform precise, targeted image edits, thereby leveraging the best of both worlds."],"url":"http://arxiv.org/abs/2403.19645v1","category":"cs.CV"}
{"created":"2024-03-28 17:54:33","title":"Almost All Quantum Channels Are Diagonalizable","abstract":"We prove the statement \"The collection of all elements of $\\mathcal S$ which have only simple eigenvalues is dense in $\\mathcal S$\" for different sets $\\mathcal S$, including: all quantum channels, the unital channels, the positive trace-preserving maps, all Lindbladians (GKSL-generators), and all time-dependent Markovian channels. Therefore any element from each of these sets can always be approximated by diagonalizable elements of the same set to arbitrary precision.","sentences":["We prove the statement \"The collection of all elements of $\\mathcal S$ which have only simple eigenvalues is dense in $\\mathcal S$\" for different sets $\\mathcal S$, including: all quantum channels, the unital channels, the positive trace-preserving maps, all Lindbladians (GKSL-generators), and all time-dependent Markovian channels.","Therefore any element from each of these sets can always be approximated by diagonalizable elements of the same set to arbitrary precision."],"url":"http://arxiv.org/abs/2403.19643v1","category":"quant-ph"}
{"created":"2024-03-28 17:53:52","title":"Square patterns in dynamical orbits","abstract":"Let $q$ be an odd prime power. Let $f\\in \\mathbb{F}_q[x]$ be a polynomial having degree at least $2$, $a\\in \\mathbb{F}_q$, and denote by $f^n$ the $n$-th iteration of $f$. Let $\\chi$ be the quadratic character of $\\mathbb{F}_q$, and $\\mathcal{O}_f(a)$ the forward orbit of $a$ under iteration by $f$. Suppose that the sequence $(\\chi(f^n(a)))_{n\\geq 1}$ is periodic, and $m$ is its period. Assuming a mild and generic condition on $f$, we show that, up to a constant, $m$ can be bounded from below by $|\\mathcal{O}_f(a)|/q^\\frac{2\\log_{2}(d)+1}{2\\log_2(d)+2}$. More informally, we prove that the period of the appearance of squares in an orbit of an element provides an upper bound for the size of the orbit itself. Using a similar method, we can also prove that, up to a constant, we cannot have more than $q^\\frac{2\\log_2(d)+1}{2\\log_2(d)+2}$ consecutive squares or non-squares in the forward orbit of $a$. In addition, we provide a classification of all polynomials for which our generic condition does not hold.","sentences":["Let $q$ be an odd prime power.","Let $f\\in \\mathbb{F}_q[x]$ be a polynomial having degree at least $2$, $a\\in \\mathbb{F}_q$, and denote by $f^n$ the $n$-th iteration of $f$. Let $\\chi$ be the quadratic character of $\\mathbb{F}_q$, and $\\mathcal{O}_f(a)$ the forward orbit of $a$ under iteration by $f$. Suppose that the sequence $(\\chi(f^n(a)))_{n\\geq 1}$ is periodic, and $m$ is its period.","Assuming a mild and generic condition on $f$, we show that, up to a constant, $m$ can be bounded from below by $|\\mathcal{O}_f(a)|/q^\\frac{2\\log_{2}(d)+1}{2\\log_2(d)+2}$. More informally, we prove that the period of the appearance of squares in an orbit of an element provides an upper bound for the size of the orbit itself.","Using a similar method, we can also prove that, up to a constant, we cannot have more than $q^\\frac{2\\log_2(d)+1}{2\\log_2(d)+2}$ consecutive squares or non-squares in the forward orbit of $a$. In addition, we provide a classification of all polynomials for which our generic condition does not hold."],"url":"http://arxiv.org/abs/2403.19642v1","category":"math.NT"}
{"created":"2024-03-28 17:53:26","title":"Energy-Optimal Multi-Agent Navigation as a Strategic-Form Game","abstract":"This extended abstracts presents a method to generate energy-optimal trajectories for multi-agent systems as a strategic-form game. Using recent results in optimal control, we demonstrate that an energy-optimal trajectory can be generated in milliseconds if the sequence of constraint activations is known a priori. Thus, rather than selecting an infinite-dimensional action from a function space, the agents select their actions from a finite number of constraints and determine the time that each becomes active. Furthermore, the agents can exactly encode their trajectory in a set of real numbers, rather than communicating their control action as an infinite-dimensional function. We demonstrate the performance of this algorithm in simulation and find an optimal trajectory in 45 milliseconds on a tablet PC.","sentences":["This extended abstracts presents a method to generate energy-optimal trajectories for multi-agent systems as a strategic-form game.","Using recent results in optimal control, we demonstrate that an energy-optimal trajectory can be generated in milliseconds if the sequence of constraint activations is known a priori.","Thus, rather than selecting an infinite-dimensional action from a function space, the agents select their actions from a finite number of constraints and determine the time that each becomes active.","Furthermore, the agents can exactly encode their trajectory in a set of real numbers, rather than communicating their control action as an infinite-dimensional function.","We demonstrate the performance of this algorithm in simulation and find an optimal trajectory in 45 milliseconds on a tablet PC."],"url":"http://arxiv.org/abs/2403.19641v1","category":"math.OC"}
{"created":"2024-03-28 17:52:37","title":"Linear Programming in Isabelle/HOL","abstract":"Linear programming describes the problem of optimising a linear objective function over a set of constraints on its variables. In this paper we present a solver for linear programs implemented in the proof assistant Isabelle/HOL. This allows formally proving its soundness, termination, and other properties. We base these results on a previous formalisation of the simplex algorithm which does not take optimisation problems into account. Using the weak duality theorem of linear programming we obtain an algorithm for solving linear programs. Using Isabelle's code generation mechanism we can generate an external solver for linear programs.","sentences":["Linear programming describes the problem of optimising a linear objective function over a set of constraints on its variables.","In this paper we present a solver for linear programs implemented in the proof assistant Isabelle/HOL.","This allows formally proving its soundness, termination, and other properties.","We base these results on a previous formalisation of the simplex algorithm which does not take optimisation problems into account.","Using the weak duality theorem of linear programming we obtain an algorithm for solving linear programs.","Using Isabelle's code generation mechanism we can generate an external solver for linear programs."],"url":"http://arxiv.org/abs/2403.19639v1","category":"cs.LO"}
{"created":"2024-03-28 17:50:41","title":"In the driver's mind: modeling the dynamics of human overtaking decisions in interactions with oncoming automated vehicles","abstract":"Understanding human behavior in overtaking scenarios is crucial for enhancing road safety in mixed traffic with automated vehicles (AVs). Computational models of behavior play a pivotal role in advancing this understanding, as they can provide insight into human behavior generalizing beyond empirical studies. However, existing studies and models of human overtaking behavior have mostly focused on scenarios with simplistic, constant-speed dynamics of oncoming vehicles, disregarding the potential of AVs to proactively influence the decision-making process of the human drivers via implicit communication. Furthermore, so far it remained unknown whether overtaking decisions of human drivers are affected by whether they are interacting with an AV or a human-driven vehicle (HDV). To address these gaps, we conducted a \"reverse Wizard-of-Oz\" driving simulator experiment with 30 participants who repeatedly interacted with oncoming AVs and HDVs, measuring the drivers' gap acceptance decisions and response times. The oncoming vehicles featured time-varying dynamics designed to influence the overtaking decisions of the participants by briefly decelerating and then recovering to their initial speed. We found that participants did not alter their overtaking behavior when interacting with oncoming AVs compared to HDVs. Furthermore, we did not find any evidence of brief decelerations of the oncoming vehicle affecting the decisions or response times of the participants. Cognitive modeling of the obtained data revealed that a generalized drift-diffusion model with dynamic drift rate and velocity-dependent decision bias best explained the gap acceptance outcomes and response times observed in the experiment. Overall, our findings highlight the potential of cognitive models for further advancing the ongoing development of safer interactions between human drivers and AVs during overtaking maneuvers.","sentences":["Understanding human behavior in overtaking scenarios is crucial for enhancing road safety in mixed traffic with automated vehicles (AVs).","Computational models of behavior play a pivotal role in advancing this understanding, as they can provide insight into human behavior generalizing beyond empirical studies.","However, existing studies and models of human overtaking behavior have mostly focused on scenarios with simplistic, constant-speed dynamics of oncoming vehicles, disregarding the potential of AVs to proactively influence the decision-making process of the human drivers via implicit communication.","Furthermore, so far it remained unknown whether overtaking decisions of human drivers are affected by whether they are interacting with an AV or a human-driven vehicle (HDV).","To address these gaps, we conducted a \"reverse Wizard-of-Oz\" driving simulator experiment with 30 participants who repeatedly interacted with oncoming AVs and HDVs, measuring the drivers' gap acceptance decisions and response times.","The oncoming vehicles featured time-varying dynamics designed to influence the overtaking decisions of the participants by briefly decelerating and then recovering to their initial speed.","We found that participants did not alter their overtaking behavior when interacting with oncoming AVs compared to HDVs.","Furthermore, we did not find any evidence of brief decelerations of the oncoming vehicle affecting the decisions or response times of the participants.","Cognitive modeling of the obtained data revealed that a generalized drift-diffusion model with dynamic drift rate and velocity-dependent decision bias best explained the gap acceptance outcomes and response times observed in the experiment.","Overall, our findings highlight the potential of cognitive models for further advancing the ongoing development of safer interactions between human drivers and AVs during overtaking maneuvers."],"url":"http://arxiv.org/abs/2403.19637v1","category":"q-bio.NC"}
{"created":"2024-03-28 17:49:56","title":"Phase-Matched Generation of Coherent Soft-X-Rays","abstract":"Phase-matched harmonic conversion of visible laser light into soft x-rays was demonstrated. The recently developed technique of guided-wave frequency conversion was used to upshift light from 800 nanometers to the range from 17 to 32 nanometers. This process increased the coherent x-ray output by factors of 10^2 to 10^3 compared to the non-phase-matched case. This source uses a small-scale (sub-millijoule) high repetition-rate laser and will enable a wide variety of new experimental investigations in linear and nonlinear x-ray science.","sentences":["Phase-matched harmonic conversion of visible laser light into soft x-rays was demonstrated.","The recently developed technique of guided-wave frequency conversion was used to upshift light from 800 nanometers to the range from 17 to 32 nanometers.","This process increased the coherent x-ray output by factors of 10^2 to 10^3 compared to the non-phase-matched case.","This source uses a small-scale (sub-millijoule) high repetition-rate laser and will enable a wide variety of new experimental investigations in linear and nonlinear x-ray science."],"url":"http://arxiv.org/abs/2403.19636v1","category":"physics.optics"}
{"created":"2024-03-28 17:48:22","title":"Lane-Change in Dense Traffic with Model Predictive Control and Neural Networks","abstract":"This paper presents an online smooth-path lane-change control framework. We focus on dense traffic where inter-vehicle space gaps are narrow, and cooperation with surrounding drivers is essential to achieve the lane-change maneuver. We propose a two-stage control framework that harmonizes Model Predictive Control (MPC) with Generative Adversarial Networks (GAN) by utilizing driving intentions to generate smooth lane-change maneuvers. To improve performance in practice, the system is augmented with an adaptive safety boundary and a Kalman Filter to mitigate sensor noise. Simulation studies are investigated in different levels of traffic density and cooperativeness of other drivers. The simulation results support the effectiveness, driving comfort, and safety of the proposed method.","sentences":["This paper presents an online smooth-path lane-change control framework.","We focus on dense traffic where inter-vehicle space gaps are narrow, and cooperation with surrounding drivers is essential to achieve the lane-change maneuver.","We propose a two-stage control framework that harmonizes Model Predictive Control (MPC) with Generative Adversarial Networks (GAN) by utilizing driving intentions to generate smooth lane-change maneuvers.","To improve performance in practice, the system is augmented with an adaptive safety boundary and a Kalman Filter to mitigate sensor noise.","Simulation studies are investigated in different levels of traffic density and cooperativeness of other drivers.","The simulation results support the effectiveness, driving comfort, and safety of the proposed method."],"url":"http://arxiv.org/abs/2403.19633v1","category":"eess.SY"}
{"created":"2024-03-28 17:47:19","title":"Retrieval-Enhanced Knowledge Editing for Multi-Hop Question Answering in Language Models","abstract":"Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses. This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions. To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering. RAE first retrieves edited facts and then refines the language model through in-context learning. Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive similarity-based searches might miss. Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem. Our framework is supported by theoretical justification for its fact retrieval efficacy. Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge.","sentences":["Large Language Models (LLMs) have shown proficiency in question-answering tasks but often struggle to integrate real-time knowledge updates, leading to potentially outdated or inaccurate responses.","This problem becomes even more challenging when dealing with multi-hop questions since they require LLMs to update and integrate multiple knowledge pieces relevant to the questions.","To tackle the problem, we propose the Retrieval-Augmented model Editing (RAE) framework tailored for multi-hop question answering.","RAE first retrieves edited facts and then refines the language model through in-context learning.","Specifically, our retrieval approach, based on mutual information maximization, leverages the reasoning abilities of LLMs to identify chain facts that na\\\"ive similarity-based searches might miss.","Additionally, our framework incorporates a pruning strategy to eliminate redundant information from the retrieved facts, which enhances the editing accuracy and mitigates the hallucination problem.","Our framework is supported by theoretical justification for its fact retrieval efficacy.","Finally, comprehensive evaluation across various LLMs validates RAE's ability in providing accurate answers with updated knowledge."],"url":"http://arxiv.org/abs/2403.19631v1","category":"cs.CL"}
{"created":"2024-03-28 17:46:25","title":"Metric Learning from Limited Pairwise Preference Comparisons","abstract":"We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item. These items are embedded into $\\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users. While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons. We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible. We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users. However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric can be jointly identified. We present a divide-and-conquer approach that achieves this, and provide theoretical recovery guarantees and empirical validation.","sentences":["We study metric learning from preference comparisons under the ideal point model, in which a user prefers an item over another if it is closer to their latent ideal item.","These items are embedded into $\\mathbb{R}^d$ equipped with an unknown Mahalanobis distance shared across users.","While recent work shows that it is possible to simultaneously recover the metric and ideal items given $\\mathcal{O}(d)$ pairwise comparisons per user, in practice we often have a limited budget of $o(d)$ comparisons.","We study whether the metric can still be recovered, even though it is known that learning individual ideal items is now no longer possible.","We show that in general, $o(d)$ comparisons reveals no information about the metric, even with infinitely many users.","However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric can be jointly identified.","We present a divide-and-conquer approach that achieves this, and provide theoretical recovery guarantees and empirical validation."],"url":"http://arxiv.org/abs/2403.19629v1","category":"cs.LG"}
{"created":"2024-03-28 17:45:03","title":"On the large interaction asymptotics of the free energy density of the Ising chain with disordered centered external field","abstract":"This article completes [G. Giacomin, R. Greenblatt, ALEA 2022] by identifying explicitly the leading coefficient in the asymptotic development of the free energy density of the centered Random Field Ising Chain as the spin-spin interaction of the chain goes to infinity, under general assumptions on the disorder law.","sentences":["This article completes [G. Giacomin, R. Greenblatt, ALEA 2022] by identifying explicitly the leading coefficient in the asymptotic development of the free energy density of the centered Random Field Ising Chain as the spin-spin interaction of the chain goes to infinity, under general assumptions on the disorder law."],"url":"http://arxiv.org/abs/2403.19626v1","category":"math.PR"}
{"created":"2024-03-28 17:42:54","title":"RH20T-P: A Primitive-Level Robotic Dataset Towards Composable Generalization Agents","abstract":"The ultimate goals of robotic learning is to acquire a comprehensive and generalizable robotic system capable of performing both seen skills within the training distribution and unseen skills in novel environments. Recent progress in utilizing language models as high-level planners has demonstrated that the complexity of tasks can be reduced through decomposing them into primitive-level plans, making it possible to generalize on novel robotic tasks in a composable manner. Despite the promising future, the community is not yet adequately prepared for composable generalization agents, particularly due to the lack of primitive-level real-world robotic datasets. In this paper, we propose a primitive-level robotic dataset, namely RH20T-P, which contains about 33000 video clips covering 44 diverse and complicated robotic tasks. Each clip is manually annotated according to a set of meticulously designed primitive skills, facilitating the future development of composable generalization agents. To validate the effectiveness of RH20T-P, we also construct a potential and scalable agent based on RH20T-P, called RA-P. Equipped with two planners specialized in task decomposition and motion planning, RA-P can adapt to novel physical skills through composable generalization. Our website and videos can be found at https://sites.google.com/view/rh20t-primitive/main. Dataset and code will be made available soon.","sentences":["The ultimate goals of robotic learning is to acquire a comprehensive and generalizable robotic system capable of performing both seen skills within the training distribution and unseen skills in novel environments.","Recent progress in utilizing language models as high-level planners has demonstrated that the complexity of tasks can be reduced through decomposing them into primitive-level plans, making it possible to generalize on novel robotic tasks in a composable manner.","Despite the promising future, the community is not yet adequately prepared for composable generalization agents, particularly due to the lack of primitive-level real-world robotic datasets.","In this paper, we propose a primitive-level robotic dataset, namely RH20T-P, which contains about 33000 video clips covering 44 diverse and complicated robotic tasks.","Each clip is manually annotated according to a set of meticulously designed primitive skills, facilitating the future development of composable generalization agents.","To validate the effectiveness of RH20T-P, we also construct a potential and scalable agent based on RH20T-P, called RA-P. Equipped with two planners specialized in task decomposition and motion planning, RA-P can adapt to novel physical skills through composable generalization.","Our website and videos can be found at https://sites.google.com/view/rh20t-primitive/main.","Dataset and code will be made available soon."],"url":"http://arxiv.org/abs/2403.19622v1","category":"cs.RO"}
{"created":"2024-03-28 17:40:15","title":"Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models","abstract":"Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images. However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output. In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images. We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images. In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants. Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the images through local search based on an aesthetic measure. We evaluate the effectiveness of this approach by comparing the results produced by the automatic and collaborative interactive evolution. The results show that the proposed approach can generate highly attractive art images when the evolution is guided by collaborative human feedback.","sentences":["Generative Adversarial Networks (GANs) have shown great success in generating high quality images and are thus used as one of the main approaches to generate art images.","However, usually the image generation process involves sampling from the latent space of the learned art representations, allowing little control over the output.","In this work, we first employ GANs that are trained to produce creative images using an architecture known as Creative Adversarial Networks (CANs), then, we employ an evolutionary approach to navigate within the latent space of the models to discover images.","We use automatic aesthetic and collaborative interactive human evaluation metrics to assess the generated images.","In the human interactive evaluation case, we propose a collaborative evaluation based on the assessments of several participants.","Furthermore, we also experiment with an intelligent mutation operator that aims to improve the quality of the images through local search based on an aesthetic measure.","We evaluate the effectiveness of this approach by comparing the results produced by the automatic and collaborative interactive evolution.","The results show that the proposed approach can generate highly attractive art images when the evolution is guided by collaborative human feedback."],"url":"http://arxiv.org/abs/2403.19620v1","category":"cs.NE"}
{"created":"2024-03-28 17:37:45","title":"Existence of a global fundamental solution for H\u00f6rmander operators","abstract":"We prove that for a simply connected manifold $M$ - and a vast class of non-simply connected manifolds - the existence of a fundamental solution for a differential operator $\\mathcal L=\\sum_{\\alpha\\in\\mathbb N^q} r_\\alpha\\cdot X^\\alpha$ of finite degree over $M$, follows via a saturation method from the existence of a fundamental solution for the associated \"lifted\" operator over a group $G$. Given some smooth complete vector fields $X_1,\\dots, X_q$ on $M$, suppose that they generate a finite dimensional Lie algebra $\\mathfrak g$ satisfying the H\\\"ormander's condition. The simply connected Lie group $G$ is the unique such that $\\mbox{Lie}(G)\\cong\\mathfrak g$. It has the property that a right $G$-action exists over $M$, faithful and transitive, inducing a natural projection $E\\colon G\\to M$. We generalize an approach developed by Biagi and Bonfiglioli. In particular we represent the group $G$ as a direct product $M\\times G^z$ where the model fiber $G^z$ has a group structure. Our approach doesn't need any nilpotency hypothesis on the group $G$, therefore it broadens the spectrum of cases where techniques of the kind \"lifting and approximation\", as those introduce by the works of Rothschild, Stein, Goodman, can be applied.","sentences":["We prove that for a simply connected manifold $M$ - and a vast class of non-simply connected manifolds - the existence of a fundamental solution for a differential operator $\\mathcal L=\\sum_{\\alpha\\in\\mathbb N^q} r_\\alpha\\cdot X^\\alpha$ of finite degree over $M$, follows via a saturation method from the existence of a fundamental solution for the associated \"lifted\" operator over a group $G$. Given some smooth complete vector fields $X_1,\\dots, X_q$ on $M$, suppose that they generate a finite dimensional Lie algebra $\\mathfrak g$ satisfying the H\\\"ormander's condition.","The simply connected Lie group $G$ is the unique such that $\\mbox{Lie}(G)\\cong\\mathfrak g$.","It has the property that a right $G$-action exists over $M$, faithful and transitive, inducing a natural projection $E\\colon G\\to M$.","We generalize an approach developed by Biagi and Bonfiglioli.","In particular we represent the group $G$ as a direct product $M\\times G^z$ where the model fiber $G^z$ has a group structure.","Our approach doesn't need any nilpotency hypothesis on the group $G$, therefore it broadens the spectrum of cases where techniques of the kind \"lifting and approximation\", as those introduce by the works of Rothschild, Stein, Goodman, can be applied."],"url":"http://arxiv.org/abs/2403.19619v1","category":"math.AP"}
{"created":"2024-03-28 17:29:12","title":"Classical Kerr-Schild double copy in bigravity for maximally symmetric spacetimes","abstract":"A generalized Kerr-Schild ansatz is proposed for the two metrics in bigravity which leads to linear interactions between the metrics and can be studied in the context of the double copy. By contracting the resulting spin-2 field bigravity equations of motion using Killing vector fields, we arrive to the single and zeroth copy equations for our ansatz. For the case of stationary solutions, we obtain two Maxwell and two conformally coupled equations for the single and zeroth copy respectively, and we do not have linear interactions. In the time-dependent case we obtain equations for our fields which are coupled. By decoupling these equations and at the zeroth copy level, we obtain a massless and a massive field whose mass is proportional to the Fierz-Pauli mass and depends on the interaction coefficients of the interaction potential between the metrics.","sentences":["A generalized Kerr-Schild ansatz is proposed for the two metrics in bigravity which leads to linear interactions between the metrics and can be studied in the context of the double copy.","By contracting the resulting spin-2 field bigravity equations of motion using Killing vector fields, we arrive to the single and zeroth copy equations for our ansatz.","For the case of stationary solutions, we obtain two Maxwell and two conformally coupled equations for the single and zeroth copy respectively, and we do not have linear interactions.","In the time-dependent case we obtain equations for our fields which are coupled.","By decoupling these equations and at the zeroth copy level, we obtain a massless and a massive field whose mass is proportional to the Fierz-Pauli mass and depends on the interaction coefficients of the interaction potential between the metrics."],"url":"http://arxiv.org/abs/2403.19608v1","category":"gr-qc"}
{"created":"2024-03-28 17:28:32","title":"SAID-NeRF: Segmentation-AIDed NeRF for Depth Completion of Transparent Objects","abstract":"Acquiring accurate depth information of transparent objects using off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and Robotics. Depth estimation/completion methods are typically employed and trained on datasets with quality depth labels acquired from either simulation, additional sensors or specialized data collection setups and known 3d models. However, acquiring reliable depth information for datasets at scale is not straightforward, limiting training scalability and generalization. Neural Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide success in novel view synthesis and shape recovery. However, heuristics and controlled environments (lights, backgrounds, etc) are often required to accurately capture specular surfaces. In this paper, we propose using Visual Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to guide the NeRF reconstruction process for these objects via the simultaneous reconstruction of semantic fields and extensions to increase robustness. Our proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant performance on depth completion datasets for transparent objects and robotic grasping.","sentences":["Acquiring accurate depth information of transparent objects using off-the-shelf RGB-D cameras is a well-known challenge in Computer Vision and Robotics.","Depth estimation/completion methods are typically employed and trained on datasets with quality depth labels acquired from either simulation, additional sensors or specialized data collection setups and known 3d models.","However, acquiring reliable depth information for datasets at scale is not straightforward, limiting training scalability and generalization.","Neural Radiance Fields (NeRFs) are learning-free approaches and have demonstrated wide success in novel view synthesis and shape recovery.","However, heuristics and controlled environments (lights, backgrounds, etc) are often required to accurately capture specular surfaces.","In this paper, we propose using Visual Foundation Models (VFMs) for segmentation in a zero-shot, label-free way to guide the NeRF reconstruction process for these objects via the simultaneous reconstruction of semantic fields and extensions to increase robustness.","Our proposed method Segmentation-AIDed NeRF (SAID-NeRF) shows significant performance on depth completion datasets for transparent objects and robotic grasping."],"url":"http://arxiv.org/abs/2403.19607v1","category":"cs.RO"}
{"created":"2024-03-28 17:28:06","title":"Data-Adaptive Tradeoffs among Multiple Risks in Distribution-Free Prediction","abstract":"Decision-making pipelines are generally characterized by tradeoffs among various risk functions. It is often desirable to manage such tradeoffs in a data-adaptive manner. As we demonstrate, if this is done naively, state-of-the art uncertainty quantification methods can lead to significant violations of putative risk guarantees.   To address this issue, we develop methods that permit valid control of risk when threshold and tradeoff parameters are chosen adaptively. Our methodology supports monotone and nearly-monotone risks, but otherwise makes no distributional assumptions.   To illustrate the benefits of our approach, we carry out numerical experiments on synthetic data and the large-scale vision dataset MS-COCO.","sentences":["Decision-making pipelines are generally characterized by tradeoffs among various risk functions.","It is often desirable to manage such tradeoffs in a data-adaptive manner.","As we demonstrate, if this is done naively, state-of-the art uncertainty quantification methods can lead to significant violations of putative risk guarantees.   ","To address this issue, we develop methods that permit valid control of risk when threshold and tradeoff parameters are chosen adaptively.","Our methodology supports monotone and nearly-monotone risks, but otherwise makes no distributional assumptions.   ","To illustrate the benefits of our approach, we carry out numerical experiments on synthetic data and the large-scale vision dataset MS-COCO."],"url":"http://arxiv.org/abs/2403.19605v1","category":"stat.ME"}
{"created":"2024-03-28 17:27:44","title":"Semantic Map-based Generation of Navigation Instructions","abstract":"We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task. In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input. Conventional approaches employ a sequence of panorama images to generate navigation instructions. Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input. We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions. Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast scope for improvement. We release the code for data preparation and model training at https://github.com/chengzu-li/VLGen.","sentences":["We are interested in the generation of navigation instructions, either in their own right or as training material for robotic navigation task.","In this paper, we propose a new approach to navigation instruction generation by framing the problem as an image captioning task using semantic maps as visual input.","Conventional approaches employ a sequence of panorama images to generate navigation instructions.","Semantic maps abstract away from visual details and fuse the information in multiple panorama images into a single top-down representation, thereby reducing computational complexity to process the input.","We present a benchmark dataset for instruction generation using semantic maps, propose an initial model and ask human subjects to manually assess the quality of generated instructions.","Our initial investigations show promise in using semantic maps for instruction generation instead of a sequence of panorama images, but there is vast scope for improvement.","We release the code for data preparation and model training at https://github.com/chengzu-li/VLGen."],"url":"http://arxiv.org/abs/2403.19603v1","category":"cs.CL"}
{"created":"2024-03-28 17:26:40","title":"Topological Phases and Phase Transitions with Dipolar Symmetry Breaking","abstract":"Systems with dipole moment conservation have been of recent interest, as they realize both novel quantum dynamics and exotic ground state phases. In this work, we study some generic properties of 1-D and 2-D dipole-conserving fermionic models at integer fillings. We find that a dipolar symmetry-breaking phase can result in a mean-field band insulator whose topological indices can strongly affect the low-energy physics of the dipolar Goldstone modes. We study the 2-D topological phase transition of the mean-field ground states in the presence of the Goldstone modes. The critical theory resembles the 2+1d quantum electrodynamics coupled to massless Dirac fermions with some crucial differences and shows a novel quantum critical point featuring a nontrivial dynamical exponent. We also discuss the analogous case of 1-D dipole-conserving models and the role of topological invariants.","sentences":["Systems with dipole moment conservation have been of recent interest, as they realize both novel quantum dynamics and exotic ground state phases.","In this work, we study some generic properties of 1-D and 2-D dipole-conserving fermionic models at integer fillings.","We find that a dipolar symmetry-breaking phase can result in a mean-field band insulator whose topological indices can strongly affect the low-energy physics of the dipolar Goldstone modes.","We study the 2-D topological phase transition of the mean-field ground states in the presence of the Goldstone modes.","The critical theory resembles the 2+1d quantum electrodynamics coupled to massless Dirac fermions with some crucial differences and shows a novel quantum critical point featuring a nontrivial dynamical exponent.","We also discuss the analogous case of 1-D dipole-conserving models and the role of topological invariants."],"url":"http://arxiv.org/abs/2403.19601v1","category":"cond-mat.str-el"}
{"created":"2024-03-28 17:23:45","title":"Enhance Image Classification via Inter-Class Image Mixup with Diffusion Model","abstract":"Text-to-image (T2I) generative models have recently emerged as a powerful tool, enabling the creation of photo-realistic images and giving rise to a multitude of applications. However, the effective integration of T2I models into fundamental image classification tasks remains an open question. A prevalent strategy to bolster image classification performance is through augmenting the training set with synthetic images generated by T2I models. In this study, we scrutinize the shortcomings of both current generative and conventional data augmentation techniques. Our analysis reveals that these methods struggle to produce images that are both faithful (in terms of foreground objects) and diverse (in terms of background contexts) for domain-specific concepts. To tackle this challenge, we introduce an innovative inter-class data augmentation method known as Diff-Mix (https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by performing image translations between classes. Our empirical results demonstrate that Diff-Mix achieves a better balance between faithfulness and diversity, leading to a marked improvement in performance across diverse image classification scenarios, including few-shot, conventional, and long-tail classifications for domain-specific datasets.","sentences":["Text-to-image (T2I) generative models have recently emerged as a powerful tool, enabling the creation of photo-realistic images and giving rise to a multitude of applications.","However, the effective integration of T2I models into fundamental image classification tasks remains an open question.","A prevalent strategy to bolster image classification performance is through augmenting the training set with synthetic images generated by T2I models.","In this study, we scrutinize the shortcomings of both current generative and conventional data augmentation techniques.","Our analysis reveals that these methods struggle to produce images that are both faithful (in terms of foreground objects) and diverse (in terms of background contexts) for domain-specific concepts.","To tackle this challenge, we introduce an innovative inter-class data augmentation method known as Diff-Mix (https://github.com/Zhicaiwww/Diff-Mix), which enriches the dataset by performing image translations between classes.","Our empirical results demonstrate that Diff-Mix achieves a better balance between faithfulness and diversity, leading to a marked improvement in performance across diverse image classification scenarios, including few-shot, conventional, and long-tail classifications for domain-specific datasets."],"url":"http://arxiv.org/abs/2403.19600v1","category":"cs.CV"}
{"created":"2024-03-28 17:21:14","title":"Implications of purity constraints on light higgsinos","abstract":"The lightest supersymmetric particles could be higgsinos that have a small mixing with gauginos. If the lightest higgsino-like state makes up some or all of the dark matter with a thermal freezeout density, then its mass must be between about 100 and 1150 GeV, and dark matter searches put bounds on the amount of gaugino contamination that it can have. Motivated by the generally good agreement of flavor- and CP-violating observables with Standard Model predictions, I consider models in which the scalar particles of minimal supersymmetry are heavy enough to be essentially decoupled, except for the 125 GeV Higgs boson. I survey the resulting purity constraints as lower bounds on the gaugino masses and upper bounds on the higgsino mass splittings. I also discuss the mild excesses in recent soft lepton searches for charginos and neutralinos at the LHC, and show that they can be accommodated in these models if $\\tan\\beta$ is small and $\\mu$ is negative.","sentences":["The lightest supersymmetric particles could be higgsinos that have a small mixing with gauginos.","If the lightest higgsino-like state makes up some or all of the dark matter with a thermal freezeout density, then its mass must be between about 100 and 1150 GeV, and dark matter searches put bounds on the amount of gaugino contamination that it can have.","Motivated by the generally good agreement of flavor- and CP-violating observables with Standard Model predictions, I consider models in which the scalar particles of minimal supersymmetry are heavy enough to be essentially decoupled, except for the 125 GeV Higgs boson.","I survey the resulting purity constraints as lower bounds on the gaugino masses and upper bounds on the higgsino mass splittings.","I also discuss the mild excesses in recent soft lepton searches for charginos and neutralinos at the LHC, and show that they can be accommodated in these models if $\\tan\\beta$ is small and $\\mu$ is negative."],"url":"http://arxiv.org/abs/2403.19598v1","category":"hep-ph"}
{"created":"2024-03-28 17:19:16","title":"Situation Awareness for Driver-Centric Driving Style Adaptation","abstract":"There is evidence that the driving style of an autonomous vehicle is important to increase the acceptance and trust of the passengers. The driving situation has been found to have a significant influence on human driving behavior. However, current driving style models only partially incorporate driving environment information, limiting the alignment between an agent and the given situation. Therefore, we propose a situation-aware driving style model based on different visual feature encoders pretrained on fleet data, as well as driving behavior predictors, which are adapted to the driving style of a specific driver. Our experiments show that the proposed method outperforms static driving styles significantly and forms plausible situation clusters. Furthermore, we found that feature encoders pretrained on our dataset lead to more precise driving behavior modeling. In contrast, feature encoders pretrained supervised and unsupervised on different data sources lead to more specific situation clusters, which can be utilized to constrain and control the driving style adaptation for specific situations. Moreover, in a real-world setting, where driving style adaptation is happening iteratively, we found the MLP-based behavior predictors achieve good performance initially but suffer from catastrophic forgetting. In contrast, behavior predictors based on situationdependent statistics can learn iteratively from continuous data streams by design. Overall, our experiments show that important information for driving behavior prediction is contained within the visual feature encoder. The dataset is publicly available at huggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation.","sentences":["There is evidence that the driving style of an autonomous vehicle is important to increase the acceptance and trust of the passengers.","The driving situation has been found to have a significant influence on human driving behavior.","However, current driving style models only partially incorporate driving environment information, limiting the alignment between an agent and the given situation.","Therefore, we propose a situation-aware driving style model based on different visual feature encoders pretrained on fleet data, as well as driving behavior predictors, which are adapted to the driving style of a specific driver.","Our experiments show that the proposed method outperforms static driving styles significantly and forms plausible situation clusters.","Furthermore, we found that feature encoders pretrained on our dataset lead to more precise driving behavior modeling.","In contrast, feature encoders pretrained supervised and unsupervised on different data sources lead to more specific situation clusters, which can be utilized to constrain and control the driving style adaptation for specific situations.","Moreover, in a real-world setting, where driving style adaptation is happening iteratively, we found the MLP-based behavior predictors achieve good performance initially but suffer from catastrophic forgetting.","In contrast, behavior predictors based on situationdependent statistics can learn iteratively from continuous data streams by design.","Overall, our experiments show that important information for driving behavior prediction is contained within the visual feature encoder.","The dataset is publicly available at huggingface.co/datasets/jHaselberger/SADC-Situation-Awareness-for-Driver-Centric-Driving-Style-Adaptation."],"url":"http://arxiv.org/abs/2403.19595v1","category":"cs.CV"}
{"created":"2024-03-28 17:15:23","title":"Frame by Familiar Frame: Understanding Replication in Video Diffusion Models","abstract":"Building on the momentum of image generation diffusion models, there is an increasing interest in video-based diffusion models. However, video generation poses greater challenges due to its higher-dimensional nature, the scarcity of training data, and the complex spatiotemporal relationships involved. Image generation models, due to their extensive data requirements, have already strained computational resources to their limits. There have been instances of these models reproducing elements from the training samples, leading to concerns and even legal disputes over sample replication. Video diffusion models, which operate with even more constrained datasets and are tasked with generating both spatial and temporal content, may be more prone to replicating samples from their training sets. Compounding the issue, these models are often evaluated using metrics that inadvertently reward replication. In our paper, we present a systematic investigation into the phenomenon of sample replication in video diffusion models. We scrutinize various recent diffusion models for video synthesis, assessing their tendency to replicate spatial and temporal content in both unconditional and conditional generation scenarios. Our study identifies strategies that are less likely to lead to replication. Furthermore, we propose new evaluation strategies that take replication into account, offering a more accurate measure of a model's ability to generate the original content.","sentences":["Building on the momentum of image generation diffusion models, there is an increasing interest in video-based diffusion models.","However, video generation poses greater challenges due to its higher-dimensional nature, the scarcity of training data, and the complex spatiotemporal relationships involved.","Image generation models, due to their extensive data requirements, have already strained computational resources to their limits.","There have been instances of these models reproducing elements from the training samples, leading to concerns and even legal disputes over sample replication.","Video diffusion models, which operate with even more constrained datasets and are tasked with generating both spatial and temporal content, may be more prone to replicating samples from their training sets.","Compounding the issue, these models are often evaluated using metrics that inadvertently reward replication.","In our paper, we present a systematic investigation into the phenomenon of sample replication in video diffusion models.","We scrutinize various recent diffusion models for video synthesis, assessing their tendency to replicate spatial and temporal content in both unconditional and conditional generation scenarios.","Our study identifies strategies that are less likely to lead to replication.","Furthermore, we propose new evaluation strategies that take replication into account, offering a more accurate measure of a model's ability to generate the original content."],"url":"http://arxiv.org/abs/2403.19593v1","category":"cs.CV"}
{"created":"2024-03-28 17:12:57","title":"Non-Linear Matter Power Spectrum Modeling in Interacting Dark Energy Cosmologies","abstract":"Understanding the behavior of the matter power spectrum on non-linear scales beyond the $\\Lambda$CDM model is crucial for accurately predicting the large-scale structure (LSS) of the Universe in non-standard cosmologies. In this work, we present an analysis of the non-linear matter power spectrum within the framework of interacting dark energy-dark matter cosmologies (IDE). We employ N-body simulations and theoretical models to investigate the impact of IDE on these non-linear scales. Beginning with N-body simulations characterized by a fixed parameter space delineated by prior observational research, we adeptly fit the simulated spectra with a simple parametric function, achieving accuracy within 5\\%. Subsequently, we refine a modified halo model tailored to the IDE cosmology, exhibiting exceptional precision in fitting the simulations down to scales of approximately 1 h/Mpc. To assess the model's robustness, we conduct a forecast analysis for the Euclid survey, employing our refined model. We find that the coupling parameter $\\xi$ will be constrained to $\\sigma(\\xi) = 0.0046$. This marks a significant improvement by an order of magnitude compared to any other current observational tests documented in the literature. These primary findings pave the way for a novel preliminary approach, enabling the utilization of IDE models for observational constraints concerning LSS data on non-linear scales.","sentences":["Understanding the behavior of the matter power spectrum on non-linear scales beyond the $\\Lambda$CDM model is crucial for accurately predicting the large-scale structure (LSS) of the Universe in non-standard cosmologies.","In this work, we present an analysis of the non-linear matter power spectrum within the framework of interacting dark energy-dark matter cosmologies (IDE).","We employ N-body simulations and theoretical models to investigate the impact of IDE on these non-linear scales.","Beginning with N-body simulations characterized by a fixed parameter space delineated by prior observational research, we adeptly fit the simulated spectra with a simple parametric function, achieving accuracy within 5\\%.","Subsequently, we refine a modified halo model tailored to the IDE cosmology, exhibiting exceptional precision in fitting the simulations down to scales of approximately 1 h/Mpc.","To assess the model's robustness, we conduct a forecast analysis for the Euclid survey, employing our refined model.","We find that the coupling parameter $\\xi$ will be constrained to $\\sigma(\\xi) =","0.0046$.","This marks a significant improvement by an order of magnitude compared to any other current observational tests documented in the literature.","These primary findings pave the way for a novel preliminary approach, enabling the utilization of IDE models for observational constraints concerning LSS data on non-linear scales."],"url":"http://arxiv.org/abs/2403.19590v1","category":"astro-ph.CO"}
{"created":"2024-03-28 17:12:55","title":"TOD3Cap: Towards 3D Dense Captioning in Outdoor Scenes","abstract":"3D dense captioning stands as a cornerstone in achieving a comprehensive understanding of 3D scenes through natural language. It has recently witnessed remarkable achievements, particularly in indoor settings. However, the exploration of 3D dense captioning in outdoor scenes is hindered by two major challenges: 1) the \\textbf{domain gap} between indoor and outdoor scenes, such as dynamics and sparse visual inputs, makes it difficult to directly adapt existing indoor methods; 2) the \\textbf{lack of data} with comprehensive box-caption pair annotations specifically tailored for outdoor scenes. To this end, we introduce the new task of outdoor 3D dense captioning. As input, we assume a LiDAR point cloud and a set of RGB images captured by the panoramic camera rig. The expected output is a set of object boxes with captions. To tackle this task, we propose the TOD3Cap network, which leverages the BEV representation to generate object box proposals and integrates Relation Q-Former with LLaMA-Adapter to generate rich captions for these objects. We also introduce the TOD3Cap dataset, the largest one to our knowledge for 3D dense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K outdoor objects from 850 scenes. Notably, our TOD3Cap network can effectively localize and caption 3D objects in outdoor scenes, which outperforms baseline methods by a significant margin (+9.6 CiDEr@0.5IoU). Code, data, and models are publicly available at https://github.com/jxbbb/TOD3Cap.","sentences":["3D dense captioning stands as a cornerstone in achieving a comprehensive understanding of 3D scenes through natural language.","It has recently witnessed remarkable achievements, particularly in indoor settings.","However, the exploration of 3D dense captioning in outdoor scenes is hindered by two major challenges: 1) the \\textbf{domain gap} between indoor and outdoor scenes, such as dynamics and sparse visual inputs, makes it difficult to directly adapt existing indoor methods; 2) the \\textbf{lack of data} with comprehensive box-caption pair annotations specifically tailored for outdoor scenes.","To this end, we introduce the new task of outdoor 3D dense captioning.","As input, we assume a LiDAR point cloud and a set of RGB images captured by the panoramic camera rig.","The expected output is a set of object boxes with captions.","To tackle this task, we propose the TOD3Cap network, which leverages the BEV representation to generate object box proposals and integrates Relation Q-Former with LLaMA-Adapter to generate rich captions for these objects.","We also introduce the TOD3Cap dataset, the largest one to our knowledge for 3D dense captioning in outdoor scenes, which contains 2.3M descriptions of 64.3K outdoor objects from 850 scenes.","Notably, our TOD3Cap network can effectively localize and caption 3D objects in outdoor scenes, which outperforms baseline methods by a significant margin (+9.6 CiDEr@0.5IoU).","Code, data, and models are publicly available at https://github.com/jxbbb/TOD3Cap."],"url":"http://arxiv.org/abs/2403.19589v1","category":"cs.CV"}
{"created":"2024-03-28 17:08:12","title":"Refining tree-decompositions so that they display the k-blocks","abstract":"Carmesin and Gollin proved that every finite graph has a canonical tree-decomposition $(T, \\mathcal{V})$ of adhesion less than $k$ that efficiently distinguishes every two distinct $k$-profiles, and which has the further property that every separable $k$-block is equal to the unique part of $(T, \\mathcal{V})$ in which it is contained.   We give a shorter proof of this result by showing that such a tree-decomposition can in fact be obtained from any canonical tight tree-decomposition of adhesion less than $k$. For this, we decompose the parts of such a tree-decomposition by further tree-decompositions. As an application, we also obtain a generalization of Carmesin and Gollin's result to locally finite graphs.","sentences":["Carmesin and Gollin proved that every finite graph has a canonical tree-decomposition $(T, \\mathcal{V})$ of adhesion less than $k$ that efficiently distinguishes every two distinct $k$-profiles, and which has the further property that every separable $k$-block is equal to the unique part of $(T, \\mathcal{V})$ in which it is contained.   ","We give a shorter proof of this result by showing that such a tree-decomposition can in fact be obtained from any canonical tight tree-decomposition of adhesion less than $k$. For this, we decompose the parts of such a tree-decomposition by further tree-decompositions.","As an application, we also obtain a generalization of Carmesin and Gollin's result to locally finite graphs."],"url":"http://arxiv.org/abs/2403.19585v1","category":"math.CO"}
{"created":"2024-03-28 17:07:02","title":"Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation","abstract":"Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval.Traditional methods typically employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or retrieval, which identifying locations by matching images with a database of image-location pairs. However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels. To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task. This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with retrieval augmented generation. Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database. It then uniquely combines query results with images itself, forming elaborate prompts customized for LMMs. When tested on benchmark datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training.","sentences":["Geolocating precise locations from images presents a challenging problem in computer vision and information retrieval.","Traditional methods typically employ either classification, which dividing the Earth surface into grid cells and classifying images accordingly, or retrieval, which identifying locations by matching images with a database of image-location pairs.","However, classification-based approaches are limited by the cell size and cannot yield precise predictions, while retrieval-based systems usually suffer from poor search quality and inadequate coverage of the global landscape at varied scale and aggregation levels.","To overcome these drawbacks, we present Img2Loc, a novel system that redefines image geolocalization as a text generation task.","This is achieved using cutting-edge large multi-modality models like GPT4V or LLaVA with retrieval augmented generation.","Img2Loc first employs CLIP-based representations to generate an image-based coordinate query database.","It then uniquely combines query results with images itself, forming elaborate prompts customized for LMMs.","When tested on benchmark datasets such as Im2GPS3k and YFCC4k, Img2Loc not only surpasses the performance of previous state-of-the-art models but does so without any model training."],"url":"http://arxiv.org/abs/2403.19584v1","category":"cs.CV"}
{"created":"2024-03-28 17:05:33","title":"Generalized law of iterated logarithm for the Lorentz gas with infinite horizon","abstract":"We obtain a generalized law of iterated logarithm for a class of dependent processes with superdiffusive behaviour. Our results apply in particular to the Lorentz gas with infinite horizon.","sentences":["We obtain a generalized law of iterated logarithm for a class of dependent processes with superdiffusive behaviour.","Our results apply in particular to the Lorentz gas with infinite horizon."],"url":"http://arxiv.org/abs/2403.19582v1","category":"math.PR"}
{"created":"2024-03-28 17:04:00","title":"Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics","abstract":"We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour. We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT). Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks. Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks. Videos are available at https://www.robot-learning.uk/keypoint-action-tokens.","sentences":["We show that off-the-shelf text-based Transformers, with no additional training, can perform few-shot in-context visual imitation learning, mapping visual observations to action sequences that emulate the demonstrator's behaviour.","We achieve this by transforming visual observations (inputs) and trajectories of actions (outputs) into sequences of tokens that a text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a framework we call Keypoint Action Tokens (KAT).","Despite being trained only on language, we show that these Transformers excel at translating tokenised visual keypoint observations into action trajectories, performing on par or better than state-of-the-art imitation learning (diffusion policies) in the low-data regime on a suite of real-world, everyday tasks.","Rather than operating in the language domain as is typical, KAT leverages text-based Transformers to operate in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning, indicating promising new avenues for repurposing natural language models for embodied tasks.","Videos are available at https://www.robot-learning.uk/keypoint-action-tokens."],"url":"http://arxiv.org/abs/2403.19578v1","category":"cs.RO"}
{"created":"2024-03-28 17:03:44","title":"A Public and Reproducible Assessment of the Topics API on Real Data","abstract":"The Topics API for the web is Google's privacy-enhancing alternative to replace third-party cookies. Results of prior work have led to an ongoing discussion between Google and research communities about the capability of Topics to trade off both utility and privacy. The central point of contention is largely around the realism of the datasets used in these analyses and their reproducibility; researchers using data collected on a small sample of users or generating synthetic datasets, while Google's results are inferred from a private dataset. In this paper, we complement prior research by performing a reproducible assessment of the latest version of the Topics API on the largest and publicly available dataset of real browsing histories. First, we measure how unique and stable real users' interests are over time. Then, we evaluate if Topics can be used to fingerprint the users from these real browsing traces by adapting methodologies from prior privacy studies. Finally, we call on web actors to perform and enable reproducible evaluations by releasing anonymized distributions. We find that 46%, 55%, and 60% of the 1207 users in the dataset are uniquely re-identified across websites after only 1, 2, and 3 observations of their topics by advertisers, respectively. This paper shows on real data that Topics does not provide the same privacy guarantees to all users, further highlighting the need for public and reproducible evaluations of the claims made by new web proposals.","sentences":["The Topics API for the web is Google's privacy-enhancing alternative to replace third-party cookies.","Results of prior work have led to an ongoing discussion between Google and research communities about the capability of Topics to trade off both utility and privacy.","The central point of contention is largely around the realism of the datasets used in these analyses and their reproducibility;","researchers using data collected on a small sample of users or generating synthetic datasets, while Google's results are inferred from a private dataset.","In this paper, we complement prior research by performing a reproducible assessment of the latest version of the Topics API on the largest and publicly available dataset of real browsing histories.","First, we measure how unique and stable real users' interests are over time.","Then, we evaluate if Topics can be used to fingerprint the users from these real browsing traces by adapting methodologies from prior privacy studies.","Finally, we call on web actors to perform and enable reproducible evaluations by releasing anonymized distributions.","We find that 46%, 55%, and 60% of the 1207 users in the dataset are uniquely re-identified across websites after only 1, 2, and 3 observations of their topics by advertisers, respectively.","This paper shows on real data that Topics does not provide the same privacy guarantees to all users, further highlighting the need for public and reproducible evaluations of the claims made by new web proposals."],"url":"http://arxiv.org/abs/2403.19577v1","category":"cs.CR"}
{"created":"2024-03-28 17:02:12","title":"The complement of tropical curves in moderate position on tropical surfaces","abstract":"L\\'opez de Medrano, Rinc\\'on and Shaw defined the Chern classes on tropical manifolds as an extension of their theory of the Chern-Schwartz-MacPherson cycles on matroids. This makes it possible to define the Riemann-Roch number of tropical Cartier divisors on compact tropical manifolds. In this paper, we introduce the notion of a moderate position, and discuss a conjecture that the Riemann-Roch number $\\operatorname{RR}(X;D)$ of a tropical submanifold $D$ of codimension $1$ in moderate position on a compact tropical manifold $X$ is equal to the topological Euler characteristic of the complement $X\\setminus D$. In particular, we prove it and its generalization when $\\dim X=2$ and $X$ admits a Delzant face structure.","sentences":["L\\'opez de Medrano, Rinc\\'on and Shaw defined the Chern classes on tropical manifolds as an extension of their theory of the Chern-Schwartz-MacPherson cycles on matroids.","This makes it possible to define the Riemann-Roch number of tropical Cartier divisors on compact tropical manifolds.","In this paper, we introduce the notion of a moderate position, and discuss a conjecture that the Riemann-Roch number $\\operatorname{RR}(X;D)$ of a tropical submanifold $D$ of codimension $1$ in moderate position on a compact tropical manifold $X$ is equal to the topological Euler characteristic of the complement $X\\setminus D$. In particular, we prove it and its generalization when $\\dim X=2$ and $X$ admits a Delzant face structure."],"url":"http://arxiv.org/abs/2403.19576v1","category":"math.AG"}
{"created":"2024-03-28 16:56:39","title":"Swarm Characteristics Classification Using Neural Networks","abstract":"Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of 20 time steps, while also demonstrating graceful degradation down to 80% accuracy under 50% noise, as well as excellent scalability to swarm sizes from 10 to 100 agents. These capabilities are promising for real-time decision-making support in defense scenarios by rapidly inferring insights about swarm behavior.","sentences":["Understanding the characteristics of swarming autonomous agents is critical for defense and security applications.","This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts.","Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics.","We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers.","Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size.","Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of 20 time steps, while also demonstrating graceful degradation down to 80% accuracy under 50% noise, as well as excellent scalability to swarm sizes from 10 to 100 agents.","These capabilities are promising for real-time decision-making support in defense scenarios by rapidly inferring insights about swarm behavior."],"url":"http://arxiv.org/abs/2403.19572v1","category":"cs.LG"}
{"created":"2024-03-28 16:48:51","title":"Level-2 IFS Thermodynamic Formalism: Gibbs probabilities in the space of probabilities and the push-forward map","abstract":"We will denote by $\\mathcal{M}$ the space of Borel probabilities on the symbolic space $\\Omega=\\{1,2...,m\\}^\\mathbb{N}$. $\\mathcal{M}$ is equipped Monge-Kantorovich metric. We consider here the push-forward map $\\mathfrak{T}:\\mathcal{M} \\to \\mathcal{M}$ as a dynamical system. The space of Borel probabilities on $\\mathcal{M}$ is denoted by $\\mathfrak{M}$. Given a continuous function $A: \\mathcal{M}\\to \\mathbb{R}$, an {\\it a priori} probability $\\Pi_0$ on $\\mathcal{M}$, and a certain convolution operation acting on pairs of probabilities on $\\mathcal{M}$, we define an associated Level-2 IFS Ruelle operator. We show the existence of an eigenfunction and an eigenprobability $\\hat{\\Pi}\\in\\mathfrak{M}$ for such an operator. Under a normalization condition for $A$, we show the existence of some $\\mathfrak{T}$-invariant probabilities $\\hat{\\Pi}\\in\\mathfrak{M}.$ We are able to define the variational entropy of such $\\hat{\\Pi}$ and a related maximization pressure problem associated to $A$. In some particular examples, we show how to get eigenprobabilities solutions on $\\mathfrak{M}$ for the Level-2 Thermodynamic Formalism problem from eigenprobabilities on $\\mathcal{M}$ for the classical (Level-1) Thermodynamic Formalism. These examples highlight the fact that our approach is a natural generalization of the classic case.","sentences":["We will denote by $\\mathcal{M}$ the space of Borel probabilities on the symbolic space $\\Omega=\\{1,2...,m\\}^\\mathbb{N}$. $\\mathcal{M}$ is equipped Monge-Kantorovich metric.","We consider here the push-forward map $\\mathfrak{T}:\\mathcal{M} \\to \\mathcal{M}$ as a dynamical system.","The space of Borel probabilities on $\\mathcal{M}$ is denoted by $\\mathfrak{M}$. Given a continuous function $A: \\mathcal{M}\\to \\mathbb{R}$, an {\\it a priori} probability $\\Pi_0$ on $\\mathcal{M}$, and a certain convolution operation acting on pairs of probabilities on $\\mathcal{M}$, we define an associated Level-2 IFS Ruelle operator.","We show the existence of an eigenfunction and an eigenprobability $\\hat{\\Pi}\\in\\mathfrak{M}$ for such an operator.","Under a normalization condition for $A$, we show the existence of some $\\mathfrak{T}$-invariant probabilities $\\hat{\\Pi}\\in\\mathfrak{M}.$ We are able to define the variational entropy of such $\\hat{\\Pi}$ and a related maximization pressure problem associated to $A$.","In some particular examples, we show how to get eigenprobabilities solutions on $\\mathfrak{M}$ for the Level-2 Thermodynamic Formalism problem from eigenprobabilities on $\\mathcal{M}$ for the classical (Level-1) Thermodynamic Formalism.","These examples highlight the fact that our approach is a natural generalization of the classic case."],"url":"http://arxiv.org/abs/2403.19566v1","category":"math.DS"}
{"created":"2024-03-28 16:48:11","title":"Closedness of the singular locus and generation for derived categories","abstract":"This work is concerned with a relationship regarding the closedness of the singular locus of a Noetherian scheme and existence of classical generators in its category of coherent sheaves, associated bounded derived category, and singularity category. Particularly, we extend an observation initially made by Iyengar and Takahashi in the affine context to the global setting. Furthermore, we furnish an example a Noetherian scheme whose bounded derived category admits a classical generator, yet not every finite scheme over it exhibits the same property.","sentences":["This work is concerned with a relationship regarding the closedness of the singular locus of a Noetherian scheme and existence of classical generators in its category of coherent sheaves, associated bounded derived category, and singularity category.","Particularly, we extend an observation initially made by Iyengar and Takahashi in the affine context to the global setting.","Furthermore, we furnish an example a Noetherian scheme whose bounded derived category admits a classical generator, yet not every finite scheme over it exhibits the same property."],"url":"http://arxiv.org/abs/2403.19564v1","category":"math.AG"}
{"created":"2024-03-28 16:47:24","title":"Flexible Analysis of Individual Heterogeneity in Event Studies: Application to the Child Penalty","abstract":"We provide a practical toolkit for analyzing effect heterogeneity in event studies. We develop an estimation algorithm and adapt existing econometric results to provide its theoretical justification. We apply these tools to Dutch administrative data to study individual heterogeneity in the child-penalty (CP) context in three ways. First, we document significant heterogeneity in the individual-level CP trajectories, emphasizing the importance of going beyond the average CP. Second, we use individual-level estimates to examine the impact of childcare supply expansion policies. Our approach uncovers nonlinear treatment effects, challenging the conventional policy evaluation methods constrained to less flexible specifications. Third, we use the individual-level estimates as a regressor on the right-hand side to study the intergenerational elasticity of the CP between mothers and daughters. After adjusting for the measurement error bias, we find the elasticity of 24\\%. Our methodological framework contributes to empirical practice by offering a flexible approach tailored to specific research questions and contexts. We provide an open-source package ('unitdid') to facilitate widespread adoption.","sentences":["We provide a practical toolkit for analyzing effect heterogeneity in event studies.","We develop an estimation algorithm and adapt existing econometric results to provide its theoretical justification.","We apply these tools to Dutch administrative data to study individual heterogeneity in the child-penalty (CP) context in three ways.","First, we document significant heterogeneity in the individual-level CP trajectories, emphasizing the importance of going beyond the average CP.","Second, we use individual-level estimates to examine the impact of childcare supply expansion policies.","Our approach uncovers nonlinear treatment effects, challenging the conventional policy evaluation methods constrained to less flexible specifications.","Third, we use the individual-level estimates as a regressor on the right-hand side to study the intergenerational elasticity of the CP between mothers and daughters.","After adjusting for the measurement error bias, we find the elasticity of 24\\%.","Our methodological framework contributes to empirical practice by offering a flexible approach tailored to specific research questions and contexts.","We provide an open-source package ('unitdid') to facilitate widespread adoption."],"url":"http://arxiv.org/abs/2403.19563v1","category":"econ.GN"}
{"created":"2024-03-28 16:46:53","title":"Self-Improved Learning for Scalable Neural Combinatorial Optimization","abstract":"The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design. However, existing methods struggle with large-scale problems, hindering their practical applicability. To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization. Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data. Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training. In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead. Comprehensive experiments on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes in both uniform and real-world distributions demonstrate the superior scalability of our method.","sentences":["The end-to-end neural combinatorial optimization (NCO) method shows promising performance in solving complex combinatorial optimization problems without the need for expert design.","However, existing methods struggle with large-scale problems, hindering their practical applicability.","To overcome this limitation, this work proposes a novel Self-Improved Learning (SIL) method for better scalability of neural combinatorial optimization.","Specifically, we develop an efficient self-improved mechanism that enables direct model training on large-scale problem instances without any labeled data.","Powered by an innovative local reconstruction approach, this method can iteratively generate better solutions by itself as pseudo-labels to guide efficient model training.","In addition, we design a linear complexity attention mechanism for the model to efficiently handle large-scale combinatorial problem instances with low computation overhead.","Comprehensive experiments on the Travelling Salesman Problem (TSP) and the Capacitated Vehicle Routing Problem (CVRP) with up to 100K nodes in both uniform and real-world distributions demonstrate the superior scalability of our method."],"url":"http://arxiv.org/abs/2403.19561v1","category":"cs.LG"}
{"created":"2024-03-28 16:43:55","title":"Experimental determination of effective light transport properties in fully anisotropic media","abstract":"Structurally anisotropic materials are ubiquitous in several application fields, yet their accurate optical characterization remains challenging due to the lack of general models linking their scattering coefficients to the macroscopic transport observables, and the need to combine multiple measurements to retrieve their direction-dependent values. Here, we present an improved method for the experimental determination of light transport tensor coefficients from the diffusive rates measured along all three directions, based on transient transmittance measurements and a generalized Monte Carlo model. We apply our method to the characterization of light transport properties in two common anisotropic materials - polytetrafluoroethylene (PTFE) tape and paper - highlighting the magnitude of systematic deviations that are typically incurred when neglecting anisotropy.","sentences":["Structurally anisotropic materials are ubiquitous in several application fields, yet their accurate optical characterization remains challenging due to the lack of general models linking their scattering coefficients to the macroscopic transport observables, and the need to combine multiple measurements to retrieve their direction-dependent values.","Here, we present an improved method for the experimental determination of light transport tensor coefficients from the diffusive rates measured along all three directions, based on transient transmittance measurements and a generalized Monte Carlo model.","We apply our method to the characterization of light transport properties in two common anisotropic materials - polytetrafluoroethylene (PTFE) tape and paper - highlighting the magnitude of systematic deviations that are typically incurred when neglecting anisotropy."],"url":"http://arxiv.org/abs/2403.19558v1","category":"physics.optics"}
{"created":"2024-03-28 16:38:26","title":"On 5-cycles and strong 5-subtournaments in a tournament of odd order n","abstract":"Let $T$ be a tournament of odd order $n\\ge 5,$ $c_{m}(T)$ be the number of its $m$-cycles, and $s_{m}(T)$ be the number of its strongly connected $m$-subtournaments. Due to work of L.W. Beineke and F. Harary, it is well known that $s_{m}(T)\\le s_{m}(RLT_{n}),$ where $RLT_{n}$ is the regular locally transitive tournament of order $n.$ For $m=3$ and $m=4,$ $c_{m}(T)$ equals $s_{m}(T),$ but it is not so for $m\\ge 5.$ As J.W. Moon pointed out in his note in 1966, the problem of determining the maximum of $c_{m}(T)$ seems very difficult in general (i.e. for $m\\ge 5$). In the present paper, based on the Komarov-Mackey formula for $c_{5}(T)$ obtained recently, we prove that $c_{5}(T)\\le (n+1)n(n-1)(n-2)(n-3)/160$ with equality holding iff $T$ is doubly regular. A formula for $s_{5}(T)$ is also deduced. With the use of it, we show that $s_{5}(T)\\le (n+1)n(n-1)(n-3)(11n-47)/1920$ with equality holding iff $T=RLT_{n}$ or $n=7$ and $T$ is regular or $n=5$ and $T$ is strong. It is also proved that for a regular tournament $T$ of (odd) order $n\\ge 9,$ a lower bound $(n+1)n(n-1)(n-3)(17n-59)/3840\\le s_{5}(T)$ holds with equality iff $T$ is doubly regular. These results are compared with the ones recently obtained by the author for $c_{5}(T).$","sentences":["Let $T$ be a tournament of odd order $n\\ge 5,$ $c_{m}(T)$ be the number of its $m$-cycles, and $s_{m}(T)$ be the number of its strongly connected $m$-subtournaments.","Due to work of L.W. Beineke and F. Harary, it is well known that $s_{m}(T)\\le s_{m}(RLT_{n}),$ where $RLT_{n}$ is the regular locally transitive tournament of order $n.$ For $m=3$ and $m=4,$ $c_{m}(T)$ equals $s_{m}(T),$ but it is not so for $m\\ge 5.$ As J.W. Moon pointed out in his note in 1966, the problem of determining the maximum of $c_{m}(T)$ seems very difficult in general (i.e. for $m\\ge 5$).","In the present paper, based on the Komarov-Mackey formula for $c_{5}(T)$ obtained recently, we prove that $c_{5}(T)\\le (n+1)n(n-1)(n-2)(n-3)/160$ with equality holding iff $T$ is doubly regular.","A formula for $s_{5}(T)$ is also deduced.","With the use of it, we show that $s_{5}(T)\\le (n+1)n(n-1)(n-3)(11n-47)/1920$ with equality holding iff $T=RLT_{n}$ or $n=7$ and $T$ is regular or $n=5$ and $T$ is strong.","It is also proved that for a regular tournament $T$ of (odd) order $n\\ge 9,$ a lower bound $(n+1)n(n-1)(n-3)(17n-59)/3840\\le s_{5}(T)$ holds with equality iff $T$ is doubly regular.","These results are compared with the ones recently obtained by the author for $c_{5}(T).$"],"url":"http://arxiv.org/abs/2403.19555v1","category":"math.CO"}
{"created":"2024-03-28 16:35:39","title":"Entanglement-based quantum information protocols designed with silicon quantum dot platform","abstract":"Electron spins in silicon quantum dot platform provide great potential for quantum information processing due to excellent physical properties and modern fabrication technologies. Spin-based quantum bit (qubit) operations are intensively studied to realize universal logic gates with a high fidelity, fast gating operations, and basic programmability. Although recent experimental achievements can be considered as remarkable results for utilizing quantum computation, more advanced quantum information protocols should be demonstrated with a large number of qubit system to enable programmability of silicon devices. Here, we computationally explore entanglement-based quantum information protocols in electrically defined five silicon quantum dot system. To this end, device simulations are employed to demonstrate $1$-qubit gate and $2$-qubit gate operations. Additionally, we discuss the implementations of three applications: the generation of magic states, entanglement swapping, and quantum teleportation in our silicon device. All the results will secure the scalability of quantum information processing with electron spin qubits in silicon quantum dot system.","sentences":["Electron spins in silicon quantum dot platform provide great potential for quantum information processing due to excellent physical properties and modern fabrication technologies.","Spin-based quantum bit (qubit) operations are intensively studied to realize universal logic gates with a high fidelity, fast gating operations, and basic programmability.","Although recent experimental achievements can be considered as remarkable results for utilizing quantum computation, more advanced quantum information protocols should be demonstrated with a large number of qubit system to enable programmability of silicon devices.","Here, we computationally explore entanglement-based quantum information protocols in electrically defined five silicon quantum dot system.","To this end, device simulations are employed to demonstrate $1$-qubit gate and $2$-qubit gate operations.","Additionally, we discuss the implementations of three applications: the generation of magic states, entanglement swapping, and quantum teleportation in our silicon device.","All the results will secure the scalability of quantum information processing with electron spin qubits in silicon quantum dot system."],"url":"http://arxiv.org/abs/2403.19551v1","category":"quant-ph"}
{"created":"2024-03-28 16:28:38","title":"WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models","abstract":"Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.","sentences":["Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks.","Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts.","Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting.","We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance.","This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis."],"url":"http://arxiv.org/abs/2403.19548v1","category":"cs.CL"}
{"created":"2024-03-28 16:28:00","title":"On the definition of the spin charge in asymptotically-flat spacetimes","abstract":"We propose a solution to a classic problem in gravitational physics consisting of defining the spin associated with asymptotically-flat spacetimes. We advocate that the correct asymptotic symmetry algebra to approach this problem is the generalized-BMS algebra $\\textsf{gbms}$ instead of the BMS algebra used hitherto in the literature for which a notion of spin is generically unavailable. We approach the problem of defining the spin charges from the perspective of coadjoint orbits of $\\textsf{gbms}$ and construct the complete set of Casimir invariants that determine $\\textsf{gbms}$ coadjoint orbits, using the notion of vorticity for $\\textsf{gbms}$. This allows us to introduce spin charges for $\\textsf{gbms}$ as the generators of area-preserving diffeomorphisms forming its isotropy subalgebra. To elucidate the parallelism between our analysis and the Poincar\\'e case, we clarify several features of the Poincar\\'e embedding in $\\textsf{gbms}$ and reveal the presence of condensate fields associated with the symmetry breaking from $\\textsf{gbms}$ to Poincar\\'e. We also introduce the notion of a rest frame available only for this extended algebra. This allows us to construct, from the spin generator, the gravitational analog of the Pauli--Luba\\'nski pseudo-vector. Finally, we obtain the $\\textsf{gbms}$ moment map, which we use to construct the gravitational spin charges and gravitational Casimirs from their dual algebra counterparts.","sentences":["We propose a solution to a classic problem in gravitational physics consisting of defining the spin associated with asymptotically-flat spacetimes.","We advocate that the correct asymptotic symmetry algebra to approach this problem is the generalized-BMS algebra $\\textsf{gbms}$ instead of the BMS algebra used hitherto in the literature for which a notion of spin is generically unavailable.","We approach the problem of defining the spin charges from the perspective of coadjoint orbits of $\\textsf{gbms}$ and construct the complete set of Casimir invariants that determine $\\textsf{gbms}$ coadjoint orbits, using the notion of vorticity for $\\textsf{gbms}$. This allows us to introduce spin charges for $\\textsf{gbms}$ as the generators of area-preserving diffeomorphisms forming its isotropy subalgebra.","To elucidate the parallelism between our analysis and the Poincar\\'e case, we clarify several features of the Poincar\\'e embedding in $\\textsf{gbms}$ and reveal the presence of condensate fields associated with the symmetry breaking from $\\textsf{gbms}$ to Poincar\\'e.","We also introduce the notion of a rest frame available only for this extended algebra.","This allows us to construct, from the spin generator, the gravitational analog of the Pauli--Luba\\'nski pseudo-vector.","Finally, we obtain the $\\textsf{gbms}$ moment map, which we use to construct the gravitational spin charges and gravitational Casimirs from their dual algebra counterparts."],"url":"http://arxiv.org/abs/2403.19547v1","category":"hep-th"}
{"created":"2024-03-28 16:27:26","title":"Croissant: A Metadata Format for ML-Ready Datasets","abstract":"Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.","sentences":["Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point.","This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks.","Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI.","Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks."],"url":"http://arxiv.org/abs/2403.19546v1","category":"cs.LG"}
{"created":"2024-03-28 16:27:20","title":"Lamarckian Inheritance Improves Robot Evolution in Dynamic Environments","abstract":"This study explores the integration of Lamarckian system into evolutionary robotics (ER), comparing it with the traditional Darwinian model across various environments. By adopting Lamarckian principles, where robots inherit learned traits, alongside Darwinian learning without inheritance, we investigate adaptation in dynamic settings. Our research, conducted in six distinct environmental setups, demonstrates that Lamarckian systems outperform Darwinian ones in adaptability and efficiency, particularly in challenging conditions. Our analysis highlights the critical role of the interplay between controller \\& morphological evolution and environment adaptation, with parent-offspring similarities and newborn \\&survivors before and after learning providing insights into the effectiveness of trait inheritance. Our findings suggest Lamarckian principles could significantly advance autonomous system design, highlighting the potential for more adaptable and robust robotic solutions in complex, real-world applications. These theoretical insights were validated using real physical robots, bridging the gap between simulation and practical application.","sentences":["This study explores the integration of Lamarckian system into evolutionary robotics (ER), comparing it with the traditional Darwinian model across various environments.","By adopting Lamarckian principles, where robots inherit learned traits, alongside Darwinian learning without inheritance, we investigate adaptation in dynamic settings.","Our research, conducted in six distinct environmental setups, demonstrates that Lamarckian systems outperform Darwinian ones in adaptability and efficiency, particularly in challenging conditions.","Our analysis highlights the critical role of the interplay between controller \\& morphological evolution and environment adaptation, with parent-offspring similarities and newborn \\&survivors before and after learning providing insights into the effectiveness of trait inheritance.","Our findings suggest Lamarckian principles could significantly advance autonomous system design, highlighting the potential for more adaptable and robust robotic solutions in complex, real-world applications.","These theoretical insights were validated using real physical robots, bridging the gap between simulation and practical application."],"url":"http://arxiv.org/abs/2403.19545v1","category":"cs.RO"}
{"created":"2024-03-28 16:21:36","title":"Comment on \"Do near-threshold molecular states mix with neighboring $\\bar QQ$ states?\"","abstract":"I comment on a paper by Christoph Hanhart and Alexey Nefediev, published in Phys. Rev. D 106, 114003 (2022). The authors discuss the interpretation of mesons close to their lowest decay threshold and present a mechanism for the formation of molecular states. The proposed formalism is then applied to the axial-vector mesons $D_{s1}(2536)$ and $D_{s1}(2460)$, presenting two scenarios for the lighter meson, namely a $D^\\star K$ molecule or a compact $c\\bar{s}$ state. The authors argue that the latter hypothesis requires a fine-tuning of the mixing angle between the $J^{PC}=1^{++}$ and $J^{PC}=1^{+-}$ $C$-parity eigenstates.   In this Comment I show that no such fine-tuning is needed, as demonstrated in an article published in Phys. Rev. D 84, 094020 (2011), where a unitarized quark model was applied to the two $C$-parity eigenstates, coupled to several two-meson channels including $D^\\star K$. The coupled-channel dynamics naturally leads to a mixing angle very close to the required one. Moreover, I argue that the $D_1(2420)$ and $D_1(2430)$ axial-vectors, not considered by the authors, as well as a lattice simulation in Phys. Rev. D 90, 034510 (2014), also not mentioned by the authors, do not lend support to a molecular interpretation of the $D_{s1}(2460)$. I conclude with some more general remarks about mesons coupling to $S$-wave thresholds.","sentences":["I comment on a paper by Christoph Hanhart and Alexey Nefediev, published in Phys.","Rev. D 106, 114003 (2022).","The authors discuss the interpretation of mesons close to their lowest decay threshold and present a mechanism for the formation of molecular states.","The proposed formalism is then applied to the axial-vector mesons $D_{s1}(2536)$ and $D_{s1}(2460)$, presenting two scenarios for the lighter meson, namely a $D^\\star K$ molecule or a compact $c\\bar{s}$ state.","The authors argue that the latter hypothesis requires a fine-tuning of the mixing angle between the $J^{PC}=1^{++}$ and $J^{PC}=1^{+-}$ $C$-parity eigenstates.   ","In this Comment I show that no such fine-tuning is needed, as demonstrated in an article published in Phys.","Rev. D 84, 094020 (2011), where a unitarized quark model was applied to the two $C$-parity eigenstates, coupled to several two-meson channels including $D^\\star K$. The coupled-channel dynamics naturally leads to a mixing angle very close to the required one.","Moreover, I argue that the $D_1(2420)$ and $D_1(2430)$ axial-vectors, not considered by the authors, as well as a lattice simulation in Phys.","Rev. D 90, 034510 (2014), also not mentioned by the authors, do not lend support to a molecular interpretation of the $D_{s1}(2460)$. I conclude with some more general remarks about mesons coupling to $S$-wave thresholds."],"url":"http://arxiv.org/abs/2403.19541v1","category":"hep-ph"}
{"created":"2024-03-28 16:10:03","title":"Bright Coherent Ultrahigh Harmonics in the keV X-Ray Regime from Mid-Infrared Femtosecond Lasers","abstract":"High harmonic generation traditionally combines ~100 near-infrared laser photons, to generate bright, phase matched, extreme ultraviolet beams when the emission from many atoms adds constructively. Here we show that by guiding a mid-infrared femtosecond laser in a high pressure gas, ultrahigh harmonics can be generated up to orders > 5000, that emerge as a bright supercontinuum that spans the entire electromagnetic spectrum from the ultraviolet to > 1.6 keV, allowing in-principle the generation of pulses as short as 2.5 attoseconds. The multi-atmosphere gas pressures required for bright, phase matched emission also supports laser beam self-confinement, further enhancing the x-ray yield. Finally, the x-ray beam exhibits high spatial coherence, even though at high gas density, the recolliding electrons responsible for high harmonic generation encounter other atoms during the emission process.","sentences":["High harmonic generation traditionally combines ~100 near-infrared laser photons, to generate bright, phase matched, extreme ultraviolet beams when the emission from many atoms adds constructively.","Here we show that by guiding a mid-infrared femtosecond laser in a high pressure gas, ultrahigh harmonics can be generated up to orders > 5000, that emerge as a bright supercontinuum that spans the entire electromagnetic spectrum from the ultraviolet to > 1.6 keV, allowing in-principle the generation of pulses as short as 2.5 attoseconds.","The multi-atmosphere gas pressures required for bright, phase matched emission also supports laser beam self-confinement, further enhancing the x-ray yield.","Finally, the x-ray beam exhibits high spatial coherence, even though at high gas density, the recolliding electrons responsible for high harmonic generation encounter other atoms during the emission process."],"url":"http://arxiv.org/abs/2403.19535v1","category":"physics.optics"}
{"created":"2024-03-28 16:06:28","title":"Coherent control from quantum committment probabilities","abstract":"We introduce a general definition of a quantum committor in order to clarify reaction mechanisms and facilitate control in processes where coherent effects are important. With a quantum committor, we generalize the notion of a transition state to quantum superpositions and quantify the effect of interference on the progress of the reaction. The formalism is applicable to any linear quantum master equation supporting metastability for which absorbing boundary conditions designating the reactant and product states can be applied. We use this formalism to determine the dependence of the quantum transition state on coherences in a polaritonic system and optimize the initialization state of a conical intersection model to control reactive outcomes, achieving yields of the desired state approaching 100%. In addition to providing a practical tool, the quantum committor provides a conceptual framework for understanding reactions in cases when classical intuitions fail.","sentences":["We introduce a general definition of a quantum committor in order to clarify reaction mechanisms and facilitate control in processes where coherent effects are important.","With a quantum committor, we generalize the notion of a transition state to quantum superpositions and quantify the effect of interference on the progress of the reaction.","The formalism is applicable to any linear quantum master equation supporting metastability for which absorbing boundary conditions designating the reactant and product states can be applied.","We use this formalism to determine the dependence of the quantum transition state on coherences in a polaritonic system and optimize the initialization state of a conical intersection model to control reactive outcomes, achieving yields of the desired state approaching 100%.","In addition to providing a practical tool, the quantum committor provides a conceptual framework for understanding reactions in cases when classical intuitions fail."],"url":"http://arxiv.org/abs/2403.19533v1","category":"physics.chem-ph"}
{"created":"2024-03-28 16:06:13","title":"SecGraph: Towards SGX-based Efficient and Confidentiality-Preserving Graph Search","abstract":"Graphs have more expressive power and are widely researched in various search demand scenarios, compared with traditional relational and XML models. Today, many graph search services have been deployed on a third-party server, which can alleviate users from the burdens of maintaining large-scale graphs and huge computation costs. Nevertheless, outsourcing graph search services to the third-party server may invade users' privacy. PeGraph was recently proposed to achieve the encrypted search over the social graph. The main idea of PeGraph is to maintain two data structures XSet and TSet motivated by the OXT technology to support encrypted conductive search. However, PeGraph still has some limitations. First, PeGraph suffers from high communication and computation costs in search operations. Second, PeGraph cannot support encrypted search over dynamic graphs. In this paper, we propose an SGX-based efficient and confidentiality-preserving graph search scheme SecGraph that can support insertion and deletion operations. We first design a new proxy-token generation method to reduce the communication cost. Then, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to reduce the computation cost. Finally, we design a new dynamic version of TSet named Twin-TSet to enable encrypted search over dynamic graphs. We have demonstrated the confidentiality preservation property of SecGraph through rigorous security analysis. Experiment results show that SecGraph yields up to 208x improvement in search time compared with PeGraph and the communication cost in PeGraph is up to 540x larger than that in SecGraph.","sentences":["Graphs have more expressive power and are widely researched in various search demand scenarios, compared with traditional relational and XML models.","Today, many graph search services have been deployed on a third-party server, which can alleviate users from the burdens of maintaining large-scale graphs and huge computation costs.","Nevertheless, outsourcing graph search services to the third-party server may invade users' privacy.","PeGraph was recently proposed to achieve the encrypted search over the social graph.","The main idea of PeGraph is to maintain two data structures XSet and TSet motivated by the OXT technology to support encrypted conductive search.","However, PeGraph still has some limitations.","First, PeGraph suffers from high communication and computation costs in search operations.","Second, PeGraph cannot support encrypted search over dynamic graphs.","In this paper, we propose an SGX-based efficient and confidentiality-preserving graph search scheme SecGraph that can support insertion and deletion operations.","We first design a new proxy-token generation method to reduce the communication cost.","Then, we design an LDCF-encoded XSet based on the Logarithmic Dynamic Cuckoo Filter to reduce the computation cost.","Finally, we design a new dynamic version of TSet named Twin-TSet to enable encrypted search over dynamic graphs.","We have demonstrated the confidentiality preservation property of SecGraph through rigorous security analysis.","Experiment results show that SecGraph yields up to 208x improvement in search time compared with PeGraph and the communication cost in PeGraph is up to 540x larger than that in SecGraph."],"url":"http://arxiv.org/abs/2403.19531v1","category":"cs.CR"}
{"created":"2024-03-28 16:06:04","title":"Disentangling degree and tie strength heterogeneity in egocentric social networks","abstract":"The structure of personal networks reflects how we organise and maintain social relationships. The distribution of tie strengths in personal networks is heterogeneous, with a few close, emotionally intense relationships and a larger number of weaker ties. Recent results indicate this feature is universal across communication channels. Within this general pattern, there is a substantial and persistent inter-individual variation that is also similarly distributed among channels. The reason for the observed universality is yet unclear -- one possibility is that people's traits determine their personal network features on any channel. To address this hypothesis, we need to compare an individual's personal networks across channels, which is a non-trivial task: while we are interested in measuring the differences in tie strength heterogeneity, personal network size is also expected to vary a lot across channels. Therefore, for any measure that compares personal networks, one needs to understand the sensitivity with respect to network size. Here, we study different measures of personal network similarity and show that a recently introduced alter-preferentiality parameter and the Gini coefficient are equally suitable measures for tie strength heterogeneity, as they are fairly insensitive to differences in network size. With these measures, we show that the earlier observed individual-level persistence of personal network structure cannot be attributed to network size stability alone, but that the tie strength heterogeneity is persistent too. We also demonstrate the effectiveness of the two measures on multichannel data, where tie strength heterogeneity in personal networks is seen to moderately correlate for the same users across two communication channels (calls and text messages).","sentences":["The structure of personal networks reflects how we organise and maintain social relationships.","The distribution of tie strengths in personal networks is heterogeneous, with a few close, emotionally intense relationships and a larger number of weaker ties.","Recent results indicate this feature is universal across communication channels.","Within this general pattern, there is a substantial and persistent inter-individual variation that is also similarly distributed among channels.","The reason for the observed universality is yet unclear -- one possibility is that people's traits determine their personal network features on any channel.","To address this hypothesis, we need to compare an individual's personal networks across channels, which is a non-trivial task: while we are interested in measuring the differences in tie strength heterogeneity, personal network size is also expected to vary a lot across channels.","Therefore, for any measure that compares personal networks, one needs to understand the sensitivity with respect to network size.","Here, we study different measures of personal network similarity and show that a recently introduced alter-preferentiality parameter and the Gini coefficient are equally suitable measures for tie strength heterogeneity, as they are fairly insensitive to differences in network size.","With these measures, we show that the earlier observed individual-level persistence of personal network structure cannot be attributed to network size stability alone, but that the tie strength heterogeneity is persistent too.","We also demonstrate the effectiveness of the two measures on multichannel data, where tie strength heterogeneity in personal networks is seen to moderately correlate for the same users across two communication channels (calls and text messages)."],"url":"http://arxiv.org/abs/2403.19529v1","category":"physics.soc-ph"}
{"created":"2024-03-28 16:02:03","title":"Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation","abstract":"Category-level 6D object pose estimation aims to estimate the rotation, translation and size of unseen instances within specific categories. In this area, dense correspondence-based methods have achieved leading performance. However, they do not explicitly consider the local and global geometric information of different instances, resulting in poor generalization ability to unseen instances with significant shape variations. To deal with this problem, we propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learning method for category-level 6D object pose estimation (AG-Pose), which includes two key designs: (1) The first design is an Instance-Adaptive Keypoint Detection module, which can adaptively detect a set of sparse keypoints for various instances to represent their geometric structures. (2) The second design is a Geometric-Aware Feature Aggregation module, which can efficiently integrate the local and global geometric information into keypoint features. These two modules can work together to establish robust keypoint-level correspondences for unseen instances, thus enhancing the generalization ability of the model.Experimental results on CAMERA25 and REAL275 datasets show that the proposed AG-Pose outperforms state-of-the-art methods by a large margin without category-specific shape priors.","sentences":["Category-level 6D object pose estimation aims to estimate the rotation, translation and size of unseen instances within specific categories.","In this area, dense correspondence-based methods have achieved leading performance.","However, they do not explicitly consider the local and global geometric information of different instances, resulting in poor generalization ability to unseen instances with significant shape variations.","To deal with this problem, we propose a novel Instance-Adaptive and Geometric-Aware Keypoint Learning method for category-level 6D object pose estimation (AG-Pose), which includes two key designs: (1) The first design is an Instance-Adaptive Keypoint Detection module, which can adaptively detect a set of sparse keypoints for various instances to represent their geometric structures.","(2) The second design is a Geometric-Aware Feature Aggregation module, which can efficiently integrate the local and global geometric information into keypoint features.","These two modules can work together to establish robust keypoint-level correspondences for unseen instances, thus enhancing the generalization ability of the model.","Experimental results on CAMERA25 and REAL275 datasets show that the proposed AG-Pose outperforms state-of-the-art methods by a large margin without category-specific shape priors."],"url":"http://arxiv.org/abs/2403.19527v1","category":"cs.CV"}
{"created":"2024-03-28 15:54:59","title":"Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models","abstract":"In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks. In zero-shot scenarios, given a prompt like \"The capital of France is,\" task-specific attention heads extract the topic entity, such as \"France,\" from the context and pass it to subsequent MLPs to recall the required answer such as \"Paris.\" We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans. Through this method, we quantify the function of the MLP layer following these task-specific heads. In the residual stream, it either erases or amplifies the information originating from individual heads. Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer. These zero-shot mechanisms are also employed in few-shot scenarios. Additionally, we observed a widely existent anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions. We mitigate this suppression by leveraging our interpretation to improve factual recall performance. Our interpretations have been evaluated across various language models, from the GPT-2 families to 1.3B OPT, and across tasks covering different domains of factual knowledge.","sentences":["In this paper, we deeply explore the mechanisms employed by Transformer-based language models in factual recall tasks.","In zero-shot scenarios, given a prompt like \"The capital of France is,\" task-specific attention heads extract the topic entity, such as \"France,\" from the context and pass it to subsequent MLPs to recall the required answer such as \"Paris.\"","We introduce a novel analysis method aimed at decomposing the outputs of the MLP into components understandable by humans.","Through this method, we quantify the function of the MLP layer following these task-specific heads.","In the residual stream, it either erases or amplifies the information originating from individual heads.","Moreover, it generates a component that redirects the residual stream towards the direction of its expected answer.","These zero-shot mechanisms are also employed in few-shot scenarios.","Additionally, we observed a widely existent anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions.","We mitigate this suppression by leveraging our interpretation to improve factual recall performance.","Our interpretations have been evaluated across various language models, from the GPT-2 families to 1.3B OPT, and across tasks covering different domains of factual knowledge."],"url":"http://arxiv.org/abs/2403.19521v1","category":"cs.CL"}
{"created":"2024-03-28 15:52:33","title":"Doubly special relativity as a non-local quantum field theory","abstract":"In this work, we present the technical details of the discussion presented in [J.J. Relancio, L.Santamar\\'ia-Sanz (2024) arXiv:2403.18772], where we establish the basis of quantum theories of the free massive scalar, the massive fermionic, and the electromagnetic fields, in a doubly special relativity scenario. This construction is based on a geometrical interpretation of the kinematics of these kind of theories. In order to describe the modified actions, we find that a higher (indeed infinite) derivative field theory is needed, from which the deformed kinematics can be read. From our construction we are able to restrict the possible models of doubly special relativity to particular bases that preserve linear Lorentz invariance. We quantize the theories and also obtain a deformed version of the Maxwell equations. We analyze the electromagnetic vector potential either for an electric point-like source and a magnetic dipole. We observe that the electric and magnetic fields do not diverge at the origin for some models described with an anti de Sitter space but do for the de Sitter one in both problems.","sentences":["In this work, we present the technical details of the discussion presented in [J.J. Relancio, L.Santamar\\'ia-Sanz (2024) arXiv:2403.18772], where we establish the basis of quantum theories of the free massive scalar, the massive fermionic, and the electromagnetic fields, in a doubly special relativity scenario.","This construction is based on a geometrical interpretation of the kinematics of these kind of theories.","In order to describe the modified actions, we find that a higher (indeed infinite) derivative field theory is needed, from which the deformed kinematics can be read.","From our construction we are able to restrict the possible models of doubly special relativity to particular bases that preserve linear Lorentz invariance.","We quantize the theories and also obtain a deformed version of the Maxwell equations.","We analyze the electromagnetic vector potential either for an electric point-like source and a magnetic dipole.","We observe that the electric and magnetic fields do not diverge at the origin for some models described with an anti de Sitter space but do for the de Sitter one in both problems."],"url":"http://arxiv.org/abs/2403.19520v1","category":"hep-th"}
{"created":"2024-03-28 15:49:37","title":"Symmetry-guided inverse design of self-assembling multiscale DNA origami tilings","abstract":"Recent advances enable the creation of nanoscale building blocks with complex geometries and interaction specificities for self-assembly. This nearly boundless design space necessitates design principles for defining the mutual interactions between multiple particle species to target a user-specified complex structure or pattern. In this article, we develop a symmetry-based method to generate the interaction matrices that specify the assembly of two-dimensional tilings which we illustrate using equilateral triangles. By exploiting the allowed 2D symmetries, we develop an algorithmic approach by which any periodic 2D tiling can be generated from an arbitrarily large number of subunit species, notably addressing an unmet challenge of engineering 2D crystals with periodicities that can be arbitrarily larger than subunit size. To demonstrate the utility of our design approach, we encode specific interactions between triangular subunits synthesized by DNA origami and show that we can guide their self-assembly into tilings with a wide variety of symmetries, using up to 12 unique species of triangles. By conjugating specific triangles with gold nanoparticles, we fabricate gold-nanoparticle supercrystals whose lattice parameter spans up to 300 nm. Finally, to generate economical design rules, we compare the design economy of various tilings. In particular, we show that (1) higher symmetries allow assembly of larger unit cells with fewer subunits and (2) linear supercrystals can be designed more economically using linear primitive unit cells. This work provides a simple algorithmic approach to designing periodic assemblies, which may open new doors to the multiscale assembly of superlattices of nanostructured \"metatoms\" with engineered plasmonic functions.","sentences":["Recent advances enable the creation of nanoscale building blocks with complex geometries and interaction specificities for self-assembly.","This nearly boundless design space necessitates design principles for defining the mutual interactions between multiple particle species to target a user-specified complex structure or pattern.","In this article, we develop a symmetry-based method to generate the interaction matrices that specify the assembly of two-dimensional tilings which we illustrate using equilateral triangles.","By exploiting the allowed 2D symmetries, we develop an algorithmic approach by which any periodic 2D tiling can be generated from an arbitrarily large number of subunit species, notably addressing an unmet challenge of engineering 2D crystals with periodicities that can be arbitrarily larger than subunit size.","To demonstrate the utility of our design approach, we encode specific interactions between triangular subunits synthesized by DNA origami and show that we can guide their self-assembly into tilings with a wide variety of symmetries, using up to 12 unique species of triangles.","By conjugating specific triangles with gold nanoparticles, we fabricate gold-nanoparticle supercrystals whose lattice parameter spans up to 300 nm.","Finally, to generate economical design rules, we compare the design economy of various tilings.","In particular, we show that (1) higher symmetries allow assembly of larger unit cells with fewer subunits and (2) linear supercrystals can be designed more economically using linear primitive unit cells.","This work provides a simple algorithmic approach to designing periodic assemblies, which may open new doors to the multiscale assembly of superlattices of nanostructured \"metatoms\" with engineered plasmonic functions."],"url":"http://arxiv.org/abs/2403.19518v1","category":"cond-mat.soft"}
{"created":"2024-03-28 15:45:09","title":"On Bootstrapping Lasso in Generalized Linear Models and the Cross Validation","abstract":"Generalized linear models or GLM constitutes an important set of models which generalizes the ordinary linear regression by connecting the response variable with the covariates through arbitrary link functions. On the other hand, Lasso is a popular and easy to implement penalization method in regression when all the covariates are not relevant. However, Lasso generally has non-tractable asymptotic distribution and hence development of an alternative method of distributional approximation is required for the purpose of statistical inference. In this paper, we develop a Bootstrap method which works as an approximation of the distribution of the Lasso estimator for all the sub-models of GLM. To connect the distributional approximation theory based on the proposed Bootstrap method with the practical implementation of Lasso, we explore the asymptotic properties of K-fold cross validation-based penalty parameter. The results established essentially justifies drawing valid statistical inference regarding the unknown parameters based on the proposed Bootstrap method for any sub model of GLM after selecting the penalty parameter using K-fold cross validation. Good finite sample properties are also shown through a moderately large simulation study. The method is also implemented on a real data set.","sentences":["Generalized linear models or GLM constitutes an important set of models which generalizes the ordinary linear regression by connecting the response variable with the covariates through arbitrary link functions.","On the other hand, Lasso is a popular and easy to implement penalization method in regression when all the covariates are not relevant.","However, Lasso generally has non-tractable asymptotic distribution and hence development of an alternative method of distributional approximation is required for the purpose of statistical inference.","In this paper, we develop a Bootstrap method which works as an approximation of the distribution of the Lasso estimator for all the sub-models of GLM.","To connect the distributional approximation theory based on the proposed Bootstrap method with the practical implementation of Lasso, we explore the asymptotic properties of K-fold cross validation-based penalty parameter.","The results established essentially justifies drawing valid statistical inference regarding the unknown parameters based on the proposed Bootstrap method for any sub model of GLM after selecting the penalty parameter using K-fold cross validation.","Good finite sample properties are also shown through a moderately large simulation study.","The method is also implemented on a real data set."],"url":"http://arxiv.org/abs/2403.19515v1","category":"stat.ME"}
{"created":"2024-03-28 15:45:03","title":"CDIMC-net: Cognitive Deep Incomplete Multi-view Clustering Network","abstract":"In recent years, incomplete multi-view clustering, which studies the challenging multi-view clustering problem on missing views, has received growing research interests. Although a series of methods have been proposed to address this issue, the following problems still exist: 1) Almost all of the existing methods are based on shallow models, which is difficult to obtain discriminative common representations. 2) These methods are generally sensitive to noise or outliers since the negative samples are treated equally as the important samples. In this paper, we propose a novel incomplete multi-view clustering network, called Cognitive Deep Incomplete Multi-view Clustering Network (CDIMC-net), to address these issues. Specifically, it captures the high-level features and local structure of each view by incorporating the view-specific deep encoders and graph embedding strategy into a framework. Moreover, based on the human cognition, i.e., learning from easy to hard, it introduces a self-paced strategy to select the most confident samples for model training, which can reduce the negative influence of outliers. Experimental results on several incomplete datasets show that CDIMC-net outperforms the state-of-the-art incomplete multi-view clustering methods.","sentences":["In recent years, incomplete multi-view clustering, which studies the challenging multi-view clustering problem on missing views, has received growing research interests.","Although a series of methods have been proposed to address this issue, the following problems still exist: 1) Almost all of the existing methods are based on shallow models, which is difficult to obtain discriminative common representations.","2)","These methods are generally sensitive to noise or outliers since the negative samples are treated equally as the important samples.","In this paper, we propose a novel incomplete multi-view clustering network, called Cognitive Deep Incomplete Multi-view Clustering Network (CDIMC-net), to address these issues.","Specifically, it captures the high-level features and local structure of each view by incorporating the view-specific deep encoders and graph embedding strategy into a framework.","Moreover, based on the human cognition, i.e., learning from easy to hard, it introduces a self-paced strategy to select the most confident samples for model training, which can reduce the negative influence of outliers.","Experimental results on several incomplete datasets show that CDIMC-net outperforms the state-of-the-art incomplete multi-view clustering methods."],"url":"http://arxiv.org/abs/2403.19514v1","category":"cs.CV"}
{"created":"2024-03-28 15:44:32","title":"The profit-oriented hub line location problem with elastic demand","abstract":"This paper deals with an extension of the hub line location problem considering demand elasticity with respect to travel times. The proposed model aims to capture the impact the hub network topology has on demand. The objective is to maximize the total revenue generated by each unit of demand using the hub line. We propose mixed-integer nonlinear formulations to model this problem. We study some properties of the nonlinear objective function associated with these formulations. Due to the inherent complexity involved in solving these nonlinear formulations with state-of-the-art solvers, we also present alternative mixed-integer linear programming formulations. Computational results compare the proposed formulations and the benefits of the presented model using benchmark instances commonly used in hub location. Moreover, a sensitivity analysis study is carried out with real data from the city of Montreal, Canada, to demonstrate the added value of incorporating demand elasticity when using the proposed model for public transportation planning.","sentences":["This paper deals with an extension of the hub line location problem considering demand elasticity with respect to travel times.","The proposed model aims to capture the impact the hub network topology has on demand.","The objective is to maximize the total revenue generated by each unit of demand using the hub line.","We propose mixed-integer nonlinear formulations to model this problem.","We study some properties of the nonlinear objective function associated with these formulations.","Due to the inherent complexity involved in solving these nonlinear formulations with state-of-the-art solvers, we also present alternative mixed-integer linear programming formulations.","Computational results compare the proposed formulations and the benefits of the presented model using benchmark instances commonly used in hub location.","Moreover, a sensitivity analysis study is carried out with real data from the city of Montreal, Canada, to demonstrate the added value of incorporating demand elasticity when using the proposed model for public transportation planning."],"url":"http://arxiv.org/abs/2403.19513v1","category":"math.OC"}
{"created":"2024-03-28 15:44:18","title":"Improving Clinical NLP Performance through Language Model-Generated Synthetic Clinical Data","abstract":"Generative models have been showing potential for producing data in mass. This study explores the enhancement of clinical natural language processing performance by utilizing synthetic data generated from advanced language models. Promising results show feasible applications in such a high-stakes domain.","sentences":["Generative models have been showing potential for producing data in mass.","This study explores the enhancement of clinical natural language processing performance by utilizing synthetic data generated from advanced language models.","Promising results show feasible applications in such a high-stakes domain."],"url":"http://arxiv.org/abs/2403.19511v1","category":"cs.CL"}
{"created":"2024-03-28 15:42:07","title":"Phonetic Segmentation of the UCLA Phonetics Lab Archive","abstract":"Research in speech technologies and comparative linguistics depends on access to diverse and accessible speech data. The UCLA Phonetics Lab Archive is one of the earliest multilingual speech corpora, with long-form audio recordings and phonetic transcriptions for 314 languages (Ladefoged et al., 2009). Recently, 95 of these languages were time-aligned with word-level phonetic transcriptions (Li et al., 2021). Here we present VoxAngeles, a corpus of audited phonetic transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive, which uses the 95-language CMU re-release as our starting point. VoxAngeles also includes word- and phone-level segmentations from the original UCLA corpus, as well as phonetic measurements of word and phone durations, vowel formants, and vowel f0. This corpus enhances the usability of the original data, particularly for quantitative phonetic typology, as demonstrated through a case study of vowel intrinsic f0. We also discuss the utility of the VoxAngeles corpus for general research and pedagogy in crosslinguistic phonetics, as well as for low-resource and multilingual speech technologies. VoxAngeles is free to download and use under a CC-BY-NC 4.0 license.","sentences":["Research in speech technologies and comparative linguistics depends on access to diverse and accessible speech data.","The UCLA Phonetics Lab Archive is one of the earliest multilingual speech corpora, with long-form audio recordings and phonetic transcriptions for 314 languages (Ladefoged et al., 2009).","Recently, 95 of these languages were time-aligned with word-level phonetic transcriptions (Li et al., 2021).","Here we present VoxAngeles, a corpus of audited phonetic transcriptions and phone-level alignments of the UCLA Phonetics Lab Archive, which uses the 95-language CMU re-release as our starting point.","VoxAngeles also includes word- and phone-level segmentations from the original UCLA corpus, as well as phonetic measurements of word and phone durations, vowel formants, and vowel f0.","This corpus enhances the usability of the original data, particularly for quantitative phonetic typology, as demonstrated through a case study of vowel intrinsic f0.","We also discuss the utility of the VoxAngeles corpus for general research and pedagogy in crosslinguistic phonetics, as well as for low-resource and multilingual speech technologies.","VoxAngeles is free to download and use under a CC-BY-NC 4.0 license."],"url":"http://arxiv.org/abs/2403.19509v1","category":"cs.CL"}
{"created":"2024-03-28 15:41:43","title":"Debiasing Cardiac Imaging with Controlled Latent Diffusion Models","abstract":"The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks using a large-cohort study, specifically, the UK Biobank. We assess our method by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demonstrate the effectiveness of the proposed approach in mitigating dataset imbalances, such as the scarcity of younger patients or individuals with normal BMI level suffering from heart failure. This work represents a major step towards the adoption of synthetic data for the development of fair and generalizable models for medical classification tasks. Notably, we conduct all our experiments using a single, consumer-level GPU to highlight the feasibility of our approach within resource-constrained environments. Our code is available at https://github.com/faildeny/debiasing-cardiac-mri.","sentences":["The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data.","To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition.","We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks using a large-cohort study, specifically, the UK Biobank.","We assess our method by evaluating the realism of the generated images using established quantitative metrics.","Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples.","Our experiments demonstrate the effectiveness of the proposed approach in mitigating dataset imbalances, such as the scarcity of younger patients or individuals with normal BMI level suffering from heart failure.","This work represents a major step towards the adoption of synthetic data for the development of fair and generalizable models for medical classification tasks.","Notably, we conduct all our experiments using a single, consumer-level GPU to highlight the feasibility of our approach within resource-constrained environments.","Our code is available at https://github.com/faildeny/debiasing-cardiac-mri."],"url":"http://arxiv.org/abs/2403.19508v1","category":"eess.IV"}
{"created":"2024-03-28 15:35:24","title":"Overlap violations in external validity","abstract":"Estimating externally valid causal effects is a foundational problem in the social and biomedical sciences. Generalizing or transporting causal estimates from an experimental sample to a target population of interest relies on an overlap assumption between the experimental sample and the target population--i.e., all units in the target population must have a non-zero probability of being included in the experiment. In practice, having full overlap between an experimental sample and a target population can be implausible. In the following paper, we introduce a framework for considering external validity in the presence of overlap violations. We introduce a novel bias decomposition that parameterizes the bias from an overlap violation into two components: (1) the proportion of units omitted, and (2) the degree to which omitting the units moderates the treatment effect. The bias decomposition offers an intuitive and straightforward approach to conducting sensitivity analysis to assess robustness to overlap violations. Furthermore, we introduce a suite of sensitivity tools in the form of summary measures and benchmarking, which help researchers consider the plausibility of the overlap violations. We apply the proposed framework on an experiment evaluating the impact of a cash transfer program in Northern Uganda.","sentences":["Estimating externally valid causal effects is a foundational problem in the social and biomedical sciences.","Generalizing or transporting causal estimates from an experimental sample to a target population of interest relies on an overlap assumption between the experimental sample and the target population--i.e., all units in the target population must have a non-zero probability of being included in the experiment.","In practice, having full overlap between an experimental sample and a target population can be implausible.","In the following paper, we introduce a framework for considering external validity in the presence of overlap violations.","We introduce a novel bias decomposition that parameterizes the bias from an overlap violation into two components: (1) the proportion of units omitted, and (2) the degree to which omitting the units moderates the treatment effect.","The bias decomposition offers an intuitive and straightforward approach to conducting sensitivity analysis to assess robustness to overlap violations.","Furthermore, we introduce a suite of sensitivity tools in the form of summary measures and benchmarking, which help researchers consider the plausibility of the overlap violations.","We apply the proposed framework on an experiment evaluating the impact of a cash transfer program in Northern Uganda."],"url":"http://arxiv.org/abs/2403.19504v1","category":"stat.ME"}
{"created":"2024-03-28 15:33:17","title":"On the potential of quantum walks for modeling financial return distributions","abstract":"Accurate modeling of the temporal evolution of asset prices is crucial for understanding financial markets. We explore the potential of discrete-time quantum walks to model the evolution of asset prices. Return distributions obtained from a model based on the quantum walk algorithm are compared with those obtained from classical methodologies. We focus on specific limitations of the classical models, and illustrate that the quantum walk model possesses great flexibility in overcoming these. This includes the potential to generate asymmetric return distributions with complex market tendencies and higher probabilities for extreme events than in some of the classical models. Furthermore, the temporal evolution in the quantum walk possesses the potential to provide asset price dynamics.","sentences":["Accurate modeling of the temporal evolution of asset prices is crucial for understanding financial markets.","We explore the potential of discrete-time quantum walks to model the evolution of asset prices.","Return distributions obtained from a model based on the quantum walk algorithm are compared with those obtained from classical methodologies.","We focus on specific limitations of the classical models, and illustrate that the quantum walk model possesses great flexibility in overcoming these.","This includes the potential to generate asymmetric return distributions with complex market tendencies and higher probabilities for extreme events than in some of the classical models.","Furthermore, the temporal evolution in the quantum walk possesses the potential to provide asset price dynamics."],"url":"http://arxiv.org/abs/2403.19502v1","category":"q-fin.ST"}
{"created":"2024-03-28 15:23:52","title":"Segmentation tool for images of cracks","abstract":"Safety-critical infrastructures, such as bridges, are periodically inspected to check for existing damage, such as fatigue cracks and corrosion, and to guarantee the safe use of the infrastructure. Visual inspection is the most frequent type of general inspection, despite the fact that its detection capability is rather limited, especially for fatigue cracks. Machine learning algorithms can be used for augmenting the capability of classical visual inspection of bridge structures, however, the implementation of such an algorithm requires a massive annotated training dataset, which is time-consuming to produce. This paper proposes a semi-automatic crack segmentation tool that eases the manual segmentation of cracks on images needed to create a training dataset for a machine learning algorithm. Also, it can be used to measure the geometry of the crack. This tool makes use of an image processing algorithm, which was initially developed for the analysis of vascular systems on retinal images. The algorithm relies on a multi-orientation wavelet transform, which is applied to the image to construct the so-called \"orientation scores\", i.e. a modified version of the image. Afterwards, the filtered orientation scores are used to formulate an optimal path problem that identifies the crack. The globally optimal path between manually selected crack endpoints is computed, using a state-of-the-art geometric tracking method. The pixel-wise segmentation is done afterwards using the obtained crack path. The proposed method outperforms fully automatic methods and shows potential to be an adequate alternative to the manual data annotation.","sentences":["Safety-critical infrastructures, such as bridges, are periodically inspected to check for existing damage, such as fatigue cracks and corrosion, and to guarantee the safe use of the infrastructure.","Visual inspection is the most frequent type of general inspection, despite the fact that its detection capability is rather limited, especially for fatigue cracks.","Machine learning algorithms can be used for augmenting the capability of classical visual inspection of bridge structures, however, the implementation of such an algorithm requires a massive annotated training dataset, which is time-consuming to produce.","This paper proposes a semi-automatic crack segmentation tool that eases the manual segmentation of cracks on images needed to create a training dataset for a machine learning algorithm.","Also, it can be used to measure the geometry of the crack.","This tool makes use of an image processing algorithm, which was initially developed for the analysis of vascular systems on retinal images.","The algorithm relies on a multi-orientation wavelet transform, which is applied to the image to construct the so-called \"orientation scores\", i.e. a modified version of the image.","Afterwards, the filtered orientation scores are used to formulate an optimal path problem that identifies the crack.","The globally optimal path between manually selected crack endpoints is computed, using a state-of-the-art geometric tracking method.","The pixel-wise segmentation is done afterwards using the obtained crack path.","The proposed method outperforms fully automatic methods and shows potential to be an adequate alternative to the manual data annotation."],"url":"http://arxiv.org/abs/2403.19492v1","category":"cs.CV"}
{"created":"2024-03-28 15:23:51","title":"Effects of lunisolar perturbations on TianQin constellation: An analytical model","abstract":"TianQin is a proposed space-based gravitational-wave observatory mission that critically relies on the stability of an equilateral-triangle constellation. Comprising three satellites in high Earth orbits of a $ 10^5 $ km radius, this constellation's geometric configuration is significantly affected by gravitational perturbations, primarily originating from the Moon and the Sun. In this paper, we present an analytical model to quantify the effects of lunisolar perturbations on the TianQin constellation, derived using Lagrange's planetary equations. The model provides expressions for three kinematic indicators of the constellation: arm-lengths, relative line-of-sight velocities, and breathing angles. Analysis of these indicators reveals that lunisolar perturbations can distort the constellation triangle, resulting in three distinct variations: linear drift, bias, and fluctuation. Furthermore, it is shown that these distortions can be optimized to display solely fluctuating behavior, under certain predefined conditions. These results can serve as the theoretical foundation for numerical simulations and offer insights for engineering a stable constellation in the future.","sentences":["TianQin is a proposed space-based gravitational-wave observatory mission that critically relies on the stability of an equilateral-triangle constellation.","Comprising three satellites in high Earth orbits of a $ 10^5 $ km radius, this constellation's geometric configuration is significantly affected by gravitational perturbations, primarily originating from the Moon and the Sun.","In this paper, we present an analytical model to quantify the effects of lunisolar perturbations on the TianQin constellation, derived using Lagrange's planetary equations.","The model provides expressions for three kinematic indicators of the constellation: arm-lengths, relative line-of-sight velocities, and breathing angles.","Analysis of these indicators reveals that lunisolar perturbations can distort the constellation triangle, resulting in three distinct variations: linear drift, bias, and fluctuation.","Furthermore, it is shown that these distortions can be optimized to display solely fluctuating behavior, under certain predefined conditions.","These results can serve as the theoretical foundation for numerical simulations and offer insights for engineering a stable constellation in the future."],"url":"http://arxiv.org/abs/2403.19491v1","category":"gr-qc"}
{"created":"2024-03-28 15:21:23","title":"Evolving Assembly Code in an Adversarial Environment","abstract":"In this work, we evolve assembly code for the CodeGuru competition. The competition's goal is to create a survivor -- an assembly program that runs the longest in shared memory, by resisting attacks from adversary survivors and finding their weaknesses. For evolving top-notch solvers, we specify a Backus Normal Form (BNF) for the assembly language and synthesize the code from scratch using Genetic Programming (GP). We evaluate the survivors by running CodeGuru games against human-written winning survivors. Our evolved programs found weaknesses in the programs they were trained against and utilized them. In addition, we compare our approach with a Large-Language Model, demonstrating that the latter cannot generate a survivor that can win at any competition. This work has important applications for cyber-security, as we utilize evolution to detect weaknesses in survivors. The assembly BNF is domain-independent; thus, by modifying the fitness function, it can detect code weaknesses and help fix them. Finally, the CodeGuru competition offers a novel platform for analyzing GP and code evolution in adversarial environments. To support further research in this direction, we provide a thorough qualitative analysis of the evolved survivors and the weaknesses found.","sentences":["In this work, we evolve assembly code for the CodeGuru competition.","The competition's goal is to create a survivor -- an assembly program that runs the longest in shared memory, by resisting attacks from adversary survivors and finding their weaknesses.","For evolving top-notch solvers, we specify a Backus Normal Form (BNF) for the assembly language and synthesize the code from scratch using Genetic Programming (GP).","We evaluate the survivors by running CodeGuru games against human-written winning survivors.","Our evolved programs found weaknesses in the programs they were trained against and utilized them.","In addition, we compare our approach with a Large-Language Model, demonstrating that the latter cannot generate a survivor that can win at any competition.","This work has important applications for cyber-security, as we utilize evolution to detect weaknesses in survivors.","The assembly BNF is domain-independent; thus, by modifying the fitness function, it can detect code weaknesses and help fix them.","Finally, the CodeGuru competition offers a novel platform for analyzing GP and code evolution in adversarial environments.","To support further research in this direction, we provide a thorough qualitative analysis of the evolved survivors and the weaknesses found."],"url":"http://arxiv.org/abs/2403.19489v1","category":"cs.NE"}
{"created":"2024-03-28 15:20:25","title":"Mappings contracting triangles","abstract":"The aim of the current paper is to introduce a new class of contractive mappings, which are contracting (a feature of) triangles. We prove that maps contracting triangles are continuous and give the fixed point result for such mappings. We emphasize that our main theorem encompasses many functions, with significant applicability, for which the result holds, thereby representing a notable advancement in this research domain.","sentences":["The aim of the current paper is to introduce a new class of contractive mappings, which are contracting (a feature of) triangles.","We prove that maps contracting triangles are continuous and give the fixed point result for such mappings.","We emphasize that our main theorem encompasses many functions, with significant applicability, for which the result holds, thereby representing a notable advancement in this research domain."],"url":"http://arxiv.org/abs/2403.19488v1","category":"math.GN"}
{"created":"2024-03-28 15:19:34","title":"On the nonlinear thin obstacle problem","abstract":"The thin obstacle problem or $n$-dimensional Signorini problem is a classical variational problem arising in several applications, starting with its first introduction in elasticity theory. The vast literature concerns mostly quadratic energies, whereas only partial results have been proved in the nonlinear case. In this paper we consider the thin boundary obstacle problem for a general class of nonlineraities and we prove the optimal $C^{1, \\frac{1}{2}}$-regularity of the solutions in any space dimension.","sentences":["The thin obstacle problem or $n$-dimensional Signorini problem is a classical variational problem arising in several applications, starting with its first introduction in elasticity theory.","The vast literature concerns mostly quadratic energies, whereas only partial results have been proved in the nonlinear case.","In this paper we consider the thin boundary obstacle problem for a general class of nonlineraities and we prove the optimal $C^{1, \\frac{1}{2}}$-regularity of the solutions in any space dimension."],"url":"http://arxiv.org/abs/2403.19487v1","category":"math.AP"}
{"created":"2024-03-28 15:16:11","title":"Distributionally robust monopoly pricing: Switching from low to high prices in volatile markets","abstract":"Traditional monopoly pricing assumes sellers have full information about consumer valuations. We consider monopoly pricing under limited information, where a seller only knows the mean, variance and support of the valuation distribution. The objective is to maximize expected revenue by selecting the optimal fixed price. We adopt a distributionally robust framework, where the seller considers all valuation distributions that comply with the limited information. We formulate a maximin problem which seeks to maximize expected revenue for the worst-case valuation distribution. The minimization problem that identifies the worst-case valuation distribution is solved using primal-dual methods, and in turn leads to an explicitly solvable maximization problem. This yields a closed-form optimal pricing policy and a new fundamental principle prescribing when to use low and high robust prices. We show that the optimal policy switches from low to high prices when variance becomes sufficiently large, yielding significant performance gains compared with existing robust prices that generally decay with market uncertainty. This presents guidelines for when the seller should switch from targeting mass markets to niche markets. Similar guidelines are obtained for delay-prone services with rational utility-maximizing customers, underlining the universality and wide applicability of the low-high pricing principle.","sentences":["Traditional monopoly pricing assumes sellers have full information about consumer valuations.","We consider monopoly pricing under limited information, where a seller only knows the mean, variance and support of the valuation distribution.","The objective is to maximize expected revenue by selecting the optimal fixed price.","We adopt a distributionally robust framework, where the seller considers all valuation distributions that comply with the limited information.","We formulate a maximin problem which seeks to maximize expected revenue for the worst-case valuation distribution.","The minimization problem that identifies the worst-case valuation distribution is solved using primal-dual methods, and in turn leads to an explicitly solvable maximization problem.","This yields a closed-form optimal pricing policy and a new fundamental principle prescribing when to use low and high robust prices.","We show that the optimal policy switches from low to high prices when variance becomes sufficiently large, yielding significant performance gains compared with existing robust prices that generally decay with market uncertainty.","This presents guidelines for when the seller should switch from targeting mass markets to niche markets.","Similar guidelines are obtained for delay-prone services with rational utility-maximizing customers, underlining the universality and wide applicability of the low-high pricing principle."],"url":"http://arxiv.org/abs/2403.19486v1","category":"math.OC"}
{"created":"2024-03-28 15:10:02","title":"The linear sampling method for data generated by small random scatterers","abstract":"We present an extension of the linear sampling method for solving the sound-soft inverse scattering problem in two dimensions with data generated by randomly distributed small scatterers. The theoretical justification of our novel sampling method is based on a rigorous asymptotic model, a modified Helmholtz--Kirchhoff identity, and our previous work on the linear sampling method for random sources. Our numerical implementation incorporates boundary elements, Singular Value Decomposition, Tikhonov regularization, and Morozov's discrepancy principle. We showcase the robustness and accuracy of our algorithms with a series of numerical experiments.","sentences":["We present an extension of the linear sampling method for solving the sound-soft inverse scattering problem in two dimensions with data generated by randomly distributed small scatterers.","The theoretical justification of our novel sampling method is based on a rigorous asymptotic model, a modified Helmholtz--Kirchhoff identity, and our previous work on the linear sampling method for random sources.","Our numerical implementation incorporates boundary elements, Singular Value Decomposition, Tikhonov regularization, and Morozov's discrepancy principle.","We showcase the robustness and accuracy of our algorithms with a series of numerical experiments."],"url":"http://arxiv.org/abs/2403.19482v1","category":"math.NA"}
{"created":"2024-03-28 15:08:51","title":"$H$-Consistency Guarantees for Regression","abstract":"We present a detailed study of $H$-consistency bounds for regression. We first present new theorems that generalize the tools previously given to establish $H$-consistency bounds. This generalization proves essential for analyzing $H$-consistency bounds specific to regression. Next, we prove a series of novel $H$-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set. This includes positive results for the Huber loss, all $\\ell_p$ losses, $p \\geq 1$, the squared $\\epsilon$-insensitive loss, as well as a negative result for the $\\epsilon$-insensitive loss used in squared Support Vector Regression (SVR). We further leverage our analysis of $H$-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5). This readily establishes novel algorithms for adversarial regression, for which we report favorable experimental results in Section 6.","sentences":["We present a detailed study of $H$-consistency bounds for regression.","We first present new theorems that generalize the tools previously given to establish $H$-consistency bounds.","This generalization proves essential for analyzing $H$-consistency bounds specific to regression.","Next, we prove a series of novel $H$-consistency bounds for surrogate loss functions of the squared loss, under the assumption of a symmetric distribution and a bounded hypothesis set.","This includes positive results for the Huber loss, all $\\ell_p$ losses, $p \\geq 1$, the squared $\\epsilon$-insensitive loss, as well as a negative result for the $\\epsilon$-insensitive loss used in squared Support Vector Regression (SVR).","We further leverage our analysis of $H$-consistency for regression and derive principled surrogate losses for adversarial regression (Section 5).","This readily establishes novel algorithms for adversarial regression, for which we report favorable experimental results in Section 6."],"url":"http://arxiv.org/abs/2403.19480v1","category":"cs.LG"}
{"created":"2024-03-28 15:05:39","title":"Parallel and real-time post-processing for quantum random number generators","abstract":"Quantum systems are particularly suited for generating true randomness due to their inherent unpredictability, which can be justified on physical principles. However, practical implementations of Quantum RNGs (QRNGs) are always subject to noise, or uncontrollable influences, diminishing the quality of raw randomness produced. This necessitates post-processing to convert raw output into genuine randomness. In current QRNG implementations, the critical issue of seed updating is often overlooked, risking security vulnerabilities due to increased security parameters when seeds are reused in post-processing, and frequent seed updates fail to yield net randomness, while reusing seeds relies on the assumption that the original sequence inputs are independent.In this work, we have provided a specific scheme for seed updates that balances practicality and security, exploring the parallel and real-time implementation of multiple seed real-time updating toeplitz hash extractors in an FPGA to achieve parallel QRNGs, focusing on efficient hardware computation resource use. Through logic optimization, we achieved a greater number of parallel channels and a post-processing matrix size three times larger than previous works on the same FPGA platform, utilizing fewer logic resources. This resulted in a higher rate of random number generation and enhanced security. Furthermore, with the use of higher-performance ADCs, we attained a random number production rate exceeding 20Gbps.High-speed random number transfer and seed updating were achieved using the PCIe high-speed interface.This marks a significant step toward chip-based parallel QRNGs, enhancing the practicality of CV QRNGs in trusted, device-independent, and semi-device-independent scenarios.","sentences":["Quantum systems are particularly suited for generating true randomness due to their inherent unpredictability, which can be justified on physical principles.","However, practical implementations of Quantum RNGs (QRNGs) are always subject to noise, or uncontrollable influences, diminishing the quality of raw randomness produced.","This necessitates post-processing to convert raw output into genuine randomness.","In current QRNG implementations, the critical issue of seed updating is often overlooked, risking security vulnerabilities due to increased security parameters when seeds are reused in post-processing, and frequent seed updates fail to yield net randomness, while reusing seeds relies on the assumption that the original sequence inputs are independent.","In this work, we have provided a specific scheme for seed updates that balances practicality and security, exploring the parallel and real-time implementation of multiple seed real-time updating toeplitz hash extractors in an FPGA to achieve parallel QRNGs, focusing on efficient hardware computation resource use.","Through logic optimization, we achieved a greater number of parallel channels and a post-processing matrix size three times larger than previous works on the same FPGA platform, utilizing fewer logic resources.","This resulted in a higher rate of random number generation and enhanced security.","Furthermore, with the use of higher-performance ADCs, we attained a random number production rate exceeding 20Gbps.","High-speed random number transfer and seed updating were achieved using the PCIe high-speed interface.","This marks a significant step toward chip-based parallel QRNGs, enhancing the practicality of CV QRNGs in trusted, device-independent, and semi-device-independent scenarios."],"url":"http://arxiv.org/abs/2403.19479v1","category":"quant-ph"}
{"created":"2024-03-28 15:03:05","title":"Real-time Geoinformation Systems to Improve the Quality, Scalability, and Cost of Internet of Things for Agri-environment Research","abstract":"With the increasing emphasis on machine learning and artificial intelligence to drive knowledge discovery in the agricultural sciences, spatial internet of things (IoT) technologies have become increasingly important for collecting real-time, high resolution data for these models. However, managing large fleets of devices while maintaining high data quality remains an ongoing challenge as scientists iterate from prototype to mature end-to-end applications. Here, we provide a set of case studies using the framework of technology readiness levels for an open source spatial IoT system. The spatial IoT systems underwent 3 major and 14 minor system versions, had over 2,727 devices manufactured both in academic and commercial contexts, and are either in active or planned deployment across four continents. Our results show the evolution of a generalizable, open source spatial IoT system designed for agricultural scientists, and provide a model for academic researchers to overcome the challenges that exist in going from one-off prototypes to thousands of internet-connected devices.","sentences":["With the increasing emphasis on machine learning and artificial intelligence to drive knowledge discovery in the agricultural sciences, spatial internet of things (IoT) technologies have become increasingly important for collecting real-time, high resolution data for these models.","However, managing large fleets of devices while maintaining high data quality remains an ongoing challenge as scientists iterate from prototype to mature end-to-end applications.","Here, we provide a set of case studies using the framework of technology readiness levels for an open source spatial IoT system.","The spatial IoT systems underwent 3 major and 14 minor system versions, had over 2,727 devices manufactured both in academic and commercial contexts, and are either in active or planned deployment across four continents.","Our results show the evolution of a generalizable, open source spatial IoT system designed for agricultural scientists, and provide a model for academic researchers to overcome the challenges that exist in going from one-off prototypes to thousands of internet-connected devices."],"url":"http://arxiv.org/abs/2403.19477v1","category":"q-bio.QM"}
{"created":"2024-03-28 15:02:36","title":"Solving the waste bin location problem with uncertain waste generation rate: a bi-objective robust optimization approach","abstract":"An efficient Municipal solid waste (MSW) system is critical to modern cities in order to enhance sustainability and livability of urban life. With this aim, the planning phase of the MSW system should be carefully addressed by decision makers. However, planning success is dependent on many sources of uncertainty that can affect key parameters of the system, e.g., the waste generation rate in an urban area. With this in mind, this paper contributes with a robust optimization model to design the network of collection points (i.e., location and storage capacity), which are the first points of contact with the MSW system. A central feature of the model is a bi-objective function that aims at simultaneously minimizing the network costs of collection points and the required collection frequency to gather the accumulated waste (as a proxy of the collection cost). The value of the model is demonstrated by comparing its solutions with those obtained from its deterministic counterpart over a set of realistic instances considering different scenarios defined by different waste generation rates. The results show that the robust model finds competitive solutions in almost all cases investigated. An additional benefit of the model is that it allows the user to explore trade-offs between the two objectives.","sentences":["An efficient Municipal solid waste (MSW) system is critical to modern cities in order to enhance sustainability and livability of urban life.","With this aim, the planning phase of the MSW system should be carefully addressed by decision makers.","However, planning success is dependent on many sources of uncertainty that can affect key parameters of the system, e.g., the waste generation rate in an urban area.","With this in mind, this paper contributes with a robust optimization model to design the network of collection points (i.e., location and storage capacity), which are the first points of contact with the MSW system.","A central feature of the model is a bi-objective function that aims at simultaneously minimizing the network costs of collection points and the required collection frequency to gather the accumulated waste (as a proxy of the collection cost).","The value of the model is demonstrated by comparing its solutions with those obtained from its deterministic counterpart over a set of realistic instances considering different scenarios defined by different waste generation rates.","The results show that the robust model finds competitive solutions in almost all cases investigated.","An additional benefit of the model is that it allows the user to explore trade-offs between the two objectives."],"url":"http://arxiv.org/abs/2403.19476v1","category":"math.OC"}
{"created":"2024-03-28 14:58:19","title":"Assembly of Constructible Factorization Algebras","abstract":"We provide a toolbox of extension, gluing, and assembly techniques for factorization algebras.   Using these tools, we fill various gaps in the literature on factorization algebras on stratified manifolds, the main one being that constructible factorization algebras form a sheaf of symmetric monoidal $\\infty$-categories. Additionally, we explain how to assemble constructible factorization algebras from the data on the individual strata together with module structures associated to the relative links; thus answering a question by Ayala.   Along the way, we give detailed proofs of the following facts which are also of independent interest: constructibility is a local condition; the $\\infty$-category of disks is a localization of any sufficiently fine poset of disks; constructibility implies the Weiss condition on disks. For each of these, variants or special cases already existed, but they were either incomplete or not general enough.","sentences":["We provide a toolbox of extension, gluing, and assembly techniques for factorization algebras.   ","Using these tools, we fill various gaps in the literature on factorization algebras on stratified manifolds, the main one being that constructible factorization algebras form a sheaf of symmetric monoidal $\\infty$-categories.","Additionally, we explain how to assemble constructible factorization algebras from the data on the individual strata together with module structures associated to the relative links; thus answering a question by Ayala.   ","Along the way, we give detailed proofs of the following facts which are also of independent interest: constructibility is a local condition; the $\\infty$-category of disks is a localization of any sufficiently fine poset of disks; constructibility implies the Weiss condition on disks.","For each of these, variants or special cases already existed, but they were either incomplete or not general enough."],"url":"http://arxiv.org/abs/2403.19472v1","category":"math.AT"}
{"created":"2024-03-28 14:56:03","title":"Network Flow Models for Robust Binary Optimization with Selective Adaptability","abstract":"Adaptive robust optimization problems have received significant attention in recent years, but remain notoriously difficult to solve when recourse decisions are discrete in nature. In this paper, we propose new reformulation techniques for adaptive robust binary optimization (ARBO) problems with objective uncertainty. Without loss of generality, we focus on ARBO problems with \"selective adaptability\", a term we coin to describe a common class of linking constraints between first-stage and second-stage solutions. Our main contribution revolves around a collection of exact and approximate network flow reformulations for the ARBO problem, which we develop by building upon ideas from the decision diagram literature. Our proposed models can generate feasible solutions, primal bounds and dual bounds, while their size and approximation quality can be precisely controlled through user-specified parameters. Furthermore, and in contrast with existing solution methods, these models are easy to implement and can be solved directly with standard off-the-shelf solvers. Through an extensive set of computational experiments, we show that our models can generate high-quality solutions and dual bounds in significantly less time than popular benchmark methods, often by orders of magnitude.","sentences":["Adaptive robust optimization problems have received significant attention in recent years, but remain notoriously difficult to solve when recourse decisions are discrete in nature.","In this paper, we propose new reformulation techniques for adaptive robust binary optimization (ARBO) problems with objective uncertainty.","Without loss of generality, we focus on ARBO problems with \"selective adaptability\", a term we coin to describe a common class of linking constraints between first-stage and second-stage solutions.","Our main contribution revolves around a collection of exact and approximate network flow reformulations for the ARBO problem, which we develop by building upon ideas from the decision diagram literature.","Our proposed models can generate feasible solutions, primal bounds and dual bounds, while their size and approximation quality can be precisely controlled through user-specified parameters.","Furthermore, and in contrast with existing solution methods, these models are easy to implement and can be solved directly with standard off-the-shelf solvers.","Through an extensive set of computational experiments, we show that our models can generate high-quality solutions and dual bounds in significantly less time than popular benchmark methods, often by orders of magnitude."],"url":"http://arxiv.org/abs/2403.19471v1","category":"math.OC"}
{"created":"2024-03-28 14:47:32","title":"Beyond Talking -- Generating Holistic 3D Human Dyadic Motion for Communication","abstract":"In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners. Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements. We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener. We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously. These designs ensure that the results we generate are both coordinated and diverse. Our approach demonstrates state-of-the-art performance on two benchmark datasets. Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research. Our HoCo dataset and code will be released for research purposes upon acceptance.","sentences":["In this paper, we introduce an innovative task focused on human communication, aiming to generate 3D holistic human motions for both speakers and listeners.","Central to our approach is the incorporation of factorization to decouple audio features and the combination of textual semantic information, thereby facilitating the creation of more realistic and coordinated movements.","We separately train VQ-VAEs with respect to the holistic motions of both speaker and listener.","We consider the real-time mutual influence between the speaker and the listener and propose a novel chain-like transformer-based auto-regressive model specifically designed to characterize real-world communication scenarios effectively which can generate the motions of both the speaker and the listener simultaneously.","These designs ensure that the results we generate are both coordinated and diverse.","Our approach demonstrates state-of-the-art performance on two benchmark datasets.","Furthermore, we introduce the HoCo holistic communication dataset, which is a valuable resource for future research.","Our HoCo dataset and code will be released for research purposes upon acceptance."],"url":"http://arxiv.org/abs/2403.19467v1","category":"cs.CV"}
{"created":"2024-03-28 14:43:41","title":"Well-Posedness of the generalised Dean-Kawasaki Equation with correlated noise on bounded domains","abstract":"In this paper, we extend the notion of stochastic kinetic solutions introduced in arXiv:2108.08858 to establish the well-posedness of stochastic kinetic solutions of generalized Dean-Kawasaki equations with correlated noise on bounded, $C^2$-domains with Dirichlet boundary conditions. The results apply to a wide class of non-negative boundary data, which is based on certain a priori estimates for the solutions, that encompasses all non-negative constant functions including zero and all smooth functions bounded away from zero.","sentences":["In this paper, we extend the notion of stochastic kinetic solutions introduced in arXiv:2108.08858 to establish the well-posedness of stochastic kinetic solutions of generalized Dean-Kawasaki equations with correlated noise on bounded, $C^2$-domains with Dirichlet boundary conditions.","The results apply to a wide class of non-negative boundary data, which is based on certain a priori estimates for the solutions, that encompasses all non-negative constant functions including zero and all smooth functions bounded away from zero."],"url":"http://arxiv.org/abs/2403.19466v1","category":"math.PR"}
{"created":"2024-03-28 14:38:49","title":"Strain distribution in WS2 monolayers detected through Polarization-resolved Second Harmonic Generation","abstract":"Two-dimensional (2D) graphene and graphene-related materials (GRMs) show great promise for future electronic devices. Nevertheless, GRMs result distinct properties under the influence of the substrate that serves as support through uneven compression/ elongation of GRMs surface atoms. Strain in GRM monolayers is the most common feature that alters the interatomic distances, band structure, providing a new degree of freedom that allows regulation of their electronic properties and introducing the field of straintronics. Having an all-optical detection, a minimally invasive tool that rapidly probes strain in large areas of GRM monolayers, would be of great importance in the research and development of novel 2D devices. Here, we use Polarization-resolved Second Harmonic Generation (P-SHG) optical imaging to identify strain distribution, induced in a single layer of WS2 placed on a pre-patterned Si/SiO2 substrate with cylindrical wells. By fitting the P-SHG data pixel-by-pixel, we produce spatially resolved images of the crystal armchair direction. In regions where the WS2 monolayer conforms to the pattern topography, a distinct cross-shaped pattern is evident in the armchair image owing to strain. The presence of strain in these regions is independently confirmed using a combination of atomic force microscopy and Raman mapping.","sentences":["Two-dimensional (2D) graphene and graphene-related materials (GRMs) show great promise for future electronic devices.","Nevertheless, GRMs result distinct properties under the influence of the substrate that serves as support through uneven compression/ elongation of GRMs surface atoms.","Strain in GRM monolayers is the most common feature that alters the interatomic distances, band structure, providing a new degree of freedom that allows regulation of their electronic properties and introducing the field of straintronics.","Having an all-optical detection, a minimally invasive tool that rapidly probes strain in large areas of GRM monolayers, would be of great importance in the research and development of novel 2D devices.","Here, we use Polarization-resolved Second Harmonic Generation (P-SHG) optical imaging to identify strain distribution, induced in a single layer of WS2 placed on a pre-patterned Si/SiO2 substrate with cylindrical wells.","By fitting the P-SHG data pixel-by-pixel, we produce spatially resolved images of the crystal armchair direction.","In regions where the WS2 monolayer conforms to the pattern topography, a distinct cross-shaped pattern is evident in the armchair image owing to strain.","The presence of strain in these regions is independently confirmed using a combination of atomic force microscopy and Raman mapping."],"url":"http://arxiv.org/abs/2403.19465v1","category":"physics.app-ph"}
{"created":"2024-03-28 14:36:41","title":"Comparative study of magnetic quantum oscillations in Hall and transverse magnetoresistance","abstract":"Magnetic quantum oscillations (MQO) of Hall coefficient are measured in rare-earth tritelluride TmTe$_{3}$ and shown to be much stronger and persist to higher temperature than the Shubnikov oscillations. It is general for MQO in strongly anisotropic metals, and the combined measurements of Hall and diagonal magnetoresistance provide useful informations about the electronic structure. The ratio of their MQO amplitudes depends linearly on magnetic field, and its slope gives a simple and accurate measurement tool of the electron mean free time and its temperature dependence.","sentences":["Magnetic quantum oscillations (MQO) of Hall coefficient are measured in rare-earth tritelluride TmTe$_{3}$ and shown to be much stronger and persist to higher temperature than the Shubnikov oscillations.","It is general for MQO in strongly anisotropic metals, and the combined measurements of Hall and diagonal magnetoresistance provide useful informations about the electronic structure.","The ratio of their MQO amplitudes depends linearly on magnetic field, and its slope gives a simple and accurate measurement tool of the electron mean free time and its temperature dependence."],"url":"http://arxiv.org/abs/2403.19463v1","category":"cond-mat.str-el"}
{"created":"2024-03-28 14:32:57","title":"Learning Sampling Distribution and Safety Filter for Autonomous Driving with VQ-VAE and Differentiable Optimization","abstract":"Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving. Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid). Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE). However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE. Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution. The VQ-VAE is trained with demonstration data of optimal trajectories. We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance. We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter. We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds.","sentences":["Sampling trajectories from a distribution followed by ranking them based on a specified cost function is a common approach in autonomous driving.","Typically, the sampling distribution is hand-crafted (e.g a Gaussian, or a grid).","Recently, there have been efforts towards learning the sampling distribution through generative models such as Conditional Variational Autoencoder (CVAE).","However, these approaches fail to capture the multi-modality of the driving behaviour due to the Gaussian latent prior of the CVAE.","Thus, in this paper, we re-imagine the distribution learning through vector quantized variational autoencoder (VQ-VAE), whose discrete latent-space is well equipped to capture multi-modal sampling distribution.","The VQ-VAE is trained with demonstration data of optimal trajectories.","We further propose a differentiable optimization based safety filter to minimally correct the VQVAE sampled trajectories to ensure collision avoidance.","We use backpropagation through the optimization layers in a self-supervised learning set-up to learn good initialization and optimal parameters of the safety filter.","We perform extensive comparisons with state-of-the-art CVAE-based baseline in dense and aggressive traffic scenarios and show a reduction of up to 12 times in collision-rate while being competitive in driving speeds."],"url":"http://arxiv.org/abs/2403.19461v1","category":"cs.RO"}
{"created":"2024-03-28 14:31:10","title":"RiEMann: Near Real-Time SE(3)-Equivariant Robot Manipulation without Point Cloud Segmentation","abstract":"We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from scene point cloud input. Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the target poses of objects for manipulation without any object segmentation. RiEMann learns a manipulation task from scratch with 5 to 10 demonstrations, generalizes to unseen SE(3) transformations and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose change of the target object. The scalable action space of RiEMann facilitates the addition of custom equivariant actions such as the direction of turning the faucet, which makes articulated object manipulation possible for RiEMann. In simulation and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks with a total of 25 variants and show that RiEMann outperforms baselines in both task success rates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%), and achieves a 5.4 frames per second (FPS) network inference speed. Code and video results are available at https://riemann-web.github.io/.","sentences":["We present RiEMann, an end-to-end near Real-time SE(3)-Equivariant Robot Manipulation imitation learning framework from scene point cloud input.","Compared to previous methods that rely on descriptor field matching, RiEMann directly predicts the target poses of objects for manipulation without any object segmentation.","RiEMann learns a manipulation task from scratch with 5 to 10 demonstrations, generalizes to unseen SE(3) transformations and instances of target objects, resists visual interference of distracting objects, and follows the near real-time pose change of the target object.","The scalable action space of RiEMann facilitates the addition of custom equivariant actions such as the direction of turning the faucet, which makes articulated object manipulation possible for RiEMann.","In simulation and real-world 6-DOF robot manipulation experiments, we test RiEMann on 5 categories of manipulation tasks with a total of 25 variants and show that RiEMann outperforms baselines in both task success rates and SE(3) geodesic distance errors on predicted poses (reduced by 68.6%), and achieves a 5.4 frames per second (FPS) network inference speed.","Code and video results are available at https://riemann-web.github.io/."],"url":"http://arxiv.org/abs/2403.19460v1","category":"cs.RO"}
{"created":"2024-03-28 14:31:01","title":"NeuroLGP-SM: A Surrogate-assisted Neuroevolution Approach using Linear Genetic Programming","abstract":"Evolutionary algorithms are increasingly recognised as a viable computational approach for the automated optimisation of deep neural networks (DNNs) within artificial intelligence. This method extends to the training of DNNs, an approach known as neuroevolution. However, neuroevolution is an inherently resource-intensive process, with certain studies reporting the consumption of thousands of GPU days for refining and training a single DNN network. To address the computational challenges associated with neuroevolution while still attaining good DNN accuracy, surrogate models emerge as a pragmatic solution. Despite their potential, the integration of surrogate models into neuroevolution is still in its early stages, hindered by factors such as the effective use of high-dimensional data and the representation employed in neuroevolution. In this context, we address these challenges by employing a suitable representation based on Linear Genetic Programming, denoted as NeuroLGP, and leveraging Kriging Partial Least Squares. The amalgamation of these two techniques culminates in our proposed methodology known as the NeuroLGP-Surrogate Model (NeuroLGP-SM). For comparison purposes, we also code and use a baseline approach incorporating a repair mechanism, a common practice in neuroevolution. Notably, the baseline approach surpasses the renowned VGG-16 model in accuracy. Given the computational intensity inherent in DNN operations, a singular run is typically the norm. To evaluate the efficacy of our proposed approach, we conducted 96 independent runs. Significantly, our methodologies consistently outperform the baseline, with the SM model demonstrating superior accuracy or comparable results to the NeuroLGP approach. Noteworthy is the additional advantage that the SM approach exhibits a 25% reduction in computational requirements, further emphasising its efficiency for neuroevolution.","sentences":["Evolutionary algorithms are increasingly recognised as a viable computational approach for the automated optimisation of deep neural networks (DNNs) within artificial intelligence.","This method extends to the training of DNNs, an approach known as neuroevolution.","However, neuroevolution is an inherently resource-intensive process, with certain studies reporting the consumption of thousands of GPU days for refining and training a single DNN network.","To address the computational challenges associated with neuroevolution while still attaining good DNN accuracy, surrogate models emerge as a pragmatic solution.","Despite their potential, the integration of surrogate models into neuroevolution is still in its early stages, hindered by factors such as the effective use of high-dimensional data and the representation employed in neuroevolution.","In this context, we address these challenges by employing a suitable representation based on Linear Genetic Programming, denoted as NeuroLGP, and leveraging Kriging Partial Least Squares.","The amalgamation of these two techniques culminates in our proposed methodology known as the NeuroLGP-Surrogate Model (NeuroLGP-SM).","For comparison purposes, we also code and use a baseline approach incorporating a repair mechanism, a common practice in neuroevolution.","Notably, the baseline approach surpasses the renowned VGG-16 model in accuracy.","Given the computational intensity inherent in DNN operations, a singular run is typically the norm.","To evaluate the efficacy of our proposed approach, we conducted 96 independent runs.","Significantly, our methodologies consistently outperform the baseline, with the SM model demonstrating superior accuracy or comparable results to the NeuroLGP approach.","Noteworthy is the additional advantage that the SM approach exhibits a 25% reduction in computational requirements, further emphasising its efficiency for neuroevolution."],"url":"http://arxiv.org/abs/2403.19459v1","category":"cs.NE"}
{"created":"2024-03-28 14:30:40","title":"Evaluation of Transit cosmological model in $f(R,T^\u03c6)$ theory of gravity","abstract":"We have explored a transitioning cosmic model, depicting late-time accelerated expansion in $f(R,T^{\\phi})$ theory of gravity for an isotropic and homogeneous universe, where the trace of energy-momentum tensor $T^{\\phi}$ is the function of the self-interacting scalar field $\\phi$. We have proposed an explicit solution to the derived model by utilizing a scale factor of the hybrid form $a(t) = t^{\\alpha} e^{\\beta t}$, where $\\alpha$ and $\\beta$ are constants. To evaluate the best-fit values of free parameters of the suggested model, the statistical analysis based on the Markov Chain Monte Carlo (MCMC) method has been employed on 57 OHD points. We have described the dynamical features of the model like energy density, cosmic pressure, and equation of state parameter in the context of scalar field $\\phi$. We have also described the potential and behavior of the scalar field for quintessence and phantom scenarios. The deceleration parameter depicts a transitioning universe with signature flipping at $z_t = 0.82$ with the present value of deceleration parameter $q_0=-0.41$. The violation of SEC for the derived model indicates the cosmic expansion at a faster rate. We have used statefinders to diagnose the model. The findings for our theoretical model indicate that the derived model agrees with observed findings within a particular range of limitations.","sentences":["We have explored a transitioning cosmic model, depicting late-time accelerated expansion in $f(R,T^{\\phi})$ theory of gravity for an isotropic and homogeneous universe, where the trace of energy-momentum tensor $T^{\\phi}$ is the function of the self-interacting scalar field $\\phi$. We have proposed an explicit solution to the derived model by utilizing a scale factor of the hybrid form $a(t) = t^{\\alpha} e^{\\beta t}$, where $\\alpha$ and $\\beta$ are constants.","To evaluate the best-fit values of free parameters of the suggested model, the statistical analysis based on the Markov Chain Monte Carlo (MCMC) method has been employed on 57 OHD points.","We have described the dynamical features of the model like energy density, cosmic pressure, and equation of state parameter in the context of scalar field $\\phi$. We have also described the potential and behavior of the scalar field for quintessence and phantom scenarios.","The deceleration parameter depicts a transitioning universe with signature flipping at $z_t = 0.82$ with the present value of deceleration parameter $q_0=-0.41$. The violation of SEC for the derived model indicates the cosmic expansion at a faster rate.","We have used statefinders to diagnose the model.","The findings for our theoretical model indicate that the derived model agrees with observed findings within a particular range of limitations."],"url":"http://arxiv.org/abs/2403.19458v1","category":"gr-qc"}
{"created":"2024-03-28 14:29:26","title":"Transmissive RIS Transmitter Enabled Spatial Modulation for MIMO Systems","abstract":"In this paper, we propose a novel transmissive reconfigurable intelligent surface (TRIS) transmitter-enabled spatial modulation (SM) multiple-input multiple-output (MIMO) system. In the transmission phase, a column-wise activation strategy is implemented for the TRIS panel, where the specific column elements are activated per time slot. Concurrently, the receiver employs the maximum likelihood detection technique. Based on this, for the transmit signals, we derive the closed-form expressions for the upper bounds of the average bit error probability (ABEP) of the proposed scheme from different perspectives, employing both vector-based and element-based approaches. Furthermore, we provide the asymptotic closed-form expressions for the ABEP of the TRIS-SM scheme, as well as the diversity gain. To improve the performance of the proposed TRIS-SM system, we optimize ABEP with a fixed data rate. Additionally, we provide lower bounds to simplify the computational complexity of improved TRIS-SM scheme. The Monte Carlo simulation method is used to validate the theoretical derivations exhaustively. The results demonstrate that the proposed TRIS-SM scheme can achieve better ABEP performance compared to the conventional SM scheme. Furthermore, the improved TRIS-SM scheme outperforms the TRIS-SM scheme in terms of reliability.","sentences":["In this paper, we propose a novel transmissive reconfigurable intelligent surface (TRIS) transmitter-enabled spatial modulation (SM) multiple-input multiple-output (MIMO) system.","In the transmission phase, a column-wise activation strategy is implemented for the TRIS panel, where the specific column elements are activated per time slot.","Concurrently, the receiver employs the maximum likelihood detection technique.","Based on this, for the transmit signals, we derive the closed-form expressions for the upper bounds of the average bit error probability (ABEP) of the proposed scheme from different perspectives, employing both vector-based and element-based approaches.","Furthermore, we provide the asymptotic closed-form expressions for the ABEP of the TRIS-SM scheme, as well as the diversity gain.","To improve the performance of the proposed TRIS-SM system, we optimize ABEP with a fixed data rate.","Additionally, we provide lower bounds to simplify the computational complexity of improved TRIS-SM scheme.","The Monte Carlo simulation method is used to validate the theoretical derivations exhaustively.","The results demonstrate that the proposed TRIS-SM scheme can achieve better ABEP performance compared to the conventional SM scheme.","Furthermore, the improved TRIS-SM scheme outperforms the TRIS-SM scheme in terms of reliability."],"url":"http://arxiv.org/abs/2403.19457v1","category":"cs.IT"}
{"created":"2024-03-28 14:27:36","title":"Break-for-Make: Modular Low-Rank Adaptations for Composable Content-Style Customization","abstract":"Personalized generation paradigms empower designers to customize visual intellectual properties with the help of textual descriptions by tuning or adapting pre-trained text-to-image models on a few images. Recent works explore approaches for concurrently customizing both content and detailed visual style appearance. However, these existing approaches often generate images where the content and style are entangled. In this study, we reconsider the customization of content and style concepts from the perspective of parameter space construction. Unlike existing methods that utilize a shared parameter space for content and style, we propose a learning framework that separates the parameter space to facilitate individual learning of content and style, thereby enabling disentangled content and style. To achieve this goal, we introduce \"partly learnable projection\" (PLP) matrices to separate the original adapters into divided sub-parameter spaces. We propose \"break-for-make\" customization learning pipeline based on PLP, which is simple yet effective. We break the original adapters into \"up projection\" and \"down projection\", train content and style PLPs individually with the guidance of corresponding textual prompts in the separate adapters, and maintain generalization by employing a multi-correspondence projection learning strategy. Based on the adapters broken apart for separate training content and style, we then make the entity parameter space by reconstructing the content and style PLPs matrices, followed by fine-tuning the combined adapter to generate the target object with the desired appearance. Experiments on various styles, including textures, materials, and artistic style, show that our method outperforms state-of-the-art single/multiple concept learning pipelines in terms of content-style-prompt alignment.","sentences":["Personalized generation paradigms empower designers to customize visual intellectual properties with the help of textual descriptions by tuning or adapting pre-trained text-to-image models on a few images.","Recent works explore approaches for concurrently customizing both content and detailed visual style appearance.","However, these existing approaches often generate images where the content and style are entangled.","In this study, we reconsider the customization of content and style concepts from the perspective of parameter space construction.","Unlike existing methods that utilize a shared parameter space for content and style, we propose a learning framework that separates the parameter space to facilitate individual learning of content and style, thereby enabling disentangled content and style.","To achieve this goal, we introduce \"partly learnable projection\" (PLP) matrices to separate the original adapters into divided sub-parameter spaces.","We propose \"break-for-make\" customization learning pipeline based on PLP, which is simple yet effective.","We break the original adapters into \"up projection\" and \"down projection\", train content and style PLPs individually with the guidance of corresponding textual prompts in the separate adapters, and maintain generalization by employing a multi-correspondence projection learning strategy.","Based on the adapters broken apart for separate training content and style, we then make the entity parameter space by reconstructing the content and style PLPs matrices, followed by fine-tuning the combined adapter to generate the target object with the desired appearance.","Experiments on various styles, including textures, materials, and artistic style, show that our method outperforms state-of-the-art single/multiple concept learning pipelines in terms of content-style-prompt alignment."],"url":"http://arxiv.org/abs/2403.19456v1","category":"cs.CV"}
{"created":"2024-03-28 14:22:54","title":"JDocQA: Japanese Document Question Answering Dataset for Generative Language Models","abstract":"Document question answering is a task of question answering on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society. This is known as a quite challenging task because it requires not only text understanding but also understanding of figures and tables, and hence visual question answering (VQA) methods are often examined in addition to textual approaches. We introduce Japanese Document Question Answering (JDocQA), a large-scale document-based QA dataset, essentially requiring both visual and textual information to answer questions, which comprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese. Each QA instance includes references to the document pages and bounding boxes for the answer clues. We incorporate multiple categories of questions and unanswerable questions from the document for realistic question-answering applications. We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs) and multimodal models. Incorporating unanswerable questions in finetuning may contribute to harnessing the so-called hallucination generation.","sentences":["Document question answering is a task of question answering on given documents such as reports, slides, pamphlets, and websites, and it is a truly demanding task as paper and electronic forms of documents are so common in our society.","This is known as a quite challenging task because it requires not only text understanding but also understanding of figures and tables, and hence visual question answering (VQA) methods are often examined in addition to textual approaches.","We introduce Japanese Document Question Answering (JDocQA), a large-scale document-based QA dataset, essentially requiring both visual and textual information to answer questions, which comprises 5,504 documents in PDF format and annotated 11,600 question-and-answer instances in Japanese.","Each QA instance includes references to the document pages and bounding boxes for the answer clues.","We incorporate multiple categories of questions and unanswerable questions from the document for realistic question-answering applications.","We empirically evaluate the effectiveness of our dataset with text-based large language models (LLMs) and multimodal models.","Incorporating unanswerable questions in finetuning may contribute to harnessing the so-called hallucination generation."],"url":"http://arxiv.org/abs/2403.19454v1","category":"cs.CL"}
{"created":"2024-03-28 14:22:25","title":"A partitioned dipole-antenna shower with improved transverse recoil","abstract":"The implementation of a new final-state parton-shower algorithm in the Pythia event generator is described. The shower algorithm, dubbed Apollo, combines central aspects of the Vincia antenna shower with the global transverse-recoil scheme of the Alaric framework in order to achieve formal consistency with next-to-leading logarithmic (NLL) resummation. The shower algorithm is constructed in such a way that it facilitates a straightforward combination with fixed-order calculations. As an explicit proof of concept, a general scheme for matrix-element corrections (MECs) and two separate multiplicative next-to-leading order (NLO) matching schemes are outlined. It is argued that both matching schemes retain the logarithmic accuracy of the shower. The improved modelling of radiation is examined by contrasting the new algorithm with existing leading-logarithmic parton showers in Pythia.","sentences":["The implementation of a new final-state parton-shower algorithm in the Pythia event generator is described.","The shower algorithm, dubbed Apollo, combines central aspects of the Vincia antenna shower with the global transverse-recoil scheme of the Alaric framework in order to achieve formal consistency with next-to-leading logarithmic (NLL) resummation.","The shower algorithm is constructed in such a way that it facilitates a straightforward combination with fixed-order calculations.","As an explicit proof of concept, a general scheme for matrix-element corrections (MECs) and two separate multiplicative next-to-leading order (NLO) matching schemes are outlined.","It is argued that both matching schemes retain the logarithmic accuracy of the shower.","The improved modelling of radiation is examined by contrasting the new algorithm with existing leading-logarithmic parton showers in Pythia."],"url":"http://arxiv.org/abs/2403.19452v1","category":"hep-ph"}
{"created":"2024-03-28 14:17:50","title":"Observational Constraints on the Maximum Masses of White Dwarfs, Neutron Stars, and Exotic Stars in Non-Minimal Derivative Coupling Gravity","abstract":"The advancement of astronomical observations opens the possibility of testing our current understanding of gravitational theory in the strong-field regime and probing any deviation from general relativity. We explore to what extent compact stars predicted by non-minimal derivative coupling (NMDC) gravity theory agree with observed data. We investigate white dwarfs (WDs), neutron stars (NSs), and quark stars (QSs) mass and radius in various values of constant scalar $|Q_{\\infty}|$ at coupling strength of $\\eta=\\pm1$. This study focuses on the astrophysical impacts of altering maximum masses by values of $|Q_{\\infty}|$ and $\\eta$. From an observational point of view, we found that WD stars are consistent with ultra-cold WD data at $|Q_{\\infty}| \\lesssim 0.2$. We also found that QS has a similar impact of mass-radius to NS, where the modification is more significant at higher (central) density. For NS and QS EoSs, the value $|Q_{\\infty}|$ strongly alters the critical mass and might eliminate the $M-\\rho_c$ turning point in the negative $\\eta$ case. In that case, the sufficiently large $|Q_{\\infty}|$ could predict $M>2.6 M_\\odot$ NS and QS, i.e., larger than GW190814 secondary counterpart. We suggest that the lower mass gap in the gravitational wave and x-ray binary mass population data might restrict the theory's $|Q_{\\infty}|$.","sentences":["The advancement of astronomical observations opens the possibility of testing our current understanding of gravitational theory in the strong-field regime and probing any deviation from general relativity.","We explore to what extent compact stars predicted by non-minimal derivative coupling (NMDC) gravity theory agree with observed data.","We investigate white dwarfs (WDs), neutron stars (NSs), and quark stars (QSs) mass and radius in various values of constant scalar $|Q_{\\infty}|$ at coupling strength of $\\eta=\\pm1$. This study focuses on the astrophysical impacts of altering maximum masses by values of $|Q_{\\infty}|$ and $\\eta$. From an observational point of view, we found that WD stars are consistent with ultra-cold WD data at $|Q_{\\infty}|","\\lesssim 0.2$.","We also found that QS has a similar impact of mass-radius to NS, where the modification is more significant at higher (central) density.","For NS and QS EoSs, the value $|Q_{\\infty}|$ strongly alters the critical mass and might eliminate the $M-\\rho_c$ turning point in the negative $\\eta$ case.","In that case, the sufficiently large $|Q_{\\infty}|$ could predict $M>2.6 M_\\odot$ NS and QS, i.e., larger than GW190814 secondary counterpart.","We suggest that the lower mass gap in the gravitational wave and x-ray binary mass population data might restrict the theory's $|Q_{\\infty}|$."],"url":"http://arxiv.org/abs/2403.19450v1","category":"gr-qc"}
{"created":"2024-03-28 14:17:19","title":"O-RAN for Energy-Efficient Serving Cluster Formulation in User-Centric Cell-Free MMIMO","abstract":"The 6G Massive Multiple-Input Multiple-Output (MMIMO) networks can follow the so-called User-Centric Cell-Free (UCCF) architecture, where a single user is served by multiple Access Points (APs) coordinated by the Central Processing Unit (CPU). In this paper, we propose how O-RAN functionalities, i.e., rApp-xApp pair, can be used for energy-efficient Serving Cluster Formulation (SCF). Simulation studies show up to 37\\% gain in Energy Efficiency (EE) of the proposed solution over the state-of-the-art Network-Centric (NC) designs.","sentences":["The 6G Massive Multiple-Input Multiple-Output (MMIMO) networks can follow the so-called User-Centric Cell-Free (UCCF) architecture, where a single user is served by multiple Access Points (APs) coordinated by the Central Processing Unit (CPU).","In this paper, we propose how O-RAN functionalities, i.e., rApp-xApp pair, can be used for energy-efficient Serving Cluster Formulation (SCF).","Simulation studies show up to 37\\% gain in Energy Efficiency (EE) of the proposed solution over the state-of-the-art Network-Centric (NC) designs."],"url":"http://arxiv.org/abs/2403.19449v1","category":"cs.IT"}
{"created":"2024-03-28 14:16:23","title":"Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients","abstract":"Kakade's natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization. We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side. Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential. Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the geometry of the linear program. Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results. We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error. In particular, these general results cover the case of state-action natural policy gradients.","sentences":["Kakade's natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization.","We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side.","Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential.","Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the geometry of the linear program.","Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results.","We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error.","In particular, these general results cover the case of state-action natural policy gradients."],"url":"http://arxiv.org/abs/2403.19448v1","category":"math.OC"}
{"created":"2024-03-28 14:15:13","title":"Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays","abstract":"The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex black-box deep learning models in real-world applications. Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use. To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process. On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection. Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 > 0.9), while also generating clinically relevant and more reliable explanations than existing techniques. We evaluate our approach against post-hoc image XAI techniques LIME and SHAP, as well as CXR-LLaVA, a recent textual XAI tool which operates in the context of question answering on chest X-rays.","sentences":["The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex black-box deep learning models in real-world applications.","Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use.","To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process.","On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection.","Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 > 0.9), while also generating clinically relevant and more reliable explanations than existing techniques.","We evaluate our approach against post-hoc image XAI techniques LIME and SHAP, as well as CXR-LLaVA, a recent textual XAI tool which operates in the context of question answering on chest X-rays."],"url":"http://arxiv.org/abs/2403.19444v1","category":"cs.LG"}
{"created":"2024-03-28 14:15:10","title":"Mixed Preference Optimization: Reinforcement Learning with Data Selection and Better Reference Model","abstract":"Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language. However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values. This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO). By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches. Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model. Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of reward (easy), and those with small gaps (difficult). The first stage allows us to obtain a relatively optimal policy (LLM) model quickly, whereas the second stage refines LLM with online RLHF, thus mitigating the distribution shift issue associated with DPO. Experiments are conducted on two public alignment datasets, namely HH-RLHF and TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human evaluation.","sentences":["Large Language Models (LLMs) have become increasingly popular due to their ability to process and generate natural language.","However, as they are trained on massive datasets of text, LLMs can inherit harmful biases and produce outputs that are not aligned with human values.","This paper studies two main approaches to LLM alignment: Reinforcement Learning with Human Feedback (RLHF) and contrastive learning-based methods like Direct Preference Optimization (DPO).","By analyzing the stability and robustness of RLHF and DPO, we propose MPO (Mixed Preference Optimization), a novel method that mitigates the weaknesses of both approaches.","Specifically, we propose a two-stage training procedure: first train DPO on an easy dataset, and then perform RLHF on a difficult set with DPO model being the reference model.","Here, the easy and difficult sets are constructed by a well-trained reward model that splits response pairs into those with large gaps of reward (easy), and those with small gaps (difficult).","The first stage allows us to obtain a relatively optimal policy (LLM) model quickly, whereas the second stage refines LLM with online RLHF, thus mitigating the distribution shift issue associated with DPO.","Experiments are conducted on two public alignment datasets, namely HH-RLHF and TLDR, demonstrating the effectiveness of MPO, both in terms of GPT4 and human evaluation."],"url":"http://arxiv.org/abs/2403.19443v1","category":"cs.CL"}
{"created":"2024-03-28 14:10:48","title":"Gaussian Formalism: Concrete Realization of Joint Measurement for Heisenberg's Uncertainty Relation for Errors","abstract":"We point out that the Gaussian wave-packet formalism can serve as a concrete realization of the joint measurement of position and momentum, which is an essential element in understanding Heisenberg's original philosophy of the uncertainty principle, in line with the universal framework of error, disturbance, and their uncertainty relations developed by Lee and Tsutsui. We show that our joint measurement in the Gaussian phase space, being a Positive Operator-Valued Measure (POVM) measurement, smoothly interpolates between the projective measurements of position and momentum. We, for the first time, have obtained the Lee-Tsutsui (LT) error and the refined Lee error for the position-momentum measurement. We find that the LT uncertainty relation becomes trivial, $0=0$, in the limiting case of projective measurement of either position or momentum. Remarkably, in contrast to the LT relation, the refined Lee uncertainty relation, which assesses errors for local representability, provides a constant lower bound unaffected by these limits and is invariably saturated, for a pure Gaussian initial state. The obtained lower bound is in agreement with Heisenberg's value.","sentences":["We point out that the Gaussian wave-packet formalism can serve as a concrete realization of the joint measurement of position and momentum, which is an essential element in understanding Heisenberg's original philosophy of the uncertainty principle, in line with the universal framework of error, disturbance, and their uncertainty relations developed by Lee and Tsutsui.","We show that our joint measurement in the Gaussian phase space, being a Positive Operator-Valued Measure (POVM) measurement, smoothly interpolates between the projective measurements of position and momentum.","We, for the first time, have obtained the Lee-Tsutsui (LT) error and the refined Lee error for the position-momentum measurement.","We find that the LT uncertainty relation becomes trivial, $0=0$, in the limiting case of projective measurement of either position or momentum.","Remarkably, in contrast to the LT relation, the refined Lee uncertainty relation, which assesses errors for local representability, provides a constant lower bound unaffected by these limits and is invariably saturated, for a pure Gaussian initial state.","The obtained lower bound is in agreement with Heisenberg's value."],"url":"http://arxiv.org/abs/2403.19440v1","category":"hep-ph"}
{"created":"2024-03-28 14:07:13","title":"SubjectDrive: Scaling Generative Data in Autonomous Driving via Subject Control","abstract":"Autonomous driving progress relies on large-scale annotated datasets. In this work, we explore the potential of generative models to produce vast quantities of freely-labeled data for autonomous driving applications and present SubjectDrive, the first model proven to scale generative data production in a way that could continuously improve autonomous driving applications. We investigate the impact of scaling up the quantity of generative data on the performance of downstream perception models and find that enhancing data diversity plays a crucial role in effectively scaling generative data production. Therefore, we have developed a novel model equipped with a subject control mechanism, which allows the generative model to leverage diverse external data sources for producing varied and useful data. Extensive evaluations confirm SubjectDrive's efficacy in generating scalable autonomous driving training data, marking a significant step toward revolutionizing data production methods in this field.","sentences":["Autonomous driving progress relies on large-scale annotated datasets.","In this work, we explore the potential of generative models to produce vast quantities of freely-labeled data for autonomous driving applications and present SubjectDrive, the first model proven to scale generative data production in a way that could continuously improve autonomous driving applications.","We investigate the impact of scaling up the quantity of generative data on the performance of downstream perception models and find that enhancing data diversity plays a crucial role in effectively scaling generative data production.","Therefore, we have developed a novel model equipped with a subject control mechanism, which allows the generative model to leverage diverse external data sources for producing varied and useful data.","Extensive evaluations confirm SubjectDrive's efficacy in generating scalable autonomous driving training data, marking a significant step toward revolutionizing data production methods in this field."],"url":"http://arxiv.org/abs/2403.19438v1","category":"cs.CV"}
{"created":"2024-03-28 14:06:34","title":"The largest-$K$-norm for general measure spaces and a DC Reformulation for $L^0$-Constrained Problems in Function Spaces","abstract":"We consider constraints on the measure of the support for integrable functions on arbitrary measure spaces. It is shown that this non-convex and discontinuous constraint can be equivalently reformulated by the difference of two convex and continuous functions, namely the $L^1$-norm and the so-called largest-$K$-norm. The largest-$K$-norm is studied and its convex subdifferential is derived. An exemplary problem is solved by applying a DC method to the reformulated problem.","sentences":["We consider constraints on the measure of the support for integrable functions on arbitrary measure spaces.","It is shown that this non-convex and discontinuous constraint can be equivalently reformulated by the difference of two convex and continuous functions, namely the $L^1$-norm and the so-called largest-$K$-norm.","The largest-$K$-norm is studied and its convex subdifferential is derived.","An exemplary problem is solved by applying a DC method to the reformulated problem."],"url":"http://arxiv.org/abs/2403.19437v1","category":"math.OC"}
{"created":"2024-03-28 14:04:17","title":"BAMM: Bidirectional Autoregressive Motion Model","abstract":"Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process. However, these models face great limitations in usability by requiring prior knowledge of the motion length. Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities. To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework. BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy. By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length. This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability. Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures.","sentences":["Generating human motion from text has been dominated by denoising motion models either through diffusion or generative masking process.","However, these models face great limitations in usability by requiring prior knowledge of the motion length.","Conversely, autoregressive motion models address this limitation by adaptively predicting motion endpoints, at the cost of degraded generation quality and editing capabilities.","To address these challenges, we propose Bidirectional Autoregressive Motion Model (BAMM), a novel text-to-motion generation framework.","BAMM consists of two key components: (1) a motion tokenizer that transforms 3D human motion into discrete tokens in latent space, and (2) a masked self-attention transformer that autoregressively predicts randomly masked tokens via a hybrid attention masking strategy.","By unifying generative masked modeling and autoregressive modeling, BAMM captures rich and bidirectional dependencies among motion tokens, while learning the probabilistic mapping from textual inputs to motion outputs with dynamically-adjusted motion sequence length.","This feature enables BAMM to simultaneously achieving high-quality motion generation with enhanced usability and built-in motion editability.","Extensive experiments on HumanML3D and KIT-ML datasets demonstrate that BAMM surpasses current state-of-the-art methods in both qualitative and quantitative measures."],"url":"http://arxiv.org/abs/2403.19435v1","category":"cs.CV"}
{"created":"2024-03-28 14:03:12","title":"Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes","abstract":"Data accuracy is essential for scientific research and policy development. The National Violent Death Reporting System (NVDRS) data is widely used for discovering the patterns and causes of death. Recent studies suggested the annotation inconsistencies within the NVDRS and the potential impact on erroneous suicide-cause attributions. We present an empirical Natural Language Processing (NLP) approach to detect annotation inconsistencies and adopt a cross-validation-like paradigm to identify problematic instances. We analyzed 267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our results showed that incorporating the target state's data into training the suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the target state's test set and a decrease of 1.1% on other states' test set. To conclude, we demonstrated the annotation inconsistencies in NVDRS's death investigation notes, identified problematic instances, evaluated the effectiveness of correcting problematic instances, and eventually proposed an NLP improvement solution.","sentences":["Data accuracy is essential for scientific research and policy development.","The National Violent Death Reporting System (NVDRS) data is widely used for discovering the patterns and causes of death.","Recent studies suggested the annotation inconsistencies within the NVDRS and the potential impact on erroneous suicide-cause attributions.","We present an empirical Natural Language Processing (NLP) approach to detect annotation inconsistencies and adopt a cross-validation-like paradigm to identify problematic instances.","We analyzed 267,804 suicide death incidents between 2003 and 2020 from the NVDRS.","Our results showed that incorporating the target state's data into training the suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the target state's test set and a decrease of 1.1% on other states' test set.","To conclude, we demonstrated the annotation inconsistencies in NVDRS's death investigation notes, identified problematic instances, evaluated the effectiveness of correcting problematic instances, and eventually proposed an NLP improvement solution."],"url":"http://arxiv.org/abs/2403.19432v1","category":"cs.CL"}
{"created":"2024-03-28 14:02:35","title":"Base-extension Semantics for S5 Modal Logic","abstract":"We develop a proof-theoretic semantics -- in particular, a base-extension semantics -- for multi-agent S5 modal logic (and hence also for the usual unindexed S5). Following the inferentialist interpretation of logic, this gives us a semantics in which validity is based on proof rather than truth. In base-extension semantics, the validity of formulae is generated by provability in a `base' of atomic rules and an inductive definition of the validity of the connectives. Base-extension semantics for many interesting logics has been explored by several authors and, in particular, a base-extension semantics for the modal logics K, KT, K4, and S4 has been developed by the present authors. Here, we give a base-extension semantics for multi-agent S5 with $\\square_a$, for an agent a, as our primary operators, framed as the knowledge operator K_a. Similarly to Kripke semantics, we make use of relational structure between bases, allowing us to establish a correspondence between certain bases and worlds. We use this to establish the appropriate soundness and completeness results. We conclude by discussing how this semantics can be extended to Dynamic Epistemic Logics (DEL) starting with Public Announcement Logic (PAL).","sentences":["We develop a proof-theoretic semantics -- in particular, a base-extension semantics -- for multi-agent S5 modal logic (and hence also for the usual unindexed S5).","Following the inferentialist interpretation of logic, this gives us a semantics in which validity is based on proof rather than truth.","In base-extension semantics, the validity of formulae is generated by provability in a `base' of atomic rules and an inductive definition of the validity of the connectives.","Base-extension semantics for many interesting logics has been explored by several authors and, in particular, a base-extension semantics for the modal logics K, KT, K4, and S4 has been developed by the present authors.","Here, we give a base-extension semantics for multi-agent S5 with $\\square_a$, for an agent a, as our primary operators, framed as the knowledge operator K_a.","Similarly to Kripke semantics, we make use of relational structure between bases, allowing us to establish a correspondence between certain bases and worlds.","We use this to establish the appropriate soundness and completeness results.","We conclude by discussing how this semantics can be extended to Dynamic Epistemic Logics (DEL) starting with Public Announcement Logic (PAL)."],"url":"http://arxiv.org/abs/2403.19431v1","category":"math.LO"}
{"created":"2024-03-28 13:56:56","title":"Dynamic Phase Enabled Topological Mode Steering in Composite Su-Schrieffer-Heeger Waveguide Arrays","abstract":"Topological boundary states localize at interfaces whenever the interface implies a change of the associated topological invariant encoded in the geometric phase. The generically present dynamic phase, however, which is energy and time dependent, has been known to be non-universal, and hence not to intertwine with any topological geometric phase. Using the example of topological zero modes in composite Su-Schrieffer-Heeger (c-SSH) waveguide arrays with a central defect, we report on the selective excitation and transition of topological boundary mode based on dynamic phase-steered interferences. Our work thus provides a new knob for the control and manipulation of topological states in composite photonic devices, indicating promising applications where topological modes and their bandwidth can be jointly controlled by the dynamic phase, geometric phase, and wavelength in on-chip topological devices.","sentences":["Topological boundary states localize at interfaces whenever the interface implies a change of the associated topological invariant encoded in the geometric phase.","The generically present dynamic phase, however, which is energy and time dependent, has been known to be non-universal, and hence not to intertwine with any topological geometric phase.","Using the example of topological zero modes in composite Su-Schrieffer-Heeger (c-SSH) waveguide arrays with a central defect, we report on the selective excitation and transition of topological boundary mode based on dynamic phase-steered interferences.","Our work thus provides a new knob for the control and manipulation of topological states in composite photonic devices, indicating promising applications where topological modes and their bandwidth can be jointly controlled by the dynamic phase, geometric phase, and wavelength in on-chip topological devices."],"url":"http://arxiv.org/abs/2403.19427v1","category":"physics.optics"}
{"created":"2024-03-28 13:56:26","title":"A Robust Ensemble Algorithm for Ischemic Stroke Lesion Segmentation: Generalizability and Clinical Utility Beyond the ISLES Challenge","abstract":"Diffusion-weighted MRI (DWI) is essential for stroke diagnosis, treatment decisions, and prognosis. However, image and disease variability hinder the development of generalizable AI algorithms with clinical value. We address this gap by presenting a novel ensemble algorithm derived from the 2022 Ischemic Stroke Lesion Segmentation (ISLES) challenge. ISLES'22 provided 400 patient scans with ischemic stroke from various medical centers, facilitating the development of a wide range of cutting-edge segmentation algorithms by the research community. Through collaboration with leading teams, we combined top-performing algorithms into an ensemble model that overcomes the limitations of individual solutions. Our ensemble model achieved superior ischemic lesion detection and segmentation accuracy on our internal test set compared to individual algorithms. This accuracy generalized well across diverse image and disease variables. Furthermore, the model excelled in extracting clinical biomarkers. Notably, in a Turing-like test, neuroradiologists consistently preferred the algorithm's segmentations over manual expert efforts, highlighting increased comprehensiveness and precision. Validation using a real-world external dataset (N=1686) confirmed the model's generalizability. The algorithm's outputs also demonstrated strong correlations with clinical scores (admission NIHSS and 90-day mRS) on par with or exceeding expert-derived results, underlining its clinical relevance. This study offers two key findings. First, we present an ensemble algorithm (https://github.com/Tabrisrei/ISLES22_Ensemble) that detects and segments ischemic stroke lesions on DWI across diverse scenarios on par with expert (neuro)radiologists. Second, we show the potential for biomedical challenge outputs to extend beyond the challenge's initial objectives, demonstrating their real-world clinical applicability.","sentences":["Diffusion-weighted MRI (DWI) is essential for stroke diagnosis, treatment decisions, and prognosis.","However, image and disease variability hinder the development of generalizable AI algorithms with clinical value.","We address this gap by presenting a novel ensemble algorithm derived from the 2022 Ischemic Stroke Lesion Segmentation (ISLES) challenge.","ISLES'22 provided 400 patient scans with ischemic stroke from various medical centers, facilitating the development of a wide range of cutting-edge segmentation algorithms by the research community.","Through collaboration with leading teams, we combined top-performing algorithms into an ensemble model that overcomes the limitations of individual solutions.","Our ensemble model achieved superior ischemic lesion detection and segmentation accuracy on our internal test set compared to individual algorithms.","This accuracy generalized well across diverse image and disease variables.","Furthermore, the model excelled in extracting clinical biomarkers.","Notably, in a Turing-like test, neuroradiologists consistently preferred the algorithm's segmentations over manual expert efforts, highlighting increased comprehensiveness and precision.","Validation using a real-world external dataset (N=1686) confirmed the model's generalizability.","The algorithm's outputs also demonstrated strong correlations with clinical scores (admission NIHSS and 90-day mRS) on par with or exceeding expert-derived results, underlining its clinical relevance.","This study offers two key findings.","First, we present an ensemble algorithm (https://github.com/Tabrisrei/ISLES22_Ensemble) that detects and segments ischemic stroke lesions on DWI across diverse scenarios on par with expert (neuro)radiologists.","Second, we show the potential for biomedical challenge outputs to extend beyond the challenge's initial objectives, demonstrating their real-world clinical applicability."],"url":"http://arxiv.org/abs/2403.19425v1","category":"eess.IV"}
{"created":"2024-03-28 13:56:23","title":"The Role of Syntactic Span Preferences in Post-Hoc Explanation Disagreement","abstract":"Post-hoc explanation methods are an important tool for increasing model transparency for users. Unfortunately, the currently used methods for attributing token importance often yield diverging patterns. In this work, we study potential sources of disagreement across methods from a linguistic perspective. We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences. Token-level differences between methods are smoothed out if we compare them on the syntactic span level. We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size $k$. We systematically investigate the interaction between $k$ and spans and propose an improved configuration for selecting important tokens.","sentences":["Post-hoc explanation methods are an important tool for increasing model transparency for users.","Unfortunately, the currently used methods for attributing token importance often yield diverging patterns.","In this work, we study potential sources of disagreement across methods from a linguistic perspective.","We find that different methods systematically select different classes of words and that methods that agree most with other methods and with humans display similar linguistic preferences.","Token-level differences between methods are smoothed out if we compare them on the syntactic span level.","We also find higher agreement across methods by estimating the most important spans dynamically instead of relying on a fixed subset of size $k$. We systematically investigate the interaction between $k$ and spans and propose an improved configuration for selecting important tokens."],"url":"http://arxiv.org/abs/2403.19424v1","category":"cs.CL"}
{"created":"2024-03-28 13:52:12","title":"Scaling up ridge regression for brain encoding in a massive individual fMRI dataset","abstract":"Brain encoding with neuroimaging data is an established analysis aimed at predicting human brain activity directly from complex stimuli features such as movie frames. Typically, these features are the latent space representation from an artificial neural network, and the stimuli are image, audio, or text inputs. Ridge regression is a popular prediction model for brain encoding due to its good out-of-sample generalization performance. However, training a ridge regression model can be highly time-consuming when dealing with large-scale deep functional magnetic resonance imaging (fMRI) datasets that include many space-time samples of brain activity. This paper evaluates different parallelization techniques to reduce the training time of brain encoding with ridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI resource currently available. With multi-threading, our results show that the Intel Math Kernel Library (MKL) significantly outperforms the OpenBLAS library, being 1.9 times faster using 32 threads on a single machine. We then evaluated the Dask multi-CPU implementation of ridge regression readily available in scikit-learn (MultiOutput), and we proposed a new \"batch\" version of Dask parallelization, motivated by a time complexity analysis. In line with our theoretical analysis, MultiOutput parallelization was found to be impractical, i.e., slower than multi-threading on a single machine. In contrast, the Batch-MultiOutput regression scaled well across compute nodes and threads, providing speed-ups of up to 33 times with 8 compute nodes and 32 threads compared to a single-threaded scikit-learn execution. Batch parallelization using Dask thus emerges as a scalable approach for brain encoding with ridge regression on high-performance computing systems using scikit-learn and large fMRI datasets.","sentences":["Brain encoding with neuroimaging data is an established analysis aimed at predicting human brain activity directly from complex stimuli features such as movie frames.","Typically, these features are the latent space representation from an artificial neural network, and the stimuli are image, audio, or text inputs.","Ridge regression is a popular prediction model for brain encoding due to its good out-of-sample generalization performance.","However, training a ridge regression model can be highly time-consuming when dealing with large-scale deep functional magnetic resonance imaging (fMRI) datasets that include many space-time samples of brain activity.","This paper evaluates different parallelization techniques to reduce the training time of brain encoding with ridge regression on the CNeuroMod Friends dataset, one of the largest deep fMRI resource currently available.","With multi-threading, our results show that the Intel Math Kernel Library (MKL) significantly outperforms the OpenBLAS library, being 1.9 times faster using 32 threads on a single machine.","We then evaluated the Dask multi-CPU implementation of ridge regression readily available in scikit-learn (MultiOutput), and we proposed a new \"batch\" version of Dask parallelization, motivated by a time complexity analysis.","In line with our theoretical analysis, MultiOutput parallelization was found to be impractical, i.e., slower than multi-threading on a single machine.","In contrast, the Batch-MultiOutput regression scaled well across compute nodes and threads, providing speed-ups of up to 33 times with 8 compute nodes and 32 threads compared to a single-threaded scikit-learn execution.","Batch parallelization using Dask thus emerges as a scalable approach for brain encoding with ridge regression on high-performance computing systems using scikit-learn and large fMRI datasets."],"url":"http://arxiv.org/abs/2403.19421v1","category":"cs.LG"}
{"created":"2024-03-28 13:50:24","title":"Fairness in Ranking: Robustness through Randomization without the Protected Attribute","abstract":"There has been great interest in fairness in machine learning, especially in relation to classification problems. In ranking-related problems, such as in online advertising, recommender systems, and HR automation, much work on fairness remains to be done. Two complications arise: first, the protected attribute may not be available in many applications. Second, there are multiple measures of fairness of rankings, and optimization-based methods utilizing a single measure of fairness of rankings may produce rankings that are unfair with respect to other measures. In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute. In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed methods.","sentences":["There has been great interest in fairness in machine learning, especially in relation to classification problems.","In ranking-related problems, such as in online advertising, recommender systems, and HR automation, much work on fairness remains to be done.","Two complications arise: first, the protected attribute may not be available in many applications.","Second, there are multiple measures of fairness of rankings, and optimization-based methods utilizing a single measure of fairness of rankings may produce rankings that are unfair with respect to other measures.","In this work, we propose a randomized method for post-processing rankings, which do not require the availability of the protected attribute.","In an extensive numerical study, we show the robustness of our methods with respect to P-Fairness and effectiveness with respect to Normalized Discounted Cumulative Gain (NDCG) from the baseline ranking, improving on previously proposed methods."],"url":"http://arxiv.org/abs/2403.19419v1","category":"cs.LG"}
{"created":"2024-03-28 13:49:43","title":"Constants of Motion for Conserved and Non-conserved Dynamics","abstract":"This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion. This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators. For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped. The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\\em total} system (i.e., oscillator plus dissipative environment). For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions. In addition, a constant is identified which generalizes angular momentum for all ratios of the frequencies. The approach presented here can produce {\\em multiple} constants of motion from a {\\em single}, generic data set.","sentences":["This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion.","This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators.","For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped.","The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\\em total} system (i.e., oscillator plus dissipative environment).","For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions.","In addition, a constant is identified which generalizes angular momentum for all ratios of the frequencies.","The approach presented here can produce {\\em multiple} constants of motion from a {\\em single}, generic data set."],"url":"http://arxiv.org/abs/2403.19418v1","category":"cs.LG"}
{"created":"2024-03-28 13:47:19","title":"OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion","abstract":"We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities. In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task. OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment. The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent. OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects. This extensive collection supports applications such as interaction reconstruction and motion synthesis. Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC). CTC aims to generate a sequence of bimanual manipulation to achieve task objectives. Within the CTC framework, we employ Large Language Models (LLMs) to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task. OAKINK2 datasets and models are available at https://oakink.net/v2.","sentences":["We present OAKINK2, a dataset of bimanual object manipulation tasks for complex daily activities.","In pursuit of constructing the complex tasks into a structured representation, OAKINK2 introduces three level of abstraction to organize the manipulation tasks: Affordance, Primitive Task, and Complex Task.","OAKINK2 features on an object-centric perspective for decoding the complex tasks, treating them as a sequence of object affordance fulfillment.","The first level, Affordance, outlines the functionalities that objects in the scene can afford, the second level, Primitive Task, describes the minimal interaction units that humans interact with the object to achieve its affordance, and the third level, Complex Task, illustrates how Primitive Tasks are composed and interdependent.","OAKINK2 dataset provides multi-view image streams and precise pose annotations for the human body, hands and various interacting objects.","This extensive collection supports applications such as interaction reconstruction and motion synthesis.","Based on the 3-level abstraction of OAKINK2, we explore a task-oriented framework for Complex Task Completion (CTC).","CTC aims to generate a sequence of bimanual manipulation to achieve task objectives.","Within the CTC framework, we employ Large Language Models (LLMs) to decompose the complex task objectives into sequences of Primitive Tasks and have developed a Motion Fulfillment Model that generates bimanual hand motion for each Primitive Task.","OAKINK2 datasets and models are available at https://oakink.net/v2."],"url":"http://arxiv.org/abs/2403.19417v1","category":"cs.CV"}
{"created":"2024-03-28 13:42:43","title":"Navigating Eukaryotic Genome Annotation Pipelines: A Route Map to BRAKER, Galba, and TSEBRA","abstract":"Annotating the structure of protein-coding genes represents a major challenge in the analysis of eukaryotic genomes. This task sets the groundwork for subsequent genomic studies aimed at understanding the functions of individual genes. BRAKER and Galba are two fully automated and containerized pipelines designed to perform accurate genome annotation. BRAKER integrates the GeneMark-ETP and AUGUSTUS gene finders, employing the TSEBRA combiner to attain high sensitivity and precision. BRAKER is adept at handling genomes of any size, provided that it has access to both transcript expression sequencing data and an extensive protein database from the target clade. In particular, BRAKER demonstrates high accuracy even with only one type of these extrinsic evidence sources, although it should be noted that accuracy diminishes for larger genomes under such conditions. In contrast, Galba adopts a distinct methodology utilizing the outcomes of direct protein-to-genome spliced alignments using miniprot to generate training genes and evidence for gene prediction in AUGUSTUS. Galba has superior accuracy in large genomes if protein sequences are the only source of evidence. This chapter provides practical guidelines for employing both pipelines in the annotation of eukaryotic genomes, with a focus on insect genomes.","sentences":["Annotating the structure of protein-coding genes represents a major challenge in the analysis of eukaryotic genomes.","This task sets the groundwork for subsequent genomic studies aimed at understanding the functions of individual genes.","BRAKER and Galba are two fully automated and containerized pipelines designed to perform accurate genome annotation.","BRAKER integrates the GeneMark-ETP and AUGUSTUS gene finders, employing the TSEBRA combiner to attain high sensitivity and precision.","BRAKER is adept at handling genomes of any size, provided that it has access to both transcript expression sequencing data and an extensive protein database from the target clade.","In particular, BRAKER demonstrates high accuracy even with only one type of these extrinsic evidence sources, although it should be noted that accuracy diminishes for larger genomes under such conditions.","In contrast, Galba adopts a distinct methodology utilizing the outcomes of direct protein-to-genome spliced alignments using miniprot to generate training genes and evidence for gene prediction in AUGUSTUS.","Galba has superior accuracy in large genomes if protein sequences are the only source of evidence.","This chapter provides practical guidelines for employing both pipelines in the annotation of eukaryotic genomes, with a focus on insect genomes."],"url":"http://arxiv.org/abs/2403.19416v1","category":"q-bio.GN"}
{"created":"2024-03-28 13:39:55","title":"Brain-Shift: Unsupervised Pseudo-Healthy Brain Synthesis for Novel Biomarker Extraction in Chronic Subdural Hematoma","abstract":"Chronic subdural hematoma (cSDH) is a common neurological condition characterized by the accumulation of blood between the brain and the dura mater. This accumulation of blood can exert pressure on the brain, potentially leading to fatal outcomes. Treatment options for cSDH are limited to invasive surgery or non-invasive management. Traditionally, the midline shift, hand-measured by experts from an ideal sagittal plane, and the hematoma volume have been the primary metrics for quantifying and analyzing cSDH. However, these approaches do not quantify the local 3D brain deformation caused by cSDH. We propose a novel method using anatomy-aware unsupervised diffeomorphic pseudo-healthy synthesis to generate brain deformation fields. The deformation fields derived from this process are utilized to extract biomarkers that quantify the shift in the brain due to cSDH. We use CT scans of 121 patients for training and validation of our method and find that our metrics allow the identification of patients who require surgery. Our results indicate that automatically obtained brain deformation fields might contain prognostic value for personalized cSDH treatment. Our implementation is available on: github.com/Barisimre/brain-morphing","sentences":["Chronic subdural hematoma (cSDH) is a common neurological condition characterized by the accumulation of blood between the brain and the dura mater.","This accumulation of blood can exert pressure on the brain, potentially leading to fatal outcomes.","Treatment options for cSDH are limited to invasive surgery or non-invasive management.","Traditionally, the midline shift, hand-measured by experts from an ideal sagittal plane, and the hematoma volume have been the primary metrics for quantifying and analyzing cSDH.","However, these approaches do not quantify the local 3D brain deformation caused by cSDH.","We propose a novel method using anatomy-aware unsupervised diffeomorphic pseudo-healthy synthesis to generate brain deformation fields.","The deformation fields derived from this process are utilized to extract biomarkers that quantify the shift in the brain due to cSDH.","We use CT scans of 121 patients for training and validation of our method and find that our metrics allow the identification of patients who require surgery.","Our results indicate that automatically obtained brain deformation fields might contain prognostic value for personalized cSDH treatment.","Our implementation is available on: github.com/Barisimre/brain-morphing"],"url":"http://arxiv.org/abs/2403.19415v1","category":"eess.IV"}
{"created":"2024-03-28 13:38:13","title":"BP4ER: Bootstrap Prompting for Explicit Reasoning in Medical Dialogue Generation","abstract":"Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value. Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities. While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation. To address these limitations, we propose the method Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's multi-step reasoning process and iteratively enhance this reasoning process. We employ a least-to-most prompting strategy to guide a large language model (LLM) in explicit reasoning, breaking down MDG into simpler sub-questions. These sub-questions build on answers from previous ones. Additionally, we also introduce two distinct bootstrapping techniques for prompting, which autonomously correct errors and facilitate the LLM's explicit reasoning. This approach eliminates the need for entity annotation and increases the transparency of the MDG process by explicitly generating the intermediate reasoning chain. The experimental findings on the two public datasets indicate that BP4ER outperforms state-of-the-art methods in terms of both objective and subjective evaluation metrics.","sentences":["Medical dialogue generation (MDG) has gained increasing attention due to its substantial practical value.","Previous works typically employ a sequence-to-sequence framework to generate medical responses by modeling dialogue context as sequential text with annotated medical entities.","While these methods have been successful in generating fluent responses, they fail to provide process explanations of reasoning and require extensive entity annotation.","To address these limitations, we propose the method Bootstrap Prompting for Explicit Reasoning in MDG (BP4ER), which explicitly model MDG's multi-step reasoning process and iteratively enhance this reasoning process.","We employ a least-to-most prompting strategy to guide a large language model (LLM) in explicit reasoning, breaking down MDG into simpler sub-questions.","These sub-questions build on answers from previous ones.","Additionally, we also introduce two distinct bootstrapping techniques for prompting, which autonomously correct errors and facilitate the LLM's explicit reasoning.","This approach eliminates the need for entity annotation and increases the transparency of the MDG process by explicitly generating the intermediate reasoning chain.","The experimental findings on the two public datasets indicate that BP4ER outperforms state-of-the-art methods in terms of both objective and subjective evaluation metrics."],"url":"http://arxiv.org/abs/2403.19414v1","category":"cs.CL"}
{"created":"2024-03-28 13:35:46","title":"A diving heuristic for mixed-integer problems with unbounded semi-continuous variables","abstract":"Semi-continuous decision variables arise naturally in many real-world applications. They are defined to take either value zero or any value within a specified range, and occur mainly to prevent small nonzero values in the solution. One particular challenge that can come with semi-continuous variables in practical models is that their upper bound may be large or even infinite. In this article, we briefly discuss these challenges, and present a new diving heuristic tailored for mixed-integer optimization problems with general semi-continuous variables. The heuristic is designed to work independently of whether the semi-continuous variables are bounded from above, and thus circumvents the specific difficulties that come with unbounded semi-continuous variables. We conduct extensive computational experiments on three different test sets, integrating the heuristic in an open-source MIP solver. The results indicate that this heuristic is a successful tool for finding high-quality solutions in negligible time. At the root node the primal gap is reduced by an average of 5 % up to 21 %, and considering the overall performance improvement, the primal integral is reduced by 2 % to 17 % on average.","sentences":["Semi-continuous decision variables arise naturally in many real-world applications.","They are defined to take either value zero or any value within a specified range, and occur mainly to prevent small nonzero values in the solution.","One particular challenge that can come with semi-continuous variables in practical models is that their upper bound may be large or even infinite.","In this article, we briefly discuss these challenges, and present a new diving heuristic tailored for mixed-integer optimization problems with general semi-continuous variables.","The heuristic is designed to work independently of whether the semi-continuous variables are bounded from above, and thus circumvents the specific difficulties that come with unbounded semi-continuous variables.","We conduct extensive computational experiments on three different test sets, integrating the heuristic in an open-source MIP solver.","The results indicate that this heuristic is a successful tool for finding high-quality solutions in negligible time.","At the root node the primal gap is reduced by an average of 5 % up to 21 %, and considering the overall performance improvement, the primal integral is reduced by 2 % to 17 % on average."],"url":"http://arxiv.org/abs/2403.19411v1","category":"math.OC"}
{"created":"2024-03-28 13:34:49","title":"Channel Deduction: A New Learning Framework to Acquire Channel from Outdated Samples and Coarse Estimate","abstract":"How to reduce the pilot overhead required for channel estimation? How to deal with the channel dynamic changes and error propagation in channel prediction? To jointly address these two critical issues in next-generation transceiver design, in this paper, we propose a novel framework named channel deduction for high-dimensional channel acquisition in multiple-input multiple-output (MIMO)-orthogonal frequency division multiplexing (OFDM) systems. Specifically, it makes use of the outdated channel information of past time slots, performs coarse estimation for the current channel with a relatively small number of pilots, and then fuses these two information to obtain a complete representation of the present channel. The rationale is to align the current channel representation to both the latent channel features within the past samples and the coarse estimate of current channel at the pilots, which, in a sense, behaves as a complementary combination of estimation and prediction and thus reduces the overall overhead. To fully exploit the highly nonlinear correlations in time, space, and frequency domains, we resort to learning-based implementation approaches. By using the highly efficient complex-domain multilayer perceptron (MLP)-mixer for crossing space-frequency domain representation and the recurrence-based or attention-based mechanisms for the past-present interaction, we respectively design two different channel deduction neural networks (CDNets). We provide a general procedure of data collection, training, and deployment to standardize the application of CDNets. Comprehensive experimental evaluations in accuracy, robustness, and efficiency demonstrate the superiority of the proposed approach, which reduces the pilot overhead by up to 88.9% compared to state-of-the-art estimation approaches and enables continuous operating even under unknown user movement and error propagation.","sentences":["How to reduce the pilot overhead required for channel estimation?","How to deal with the channel dynamic changes and error propagation in channel prediction?","To jointly address these two critical issues in next-generation transceiver design, in this paper, we propose a novel framework named channel deduction for high-dimensional channel acquisition in multiple-input multiple-output (MIMO)-orthogonal frequency division multiplexing (OFDM) systems.","Specifically, it makes use of the outdated channel information of past time slots, performs coarse estimation for the current channel with a relatively small number of pilots, and then fuses these two information to obtain a complete representation of the present channel.","The rationale is to align the current channel representation to both the latent channel features within the past samples and the coarse estimate of current channel at the pilots, which, in a sense, behaves as a complementary combination of estimation and prediction and thus reduces the overall overhead.","To fully exploit the highly nonlinear correlations in time, space, and frequency domains, we resort to learning-based implementation approaches.","By using the highly efficient complex-domain multilayer perceptron (MLP)-mixer for crossing space-frequency domain representation and the recurrence-based or attention-based mechanisms for the past-present interaction, we respectively design two different channel deduction neural networks (CDNets).","We provide a general procedure of data collection, training, and deployment to standardize the application of CDNets.","Comprehensive experimental evaluations in accuracy, robustness, and efficiency demonstrate the superiority of the proposed approach, which reduces the pilot overhead by up to 88.9% compared to state-of-the-art estimation approaches and enables continuous operating even under unknown user movement and error propagation."],"url":"http://arxiv.org/abs/2403.19409v1","category":"eess.SP"}
{"created":"2024-03-28 13:33:39","title":"Queued quantum collision models","abstract":"Collision models describe the sequential interactions of a system with independent ancillas. Motivated by recent advances in neutral atom arrays, in this Letter we investigate a model where the ancillas are governed by a classical controller that allows them to queue up while they wait for their turn to interact with the system. The ancillas can undergo individual open dynamics while they wait, which may cause them to decohere. The system, which plays the role of the server in the queue, can also undergo its own open dynamics whenever it is idle. We first show that this framework generalizes existing approaches for quantum collision models, recovering the deterministic and stochastic formulations in the appropriate limits. Next, we show how the classical queueing dynamics introduces non-trivial effects in the quantum collisions, that can lead to different phases in the system-ancilla response. We illustrate the idea with a model of coherence transfer under noisy waiting dynamics.","sentences":["Collision models describe the sequential interactions of a system with independent ancillas.","Motivated by recent advances in neutral atom arrays, in this Letter we investigate a model where the ancillas are governed by a classical controller that allows them to queue up while they wait for their turn to interact with the system.","The ancillas can undergo individual open dynamics while they wait, which may cause them to decohere.","The system, which plays the role of the server in the queue, can also undergo its own open dynamics whenever it is idle.","We first show that this framework generalizes existing approaches for quantum collision models, recovering the deterministic and stochastic formulations in the appropriate limits.","Next, we show how the classical queueing dynamics introduces non-trivial effects in the quantum collisions, that can lead to different phases in the system-ancilla response.","We illustrate the idea with a model of coherence transfer under noisy waiting dynamics."],"url":"http://arxiv.org/abs/2403.19408v1","category":"quant-ph"}
{"created":"2024-03-28 13:32:49","title":"Towards Temporally Consistent Referring Video Object Segmentation","abstract":"Referring Video Object Segmentation (R-VOS) methods face challenges in maintaining consistent object segmentation due to temporal context variability and the presence of other visually similar objects. We propose an end-to-end R-VOS paradigm that explicitly models temporal instance consistency alongside the referring segmentation. Specifically, we introduce a novel hybrid memory that facilitates inter-frame collaboration for robust spatio-temporal matching and propagation. Features of frames with automatically generated high-quality reference masks are propagated to segment the remaining frames based on multi-granularity association to achieve temporally consistent R-VOS. Furthermore, we propose a new Mask Consistency Score (MCS) metric to evaluate the temporal consistency of video segmentation. Extensive experiments demonstrate that our approach enhances temporal consistency by a significant margin, leading to top-ranked performance on popular R-VOS benchmarks, i.e., Ref-YouTube-VOS (67.1%) and Ref-DAVIS17 (65.6%).","sentences":["Referring Video Object Segmentation (R-VOS) methods face challenges in maintaining consistent object segmentation due to temporal context variability and the presence of other visually similar objects.","We propose an end-to-end R-VOS paradigm that explicitly models temporal instance consistency alongside the referring segmentation.","Specifically, we introduce a novel hybrid memory that facilitates inter-frame collaboration for robust spatio-temporal matching and propagation.","Features of frames with automatically generated high-quality reference masks are propagated to segment the remaining frames based on multi-granularity association to achieve temporally consistent R-VOS.","Furthermore, we propose a new Mask Consistency Score (MCS) metric to evaluate the temporal consistency of video segmentation.","Extensive experiments demonstrate that our approach enhances temporal consistency by a significant margin, leading to top-ranked performance on popular R-VOS benchmarks, i.e., Ref-YouTube-VOS (67.1%) and Ref-DAVIS17 (65.6%)."],"url":"http://arxiv.org/abs/2403.19407v1","category":"cs.CV"}
{"created":"2024-03-28 13:29:29","title":"Tabular Learning: Encoding for Entity and Context Embeddings","abstract":"Examining the effect of different encoding techniques on entity and context embeddings, the goal of this work is to challenge commonly used Ordinal encoding for tabular learning. Applying different preprocessing methods and network architectures over several datasets resulted in a benchmark on how the encoders influence the learning outcome of the networks. By keeping the test, validation and training data consistent, results have shown that ordinal encoding is not the most suited encoder for categorical data in terms of preprocessing the data and thereafter, classifying the target variable correctly. A better outcome was achieved, encoding the features based on string similarities by computing a similarity matrix as input for the network. This is the case for both, entity and context embeddings, where the transformer architecture showed improved performance for Ordinal and Similarity encoding with regard to multi-label classification tasks.","sentences":["Examining the effect of different encoding techniques on entity and context embeddings, the goal of this work is to challenge commonly used Ordinal encoding for tabular learning.","Applying different preprocessing methods and network architectures over several datasets resulted in a benchmark on how the encoders influence the learning outcome of the networks.","By keeping the test, validation and training data consistent, results have shown that ordinal encoding is not the most suited encoder for categorical data in terms of preprocessing the data and thereafter, classifying the target variable correctly.","A better outcome was achieved, encoding the features based on string similarities by computing a similarity matrix as input for the network.","This is the case for both, entity and context embeddings, where the transformer architecture showed improved performance for Ordinal and Similarity encoding with regard to multi-label classification tasks."],"url":"http://arxiv.org/abs/2403.19405v1","category":"cs.LG"}
{"created":"2024-03-28 13:24:18","title":"Hardness of Learning Boolean Functions from Label Proportions","abstract":"In recent years the framework of learning from label proportions (LLP) has been gaining importance in machine learning. In this setting, the training examples are aggregated into subsets or bags and only the average label per bag is available for learning an example-level predictor. This generalizes traditional PAC learning which is the special case of unit-sized bags. The computational learning aspects of LLP were studied in recent works (Saket, NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for learning halfspaces in the LLP setting. In this work we focus on the intractability of LLP learning Boolean functions. Our first result shows that given a collection of bags of size at most $2$ which are consistent with an OR function, it is NP-hard to find a CNF of constantly many clauses which satisfies any constant-fraction of the bags. This is in contrast with the work of (Saket, NeurIPS'21) which gave a $(2/5)$-approximation for learning ORs using a halfspace. Thus, our result provides a separation between constant clause CNFs and halfspaces as hypotheses for LLP learning ORs.   Next, we prove the hardness of satisfying more than $1/2 + o(1)$ fraction of such bags using a $t$-DNF (i.e. DNF where each term has $\\leq t$ literals) for any constant $t$. In usual PAC learning such a hardness was known (Khot-Saket, FOCS'08) only for learning noisy ORs. We also study the learnability of parities and show that it is NP-hard to satisfy more than $(q/2^{q-1} + o(1))$-fraction of $q$-sized bags which are consistent with a parity using a parity, while a random parity based algorithm achieves a $(1/2^{q-2})$-approximation.","sentences":["In recent years the framework of learning from label proportions (LLP) has been gaining importance in machine learning.","In this setting, the training examples are aggregated into subsets or bags and only the average label per bag is available for learning an example-level predictor.","This generalizes traditional PAC learning which is the special case of unit-sized bags.","The computational learning aspects of LLP were studied in recent works (Saket, NeurIPS'21; Saket, NeurIPS'22) which showed algorithms and hardness for learning halfspaces in the LLP setting.","In this work we focus on the intractability of LLP learning Boolean functions.","Our first result shows that given a collection of bags of size at most $2$ which are consistent with an OR function, it is NP-hard to find a CNF of constantly many clauses which satisfies any constant-fraction of the bags.","This is in contrast with the work of (Saket, NeurIPS'21) which gave a $(2/5)$-approximation for learning ORs using a halfspace.","Thus, our result provides a separation between constant clause CNFs and halfspaces as hypotheses for LLP learning ORs.   ","Next, we prove the hardness of satisfying more than $1/2 + o(1)$ fraction of such bags using a $t$-DNF (i.e. DNF where each term has $\\leq t$ literals) for any constant $t$. In usual PAC learning such a hardness was known (Khot-Saket, FOCS'08) only for learning noisy ORs.","We also study the learnability of parities and show that it is NP-hard to satisfy more than $(q/2^{q-1} + o(1))$-fraction of $q$-sized bags which are consistent with a parity using a parity, while a random parity based algorithm achieves a $(1/2^{q-2})$-approximation."],"url":"http://arxiv.org/abs/2403.19401v1","category":"cs.CC"}
{"created":"2024-03-28 13:11:09","title":"Cycling on the Freeway: The Perilous State of Open Source Neuroscience Software","abstract":"Most scientists need software to perform their research (Barker et al., 2020; Carver et al., 2022; Hettrick, 2014; Hettrick et al., 2014; Switters and Osimo, 2019), and neuroscientists are no exception. Whether we work with reaction times, electrophysiological signals, or magnetic resonance imaging data, we rely on software to acquire, analyze, and statistically evaluate the raw data we obtain - or to generate such data if we work with simulations. In recent years there has been a shift toward relying on free, open-source scientific software (FOSSS) for neuroscience data analysis (Poldrack et al., 2019), in line with the broader open science movement in academia (McKiernan et al., 2016) and wider industry trends (Eghbal, 2016). Importantly, FOSSS is typically developed by working scientists (not professional software developers) which sets up a precarious situation given the nature of the typical academic workplace (wherein academics, especially in their early careers, are on short and fixed term contracts). In this paper, we will argue that the existing ecosystem of neuroscientific open source software is brittle, and discuss why and how the neuroscience community needs to come together to ensure a healthy growth of our software landscape to the benefit of all.","sentences":["Most scientists need software to perform their research (Barker et al., 2020; Carver et al., 2022; Hettrick, 2014; Hettrick et al., 2014; Switters and Osimo, 2019), and neuroscientists are no exception.","Whether we work with reaction times, electrophysiological signals, or magnetic resonance imaging data, we rely on software to acquire, analyze, and statistically evaluate the raw data we obtain - or to generate such data if we work with simulations.","In recent years there has been a shift toward relying on free, open-source scientific software (FOSSS) for neuroscience data analysis (Poldrack et al., 2019), in line with the broader open science movement in academia (McKiernan et al., 2016) and wider industry trends (Eghbal, 2016).","Importantly, FOSSS is typically developed by working scientists (not professional software developers) which sets up a precarious situation given the nature of the typical academic workplace (wherein academics, especially in their early careers, are on short and fixed term contracts).","In this paper, we will argue that the existing ecosystem of neuroscientific open source software is brittle, and discuss why and how the neuroscience community needs to come together to ensure a healthy growth of our software landscape to the benefit of all."],"url":"http://arxiv.org/abs/2403.19394v1","category":"cs.CY"}
{"created":"2024-03-28 13:06:08","title":"Memory signatures in path curvature of self-avoidant model particles are revealed by time delayed self mutual information","abstract":"Emergent behavior in active systems is a complex byproduct of local, often pairwise, interactions. One such interaction is self-avoidance, which experimentally can arise as a response to self-generated environmental signals; such experiments have inspired non-Markovian mathematical models. In previous work, we set out to find ``hallmarks of self-avoidant memory\" in a particle model for environmentally responsive swimming droplets. In our analysis, we found that transient self-trapping was a spatial hallmark of the particle's self-avoidant memory response. The self-trapping results from the combined effects of behaviors at multiple scales: random reorientations, which occur on the diffusion scale, and the self-avoidant memory response, which occurs on the ballistic (and longer) timescales. In this work, we use the path curvature as it encodes the self-trapping response to estimate an ``effective memory lifetime\" by analyzing the decay of its time-delayed mutual information and subsequently determining the longevity of significant nonlinear correlations. This effective memory lifetime (EML) is longer in systems where the curvature is a product of both self-avoidance and random reorientations as compared to systems without self-avoidance.","sentences":["Emergent behavior in active systems is a complex byproduct of local, often pairwise, interactions.","One such interaction is self-avoidance, which experimentally can arise as a response to self-generated environmental signals; such experiments have inspired non-Markovian mathematical models.","In previous work, we set out to find ``hallmarks of self-avoidant memory\" in a particle model for environmentally responsive swimming droplets.","In our analysis, we found that transient self-trapping was a spatial hallmark of the particle's self-avoidant memory response.","The self-trapping results from the combined effects of behaviors at multiple scales: random reorientations, which occur on the diffusion scale, and the self-avoidant memory response, which occurs on the ballistic (and longer) timescales.","In this work, we use the path curvature as it encodes the self-trapping response to estimate an ``effective memory lifetime\" by analyzing the decay of its time-delayed mutual information and subsequently determining the longevity of significant nonlinear correlations.","This effective memory lifetime (EML) is longer in systems where the curvature is a product of both self-avoidance and random reorientations as compared to systems without self-avoidance."],"url":"http://arxiv.org/abs/2403.19393v1","category":"cond-mat.soft"}
{"created":"2024-03-28 13:06:07","title":"Tachyonic instability and spontaneous scalarization in parameterized Schwarzschild-like black holes","abstract":"We study the phenomenon of spontaneous scalarization in parameterized Schwarzschild-like black holes. Two metrics are considered, the Konoplya-Zhidenko metric and the Johannsen-Psaltis metric. While these metrics can mimic the Schwarzschild black hole well in the weak-field regime, they have deformed geometries in the near-horizon strong-field region. Such deformations notably influence the emergence of tachyonic instability and subsequent spontaneous scalarization, enabling a clear distinction between these parameterized metrics and the standard Schwarzschild metric. These results suggest a possible way to test the parameterized black holes and thus the Kerr hypothesis by observing the phenomenon of spontaneous scalarization.","sentences":["We study the phenomenon of spontaneous scalarization in parameterized Schwarzschild-like black holes.","Two metrics are considered, the Konoplya-Zhidenko metric and the Johannsen-Psaltis metric.","While these metrics can mimic the Schwarzschild black hole well in the weak-field regime, they have deformed geometries in the near-horizon strong-field region.","Such deformations notably influence the emergence of tachyonic instability and subsequent spontaneous scalarization, enabling a clear distinction between these parameterized metrics and the standard Schwarzschild metric.","These results suggest a possible way to test the parameterized black holes and thus the Kerr hypothesis by observing the phenomenon of spontaneous scalarization."],"url":"http://arxiv.org/abs/2403.19392v1","category":"gr-qc"}
{"created":"2024-03-28 13:01:18","title":"Checkpoint Merging via Bayesian Optimization in LLM Pretraining","abstract":"The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs. To alleviate this issue, we propose checkpoint merging in pretraining LLM. This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization. Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining.","sentences":["The rapid proliferation of large language models (LLMs) such as GPT-4 and Gemini underscores the intense demand for resources during their training processes, posing significant challenges due to substantial computational and environmental costs.","To alleviate this issue, we propose checkpoint merging in pretraining LLM.","This method utilizes LLM checkpoints with shared training trajectories, and is rooted in an extensive search space exploration for the best merging weight via Bayesian optimization.","Through various experiments, we demonstrate that: (1) Our proposed methodology exhibits the capacity to augment pretraining, presenting an opportunity akin to obtaining substantial benefits at minimal cost; (2) Our proposed methodology, despite requiring a given held-out dataset, still demonstrates robust generalization capabilities across diverse domains, a pivotal aspect in pretraining."],"url":"http://arxiv.org/abs/2403.19390v1","category":"cs.CL"}
{"created":"2024-03-28 12:58:48","title":"Spin couplings as witnesses of Planck scale phenomenology","abstract":"Modified dispersion relations (MDRs) provide a phenomenological approach beyond the Standard Model, intended to capture tentative nonperturbative quantum gravity effects. Here, we show how to constrain a large class of MDRs by deriving the explicit corrections they induce upon the fermion spin coupling with an external magnetic field in the Pauli equation, the non-relativistic limit of the Dirac equation. We complete the general theoretical analysis with explicit phenomenological examples for which the physical free parameters can be bounded by high-precision tests, in particular the ones based on the measurement of the electron anomalous magnetic moment in a Penning trap.","sentences":["Modified dispersion relations (MDRs) provide a phenomenological approach beyond the Standard Model, intended to capture tentative nonperturbative quantum gravity effects.","Here, we show how to constrain a large class of MDRs by deriving the explicit corrections they induce upon the fermion spin coupling with an external magnetic field in the Pauli equation, the non-relativistic limit of the Dirac equation.","We complete the general theoretical analysis with explicit phenomenological examples for which the physical free parameters can be bounded by high-precision tests, in particular the ones based on the measurement of the electron anomalous magnetic moment in a Penning trap."],"url":"http://arxiv.org/abs/2403.19389v1","category":"hep-th"}
{"created":"2024-03-28 12:56:53","title":"Cosystolic Expansion of Sheaves on Posets with Applications to Good 2-Query LTCs and Lifted Codes","abstract":"We study sheaves on posets, showing that cosystolic expansion of such sheaves can be derived from local expansion conditions of the sheaf and the poset (typically a high dimensional expander). When the poset at hand is a cell complex, a sheaf on it may be thought of as generalizing coefficient groups used for defining homology and cohomology, by letting the coefficient group vary along the cell complex. Previous works established local criteria for cosystolic expansion only for simplicial complexes and with respect to constant coefficients. Cosystolic expansion of sheaves is related to property testing. We use this relation and our local criterion for cosystolic expansion to give two applications to locally testable codes (LTCs).   First, we show the existence of good $2$-query LTCs. These codes are related to the recent good $q$-query LTCs of Dinur et. al and Panteleev-Kalachev, being the formers' so-called line codes, but we get them from a new, more illuminating perspective, namely, by realizing them as cocycle codes of sheaves over posets. We then derive their good properties directly from our criterion for cosystolic expansion.   Second, we give a local criterion for a a lifted code (with some auxiliary structure) to be locally testable. This improves on a previous work of Dikstein et. al, where it was shown that one can obtain local testability of lifted codes from a mixture of local and global conditions.","sentences":["We study sheaves on posets, showing that cosystolic expansion of such sheaves can be derived from local expansion conditions of the sheaf and the poset (typically a high dimensional expander).","When the poset at hand is a cell complex, a sheaf on it may be thought of as generalizing coefficient groups used for defining homology and cohomology, by letting the coefficient group vary along the cell complex.","Previous works established local criteria for cosystolic expansion only for simplicial complexes and with respect to constant coefficients.","Cosystolic expansion of sheaves is related to property testing.","We use this relation and our local criterion for cosystolic expansion to give two applications to locally testable codes (LTCs).   ","First, we show the existence of good $2$-query LTCs.","These codes are related to the recent good $q$-query LTCs of Dinur et.","al and Panteleev-Kalachev, being the formers' so-called line codes, but we get them from a new, more illuminating perspective, namely, by realizing them as cocycle codes of sheaves over posets.","We then derive their good properties directly from our criterion for cosystolic expansion.   ","Second, we give a local criterion for a a lifted code (with some auxiliary structure) to be locally testable.","This improves on a previous work of Dikstein et.","al, where it was shown that one can obtain local testability of lifted codes from a mixture of local and global conditions."],"url":"http://arxiv.org/abs/2403.19388v1","category":"math.CO"}
{"created":"2024-03-28 12:51:15","title":"PointCloud-Text Matching: Benchmark Datasets and a Baseline","abstract":"In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query. PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval. However, there exists no suitable and targeted dataset for PTM in practice. Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA. We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM. To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa). RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrastive Learning module (RNCL). Specifically, DAP leverages token-level and feature-level attention to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity. To handle noisy correspondence, RNCL divides negative pairs, which are much less error-prone than positive pairs, into clean and noisy subsets, and assigns them forward and reverse optimization directions respectively, thus enhancing robustness against noisy correspondence. We conduct extensive experiments on our benchmarks and demonstrate the superiority of our RoMa.","sentences":["In this paper, we present and study a new instance-level retrieval task: PointCloud-Text Matching~(PTM), which aims to find the exact cross-modal instance that matches a given point-cloud query or text query.","PTM could be applied to various scenarios, such as indoor/urban-canyon localization and scene retrieval.","However, there exists no suitable and targeted dataset for PTM in practice.","Therefore, we construct three new PTM benchmark datasets, namely 3D2T-SR, 3D2T-NR, and 3D2T-QA.","We observe that the data is challenging and with noisy correspondence due to the sparsity, noise, or disorder of point clouds and the ambiguity, vagueness, or incompleteness of texts, which make existing cross-modal matching methods ineffective for PTM.","To tackle these challenges, we propose a PTM baseline, named Robust PointCloud-Text Matching method (RoMa).","RoMa consists of two modules: a Dual Attention Perception module (DAP) and a Robust Negative Contrastive Learning module (RNCL).","Specifically, DAP leverages token-level and feature-level attention to adaptively focus on useful local and global features, and aggregate them into common representations, thereby reducing the adverse impact of noise and ambiguity.","To handle noisy correspondence, RNCL divides negative pairs, which are much less error-prone than positive pairs, into clean and noisy subsets, and assigns them forward and reverse optimization directions respectively, thus enhancing robustness against noisy correspondence.","We conduct extensive experiments on our benchmarks and demonstrate the superiority of our RoMa."],"url":"http://arxiv.org/abs/2403.19386v1","category":"cs.CV"}
{"created":"2024-03-28 12:49:29","title":"Even-order optical harmonics generated from centrosymmetric-material metasurfaces","abstract":"Generation of even-order optical harmonics requires noncentrosymmetric structures being conventionally observed in crystals lacking the center of inversion. In centrosymmetric systems, even-order harmonics may arise, e.g., at surfaces but such effects are usually very weak. Here we observe optical harmonics up to 4-th order generated under the normal incidence from centrosymmetric dielectric metasurfaces empowered by resonances. We design silicon metasurfaces supporting optical quasibound states in the continuum and guided-mode resonances, and demonstrate the enhancement of second-harmonic signals by over three orders of magnitude compared to nonresonant thin films. Under the optimal conditions, the brightness of the second harmonic approaches that of the third harmonic, and the 4th-order harmonic becomes detectable.","sentences":["Generation of even-order optical harmonics requires noncentrosymmetric structures being conventionally observed in crystals lacking the center of inversion.","In centrosymmetric systems, even-order harmonics may arise, e.g., at surfaces but such effects are usually very weak.","Here we observe optical harmonics up to 4-th order generated under the normal incidence from centrosymmetric dielectric metasurfaces empowered by resonances.","We design silicon metasurfaces supporting optical quasibound states in the continuum and guided-mode resonances, and demonstrate the enhancement of second-harmonic signals by over three orders of magnitude compared to nonresonant thin films.","Under the optimal conditions, the brightness of the second harmonic approaches that of the third harmonic, and the 4th-order harmonic becomes detectable."],"url":"http://arxiv.org/abs/2403.19385v1","category":"physics.optics"}
{"created":"2024-03-28 12:45:43","title":"Impact of JLab data on the determination of GPDs at zero skewness and new insights from transition form factors $ N\\rightarrow \u0394$","abstract":"It is well established now that the generalized parton distributions (GPDs) at zero skewness are playing important roles in some physical process such as elastic electron-nucleon scattering, elastic (anti)neutrino-nucleon scattering, and wide-angle Compton scattering (WACS) via various types of form factors (FFs). In this study, we are going to utilize the recent JLab measurements of the elastic electron-nucleon scattering reduced cross-section, namely GMp12, as a touchstone to unravel the tension observed between the measurements of the WACS cross-section and the data of the proton magnetic FF $ G_M^p $. We also investigate the impact of GMp12 data on valence unpolarized GPDs $ H_v^q $ and $ E_v^q $ at zero skewness by performing some $ \\chi^2 $ analyses of the related experimental data. By calculating the electric and scalar quadrupole ratios, $ R_{EM} $ and $ R_{SM} $, and magnetic transition FF $ G^*_M/3G_D $ related to the nucleon-to-delta ($ N\\rightarrow \\Delta $) transition using the extracted GPDs and comparing the results with corresponding experimental measurements, we show that our results are in an excellent consistency with experiments, indicating the universality property of GPDs. We emphasize that the inclusion of these data in the future analysis of GPDs can significantly the extracted GPDs especially their uncertainties at smaller values of $ Q^2 $.","sentences":["It is well established now that the generalized parton distributions (GPDs) at zero skewness are playing important roles in some physical process such as elastic electron-nucleon scattering, elastic (anti)neutrino-nucleon scattering, and wide-angle Compton scattering (WACS) via various types of form factors (FFs).","In this study, we are going to utilize the recent JLab measurements of the elastic electron-nucleon scattering reduced cross-section, namely GMp12, as a touchstone to unravel the tension observed between the measurements of the WACS cross-section and the data of the proton magnetic FF $ G_M^p $.","We also investigate the impact of GMp12 data on valence unpolarized GPDs $ H_v^q $ and $ E_v^q $ at zero skewness by performing some $ \\chi^2 $ analyses of the related experimental data.","By calculating the electric and scalar quadrupole ratios, $ R_{EM} $ and $ R_{SM} $, and magnetic transition FF $ G^*_M/3G_D $ related to the nucleon-to-delta ($ N\\rightarrow \\Delta $) transition using the extracted GPDs and comparing the results with corresponding experimental measurements, we show that our results are in an excellent consistency with experiments, indicating the universality property of GPDs.","We emphasize that the inclusion of these data in the future analysis of GPDs can significantly the extracted GPDs especially their uncertainties at smaller values of $ Q^2 $."],"url":"http://arxiv.org/abs/2403.19384v1","category":"hep-ph"}
{"created":"2024-03-28 12:45:34","title":"Vacuum Petrov type D horizons of non-trivial $U(1)$ bundle structure over Riemann surfaces with genus $> 0$","abstract":"We consider isolated horizons (Killing horizons up to the second order) whose null flow has the structure of a U(1) principal fiber bundle over a compact Riemann surface. We impose the vacuum Einstein equations (with the cosmological constant) and the condition that the spacetime Weyl tensor is of Petrov D type on the geometry of the horizons. We derive all the solutions in the case when the genus of the surface is $>1$. By doing so for all the non-trivial bundles, we complete the classification. We construct the embedding spacetimes and show that they are locally isometric to the toroidal or hyperbolic generalization of the Taub-NUT-(anti-) de Sitter spacetimes for horizons of genus $1$ or $>1$ respectively, after performing Misner's identification of the spacetime. The horizon bundle structure can be naturally extended to bundle structure defined on the entire spacetime.","sentences":["We consider isolated horizons (Killing horizons up to the second order) whose null flow has the structure of a U(1) principal fiber bundle over a compact Riemann surface.","We impose the vacuum Einstein equations (with the cosmological constant) and the condition that the spacetime Weyl tensor is of Petrov D type on the geometry of the horizons.","We derive all the solutions in the case when the genus of the surface is $>1$. By doing so for all the non-trivial bundles, we complete the classification.","We construct the embedding spacetimes and show that they are locally isometric to the toroidal or hyperbolic generalization of the Taub-NUT-(anti-) de Sitter spacetimes for horizons of genus $1$ or $>1$ respectively, after performing Misner's identification of the spacetime.","The horizon bundle structure can be naturally extended to bundle structure defined on the entire spacetime."],"url":"http://arxiv.org/abs/2403.19383v1","category":"gr-qc"}
{"created":"2024-03-28 12:42:25","title":"On Uncertainty Quantification for Near-Bayes Optimal Algorithms","abstract":"Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications. Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart. In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t. an unknown task distribution. We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm. We further propose a practical uncertainty quantification method that apply to general ML algorithms. Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method.","sentences":["Bayesian modelling allows for the quantification of predictive uncertainty which is crucial in safety-critical applications.","Yet for many machine learning (ML) algorithms, it is difficult to construct or implement their Bayesian counterpart.","In this work we present a promising approach to address this challenge, based on the hypothesis that commonly used ML algorithms are efficient across a wide variety of tasks and may thus be near Bayes-optimal w.r.t.","an unknown task distribution.","We prove that it is possible to recover the Bayesian posterior defined by the task distribution, which is unknown but optimal in this setting, by building a martingale posterior using the algorithm.","We further propose a practical uncertainty quantification method that apply to general ML algorithms.","Experiments based on a variety of non-NN and NN algorithms demonstrate the efficacy of our method."],"url":"http://arxiv.org/abs/2403.19381v1","category":"stat.ML"}
{"created":"2024-03-28 12:38:21","title":"NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data","abstract":"The acquisition of objects outside the Line-of-Sight of cameras is a very intriguing but also extremely challenging research topic. Recent works showed the feasibility of this idea exploiting transient imaging data produced by custom direct Time of Flight sensors. In this paper, for the first time, we tackle this problem using only data from an off-the-shelf indirect Time of Flight sensor without any further hardware requirement. We introduced a Deep Learning model able to re-frame the surfaces where light bounces happen as a virtual mirror. This modeling makes the task easier to handle and also facilitates the construction of annotated training data. From the obtained data it is possible to retrieve the depth information of the hidden scene. We also provide a first-in-its-kind synthetic dataset for the task and demonstrate the feasibility of the proposed idea over it.","sentences":["The acquisition of objects outside the Line-of-Sight of cameras is a very intriguing but also extremely challenging research topic.","Recent works showed the feasibility of this idea exploiting transient imaging data produced by custom direct Time of Flight sensors.","In this paper, for the first time, we tackle this problem using only data from an off-the-shelf indirect Time of Flight sensor without any further hardware requirement.","We introduced a Deep Learning model able to re-frame the surfaces where light bounces happen as a virtual mirror.","This modeling makes the task easier to handle and also facilitates the construction of annotated training data.","From the obtained data it is possible to retrieve the depth information of the hidden scene.","We also provide a first-in-its-kind synthetic dataset for the task and demonstrate the feasibility of the proposed idea over it."],"url":"http://arxiv.org/abs/2403.19376v1","category":"cs.CV"}
{"created":"2024-03-28 12:37:11","title":"Multi-Agent Team Access Monitoring: Environments that Benefit from Target Information Sharing","abstract":"Robotic access monitoring of multiple target areas has applications including checkpoint enforcement, surveillance and containment of fire and flood hazards. Monitoring access for a single target region has been successfully modeled as a minimum-cut problem. We generalize this model to support multiple target areas using two approaches: iterating on individual targets and examining the collections of targets holistically. Through simulation we measure the performance of each approach on different scenarios.","sentences":["Robotic access monitoring of multiple target areas has applications including checkpoint enforcement, surveillance and containment of fire and flood hazards.","Monitoring access for a single target region has been successfully modeled as a minimum-cut problem.","We generalize this model to support multiple target areas using two approaches: iterating on individual targets and examining the collections of targets holistically.","Through simulation we measure the performance of each approach on different scenarios."],"url":"http://arxiv.org/abs/2403.19375v1","category":"cs.RO"}
{"created":"2024-03-28 12:33:34","title":"Origin of multiple Lifshitz transitions in the Weyl semi-metal RhSi","abstract":"It is known from density functional theory (DFT) calculations that RhSi has a multifold degenerate Dirac point at the Fermi energy, with the dominant states in the low-energy region displaying mostly Rh $d$ character. Using DFT+U, we calculate the band structure by considering an effective local interaction on the Rh $d$ states, with a realistic effective Hubbard $U_\\textrm{eff}=2.5$ eV derived from a constrained random-phase approximation calculation, and find the emergence of a double hump structure close to the Fermi energy.By further deriving a low-energy tight-binding model from our first-principles results, we show that the double hump is a direct consequence of a competition between the Rh $d$-Rh $d$ and Rh $d$-Si $p$ interactions, which differ in their momentum dependence. As a consequence, through an artificial tuning of the energy level of the Si $p$ orbitals this hump structure can be suppressed due to the effectively reduced Rh $d$ -Si $p$ interaction.This peculiar low-energy electronic structure additionally results in that a small hole/electron doping ($\\sim$ 0.1 $\\%$) can tune the Fermi surface topology, going from closed to open Fermi surfaces, which has dramatic consequences for the thermal transport.","sentences":["It is known from density functional theory (DFT) calculations that RhSi has a multifold degenerate Dirac point at the Fermi energy, with the dominant states in the low-energy region displaying mostly Rh $d$ character.","Using DFT+U, we calculate the band structure by considering an effective local interaction on the Rh $d$ states, with a realistic effective Hubbard $U_\\textrm{eff}=2.5$ eV derived from a constrained random-phase approximation calculation, and find the emergence of a double hump structure close to the Fermi energy.","By further deriving a low-energy tight-binding model from our first-principles results, we show that the double hump is a direct consequence of a competition between the Rh $d$-Rh $d$ and Rh $d$-Si $p$ interactions, which differ in their momentum dependence.","As a consequence, through an artificial tuning of the energy level of the Si $p$ orbitals this hump structure can be suppressed due to the effectively reduced Rh $d$ -Si","$p$ interaction.","This peculiar low-energy electronic structure additionally results in that a small hole/electron doping ($\\sim$ 0.1 $\\%$) can tune the Fermi surface topology, going from closed to open Fermi surfaces, which has dramatic consequences for the thermal transport."],"url":"http://arxiv.org/abs/2403.19370v1","category":"cond-mat.str-el"}
{"created":"2024-03-28 12:32:24","title":"RAIL: Robot Affordance Imagination with Large Language Models","abstract":"This paper introduces an automatic affordance reasoning paradigm tailored to minimal semantic inputs, addressing the critical challenges of classifying and manipulating unseen classes of objects in household settings. Inspired by human cognitive processes, our method integrates generative language models and physics-based simulators to foster analytical thinking and creative imagination of novel affordances. Structured with a tripartite framework consisting of analysis, imagination, and evaluation, our system \"analyzes\" the requested affordance names into interaction-based definitions, \"imagines\" the virtual scenarios, and \"evaluates\" the object affordance. If an object is recognized as possessing the requested affordance, our method also predicts the optimal pose for such functionality, and how a potential user can interact with it. Tuned on only a few synthetic examples across 3 affordance classes, our pipeline achieves a very high success rate on affordance classification and functional pose prediction of 8 classes of novel objects, outperforming learning-based baselines. Validation through real robot manipulating experiments demonstrates the practical applicability of the imagined user interaction, showcasing the system's ability to independently conceptualize unseen affordances and interact with new objects and scenarios in everyday settings.","sentences":["This paper introduces an automatic affordance reasoning paradigm tailored to minimal semantic inputs, addressing the critical challenges of classifying and manipulating unseen classes of objects in household settings.","Inspired by human cognitive processes, our method integrates generative language models and physics-based simulators to foster analytical thinking and creative imagination of novel affordances.","Structured with a tripartite framework consisting of analysis, imagination, and evaluation, our system \"analyzes\" the requested affordance names into interaction-based definitions, \"imagines\" the virtual scenarios, and \"evaluates\" the object affordance.","If an object is recognized as possessing the requested affordance, our method also predicts the optimal pose for such functionality, and how a potential user can interact with it.","Tuned on only a few synthetic examples across 3 affordance classes, our pipeline achieves a very high success rate on affordance classification and functional pose prediction of 8 classes of novel objects, outperforming learning-based baselines.","Validation through real robot manipulating experiments demonstrates the practical applicability of the imagined user interaction, showcasing the system's ability to independently conceptualize unseen affordances and interact with new objects and scenarios in everyday settings."],"url":"http://arxiv.org/abs/2403.19369v1","category":"cs.RO"}
{"created":"2024-03-28 12:28:58","title":"Infrared Small Target Detection with Scale and Location Sensitivity","abstract":"Recently, infrared small target detection (IRSTD) has been dominated by deep-learning-based methods. However, these methods mainly focus on the design of complex model structures to extract discriminative features, leaving the loss functions for IRSTD under-explored. For example, the widely used Intersection over Union (IoU) and Dice losses lack sensitivity to the scales and locations of targets, limiting the detection performance of detectors. In this paper, we focus on boosting detection performance with a more effective loss but a simpler model structure. Specifically, we first propose a novel Scale and Location Sensitive (SLS) loss to handle the limitations of existing losses: 1) for scale sensitivity, we compute a weight for the IoU loss based on target scales to help the detector distinguish targets with different scales: 2) for location sensitivity, we introduce a penalty term based on the center points of targets to help the detector localize targets more precisely. Then, we design a simple Multi-Scale Head to the plain U-Net (MSHNet). By applying SLS loss to each scale of the predictions, our MSHNet outperforms existing state-of-the-art methods by a large margin. In addition, the detection performance of existing detectors can be further improved when trained with our SLS loss, demonstrating the effectiveness and generalization of our SLS loss. The code is available at https://github.com/ying-fu/MSHNet.","sentences":["Recently, infrared small target detection (IRSTD) has been dominated by deep-learning-based methods.","However, these methods mainly focus on the design of complex model structures to extract discriminative features, leaving the loss functions for IRSTD under-explored.","For example, the widely used Intersection over Union (IoU) and Dice losses lack sensitivity to the scales and locations of targets, limiting the detection performance of detectors.","In this paper, we focus on boosting detection performance with a more effective loss but a simpler model structure.","Specifically, we first propose a novel Scale and Location Sensitive (SLS) loss to handle the limitations of existing losses: 1) for scale sensitivity, we compute a weight for the IoU loss based on target scales to help the detector distinguish targets with different scales: 2) for location sensitivity, we introduce a penalty term based on the center points of targets to help the detector localize targets more precisely.","Then, we design a simple Multi-Scale Head to the plain U-Net (MSHNet).","By applying SLS loss to each scale of the predictions, our MSHNet outperforms existing state-of-the-art methods by a large margin.","In addition, the detection performance of existing detectors can be further improved when trained with our SLS loss, demonstrating the effectiveness and generalization of our SLS loss.","The code is available at https://github.com/ying-fu/MSHNet."],"url":"http://arxiv.org/abs/2403.19366v1","category":"cs.CV"}
{"created":"2024-03-28 12:19:46","title":"Polyadic sigma matrices","abstract":"We generalize $\\sigma$-matrices to higher arities using the polyadization procedure proposed by the author. We build the nonderived $n$-ary version of $SU\\left( 2\\right) $ using cyclic shift block matrices. We define a new function, the polyadic trace, which has an additivity property analogous to the ordinary trace for block diagonal matrices and which can be used to build the corresponding invariants. The elementary $\\Sigma$-matrices introduced here play a role similar to ordinary matrix units, and their sums are full $\\Sigma$-matrices which can be treated as a polyadic analog of $\\sigma$-matrices. The presentation of $n$-ary $SU\\left( 2\\right) $ in terms of full $\\Sigma$-matrices is done using the Hadamard product. We then generalize the Pauli group in two ways: for the binary case we introduce the extended phase shifted $\\sigma$-matrices with multipliers in cyclic groups of order $4q$ ($q>4$), and for the polyadic case we construct the correspondent finite $n$-ary semigroup of phase-shifted elementary $\\Sigma$-matrices of order $4q\\left( n-1\\right) +1$, and the finite $n$-ary group of phase-shifted full $\\Sigma$-matrices of order $4q$. Finally, we introduce the finite $n$-ary group of heterogeneous full $\\mathit{\\Sigma}^{het}$-matrices of order $\\left( 4q\\left( n-1\\right) \\right) ^{4}$. Some examples of the lowest arities are presented.","sentences":["We generalize $\\sigma$-matrices to higher arities using the polyadization procedure proposed by the author.","We build the nonderived $n$-ary version of $SU\\left( 2\\right) $ using cyclic shift block matrices.","We define a new function, the polyadic trace, which has an additivity property analogous to the ordinary trace for block diagonal matrices and which can be used to build the corresponding invariants.","The elementary $\\Sigma$-matrices introduced here play a role similar to ordinary matrix units, and their sums are full $\\Sigma$-matrices which can be treated as a polyadic analog of $\\sigma$-matrices.","The presentation of $n$-ary $SU\\left( 2\\right) $ in terms of full $\\Sigma$-matrices is done using the Hadamard product.","We then generalize the Pauli group in two ways: for the binary case we introduce the extended phase shifted $\\sigma$-matrices with multipliers in cyclic groups of order $4q$ ($q>4$), and for the polyadic case we construct the correspondent finite $n$-ary semigroup of phase-shifted elementary $\\Sigma$-matrices of order $4q\\left( n-1\\right) +1$, and the finite $n$-ary group of phase-shifted full $\\Sigma$-matrices of order $4q$. Finally, we introduce the finite $n$-ary group of heterogeneous full $\\mathit{\\Sigma}^{het}$-matrices of order $\\left( 4q\\left( n-1\\right) \\right) ^{4}$.","Some examples of the lowest arities are presented."],"url":"http://arxiv.org/abs/2403.19361v1","category":"math.GR"}
{"created":"2024-03-28 12:15:21","title":"Radio imaging of gravitationally lensed radio-quiet quasars","abstract":"We present 6-GHz Very Large Array radio images of 70 gravitational lens systems at 300-mas resolution, in which the source is an optically-selected quasar, and nearly all of which have two lensed images. We find that about in half of the systems (40/70, with 33/70 secure), one or more lensed images are detected down to our detection limit of 20microJy/beam, similar to previous investigations and reinforcing the conclusion that typical optically-selected quasars have intrinsic GHz radio flux densities of a few microJy ($\\sim10^{23}$WHz$^{-1}$) at redshifts of 1--2. In addition, for ten cases it is likely that the lensing galaxies are detected in the radio. Available detections of, and limits on the far-infrared luminosities from the literature, suggest that nearly all of the sample lie on the radio-FIR correlation typical of star-forming galaxies, and that their radio luminosities are at least compatible with the radio emission being produced by star formation processes. One object, WISE2329$-$1258, has an extra radio component that is not present in optical images, and is difficult to explain using simple lens models. In-band spectral indices, where these can be determined, are generally moderately steep and consistent with synchrotron processes either from star-formation/supernovae or AGN. Comparison of the A/B image flux ratios at radio and optical wavelengths suggests a 10 per cent level contribution from finite source effects or optical extinction to the optical flux ratios, together with sporadic larger discrepancies that are likely to be due to optical microlensing.","sentences":["We present 6-GHz Very Large Array radio images of 70 gravitational lens systems at 300-mas resolution, in which the source is an optically-selected quasar, and nearly all of which have two lensed images.","We find that about in half of the systems (40/70, with 33/70 secure), one or more lensed images are detected down to our detection limit of 20microJy/beam, similar to previous investigations and reinforcing the conclusion that typical optically-selected quasars have intrinsic GHz radio flux densities of a few microJy ($\\sim10^{23}$WHz$^{-1}$) at redshifts of 1--2.","In addition, for ten cases it is likely that the lensing galaxies are detected in the radio.","Available detections of, and limits on the far-infrared luminosities from the literature, suggest that nearly all of the sample lie on the radio-FIR correlation typical of star-forming galaxies, and that their radio luminosities are at least compatible with the radio emission being produced by star formation processes.","One object, WISE2329$-$1258, has an extra radio component that is not present in optical images, and is difficult to explain using simple lens models.","In-band spectral indices, where these can be determined, are generally moderately steep and consistent with synchrotron processes either from star-formation/supernovae or AGN.","Comparison of the A/B image flux ratios at radio and optical wavelengths suggests a 10 per cent level contribution from finite source effects or optical extinction to the optical flux ratios, together with sporadic larger discrepancies that are likely to be due to optical microlensing."],"url":"http://arxiv.org/abs/2403.19357v1","category":"astro-ph.GA"}
{"created":"2024-03-28 12:11:58","title":"A robust two-level overlapping preconditioner for Darcy flow in high-contrast media","abstract":"In this article, a two-level overlapping domain decomposition preconditioner is developed for solving linear algebraic systems obtained from simulating Darcy flow in high-contrast media. Our preconditioner starts at a mixed finite element method for discretizing the partial differential equation by Darcy's law with the no-flux boundary condition and is then followed by a velocity elimination technique to yield a linear algebraic system with only unknowns of pressure. Then, our main objective is to design a robust and efficient domain decomposition preconditioner for this system, which is accomplished by engineering a multiscale coarse space that is capable of characterizing high-contrast features of the permeability field. A generalized eigenvalue problem is solved in each non-overlapping coarse element in a communication-free manner to form the global solver, which is accompanied by local solvers originated from additive Schwarz methods but with a non-Galerkin discretization to derive the two-level preconditioner. We provide a rigorous analysis that indicates that the condition number of the preconditioned system could be bounded above with several assumptions. Extensive numerical experiments with various types of three-dimensional high-contrast models are exhibited. In particular, we study the robustness against the contrast of the media as well as the influences of numbers of eigenfunctions, oversampling sizes, and subdomain partitions on the efficiency of the proposed preconditioner. Besides, strong and weak scalability performances are also examined.","sentences":["In this article, a two-level overlapping domain decomposition preconditioner is developed for solving linear algebraic systems obtained from simulating Darcy flow in high-contrast media.","Our preconditioner starts at a mixed finite element method for discretizing the partial differential equation by Darcy's law with the no-flux boundary condition and is then followed by a velocity elimination technique to yield a linear algebraic system with only unknowns of pressure.","Then, our main objective is to design a robust and efficient domain decomposition preconditioner for this system, which is accomplished by engineering a multiscale coarse space that is capable of characterizing high-contrast features of the permeability field.","A generalized eigenvalue problem is solved in each non-overlapping coarse element in a communication-free manner to form the global solver, which is accompanied by local solvers originated from additive Schwarz methods but with a non-Galerkin discretization to derive the two-level preconditioner.","We provide a rigorous analysis that indicates that the condition number of the preconditioned system could be bounded above with several assumptions.","Extensive numerical experiments with various types of three-dimensional high-contrast models are exhibited.","In particular, we study the robustness against the contrast of the media as well as the influences of numbers of eigenfunctions, oversampling sizes, and subdomain partitions on the efficiency of the proposed preconditioner.","Besides, strong and weak scalability performances are also examined."],"url":"http://arxiv.org/abs/2403.19356v1","category":"math.NA"}
{"created":"2024-03-28 12:11:29","title":"Artificial Intelligence (AI) Based Prediction of Mortality, for COVID-19 Patients","abstract":"For severely affected COVID-19 patients, it is crucial to identify high-risk patients and predict survival and need for intensive care (ICU). Most of the proposed models are not well reported making them less reproducible and prone to high risk of bias particularly in presence of imbalance data/class. In this study, the performances of nine machine and deep learning algorithms in combination with two widely used feature selection methods were investigated to predict last status representing mortality, ICU requirement, and ventilation days. Fivefold cross-validation was used for training and validation purposes. To minimize bias, the training and testing sets were split maintaining similar distributions. Only 10 out of 122 features were found to be useful in prediction modelling with Acute kidney injury during hospitalization feature being the most important one. The algorithms performances depend on feature numbers and data pre-processing techniques. LSTM performs the best in predicting last status and ICU requirement with 90%, 92%, 86% and 95% accuracy, sensitivity, specificity, and AUC respectively. DNN performs the best in predicting Ventilation days with 88% accuracy. Considering all the factors and limitations including absence of exact time point of clinical onset, LSTM with carefully selected features can accurately predict last status and ICU requirement. DNN performs the best in predicting Ventilation days. Appropriate machine learning algorithm with carefully selected features and balance data can accurately predict mortality, ICU requirement and ventilation support. Such model can be very useful in emergency and pandemic where prompt and precise","sentences":["For severely affected COVID-19 patients, it is crucial to identify high-risk patients and predict survival and need for intensive care (ICU).","Most of the proposed models are not well reported making them less reproducible and prone to high risk of bias particularly in presence of imbalance data/class.","In this study, the performances of nine machine and deep learning algorithms in combination with two widely used feature selection methods were investigated to predict last status representing mortality, ICU requirement, and ventilation days.","Fivefold cross-validation was used for training and validation purposes.","To minimize bias, the training and testing sets were split maintaining similar distributions.","Only 10 out of 122 features were found to be useful in prediction modelling with Acute kidney injury during hospitalization feature being the most important one.","The algorithms performances depend on feature numbers and data pre-processing techniques.","LSTM performs the best in predicting last status and ICU requirement with 90%, 92%, 86% and 95% accuracy, sensitivity, specificity, and AUC respectively.","DNN performs the best in predicting Ventilation days with 88% accuracy.","Considering all the factors and limitations including absence of exact time point of clinical onset, LSTM with carefully selected features can accurately predict last status and ICU requirement.","DNN performs the best in predicting Ventilation days.","Appropriate machine learning algorithm with carefully selected features and balance data can accurately predict mortality, ICU requirement and ventilation support.","Such model can be very useful in emergency and pandemic where prompt and precise"],"url":"http://arxiv.org/abs/2403.19355v1","category":"cs.LG"}
{"created":"2024-03-28 12:10:30","title":"AIpom at SemEval-2024 Task 8: Detecting AI-produced Outputs in M4","abstract":"This paper describes AIpom, a system designed to detect a boundary between human-written and machine-generated text (SemEval-2024 Task 8, Subtask C: Human-Machine Mixed Text Detection). We propose a two-stage pipeline combining predictions from an instruction-tuned decoder-only model and encoder-only sequence taggers. AIpom is ranked second on the leaderboard while achieving a Mean Absolute Error of 15.94. Ablation studies confirm the benefits of pipelining encoder and decoder models, particularly in terms of improved performance.","sentences":["This paper describes AIpom, a system designed to detect a boundary between human-written and machine-generated text (SemEval-2024 Task 8, Subtask C: Human-Machine Mixed Text Detection).","We propose a two-stage pipeline combining predictions from an instruction-tuned decoder-only model and encoder-only sequence taggers.","AIpom is ranked second on the leaderboard while achieving a Mean Absolute Error of 15.94.","Ablation studies confirm the benefits of pipelining encoder and decoder models, particularly in terms of improved performance."],"url":"http://arxiv.org/abs/2403.19354v1","category":"cs.CL"}
{"created":"2024-03-28 12:07:28","title":"Development of wavelength-shifting PEN foils for next generation experiments","abstract":"Polyethylene naphthalate (PEN) foils have been demonstrated as a wavelength shifter suitable for operation in liquid argon. At the same time, wavelength shifting efficiency of technical grades of PEN, commercially available on the market, is lower than that of tetraphenyl butadiene (TPB). This paper reports on an R&D program focused on exploring the intrinsic limitations of PEN and optimizing it for the highest achievable wavelength shifting efficiency.","sentences":["Polyethylene naphthalate (PEN) foils have been demonstrated as a wavelength shifter suitable for operation in liquid argon.","At the same time, wavelength shifting efficiency of technical grades of PEN, commercially available on the market, is lower than that of tetraphenyl butadiene (TPB).","This paper reports on an R&D program focused on exploring the intrinsic limitations of PEN and optimizing it for the highest achievable wavelength shifting efficiency."],"url":"http://arxiv.org/abs/2403.19350v1","category":"astro-ph.IM"}
{"created":"2024-03-28 12:07:17","title":"Towards a sensing model using random laser combined with diffuse reflectance spectroscopy","abstract":"The previous research proves that the random laser emission reflects not only the scattering properties but also the absorption properties. The random laser is therefore considered a potential tool for optical properties sensing. Although the qualitative sensing using the random laser is extensively investigated, a quantitative measurement is still rare. In this study, a generalized mathematical quantitative model using random laser combined with diffuse reflectance spectroscopy is proposed for optical sensing in turbid media. This model describes the gain effect of the active medium and the optical properties effect of the passive medium separately. Rhodamine 6G is used as the active medium. Intralipid and ink are employed to demonstrate the effect of the scattering and absorption, respectively. The peak wavelength shift of the random laser is proved to be an ideal sensing parameter for this sensing model. It is also revealed that the scaling parameters in the sensing model are interrelated and can be simplified to one. With this combined model, the direct sensing of optical properties in diverse turbid media is promising.","sentences":["The previous research proves that the random laser emission reflects not only the scattering properties but also the absorption properties.","The random laser is therefore considered a potential tool for optical properties sensing.","Although the qualitative sensing using the random laser is extensively investigated, a quantitative measurement is still rare.","In this study, a generalized mathematical quantitative model using random laser combined with diffuse reflectance spectroscopy is proposed for optical sensing in turbid media.","This model describes the gain effect of the active medium and the optical properties effect of the passive medium separately.","Rhodamine 6G is used as the active medium.","Intralipid and ink are employed to demonstrate the effect of the scattering and absorption, respectively.","The peak wavelength shift of the random laser is proved to be an ideal sensing parameter for this sensing model.","It is also revealed that the scaling parameters in the sensing model are interrelated and can be simplified to one.","With this combined model, the direct sensing of optical properties in diverse turbid media is promising."],"url":"http://arxiv.org/abs/2403.19349v1","category":"physics.optics"}
{"created":"2024-03-28 12:07:10","title":"Efficient Anchor Point Deployment for Low Latency Connectivity in MEC-Assisted C-V2X Scenarios","abstract":"Next-generation cellular networks will play a key role in the evolution of different vertical industries. Low latency will be a major requirement in many related uses cases. This requirement is specially challenging in scenarios with high mobility of end devices, such as vehicular communications. The Multi-Access Edge Computing (MEC) paradigm seeks to satisfy it. In this article we propose the dynamic deployment of anchor point network functions at edge locations and the assignment of terminals to these anchor points with the joint objective of minimizing communications latency and reducing network overhead. We formally define the problem as a multi-objective optimization and also propose a novel heuristic greedy algorithm for approximating the solution. This algorithm compares favorably with baseline and state-of-the-art strategies for latency minimization while reducing the overhead caused by network reconfigurations.","sentences":["Next-generation cellular networks will play a key role in the evolution of different vertical industries.","Low latency will be a major requirement in many related uses cases.","This requirement is specially challenging in scenarios with high mobility of end devices, such as vehicular communications.","The Multi-Access Edge Computing (MEC) paradigm seeks to satisfy it.","In this article we propose the dynamic deployment of anchor point network functions at edge locations and the assignment of terminals to these anchor points with the joint objective of minimizing communications latency and reducing network overhead.","We formally define the problem as a multi-objective optimization and also propose a novel heuristic greedy algorithm for approximating the solution.","This algorithm compares favorably with baseline and state-of-the-art strategies for latency minimization while reducing the overhead caused by network reconfigurations."],"url":"http://arxiv.org/abs/2403.19348v1","category":"cs.NI"}
{"created":"2024-03-28 12:05:15","title":"Breaking the Length Barrier: LLM-Enhanced CTR Prediction in Long Textual User Behaviors","abstract":"With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction. However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors. As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items. To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling. Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions. Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from extensive user sequences and stores them in the offline database. Subsequently, the deeper, trainable layers of the LLM facilitate intricate inter-behavior interactions, thereby generating comprehensive user embeddings. This separation allows the learning of high-level user representations to be independent of low-level behavior encoding, significantly reducing computational complexity. Finally, these refined user embeddings, in conjunction with correspondingly processed item embeddings, are incorporated into the CTR model to compute the CTR scores. Extensive experimental results show that BAHE reduces training time and memory by five times for CTR models using LLMs, especially with longer user sequences. BAHE has been deployed in a real-world system, allowing for daily updates of 50 million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR prediction.","sentences":["With the rise of large language models (LLMs), recent works have leveraged LLMs to improve the performance of click-through rate (CTR) prediction.","However, we argue that a critical obstacle remains in deploying LLMs for practical use: the efficiency of LLMs when processing long textual user behaviors.","As user sequences grow longer, the current efficiency of LLMs is inadequate for training on billions of users and items.","To break through the efficiency barrier of LLMs, we propose Behavior Aggregated Hierarchical Encoding (BAHE) to enhance the efficiency of LLM-based CTR modeling.","Specifically, BAHE proposes a novel hierarchical architecture that decouples the encoding of user behaviors from inter-behavior interactions.","Firstly, to prevent computational redundancy from repeated encoding of identical user behaviors, BAHE employs the LLM's pre-trained shallow layers to extract embeddings of the most granular, atomic user behaviors from extensive user sequences and stores them in the offline database.","Subsequently, the deeper, trainable layers of the LLM facilitate intricate inter-behavior interactions, thereby generating comprehensive user embeddings.","This separation allows the learning of high-level user representations to be independent of low-level behavior encoding, significantly reducing computational complexity.","Finally, these refined user embeddings, in conjunction with correspondingly processed item embeddings, are incorporated into the CTR model to compute the CTR scores.","Extensive experimental results show that BAHE reduces training time and memory by five times for CTR models using LLMs, especially with longer user sequences.","BAHE has been deployed in a real-world system, allowing for daily updates of 50 million CTR data on 8 A100 GPUs, making LLMs practical for industrial CTR prediction."],"url":"http://arxiv.org/abs/2403.19347v1","category":"cs.IR"}
{"created":"2024-03-28 12:04:28","title":"Large Language Models Are Unconscious of Unreasonability in Math Problems","abstract":"Large language models (LLMs) demonstrate substantial capabilities in solving math problems. However, they tend to produce hallucinations when given questions containing unreasonable errors. In this paper, we study the behavior of LLMs when faced with unreasonable math problems and further explore their potential to address these problems. First, we construct the Unreasonable Math Problem (UMP) benchmark to examine the error detection ability of LLMs. Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content. In order to improve their ability of error detection and correction, we further design a strategic prompt template called Critical Calculation and Conclusion(CCC). With CCC, LLMs can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios.","sentences":["Large language models (LLMs) demonstrate substantial capabilities in solving math problems.","However, they tend to produce hallucinations when given questions containing unreasonable errors.","In this paper, we study the behavior of LLMs when faced with unreasonable math problems and further explore their potential to address these problems.","First, we construct the Unreasonable Math Problem (UMP) benchmark to examine the error detection ability of LLMs.","Experiments show that LLMs are able to detect unreasonable errors, but still fail in generating non-hallucinatory content.","In order to improve their ability of error detection and correction, we further design a strategic prompt template called Critical Calculation and Conclusion(CCC).","With CCC, LLMs can better self-evaluate and detect unreasonable errors in math questions, making them more reliable and safe in practical application scenarios."],"url":"http://arxiv.org/abs/2403.19346v1","category":"cs.CL"}
{"created":"2024-03-28 12:02:45","title":"Intelligent Classification and Personalized Recommendation of E-commerce Products Based on Machine Learning","abstract":"With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice. Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements. Such systems not only enhance user experience and satisfaction but also furnish opportunities for businesses and platforms to augment user engagement, sales, and advertising efficacy.This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems. It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains. Furthermore, it delves into the challenges confronting personalized recommendation systems in e-commerce, including data privacy, algorithmic bias, scalability, and the cold start problem. Strategies to address these challenges are elucidated.Subsequently, the paper outlines a personalized recommendation system leveraging the BERT model and nearest neighbor algorithm, specifically tailored to address the exigencies of the eBay e-commerce platform. The efficacy of this recommendation system is substantiated through manual evaluation, and a practical application operational guide and structured output recommendation results are furnished to ensure the system's operability and scalability.","sentences":["With the rapid evolution of the Internet and the exponential proliferation of information, users encounter information overload and the conundrum of choice.","Personalized recommendation systems play a pivotal role in alleviating this burden by aiding users in filtering and selecting information tailored to their preferences and requirements.","Such systems not only enhance user experience and satisfaction but also furnish opportunities for businesses and platforms to augment user engagement, sales, and advertising efficacy.","This paper undertakes a comparative analysis between the operational mechanisms of traditional e-commerce commodity classification systems and personalized recommendation systems.","It delineates the significance and application of personalized recommendation systems across e-commerce, content information, and media domains.","Furthermore, it delves into the challenges confronting personalized recommendation systems in e-commerce, including data privacy, algorithmic bias, scalability, and the cold start problem.","Strategies to address these challenges are elucidated.","Subsequently, the paper outlines a personalized recommendation system leveraging the BERT model and nearest neighbor algorithm, specifically tailored to address the exigencies of the eBay e-commerce platform.","The efficacy of this recommendation system is substantiated through manual evaluation, and a practical application operational guide and structured output recommendation results are furnished to ensure the system's operability and scalability."],"url":"http://arxiv.org/abs/2403.19345v1","category":"cs.IR"}
{"created":"2024-03-28 11:57:08","title":"Dataverse: Open-Source ETL (Extract, Transform, Load) Pipeline for Large Language Models","abstract":"To address the challenges associated with data processing at scale, we propose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline for large language models (LLMs) with a user-friendly design at its core. Easy addition of custom processors with block-based interface in Dataverse allows users to readily and efficiently use Dataverse to build their own ETL pipeline. We hope that Dataverse will serve as a vital tool for LLM development and open source the entire library to welcome community contribution. Additionally, we provide a concise, two-minute video demonstration of our system, illustrating its capabilities and implementation.","sentences":["To address the challenges associated with data processing at scale, we propose Dataverse, a unified open-source Extract-Transform-Load (ETL) pipeline for large language models (LLMs) with a user-friendly design at its core.","Easy addition of custom processors with block-based interface in Dataverse allows users to readily and efficiently use Dataverse to build their own ETL pipeline.","We hope that Dataverse will serve as a vital tool for LLM development and open source the entire library to welcome community contribution.","Additionally, we provide a concise, two-minute video demonstration of our system, illustrating its capabilities and implementation."],"url":"http://arxiv.org/abs/2403.19340v1","category":"cs.CL"}
{"created":"2024-03-28 11:55:18","title":"Introducing scalar leptoquarks into a 3-3-1 model to solve the $(g-2)_\u03bc$ puzzle","abstract":"In this work we introduce scalar leptoquarks into the 3-3-1 model with right-handed neutrinos with the aim of solving the $(g-2)_{\\mu}$ puzzle. We show that besides the model supports leptoquarks in the octet, sextet, triplet and singlet representations, we identified that only one specif leptoquark in the singlet representation leads to flip of chirality as required to generate positive and robust contribution to the $(g-2)_\\mu$. Then we calculate its contributions to $(g-2)_\\mu$ and to the decay process $\\mu \\rightarrow e \\gamma$ and discuss the results.","sentences":["In this work we introduce scalar leptoquarks into the 3-3-1 model with right-handed neutrinos with the aim of solving the $(g-2)_{\\mu}$ puzzle.","We show that besides the model supports leptoquarks in the octet, sextet, triplet and singlet representations, we identified that only one specif leptoquark in the singlet representation leads to flip of chirality as required to generate positive and robust contribution to the $(g-2)_\\mu$. Then we calculate its contributions to $(g-2)_\\mu$ and to the decay process $\\mu \\rightarrow e \\gamma$ and discuss the results."],"url":"http://arxiv.org/abs/2403.19338v1","category":"hep-ph"}
{"created":"2024-03-28 11:55:12","title":"The Hierarchical Cosmic Web and Assembly Bias","abstract":"Accurate modeling of galaxy distributions is paramount for cosmological analysis using galaxy redshift surveys. However, this endeavor is often hindered by the computational complexity of resolving the dark matter halos that host these galaxies. To address this challenge, we propose the development of effective assembly bias models down to small scales, i.e., going beyond the local density dependence capturing non-local cosmic evolution. We introduce a hierarchical cosmic web classification that indirectly captures up to third-order long- and short-range non-local bias terms. This classification system also enables us to maintain positive definite parametric bias expansions. Specifically, we subdivide the traditional cosmic web classification, which is based on the eigenvalues of the tidal field tensor derived, with an additional classification based on the Hessian matrix of the negative density contrast. We obtain the large-scale dark matter field on a mesh with $\\sim3.9\\,h^{-1}$ Mpc cell side resolution through Augmented Lagrangian Perturbation Theory. To assess the effectiveness of our model, we conduct tests using a reference halo catalogue extracted from the UNIT project simulation, which was run within a cubical volume of 1 $h^{-1}$ Gpc side. The resulting mock halo catalogs, generated trough our approach, exhibit a high level of accuracy in terms of the one-, two- and three-point statistics. They reproduce the reference power spectrum within better than 2 percent accuracy up to wavenumbers $k\\sim0.8\\,h$ Mpc$^{-1}$ and provide accurate bispectra within the scales that are crucial for cosmological analysis. This effective bias approach provides a forward model appropriate for field-level cosmological inference and holds significant potential for facilitating cosmological analysis of galaxy redshift surveys, particularly in the context of projects such as DESI, EUCLID, and LSST.","sentences":["Accurate modeling of galaxy distributions is paramount for cosmological analysis using galaxy redshift surveys.","However, this endeavor is often hindered by the computational complexity of resolving the dark matter halos that host these galaxies.","To address this challenge, we propose the development of effective assembly bias models down to small scales, i.e., going beyond the local density dependence capturing non-local cosmic evolution.","We introduce a hierarchical cosmic web classification that indirectly captures up to third-order long- and short-range non-local bias terms.","This classification system also enables us to maintain positive definite parametric bias expansions.","Specifically, we subdivide the traditional cosmic web classification, which is based on the eigenvalues of the tidal field tensor derived, with an additional classification based on the Hessian matrix of the negative density contrast.","We obtain the large-scale dark matter field on a mesh with $\\sim3.9\\,h^{-1}$ Mpc cell side resolution through Augmented Lagrangian Perturbation Theory.","To assess the effectiveness of our model, we conduct tests using a reference halo catalogue extracted from the UNIT project simulation, which was run within a cubical volume of 1 $h^{-1}$ Gpc side.","The resulting mock halo catalogs, generated trough our approach, exhibit a high level of accuracy in terms of the one-, two- and three-point statistics.","They reproduce the reference power spectrum within better than 2 percent accuracy up to wavenumbers $k\\sim0.8\\,h$ Mpc$^{-1}$ and provide accurate bispectra within the scales that are crucial for cosmological analysis.","This effective bias approach provides a forward model appropriate for field-level cosmological inference and holds significant potential for facilitating cosmological analysis of galaxy redshift surveys, particularly in the context of projects such as DESI, EUCLID, and LSST."],"url":"http://arxiv.org/abs/2403.19337v1","category":"astro-ph.CO"}
{"created":"2024-03-28 11:52:42","title":"IVLMap: Instance-Aware Visual Language Grounding for Consumer Robot Navigation","abstract":"Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings. Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation. However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object. To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view. Such indexing is instance-level and attribute-level. In particular, when integrated with a large language model, IVLMap demonstrates the capability to i) transform natural language into navigation targets with instance and attribute information, enabling precise localization, and ii) accomplish zero-shot end-to-end navigation tasks based on natural language commands. Extensive navigation experiments are conducted. Simulation results illustrate that our method can achieve an average improvement of 14.4\\% in navigation accuracy. Code and demo are released at https://ivlmap.github.io/.","sentences":["Vision-and-Language Navigation (VLN) is a challenging task that requires a robot to navigate in photo-realistic environments with human natural language promptings.","Recent studies aim to handle this task by constructing the semantic spatial map representation of the environment, and then leveraging the strong ability of reasoning in large language models for generalizing code for guiding the robot navigation.","However, these methods face limitations in instance-level and attribute-level navigation tasks as they cannot distinguish different instances of the same object.","To address this challenge, we propose a new method, namely, Instance-aware Visual Language Map (IVLMap), to empower the robot with instance-level and attribute-level semantic mapping, where it is autonomously constructed by fusing the RGBD video data collected from the robot agent with special-designed natural language map indexing in the bird's-in-eye view.","Such indexing is instance-level and attribute-level.","In particular, when integrated with a large language model, IVLMap demonstrates the capability to i) transform natural language into navigation targets with instance and attribute information, enabling precise localization, and ii) accomplish zero-shot end-to-end navigation tasks based on natural language commands.","Extensive navigation experiments are conducted.","Simulation results illustrate that our method can achieve an average improvement of 14.4\\% in navigation accuracy.","Code and demo are released at https://ivlmap.github.io/."],"url":"http://arxiv.org/abs/2403.19336v1","category":"cs.CV"}
{"created":"2024-03-28 11:50:23","title":"Test-Time Domain Generalization for Face Anti-Spoofing","abstract":"Face Anti-Spoofing (FAS) is pivotal in safeguarding facial recognition systems against presentation attacks. While domain generalization (DG) methods have been developed to enhance FAS performance, they predominantly focus on learning domain-invariant features during training, which may not guarantee generalizability to unseen data that differs largely from the source distributions. Our insight is that testing data can serve as a valuable resource to enhance the generalizability beyond mere evaluation for DG FAS. In this paper, we introduce a novel Test-Time Domain Generalization (TTDG) framework for FAS, which leverages the testing data to boost the model's generalizability. Our method, consisting of Test-Time Style Projection (TTSP) and Diverse Style Shifts Simulation (DSSS), effectively projects the unseen data to the seen domain space. In particular, we first introduce the innovative TTSP to project the styles of the arbitrarily unseen samples of the testing distribution to the known source space of the training distributions. We then design the efficient DSSS to synthesize diverse style shifts via learnable style bases with two specifically designed losses in a hyperspherical feature space. Our method eliminates the need for model updates at the test time and can be seamlessly integrated into not only the CNN but also ViT backbones. Comprehensive experiments on widely used cross-domain FAS benchmarks demonstrate our method's state-of-the-art performance and effectiveness.","sentences":["Face Anti-Spoofing (FAS) is pivotal in safeguarding facial recognition systems against presentation attacks.","While domain generalization (DG) methods have been developed to enhance FAS performance, they predominantly focus on learning domain-invariant features during training, which may not guarantee generalizability to unseen data that differs largely from the source distributions.","Our insight is that testing data can serve as a valuable resource to enhance the generalizability beyond mere evaluation for DG FAS.","In this paper, we introduce a novel Test-Time Domain Generalization (TTDG) framework for FAS, which leverages the testing data to boost the model's generalizability.","Our method, consisting of Test-Time Style Projection (TTSP) and Diverse Style Shifts Simulation (DSSS), effectively projects the unseen data to the seen domain space.","In particular, we first introduce the innovative TTSP to project the styles of the arbitrarily unseen samples of the testing distribution to the known source space of the training distributions.","We then design the efficient DSSS to synthesize diverse style shifts via learnable style bases with two specifically designed losses in a hyperspherical feature space.","Our method eliminates the need for model updates at the test time and can be seamlessly integrated into not only the CNN but also ViT backbones.","Comprehensive experiments on widely used cross-domain FAS benchmarks demonstrate our method's state-of-the-art performance and effectiveness."],"url":"http://arxiv.org/abs/2403.19334v1","category":"cs.CV"}
{"created":"2024-03-28 11:38:13","title":"Complex generalized Gauss-Radau quadrature rules for Hankel transforms of integer order","abstract":"Complex Gaussian quadrature rules for oscillatory integral transforms have the advantage that they can achieve optimal asymptotic order. However, their existence for Hankel transform can only be guaranteed when the order of the transform belongs to $[0,1/2]$. In this paper we consider the construction of generalized Gauss-Radau quadrature rules for Hankel transform. We show that, if adding certain value and derivative information at the left endpoint, then complex generalized Gauss-Radau quadrature rules for Hankel transform of integer order can be constructed with theoretical guarantees. Orthogonal polynomials that are closely related to such quadrature rules are investigated and their existence for even degrees is proved. Numerical experiments are presented to confirm our findings.","sentences":["Complex Gaussian quadrature rules for oscillatory integral transforms have the advantage that they can achieve optimal asymptotic order.","However, their existence for Hankel transform can only be guaranteed when the order of the transform belongs to $[0,1/2]$. In this paper we consider the construction of generalized Gauss-Radau quadrature rules for Hankel transform.","We show that, if adding certain value and derivative information at the left endpoint, then complex generalized Gauss-Radau quadrature rules for Hankel transform of integer order can be constructed with theoretical guarantees.","Orthogonal polynomials that are closely related to such quadrature rules are investigated and their existence for even degrees is proved.","Numerical experiments are presented to confirm our findings."],"url":"http://arxiv.org/abs/2403.19328v1","category":"math.NA"}
{"created":"2024-03-28 11:30:36","title":"Rapid nonlinear convex guidance via overparameterized monomial coordinates and fundamental solution expansions","abstract":"This paper introduces a framework by which the nonlinear trajectory optimization problem is posed as a path-planning problem in a space liberated of dynamics. In this space, general state constraints for continuous and impulsive control problems are encoded as linear constraints on the native overparameterized variables. This framework is enabled by nonlinear expansion in the vicinity of a reference in terms of fundamental solutions and a minimal nonlinear basis of mixed monomials in problem initial conditions. The former can be computed using state transition tensors, differential algebra, or analytic approaches, and the latter is computed analytically. Nonlinear guidance schemes are proposed taking advantage of this framework, including a successive convex programming scheme for delta-V minimizing trajectory optimization. This work enables a stable and highly rapid nonlinear guidance implementation without the need for collocation or real-time integration.","sentences":["This paper introduces a framework by which the nonlinear trajectory optimization problem is posed as a path-planning problem in a space liberated of dynamics.","In this space, general state constraints for continuous and impulsive control problems are encoded as linear constraints on the native overparameterized variables.","This framework is enabled by nonlinear expansion in the vicinity of a reference in terms of fundamental solutions and a minimal nonlinear basis of mixed monomials in problem initial conditions.","The former can be computed using state transition tensors, differential algebra, or analytic approaches, and the latter is computed analytically.","Nonlinear guidance schemes are proposed taking advantage of this framework, including a successive convex programming scheme for delta-V minimizing trajectory optimization.","This work enables a stable and highly rapid nonlinear guidance implementation without the need for collocation or real-time integration."],"url":"http://arxiv.org/abs/2403.19324v1","category":"math.OC"}
{"created":"2024-03-28 11:22:53","title":"Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field Representation and Generation","abstract":"We present Mesh2NeRF, an approach to derive ground-truth radiance fields from textured meshes for 3D generation tasks. Many 3D generative approaches represent 3D scenes as radiance fields for training. Their ground-truth radiance fields are usually fitted from multi-view renderings from a large-scale synthetic 3D dataset, which often results in artifacts due to occlusions or under-fitting issues. In Mesh2NeRF, we propose an analytic solution to directly obtain ground-truth radiance fields from 3D meshes, characterizing the density field with an occupancy function featuring a defined surface thickness, and determining view-dependent color through a reflection function considering both the mesh and environment lighting. Mesh2NeRF extracts accurate radiance fields which provides direct supervision for training generative NeRFs and single scene representation. We validate the effectiveness of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in PSNR for view synthesis in single scene representation on the ABO dataset, a 0.69 PSNR enhancement in the single-view conditional generation of ShapeNet Cars, and notably improved mesh extraction from NeRF in the unconditional generation of Objaverse Mugs.","sentences":["We present Mesh2NeRF, an approach to derive ground-truth radiance fields from textured meshes for 3D generation tasks.","Many 3D generative approaches represent 3D scenes as radiance fields for training.","Their ground-truth radiance fields are usually fitted from multi-view renderings from a large-scale synthetic 3D dataset, which often results in artifacts due to occlusions or under-fitting issues.","In Mesh2NeRF, we propose an analytic solution to directly obtain ground-truth radiance fields from 3D meshes, characterizing the density field with an occupancy function featuring a defined surface thickness, and determining view-dependent color through a reflection function considering both the mesh and environment lighting.","Mesh2NeRF extracts accurate radiance fields which provides direct supervision for training generative NeRFs and single scene representation.","We validate the effectiveness of Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in PSNR for view synthesis in single scene representation on the ABO dataset, a 0.69 PSNR enhancement in the single-view conditional generation of ShapeNet Cars, and notably improved mesh extraction from NeRF in the unconditional generation of Objaverse Mugs."],"url":"http://arxiv.org/abs/2403.19319v1","category":"cs.CV"}
{"created":"2024-03-28 11:21:12","title":"TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios","abstract":"We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios. We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data. To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios. Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs. We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction.","sentences":["We introduce TableLLM, a robust large language model (LLM) with 13 billion parameters, purpose-built for proficiently handling tabular data manipulation tasks, whether they are embedded within documents or spreadsheets, catering to real-world office scenarios.","We propose a distant supervision method for training, which comprises a reasoning process extension strategy, aiding in training LLMs to understand reasoning patterns more effectively as well as a cross-way validation strategy, ensuring the quality of the automatically generated data.","To evaluate the performance of TableLLM, we have crafted a benchmark tailored to address both document and spreadsheet formats as well as constructed a well-organized evaluation pipeline capable of handling both scenarios.","Thorough evaluations underscore the advantages of TableLLM when compared to various existing general-purpose and tabular data-focused LLMs.","We have publicly released the model checkpoint, source code, benchmarks, and a web application for user interaction."],"url":"http://arxiv.org/abs/2403.19318v1","category":"cs.CL"}
{"created":"2024-03-28 11:18:31","title":"Beyond Borders: Investigating Cross-Jurisdiction Transfer in Legal Case Summarization","abstract":"Legal professionals face the challenge of managing an overwhelming volume of lengthy judgments, making automated legal case summarization crucial. However, prior approaches mainly focused on training and evaluating these models within the same jurisdiction. In this study, we explore the cross-jurisdictional generalizability of legal case summarization models.Specifically, we explore how to effectively summarize legal cases of a target jurisdiction where reference summaries are not available. In particular, we investigate whether supplementing models with unlabeled target jurisdiction corpus and extractive silver summaries obtained from unsupervised algorithms on target data enhances transfer performance. Our comprehensive study on three datasets from different jurisdictions highlights the role of pre-training in improving transfer performance. We shed light on the pivotal influence of jurisdictional similarity in selecting optimal source datasets for effective transfer. Furthermore, our findings underscore that incorporating unlabeled target data yields improvements in general pre-trained models, with additional gains when silver summaries are introduced. This augmentation is especially valuable when dealing with extractive datasets and scenarios featuring limited alignment between source and target jurisdictions. Our study provides key insights for developing adaptable legal case summarization systems, transcending jurisdictional boundaries.","sentences":["Legal professionals face the challenge of managing an overwhelming volume of lengthy judgments, making automated legal case summarization crucial.","However, prior approaches mainly focused on training and evaluating these models within the same jurisdiction.","In this study, we explore the cross-jurisdictional generalizability of legal case summarization models.","Specifically, we explore how to effectively summarize legal cases of a target jurisdiction where reference summaries are not available.","In particular, we investigate whether supplementing models with unlabeled target jurisdiction corpus and extractive silver summaries obtained from unsupervised algorithms on target data enhances transfer performance.","Our comprehensive study on three datasets from different jurisdictions highlights the role of pre-training in improving transfer performance.","We shed light on the pivotal influence of jurisdictional similarity in selecting optimal source datasets for effective transfer.","Furthermore, our findings underscore that incorporating unlabeled target data yields improvements in general pre-trained models, with additional gains when silver summaries are introduced.","This augmentation is especially valuable when dealing with extractive datasets and scenarios featuring limited alignment between source and target jurisdictions.","Our study provides key insights for developing adaptable legal case summarization systems, transcending jurisdictional boundaries."],"url":"http://arxiv.org/abs/2403.19317v1","category":"cs.CL"}
{"created":"2024-03-28 11:17:00","title":"Hypergraph-based Multi-View Action Recognition using Event Cameras","abstract":"Action recognition from video data forms a cornerstone with wide-ranging applications. Single-view action recognition faces limitations due to its reliance on a single viewpoint. In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy. Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition. However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment. To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework. HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network. By treating segments as vertices and constructing hyperedges using rule-based and KNN-based strategies, a multi-view hypergraph neural network that captures relationships across viewpoint and temporal features is established. The vertex attention hypergraph propagation is also introduced for enhanced feature fusion. To prompt research in this area, we present the largest multi-view event-based action dataset $\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6 viewpoints, which surpasses existing datasets by over tenfold. Experimental results show that HyperMV significantly outperforms baselines in both cross-subject and cross-view scenarios, and also exceeds the state-of-the-arts in frame-based multi-view action recognition.","sentences":["Action recognition from video data forms a cornerstone with wide-ranging applications.","Single-view action recognition faces limitations due to its reliance on a single viewpoint.","In contrast, multi-view approaches capture complementary information from various viewpoints for improved accuracy.","Recently, event cameras have emerged as innovative bio-inspired sensors, leading to advancements in event-based action recognition.","However, existing works predominantly focus on single-view scenarios, leaving a gap in multi-view event data exploitation, particularly in challenges like information deficit and semantic misalignment.","To bridge this gap, we introduce HyperMV, a multi-view event-based action recognition framework.","HyperMV converts discrete event data into frame-like representations and extracts view-related features using a shared convolutional network.","By treating segments as vertices and constructing hyperedges using rule-based and KNN-based strategies, a multi-view hypergraph neural network that captures relationships across viewpoint and temporal features is established.","The vertex attention hypergraph propagation is also introduced for enhanced feature fusion.","To prompt research in this area, we present the largest multi-view event-based action dataset $\\text{THU}^{\\text{MV-EACT}}\\text{-50}$, comprising 50 actions from 6 viewpoints, which surpasses existing datasets by over tenfold.","Experimental results show that HyperMV significantly outperforms baselines in both cross-subject and cross-view scenarios, and also exceeds the state-of-the-arts in frame-based multi-view action recognition."],"url":"http://arxiv.org/abs/2403.19316v1","category":"cs.CV"}
{"created":"2024-03-28 11:15:39","title":"Mass fluctuations in non-rotating BTZ black holes","abstract":"We investigate the impact of oscillations of a black-hole mass around its average value on the three-dimensional black hole geometry. Drawing on a classical framework that conceptualizes fluctuations near an event horizon as mass variations, we introduce a model where the metric of a black hole, formed from the collapse of a massive null shell, exhibits oscillatory behavior in spherical modes. This dynamic is encapsulated by a non-rotating BTZ-Vaidya solution, characterized by the black hole mass fluctuating at a resonant frequency $\\omega$ and a small amplitude parameter $\\mu_0$. Using a perturbative approach, solutions to the null geodesic equation are determined up to the second order in $\\mu_0$. The temporal fluctuations of the event horizon's location induce alterations in the thermodynamic variables' values. Upon calculating the time-averaged values, it is observed that the mean Hawking temperature experiences a slight decrease due to these fluctuations, while the mean entropy exhibits an increase, deviating from trends observed in four- and higher-dimensional spacetimes. Further, the study delves into the influence of these fluctuations on the trajectories of null rays near the horizon, ultimately reaching the anti-de Sitter boundary at late times. The analytical computation of the general solution for the perturbed rays up to the second order underscores the novel approach of this study in examining the effects of mass oscillations on black hole thermodynamics and geometry, contributing a unique perspective to the field.","sentences":["We investigate the impact of oscillations of a black-hole mass around its average value on the three-dimensional black hole geometry.","Drawing on a classical framework that conceptualizes fluctuations near an event horizon as mass variations, we introduce a model where the metric of a black hole, formed from the collapse of a massive null shell, exhibits oscillatory behavior in spherical modes.","This dynamic is encapsulated by a non-rotating BTZ-Vaidya solution, characterized by the black hole mass fluctuating at a resonant frequency $\\omega$ and a small amplitude parameter $\\mu_0$. Using a perturbative approach, solutions to the null geodesic equation are determined up to the second order in $\\mu_0$. The temporal fluctuations of the event horizon's location induce alterations in the thermodynamic variables' values.","Upon calculating the time-averaged values, it is observed that the mean Hawking temperature experiences a slight decrease due to these fluctuations, while the mean entropy exhibits an increase, deviating from trends observed in four- and higher-dimensional spacetimes.","Further, the study delves into the influence of these fluctuations on the trajectories of null rays near the horizon, ultimately reaching the anti-de Sitter boundary at late times.","The analytical computation of the general solution for the perturbed rays up to the second order underscores the novel approach of this study in examining the effects of mass oscillations on black hole thermodynamics and geometry, contributing a unique perspective to the field."],"url":"http://arxiv.org/abs/2403.19315v1","category":"gr-qc"}
{"created":"2024-03-28 11:02:34","title":"A Photonic Floquet Scattering Matrix for Wavefront-Shaping in Time-Periodic Media","abstract":"The physics of waves in time-varying media provides numerous opportunities for wave control that are unattainable with static media. In particular, Floquet systems with a periodic time modulation are currently of considerable interest. Here, we demonstrate how the scattering properties of a finite Floquet medium can be correctly described by a static Floquet scattering matrix, which satisfies a pseudo-unitary relation. This algebraic property is a consequence of the conservation of wave action for which we formulate here a continuity equation. Using this Floquet scattering matrix, we further demonstrate how it can be used to generalize also the seminal Wigner-Smith operator from static to Floquet systems. The eigenstates of the corresponding Floquet Wigner-Smith matrix are shown to be light pulses that are optimally shaped both in their spatial and temporal degrees of freedom for the optical micromanipulation of time-varying media.","sentences":["The physics of waves in time-varying media provides numerous opportunities for wave control that are unattainable with static media.","In particular, Floquet systems with a periodic time modulation are currently of considerable interest.","Here, we demonstrate how the scattering properties of a finite Floquet medium can be correctly described by a static Floquet scattering matrix, which satisfies a pseudo-unitary relation.","This algebraic property is a consequence of the conservation of wave action for which we formulate here a continuity equation.","Using this Floquet scattering matrix, we further demonstrate how it can be used to generalize also the seminal Wigner-Smith operator from static to Floquet systems.","The eigenstates of the corresponding Floquet Wigner-Smith matrix are shown to be light pulses that are optimally shaped both in their spatial and temporal degrees of freedom for the optical micromanipulation of time-varying media."],"url":"http://arxiv.org/abs/2403.19311v1","category":"physics.optics"}
{"created":"2024-03-28 10:47:30","title":"Maximum Nim and Josephus Problem","abstract":"In this study, we study the relation between Grundy numbers of a Maximum Nim and Josephus problem. Let f(x) = floor(x/k), where floor( ) is the floor function and k is a positive integer. We prove that there is a simple relation with a Maximum Nim with the rule function f and the Josephus problem in which every k-th numbers are to be removed.","sentences":["In this study, we study the relation between Grundy numbers of a Maximum Nim and Josephus problem.","Let f(x)","= floor(x/k), where floor( ) is the floor function and k is a positive integer.","We prove that there is a simple relation with a Maximum Nim with the rule function f and the Josephus problem in which every k-th numbers are to be removed."],"url":"http://arxiv.org/abs/2403.19308v1","category":"math.CO"}
{"created":"2024-03-28 10:42:49","title":"Sparse Generation: Making Pseudo Labels Sparse for weakly supervision with points","abstract":"In recent years, research on point weakly supervised object detection (PWSOD) methods in the field of computer vision has attracted people's attention. However, existing pseudo labels generation methods perform poorly in a small amount of supervised annotation data and dense object detection tasks. We consider the generation of weakly supervised pseudo labels as the result of model's sparse output, and propose a method called Sparse Generation to make pseudo labels sparse. It constructs dense tensors through the relationship between data and detector model, optimizes three of its parameters, and obtains a sparse tensor via coordinated calculation, thereby indirectly obtaining higher quality pseudo labels, and solving the model's density problem in the situation of only a small amount of supervised annotation data can be used. On two broadly used open-source datasets (RSOD, SIMD) and a self-built dataset (Bullet-Hole), the experimental results showed that the proposed method has a significant advantage in terms of overall performance metrics, comparing to that state-of-the-art method.","sentences":["In recent years, research on point weakly supervised object detection (PWSOD) methods in the field of computer vision has attracted people's attention.","However, existing pseudo labels generation methods perform poorly in a small amount of supervised annotation data and dense object detection tasks.","We consider the generation of weakly supervised pseudo labels as the result of model's sparse output, and propose a method called Sparse Generation to make pseudo labels sparse.","It constructs dense tensors through the relationship between data and detector model, optimizes three of its parameters, and obtains a sparse tensor via coordinated calculation, thereby indirectly obtaining higher quality pseudo labels, and solving the model's density problem in the situation of only a small amount of supervised annotation data can be used.","On two broadly used open-source datasets (RSOD, SIMD) and a self-built dataset (Bullet-Hole), the experimental results showed that the proposed method has a significant advantage in terms of overall performance metrics, comparing to that state-of-the-art method."],"url":"http://arxiv.org/abs/2403.19306v1","category":"cs.CV"}
{"created":"2024-03-28 10:41:47","title":"MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation","abstract":"Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A \"Multi-Agent Text Evaluation framework\" where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios.","sentences":["Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues.","Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge.","Addressing this, recent work has explored the possibility of using LLMs as evaluators.","While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability.","To address these issues, we propose the MATEval: A \"Multi-Agent Text Evaluation framework\" where all agents are played by LLMs like GPT-4.","The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text.","Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring.","Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text.","Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios."],"url":"http://arxiv.org/abs/2403.19305v1","category":"cs.CL"}
{"created":"2024-03-28 10:40:26","title":"Developing generative AI chatbots conceptual framework for higher education","abstract":"This research explores the quickly changing field of generative artificial intelligence (GAI) chatbots in higher education, an industry that is undergoing major technological changes. AI chatbots, such as ChatGPT, HuggingChat, and Google Bard, are becoming more and more common in a variety of sectors, including education. Their acceptance is still in its early phases, with a variety of prospects and obstacles. However, their potential in higher education is particularly noteworthy, providing lecturers and students with affordable, individualized support. Creating a comprehensive framework to aid the usage of generative AI chatbots in higher education institutions (HEIs) is the aim of this project. The Chukwuere Generative AI Chatbots Acceptance Model (CGAICAM) is the result of this study's synthesis of elements from well-known frameworks, including the TAM, UTAUT2, TPB, and others along with variables like optimism, innovativeness, discomfort, insecurity, and others. Using a research method that encompasses a comprehensive analysis of extant literature from databases such as IEEE, ACM, ScienceDirect, and Google Scholar, the study aims to comprehend the implications of AI Chatbots on higher education and pinpoint critical elements for their efficacious implementation. Peer-reviewed English-language publications published between 2020 and 2023 with a focus on the use of AI chatbots in higher education were the main focus of the search criteria. The results demonstrate how much AI chatbots can do to improve student engagement, streamline the educational process, and support administrative and research duties. But there are also clear difficulties, such as unfavorable student sentiments, doubts about the veracity of material produced by AI, and unease and nervousness with new technologies.","sentences":["This research explores the quickly changing field of generative artificial intelligence (GAI) chatbots in higher education, an industry that is undergoing major technological changes.","AI chatbots, such as ChatGPT, HuggingChat, and Google Bard, are becoming more and more common in a variety of sectors, including education.","Their acceptance is still in its early phases, with a variety of prospects and obstacles.","However, their potential in higher education is particularly noteworthy, providing lecturers and students with affordable, individualized support.","Creating a comprehensive framework to aid the usage of generative AI chatbots in higher education institutions (HEIs) is the aim of this project.","The Chukwuere Generative AI Chatbots Acceptance Model (CGAICAM) is the result of this study's synthesis of elements from well-known frameworks, including the TAM, UTAUT2, TPB, and others along with variables like optimism, innovativeness, discomfort, insecurity, and others.","Using a research method that encompasses a comprehensive analysis of extant literature from databases such as IEEE, ACM, ScienceDirect, and Google Scholar, the study aims to comprehend the implications of AI Chatbots on higher education and pinpoint critical elements for their efficacious implementation.","Peer-reviewed English-language publications published between 2020 and 2023 with a focus on the use of AI chatbots in higher education were the main focus of the search criteria.","The results demonstrate how much AI chatbots can do to improve student engagement, streamline the educational process, and support administrative and research duties.","But there are also clear difficulties, such as unfavorable student sentiments, doubts about the veracity of material produced by AI, and unease and nervousness with new technologies."],"url":"http://arxiv.org/abs/2403.19303v1","category":"cs.CY"}
{"created":"2024-03-28 10:40:22","title":"Generate then Retrieve: Conversational Response Retrieval Using LLMs as Answer and Query Generators","abstract":"CIS is a prominent area in IR that focuses on developing interactive knowledge assistants. These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information. To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval. In this paper, we propose three different methods for generating multiple queries to enhance the retrieval. In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries. We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings. In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments. Our experiments reveal the effectiveness of our proposed models on the TREC iKAT dataset.","sentences":["CIS is a prominent area in IR that focuses on developing interactive knowledge assistants.","These systems must adeptly comprehend the user's information requirements within the conversational context and retrieve the relevant information.","To this aim, the existing approaches model the user's information needs with one query called rewritten query and use this query for passage retrieval.","In this paper, we propose three different methods for generating multiple queries to enhance the retrieval.","In these methods, we leverage the capabilities of large language models (LLMs) in understanding the user's information need and generating an appropriate response, to generate multiple queries.","We implement and evaluate the proposed models utilizing various LLMs including GPT-4 and Llama-2 chat in zero-shot and few-shot settings.","In addition, we propose a new benchmark for TREC iKAT based on gpt 3.5 judgments.","Our experiments reveal the effectiveness of our proposed models on the TREC iKAT dataset."],"url":"http://arxiv.org/abs/2403.19302v1","category":"cs.IR"}
{"created":"2024-03-28 10:40:11","title":"Unveiling the Significance of Intrinsic Sensitivity and Multiple Scattering in Speckle Metrology","abstract":"Speckle patterns are a powerful tool for high-precision metrology, as they allow remarkable performance in relatively simple setups. Nonetheless, researchers in this field follow rather distinct paths due to underappreciated general principles underlying speckle phenomena. Here, we advise on a universal metric of intrinsic speckle sensitivity, and on the advantages and disadvantages of multiple scattering. This will catalyse progress in speckle metrology but will also translate to other domains of disordered optics which are undergoing rapid developments at present.","sentences":["Speckle patterns are a powerful tool for high-precision metrology, as they allow remarkable performance in relatively simple setups.","Nonetheless, researchers in this field follow rather distinct paths due to underappreciated general principles underlying speckle phenomena.","Here, we advise on a universal metric of intrinsic speckle sensitivity, and on the advantages and disadvantages of multiple scattering.","This will catalyse progress in speckle metrology but will also translate to other domains of disordered optics which are undergoing rapid developments at present."],"url":"http://arxiv.org/abs/2403.19301v1","category":"physics.optics"}
{"created":"2024-03-28 10:19:36","title":"Graph Neural Networks for Treatment Effect Prediction","abstract":"Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random underlining the need for models that can generalize with limited labeled samples to reduce experimental risks.","sentences":["Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings.","Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk.","However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create.","In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data.","Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding.","Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget.","The framework is flexible since each step can be used separately with other models or policies.","The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random underlining the need for models that can generalize with limited labeled samples to reduce experimental risks."],"url":"http://arxiv.org/abs/2403.19289v1","category":"cs.LG"}
{"created":"2024-03-28 10:19:18","title":"CoderUJB: An Executable and Unified Java Benchmark for Practical Programming Scenarios","abstract":"In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount. Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development. To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production. CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks. Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance. The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection). Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies. CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering.","sentences":["In the evolving landscape of large language models (LLMs) tailored for software engineering, the need for benchmarks that accurately reflect real-world development scenarios is paramount.","Current benchmarks are either too simplistic or fail to capture the multi-tasking nature of software development.","To address this, we introduce CoderUJB, a new benchmark designed to evaluate LLMs across diverse Java programming tasks that are executable and reflective of actual development scenarios, acknowledging Java's prevalence in real-world software production.","CoderUJB comprises 2,239 programming questions derived from 17 real open-source Java projects and spans five practical programming tasks.","Our empirical study on this benchmark investigates the coding abilities of various open-source and closed-source LLMs, examining the effects of continued pre-training in specific programming languages code and instruction fine-tuning on their performance.","The findings indicate that while LLMs exhibit strong potential, challenges remain, particularly in non-functional code generation (e.g., test generation and defect detection).","Importantly, our results advise caution in the specific programming languages continued pre-training and instruction fine-tuning, as these techniques could hinder model performance on certain tasks, suggesting the need for more nuanced strategies.","CoderUJB thus marks a significant step towards more realistic evaluations of programming capabilities in LLMs, and our study provides valuable insights for the future development of these models in software engineering."],"url":"http://arxiv.org/abs/2403.19287v1","category":"cs.SE"}
{"created":"2024-03-28 10:14:24","title":"Adaptive optimization of isogeometric multi-patch discretizations using artificial neural networks","abstract":"In isogeometric analysis, isogeometric function spaces are employed for accurately representing the solution to a partial differential equation (PDE) on a parameterized domain. They are generated from a tensor-product spline space by composing the basis functions with the inverse of the parameterization. Depending on the geometry of the domain and on the data of the PDE, the solution might not have maximum Sobolev regularity, leading to a reduced convergence rate. In this case it is necessary to reduce the local mesh size close to the singularities. The classical approach is to perform adaptive h-refinement, which either leads to an unnecessarily large number of degrees of freedom or to a spline space that does not possess a tensor-product structure. Based on the concept of r-adaptivity we present a novel approach for finding a suitable isogeometric function space for a given PDE without sacrificing the tensor-product structure of the underlying spline space. In particular, we use the fact that different reparameterizations of the same computational domain lead to different isogeometric function spaces while preserving the geometry. Starting from a multi-patch domain consisting of bilinearly parameterized patches, we aim to find the biquadratic multi-patch parameterization that leads to the isogeometric function space with the smallest best approximation error of the solution. In order to estimate the location of the optimal control points, we employ a trained residual neural network that is applied to the graph surfaces of the approximated solution and its derivatives. In our experimental results, we observe that our new method results in a vast improvement of the approximation error for different PDE problems on multi-patch domains.","sentences":["In isogeometric analysis, isogeometric function spaces are employed for accurately representing the solution to a partial differential equation (PDE) on a parameterized domain.","They are generated from a tensor-product spline space by composing the basis functions with the inverse of the parameterization.","Depending on the geometry of the domain and on the data of the PDE, the solution might not have maximum Sobolev regularity, leading to a reduced convergence rate.","In this case it is necessary to reduce the local mesh size close to the singularities.","The classical approach is to perform adaptive h-refinement, which either leads to an unnecessarily large number of degrees of freedom or to a spline space that does not possess a tensor-product structure.","Based on the concept of r-adaptivity we present a novel approach for finding a suitable isogeometric function space for a given PDE without sacrificing the tensor-product structure of the underlying spline space.","In particular, we use the fact that different reparameterizations of the same computational domain lead to different isogeometric function spaces while preserving the geometry.","Starting from a multi-patch domain consisting of bilinearly parameterized patches, we aim to find the biquadratic multi-patch parameterization that leads to the isogeometric function space with the smallest best approximation error of the solution.","In order to estimate the location of the optimal control points, we employ a trained residual neural network that is applied to the graph surfaces of the approximated solution and its derivatives.","In our experimental results, we observe that our new method results in a vast improvement of the approximation error for different PDE problems on multi-patch domains."],"url":"http://arxiv.org/abs/2403.19286v1","category":"math.NA"}
{"created":"2024-03-28 10:05:04","title":"Cohen-Macaulay representations of invariant subrings","abstract":"We investigate Cohen-Macaulay representations of quotient singularities $R:=l[[x_1,\\cdots,x_d]]^G$ where $l$ is a field and $G$ is a finite group acting on $l[[x_1,\\cdots,x_d]]$ as a ring, not necessarily as an $l$-algebra, with $|G|$ not divided by ${\\rm char}\\, l$. In fact, if we put $k:=l^G$, then we may assume that $G$ is a subgroup of $GL_d(l)\\rtimes{\\rm Gal}(l/k)$. We show that $R$ has a generator giving an NCCR and therefore when $d=2$, it is of finite Cohen-Macaulay type. Additionally, we completely describe its Auslander-Reiten quiver, which is often non-simply laced, as a quotient of a certain quiver. Moreover, we also prove that two-dimensional rings of finite Cohen-Macaulay type of equicharacteristic zero are precisely quotient singularities of this form, which generalizes Esnault and Auslander's result. Combining these results, we prove that the quivers that may arise as the Auslander-Reiten quiver of a two-dimensional Gorenstein ring of finite Cohen-Macaulay type of equicharacteristic zero are either doubles of extended Dynkin diagrams or ones having loops. To accomplish these, we establish two results which are of independent interest. First, we prove the existence of $(d-1)$-almost split sequences for arbitrary Cohen-Macaulay normal rings having NCCR which are not necessarily isolated singularities. Second, we make it possible to determine simple modules over skew group algebras by relating them to those over certain group algebras.","sentences":["We investigate Cohen-Macaulay representations of quotient singularities $R:=l[[x_1,\\cdots,x_d]]^G$ where $l$ is a field and $G$ is a finite group acting on $l[[x_1,\\cdots,x_d]]$ as a ring, not necessarily as an $l$-algebra, with $|G|$ not divided by ${\\rm char}\\, l$. In fact, if we put $k:=l^G$, then we may assume that $G$ is a subgroup of $GL_d(l)\\rtimes{\\rm Gal}(l/k)$. We show that $R$ has a generator giving an NCCR and therefore when $d=2$, it is of finite Cohen-Macaulay type.","Additionally, we completely describe its Auslander-Reiten quiver, which is often non-simply laced, as a quotient of a certain quiver.","Moreover, we also prove that two-dimensional rings of finite Cohen-Macaulay type of equicharacteristic zero are precisely quotient singularities of this form, which generalizes Esnault and Auslander's result.","Combining these results, we prove that the quivers that may arise as the Auslander-Reiten quiver of a two-dimensional Gorenstein ring of finite Cohen-Macaulay type of equicharacteristic zero are either doubles of extended Dynkin diagrams or ones having loops.","To accomplish these, we establish two results which are of independent interest.","First, we prove the existence of $(d-1)$-almost split sequences for arbitrary Cohen-Macaulay normal rings having NCCR which are not necessarily isolated singularities.","Second, we make it possible to determine simple modules over skew group algebras by relating them to those over certain group algebras."],"url":"http://arxiv.org/abs/2403.19282v1","category":"math.AC"}
{"created":"2024-03-28 10:02:10","title":"Fine-Tuning Language Models with Reward Learning on Policy","abstract":"Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences. RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially. Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution. Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize. In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution. Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples. Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs. Extensive experiments on three benchmark datasets show that RLP consistently outperforms the state-of-the-art. Our code is available at \\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}.","sentences":["Reinforcement learning from human feedback (RLHF) has emerged as an effective approach to aligning large language models (LLMs) to human preferences.","RLHF contains three steps, i.e., human preference collecting, reward learning, and policy optimization, which are usually performed serially.","Despite its popularity, however, (fixed) reward models may suffer from inaccurate off-distribution, since policy optimization continuously shifts LLMs' data distribution.","Repeatedly collecting new preference data from the latest LLMs may alleviate this issue, which unfortunately makes the resulting system more complicated and difficult to optimize.","In this paper, we propose reward learning on policy (RLP), an unsupervised framework that refines a reward model using policy samples to keep it on-distribution.","Specifically, an unsupervised multi-view learning method is introduced to learn robust representations of policy samples.","Meanwhile, a synthetic preference generation approach is developed to simulate high-quality preference data with policy outputs.","Extensive experiments on three benchmark datasets show that RLP consistently outperforms the state-of-the-art.","Our code is available at \\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/rlp}."],"url":"http://arxiv.org/abs/2403.19279v1","category":"cs.CL"}
{"created":"2024-03-28 10:01:52","title":"Influence of disorder at Insulator-Metal interface on spin transport","abstract":"Motivated by experimental work showing enhancement of spin transport between Yttrium Iron Garnet and Platinum by a thin antiferromagnetic insulator between them, we consider spin transport through the interface of a non-magnetic metal and compensated antiferromagnetically ordered insulator and focus on the significance of the interface itself. The spin transport is carried by spin-polarized electrons in the metal and by magnons in the insulator. We compute the spin current in the presence of a spin accumulation in the metal, cause by the spin Hall effect, and a thermal gradient using Fermi's Golden Rule in the presence of interfacial disorder. For a perfectly clean interface, the in-plane momentum is conserved by the electron-magnon scattering events that govern the spin transport through the interface. We calculate how disorder-induced broadening of scattering matrix elements with respect to the in-plane momentum influences the spin current. As a general result, we observe that for many experimental setups, specifically for high temperatures, one should expect a rather small effect of interface disorder on the measured spin current, while for small temperatures there is a significant reduction of a spin current with increasing disorder.","sentences":["Motivated by experimental work showing enhancement of spin transport between Yttrium Iron Garnet and Platinum by a thin antiferromagnetic insulator between them, we consider spin transport through the interface of a non-magnetic metal and compensated antiferromagnetically ordered insulator and focus on the significance of the interface itself.","The spin transport is carried by spin-polarized electrons in the metal and by magnons in the insulator.","We compute the spin current in the presence of a spin accumulation in the metal, cause by the spin Hall effect, and a thermal gradient using Fermi's Golden Rule in the presence of interfacial disorder.","For a perfectly clean interface, the in-plane momentum is conserved by the electron-magnon scattering events that govern the spin transport through the interface.","We calculate how disorder-induced broadening of scattering matrix elements with respect to the in-plane momentum influences the spin current.","As a general result, we observe that for many experimental setups, specifically for high temperatures, one should expect a rather small effect of interface disorder on the measured spin current, while for small temperatures there is a significant reduction of a spin current with increasing disorder."],"url":"http://arxiv.org/abs/2403.19277v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 10:01:23","title":"Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent","abstract":"Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks. However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent. To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information. For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge. For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action. To make the agent suitable for social media, we design five basic modules for it: persona, planning, action, memory and reflection. To provide an interaction and verification environment for the agent, we build a social media simulation sandbox. In the experimental verification, automatic and human evaluations demonstrated the effectiveness of the agent we constructed.","sentences":["Constructing personalized and anthropomorphic agents holds significant importance in the simulation of social networks.","However, there are still two key problems in existing works: the agent possesses world knowledge that does not belong to its personas, and it cannot eliminate the interference of diverse persona information on current actions, which reduces the personalization and anthropomorphism of the agent.","To solve the above problems, we construct the social media agent based on personalized knowledge and dynamic persona information.","For personalized knowledge, we add external knowledge sources and match them with the persona information of agents, thereby giving the agent personalized world knowledge.","For dynamic persona information, we use current action information to internally retrieve the persona information of the agent, thereby reducing the interference of diverse persona information on the current action.","To make the agent suitable for social media, we design five basic modules for it: persona, planning, action, memory and reflection.","To provide an interaction and verification environment for the agent, we build a social media simulation sandbox.","In the experimental verification, automatic and human evaluations demonstrated the effectiveness of the agent we constructed."],"url":"http://arxiv.org/abs/2403.19275v1","category":"cs.CL"}
{"created":"2024-03-28 09:58:58","title":"Extracting coherent sets in aperiodically driven flows from generators of Mather semigroups","abstract":"Coherent sets are time-dependent regions in the physical space of nonautonomous flows that exhibit little mixing with their neighborhoods, robustly under small random perturbations of the flow. They thus characterize the global long-term transport behavior of the system. We propose a framework to extract such time-dependent families of coherent sets for nonautonomous systems with an ergodic driving dynamics and (small) Brownian noise in physical space. Our construction involves the assembly and analysis of an operator on functions over the augmented space of the associated skew product that, for each fixed state of the driving, propagates distributions on the corresponding physical-space fibre according to the dynamics. This time-dependent operator has the structure of a semigroup (it is called the Mather semigroup), and we show that a spectral analysis of its generator allows for a trajectory-free computation of coherent families, simultaneously for all states of the driving. Additionally, for quasi-periodically driven torus flows, we propose a tailored Fourier discretization scheme for this generator and demonstrate our method by means of three examples of two-dimensional flows.","sentences":["Coherent sets are time-dependent regions in the physical space of nonautonomous flows that exhibit little mixing with their neighborhoods, robustly under small random perturbations of the flow.","They thus characterize the global long-term transport behavior of the system.","We propose a framework to extract such time-dependent families of coherent sets for nonautonomous systems with an ergodic driving dynamics and (small) Brownian noise in physical space.","Our construction involves the assembly and analysis of an operator on functions over the augmented space of the associated skew product that, for each fixed state of the driving, propagates distributions on the corresponding physical-space fibre according to the dynamics.","This time-dependent operator has the structure of a semigroup (it is called the Mather semigroup), and we show that a spectral analysis of its generator allows for a trajectory-free computation of coherent families, simultaneously for all states of the driving.","Additionally, for quasi-periodically driven torus flows, we propose a tailored Fourier discretization scheme for this generator and demonstrate our method by means of three examples of two-dimensional flows."],"url":"http://arxiv.org/abs/2403.19274v1","category":"math.DS"}
{"created":"2024-03-28 09:57:50","title":"A Machine Learning Approach for Crop Yield and Disease Prediction Integrating Soil Nutrition and Weather Factors","abstract":"The development of an intelligent agricultural decision-supporting system for crop selection and disease forecasting in Bangladesh is the main objective of this work. The economy of the nation depends heavily on agriculture. However, choosing crops with better production rates and efficiently controlling crop disease are obstacles that farmers have to face. These issues are addressed in this research by utilizing machine learning methods and real-world datasets. The recommended approach uses a variety of datasets on the production of crops, soil conditions, agro-meteorological regions, crop disease, and meteorological factors. These datasets offer insightful information on disease trends, soil nutrition demand of crops, and agricultural production history. By incorporating this knowledge, the model first recommends the list of primarily selected crops based on the soil nutrition of a particular user location. Then the predictions of meteorological variables like temperature, rainfall, and humidity are made using SARIMAX models. These weather predictions are then used to forecast the possibilities of diseases for the primary crops list by utilizing the support vector classifier. Finally, the developed model makes use of the decision tree regression model to forecast crop yield and provides a final crop list along with associated possible disease forecast. Utilizing the outcome of the model, farmers may choose the best productive crops as well as prevent crop diseases and reduce output losses by taking preventive actions. Consequently, planning and decision-making processes are supported and farmers can predict possible crop yields. Overall, by offering a detailed decision support system for crop selection and disease prediction, this work can play a vital role in advancing agricultural practices in Bangladesh.","sentences":["The development of an intelligent agricultural decision-supporting system for crop selection and disease forecasting in Bangladesh is the main objective of this work.","The economy of the nation depends heavily on agriculture.","However, choosing crops with better production rates and efficiently controlling crop disease are obstacles that farmers have to face.","These issues are addressed in this research by utilizing machine learning methods and real-world datasets.","The recommended approach uses a variety of datasets on the production of crops, soil conditions, agro-meteorological regions, crop disease, and meteorological factors.","These datasets offer insightful information on disease trends, soil nutrition demand of crops, and agricultural production history.","By incorporating this knowledge, the model first recommends the list of primarily selected crops based on the soil nutrition of a particular user location.","Then the predictions of meteorological variables like temperature, rainfall, and humidity are made using SARIMAX models.","These weather predictions are then used to forecast the possibilities of diseases for the primary crops list by utilizing the support vector classifier.","Finally, the developed model makes use of the decision tree regression model to forecast crop yield and provides a final crop list along with associated possible disease forecast.","Utilizing the outcome of the model, farmers may choose the best productive crops as well as prevent crop diseases and reduce output losses by taking preventive actions.","Consequently, planning and decision-making processes are supported and farmers can predict possible crop yields.","Overall, by offering a detailed decision support system for crop selection and disease prediction, this work can play a vital role in advancing agricultural practices in Bangladesh."],"url":"http://arxiv.org/abs/2403.19273v1","category":"cs.LG"}
{"created":"2024-03-28 09:56:26","title":"DeepSample: DNN sampling-based testing for operational accuracy assessment","abstract":"Deep Neural Networks (DNN) are core components for classification and regression tasks of many software systems. Companies incur in high costs for testing DNN with datasets representative of the inputs expected in operation, as these need to be manually labelled. The challenge is to select a representative set of test inputs as small as possible to reduce the labelling cost, while sufficing to yield unbiased high-confidence estimates of the expected DNN accuracy. At the same time, testers are interested in exposing as many DNN mispredictions as possible to improve the DNN, ending up in the need for techniques pursuing a threefold aim: small dataset size, trustworthy estimates, mispredictions exposure. This study presents DeepSample, a family of DNN testing techniques for cost-effective accuracy assessment based on probabilistic sampling. We investigate whether, to what extent, and under which conditions probabilistic sampling can help to tackle the outlined challenge. We implement five new sampling-based testing techniques, and perform a comprehensive comparison of such techniques and of three further state-of-the-art techniques for both DNN classification and regression tasks. Results serve as guidance for best use of sampling-based testing for faithful and high-confidence estimates of DNN accuracy in operation at low cost.","sentences":["Deep Neural Networks (DNN) are core components for classification and regression tasks of many software systems.","Companies incur in high costs for testing DNN with datasets representative of the inputs expected in operation, as these need to be manually labelled.","The challenge is to select a representative set of test inputs as small as possible to reduce the labelling cost, while sufficing to yield unbiased high-confidence estimates of the expected DNN accuracy.","At the same time, testers are interested in exposing as many DNN mispredictions as possible to improve the DNN, ending up in the need for techniques pursuing a threefold aim: small dataset size, trustworthy estimates, mispredictions exposure.","This study presents DeepSample, a family of DNN testing techniques for cost-effective accuracy assessment based on probabilistic sampling.","We investigate whether, to what extent, and under which conditions probabilistic sampling can help to tackle the outlined challenge.","We implement five new sampling-based testing techniques, and perform a comprehensive comparison of such techniques and of three further state-of-the-art techniques for both DNN classification and regression tasks.","Results serve as guidance for best use of sampling-based testing for faithful and high-confidence estimates of DNN accuracy in operation at low cost."],"url":"http://arxiv.org/abs/2403.19271v1","category":"cs.SE"}
{"created":"2024-03-28 09:56:04","title":"sDPO: Don't Use Your Data All at Once","abstract":"As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important. We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning. This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once. We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework. Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters.","sentences":["As development of large language models (LLM) progresses, aligning them with human preferences has become increasingly important.","We propose stepwise DPO (sDPO), an extension of the recently popularized direct preference optimization (DPO) for alignment tuning.","This approach involves dividing the available preference datasets and utilizing them in a stepwise manner, rather than employing it all at once.","We demonstrate that this method facilitates the use of more precisely aligned reference models within the DPO training framework.","Furthermore, sDPO trains the final model to be more performant, even outperforming other popular LLMs with more parameters."],"url":"http://arxiv.org/abs/2403.19270v1","category":"cs.CL"}
{"created":"2024-03-28 09:53:41","title":"MineLand: Simulating Large-Scale Multi-Agent Interactions with Limited Multimodal Senses and Physical Needs","abstract":"Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions. We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs. Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources. This fosters dynamic and valid multi-agent interactions. We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling. Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior. The source code of MineLand and Alex is openly available at https://github.com/cocacola-lab/MineLand.","sentences":["Conventional multi-agent simulators often assume perfect information and limitless capabilities, hindering the ecological validity of social interactions.","We propose a multi-agent Minecraft simulator, MineLand, that bridges this gap by introducing limited multimodal senses and physical needs.","Our simulator supports up to 48 agents with limited visual, auditory, and environmental awareness, forcing them to actively communicate and collaborate to fulfill physical needs like food and resources.","This fosters dynamic and valid multi-agent interactions.","We further introduce an AI agent framework, Alex, inspired by multitasking theory, enabling agents to handle intricate coordination and scheduling.","Our experiments demonstrate that the simulator, the corresponding benchmark, and the AI agent framework contribute to more ecological and nuanced collective behavior.","The source code of MineLand and Alex is openly available at https://github.com/cocacola-lab/MineLand."],"url":"http://arxiv.org/abs/2403.19267v1","category":"cs.CL"}
{"created":"2024-03-28 09:44:20","title":"Neural Fields for 3D Tracking of Anatomy and Surgical Instruments in Monocular Laparoscopic Video Clips","abstract":"Laparoscopic video tracking primarily focuses on two target types: surgical instruments and anatomy. The former could be used for skill assessment, while the latter is necessary for the projection of virtual overlays. Where instrument and anatomy tracking have often been considered two separate problems, in this paper, we propose a method for joint tracking of all structures simultaneously. Based on a single 2D monocular video clip, we train a neural field to represent a continuous spatiotemporal scene, used to create 3D tracks of all surfaces visible in at least one frame. Due to the small size of instruments, they generally cover a small part of the image only, resulting in decreased tracking accuracy. Therefore, we propose enhanced class weighting to improve the instrument tracks. We evaluate tracking on video clips from laparoscopic cholecystectomies, where we find mean tracking accuracies of 92.4% for anatomical structures and 87.4% for instruments. Additionally, we assess the quality of depth maps obtained from the method's scene reconstructions. We show that these pseudo-depths have comparable quality to a state-of-the-art pre-trained depth estimator. On laparoscopic videos in the SCARED dataset, the method predicts depth with an MAE of 2.9 mm and a relative error of 9.2%. These results show the feasibility of using neural fields for monocular 3D reconstruction of laparoscopic scenes.","sentences":["Laparoscopic video tracking primarily focuses on two target types: surgical instruments and anatomy.","The former could be used for skill assessment, while the latter is necessary for the projection of virtual overlays.","Where instrument and anatomy tracking have often been considered two separate problems, in this paper, we propose a method for joint tracking of all structures simultaneously.","Based on a single 2D monocular video clip, we train a neural field to represent a continuous spatiotemporal scene, used to create 3D tracks of all surfaces visible in at least one frame.","Due to the small size of instruments, they generally cover a small part of the image only, resulting in decreased tracking accuracy.","Therefore, we propose enhanced class weighting to improve the instrument tracks.","We evaluate tracking on video clips from laparoscopic cholecystectomies, where we find mean tracking accuracies of 92.4% for anatomical structures and 87.4% for instruments.","Additionally, we assess the quality of depth maps obtained from the method's scene reconstructions.","We show that these pseudo-depths have comparable quality to a state-of-the-art pre-trained depth estimator.","On laparoscopic videos in the SCARED dataset, the method predicts depth with an MAE of 2.9 mm and a relative error of 9.2%.","These results show the feasibility of using neural fields for monocular 3D reconstruction of laparoscopic scenes."],"url":"http://arxiv.org/abs/2403.19265v1","category":"cs.CV"}
{"created":"2024-03-28 09:36:55","title":"Removing the need for ground truth UWB data collection: self-supervised ranging error correction using deep reinforcement learning","abstract":"Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential. However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags. Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments. This paper proposes a novel self-supervised deep reinforcement learning approach that does not require labeled ground truth data. A reinforcement learning agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges. The agent learns, self-supervised, by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening. Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art supervised methods, overcoming data dependency and lack of generalizability limitations. This makes self-supervised deep reinforcement learning a promising solution for practical and scalable UWB-ranging error correction.","sentences":["Indoor positioning using UWB technology has gained interest due to its centimeter-level accuracy potential.","However, multipath effects and non-line-of-sight conditions cause ranging errors between anchors and tags.","Existing approaches for mitigating these ranging errors rely on collecting large labeled datasets, making them impractical for real-world deployments.","This paper proposes a novel self-supervised deep reinforcement learning approach that does not require labeled ground truth data.","A reinforcement learning agent uses the channel impulse response as a state and predicts corrections to minimize the error between corrected and estimated ranges.","The agent learns, self-supervised, by iteratively improving corrections that are generated by combining the predictability of trajectories with filtering and smoothening.","Experiments on real-world UWB measurements demonstrate comparable performance to state-of-the-art supervised methods, overcoming data dependency and lack of generalizability limitations.","This makes self-supervised deep reinforcement learning a promising solution for practical and scalable UWB-ranging error correction."],"url":"http://arxiv.org/abs/2403.19262v1","category":"eess.SP"}
{"created":"2024-03-28 09:35:40","title":"High-Temperature Modifications of Charged Casimir Wormholes","abstract":"In this letter, we extend the investigation of the consequences of thermal fluctuations on the Casimir effect within the context of a traversable wormhole, recently proposed by Garattini \\& Faizal, arXiv:2403.15174 [gr-qc], subject to charge contributions. Specifically, our focus is on scenarios where the plates exhibit radial variation. In our analysis, we initially concentrate on the high temperature approximation, considering solely the influence of charge on the thermal Casimir wormholes. Additionally, upon incorporating Generalized Uncertainty Principle (GUP) corrections to the Casimir energy, we obtain a new class of wormhole solutions. Notably, we establish that the flare-out condition remains consistently satisfied. Intriguingly, our findings reveal that both the charge and GUP contributions serve to further enlarge the throat's size.","sentences":["In this letter, we extend the investigation of the consequences of thermal fluctuations on the Casimir effect within the context of a traversable wormhole, recently proposed by Garattini \\& Faizal, arXiv:2403.15174","[gr-qc], subject to charge contributions.","Specifically, our focus is on scenarios where the plates exhibit radial variation.","In our analysis, we initially concentrate on the high temperature approximation, considering solely the influence of charge on the thermal Casimir wormholes.","Additionally, upon incorporating Generalized Uncertainty Principle (GUP) corrections to the Casimir energy, we obtain a new class of wormhole solutions.","Notably, we establish that the flare-out condition remains consistently satisfied.","Intriguingly, our findings reveal that both the charge and GUP contributions serve to further enlarge the throat's size."],"url":"http://arxiv.org/abs/2403.19261v1","category":"hep-th"}
{"created":"2024-03-28 09:34:31","title":"NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data","abstract":"To address the global issue of hateful content proliferating in online platforms, hate speech detection (HSD) models are typically developed on datasets collected in the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on curated samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature largely overestimates real-world performance on representative data. We also propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance. Finally, we show that in this context, a human-in-the-loop approach to content moderation where humans review 1% of Nigerian tweets flagged as hateful would enable to moderate 60% of all hateful content. Taken together, these results pave the way towards robust HSD systems and a better protection of social media users from hateful content in low-resource settings.","sentences":["To address the global issue of hateful content proliferating in online platforms, hate speech detection (HSD) models are typically developed on datasets collected in the United States, thereby failing to generalize to English dialects from the Majority World.","Furthermore, HSD models are often evaluated on curated samples, raising concerns about overestimating model performance in real-world settings.","In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets.","We demonstrate that HSD evaluated on biased datasets traditionally used in the literature largely overestimates real-world performance on representative data.","We also propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance.","Finally, we show that in this context, a human-in-the-loop approach to content moderation where humans review 1% of Nigerian tweets flagged as hateful would enable to moderate 60% of all hateful content.","Taken together, these results pave the way towards robust HSD systems and a better protection of social media users from hateful content in low-resource settings."],"url":"http://arxiv.org/abs/2403.19260v1","category":"cs.CL"}
{"created":"2024-03-28 09:30:30","title":"Finite-time Scaling beyond the Kibble-Zurek Prerequisite: Driven Critical Dynamics in Strongly Interacting Dirac Systems","abstract":"In conventional quantum critical point (QCP) characterized by order parameter fluctuations, the celebrated Kibble-Zurek mechanism (KZM) and finite-time scaling (FTS) theory provide universal descriptions of the driven critical dynamics. However, in strongly correlated fermionic systems where gapless fermions are usually present in vicinity of QCP, the driven dynamics has rarely been explored. In this Letter, we investigate the driven critical dynamics in two-dimensional Dirac systems, which harbor semimetal and Mott insulator phases separated by the QCP triggered by the interplay between fluctuations of gapless Dirac fermions and order-parameter bosons. By studying the evolution of physical quantities for different driving rates through large-scale quantum Monte Carlo simulation, we confirm that the driven dynamics is described by the FTS form. Accordingly, our results significantly generalize the KZM theory by relaxing its requirement for a gapped initial state to the system accommodating gapless Dirac fermionic excitation. Through successfully extending the KZM and FTS theory to Dirac QCP, our work not only brings new fundamental perspective into the nonequilibrium critical dynamics, but also provides a novel theoretical approach to fathom quantum critical properties in fermionic systems.","sentences":["In conventional quantum critical point (QCP) characterized by order parameter fluctuations, the celebrated Kibble-Zurek mechanism (KZM) and finite-time scaling (FTS) theory provide universal descriptions of the driven critical dynamics.","However, in strongly correlated fermionic systems where gapless fermions are usually present in vicinity of QCP, the driven dynamics has rarely been explored.","In this Letter, we investigate the driven critical dynamics in two-dimensional Dirac systems, which harbor semimetal and Mott insulator phases separated by the QCP triggered by the interplay between fluctuations of gapless Dirac fermions and order-parameter bosons.","By studying the evolution of physical quantities for different driving rates through large-scale quantum Monte Carlo simulation, we confirm that the driven dynamics is described by the FTS form.","Accordingly, our results significantly generalize the KZM theory by relaxing its requirement for a gapped initial state to the system accommodating gapless Dirac fermionic excitation.","Through successfully extending the KZM and FTS theory to Dirac QCP, our work not only brings new fundamental perspective into the nonequilibrium critical dynamics, but also provides a novel theoretical approach to fathom quantum critical properties in fermionic systems."],"url":"http://arxiv.org/abs/2403.19258v1","category":"cond-mat.str-el"}
{"created":"2024-03-28 09:21:00","title":"Imperceptible Protection against Style Imitation from Diffusion Models","abstract":"Recent progress in diffusion models has profoundly enhanced the fidelity of image generation. However, this has raised concerns about copyright infringements. While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality. Recognizing the importance of maintaining this, we develop a visually improved protection method that preserves its protection capability. To this end, we create a perceptual map to identify areas most sensitive to human eyes. We then adjust the protection intensity guided by an instance-aware refinement. We also integrate a perceptual constraints bank to further improve the imperceptibility. Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy.","sentences":["Recent progress in diffusion models has profoundly enhanced the fidelity of image generation.","However, this has raised concerns about copyright infringements.","While prior methods have introduced adversarial perturbations to prevent style imitation, most are accompanied by the degradation of artworks' visual quality.","Recognizing the importance of maintaining this, we develop a visually improved protection method that preserves its protection capability.","To this end, we create a perceptual map to identify areas most sensitive to human eyes.","We then adjust the protection intensity guided by an instance-aware refinement.","We also integrate a perceptual constraints bank to further improve the imperceptibility.","Results show that our method substantially elevates the quality of the protected image without compromising on protection efficacy."],"url":"http://arxiv.org/abs/2403.19254v1","category":"cs.CV"}
{"created":"2024-03-28 09:08:45","title":"Genos: General In-Network Unsupervised Intrusion Detection by Rule Extraction","abstract":"Anomaly-based network intrusion detection systems (A-NIDS) use unsupervised models to detect unforeseen attacks. However, existing A-NIDS solutions suffer from low throughput, lack of interpretability, and high maintenance costs. Recent in-network intelligence (INI) exploits programmable switches to offer line-rate deployment of NIDS. Nevertheless, current in-network NIDS are either model-specific or only apply to supervised models. In this paper, we propose Genos, a general in-network framework for unsupervised A-NIDS by rule extraction, which consists of a Model Compiler, a Model Interpreter, and a Model Debugger. Specifically, observing benign data are multimodal and usually located in multiple subspaces in the feature space, we utilize a divide-and-conquer approach for model-agnostic rule extraction. In the Model Compiler, we first propose a tree-based clustering algorithm to partition the feature space into subspaces, then design a decision boundary estimation mechanism to approximate the source model in each subspace. The Model Interpreter interprets predictions by important attributes to aid network operators in understanding the predictions. The Model Debugger conducts incremental updating to rectify errors by only fine-tuning rules on affected subspaces, thus reducing maintenance costs. We implement a prototype using physical hardware, and experiments demonstrate its superior performance of 100 Gbps throughput, great interpretability, and trivial updating overhead.","sentences":["Anomaly-based network intrusion detection systems (A-NIDS) use unsupervised models to detect unforeseen attacks.","However, existing A-NIDS solutions suffer from low throughput, lack of interpretability, and high maintenance costs.","Recent in-network intelligence (INI) exploits programmable switches to offer line-rate deployment of NIDS.","Nevertheless, current in-network NIDS are either model-specific or only apply to supervised models.","In this paper, we propose Genos, a general in-network framework for unsupervised A-NIDS by rule extraction, which consists of a Model Compiler, a Model Interpreter, and a Model Debugger.","Specifically, observing benign data are multimodal and usually located in multiple subspaces in the feature space, we utilize a divide-and-conquer approach for model-agnostic rule extraction.","In the Model Compiler, we first propose a tree-based clustering algorithm to partition the feature space into subspaces, then design a decision boundary estimation mechanism to approximate the source model in each subspace.","The Model Interpreter interprets predictions by important attributes to aid network operators in understanding the predictions.","The Model Debugger conducts incremental updating to rectify errors by only fine-tuning rules on affected subspaces, thus reducing maintenance costs.","We implement a prototype using physical hardware, and experiments demonstrate its superior performance of 100 Gbps throughput, great interpretability, and trivial updating overhead."],"url":"http://arxiv.org/abs/2403.19248v1","category":"cs.CR"}
{"created":"2024-03-28 09:07:57","title":"Dephasing Noise Simulation for Coherence-Generating Devices","abstract":"Advancing quantum technologies necessitates an in-depth exploration of how operations generate quantum resources and respond to noise. Crucial are gates generating quantum coherence and the challenge of mitigating gate dephasing noise. Precisely, we study the dephasing noise that reduces the coherence-generating power of quantum gates, its simulation, and critical factors. Our primary contribution lies in a theorem characterizing the full set of dephasing noises in gates, adaptable to the simulation by any predefined operation set. In particular, we apply our result to quantify the memory adaptability required for a dephasing noise to arise. Furthermore, we analytically calculate the quantifier for gates acting on qubit systems, thereby fully characterizing this scenario. Next, we show how our results reveal the structure of non-trivial dephasing noise affecting qubit gates and apply them to experimental data, conclusively demonstrating the existence of a gate's dephasing noise, which is irreducible to dephasing of either input or output states. Finally, we show how our study contributes to addressing an open question in the resource theory of coherence generation.","sentences":["Advancing quantum technologies necessitates an in-depth exploration of how operations generate quantum resources and respond to noise.","Crucial are gates generating quantum coherence and the challenge of mitigating gate dephasing noise.","Precisely, we study the dephasing noise that reduces the coherence-generating power of quantum gates, its simulation, and critical factors.","Our primary contribution lies in a theorem characterizing the full set of dephasing noises in gates, adaptable to the simulation by any predefined operation set.","In particular, we apply our result to quantify the memory adaptability required for a dephasing noise to arise.","Furthermore, we analytically calculate the quantifier for gates acting on qubit systems, thereby fully characterizing this scenario.","Next, we show how our results reveal the structure of non-trivial dephasing noise affecting qubit gates and apply them to experimental data, conclusively demonstrating the existence of a gate's dephasing noise, which is irreducible to dephasing of either input or output states.","Finally, we show how our study contributes to addressing an open question in the resource theory of coherence generation."],"url":"http://arxiv.org/abs/2403.19247v1","category":"quant-ph"}
{"created":"2024-03-28 09:00:05","title":"The use of ChatGPT in higher education: The advantages and disadvantages","abstract":"Higher education scholars are interested in an artificial intelligence (AI) technology called ChatGPT, which was developed by OpenAI. Whether ChatGPT can improve learning is still a topic of debate among experts. This concise overview of the literature examines the application of ChatGPT in higher education to comprehend and produce high-level instruction. By examining the essential literature, this study seeks to provide a thorough assessment of the advantages and disadvantages of utilizing ChatGPT in higher education settings. But it's crucial to consider both the positive and negative elements. For this rapid review, the researcher searched Google Scholar, Scopus, and others between January 2023 and July 2023 for prior research from various publications. These studies were examined. The study found that employing ChatGPT in higher education is beneficial for a number of reasons. It can provide individualized instruction, and prompt feedback, facilitate access to learning, and promote student interaction. These benefits could improve the learning environment and make it more fun for academics and students. The cons of ChatGPT are equally present. These problems include the inability to comprehend emotions, the lack of social interaction chances, technological limitations, and the dangers of depending too much on ChatGPT for higher education. Higher education should combine ChatGPT with other teaching techniques to provide students and lecturers with a comprehensive education. However, it is crucial to consider the positives, negatives, and moral issues before adopting ChatGPT in the classroom.","sentences":["Higher education scholars are interested in an artificial intelligence (AI) technology called ChatGPT, which was developed by OpenAI.","Whether ChatGPT can improve learning is still a topic of debate among experts.","This concise overview of the literature examines the application of ChatGPT in higher education to comprehend and produce high-level instruction.","By examining the essential literature, this study seeks to provide a thorough assessment of the advantages and disadvantages of utilizing ChatGPT in higher education settings.","But it's crucial to consider both the positive and negative elements.","For this rapid review, the researcher searched Google Scholar, Scopus, and others between January 2023 and July 2023 for prior research from various publications.","These studies were examined.","The study found that employing ChatGPT in higher education is beneficial for a number of reasons.","It can provide individualized instruction, and prompt feedback, facilitate access to learning, and promote student interaction.","These benefits could improve the learning environment and make it more fun for academics and students.","The cons of ChatGPT are equally present.","These problems include the inability to comprehend emotions, the lack of social interaction chances, technological limitations, and the dangers of depending too much on ChatGPT for higher education.","Higher education should combine ChatGPT with other teaching techniques to provide students and lecturers with a comprehensive education.","However, it is crucial to consider the positives, negatives, and moral issues before adopting ChatGPT in the classroom."],"url":"http://arxiv.org/abs/2403.19245v1","category":"cs.CY"}
{"created":"2024-03-28 08:59:50","title":"The role of chemo-mechanical modelling in the development of battery technology -- a perspective","abstract":"In the race to reduce global CO2 emissions and achieve net-zero, chemomechanics must play a critical role in the technological development of current and next-generation batteries to improve their energy storage capabilities and their lifetime. Many degradation processes arise through mechanics via the development of diffusion-induced stress and volumetric strains within the various constituent materials in a battery. From particle cracking in lithium-ion batteries to lithium dendrite-based fracture of solid electrolytes in solid-state batteries, it is clear that significant barriers exist in the development of these energy storage systems, where chemomechanics plays a central part. To accelerate technological and scientific advances in this area, multi-scale and highly coupled multiphysics modelling must be carried out that includes mechanics-based phenomena. In this perspective article, we provide an introduction to chemomechanical modelling, the various physical problems that it addresses, and the issues that need to be resolved in order to expand its use within the field of battery technology.","sentences":["In the race to reduce global CO2 emissions and achieve net-zero, chemomechanics must play a critical role in the technological development of current and next-generation batteries to improve their energy storage capabilities and their lifetime.","Many degradation processes arise through mechanics via the development of diffusion-induced stress and volumetric strains within the various constituent materials in a battery.","From particle cracking in lithium-ion batteries to lithium dendrite-based fracture of solid electrolytes in solid-state batteries, it is clear that significant barriers exist in the development of these energy storage systems, where chemomechanics plays a central part.","To accelerate technological and scientific advances in this area, multi-scale and highly coupled multiphysics modelling must be carried out that includes mechanics-based phenomena.","In this perspective article, we provide an introduction to chemomechanical modelling, the various physical problems that it addresses, and the issues that need to be resolved in order to expand its use within the field of battery technology."],"url":"http://arxiv.org/abs/2403.19244v1","category":"physics.chem-ph"}
{"created":"2024-03-28 08:54:14","title":"Capillary-lubrication force between rotating cylinders separated by a fluid interface","abstract":"Two cylinders rotating next to each other generate a large hydrodynamic force if the intermediate space is filled with a viscous fluid. Herein, we explore the case where the cylinders are separated by two layers of viscous immiscible fluids, in the limit of small capillary deformation of the fluid interface. As the interface deformation breaks the system's symmetry, a novel force characteristic of soft lubrication is generated. We calculate this capillary-lubrication force, which is split into velocity-dependant and acceleration-dependant contributions. Furthermore, we analyze the variations induced by modifying the viscosity ratio between the two fluid layers, their thickness ratio, and the Bond number. Unlike standard elastic cases, where a repelling soft-lubrication lift force has been abundantly reported, the current fluid bilayer setting can also exhibit an attractive force due to the non-monotonic deflection of the fluid interface when varying the sublayer thickness. Besides, at high Bond numbers, the system's response becomes analogous to the one of a Winkler-like substrate with a viscous flow inside.","sentences":["Two cylinders rotating next to each other generate a large hydrodynamic force if the intermediate space is filled with a viscous fluid.","Herein, we explore the case where the cylinders are separated by two layers of viscous immiscible fluids, in the limit of small capillary deformation of the fluid interface.","As the interface deformation breaks the system's symmetry, a novel force characteristic of soft lubrication is generated.","We calculate this capillary-lubrication force, which is split into velocity-dependant and acceleration-dependant contributions.","Furthermore, we analyze the variations induced by modifying the viscosity ratio between the two fluid layers, their thickness ratio, and the Bond number.","Unlike standard elastic cases, where a repelling soft-lubrication lift force has been abundantly reported, the current fluid bilayer setting can also exhibit an attractive force due to the non-monotonic deflection of the fluid interface when varying the sublayer thickness.","Besides, at high Bond numbers, the system's response becomes analogous to the one of a Winkler-like substrate with a viscous flow inside."],"url":"http://arxiv.org/abs/2403.19241v1","category":"cond-mat.soft"}
{"created":"2024-03-28 08:49:35","title":"Taming Lookup Tables for Efficient Image Retouching","abstract":"The widespread use of high-definition screens in edge devices, such as end-user cameras, smartphones, and televisions, is spurring a significant demand for image enhancement. Existing enhancement models often optimize for high performance while falling short of reducing hardware inference time and power consumption, especially on edge devices with constrained computing and storage resources. To this end, we propose Image Color Enhancement Lookup Table (ICELUT) that adopts LUTs for extremely efficient edge inference, without any convolutional neural network (CNN). During training, we leverage pointwise (1x1) convolution to extract color information, alongside a split fully connected layer to incorporate global information. Both components are then seamlessly converted into LUTs for hardware-agnostic deployment. ICELUT achieves near-state-of-the-art performance and remarkably low power consumption. We observe that the pointwise network structure exhibits robust scalability, upkeeping the performance even with a heavily downsampled 32x32 input image. These enable ICELUT, the first-ever purely LUT-based image enhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at least one order faster than any CNN solution. Codes are available at https://github.com/Stephen0808/ICELUT.","sentences":["The widespread use of high-definition screens in edge devices, such as end-user cameras, smartphones, and televisions, is spurring a significant demand for image enhancement.","Existing enhancement models often optimize for high performance while falling short of reducing hardware inference time and power consumption, especially on edge devices with constrained computing and storage resources.","To this end, we propose Image Color Enhancement Lookup Table (ICELUT) that adopts LUTs for extremely efficient edge inference, without any convolutional neural network (CNN).","During training, we leverage pointwise (1x1) convolution to extract color information, alongside a split fully connected layer to incorporate global information.","Both components are then seamlessly converted into LUTs for hardware-agnostic deployment.","ICELUT achieves near-state-of-the-art performance and remarkably low power consumption.","We observe that the pointwise network structure exhibits robust scalability, upkeeping the performance even with a heavily downsampled 32x32 input image.","These enable ICELUT, the first-ever purely LUT-based image enhancer, to reach an unprecedented speed of 0.4ms on GPU and 7ms on CPU, at least one order faster than any CNN solution.","Codes are available at https://github.com/Stephen0808/ICELUT."],"url":"http://arxiv.org/abs/2403.19238v1","category":"cs.CV"}
{"created":"2024-03-28 08:48:41","title":"Extreme change-point detection","abstract":"We examine rules for predicting whether a point in $\\mathbb{R}$ generated from a 50-50 mixture of two different probability distributions came from one distribution or the other, given limited (or no) information on the two distributions, and, as clues, one point generated randomly from each of the two distributions. We prove that nearest-neighbor prediction does better than chance when we know the two distributions are Gaussian densities without knowing their parameter values. We conjecture that this result holds for general probability distributions and, furthermore, that the nearest-neighbor rule is optimal in this setting, i.e., no other rule can do better than it if we do not know the distributions or do not know their parameters, or both.","sentences":["We examine rules for predicting whether a point in $\\mathbb{R}$ generated from a 50-50 mixture of two different probability distributions came from one distribution or the other, given limited (or no) information on the two distributions, and, as clues, one point generated randomly from each of the two distributions.","We prove that nearest-neighbor prediction does better than chance when we know the two distributions are Gaussian densities without knowing their parameter values.","We conjecture that this result holds for general probability distributions and, furthermore, that the nearest-neighbor rule is optimal in this setting, i.e., no other rule can do better than it if we do not know the distributions or do not know their parameters, or both."],"url":"http://arxiv.org/abs/2403.19237v1","category":"math.ST"}
{"created":"2024-03-28 08:47:02","title":"DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation","abstract":"While large-scale pre-trained text-to-image models can synthesize diverse and high-quality human-centered images, novel challenges arise with a nuanced task of \"identity fine editing\": precisely modifying specific features of a subject while maintaining its inherent identity and context. Existing personalization methods either require time-consuming optimization or learning additional encoders, adept in \"identity re-contextualization\". However, they often struggle with detailed and sensitive tasks like human face editing. To address these challenges, we introduce DreamSalon, a noise-guided, staged-editing framework, uniquely focusing on detailed image manipulations and identity-context preservation. By discerning editing and boosting stages via the frequency and gradient of predicted noises, DreamSalon first performs detailed manipulations on specific features in the editing stage, guided by high-frequency information, and then employs stochastic denoising in the boosting stage to improve image quality. For more precise editing, DreamSalon semantically mixes source and target textual prompts, guided by differences in their embedding covariances, to direct the model's focus on specific manipulation areas. Our experiments demonstrate DreamSalon's ability to efficiently and faithfully edit fine details on human faces, outperforming existing methods both qualitatively and quantitatively.","sentences":["While large-scale pre-trained text-to-image models can synthesize diverse and high-quality human-centered images, novel challenges arise with a nuanced task of \"identity fine editing\": precisely modifying specific features of a subject while maintaining its inherent identity and context.","Existing personalization methods either require time-consuming optimization or learning additional encoders, adept in \"identity re-contextualization\".","However, they often struggle with detailed and sensitive tasks like human face editing.","To address these challenges, we introduce DreamSalon, a noise-guided, staged-editing framework, uniquely focusing on detailed image manipulations and identity-context preservation.","By discerning editing and boosting stages via the frequency and gradient of predicted noises, DreamSalon first performs detailed manipulations on specific features in the editing stage, guided by high-frequency information, and then employs stochastic denoising in the boosting stage to improve image quality.","For more precise editing, DreamSalon semantically mixes source and target textual prompts, guided by differences in their embedding covariances, to direct the model's focus on specific manipulation areas.","Our experiments demonstrate DreamSalon's ability to efficiently and faithfully edit fine details on human faces, outperforming existing methods both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2403.19235v1","category":"cs.CV"}
{"created":"2024-03-28 08:43:20","title":"Exploring critical behavior of thermodynamic variables of the Kerr-Newman-AdS black hole in the restricted phase space","abstract":"The present work delves into examining the thermodynamic properties of the four-dimensional Kerr-Newmann-AdS black hole, employing the recently proposed framework of restricted phase space thermodynamics (RPST). This approach introduces a novel set of paired thermodynamic variables: the central charge $C$ of the corresponding dual conformal field theory (CFT) and the chemical potential $\\mu$. Through simple analysis, we establish fundamental relationships such as the Euler relation, Gibbs-Duhem relation, and the zeroth order homogeneity of intensive variables. Employing numerical techniques, we explore the thermodynamic processes between these conjugate variables. Our investigation reveals the first-order and second-order phase transitions across various macroscopic processes. Despite the absence of complete analytical expressions, our findings unveil striking similarities in behavior between RN-AdS and Kerr-AdS, underscoring the presence of underlying universality within the RPST formalism.","sentences":["The present work delves into examining the thermodynamic properties of the four-dimensional Kerr-Newmann-AdS black hole, employing the recently proposed framework of restricted phase space thermodynamics (RPST).","This approach introduces a novel set of paired thermodynamic variables: the central charge $C$ of the corresponding dual conformal field theory (CFT) and the chemical potential $\\mu$. Through simple analysis, we establish fundamental relationships such as the Euler relation, Gibbs-Duhem relation, and the zeroth order homogeneity of intensive variables.","Employing numerical techniques, we explore the thermodynamic processes between these conjugate variables.","Our investigation reveals the first-order and second-order phase transitions across various macroscopic processes.","Despite the absence of complete analytical expressions, our findings unveil striking similarities in behavior between RN-AdS and Kerr-AdS, underscoring the presence of underlying universality within the RPST formalism."],"url":"http://arxiv.org/abs/2403.19229v1","category":"gr-qc"}
{"created":"2024-03-28 08:39:44","title":"Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment","abstract":"Weakly-supervised action segmentation is a task of learning to partition a long video into several action segments, where training videos are only accompanied by transcripts (ordered list of actions). Most of existing methods need to infer pseudo segmentation for training by serial alignment between all frames and the transcript, which is time-consuming and hard to be parallelized while training. In this work, we aim to escape from this inefficient alignment with massive but redundant frames, and instead to directly localize a few action transitions for pseudo segmentation generation, where a transition refers to the change from an action segment to its next adjacent one in the transcript. As the true transitions are submerged in noisy boundaries due to intra-segment visual variation, we propose a novel Action-Transition-Aware Boundary Alignment (ATBA) framework to efficiently and effectively filter out noisy boundaries and detect transitions. In addition, to boost the semantic learning in the case that noise is inevitably present in the pseudo segmentation, we also introduce video-level losses to utilize the trusted video-level supervision. Extensive experiments show the effectiveness of our approach on both performance and training speed.","sentences":["Weakly-supervised action segmentation is a task of learning to partition a long video into several action segments, where training videos are only accompanied by transcripts (ordered list of actions).","Most of existing methods need to infer pseudo segmentation for training by serial alignment between all frames and the transcript, which is time-consuming and hard to be parallelized while training.","In this work, we aim to escape from this inefficient alignment with massive but redundant frames, and instead to directly localize a few action transitions for pseudo segmentation generation, where a transition refers to the change from an action segment to its next adjacent one in the transcript.","As the true transitions are submerged in noisy boundaries due to intra-segment visual variation, we propose a novel Action-Transition-Aware Boundary Alignment (ATBA) framework to efficiently and effectively filter out noisy boundaries and detect transitions.","In addition, to boost the semantic learning in the case that noise is inevitably present in the pseudo segmentation, we also introduce video-level losses to utilize the trusted video-level supervision.","Extensive experiments show the effectiveness of our approach on both performance and training speed."],"url":"http://arxiv.org/abs/2403.19225v1","category":"cs.CV"}
{"created":"2024-03-28 08:35:46","title":"Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality","abstract":"Video paragraph captioning (VPC) involves generating detailed narratives for long videos, utilizing supportive modalities such as speech and event boundaries. However, the existing models are constrained by the assumption of constant availability of a single auxiliary modality, which is impractical given the diversity and unpredictable nature of real-world scenarios. To this end, we propose a Missing-Resistant framework MR-VPC that effectively harnesses all available auxiliary inputs and maintains resilience even in the absence of certain modalities. Under this framework, we propose the Multimodal VPC (MVPC) architecture integrating video, speech, and event boundary inputs in a unified manner to process various auxiliary inputs. Moreover, to fortify the model against incomplete data, we introduce DropAM, a data augmentation strategy that randomly omits auxiliary inputs, paired with DistillAM, a regularization target that distills knowledge from teacher models trained on modality-complete data, enabling efficient learning in modality-deficient environments. Through exhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has proven to deliver superior performance on modality-complete and modality-missing test data. This work highlights the significance of developing resilient VPC models and paves the way for more adaptive, robust multimodal video understanding.","sentences":["Video paragraph captioning (VPC) involves generating detailed narratives for long videos, utilizing supportive modalities such as speech and event boundaries.","However, the existing models are constrained by the assumption of constant availability of a single auxiliary modality, which is impractical given the diversity and unpredictable nature of real-world scenarios.","To this end, we propose a Missing-Resistant framework MR-VPC that effectively harnesses all available auxiliary inputs and maintains resilience even in the absence of certain modalities.","Under this framework, we propose the Multimodal VPC (MVPC) architecture integrating video, speech, and event boundary inputs in a unified manner to process various auxiliary inputs.","Moreover, to fortify the model against incomplete data, we introduce DropAM, a data augmentation strategy that randomly omits auxiliary inputs, paired with DistillAM, a regularization target that distills knowledge from teacher models trained on modality-complete data, enabling efficient learning in modality-deficient environments.","Through exhaustive experimentation on YouCook2 and ActivityNet Captions, MR-VPC has proven to deliver superior performance on modality-complete and modality-missing test data.","This work highlights the significance of developing resilient VPC models and paves the way for more adaptive, robust multimodal video understanding."],"url":"http://arxiv.org/abs/2403.19221v1","category":"cs.CV"}
{"created":"2024-03-28 08:34:04","title":"GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds","abstract":"Point clouds captured by different sensors such as RGB-D cameras and LiDAR possess non-negligible domain gaps. Most existing methods design different network architectures and train separately on point clouds from various sensors. Typically, point-based methods achieve outstanding performances on even-distributed dense point clouds from RGB-D cameras, while voxel-based methods are more efficient for large-range sparse LiDAR point clouds. In this paper, we propose geometry-to-voxel auxiliary learning to enable voxel representations to access point-level geometric information, which supports better generalisation of the voxel-based backbone with additional interpretations of multi-sensor point clouds. Specifically, we construct hierarchical geometry pools generated by a voxel-guided dynamic point network, which efficiently provide auxiliary fine-grained geometric information adapted to different stages of voxel features. We conduct experiments on joint multi-sensor datasets to demonstrate the effectiveness of GeoAuxNet. Enjoying elaborate geometric information, our method outperforms other models collectively trained on multi-sensor datasets, and achieve competitive results with the-state-of-art experts on each single dataset.","sentences":["Point clouds captured by different sensors such as RGB-D cameras and LiDAR possess non-negligible domain gaps.","Most existing methods design different network architectures and train separately on point clouds from various sensors.","Typically, point-based methods achieve outstanding performances on even-distributed dense point clouds from RGB-D cameras, while voxel-based methods are more efficient for large-range sparse LiDAR point clouds.","In this paper, we propose geometry-to-voxel auxiliary learning to enable voxel representations to access point-level geometric information, which supports better generalisation of the voxel-based backbone with additional interpretations of multi-sensor point clouds.","Specifically, we construct hierarchical geometry pools generated by a voxel-guided dynamic point network, which efficiently provide auxiliary fine-grained geometric information adapted to different stages of voxel features.","We conduct experiments on joint multi-sensor datasets to demonstrate the effectiveness of GeoAuxNet.","Enjoying elaborate geometric information, our method outperforms other models collectively trained on multi-sensor datasets, and achieve competitive results with the-state-of-art experts on each single dataset."],"url":"http://arxiv.org/abs/2403.19220v1","category":"cs.CV"}
{"created":"2024-03-28 08:30:14","title":"Blind Identification of Binaural Room Impulse Responses from Smart Glasses","abstract":"Smart glasses are increasingly recognized as a key medium for augmented reality, offering a hands-free platform with integrated microphones and non-ear-occluding loudspeakers to seamlessly mix virtual sound sources into the real-world acoustic scene. To convincingly integrate virtual sound sources, the room acoustic rendering of the virtual sources must match the real-world acoustics. Information about a user's acoustic environment however is typically not available. This work uses a microphone array in a pair of smart glasses to blindly identify binaural room impulse responses (BRIRs) from a few seconds of speech in the real-world environment. The proposed method uses dereverberation and beamforming to generate a pseudo reference signal that is used by a multichannel Wiener filter to estimate room impulse responses which are then converted to BRIRs. The multichannel room impulse responses can be used to estimate room acoustic parameters which is shown to outperform baseline algorithms in the estimation of reverberation time and direct-to-reverberant energy ratio. Results from a listening experiment further indicate that the estimated BRIRs often reproduce the real-world room acoustics perceptually more convincingly than measured BRIRs from other rooms with similar geometry.","sentences":["Smart glasses are increasingly recognized as a key medium for augmented reality, offering a hands-free platform with integrated microphones and non-ear-occluding loudspeakers to seamlessly mix virtual sound sources into the real-world acoustic scene.","To convincingly integrate virtual sound sources, the room acoustic rendering of the virtual sources must match the real-world acoustics.","Information about a user's acoustic environment however is typically not available.","This work uses a microphone array in a pair of smart glasses to blindly identify binaural room impulse responses (BRIRs) from a few seconds of speech in the real-world environment.","The proposed method uses dereverberation and beamforming to generate a pseudo reference signal that is used by a multichannel Wiener filter to estimate room impulse responses which are then converted to BRIRs.","The multichannel room impulse responses can be used to estimate room acoustic parameters which is shown to outperform baseline algorithms in the estimation of reverberation time and direct-to-reverberant energy ratio.","Results from a listening experiment further indicate that the estimated BRIRs often reproduce the real-world room acoustics perceptually more convincingly than measured BRIRs from other rooms with similar geometry."],"url":"http://arxiv.org/abs/2403.19217v1","category":"eess.AS"}
{"created":"2024-03-28 08:27:44","title":"Are Large Language Models Good at Utility Judgments?","abstract":"Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently. Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility. Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering. In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA. Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs. Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages. Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design. And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results. (iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation. We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs. Our code and benchmark can be found at \\url{https://github.com/ict-bigdatalab/utility_judgments}.","sentences":["Retrieval-augmented generation (RAG) is considered to be a promising approach to alleviate the hallucination issue of large language models (LLMs), and it has received widespread attention from researchers recently.","Due to the limitation in the semantic understanding of retrieval models, the success of RAG heavily lies on the ability of LLMs to identify passages with utility.","Recent efforts have explored the ability of LLMs to assess the relevance of passages in retrieval, but there has been limited work on evaluating the utility of passages in supporting question answering.","In this work, we conduct a comprehensive study about the capabilities of LLMs in utility evaluation for open-domain QA.","Specifically, we introduce a benchmarking procedure and collection of candidate passages with different characteristics, facilitating a series of experiments with five representative LLMs.","Our experiments reveal that: (i) well-instructed LLMs can distinguish between relevance and utility, and that LLMs are highly receptive to newly generated counterfactual passages.","Moreover, (ii) we scrutinize key factors that affect utility judgments in the instruction design.","And finally, (iii) to verify the efficacy of utility judgments in practical retrieval augmentation applications, we delve into LLMs' QA capabilities using the evidence judged with utility and direct dense retrieval results.","(iv) We propose a k-sampling, listwise approach to reduce the dependency of LLMs on the sequence of input passages, thereby facilitating subsequent answer generation.","We believe that the way we formalize and study the problem along with our findings contributes to a critical assessment of retrieval-augmented LLMs.","Our code and benchmark can be found at \\url{https://github.com/ict-bigdatalab/utility_judgments}."],"url":"http://arxiv.org/abs/2403.19216v1","category":"cs.IR"}
{"created":"2024-03-28 08:27:32","title":"Dynamical reconstruction of the $\u039b$CDM model in hybrid metric-Palatini gravity","abstract":"In this work, we apply the formalism of dynamical systems to analyze the viability of the $\\Lambda$CDM model in a generalized form of the hybrid metric-Palatini gravity theory written in terms of its dynamically equivalent scalar-tensor representation. Adopting a matter distribution composed of two relativistic fluids described by the equations of state of radiation and pressureless dust, one verifies that the cosmological phase space features the usual curvature-dominated, radiation-dominated, matter-dominated, and exponentially accelerated fixed points, even in the absence of a dark energy component. A numerical integration of the dynamical equations describing the system, subjected to initial conditions consistent with the cosmographic observations from the Planck satellite and weak-field solar system dynamics, shows that cosmological solutions with the same behavior as the $\\Lambda$CDM model in General Relativity (GR) are attainable in this theory, with the deviations from GR being exponentially suppressed at early-times and the scalar-field potential effectively playing the role of dark energy at late times.","sentences":["In this work, we apply the formalism of dynamical systems to analyze the viability of the $\\Lambda$CDM model in a generalized form of the hybrid metric-Palatini gravity theory written in terms of its dynamically equivalent scalar-tensor representation.","Adopting a matter distribution composed of two relativistic fluids described by the equations of state of radiation and pressureless dust, one verifies that the cosmological phase space features the usual curvature-dominated, radiation-dominated, matter-dominated, and exponentially accelerated fixed points, even in the absence of a dark energy component.","A numerical integration of the dynamical equations describing the system, subjected to initial conditions consistent with the cosmographic observations from the Planck satellite and weak-field solar system dynamics, shows that cosmological solutions with the same behavior as the $\\Lambda$CDM model in General Relativity (GR) are attainable in this theory, with the deviations from GR being exponentially suppressed at early-times and the scalar-field potential effectively playing the role of dark energy at late times."],"url":"http://arxiv.org/abs/2403.19215v1","category":"gr-qc"}
{"created":"2024-03-28 17:21:05","title":"Reference Energies for Double Excitations: Improvement and Extension","abstract":"In the realm of photochemistry, the significance of double excitations (also known as doubly-excited states), where two electrons are concurrently elevated to higher energy levels, lies in their involvement in key electronic transitions essential in light-induced chemical reactions as well as their challenging nature from the computational theoretical chemistry point of view. Based on state-of-the-art electronic structure methods (such as high-order coupled-cluster, selected configuration interaction, and multiconfigurational methods), we improve and expand our prior set of accurate reference excitation energies for electronic states exhibiting a substantial amount of double excitations [http://dx.doi.org/10.1021/acs.jctc.8b01205; Loos et al. J. Chem. Theory Comput. 2019, 15, 1939]. This extended collection encompasses 47 electronic transitions across 26 molecular systems that we separate into two distinct subsets: (i) 28 \"genuine\" doubly-excited states where the transitions almost exclusively involve doubly-excited configurations and (ii) 19 \"partial\" doubly-excited states which exhibit a more balanced character between singly- and doubly-excited configurations. For each subset, we assess the performance of high-order coupled-cluster (CC3, CCSDT, CC4, and CCSDTQ) and multiconfigurational methods (CASPT2, CASPT3, PC-NEVPT2, and SC-NEVPT2). Using as a probe the percentage of single excitations involved in a given transition ($\\%T_1$) computed at the CC3 level, we also propose a simple correction that reduces the errors of CC3 by a factor of 3, for both sets of excitations. We hope that this more complete and diverse compilation of double excitations will help future developments of electronic excited-state methodologies.","sentences":["In the realm of photochemistry, the significance of double excitations (also known as doubly-excited states), where two electrons are concurrently elevated to higher energy levels, lies in their involvement in key electronic transitions essential in light-induced chemical reactions as well as their challenging nature from the computational theoretical chemistry point of view.","Based on state-of-the-art electronic structure methods (such as high-order coupled-cluster, selected configuration interaction, and multiconfigurational methods), we improve and expand our prior set of accurate reference excitation energies for electronic states exhibiting a substantial amount of double excitations [http://dx.doi.org/10.1021/acs.jctc.8b01205; Loos et al. J. Chem.","Theory Comput.","2019, 15, 1939].","This extended collection encompasses 47 electronic transitions across 26 molecular systems that we separate into two distinct subsets: (i) 28 \"genuine\" doubly-excited states where the transitions almost exclusively involve doubly-excited configurations and (ii) 19 \"partial\" doubly-excited states which exhibit a more balanced character between singly- and doubly-excited configurations.","For each subset, we assess the performance of high-order coupled-cluster (CC3, CCSDT, CC4, and CCSDTQ) and multiconfigurational methods (CASPT2, CASPT3, PC-NEVPT2, and SC-NEVPT2).","Using as a probe the percentage of single excitations involved in a given transition ($\\%T_1$) computed at the CC3 level, we also propose a simple correction that reduces the errors of CC3 by a factor of 3, for both sets of excitations.","We hope that this more complete and diverse compilation of double excitations will help future developments of electronic excited-state methodologies."],"url":"http://arxiv.org/abs/2403.19597v1","category":"physics.chem-ph"}
{"created":"2024-03-28 17:05:04","title":"OV-Uni3DETR: Towards Unified Open-Vocabulary 3D Object Detection via Cycle-Modality Propagation","abstract":"In the current state of 3D object detection research, the severe scarcity of annotated 3D data, substantial disparities across different data modalities, and the absence of a unified architecture, have impeded the progress towards the goal of universality. In this paper, we propose \\textbf{OV-Uni3DETR}, a unified open-vocabulary 3D detector via cycle-modality propagation. Compared with existing 3D detectors, OV-Uni3DETR offers distinct advantages: 1) Open-vocabulary 3D detection: During training, it leverages various accessible data, especially extensive 2D detection images, to boost training diversity. During inference, it can detect both seen and unseen classes. 2) Modality unifying: It seamlessly accommodates input data from any given modality, effectively addressing scenarios involving disparate modalities or missing sensor information, thereby supporting test-time modality switching. 3) Scene unifying: It provides a unified multi-modal model architecture for diverse scenes collected by distinct sensors. Specifically, we propose the cycle-modality propagation, aimed at propagating knowledge bridging 2D and 3D modalities, to support the aforementioned functionalities. 2D semantic knowledge from large-vocabulary learning guides novel class discovery in the 3D domain, and 3D geometric knowledge provides localization supervision for 2D detection images. OV-Uni3DETR achieves the state-of-the-art performance on various scenarios, surpassing existing methods by more than 6\\% on average. Its performance using only RGB images is on par with or even surpasses that of previous point cloud based methods. Code and pre-trained models will be released later.","sentences":["In the current state of 3D object detection research, the severe scarcity of annotated 3D data, substantial disparities across different data modalities, and the absence of a unified architecture, have impeded the progress towards the goal of universality.","In this paper, we propose \\textbf{OV-Uni3DETR}, a unified open-vocabulary 3D detector via cycle-modality propagation.","Compared with existing 3D detectors, OV-Uni3DETR offers distinct advantages: 1) Open-vocabulary 3D detection: During training, it leverages various accessible data, especially extensive 2D detection images, to boost training diversity.","During inference, it can detect both seen and unseen classes.","2) Modality unifying: It seamlessly accommodates input data from any given modality, effectively addressing scenarios involving disparate modalities or missing sensor information, thereby supporting test-time modality switching.","3) Scene unifying: It provides a unified multi-modal model architecture for diverse scenes collected by distinct sensors.","Specifically, we propose the cycle-modality propagation, aimed at propagating knowledge bridging 2D and 3D modalities, to support the aforementioned functionalities.","2D semantic knowledge from large-vocabulary learning guides novel class discovery in the 3D domain, and 3D geometric knowledge provides localization supervision for 2D detection images.","OV-Uni3DETR achieves the state-of-the-art performance on various scenarios, surpassing existing methods by more than 6\\% on average.","Its performance using only RGB images is on par with or even surpasses that of previous point cloud based methods.","Code and pre-trained models will be released later."],"url":"http://arxiv.org/abs/2403.19580v1","category":"cs.CV"}
{"created":"2024-03-28 16:55:48","title":"Ultrafast exciton transport in van der Waals heterostructures","abstract":"Excitons in van der Waals heterostructures based on atomically thin transition metal dichalcogenides are considered as potential candidates for the formation of a superfluid state in two-dimensional systems. A number of studies reported observations of ultrafast nondiffusive propagation of excitons in van der Waals heterostructures, which was considered by their authors as possible evidence of collective effects in excitonic systems. In this paper, after a brief analysis of exciton propagation regimes in two-dimensional semiconductors, an alternative model of ultrafast exciton transport is proposed, based on the formation of waveguide modes in van der Waals heterostructures and the radiation transfer by these modes.","sentences":["Excitons in van der Waals heterostructures based on atomically thin transition metal dichalcogenides are considered as potential candidates for the formation of a superfluid state in two-dimensional systems.","A number of studies reported observations of ultrafast nondiffusive propagation of excitons in van der Waals heterostructures, which was considered by their authors as possible evidence of collective effects in excitonic systems.","In this paper, after a brief analysis of exciton propagation regimes in two-dimensional semiconductors, an alternative model of ultrafast exciton transport is proposed, based on the formation of waveguide modes in van der Waals heterostructures and the radiation transfer by these modes."],"url":"http://arxiv.org/abs/2403.19571v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 16:44:14","title":"Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset","abstract":"Hate speech detection models are only as good as the data they are trained on. Datasets sourced from social media suffer from systematic gaps and biases, leading to unreliable models with simplistic decision boundaries. Adversarial datasets, collected by exploiting model weaknesses, promise to fix this problem. However, adversarial data collection can be slow and costly, and individual annotators have limited creativity. In this paper, we introduce GAHD, a new German Adversarial Hate speech Dataset comprising ca.\\ 11k examples. During data collection, we explore new strategies for supporting annotators, to create more diverse adversarial examples more efficiently and provide a manual analysis of annotator disagreements for each strategy. Our experiments show that the resulting dataset is challenging even for state-of-the-art hate speech detection models, and that training on GAHD clearly improves model robustness. Further, we find that mixing multiple support strategies is most advantageous. We make GAHD publicly available at https://github.com/jagol/gahd.","sentences":["Hate speech detection models are only as good as the data they are trained on.","Datasets sourced from social media suffer from systematic gaps and biases, leading to unreliable models with simplistic decision boundaries.","Adversarial datasets, collected by exploiting model weaknesses, promise to fix this problem.","However, adversarial data collection can be slow and costly, and individual annotators have limited creativity.","In this paper, we introduce GAHD, a new German Adversarial Hate speech Dataset comprising ca.\\ 11k examples.","During data collection, we explore new strategies for supporting annotators, to create more diverse adversarial examples more efficiently and provide a manual analysis of annotator disagreements for each strategy.","Our experiments show that the resulting dataset is challenging even for state-of-the-art hate speech detection models, and that training on GAHD clearly improves model robustness.","Further, we find that mixing multiple support strategies is most advantageous.","We make GAHD publicly available at https://github.com/jagol/gahd."],"url":"http://arxiv.org/abs/2403.19559v1","category":"cs.CL"}
{"created":"2024-03-28 16:38:54","title":"Expectation Maximization Aided Modified Weighted Sequential Energy Detector for Distributed Cooperative Spectrum Sensing","abstract":"Distributed cooperative spectrum sensing usually involves a group of unlicensed secondary users (SUs) collaborating to detect the primary user (PU) in the channel, and thereby opportunistically utilize it without causing interference to the PU. The conventional energy detector (ED) based spectrum sensing ignores the dynamic nature of the PU by using energy statistic only from the present sensing interval for the PU detection. However, for a dynamic PU, previous studies have shown that improved detection capabilities can be achieved by aggregating both present and past energy samples in a test statistic. To this end, a weighted sequential energy detector (WSED) has been proposed, but it is based on aggregating all the collected energy samples over an observation window. For a highly dynamic PU, that involves also combining the outdated samples in the test statistic. In this paper, we propose a modified WSED (mWSED) that uses the primary user states information over the window to aggregate only the highly correlated energy samples in its test statistic. In practice, since the PU states are a priori unknown, we also develop a joint expectation-maximization and Viterbi (EM-Viterbi) algorithm based scheme to iteratively estimate the states by using the energy samples collected over the window. The estimated states are then used in mWSED to compute its test statistics, and the algorithm is referred to here as EM-mWSED. Simulation results are presented to demonstrate the states estimation performance of EM-Viterbi and the PU detection performance of EM-mWSED. The results show that, for both highly dynamic as well as slowly time-varying PU, these algorithms outperform the ED and WSED at PU detection, and their performances improve by either increasing the average number of neighbors per SU in the network, or by increasing the SNR or the number of samples per energy statistic.","sentences":["Distributed cooperative spectrum sensing usually involves a group of unlicensed secondary users (SUs) collaborating to detect the primary user (PU) in the channel, and thereby opportunistically utilize it without causing interference to the PU.","The conventional energy detector (ED) based spectrum sensing ignores the dynamic nature of the PU by using energy statistic only from the present sensing interval for the PU detection.","However, for a dynamic PU, previous studies have shown that improved detection capabilities can be achieved by aggregating both present and past energy samples in a test statistic.","To this end, a weighted sequential energy detector (WSED) has been proposed, but it is based on aggregating all the collected energy samples over an observation window.","For a highly dynamic PU, that involves also combining the outdated samples in the test statistic.","In this paper, we propose a modified WSED (mWSED) that uses the primary user states information over the window to aggregate only the highly correlated energy samples in its test statistic.","In practice, since the PU states are a priori unknown, we also develop a joint expectation-maximization and Viterbi (EM-Viterbi) algorithm based scheme to iteratively estimate the states by using the energy samples collected over the window.","The estimated states are then used in mWSED to compute its test statistics, and the algorithm is referred to here as EM-mWSED.","Simulation results are presented to demonstrate the states estimation performance of EM-Viterbi and the PU detection performance of EM-mWSED.","The results show that, for both highly dynamic as well as slowly time-varying PU, these algorithms outperform the ED and WSED at PU detection, and their performances improve by either increasing the average number of neighbors per SU in the network, or by increasing the SNR or the number of samples per energy statistic."],"url":"http://arxiv.org/abs/2403.19556v1","category":"eess.SY"}
{"created":"2024-03-28 16:06:06","title":"Detecting Financial Bots on the Ethereum Blockchain","abstract":"The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6%, while the highest-performing model for binary classification is a Random Forest with an accuracy of 83%. Our machine learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape.","sentences":["The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation.","However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity.","It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility.","In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform.","First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories.","Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses.","Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum.","The highest-performing clustering algorithm is a Gaussian Mixture Model with an average cluster purity of 82.6%, while the highest-performing model for binary classification is a Random Forest with an accuracy of 83%.","Our machine learning-based detection mechanism contributes to understanding the Ethereum ecosystem dynamics by providing additional insights into the current bot landscape."],"url":"http://arxiv.org/abs/2403.19530v1","category":"cs.CR"}
{"created":"2024-03-28 14:34:02","title":"Offline Imitation Learning from Multiple Baselines with Applications to Compiler Optimization","abstract":"This work studies a Reinforcement Learning (RL) problem in which we are given a set of trajectories collected with K baseline policies. Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space. The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space. We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound. Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary. We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach.","sentences":["This work studies a Reinforcement Learning (RL) problem in which we are given a set of trajectories collected with K baseline policies.","Each of these policies can be quite suboptimal in isolation, and have strong performance in complementary parts of the state space.","The goal is to learn a policy which performs as well as the best combination of baselines on the entire state space.","We propose a simple imitation learning based algorithm, show a sample complexity bound on its accuracy and prove that the the algorithm is minimax optimal by showing a matching lower bound.","Further, we apply the algorithm in the setting of machine learning guided compiler optimization to learn policies for inlining programs with the objective of creating a small binary.","We demonstrate that we can learn a policy that outperforms an initial policy learned via standard RL through a few iterations of our approach."],"url":"http://arxiv.org/abs/2403.19462v1","category":"cs.LG"}
{"created":"2024-03-28 13:19:16","title":"KazParC: Kazakh Parallel Corpus for Machine Translation","abstract":"We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish. The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators. Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash. Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.","sentences":["We introduce KazParC, a parallel corpus designed for machine translation across Kazakh, English, Russian, and Turkish.","The first and largest publicly available corpus of its kind, KazParC contains a collection of 371,902 parallel sentences covering different domains and developed with the assistance of human translators.","Our research efforts also extend to the development of a neural machine translation model nicknamed Tilmash.","Remarkably, the performance of Tilmash is on par with, and in certain instances, surpasses that of industry giants, such as Google Translate and Yandex Translate, as measured by standard evaluation metrics, such as BLEU and chrF. Both KazParC and Tilmash are openly available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository."],"url":"http://arxiv.org/abs/2403.19399v1","category":"cs.CL"}
{"created":"2024-03-28 13:03:57","title":"Holography of a single free matrix","abstract":"In this paper we consider the collective field theory description of a single free massless scalar matrix theory in 2+1 dimensions. The collective fields are given by $k$-local operators obtained by tracing a product of $k$-matrices. For $k=2$ and $k=3$ we argue that the collective field packages the fields associated to a single and two Regge trajectories respectively. We also determine the coordinate transformation between the coordinates of the collective field theory and the bulk AdS space time. This is used to verify that the bulk equations of motion holds in the collective field theory description.","sentences":["In this paper we consider the collective field theory description of a single free massless scalar matrix theory in 2+1 dimensions.","The collective fields are given by $k$-local operators obtained by tracing a product of $k$-matrices.","For $k=2$ and $k=3$ we argue that the collective field packages the fields associated to a single and two Regge trajectories respectively.","We also determine the coordinate transformation between the coordinates of the collective field theory and the bulk AdS space time.","This is used to verify that the bulk equations of motion holds in the collective field theory description."],"url":"http://arxiv.org/abs/2403.19391v1","category":"hep-th"}
{"created":"2024-03-28 12:29:14","title":"Truth and collection","abstract":"Answering a question of Kaye, we show that the compositional truth theory with a full collection scheme is conservative over Peano Arithmetic. We demonstrate it by showing that countable models of compositional truth which satisfy the internal induction or collection axioms can be end-extended to models of the respective theory.","sentences":["Answering a question of Kaye, we show that the compositional truth theory with a full collection scheme is conservative over Peano Arithmetic.","We demonstrate it by showing that countable models of compositional truth which satisfy the internal induction or collection axioms can be end-extended to models of the respective theory."],"url":"http://arxiv.org/abs/2403.19367v1","category":"math.LO"}
{"created":"2024-03-28 12:26:45","title":"EthioMT: Parallel Corpus for Low-resource Ethiopian Languages","abstract":"Recent research in natural language processing (NLP) has achieved impressive performance in tasks such as machine translation (MT), news classification, and question-answering in high-resource languages. However, the performance of MT leaves much to be desired for low-resource languages. This is due to the smaller size of available parallel corpora in these languages, if such corpora are available at all. NLP in Ethiopian languages suffers from the same issues due to the unavailability of publicly accessible datasets for NLP tasks, including MT. To help the research community and foster research for Ethiopian languages, we introduce EthioMT -- a new parallel corpus for 15 languages. We also create a new benchmark by collecting a dataset for better-researched languages in Ethiopia. We evaluate the newly collected corpus and the benchmark dataset for 23 Ethiopian languages using transformer and fine-tuning approaches.","sentences":["Recent research in natural language processing (NLP) has achieved impressive performance in tasks such as machine translation (MT), news classification, and question-answering in high-resource languages.","However, the performance of MT leaves much to be desired for low-resource languages.","This is due to the smaller size of available parallel corpora in these languages, if such corpora are available at all.","NLP in Ethiopian languages suffers from the same issues due to the unavailability of publicly accessible datasets for NLP tasks, including MT.","To help the research community and foster research for Ethiopian languages, we introduce EthioMT -- a new parallel corpus for 15 languages.","We also create a new benchmark by collecting a dataset for better-researched languages in Ethiopia.","We evaluate the newly collected corpus and the benchmark dataset for 23 Ethiopian languages using transformer and fine-tuning approaches."],"url":"http://arxiv.org/abs/2403.19365v1","category":"cs.CL"}
{"created":"2024-03-28 12:08:39","title":"A diverse Multilingual News Headlines Dataset from around the World","abstract":"Babel Briefings is a novel dataset featuring 4.7 million news headlines from August 2020 to November 2021, across 30 languages and 54 locations worldwide with English translations of all articles included. Designed for natural language processing and media studies, it serves as a high-quality dataset for training or evaluating language models as well as offering a simple, accessible collection of articles, for example, to analyze global news coverage and cultural narratives. As a simple demonstration of the analyses facilitated by this dataset, we use a basic procedure using a TF-IDF weighted similarity metric to group articles into clusters about the same event. We then visualize the \\emph{event signatures} of the event showing articles of which languages appear over time, revealing intuitive features based on the proximity of the event and unexpectedness of the event. The dataset is available on \\href{https://www.kaggle.com/datasets/felixludos/babel-briefings}{Kaggle} and \\href{https://huggingface.co/datasets/felixludos/babel-briefings}{HuggingFace} with accompanying \\href{https://github.com/felixludos/babel-briefings}{GitHub} code.","sentences":["Babel Briefings is a novel dataset featuring 4.7 million news headlines from August 2020 to November 2021, across 30 languages and 54 locations worldwide with English translations of all articles included.","Designed for natural language processing and media studies, it serves as a high-quality dataset for training or evaluating language models as well as offering a simple, accessible collection of articles, for example, to analyze global news coverage and cultural narratives.","As a simple demonstration of the analyses facilitated by this dataset, we use a basic procedure using a TF-IDF weighted similarity metric to group articles into clusters about the same event.","We then visualize the \\emph{event signatures} of the event showing articles of which languages appear over time, revealing intuitive features based on the proximity of the event and unexpectedness of the event.","The dataset is available on \\href{https://www.kaggle.com/datasets/felixludos/babel-briefings}{Kaggle} and \\href{https://huggingface.co/datasets/felixludos/babel-briefings}{HuggingFace} with accompanying \\href{https://github.com/felixludos/babel-briefings}{GitHub} code."],"url":"http://arxiv.org/abs/2403.19352v1","category":"cs.CL"}
{"created":"2024-03-28 11:57:06","title":"An Interactive Human-Machine Learning Interface for Collecting and Learning from Complex Annotations","abstract":"Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence. In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected. For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise counterfactual examples to complement standard binary labels as annotations for a dataset. Finally we discuss the challenges in future extensions of this work.","sentences":["Human-Computer Interaction has been shown to lead to improvements in machine learning systems by boosting model performance, accelerating learning and building user confidence.","In this work, we aim to alleviate the expectation that human annotators adapt to the constraints imposed by traditional labels by allowing for extra flexibility in the form that supervision information is collected.","For this, we propose a human-machine learning interface for binary classification tasks which enables human annotators to utilise counterfactual examples to complement standard binary labels as annotations for a dataset.","Finally we discuss the challenges in future extensions of this work."],"url":"http://arxiv.org/abs/2403.19339v1","category":"cs.LG"}
{"created":"2024-03-28 11:51:11","title":"KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes","abstract":"This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment analysis that is the first and largest publicly available dataset of its kind. KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from various sources and includes numerical ratings ranging from 1 to 5, providing a quantitative representation of customer attitudes. The study also pursued the automation of Kazakh sentiment classification through the development and evaluation of four machine learning models trained for both polarity classification and score classification. Experimental analysis included evaluation of the results considering both balanced and imbalanced scenarios. The most successful model attained an F1-score of 0.81 for polarity classification and 0.39 for score classification on the test sets. The dataset and fine-tuned models are open access and available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository.","sentences":["This paper presents KazSAnDRA, a dataset developed for Kazakh sentiment analysis that is the first and largest publicly available dataset of its kind.","KazSAnDRA comprises an extensive collection of 180,064 reviews obtained from various sources and includes numerical ratings ranging from 1 to 5, providing a quantitative representation of customer attitudes.","The study also pursued the automation of Kazakh sentiment classification through the development and evaluation of four machine learning models trained for both polarity classification and score classification.","Experimental analysis included evaluation of the results considering both balanced and imbalanced scenarios.","The most successful model attained an F1-score of 0.81 for polarity classification and 0.39 for score classification on the test sets.","The dataset and fine-tuned models are open access and available for download under the Creative Commons Attribution 4.0 International License (CC BY 4.0) through our GitHub repository."],"url":"http://arxiv.org/abs/2403.19335v1","category":"cs.CL"}
{"created":"2024-03-28 11:25:47","title":"Experience deploying an analysis facility for the Rubin Observatory's Legacy Survey of Space and Time (LSST) data","abstract":"The Vera C. Rubin Observatory is preparing for the execution of the most ambitious astronomical survey ever attempted, the Legacy Survey of Space and Time (LSST). Currently in its final phase of construction in the Andes mountains in Chile and due to start operations in 2025 for 10 years, its 8.4-meter telescope will nightly scan the southern sky and collect images of the entire visible sky every 4 nights using a 3.2 Gigapixel camera, the largest imaging device ever built for astronomy. Automated detection and classification of celestial objects will be performed by sophisticated algorithms on high-resolution images to progressively produce an astronomical catalog eventually composed of 20 billion galaxies and 17 billion stars and their associated physical properties. In this paper, we briefly present the infrastructure deployed at the French Rubin data facility (operated by IN2P3 computing center, CC-IN2P3) to deploy the Rubin Science Platform, a set of web-based services to provide effective and convenient access to LSST data for scientific analysis. We describe the main services of the platform, the components that provide those services and our deployment model. We also present the Kubernetes-based infrastructure we are experimenting with for hosting the LSST astronomical catalog, a petabyte-scale relational database developed for the specific needs of the project.","sentences":["The Vera C. Rubin Observatory is preparing for the execution of the most ambitious astronomical survey ever attempted, the Legacy Survey of Space and Time (LSST).","Currently in its final phase of construction in the Andes mountains in Chile and due to start operations in 2025 for 10 years, its 8.4-meter telescope will nightly scan the southern sky and collect images of the entire visible sky every 4 nights using a 3.2 Gigapixel camera, the largest imaging device ever built for astronomy.","Automated detection and classification of celestial objects will be performed by sophisticated algorithms on high-resolution images to progressively produce an astronomical catalog eventually composed of 20 billion galaxies and 17 billion stars and their associated physical properties.","In this paper, we briefly present the infrastructure deployed at the French Rubin data facility (operated by IN2P3 computing center, CC-IN2P3) to deploy the Rubin Science Platform, a set of web-based services to provide effective and convenient access to LSST data for scientific analysis.","We describe the main services of the platform, the components that provide those services and our deployment model.","We also present the Kubernetes-based infrastructure we are experimenting with for hosting the LSST astronomical catalog, a petabyte-scale relational database developed for the specific needs of the project."],"url":"http://arxiv.org/abs/2403.19321v1","category":"astro-ph.IM"}
{"created":"2024-03-28 09:37:06","title":"Infrared Reflection Absorption Spectroscopy Setup with Incidence Angle Selection for Surfaces of Non-Metals","abstract":"Infrared Reflection Absorption Spectroscopy (IRAS) on dielectric single crystals is challenging because the optimal incidence angles for light-adsorbate interaction coincide with regions of low IR reflectivity. Here, we introduce an optimized IRAS setup that maximizes the signal-to-noise ratio for non-metals. This is achieved by maximizing light throughput, and by selecting optimal incidence angles that directly impact the peak heights in the spectra. The setup uses a commercial FTIR spectrometer and is usable in ultra-high vacuum (UHV). Specifically, the design features sample illumination and collection mirrors with a high numerical aperture inside the UHV system, and an adjustable aperture to select the incidence angle range on the sample. This is important for p-polarized measurements on dielectrics, because the peaks in the spectra reverse direction at the Brewster angle (band inversion). The system components are connected precisely via a single flange, ensuring long-term stability. We studied the signal-to-noise (SNR) variation in p-polarized IRAS spectra for one monolayer of CO on TiO2(110) as a function of incidence angle range, where a maximum signal-to-noise ratio of 70 was achieved at 4 cm-1 resolution in five minutes measurement time. The capabilities for s-polarization are demonstrated by measuring one monolayer D2O adsorbed on a TiO2(110) surface, where a SNR of 65 was achieved at a delta_R/R0 peak height of 1.4x10-4 in twenty minutes.","sentences":["Infrared Reflection Absorption Spectroscopy (IRAS) on dielectric single crystals is challenging because the optimal incidence angles for light-adsorbate interaction coincide with regions of low IR reflectivity.","Here, we introduce an optimized IRAS setup that maximizes the signal-to-noise ratio for non-metals.","This is achieved by maximizing light throughput, and by selecting optimal incidence angles that directly impact the peak heights in the spectra.","The setup uses a commercial FTIR spectrometer and is usable in ultra-high vacuum (UHV).","Specifically, the design features sample illumination and collection mirrors with a high numerical aperture inside the UHV system, and an adjustable aperture to select the incidence angle range on the sample.","This is important for p-polarized measurements on dielectrics, because the peaks in the spectra reverse direction at the Brewster angle (band inversion).","The system components are connected precisely via a single flange, ensuring long-term stability.","We studied the signal-to-noise (SNR) variation in p-polarized IRAS spectra for one monolayer of CO on TiO2(110) as a function of incidence angle range, where a maximum signal-to-noise ratio of 70 was achieved at 4 cm-1 resolution in five minutes measurement time.","The capabilities for s-polarization are demonstrated by measuring one monolayer D2O adsorbed on a TiO2(110) surface, where a SNR of 65 was achieved at a delta_R/R0 peak height of 1.4x10-4 in twenty minutes."],"url":"http://arxiv.org/abs/2403.19263v1","category":"physics.optics"}
{"created":"2024-03-28 09:28:06","title":"Measurement of absolute branching fractions of $D_s^+$ hadronic decays","abstract":"Using $e^+ e^-$ collision data collected at the BESIII detector at center-of-mass energies between 4.128 and 4.226 GeV, corresponding to an integrated luminosity of $7.33~{\\rm fb}^{-1}$, we determine the absolute branching fractions of fifteen hadronic $D_s^{+}$ decays with a double-tag technique. In particular, we make precise measurements of the branching fractions $\\mathcal{B}(D_s^+ \\to K^+ K^- \\pi^+)=(5.49 \\pm 0.04 \\pm 0.07)\\%$, $\\mathcal{B}(D_s^+ \\to K_S^0 K^+)=(1.50 \\pm 0.01 \\pm 0.01)\\%$ and $\\mathcal{B}(D_s^+ \\to K^+ K^- \\pi^+ \\pi^0)=(5.50 \\pm 0.05 \\pm 0.11)\\%$, where the first uncertainties are statistical and the second ones are systematic. The \\emph{CP} asymmetries in these decays are also measured and all are found to be compatible with zero.","sentences":["Using $e^+ e^-$ collision data collected at the BESIII detector at center-of-mass energies between 4.128 and 4.226 GeV, corresponding to an integrated luminosity of $7.33~{\\rm fb}^{-1}$, we determine the absolute branching fractions of fifteen hadronic $D_s^{+}$ decays with a double-tag technique.","In particular, we make precise measurements of the branching fractions $\\mathcal{B}(D_s^+ \\to K^+ K^- \\pi^+)=(5.49 \\pm 0.04 \\pm 0.07)\\%$, $\\mathcal{B}(D_s^+ \\to K_S^0 K^+)=(1.50 \\pm 0.01 \\pm 0.01)\\%$ and $\\mathcal{B}(D_s^+ \\to K^+ K^- \\pi^+ \\pi^0)=(5.50 \\pm 0.05 \\pm 0.11)\\%$, where the first uncertainties are statistical and the second ones are systematic.","The \\emph{CP} asymmetries in these decays are also measured and all are found to be compatible with zero."],"url":"http://arxiv.org/abs/2403.19256v1","category":"hep-ex"}
{"created":"2024-03-28 08:52:59","title":"Cluster formation due to repulsive spanning trees in attractively coupled networks","abstract":"Ensembles of coupled nonlinear oscillators are a popular paradigm and an ideal benchmark for analyzing complex collective behaviors. The onset of cluster synchronization is found to be at the core of various technological and biological processes. The current literature has investigated cluster synchronization by focusing mostly on the case of attractive coupling among the oscillators. However, the case of two coexisting competing interactions is of practical interest due to their relevance in diverse natural settings, including neuronal networks consisting of excitatory and inhibitory neurons, the coevolving social model with voters of opposite opinions, ecological plant communities with both facilitation and competition, to name a few. In the present article, we investigate the impact of repulsive spanning trees on cluster formation within a connected network of attractively coupled limit cycle oscillators. We successfully predict which nodes belong to each cluster and the emergent frustration of the connected networks independent of the particular local dynamics at the network nodes. We also determine local asymptotic stability of the cluster states using an approach based on the formulation of a master stability function. We additionally validate the emergence of solitary states and antisynchronization for some specific choices of spanning trees and networks.","sentences":["Ensembles of coupled nonlinear oscillators are a popular paradigm and an ideal benchmark for analyzing complex collective behaviors.","The onset of cluster synchronization is found to be at the core of various technological and biological processes.","The current literature has investigated cluster synchronization by focusing mostly on the case of attractive coupling among the oscillators.","However, the case of two coexisting competing interactions is of practical interest due to their relevance in diverse natural settings, including neuronal networks consisting of excitatory and inhibitory neurons, the coevolving social model with voters of opposite opinions, ecological plant communities with both facilitation and competition, to name a few.","In the present article, we investigate the impact of repulsive spanning trees on cluster formation within a connected network of attractively coupled limit cycle oscillators.","We successfully predict which nodes belong to each cluster and the emergent frustration of the connected networks independent of the particular local dynamics at the network nodes.","We also determine local asymptotic stability of the cluster states using an approach based on the formulation of a master stability function.","We additionally validate the emergence of solitary states and antisynchronization for some specific choices of spanning trees and networks."],"url":"http://arxiv.org/abs/2403.19240v1","category":"nlin.AO"}
{"created":"2024-03-28 08:19:33","title":"Dual-Personalizing Adapter for Federated Foundation Models","abstract":"Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-time distribution shifts. To address challenges in this new setting, we explore a simple yet effective solution to learn a comprehensive foundation model. Specifically, a dual-personalizing adapter architecture (FedDPA) is proposed, comprising a global adapter and a local adapter for addressing test-time distribution shifts and personalization, respectively. Additionally, we introduce an instance-wise dynamic weighting mechanism to optimize the balance between the global and local adapters, enhancing overall performance. The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks.","sentences":["Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data.","Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data.","To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment.","However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications.","Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-time distribution shifts.","To address challenges in this new setting, we explore a simple yet effective solution to learn a comprehensive foundation model.","Specifically, a dual-personalizing adapter architecture (FedDPA) is proposed, comprising a global adapter and a local adapter for addressing test-time distribution shifts and personalization, respectively.","Additionally, we introduce an instance-wise dynamic weighting mechanism to optimize the balance between the global and local adapters, enhancing overall performance.","The effectiveness of the proposed method has been evaluated on benchmark datasets across different NLP tasks."],"url":"http://arxiv.org/abs/2403.19211v1","category":"cs.LG"}
{"created":"2024-03-28 08:18:26","title":"Cross-checking the geometric effects in heavy-ion collisions at 1 GeV/nucleon","abstract":"Employing the isospin-dependent Boltzmann-Uehling-Uhlenbeck transport model, the 1 GeV/nucleon deformed uranium-uranium ultra-central collisions are simulated. Based on sensitive observables, mean square collective flow and pion meson multiplicity, the impacts of high-momentum tails caused by short-range correlations and the symmetry energy in high-density regions on geometric effects are discussed under different reaction orientations. Finally, the neural network model for identifying reaction orientations is also developed.","sentences":["Employing the isospin-dependent Boltzmann-Uehling-Uhlenbeck transport model, the 1 GeV/nucleon deformed uranium-uranium ultra-central collisions are simulated.","Based on sensitive observables, mean square collective flow and pion meson multiplicity, the impacts of high-momentum tails caused by short-range correlations and the symmetry energy in high-density regions on geometric effects are discussed under different reaction orientations.","Finally, the neural network model for identifying reaction orientations is also developed."],"url":"http://arxiv.org/abs/2403.19208v1","category":"nucl-th"}
{"created":"2024-03-28 08:00:14","title":"Single-Shared Network with Prior-Inspired Loss for Parameter-Efficient Multi-Modal Imaging Skin Lesion Classification","abstract":"In this study, we introduce a multi-modal approach that efficiently integrates multi-scale clinical and dermoscopy features within a single network, thereby substantially reducing model parameters. The proposed method includes three novel fusion schemes.   Firstly, unlike current methods that usually employ two individual models for for clinical and dermoscopy modalities, we verified that multimodal feature can be learned by sharing the parameters of encoder while leaving the individual modal-specific classifiers.   Secondly, the shared cross-attention module can replace the individual one to efficiently interact between two modalities at multiple layers.   Thirdly, different from current methods that equally optimize dermoscopy and clinical branches, inspired by prior knowledge that dermoscopy images play a more significant role than clinical images, we propose a novel biased loss. This loss guides the single-shared network to prioritize dermoscopy information over clinical information, implicitly learning a better joint feature representation for the modal-specific task.   Extensive experiments on a well-recognized Seven-Point Checklist (SPC) dataset and a collected dataset demonstrate the effectiveness of our method on both CNN and Transformer structures. Furthermore, our method exhibits superiority in both accuracy and model parameters compared to currently advanced methods.","sentences":["In this study, we introduce a multi-modal approach that efficiently integrates multi-scale clinical and dermoscopy features within a single network, thereby substantially reducing model parameters.","The proposed method includes three novel fusion schemes.   ","Firstly, unlike current methods that usually employ two individual models for for clinical and dermoscopy modalities, we verified that multimodal feature can be learned by sharing the parameters of encoder while leaving the individual modal-specific classifiers.   ","Secondly, the shared cross-attention module can replace the individual one to efficiently interact between two modalities at multiple layers.   ","Thirdly, different from current methods that equally optimize dermoscopy and clinical branches, inspired by prior knowledge that dermoscopy images play a more significant role than clinical images, we propose a novel biased loss.","This loss guides the single-shared network to prioritize dermoscopy information over clinical information, implicitly learning a better joint feature representation for the modal-specific task.   ","Extensive experiments on a well-recognized Seven-Point Checklist (SPC) dataset and a collected dataset demonstrate the effectiveness of our method on both CNN and Transformer structures.","Furthermore, our method exhibits superiority in both accuracy and model parameters compared to currently advanced methods."],"url":"http://arxiv.org/abs/2403.19203v1","category":"eess.IV"}
{"created":"2024-03-28 07:55:29","title":"Understanding Archives: Towards New Research Interfaces Relying on the Semantic Annotation of Documents","abstract":"The digitisation campaigns carried out by libraries and archives in recent years have facilitated access to documents in their collections. However, exploring and exploiting these documents remain difficult tasks due to the sheer quantity of documents available for consultation. In this article, we show how the semantic annotation of the textual content of study corpora of archival documents allow to facilitate their exploitation and valorisation. First, we present a methodological framework for the construction of new interfaces based on textual semantics, then address the current technological obstacles and their potential solutions. We conclude by presenting a practical case of the application of this framework.","sentences":["The digitisation campaigns carried out by libraries and archives in recent years have facilitated access to documents in their collections.","However, exploring and exploiting these documents remain difficult tasks due to the sheer quantity of documents available for consultation.","In this article, we show how the semantic annotation of the textual content of study corpora of archival documents allow to facilitate their exploitation and valorisation.","First, we present a methodological framework for the construction of new interfaces based on textual semantics, then address the current technological obstacles and their potential solutions.","We conclude by presenting a practical case of the application of this framework."],"url":"http://arxiv.org/abs/2403.19201v1","category":"cs.DL"}
{"created":"2024-03-28 07:50:11","title":"Ordering Collective Unit Tasks: from Scheduling to Computational Social Choice","abstract":"We study the collective schedules problem, which consists in computing a one machine schedule of a set of tasks, knowing that a set of individuals (also called voters) have preferences regarding the order of the execution of the tasks. Our aim is to return a consensus schedule. We consider the setting in which all tasks have the same length -- such a schedule can therefore also be viewed as a ranking. We study two rules, one based on a distance criterion, and another one based one a binary criterion, and we show that these rules extend classic scheduling criteria. We also consider time constraints and precedence constraints between the tasks, and focus on two cases: the preferences of the voters fulfill these constraints, or they do not fulfill these constraints (but the collective schedule should fulfill them). In each case, either we show that the problem is NP-hard, or we provide a polynomial time algorithm which solves it. We also provide an analysis of a heuristic, which appears to be a 2 approximation of the Spearman's rule.","sentences":["We study the collective schedules problem, which consists in computing a one machine schedule of a set of tasks, knowing that a set of individuals (also called voters) have preferences regarding the order of the execution of the tasks.","Our aim is to return a consensus schedule.","We consider the setting in which all tasks have the same length -- such a schedule can therefore also be viewed as a ranking.","We study two rules, one based on a distance criterion, and another one based one a binary criterion, and we show that these rules extend classic scheduling criteria.","We also consider time constraints and precedence constraints between the tasks, and focus on two cases: the preferences of the voters fulfill these constraints, or they do not fulfill these constraints (but the collective schedule should fulfill them).","In each case, either we show that the problem is NP-hard, or we provide a polynomial time algorithm which solves it.","We also provide an analysis of a heuristic, which appears to be a 2 approximation of the Spearman's rule."],"url":"http://arxiv.org/abs/2403.19197v1","category":"cs.GT"}
{"created":"2024-03-28 07:43:49","title":"Text Data-Centric Image Captioning with Interactive Prompts","abstract":"Supervised image captioning approaches have made great progress, but it is challenging to collect high-quality human-annotated image-text data. Recently, large-scale vision and language models (e.g., CLIP) and large-scale generative language models (e.g., GPT-2) have shown strong performances in various tasks, which also provide some new solutions for image captioning with web paired data, unpaired data or even text-only data. Among them, the mainstream solution is to project image embeddings into the text embedding space with the assistance of consistent representations between image-text pairs from the CLIP model. However, the current methods still face several challenges in adapting to the diversity of data configurations in a unified solution, accurately estimating image-text embedding bias, and correcting unsatisfactory prediction results in the inference stage. This paper proposes a new Text data-centric approach with Interactive Prompts for image Captioning, named TIPCap. 1) We consider four different settings which gradually reduce the dependence on paired data. 2) We construct a mapping module driven by multivariate Gaussian distribution to mitigate the modality gap, which is applicable to the above four different settings. 3) We propose a prompt interaction module that can incorporate optional prompt information before generating captions. Extensive experiments show that our TIPCap outperforms other weakly or unsupervised image captioning methods and achieves a new state-of-the-art performance on two widely used datasets, i.e., MS-COCO and Flickr30K.","sentences":["Supervised image captioning approaches have made great progress, but it is challenging to collect high-quality human-annotated image-text data.","Recently, large-scale vision and language models (e.g., CLIP) and large-scale generative language models (e.g., GPT-2) have shown strong performances in various tasks, which also provide some new solutions for image captioning with web paired data, unpaired data or even text-only data.","Among them, the mainstream solution is to project image embeddings into the text embedding space with the assistance of consistent representations between image-text pairs from the CLIP model.","However, the current methods still face several challenges in adapting to the diversity of data configurations in a unified solution, accurately estimating image-text embedding bias, and correcting unsatisfactory prediction results in the inference stage.","This paper proposes a new Text data-centric approach with Interactive Prompts for image Captioning, named TIPCap.","1) We consider four different settings which gradually reduce the dependence on paired data.","2) We construct a mapping module driven by multivariate Gaussian distribution to mitigate the modality gap, which is applicable to the above four different settings.","3) We propose a prompt interaction module that can incorporate optional prompt information before generating captions.","Extensive experiments show that our TIPCap outperforms other weakly or unsupervised image captioning methods and achieves a new state-of-the-art performance on two widely used datasets, i.e., MS-COCO and Flickr30K."],"url":"http://arxiv.org/abs/2403.19193v1","category":"cs.CV"}
{"created":"2024-03-28 07:08:26","title":"Enhancing Trust and Privacy in Distributed Networks: A Comprehensive Survey on Blockchain-based Federated Learning","abstract":"While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities. Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm. Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes. Federated Learning (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange. Despite the growing interest in decentralized methods, their application in FL remains underexplored. This paper presents a thorough investigation into Blockchain-based FL (BCFL), spotlighting the synergy between blockchain's security features and FL's privacy-preserving model training capabilities. First, we present the taxonomy of BCFL from three aspects, including decentralized, separate networks, and reputation-based architectures. Then, we summarize the general architecture of BCFL systems, providing a comprehensive perspective on FL architectures informed by blockchain. Afterward, we analyze the application of BCFL in healthcare, IoT, and other privacy-sensitive areas. Finally, we identify future research directions of BCFL.","sentences":["While centralized servers pose a risk of being a single point of failure, decentralized approaches like blockchain offer a compelling solution by implementing a consensus mechanism among multiple entities.","Merging distributed computing with cryptographic techniques, decentralized technologies introduce a novel computing paradigm.","Blockchain ensures secure, transparent, and tamper-proof data management by validating and recording transactions via consensus across network nodes.","Federated Learning (FL), as a distributed machine learning framework, enables participants to collaboratively train models while safeguarding data privacy by avoiding direct raw data exchange.","Despite the growing interest in decentralized methods, their application in FL remains underexplored.","This paper presents a thorough investigation into Blockchain-based FL (BCFL), spotlighting the synergy between blockchain's security features and FL's privacy-preserving model training capabilities.","First, we present the taxonomy of BCFL from three aspects, including decentralized, separate networks, and reputation-based architectures.","Then, we summarize the general architecture of BCFL systems, providing a comprehensive perspective on FL architectures informed by blockchain.","Afterward, we analyze the application of BCFL in healthcare, IoT, and other privacy-sensitive areas.","Finally, we identify future research directions of BCFL."],"url":"http://arxiv.org/abs/2403.19178v1","category":"cs.CR"}
{"created":"2024-03-28 07:01:11","title":"Rethinking Information Loss in Medical Image Segmentation with Various-sized Targets","abstract":"Medical image segmentation presents the challenge of segmenting various-size targets, demanding the model to effectively capture both local and global information. Despite recent efforts using CNNs and ViTs to predict annotations of different scales, these approaches often struggle to effectively balance the detection of targets across varying sizes. Simply utilizing local information from CNNs and global relationships from ViTs without considering potential significant divergence in latent feature distributions may result in substantial information loss. To address this issue, in this paper, we will introduce a novel Stagger Network (SNet) and argues that a well-designed fusion structure can mitigate the divergence in latent feature distributions between CNNs and ViTs, thereby reducing information loss. Specifically, to emphasize both global dependencies and local focus, we design a Parallel Module to bridge the semantic gap. Meanwhile, we propose the Stagger Module, trying to fuse the selected features that are more semantically similar. An Information Recovery Module is further adopted to recover complementary information back to the network. As a key contribution, we theoretically analyze that the proposed parallel and stagger strategies would lead to less information loss, thus certifying the SNet's rationale. Experimental results clearly proved that the proposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse dataset where targets are in various sizes. Besides, it also demonstrates superiority on the ACDC and the MoNuSeg datasets where targets are with more consistent dimensions.","sentences":["Medical image segmentation presents the challenge of segmenting various-size targets, demanding the model to effectively capture both local and global information.","Despite recent efforts using CNNs and ViTs to predict annotations of different scales, these approaches often struggle to effectively balance the detection of targets across varying sizes.","Simply utilizing local information from CNNs and global relationships from ViTs without considering potential significant divergence in latent feature distributions may result in substantial information loss.","To address this issue, in this paper, we will introduce a novel Stagger Network (SNet) and argues that a well-designed fusion structure can mitigate the divergence in latent feature distributions between CNNs and ViTs, thereby reducing information loss.","Specifically, to emphasize both global dependencies and local focus, we design a Parallel Module to bridge the semantic gap.","Meanwhile, we propose the Stagger Module, trying to fuse the selected features that are more semantically similar.","An Information Recovery Module is further adopted to recover complementary information back to the network.","As a key contribution, we theoretically analyze that the proposed parallel and stagger strategies would lead to less information loss, thus certifying the SNet's rationale.","Experimental results clearly proved that the proposed SNet excels comparisons with recent SOTAs in segmenting on the Synapse dataset where targets are in various sizes.","Besides, it also demonstrates superiority on the ACDC and the MoNuSeg datasets where targets are with more consistent dimensions."],"url":"http://arxiv.org/abs/2403.19177v1","category":"cs.CV"}
{"created":"2024-03-28 06:46:45","title":"Algorithmic Ways of Seeing: Using Object Detection to Facilitate Art Exploration","abstract":"This Research through Design paper explores how object detection may be applied to a large digital art museum collection to facilitate new ways of encountering and experiencing art. We present the design and evaluation of an interactive application called SMKExplore, which allows users to explore a museum's digital collection of paintings by browsing through objects detected in the images, as a novel form of open-ended exploration. We provide three contributions. First, we show how an object detection pipeline can be integrated into a design process for visual exploration. Second, we present the design and development of an app that enables exploration of an art museum's collection. Third, we offer reflections on future possibilities for museums and HCI researchers to incorporate object detection techniques into the digitalization of museums.","sentences":["This Research through Design paper explores how object detection may be applied to a large digital art museum collection to facilitate new ways of encountering and experiencing art.","We present the design and evaluation of an interactive application called SMKExplore, which allows users to explore a museum's digital collection of paintings by browsing through objects detected in the images, as a novel form of open-ended exploration.","We provide three contributions.","First, we show how an object detection pipeline can be integrated into a design process for visual exploration.","Second, we present the design and development of an app that enables exploration of an art museum's collection.","Third, we offer reflections on future possibilities for museums and HCI researchers to incorporate object detection techniques into the digitalization of museums."],"url":"http://arxiv.org/abs/2403.19174v1","category":"cs.HC"}
{"created":"2024-03-28 06:28:35","title":"Mitigating Misleading Chain-of-Thought Reasoning with Selective Filtering","abstract":"Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains. Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT. However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models. To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain. Then, we proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly. SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tasks. Code is available at \\texttt{https://github.com/LibroWu/SelF-Reasoner}.","sentences":["Large language models have manifested remarkable capabilities by leveraging chain-of-thought (CoT) reasoning techniques to solve intricate questions through step-by-step reasoning chains.","Despite its success, the efficacy of such reasoning is inherently contingent upon the quality of CoT.","However, flawless CoT reasoning cannot be guaranteed due to the presence of indecomposable questions and the potential for erroneous reasoning chains, particularly in the case of small-scale language models.","To tackle this challenge, we propose a novel approach called the selective filtering reasoner (SelF-Reasoner) that assesses the entailment relationship between the question and the candidate reasoning chain.","Then, we proceed with CoT reasoning when the reasoning chain demonstrates confidence; otherwise, we opt to predict the answer directly.","SelF-Reasoner improves the fine-tuned T5 baseline consistently over the ScienceQA, ECQA, and LastLetter tasks.","Code is available at \\texttt{https://github.com/LibroWu/SelF-Reasoner}."],"url":"http://arxiv.org/abs/2403.19167v1","category":"cs.CL"}
{"created":"2024-03-28 06:05:14","title":"Within the Dynamic Context: Inertia-aware 3D Human Modeling with Pose Sequence","abstract":"Neural rendering techniques have significantly advanced 3D human body modeling. However, previous approaches often overlook dynamics induced by factors such as motion inertia, leading to challenges in scenarios like abrupt stops after rotation, where the pose remains static while the appearance changes. This limitation arises from reliance on a single pose as conditional input, resulting in ambiguity in mapping one pose to multiple appearances. In this study, we elucidate that variations in human appearance depend not only on the current frame's pose condition but also on past pose states. Therefore, we introduce Dyco, a novel method utilizing the delta pose sequence representation for non-rigid deformations and canonical space to effectively model temporal appearance variations. To prevent a decrease in the model's generalization ability to novel poses, we further propose low-dimensional global context to reduce unnecessary inter-body part dependencies and a quantization operation to mitigate overfitting of the delta pose sequence by the model. To validate the effectiveness of our approach, we collected a novel dataset named I3D-Human, with a focus on capturing temporal changes in clothing appearance under approximate poses. Through extensive experiments on both I3D-Human and existing datasets, our approach demonstrates superior qualitative and quantitative performance. In addition, our inertia-aware 3D human method can unprecedentedly simulate appearance changes caused by inertia at different velocities.","sentences":["Neural rendering techniques have significantly advanced 3D human body modeling.","However, previous approaches often overlook dynamics induced by factors such as motion inertia, leading to challenges in scenarios like abrupt stops after rotation, where the pose remains static while the appearance changes.","This limitation arises from reliance on a single pose as conditional input, resulting in ambiguity in mapping one pose to multiple appearances.","In this study, we elucidate that variations in human appearance depend not only on the current frame's pose condition but also on past pose states.","Therefore, we introduce Dyco, a novel method utilizing the delta pose sequence representation for non-rigid deformations and canonical space to effectively model temporal appearance variations.","To prevent a decrease in the model's generalization ability to novel poses, we further propose low-dimensional global context to reduce unnecessary inter-body part dependencies and a quantization operation to mitigate overfitting of the delta pose sequence by the model.","To validate the effectiveness of our approach, we collected a novel dataset named I3D-Human, with a focus on capturing temporal changes in clothing appearance under approximate poses.","Through extensive experiments on both I3D-Human and existing datasets, our approach demonstrates superior qualitative and quantitative performance.","In addition, our inertia-aware 3D human method can unprecedentedly simulate appearance changes caused by inertia at different velocities."],"url":"http://arxiv.org/abs/2403.19160v1","category":"cs.CV"}
{"created":"2024-03-28 05:35:22","title":"STaR-GATE: Teaching Language Models to Ask Clarifying Questions","abstract":"When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \\citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \\citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \\texttt{Questioner} -- and a \\texttt{Roleplayer} whose preferences are unknown to the \\texttt{Questioner}. By asking questions, the \\texttt{Questioner} elicits preferences from the \\texttt{Roleplayer}. The \\texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \\texttt{Oracle} with access to the \\texttt{Roleplayer}'s latent preferences. After two iterations of self-improvement, the \\texttt{Questioner} asks better questions, allowing it to generate responses that are preferred over responses from the initial model on \\highlightpink{\\textbf{72\\%}} of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.","sentences":["When prompting language models to complete a task, users often leave important aspects unsaid.","While asking questions could resolve this ambiguity \\citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions.","We explore a language model's ability to self-improve \\citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE.","We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \\texttt{Questioner} -- and a \\texttt{Roleplayer} whose preferences are unknown to the \\texttt{Questioner}.","By asking questions, the \\texttt{Questioner} elicits preferences from the \\texttt{Roleplayer}.","The \\texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \\texttt{Oracle} with access to the \\texttt{Roleplayer}'s latent preferences.","After two iterations of self-improvement, the \\texttt{Questioner} asks better questions, allowing it to generate responses that are preferred over responses from the initial model on \\highlightpink{\\textbf{72\\%}} of tasks.","Our results indicate that teaching a language model to ask better questions leads to better personalized responses."],"url":"http://arxiv.org/abs/2403.19154v1","category":"cs.CL"}
{"created":"2024-03-28 05:08:25","title":"Towards Understanding Dual BN In Hybrid Adversarial Training","abstract":"There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT). With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN and BN are used for adversarial and clean branches, respectively. A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness. In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training. This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations. We demonstrate that the domain gap between adversarial and clean samples is not very large, which is counter-intuitive considering the significant influence of adversarial perturbation on the model accuracy. We further propose a two-task hypothesis which serves as the empirical foundation and a unified framework for Hybrid-AT improvement. We also investigate Dual BN in test-time and reveal that affine parameters characterize the robustness during inference. Overall, our work sheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its underlying justification.","sentences":["There is a growing concern about applying batch normalization (BN) in adversarial training (AT), especially when the model is trained on both adversarial samples and clean samples (termed Hybrid-AT).","With the assumption that adversarial and clean samples are from two different domains, a common practice in prior works is to adopt Dual BN, where BN and BN are used for adversarial and clean branches, respectively.","A popular belief for motivating Dual BN is that estimating normalization statistics of this mixture distribution is challenging and thus disentangling it for normalization achieves stronger robustness.","In contrast to this belief, we reveal that disentangling statistics plays a less role than disentangling affine parameters in model training.","This finding aligns with prior work (Rebuffi et al., 2023), and we build upon their research for further investigations.","We demonstrate that the domain gap between adversarial and clean samples is not very large, which is counter-intuitive considering the significant influence of adversarial perturbation on the model accuracy.","We further propose a two-task hypothesis which serves as the empirical foundation and a unified framework for Hybrid-AT improvement.","We also investigate Dual BN in test-time and reveal that affine parameters characterize the robustness during inference.","Overall, our work sheds new light on understanding the mechanism of Dual BN in Hybrid-AT and its underlying justification."],"url":"http://arxiv.org/abs/2403.19150v1","category":"cs.LG"}
{"created":"2024-03-28 05:07:41","title":"Topological Cycle Graph Attention Network for Brain Functional Connectivity","abstract":"This study, we introduce a novel Topological Cycle Graph Attention Network (CycGAT), designed to delineate a functional backbone within brain functional graph--key pathways essential for signal transmissio--from non-essential, redundant connections that form cycles around this core structure. We first introduce a cycle incidence matrix that establishes an independent cycle basis within a graph, mapping its relationship with edges. We propose a cycle graph convolution that leverages a cycle adjacency matrix, derived from the cycle incidence matrix, to specifically filter edge signals in a domain of cycles. Additionally, we strengthen the representation power of the cycle graph convolution by adding an attention mechanism, which is further augmented by the introduction of edge positional encodings in cycles, to enhance the topological awareness of CycGAT. We demonstrate CycGAT's localization through simulation and its efficacy on an ABCD study's fMRI data (n=8765), comparing it with baseline models. CycGAT outperforms these models, identifying a functional backbone with significantly fewer cycles, crucial for understanding neural circuits related to general intelligence. Our code will be released once accepted.","sentences":["This study, we introduce a novel Topological Cycle Graph Attention Network (CycGAT), designed to delineate a functional backbone within brain functional graph--key pathways essential for signal transmissio--from non-essential, redundant connections that form cycles around this core structure.","We first introduce a cycle incidence matrix that establishes an independent cycle basis within a graph, mapping its relationship with edges.","We propose a cycle graph convolution that leverages a cycle adjacency matrix, derived from the cycle incidence matrix, to specifically filter edge signals in a domain of cycles.","Additionally, we strengthen the representation power of the cycle graph convolution by adding an attention mechanism, which is further augmented by the introduction of edge positional encodings in cycles, to enhance the topological awareness of CycGAT.","We demonstrate CycGAT's localization through simulation and its efficacy on an ABCD study's fMRI data (n=8765), comparing it with baseline models.","CycGAT outperforms these models, identifying a functional backbone with significantly fewer cycles, crucial for understanding neural circuits related to general intelligence.","Our code will be released once accepted."],"url":"http://arxiv.org/abs/2403.19149v1","category":"cs.LG"}
{"created":"2024-03-28 04:57:13","title":"GenAI Detection Tools, Adversarial Techniques and Implications for Inclusivity in Higher Education","abstract":"This study investigates the efficacy of six major Generative AI (GenAI) text detectors when confronted with machine-generated content that has been modified using techniques designed to evade detection by these tools (n=805). The results demonstrate that the detectors' already low accuracy rates (39.5%) show major reductions in accuracy (17.4%) when faced with manipulated content, with some techniques proving more effective than others in evading detection.   The accuracy limitations and the potential for false accusations demonstrate that these tools cannot currently be recommended for determining whether violations of academic integrity have occurred, underscoring the challenges educators face in maintaining inclusive and fair assessment practices. However, they may have a role in supporting student learning and maintaining academic integrity when used in a non-punitive manner.   These results underscore the need for a combined approach to addressing the challenges posed by GenAI in academia to promote the responsible and equitable use of these emerging technologies. The study concludes that the current limitations of AI text detectors require a critical approach for any possible implementation in HE and highlight possible alternatives to AI assessment strategies.","sentences":["This study investigates the efficacy of six major Generative AI (GenAI) text detectors when confronted with machine-generated content that has been modified using techniques designed to evade detection by these tools (n=805).","The results demonstrate that the detectors' already low accuracy rates (39.5%) show major reductions in accuracy (17.4%) when faced with manipulated content, with some techniques proving more effective than others in evading detection.   ","The accuracy limitations and the potential for false accusations demonstrate that these tools cannot currently be recommended for determining whether violations of academic integrity have occurred, underscoring the challenges educators face in maintaining inclusive and fair assessment practices.","However, they may have a role in supporting student learning and maintaining academic integrity when used in a non-punitive manner.   ","These results underscore the need for a combined approach to addressing the challenges posed by GenAI in academia to promote the responsible and equitable use of these emerging technologies.","The study concludes that the current limitations of AI text detectors require a critical approach for any possible implementation in HE and highlight possible alternatives to AI assessment strategies."],"url":"http://arxiv.org/abs/2403.19148v1","category":"cs.CY"}
{"created":"2024-03-28 04:24:56","title":"QNCD: Quantization Noise Correction for Diffusion Models","abstract":"Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity. However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process. Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings. Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process. We identify two primary quantization challenges: intra and inter quantization noise. Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step. Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions step-by-step. QNCD combats these through embedding-derived feature smoothing for eliminating intra quantization noise and an effective runtime noise estimatiation module for dynamicly filtering inter quantization noise. Extensive experiments demonstrate that our method outperforms previous quantization methods for diffusion models, achieving lossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4). Code is available at: https://github.com/huanpengchu/QNCD","sentences":["Diffusion models have revolutionized image synthesis, setting new benchmarks in quality and creativity.","However, their widespread adoption is hindered by the intensive computation required during the iterative denoising process.","Post-training quantization (PTQ) presents a solution to accelerate sampling, aibeit at the expense of sample quality, extremely in low-bit settings.","Addressing this, our study introduces a unified Quantization Noise Correction Scheme (QNCD), aimed at minishing quantization noise throughout the sampling process.","We identify two primary quantization challenges: intra and inter quantization noise.","Intra quantization noise, mainly exacerbated by embeddings in the resblock module, extends activation quantization ranges, increasing disturbances in each single denosing step.","Besides, inter quantization noise stems from cumulative quantization deviations across the entire denoising process, altering data distributions step-by-step.","QNCD combats these through embedding-derived feature smoothing for eliminating intra quantization noise and an effective runtime noise estimatiation module for dynamicly filtering inter quantization noise.","Extensive experiments demonstrate that our method outperforms previous quantization methods for diffusion models, achieving lossless results in W4A8 and W8A8 quantization settings on ImageNet (LDM-4).","Code is available at: https://github.com/huanpengchu/QNCD"],"url":"http://arxiv.org/abs/2403.19140v1","category":"cs.CV"}
{"created":"2024-03-28 04:12:13","title":"Compressing Large Language Models by Streamlining the Unimportant Layer","abstract":"Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models. Consequently, there is an increasing emphasis on compact models that exhibit high performance. In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers. Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning. In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight models and ultimately demonstrate that a single MLP can effectively fit the pruned layers. Comprehensive experiments show that our proposed method, LLM-Streamline, outperforms previous state-of-the-art (SOTA) model pruning methods.","sentences":["Large language models (LLM) have been extensively applied in various natural language tasks and domains, but their applicability is constrained by the large number of parameters of the models.","Consequently, there is an increasing emphasis on compact models that exhibit high performance.","In this study, we observe that different layers in LLM have varying degrees of perturbation on the hidden states, which allows us to identify less important layers.","Based on this phenomenon, we propose LLM-Streamline, which consists of two parts: layer pruning, where we remove a set of consecutive layers with the lowest importance in the model according to the target sparsity; and layer replacement, where we train a lightweight model to substitute the pruned layers, thereby mitigating the performance degradation caused by pruning.","In our experiments, we utilize structures such as a multi-layer perceptron (MLP) and a transformer layer as lightweight models and ultimately demonstrate that a single MLP can effectively fit the pruned layers.","Comprehensive experiments show that our proposed method, LLM-Streamline, outperforms previous state-of-the-art (SOTA) model pruning methods."],"url":"http://arxiv.org/abs/2403.19135v1","category":"cs.CL"}
{"created":"2024-03-28 03:14:18","title":"MFORT-QA: Multi-hop Few-shot Open Rich Table Question Answering","abstract":"In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis. These metrics are frequently hidden away in tables and/or their nested hyperlinks. To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information. However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s). Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts. In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps. The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question. The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT. To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner. Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions. These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM. Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods.","sentences":["In today's fast-paced industry, professionals face the challenge of summarizing a large number of documents and extracting vital information from them on a daily basis.","These metrics are frequently hidden away in tables and/or their nested hyperlinks.","To address this challenge, the approach of Table Question Answering (QA) has been developed to extract the relevant information.","However, traditional Table QA training tasks that provide a table and an answer(s) from a gold cell coordinate(s) for a question may not always ensure extracting the accurate answer(s).","Recent advancements in Large Language Models (LLMs) have opened up new possibilities for extracting information from tabular data using prompts.","In this paper, we introduce the Multi-hop Few-shot Open Rich Table QA (MFORT-QA) approach, which consists of two major steps.","The first step involves Few-Shot Learning (FSL), where relevant tables and associated contexts of hyperlinks are retrieved based on a given question.","The retrieved content is then used to construct few-shot prompts as inputs to an LLM, such as ChatGPT.","To tackle the challenge of answering complex questions, the second step leverages Chain-of-thought (CoT) prompting to decompose the complex question into a sequential chain of questions and reasoning thoughts in a multi-hop manner.","Retrieval-Augmented Generation (RAG) enhances this process by retrieving relevant tables and contexts of hyperlinks that are relevant to the resulting reasoning thoughts and questions.","These additional contexts are then used to supplement the prompt used in the first step, resulting in more accurate answers from an LLM.","Empirical results from OTT-QA demonstrate that our abstractive QA approach significantly improves the accuracy of extractive Table QA methods."],"url":"http://arxiv.org/abs/2403.19116v1","category":"cs.CL"}
{"created":"2024-03-28 03:09:42","title":"FACTOID: FACtual enTailment fOr hallucInation Detection","abstract":"The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits. However, hallucination is a significant concern. In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information. RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents. This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs. For instance, consider a prompt about the 'USA's stance on the Ukraine war''. The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...'' However, during the war the U.S. president is Joe Biden which contradicts factual reality. Moreover, current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted. To address this, we introduces a new type of TE called ``Factual Entailment (FE).'', aims to detect factual inaccuracies in content generated by LLMs while also highlighting the specific text segment that contradicts reality. We present FACTOID (FACTual enTAILment for hallucInation Detection), a benchmark dataset for FE. We propose a multi-task learning (MTL) framework for FE, incorporating state-of-the-art (SoTA) long text embeddings such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer. The proposed MTL architecture for FE achieves an avg. 40\\% improvement in accuracy on the FACTOID benchmark compared to SoTA TE methods. As FE automatically detects hallucinations, we assessed 15 modern LLMs and ranked them using our proposed Auto Hallucination Vulnerability Index (HVI_auto). This index quantifies and offers a comparative scale to evaluate and rank LLMs according to their hallucinations.","sentences":["The widespread adoption of Large Language Models (LLMs) has facilitated numerous benefits.","However, hallucination is a significant concern.","In response, Retrieval Augmented Generation (RAG) has emerged as a highly promising paradigm to improve LLM outputs by grounding them in factual information.","RAG relies on textual entailment (TE) or similar methods to check if the text produced by LLMs is supported or contradicted, compared to retrieved documents.","This paper argues that conventional TE methods are inadequate for spotting hallucinations in content generated by LLMs.","For instance, consider a prompt about the 'USA's stance on the Ukraine war''.","The AI-generated text states, ...U.S. President Barack Obama says the U.S. will not put troops in Ukraine...''","However, during the war the U.S. president is Joe Biden which contradicts factual reality.","Moreover, current TE systems are unable to accurately annotate the given text and identify the exact portion that is contradicted.","To address this, we introduces a new type of TE called ``Factual Entailment (FE).''",", aims to detect factual inaccuracies in content generated by LLMs while also highlighting the specific text segment that contradicts reality.","We present FACTOID (FACTual enTAILment for hallucInation Detection), a benchmark dataset for FE.","We propose a multi-task learning (MTL) framework for FE, incorporating state-of-the-art (SoTA) long text embeddings such as e5-mistral-7b-instruct, along with GPT-3, SpanBERT, and RoFormer.","The proposed MTL architecture for FE achieves an avg.","40\\% improvement in accuracy on the FACTOID benchmark compared to SoTA TE methods.","As FE automatically detects hallucinations, we assessed 15 modern LLMs and ranked them using our proposed Auto Hallucination Vulnerability Index (HVI_auto).","This index quantifies and offers a comparative scale to evaluate and rank LLMs according to their hallucinations."],"url":"http://arxiv.org/abs/2403.19113v1","category":"cs.CL"}
{"created":"2024-03-28 03:07:16","title":"Patch Spatio-Temporal Relation Prediction for Video Anomaly Detection","abstract":"Video Anomaly Detection (VAD), aiming to identify abnormalities within a specific context and timeframe, is crucial for intelligent Video Surveillance Systems. While recent deep learning-based VAD models have shown promising results by generating high-resolution frames, they often lack competence in preserving detailed spatial and temporal coherence in video frames. To tackle this issue, we propose a self-supervised learning approach for VAD through an inter-patch relationship prediction task. Specifically, we introduce a two-branch vision transformer network designed to capture deep visual features of video frames, addressing spatial and temporal dimensions responsible for modeling appearance and motion patterns, respectively. The inter-patch relationship in each dimension is decoupled into inter-patch similarity and the order information of each patch. To mitigate memory consumption, we convert the order information prediction task into a multi-label learning problem, and the inter-patch similarity prediction task into a distance matrix regression problem. Comprehensive experiments demonstrate the effectiveness of our method, surpassing pixel-generation-based methods by a significant margin across three public benchmarks. Additionally, our approach outperforms other self-supervised learning-based methods.","sentences":["Video Anomaly Detection (VAD), aiming to identify abnormalities within a specific context and timeframe, is crucial for intelligent Video Surveillance Systems.","While recent deep learning-based VAD models have shown promising results by generating high-resolution frames, they often lack competence in preserving detailed spatial and temporal coherence in video frames.","To tackle this issue, we propose a self-supervised learning approach for VAD through an inter-patch relationship prediction task.","Specifically, we introduce a two-branch vision transformer network designed to capture deep visual features of video frames, addressing spatial and temporal dimensions responsible for modeling appearance and motion patterns, respectively.","The inter-patch relationship in each dimension is decoupled into inter-patch similarity and the order information of each patch.","To mitigate memory consumption, we convert the order information prediction task into a multi-label learning problem, and the inter-patch similarity prediction task into a distance matrix regression problem.","Comprehensive experiments demonstrate the effectiveness of our method, surpassing pixel-generation-based methods by a significant margin across three public benchmarks.","Additionally, our approach outperforms other self-supervised learning-based methods."],"url":"http://arxiv.org/abs/2403.19111v1","category":"cs.CV"}
{"created":"2024-03-28 02:35:53","title":"Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation","abstract":"Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney.","sentences":["Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts.","This challenge has spurred the development of algorithms for automated prompt generation.","However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts.","In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models.","Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images.","Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, styles and images across multiple T2I models, including Stable Diffusion, DALL-E, and Midjourney."],"url":"http://arxiv.org/abs/2403.19103v1","category":"cs.CV"}
{"created":"2024-03-28 02:31:06","title":"AAPMT: AGI Assessment Through Prompt and Metric Transformer","abstract":"The emergence of text-to-image models marks a significant milestone in the evolution of AI-generated images (AGIs), expanding their use in diverse domains like design, entertainment, and more. Despite these breakthroughs, the quality of AGIs often remains suboptimal, highlighting the need for effective evaluation methods. These methods are crucial for assessing the quality of images relative to their textual descriptions, and they must accurately mirror human perception. Substantial progress has been achieved in this domain, with innovative techniques such as BLIP and DBCNN contributing significantly. However, recent studies, including AGIQA-3K, reveal a notable discrepancy between current methods and state-of-the-art (SOTA) standards. This gap emphasizes the necessity for a more sophisticated and precise evaluation metric. In response, our objective is to develop a model that could give ratings for metrics, which focuses on parameters like perceptual quality, authenticity, and the correspondence between text and image, that more closely aligns with human perception. In our paper, we introduce a range of effective methods, including prompt designs and the Metric Transformer. The Metric Transformer is a novel structure inspired by the complex interrelationships among various AGI quality metrics. The code is available at https://github.com/huskydoge/CS3324-Digital-Image-Processing/tree/main/Assignment1","sentences":["The emergence of text-to-image models marks a significant milestone in the evolution of AI-generated images (AGIs), expanding their use in diverse domains like design, entertainment, and more.","Despite these breakthroughs, the quality of AGIs often remains suboptimal, highlighting the need for effective evaluation methods.","These methods are crucial for assessing the quality of images relative to their textual descriptions, and they must accurately mirror human perception.","Substantial progress has been achieved in this domain, with innovative techniques such as BLIP and DBCNN contributing significantly.","However, recent studies, including AGIQA-3K, reveal a notable discrepancy between current methods and state-of-the-art (SOTA) standards.","This gap emphasizes the necessity for a more sophisticated and precise evaluation metric.","In response, our objective is to develop a model that could give ratings for metrics, which focuses on parameters like perceptual quality, authenticity, and the correspondence between text and image, that more closely aligns with human perception.","In our paper, we introduce a range of effective methods, including prompt designs and the Metric Transformer.","The Metric Transformer is a novel structure inspired by the complex interrelationships among various AGI quality metrics.","The code is available at https://github.com/huskydoge/CS3324-Digital-Image-Processing/tree/main/Assignment1"],"url":"http://arxiv.org/abs/2403.19101v1","category":"cs.CV"}
{"created":"2024-03-28 02:02:00","title":"Task2Morph: Differentiable Task-inspired Framework for Contact-Aware Robot Design","abstract":"Optimizing the morphologies and the controllers that adapt to various tasks is a critical issue in the field of robot design, aka. embodied intelligence. Previous works typically model it as a joint optimization problem and use search-based methods to find the optimal solution in the morphology space. However, they ignore the implicit knowledge of task-to-morphology mapping which can directly inspire robot design. For example, flipping heavier boxes tends to require more muscular robot arms. This paper proposes a novel and general differentiable task-inspired framework for contact-aware robot design called Task2Morph. We abstract task features highly related to task performance and use them to build a task-to-morphology mapping. Further, we embed the mapping into a differentiable robot design process, where the gradient information is leveraged for both the mapping learning and the whole optimization. The experiments are conducted on three scenarios, and the results validate that Task2Morph outperforms DiffHand, which lacks a task-inspired morphology module, in terms of efficiency and effectiveness.","sentences":["Optimizing the morphologies and the controllers that adapt to various tasks is a critical issue in the field of robot design, aka.","embodied intelligence.","Previous works typically model it as a joint optimization problem and use search-based methods to find the optimal solution in the morphology space.","However, they ignore the implicit knowledge of task-to-morphology mapping which can directly inspire robot design.","For example, flipping heavier boxes tends to require more muscular robot arms.","This paper proposes a novel and general differentiable task-inspired framework for contact-aware robot design called Task2Morph.","We abstract task features highly related to task performance and use them to build a task-to-morphology mapping.","Further, we embed the mapping into a differentiable robot design process, where the gradient information is leveraged for both the mapping learning and the whole optimization.","The experiments are conducted on three scenarios, and the results validate that Task2Morph outperforms DiffHand, which lacks a task-inspired morphology module, in terms of efficiency and effectiveness."],"url":"http://arxiv.org/abs/2403.19093v1","category":"cs.RO"}
{"created":"2024-03-28 01:58:53","title":"Observation of the semileptonic decays $D^0\\rightarrow K_S^0\u03c0^-\u03c0^0 e^+ \u03bd_e$ and $D^+\\rightarrow K_S^0\u03c0^+\u03c0^- e^+ \u03bd_e$","abstract":"By analyzing $e^+e^-$ annihilation data corresponding to an integrated luminosity of 2.93 $\\rm fb^{-1}$ collected at a center-of-mass energy of 3.773 GeV with the \\text{BESIII} detector, the first observation of the semileptonic decays $D^0\\rightarrow K_S^0\\pi^-\\pi^0 e^+ \\nu_e$ and $D^+\\rightarrow K_S^0\\pi^+\\pi^- e^+ \\nu_e$ is reported. With a dominant hadronic contribution from $K_1(1270)$, the branching fractions are measured to be $\\mathcal{B}(D^0\\rightarrow {K}_1(1270)^-(\\to K^0_S\\pi^-\\pi^0)e^+\\nu_e)=(1.69^{+0.53}_{-0.46}\\pm0.15)\\times10^{-4}$ and $\\mathcal{B}(D^+\\to \\bar{K}_1(1270)^0(\\to K^0_S\\pi^+\\pi^-)e^+\\nu_e)=(1.47^{+0.45}_{-0.40}\\pm0.20)\\times10^{-4}$ with statistical significance of 5.4$\\sigma$ and 5.6$\\sigma$, respectively. When combined with measurements of the $K_1(1270)\\to K^+\\pi^-\\pi$ decays, the absolute branching fractions are determined to be $\\mathcal{B}(D^0\\to K_1(1270)^-e^+\\nu_e)=(1.05^{+0.33}_{-0.28}\\pm0.12\\pm0.12)\\times10^{-3}$ and $\\mathcal{B}(D^+\\to \\bar{K}_1(1270)^0e^+\\nu_e)=(1.29^{+0.40}_{-0.35}\\pm0.18\\pm0.15)\\times10^{-3}$. The first and second uncertainties are statistical and systematic, respectively, and the third uncertainties originate from the assumed branching fractions of the $K_1(1270)\\to K\\pi\\pi$ decays.","sentences":["By analyzing $e^+e^-$ annihilation data corresponding to an integrated luminosity of 2.93 $\\rm fb^{-1}$ collected at a center-of-mass energy of 3.773 GeV with the \\text{BESIII} detector, the first observation of the semileptonic decays $D^0\\rightarrow K_S^0\\pi^-\\pi^0 e^+","\\nu_e$ and $D^+\\rightarrow K_S^0\\pi^+\\pi^-","e^+","\\nu_e$ is reported.","With a dominant hadronic contribution from $K_1(1270)$, the branching fractions are measured to be $\\mathcal{B}(D^0\\rightarrow {K}_1(1270)^-(\\to K^0_S\\pi^-\\pi^0)e^+\\nu_e)=(1.69^{+0.53}_{-0.46}\\pm0.15)\\times10^{-4}$ and $\\mathcal{B}(D^+\\to \\bar{K}_1(1270)^0(\\to K^0_S\\pi^+\\pi^-)e^+\\nu_e)=(1.47^{+0.45}_{-0.40}\\pm0.20)\\times10^{-4}$ with statistical significance of 5.4$\\sigma$ and 5.6$\\sigma$, respectively.","When combined with measurements of the $K_1(1270)\\to K^+\\pi^-\\pi$ decays, the absolute branching fractions are determined to be $\\mathcal{B}(D^0\\to K_1(1270)^-e^+\\nu_e)=(1.05^{+0.33}_{-0.28}\\pm0.12\\pm0.12)\\times10^{-3}$ and $\\mathcal{B}(D^+\\to \\bar{K}_1(1270)^0e^+\\nu_e)=(1.29^{+0.40}_{-0.35}\\pm0.18\\pm0.15)\\times10^{-3}$. The first and second uncertainties are statistical and systematic, respectively, and the third uncertainties originate from the assumed branching fractions of the $K_1(1270)\\to K\\pi\\pi$ decays."],"url":"http://arxiv.org/abs/2403.19091v1","category":"hep-ex"}
{"created":"2024-03-28 01:41:31","title":"Real-time accident detection and physiological signal monitoring to enhance motorbike safety and emergency response","abstract":"Rapid urbanization and improved living standards have led to a substantial increase in the number of vehicles on the road, consequently resulting in a rise in the frequency of accidents. Among these accidents, motorbike accidents pose a particularly high risk, often resulting in serious injuries or deaths. A significant number of these fatalities occur due to delayed or inadequate medical attention. To this end, we propose a novel automatic detection and notification system specifically designed for motorbike accidents. The proposed system comprises two key components: a detection system and a physiological signal monitoring system. The detection system is integrated into the helmet and consists of a microcontroller, accelerometer, GPS, GSM, and Wi-Fi modules. The physio-monitoring system incorporates a sensor for monitoring pulse rate and SpO$_{2}$ saturation. All collected data are presented on an LCD display and wirelessly transmitted to the detection system through the microcontroller of the physiological signal monitoring system. If the accelerometer readings consistently deviate from the specified threshold decided through extensive experimentation, the system identifies the event as an accident and transmits the victim's information -- including the GPS location, pulse rate, and SpO$_{2}$ saturation rate -- to the designated emergency contacts. Preliminary results demonstrate the efficacy of the proposed system in accurately detecting motorbike accidents and promptly alerting emergency contacts. We firmly believe that the proposed system has the potential to significantly mitigate the risks associated with motorbike accidents and save lives.","sentences":["Rapid urbanization and improved living standards have led to a substantial increase in the number of vehicles on the road, consequently resulting in a rise in the frequency of accidents.","Among these accidents, motorbike accidents pose a particularly high risk, often resulting in serious injuries or deaths.","A significant number of these fatalities occur due to delayed or inadequate medical attention.","To this end, we propose a novel automatic detection and notification system specifically designed for motorbike accidents.","The proposed system comprises two key components: a detection system and a physiological signal monitoring system.","The detection system is integrated into the helmet and consists of a microcontroller, accelerometer, GPS, GSM, and Wi-Fi modules.","The physio-monitoring system incorporates a sensor for monitoring pulse rate and SpO$_{2}$ saturation.","All collected data are presented on an LCD display and wirelessly transmitted to the detection system through the microcontroller of the physiological signal monitoring system.","If the accelerometer readings consistently deviate from the specified threshold decided through extensive experimentation, the system identifies the event as an accident and transmits the victim's information -- including the GPS location, pulse rate, and SpO$_{2}$ saturation rate -- to the designated emergency contacts.","Preliminary results demonstrate the efficacy of the proposed system in accurately detecting motorbike accidents and promptly alerting emergency contacts.","We firmly believe that the proposed system has the potential to significantly mitigate the risks associated with motorbike accidents and save lives."],"url":"http://arxiv.org/abs/2403.19085v1","category":"eess.SY"}
{"created":"2024-03-28 01:27:10","title":"Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep Learning: A Bayesian Deep Learning Approach","abstract":"With recent advancements in the development of artificial intelligence applications using theories and algorithms in machine learning, many accurate models can be created to train and predict on given datasets. With the realization of the importance of imaging interpretation in cancer diagnosis, this article aims to investigate the theory behind Deep Learning and Bayesian Network prediction models. Based on the advantages and drawbacks of each model, different approaches will be used to construct a Bayesian Deep Learning Model, combining the strengths while minimizing the weaknesses. Finally, the applications and accuracy of the resulting Bayesian Deep Learning approach in the health industry in classifying images will be analyzed.","sentences":["With recent advancements in the development of artificial intelligence applications using theories and algorithms in machine learning, many accurate models can be created to train and predict on given datasets.","With the realization of the importance of imaging interpretation in cancer diagnosis, this article aims to investigate the theory behind Deep Learning and Bayesian Network prediction models.","Based on the advantages and drawbacks of each model, different approaches will be used to construct a Bayesian Deep Learning Model, combining the strengths while minimizing the weaknesses.","Finally, the applications and accuracy of the resulting Bayesian Deep Learning approach in the health industry in classifying images will be analyzed."],"url":"http://arxiv.org/abs/2403.19083v1","category":"cs.LG"}
{"created":"2024-03-28 01:14:25","title":"Enhancing Conformal Prediction Using E-Test Statistics","abstract":"Conformal Prediction (CP) serves as a robust framework that quantifies uncertainty in predictions made by Machine Learning (ML) models. Unlike traditional point predictors, CP generates statistically valid prediction regions, also known as prediction intervals, based on the assumption of data exchangeability. Typically, the construction of conformal predictions hinges on p-values. This paper, however, ventures down an alternative path, harnessing the power of e-test statistics to augment the efficacy of conformal predictions by introducing a BB-predictor (bounded from the below predictor).","sentences":["Conformal Prediction (CP) serves as a robust framework that quantifies uncertainty in predictions made by Machine Learning (ML) models.","Unlike traditional point predictors, CP generates statistically valid prediction regions, also known as prediction intervals, based on the assumption of data exchangeability.","Typically, the construction of conformal predictions hinges on p-values.","This paper, however, ventures down an alternative path, harnessing the power of e-test statistics to augment the efficacy of conformal predictions by introducing a BB-predictor (bounded from the below predictor)."],"url":"http://arxiv.org/abs/2403.19082v1","category":"cs.LG"}
{"created":"2024-03-28 00:50:02","title":"MVEB: Self-Supervised Learning with Multi-View Entropy Bottleneck","abstract":"Self-supervised learning aims to learn representation that can be effectively generalized to downstream tasks. Many self-supervised approaches regard two views of an image as both the input and the self-supervised signals, assuming that either view contains the same task-relevant information and the shared information is (approximately) sufficient for predicting downstream tasks. Recent studies show that discarding superfluous information not shared between the views can improve generalization. Hence, the ideal representation is sufficient for downstream tasks and contains minimal superfluous information, termed minimal sufficient representation. One can learn this representation by maximizing the mutual information between the representation and the supervised view while eliminating superfluous information. Nevertheless, the computation of mutual information is notoriously intractable. In this work, we propose an objective termed multi-view entropy bottleneck (MVEB) to learn minimal sufficient representation effectively. MVEB simplifies the minimal sufficient learning to maximizing both the agreement between the embeddings of two views and the differential entropy of the embedding distribution. Our experiments confirm that MVEB significantly improves performance. For example, it achieves top-1 accuracy of 76.9\\% on ImageNet with a vanilla ResNet-50 backbone on linear evaluation. To the best of our knowledge, this is the new state-of-the-art result with ResNet-50.","sentences":["Self-supervised learning aims to learn representation that can be effectively generalized to downstream tasks.","Many self-supervised approaches regard two views of an image as both the input and the self-supervised signals, assuming that either view contains the same task-relevant information and the shared information is (approximately) sufficient for predicting downstream tasks.","Recent studies show that discarding superfluous information not shared between the views can improve generalization.","Hence, the ideal representation is sufficient for downstream tasks and contains minimal superfluous information, termed minimal sufficient representation.","One can learn this representation by maximizing the mutual information between the representation and the supervised view while eliminating superfluous information.","Nevertheless, the computation of mutual information is notoriously intractable.","In this work, we propose an objective termed multi-view entropy bottleneck (MVEB) to learn minimal sufficient representation effectively.","MVEB simplifies the minimal sufficient learning to maximizing both the agreement between the embeddings of two views and the differential entropy of the embedding distribution.","Our experiments confirm that MVEB significantly improves performance.","For example, it achieves top-1 accuracy of 76.9\\% on ImageNet with a vanilla ResNet-50 backbone on linear evaluation.","To the best of our knowledge, this is the new state-of-the-art result with ResNet-50."],"url":"http://arxiv.org/abs/2403.19078v1","category":"cs.CV"}
{"created":"2024-03-28 00:34:56","title":"Tiny Machine Learning: Progress and Futures","abstract":"Tiny Machine Learning (TinyML) is a new frontier of machine learning. By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence. However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms. There is also limited compiler and inference engine support for bare-metal devices. Therefore, we need to co-design the algorithm and system stack to enable TinyML. In this review, we will first discuss the definition, challenges, and applications of TinyML. We then survey the recent progress in TinyML and deep learning on MCUs. Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design. We will further extend the solution from inference to training and introduce tiny on-device training techniques. Finally, we present future directions in this area. Today's large model might be tomorrow's tiny model. The scope of TinyML should evolve and adapt over time.","sentences":["Tiny Machine Learning (TinyML) is a new frontier of machine learning.","By squeezing deep learning models into billions of IoT devices and microcontrollers (MCUs), we expand the scope of AI applications and enable ubiquitous intelligence.","However, TinyML is challenging due to hardware constraints: the tiny memory resource makes it difficult to hold deep learning models designed for cloud and mobile platforms.","There is also limited compiler and inference engine support for bare-metal devices.","Therefore, we need to co-design the algorithm and system stack to enable TinyML.","In this review, we will first discuss the definition, challenges, and applications of TinyML.","We then survey the recent progress in TinyML and deep learning on MCUs.","Next, we will introduce MCUNet, showing how we can achieve ImageNet-scale AI applications on IoT devices with system-algorithm co-design.","We will further extend the solution from inference to training and introduce tiny on-device training techniques.","Finally, we present future directions in this area.","Today's large model might be tomorrow's tiny model.","The scope of TinyML should evolve and adapt over time."],"url":"http://arxiv.org/abs/2403.19076v1","category":"cs.LG"}
{"created":"2024-03-28 00:29:15","title":"Dataflow-Aware PIM-Enabled Manycore Architecture for Deep Learning Workloads","abstract":"Processing-in-memory (PIM) has emerged as an enabler for the energy-efficient and high-performance acceleration of deep learning (DL) workloads. Resistive random-access memory (ReRAM) is one of the most promising technologies to implement PIM. However, as the complexity of Deep convolutional neural networks (DNNs) grows, we need to design a manycore architecture with multiple ReRAM-based processing elements (PEs) on a single chip. Existing PIM-based architectures mostly focus on computation while ignoring the role of communication. ReRAM-based tiled manycore architectures often involve many Processing Elements (PEs), which need to be interconnected via an efficient on-chip communication infrastructure. Simply allocating more resources (ReRAMs) to speed up only computation is ineffective if the communication infrastructure cannot keep up with it. In this paper, we highlight the design principles of a dataflow-aware PIM-enabled manycore platform tailor-made for various types of DL workloads. We consider the design challenges with both 2.5D interposer- and 3D integration-enabled architectures.","sentences":["Processing-in-memory (PIM) has emerged as an enabler for the energy-efficient and high-performance acceleration of deep learning (DL) workloads.","Resistive random-access memory (ReRAM) is one of the most promising technologies to implement PIM.","However, as the complexity of Deep convolutional neural networks (DNNs) grows, we need to design a manycore architecture with multiple ReRAM-based processing elements (PEs) on a single chip.","Existing PIM-based architectures mostly focus on computation while ignoring the role of communication.","ReRAM-based tiled manycore architectures often involve many Processing Elements (PEs), which need to be interconnected via an efficient on-chip communication infrastructure.","Simply allocating more resources (ReRAMs) to speed up only computation is ineffective if the communication infrastructure cannot keep up with it.","In this paper, we highlight the design principles of a dataflow-aware PIM-enabled manycore platform tailor-made for various types of DL workloads.","We consider the design challenges with both 2.5D interposer- and 3D integration-enabled architectures."],"url":"http://arxiv.org/abs/2403.19073v1","category":"cs.AR"}
{"created":"2024-03-28 00:11:12","title":"Generative Quanta Color Imaging","abstract":"The astonishing development of single-photon cameras has created an unprecedented opportunity for scientific and industrial imaging. However, the high data throughput generated by these 1-bit sensors creates a significant bottleneck for low-power applications. In this paper, we explore the possibility of generating a color image from a single binary frame of a single-photon camera. We evidently find this problem being particularly difficult to standard colorization approaches due to the substantial degree of exposure variation. The core innovation of our paper is an exposure synthesis model framed under a neural ordinary differential equation (Neural ODE) that allows us to generate a continuum of exposures from a single observation. This innovation ensures consistent exposure in binary images that colorizers take on, resulting in notably enhanced colorization. We demonstrate applications of the method in single-image and burst colorization and show superior generative performance over baselines. Project website can be found at https://vishal-s-p.github.io/projects/2023/generative_quanta_color.html.","sentences":["The astonishing development of single-photon cameras has created an unprecedented opportunity for scientific and industrial imaging.","However, the high data throughput generated by these 1-bit sensors creates a significant bottleneck for low-power applications.","In this paper, we explore the possibility of generating a color image from a single binary frame of a single-photon camera.","We evidently find this problem being particularly difficult to standard colorization approaches due to the substantial degree of exposure variation.","The core innovation of our paper is an exposure synthesis model framed under a neural ordinary differential equation (Neural ODE) that allows us to generate a continuum of exposures from a single observation.","This innovation ensures consistent exposure in binary images that colorizers take on, resulting in notably enhanced colorization.","We demonstrate applications of the method in single-image and burst colorization and show superior generative performance over baselines.","Project website can be found at https://vishal-s-p.github.io/projects/2023/generative_quanta_color.html."],"url":"http://arxiv.org/abs/2403.19066v1","category":"cs.CV"}
{"created":"2024-03-28 00:04:57","title":"Homemade Algebraic Geometry. Celebrating Enrique Arrondo's 60th birthday","abstract":"In this survey we recognize Enrique Arrondo's contributions over the whole of its career, recalling his professional history and collecting the results of his mathematical production.","sentences":["In this survey we recognize Enrique Arrondo's contributions over the whole of its career, recalling his professional history and collecting the results of his mathematical production."],"url":"http://arxiv.org/abs/2403.19064v1","category":"math.AG"}
{"created":"2024-03-27 23:55:02","title":"Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers","abstract":"In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover\" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive and collaborative human-robot workforce.","sentences":["In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows.","This paper introduces a human-centered approach with a ``work companion rover\" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature.","We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework.","Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive and collaborative human-robot workforce."],"url":"http://arxiv.org/abs/2403.19060v1","category":"cs.RO"}
{"created":"2024-03-27 23:45:31","title":"CAUSE: Counterfactual Assessment of User Satisfaction Estimation in Task-Oriented Dialogue Systems","abstract":"An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current benchmarks for user satisfaction estimation in TOD systems are highly skewed towards dialogues for which the user is satisfied. The effect of having a more balanced set of satisfaction labels on performance is unknown. However, balancing the data with more dissatisfactory dialogue samples requires further data collection and human annotation, which is costly and time-consuming. In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection. We gather human annotations to ensure the reliability of the generated samples. We evaluate two open-source LLMs as user satisfaction estimators on our augmented collection against state-of-the-art fine-tuned models. Our experiments show that when used as few-shot user satisfaction estimators, open-source LLMs show higher robustness to the increase in the number of dissatisfaction labels in the test collection than the fine-tuned state-of-the-art models. Our results shed light on the need for data augmentation approaches for user satisfaction estimation in TOD systems. We release our aligned counterfactual dialogues, which are curated by human annotation, to facilitate further research on this topic.","sentences":["An important unexplored aspect in previous work on user satisfaction estimation for Task-Oriented Dialogue (TOD) systems is their evaluation in terms of robustness for the identification of user dissatisfaction: current benchmarks for user satisfaction estimation in TOD systems are highly skewed towards dialogues for which the user is satisfied.","The effect of having a more balanced set of satisfaction labels on performance is unknown.","However, balancing the data with more dissatisfactory dialogue samples requires further data collection and human annotation, which is costly and time-consuming.","In this work, we leverage large language models (LLMs) and unlock their ability to generate satisfaction-aware counterfactual dialogues to augment the set of original dialogues of a test collection.","We gather human annotations to ensure the reliability of the generated samples.","We evaluate two open-source LLMs as user satisfaction estimators on our augmented collection against state-of-the-art fine-tuned models.","Our experiments show that when used as few-shot user satisfaction estimators, open-source LLMs show higher robustness to the increase in the number of dissatisfaction labels in the test collection than the fine-tuned state-of-the-art models.","Our results shed light on the need for data augmentation approaches for user satisfaction estimation in TOD systems.","We release our aligned counterfactual dialogues, which are curated by human annotation, to facilitate further research on this topic."],"url":"http://arxiv.org/abs/2403.19056v1","category":"cs.CL"}
{"created":"2024-03-27 23:23:07","title":"Preliminary mapping of ionospheric total electron content (TEC) over Ecuador using global positioning system (GPS) data","abstract":"The ionosphere affects radio signals by altering their speed, direction, and trajectory, causing a temporary delay known as ionospheric delay, which is directly related to the total electron content (TEC). Although research in other equatorial locations has explored TEC implications, qualitative research is required to predict its behavior in the ionosphere. This study aims to depict TEC intensity evolution through color maps using data from 14 GPS receivers placed across Ecuador. For this purpose, pseudorange observables collected from the stations were used to present a calculation method for the TEC and its evolution during January 2022. The results revealed an oscillatory behavior in the evolution of the TEC, with intensity peaks that, on some occasions, approach and even exceed 100 T ECU (TEC units), while its local minimums never reach zero values.","sentences":["The ionosphere affects radio signals by altering their speed, direction, and trajectory, causing a temporary delay known as ionospheric delay, which is directly related to the total electron content (TEC).","Although research in other equatorial locations has explored TEC implications, qualitative research is required to predict its behavior in the ionosphere.","This study aims to depict TEC intensity evolution through color maps using data from 14 GPS receivers placed across Ecuador.","For this purpose, pseudorange observables collected from the stations were used to present a calculation method for the TEC and its evolution during January 2022.","The results revealed an oscillatory behavior in the evolution of the TEC, with intensity peaks that, on some occasions, approach and even exceed 100 T ECU (TEC units), while its local minimums never reach zero values."],"url":"http://arxiv.org/abs/2403.19053v1","category":"physics.space-ph"}
{"created":"2024-03-27 23:10:33","title":"Detecting Generative Parroting through Overfitting Masked Autoencoders","abstract":"The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely. Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively. We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets. Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models.","sentences":["The advent of generative AI models has revolutionized digital content creation, yet it introduces challenges in maintaining copyright integrity due to generative parroting, where models mimic their training data too closely.","Our research presents a novel approach to tackle this issue by employing an overfitted Masked Autoencoder (MAE) to detect such parroted samples effectively.","We establish a detection threshold based on the mean loss across the training dataset, allowing for the precise identification of parroted content in modified datasets.","Preliminary evaluations demonstrate promising results, suggesting our method's potential to ensure ethical use and enhance the legal compliance of generative models."],"url":"http://arxiv.org/abs/2403.19050v1","category":"cs.LG"}
{"created":"2024-03-27 23:09:50","title":"Power and Play: Investigating \"License to Critique\" in Teams' AI Ethics Discussions","abstract":"Past work has sought to design AI ethics interventions-such as checklists or toolkits-to help practitioners design more ethical AI systems. However, other work demonstrates how these interventions and the principles they're based on may serve to instead limit critique to those addressed within the intervention, while rendering broader concerns illegitimate. In this paper, drawing on work examining how standards enact discursive closure and how power relations affect whether and how people raise critique, we recruit three corporate teams, and one activist team, each with prior context working with one another, to play a game designed to trigger broad discussion around AI ethics. We use this as a point of contrast to trigger reflection on their teams' past discussions, examining factors which may affect their \"license to critique\" in AI ethics discussions. We then report on how particular affordances of this game may influence discussion, and find that the hypothetical context created in the game is unlikely to be a viable mechanism for real world change. We discuss how power dynamics within a group and notions of \"scope\" affect whether people may be willing to raise critique in AI ethics discussions, and discuss our finding that games are unlikely to enable direct changes to products or practice, but may be more likely to allow members to find critically-aligned allies for future collective action.","sentences":["Past work has sought to design AI ethics interventions-such as checklists or toolkits-to help practitioners design more ethical AI systems.","However, other work demonstrates how these interventions and the principles they're based on may serve to instead limit critique to those addressed within the intervention, while rendering broader concerns illegitimate.","In this paper, drawing on work examining how standards enact discursive closure and how power relations affect whether and how people raise critique, we recruit three corporate teams, and one activist team, each with prior context working with one another, to play a game designed to trigger broad discussion around AI ethics.","We use this as a point of contrast to trigger reflection on their teams' past discussions, examining factors which may affect their \"license to critique\" in AI ethics discussions.","We then report on how particular affordances of this game may influence discussion, and find that the hypothetical context created in the game is unlikely to be a viable mechanism for real world change.","We discuss how power dynamics within a group and notions of \"scope\" affect whether people may be willing to raise critique in AI ethics discussions, and discuss our finding that games are unlikely to enable direct changes to products or practice, but may be more likely to allow members to find critically-aligned allies for future collective action."],"url":"http://arxiv.org/abs/2403.19049v1","category":"cs.CY"}
{"created":"2024-03-27 22:50:48","title":"LITA: Language Instructed Temporal-Localization Assistant","abstract":"There has been tremendous progress in multimodal Large Language Models (LLMs). Recent works have extended these models to video input with promising instruction following capabilities. However, an important missing piece is temporal localization. These models cannot accurately answer the \"When?\" questions. We identify three key aspects that limit their temporal localization capabilities: (i) time representation, (ii) architecture, and (iii) data. We address these shortcomings by proposing Language Instructed Temporal-Localization Assistant (LITA) with the following features: (1) We introduce time tokens that encode timestamps relative to the video length to better represent time in videos. (2) We introduce SlowFast tokens in the architecture to capture temporal information at fine temporal resolution. (3) We emphasize temporal localization data for LITA. In addition to leveraging existing video datasets with timestamps, we propose a new task, Reasoning Temporal Localization (RTL), along with the dataset, ActivityNet-RTL, for learning and evaluating this task. Reasoning temporal localization requires both the reasoning and temporal localization of Video LLMs. LITA demonstrates strong performance on this challenging task, nearly doubling the temporal mean intersection-over-union (mIoU) of baselines. In addition, we show that our emphasis on temporal localization also substantially improves video-based text generation compared to existing Video LLMs, including a 36% relative improvement of Temporal Understanding. Code is available at: https://github.com/NVlabs/LITA","sentences":["There has been tremendous progress in multimodal Large Language Models (LLMs).","Recent works have extended these models to video input with promising instruction following capabilities.","However, an important missing piece is temporal localization.","These models cannot accurately answer the \"When?\" questions.","We identify three key aspects that limit their temporal localization capabilities: (i) time representation, (ii) architecture, and (iii) data.","We address these shortcomings by proposing Language Instructed Temporal-Localization Assistant (LITA) with the following features: (1) We introduce time tokens that encode timestamps relative to the video length to better represent time in videos.","(2) We introduce SlowFast tokens in the architecture to capture temporal information at fine temporal resolution.","(3) We emphasize temporal localization data for LITA.","In addition to leveraging existing video datasets with timestamps, we propose a new task, Reasoning Temporal Localization (RTL), along with the dataset, ActivityNet-RTL, for learning and evaluating this task.","Reasoning temporal localization requires both the reasoning and temporal localization of Video LLMs.","LITA demonstrates strong performance on this challenging task, nearly doubling the temporal mean intersection-over-union (mIoU) of baselines.","In addition, we show that our emphasis on temporal localization also substantially improves video-based text generation compared to existing Video LLMs, including a 36% relative improvement of Temporal Understanding.","Code is available at: https://github.com/NVlabs/LITA"],"url":"http://arxiv.org/abs/2403.19046v1","category":"cs.CV"}
{"created":"2024-03-27 22:36:36","title":"Low-Complexity Estimation Algorithm and Decoupling Scheme for FRaC System","abstract":"With the leaping advances in autonomous vehicles and transportation infrastructure, dual function radar-communication (DFRC) systems have become attractive due to the size, cost and resource efficiency. A frequency modulated continuous waveform (FMCW)-based radar-communication system (FRaC) utilizing both sparse multiple-input and multiple-output (MIMO) arrays and index modulation (IM) has been proposed to form a DFRC system specifically designed for vehicular applications. In this paper, the three-dimensional (3D) parameter estimation problem in the FRaC is considered. Since the 3D-parameters including range, direction of arrival (DOA) and velocity are coupled in the estimating matrix of the FRaC system, the existing estimation algorithms cannot estimate the 3D-parameters accurately. Hence, a novel decomposed decoupled atomic norm minimization (DANM) method is proposed by splitting the 3D-parameter estimating matrix into multiple 2D matrices with sparsity constraints. Then, the 3D-parameters are estimated and efficiently and separately with the optimized decoupled estimating matrix. Moreover, the Cram\\'{e}r-Rao lower bound (CRLB) of the 3D-parameter estimation are derived, and the computational complexity of the proposed algorithm is analyzed. Simulation results show that the proposed decomposed DANM method exploits the advantage of the virtual aperture in the existence of coupling caused by IM and sparse MIMO array and outperforms the co-estimation algorithm with lower computation complexity.","sentences":["With the leaping advances in autonomous vehicles and transportation infrastructure, dual function radar-communication (DFRC) systems have become attractive due to the size, cost and resource efficiency.","A frequency modulated continuous waveform (FMCW)-based radar-communication system (FRaC) utilizing both sparse multiple-input and multiple-output (MIMO) arrays and index modulation (IM) has been proposed to form a DFRC system specifically designed for vehicular applications.","In this paper, the three-dimensional (3D) parameter estimation problem in the FRaC is considered.","Since the 3D-parameters including range, direction of arrival (DOA) and velocity are coupled in the estimating matrix of the FRaC system, the existing estimation algorithms cannot estimate the 3D-parameters accurately.","Hence, a novel decomposed decoupled atomic norm minimization (DANM) method is proposed by splitting the 3D-parameter estimating matrix into multiple 2D matrices with sparsity constraints.","Then, the 3D-parameters are estimated and efficiently and separately with the optimized decoupled estimating matrix.","Moreover, the Cram\\'{e}r-Rao lower bound (CRLB) of the 3D-parameter estimation are derived, and the computational complexity of the proposed algorithm is analyzed.","Simulation results show that the proposed decomposed DANM method exploits the advantage of the virtual aperture in the existence of coupling caused by IM and sparse MIMO array and outperforms the co-estimation algorithm with lower computation complexity."],"url":"http://arxiv.org/abs/2403.19044v1","category":"eess.SP"}
{"created":"2024-03-27 22:05:10","title":"Evaluating Large Language Models for Health-Related Text Classification Tasks with Public Social Media Data","abstract":"Large language models (LLMs) have demonstrated remarkable success in NLP tasks. However, there is a paucity of studies that attempt to evaluate their performances on social media-based health-related natural language processing tasks, which have traditionally been difficult to achieve high scores in. We benchmarked one supervised classic machine learning model based on Support Vector Machines (SVMs), three supervised pretrained language models (PLMs) based on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5 and GPT4), across 6 text classification tasks. We developed three approaches for leveraging LLMs for text classification: employing LLMs as zero-shot classifiers, us-ing LLMs as annotators to annotate training data for supervised classifiers, and utilizing LLMs with few-shot examples for augmentation of manually annotated data. Our comprehensive experiments demonstrate that employ-ing data augmentation using LLMs (GPT-4) with relatively small human-annotated data to train lightweight supervised classification models achieves superior results compared to training with human-annotated data alone. Supervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings. By leveraging this data augmentation strategy, we can harness the power of LLMs to develop smaller, more effective domain-specific NLP models. LLM-annotated data without human guidance for training light-weight supervised classification models is an ineffective strategy. However, LLM, as a zero-shot classifier, shows promise in excluding false negatives and potentially reducing the human effort required for data annotation. Future investigations are imperative to explore optimal training data sizes and the optimal amounts of augmented data.","sentences":["Large language models (LLMs) have demonstrated remarkable success in NLP tasks.","However, there is a paucity of studies that attempt to evaluate their performances on social media-based health-related natural language processing tasks, which have traditionally been difficult to achieve high scores in.","We benchmarked one supervised classic machine learning model based on Support Vector Machines (SVMs), three supervised pretrained language models (PLMs) based on RoBERTa, BERTweet, and SocBERT, and two LLM based classifiers (GPT3.5 and GPT4), across 6 text classification tasks.","We developed three approaches for leveraging LLMs for text classification: employing LLMs as zero-shot classifiers, us-ing LLMs as annotators to annotate training data for supervised classifiers, and utilizing LLMs with few-shot examples for augmentation of manually annotated data.","Our comprehensive experiments demonstrate that employ-ing data augmentation using LLMs (GPT-4) with relatively small human-annotated data to train lightweight supervised classification models achieves superior results compared to training with human-annotated data alone.","Supervised learners also outperform GPT-4 and GPT-3.5 in zero-shot settings.","By leveraging this data augmentation strategy, we can harness the power of LLMs to develop smaller, more effective domain-specific NLP models.","LLM-annotated data without human guidance for training light-weight supervised classification models is an ineffective strategy.","However, LLM, as a zero-shot classifier, shows promise in excluding false negatives and potentially reducing the human effort required for data annotation.","Future investigations are imperative to explore optimal training data sizes and the optimal amounts of augmented data."],"url":"http://arxiv.org/abs/2403.19031v1","category":"cs.CL"}
{"created":"2024-03-27 21:56:18","title":"Should I Help a Delivery Robot? Cultivating Prosocial Norms through Observations","abstract":"We propose leveraging prosocial observations to cultivate new social norms to encourage prosocial behaviors toward delivery robots. With an online experiment, we quantitatively assess updates in norm beliefs regarding human-robot prosocial behaviors through observational learning. Results demonstrate the initially perceived normativity of helping robots is influenced by familiarity with delivery robots and perceptions of robots' social intelligence. Observing human-robot prosocial interactions notably shifts peoples' normative beliefs about prosocial actions; thereby changing their perceived obligations to offer help to delivery robots. Additionally, we found that observing robots offering help to humans, rather than receiving help, more significantly increased participants' feelings of obligation to help robots. Our findings provide insights into prosocial design for future mobility systems. Improved familiarity with robot capabilities and portraying them as desirable social partners can help foster wider acceptance. Furthermore, robots need to be designed to exhibit higher levels of interactivity and reciprocal capabilities for prosocial behavior.","sentences":["We propose leveraging prosocial observations to cultivate new social norms to encourage prosocial behaviors toward delivery robots.","With an online experiment, we quantitatively assess updates in norm beliefs regarding human-robot prosocial behaviors through observational learning.","Results demonstrate the initially perceived normativity of helping robots is influenced by familiarity with delivery robots and perceptions of robots' social intelligence.","Observing human-robot prosocial interactions notably shifts peoples' normative beliefs about prosocial actions; thereby changing their perceived obligations to offer help to delivery robots.","Additionally, we found that observing robots offering help to humans, rather than receiving help, more significantly increased participants' feelings of obligation to help robots.","Our findings provide insights into prosocial design for future mobility systems.","Improved familiarity with robot capabilities and portraying them as desirable social partners can help foster wider acceptance.","Furthermore, robots need to be designed to exhibit higher levels of interactivity and reciprocal capabilities for prosocial behavior."],"url":"http://arxiv.org/abs/2403.19027v1","category":"cs.RO"}
{"created":"2024-03-27 21:43:12","title":"Egocentric Scene-aware Human Trajectory Prediction","abstract":"Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons. Such a robot needs to be able to predict the ego motion of the wearer based on egocentric vision and the surrounding scene. In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings. To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective. We present a method to predict human motion conditioning on the surrounding static scene. Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment. We introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-generating technique to speed up real-time inference of a diffusion model. We ablate our model and compare it to baselines, and results show that our model outperforms existing methods on key metrics of collision avoidance and trajectory mode coverage.","sentences":["Wearable collaborative robots stand to assist human wearers who need fall prevention assistance or wear exoskeletons.","Such a robot needs to be able to predict the ego motion of the wearer based on egocentric vision and the surrounding scene.","In this work, we leveraged body-mounted cameras and sensors to anticipate the trajectory of human wearers through complex surroundings.","To facilitate research in ego-motion prediction, we have collected a comprehensive walking scene navigation dataset centered on the user's perspective.","We present a method to predict human motion conditioning on the surrounding static scene.","Our method leverages a diffusion model to produce a distribution of potential future trajectories, taking into account the user's observation of the environment.","We introduce a compact representation to encode the user's visual memory of the surroundings, as well as an efficient sample-generating technique to speed up real-time inference of a diffusion model.","We ablate our model and compare it to baselines, and results show that our model outperforms existing methods on key metrics of collision avoidance and trajectory mode coverage."],"url":"http://arxiv.org/abs/2403.19026v1","category":"cs.CV"}
{"created":"2024-03-27 21:31:46","title":"Exploiting Symmetry in Dynamics for Model-Based Reinforcement Learning with Asymmetric Rewards","abstract":"Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy. A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry. However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics. In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied. We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries. We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model.","sentences":["Recent work in reinforcement learning has leveraged symmetries in the model to improve sample efficiency in training a policy.","A commonly used simplifying assumption is that the dynamics and reward both exhibit the same symmetry.","However, in many real-world environments, the dynamical model exhibits symmetry independent of the reward model: the reward may not satisfy the same symmetries as the dynamics.","In this paper, we investigate scenarios where only the dynamics are assumed to exhibit symmetry, extending the scope of problems in reinforcement learning and learning in control theory where symmetry techniques can be applied.","We use Cartan's moving frame method to introduce a technique for learning dynamics which, by construction, exhibit specified symmetries.","We demonstrate through numerical experiments that the proposed method learns a more accurate dynamical model."],"url":"http://arxiv.org/abs/2403.19024v1","category":"cs.LG"}
{"created":"2024-03-27 21:22:37","title":"Towards LLM-RecSys Alignment with Textual ID Learning","abstract":"Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm. However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations. To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens. This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation. Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model. Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting. Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting. The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation. Code and data are open-sourced at https://github.com/agiresearch/IDGenRec.","sentences":["Generative recommendation based on Large Language Models (LLMs) have transformed the traditional ranking-based recommendation style into a text-to-text generation paradigm.","However, in contrast to standard NLP tasks that inherently operate on human vocabulary, current research in generative recommendations struggles to effectively encode recommendation items within the text-to-text framework using concise yet meaningful ID representations.","To better align LLMs with recommendation needs, we propose IDGen, representing each item as a unique, concise, semantically rich, platform-agnostic textual ID using human language tokens.","This is achieved by training a textual ID generator alongside the LLM-based recommender, enabling seamless integration of personalized recommendations into natural language generation.","Notably, as user history is expressed in natural language and decoupled from the original dataset, our approach suggests the potential for a foundational generative recommendation model.","Experiments show that our framework consistently surpasses existing models in sequential recommendation under standard experimental setting.","Then, we explore the possibility of training a foundation recommendation model with the proposed method on data collected from 19 different datasets and tested its recommendation performance on 6 unseen datasets across different platforms under a completely zero-shot setting.","The results show that the zero-shot performance of the pre-trained foundation model is comparable to or even better than some traditional recommendation models based on supervised training, showing the potential of the IDGen paradigm serving as the foundation model for generative recommendation.","Code and data are open-sourced at https://github.com/agiresearch/IDGenRec."],"url":"http://arxiv.org/abs/2403.19021v1","category":"cs.IR"}
{"created":"2024-03-27 21:10:07","title":"ReflectSumm: A Benchmark for Course Reflection Summarization","abstract":"This paper introduces ReflectSumm, a novel summarization dataset specifically designed for summarizing students' reflective writing. The goal of ReflectSumm is to facilitate developing and evaluating novel summarization techniques tailored to real-world scenarios with little training data, %practical tasks with potential implications in the opinion summarization domain in general and the educational domain in particular. The dataset encompasses a diverse range of summarization tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications. To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines. The results provide benchmarks for facilitating further research in this area.","sentences":["This paper introduces ReflectSumm, a novel summarization dataset specifically designed for summarizing students' reflective writing.","The goal of ReflectSumm is to facilitate developing and evaluating novel summarization techniques tailored to real-world scenarios with little training data, %practical tasks with potential implications in the opinion summarization domain in general and the educational domain in particular.","The dataset encompasses a diverse range of summarization tasks and includes comprehensive metadata, enabling the exploration of various research questions and supporting different applications.","To showcase its utility, we conducted extensive evaluations using multiple state-of-the-art baselines.","The results provide benchmarks for facilitating further research in this area."],"url":"http://arxiv.org/abs/2403.19012v1","category":"cs.CL"}
{"created":"2024-03-27 20:52:30","title":"Robust Active Speaker Detection in Noisy Environments","abstract":"This paper addresses the issue of active speaker detection (ASD) in noisy environments and formulates a robust active speaker detection (rASD) problem. Existing ASD approaches leverage both audio and visual modalities, but non-speech sounds in the surrounding environment can negatively impact performance. To overcome this, we propose a novel framework that utilizes audio-visual speech separation as guidance to learn noise-free audio features. These features are then utilized in an ASD model, and both tasks are jointly optimized in an end-to-end framework. Our proposed framework mitigates residual noise and audio quality reduction issues that can occur in a naive cascaded two-stage framework that directly uses separated speech for ASD, and enables the two tasks to be optimized simultaneously. To further enhance the robustness of the audio features and handle inherent speech noises, we propose a dynamic weighted loss approach to train the speech separator. We also collected a real-world noise audio dataset to facilitate investigations. Experiments demonstrate that non-speech audio noises significantly impact ASD models, and our proposed approach improves ASD performance in noisy environments. The framework is general and can be applied to different ASD approaches to improve their robustness. Our code, models, and data will be released.","sentences":["This paper addresses the issue of active speaker detection (ASD) in noisy environments and formulates a robust active speaker detection (rASD) problem.","Existing ASD approaches leverage both audio and visual modalities, but non-speech sounds in the surrounding environment can negatively impact performance.","To overcome this, we propose a novel framework that utilizes audio-visual speech separation as guidance to learn noise-free audio features.","These features are then utilized in an ASD model, and both tasks are jointly optimized in an end-to-end framework.","Our proposed framework mitigates residual noise and audio quality reduction issues that can occur in a naive cascaded two-stage framework that directly uses separated speech for ASD, and enables the two tasks to be optimized simultaneously.","To further enhance the robustness of the audio features and handle inherent speech noises, we propose a dynamic weighted loss approach to train the speech separator.","We also collected a real-world noise audio dataset to facilitate investigations.","Experiments demonstrate that non-speech audio noises significantly impact ASD models, and our proposed approach improves ASD performance in noisy environments.","The framework is general and can be applied to different ASD approaches to improve their robustness.","Our code, models, and data will be released."],"url":"http://arxiv.org/abs/2403.19002v1","category":"cs.MM"}
{"created":"2024-03-27 20:51:02","title":"Cross--domain Fiber Cluster Shape Analysis for Language Performance Cognitive Score Prediction","abstract":"Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality. Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain. In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function. We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography. To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features. We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography. We assess the performance of the method on a large dataset including 1065 healthy young adults. The results demonstrate that both the transformer-based SFFormer model and its inter/intra feature fusion with shape, microstructure, and connectivity are informative, and together, they improve the prediction of subject-specific language performance scores. Overall, our results indicate that the shape of the brain's connections is predictive of human language function.","sentences":["Shape plays an important role in computer graphics, offering informative features to convey an object's morphology and functionality.","Shape analysis in brain imaging can help interpret structural and functionality correlations of the human brain.","In this work, we investigate the shape of the brain's 3D white matter connections and its potential predictive relationship to human cognitive function.","We reconstruct brain connections as sequences of 3D points using diffusion magnetic resonance imaging (dMRI) tractography.","To describe each connection, we extract 12 shape descriptors in addition to traditional dMRI connectivity and tissue microstructure features.","We introduce a novel framework, Shape--fused Fiber Cluster Transformer (SFFormer), that leverages a multi-head cross-attention feature fusion module to predict subject-specific language performance based on dMRI tractography.","We assess the performance of the method on a large dataset including 1065 healthy young adults.","The results demonstrate that both the transformer-based SFFormer model and its inter/intra feature fusion with shape, microstructure, and connectivity are informative, and together, they improve the prediction of subject-specific language performance scores.","Overall, our results indicate that the shape of the brain's connections is predictive of human language function."],"url":"http://arxiv.org/abs/2403.19001v1","category":"cs.CV"}
{"created":"2024-03-27 20:38:04","title":"Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems","abstract":"Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the different MSS. Within the same MSS, our framework achieves an average accuracy of 93.26\\% and 85.2\\% across 50 meta-testing tasks for Trainticket and OnlineBoutique, respectively, when provided with 10 instances for each task. In a cross-system context, our framework gets an average accuracy of 92.19\\% and 84.77\\% for the same meta-testing tasks of the respective system, also with 10 instances provided for each task. Our work demonstrates the applicability of achieving few-shot abnormal trace classification for MSS and shows how it can enable cross-system adaptability. This opens an avenue for building more generalized AIOps tools that require less system-specific data labeling for anomaly detection and root cause analysis.","sentences":["Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature.","To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis.","In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS.","Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification.","The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets.","The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the different MSS.","Within the same MSS, our framework achieves an average accuracy of 93.26\\% and 85.2\\% across 50 meta-testing tasks for Trainticket and OnlineBoutique, respectively, when provided with 10 instances for each task.","In a cross-system context, our framework gets an average accuracy of 92.19\\% and 84.77\\% for the same meta-testing tasks of the respective system, also with 10 instances provided for each task.","Our work demonstrates the applicability of achieving few-shot abnormal trace classification for MSS and shows how it can enable cross-system adaptability.","This opens an avenue for building more generalized AIOps tools that require less system-specific data labeling for anomaly detection and root cause analysis."],"url":"http://arxiv.org/abs/2403.18998v1","category":"cs.SE"}
{"created":"2024-03-27 20:27:31","title":"Causal-StoNet: Causal Inference for High-Dimensional Complex Data","abstract":"With the advancement of data science, the collection of increasingly complex datasets has become commonplace. In such datasets, the data dimension can be extremely high, and the underlying data generation process can be unknown and highly nonlinear. As a result, the task of making causal inference with high-dimensional complex data has become a fundamental problem in many disciplines, such as medicine, econometrics, and social science. However, the existing methods for causal inference are frequently developed under the assumption that the data dimension is low or that the underlying data generation process is linear or approximately linear. To address these challenges, this paper proposes a novel causal inference approach for dealing with high-dimensional complex data. The proposed approach is based on deep learning techniques, including sparse deep learning theory and stochastic neural networks, that have been developed in recent literature. By using these techniques, the proposed approach can address both the high dimensionality and unknown data generation process in a coherent way. Furthermore, the proposed approach can also be used when missing values are present in the datasets. Extensive numerical studies indicate that the proposed approach outperforms existing ones.","sentences":["With the advancement of data science, the collection of increasingly complex datasets has become commonplace.","In such datasets, the data dimension can be extremely high, and the underlying data generation process can be unknown and highly nonlinear.","As a result, the task of making causal inference with high-dimensional complex data has become a fundamental problem in many disciplines, such as medicine, econometrics, and social science.","However, the existing methods for causal inference are frequently developed under the assumption that the data dimension is low or that the underlying data generation process is linear or approximately linear.","To address these challenges, this paper proposes a novel causal inference approach for dealing with high-dimensional complex data.","The proposed approach is based on deep learning techniques, including sparse deep learning theory and stochastic neural networks, that have been developed in recent literature.","By using these techniques, the proposed approach can address both the high dimensionality and unknown data generation process in a coherent way.","Furthermore, the proposed approach can also be used when missing values are present in the datasets.","Extensive numerical studies indicate that the proposed approach outperforms existing ones."],"url":"http://arxiv.org/abs/2403.18994v1","category":"stat.ML"}
{"created":"2024-03-27 20:09:59","title":"Dealing with Imbalanced Classes in Bot-IoT Dataset","abstract":"With the rapidly spreading usage of Internet of Things (IoT) devices, a network intrusion detection system (NIDS) plays an important role in detecting and protecting various types of attacks in the IoT network. To evaluate the robustness of the NIDS in the IoT network, the existing work proposed a realistic botnet dataset in the IoT network (Bot-IoT dataset) and applied it to machine learning-based anomaly detection. This dataset contains imbalanced normal and attack packets because the number of normal packets is much smaller than that of attack ones. The nature of imbalanced data may make it difficult to identify the minority class correctly. In this thesis, to address the class imbalance problem in the Bot-IoT dataset, we propose a binary classification method with synthetic minority over-sampling techniques (SMOTE). The proposed classifier aims to detect attack packets and overcome the class imbalance problem using the SMOTE algorithm. Through numerical results, we demonstrate the proposed classifier's fundamental characteristics and the impact of imbalanced data on its performance.","sentences":["With the rapidly spreading usage of Internet of Things (IoT) devices, a network intrusion detection system (NIDS) plays an important role in detecting and protecting various types of attacks in the IoT network.","To evaluate the robustness of the NIDS in the IoT network, the existing work proposed a realistic botnet dataset in the IoT network (Bot-IoT dataset) and applied it to machine learning-based anomaly detection.","This dataset contains imbalanced normal and attack packets because the number of normal packets is much smaller than that of attack ones.","The nature of imbalanced data may make it difficult to identify the minority class correctly.","In this thesis, to address the class imbalance problem in the Bot-IoT dataset, we propose a binary classification method with synthetic minority over-sampling techniques (SMOTE).","The proposed classifier aims to detect attack packets and overcome the class imbalance problem using the SMOTE algorithm.","Through numerical results, we demonstrate the proposed classifier's fundamental characteristics and the impact of imbalanced data on its performance."],"url":"http://arxiv.org/abs/2403.18989v1","category":"cs.CR"}
{"created":"2024-03-27 20:07:39","title":"Robustness and Visual Explanation for Black Box Image, Video, and ECG Signal Classification with Reinforcement Learning","abstract":"We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D). The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types. The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency. Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models. For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions. This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types.","sentences":["We present a generic Reinforcement Learning (RL) framework optimized for crafting adversarial attacks on different model types spanning from ECG signal analysis (1D), image classification (2D), and video classification (3D).","The framework focuses on identifying sensitive regions and inducing misclassifications with minimal distortions and various distortion types.","The novel RL method outperforms state-of-the-art methods for all three applications, proving its efficiency.","Our RL approach produces superior localization masks, enhancing interpretability for image classification and ECG analysis models.","For applications such as ECG analysis, our platform highlights critical ECG segments for clinicians while ensuring resilience against prevalent distortions.","This comprehensive tool aims to bolster both resilience with adversarial training and transparency across varied applications and data types."],"url":"http://arxiv.org/abs/2403.18985v1","category":"cs.LG"}
{"created":"2024-03-27 19:52:55","title":"TextCraftor: Your Text Encoder Can be Image Quality Controller","abstract":"Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis. Despite their formidable capabilities, these models are not without their limitations. It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results. To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies. Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models? Our findings reveal that, instead of replacing the CLIP text encoder used in Stable Diffusion with other large language models, we can enhance it through our proposed fine-tuning approach, TextCraftor, leading to substantial improvements in quantitative benchmarks and human assessments. Interestingly, our technique also empowers controllable image generation through the interpolation of different text encoders fine-tuned with various rewards. We also demonstrate that TextCraftor is orthogonal to UNet finetuning, and can be combined to further improve generative quality.","sentences":["Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have revolutionized the field of content generation, enabling significant advancements in areas like image editing and video synthesis.","Despite their formidable capabilities, these models are not without their limitations.","It is still challenging to synthesize an image that aligns well with the input text, and multiple runs with carefully crafted prompts are required to achieve satisfactory results.","To mitigate these limitations, numerous studies have endeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing various technologies.","Yet, amidst these efforts, a pivotal question of text-to-image diffusion model training has remained largely unexplored: Is it possible and feasible to fine-tune the text encoder to improve the performance of text-to-image diffusion models?","Our findings reveal that, instead of replacing the CLIP text encoder used in Stable Diffusion with other large language models, we can enhance it through our proposed fine-tuning approach, TextCraftor, leading to substantial improvements in quantitative benchmarks and human assessments.","Interestingly, our technique also empowers controllable image generation through the interpolation of different text encoders fine-tuned with various rewards.","We also demonstrate that TextCraftor is orthogonal to UNet finetuning, and can be combined to further improve generative quality."],"url":"http://arxiv.org/abs/2403.18978v1","category":"cs.CV"}
{"created":"2024-03-27 19:45:09","title":"\"Sorry, Come Again?\" Prompting -- Enhancing Comprehension and Diminishing Hallucination with [PAUSE]-injected Optimal Paraphrasing","abstract":"Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs). In this paper, we introduce the Sorry, Come Again (SCA) prompting, aimed to avoid LLM hallucinations by enhancing comprehension through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay LLM generation. First, we provide an in-depth analysis of linguistic nuances: formality, readability, and concreteness of prompts for 21 LLMs, and elucidate how these nuances contribute to hallucinated generation. Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans. In such scenarios, an LLM tends to speculate and generate content based on its imagination (associative memory) to fill these information gaps. Although these speculations may occasionally align with factual information, their accuracy is not assured, often resulting in hallucination. Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle. While a specific paraphrase may suit one LLM, the same paraphrased version may elicit a different response from another LLM. Therefore, we propose an optimal paraphrasing technique to identify the most comprehensible paraphrase of a given prompt, evaluated using Integrated Gradient (and its variations) to guarantee that the LLM accurately processes all words. While reading lengthy sentences, humans often pause at various points to better comprehend the meaning read thus far. We have fine-tuned an LLM with injected [PAUSE] tokens, allowing the LLM to pause while reading lengthier prompts. This has brought several key contributions: (i) determining the optimal position to inject [PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and (iii) introducing reverse proxy tuning to fine-tune the LLM for [PAUSE] insertion.","sentences":["Hallucination has emerged as the most vulnerable aspect of contemporary Large Language Models (LLMs).","In this paper, we introduce the Sorry, Come Again (SCA) prompting, aimed to avoid LLM hallucinations by enhancing comprehension through: (i) optimal paraphrasing and (ii) injecting [PAUSE] tokens to delay LLM generation.","First, we provide an in-depth analysis of linguistic nuances: formality, readability, and concreteness of prompts for 21 LLMs, and elucidate how these nuances contribute to hallucinated generation.","Prompts with lower readability, formality, or concreteness pose comprehension challenges for LLMs, similar to those faced by humans.","In such scenarios, an LLM tends to speculate and generate content based on its imagination (associative memory) to fill these information gaps.","Although these speculations may occasionally align with factual information, their accuracy is not assured, often resulting in hallucination.","Recent studies reveal that an LLM often neglects the middle sections of extended prompts, a phenomenon termed as lost in the middle.","While a specific paraphrase may suit one LLM, the same paraphrased version may elicit a different response from another LLM.","Therefore, we propose an optimal paraphrasing technique to identify the most comprehensible paraphrase of a given prompt, evaluated using Integrated Gradient (and its variations) to guarantee that the LLM accurately processes all words.","While reading lengthy sentences, humans often pause at various points to better comprehend the meaning read thus far.","We have fine-tuned an LLM with injected [PAUSE] tokens, allowing the LLM to pause while reading lengthier prompts.","This has brought several key contributions: (i) determining the optimal position to inject [PAUSE], (ii) determining the number of [PAUSE] tokens to be inserted, and (iii) introducing reverse proxy tuning to fine-tune the LLM for [PAUSE] insertion."],"url":"http://arxiv.org/abs/2403.18976v1","category":"cs.CL"}
{"created":"2024-03-27 19:35:41","title":"A Survey on Large Language Models from Concept to Implementation","abstract":"Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting their versatility and the potential they hold for transforming diverse application sectors, thereby offering readers a comprehensive understanding of the current and future landscape of Transformer-based LLMs in practical applications.","sentences":["Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology.","This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series.","This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries.","From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design.","This survey provides an in-depth look at the latest research in Transformer models, highlighting their versatility and the potential they hold for transforming diverse application sectors, thereby offering readers a comprehensive understanding of the current and future landscape of Transformer-based LLMs in practical applications."],"url":"http://arxiv.org/abs/2403.18969v1","category":"cs.CL"}
{"created":"2024-03-27 19:30:06","title":"LORD: Large Models based Opposite Reward Design for Autonomous Driving","abstract":"Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches. However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios. Recently, large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals. However, the desired linguistic goals for autonomous driving such as \"drive safely\" are ambiguous and incomprehensible by pretrained models. On the other hand, undesired linguistic goals like \"collision\" are more concrete and tractable. In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models. Through extensive experiments, our proposed framework shows its efficiency in leveraging the power of large pretrained models for achieving safe and enhanced autonomous driving. Moreover, the proposed approach shows improved generalization capabilities as it outperforms counterpart methods across diverse and challenging driving scenarios.","sentences":["Reinforcement learning (RL) based autonomous driving has emerged as a promising alternative to data-driven imitation learning approaches.","However, crafting effective reward functions for RL poses challenges due to the complexity of defining and quantifying good driving behaviors across diverse scenarios.","Recently, large pretrained models have gained significant attention as zero-shot reward models for tasks specified with desired linguistic goals.","However, the desired linguistic goals for autonomous driving such as \"drive safely\" are ambiguous and incomprehensible by pretrained models.","On the other hand, undesired linguistic goals like \"collision\" are more concrete and tractable.","In this work, we introduce LORD, a novel large models based opposite reward design through undesired linguistic goals to enable the efficient use of large pretrained models as zero-shot reward models.","Through extensive experiments, our proposed framework shows its efficiency in leveraging the power of large pretrained models for achieving safe and enhanced autonomous driving.","Moreover, the proposed approach shows improved generalization capabilities as it outperforms counterpart methods across diverse and challenging driving scenarios."],"url":"http://arxiv.org/abs/2403.18965v1","category":"cs.RO"}
{"created":"2024-03-27 19:16:56","title":"Using Quantum Computing to Infer Dynamic Behaviors of Biological and Artificial Neural Networks","abstract":"The exploration of new problem classes for quantum computation is an active area of research. An essentially completely unexplored topic is the use of quantum algorithms and computing to explore and ask questions \\textit{about} the functional dynamics of neural networks. This is a component of the still-nascent topic of applying quantum computing to the modeling and simulations of biological and artificial neural networks. In this work, we show how a carefully constructed set of conditions can use two foundational quantum algorithms, Grover and Deutsch-Josza, in such a way that the output measurements admit an interpretation that guarantees we can infer if a simple representation of a neural network (which applies to both biological and artificial networks) after some period of time has the potential to continue sustaining dynamic activity. Or whether the dynamics are guaranteed to stop either through 'epileptic' dynamics or quiescence.","sentences":["The exploration of new problem classes for quantum computation is an active area of research.","An essentially completely unexplored topic is the use of quantum algorithms and computing to explore and ask questions \\textit{about} the functional dynamics of neural networks.","This is a component of the still-nascent topic of applying quantum computing to the modeling and simulations of biological and artificial neural networks.","In this work, we show how a carefully constructed set of conditions can use two foundational quantum algorithms, Grover and Deutsch-Josza, in such a way that the output measurements admit an interpretation that guarantees we can infer if a simple representation of a neural network (which applies to both biological and artificial networks) after some period of time has the potential to continue sustaining dynamic activity.","Or whether the dynamics are guaranteed to stop either through 'epileptic' dynamics or quiescence."],"url":"http://arxiv.org/abs/2403.18963v1","category":"quant-ph"}
{"created":"2024-03-27 19:12:24","title":"High Recall, Small Data: The Challenges of Within-System Evaluation in a Live Legal Search System","abstract":"This paper illustrates some challenges of common ranking evaluation methods for legal information retrieval (IR). We show these challenges with log data from a live legal search system and two user studies. We provide an overview of aspects of legal IR, and the implications of these aspects for the expected challenges of common evaluation methods: test collections based on explicit and implicit feedback, user surveys, and A/B testing. Next, we illustrate the challenges of common evaluation methods using data from a live, commercial, legal search engine. We specifically focus on methods for monitoring the effectiveness of (continuous) changes to document ranking by a single IR system over time. We show how the combination of characteristics in legal IR systems and limited user data can lead to challenges that cause the common evaluation methods discussed to be sub-optimal. In our future work we will therefore focus on less common evaluation methods, such as cost-based evaluation models.","sentences":["This paper illustrates some challenges of common ranking evaluation methods for legal information retrieval (IR).","We show these challenges with log data from a live legal search system and two user studies.","We provide an overview of aspects of legal IR, and the implications of these aspects for the expected challenges of common evaluation methods: test collections based on explicit and implicit feedback, user surveys, and A/B testing.","Next, we illustrate the challenges of common evaluation methods using data from a live, commercial, legal search engine.","We specifically focus on methods for monitoring the effectiveness of (continuous) changes to document ranking by a single IR system over time.","We show how the combination of characteristics in legal IR systems and limited user data can lead to challenges that cause the common evaluation methods discussed to be sub-optimal.","In our future work we will therefore focus on less common evaluation methods, such as cost-based evaluation models."],"url":"http://arxiv.org/abs/2403.18962v1","category":"cs.IR"}
{"created":"2024-03-27 19:02:56","title":"A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products","abstract":"This paper investigates the complexities of integrating Large Language Models (LLMs) into software products, with a focus on the challenges encountered for determining their readiness for release. Our systematic review of grey literature identifies common challenges in deploying LLMs, ranging from pre-training and fine-tuning to user experience considerations. The study introduces a comprehensive checklist designed to guide practitioners in evaluating key release readiness aspects such as performance, monitoring, and deployment strategies, aiming to enhance the reliability and effectiveness of LLM-based applications in real-world settings.","sentences":["This paper investigates the complexities of integrating Large Language Models (LLMs) into software products, with a focus on the challenges encountered for determining their readiness for release.","Our systematic review of grey literature identifies common challenges in deploying LLMs, ranging from pre-training and fine-tuning to user experience considerations.","The study introduces a comprehensive checklist designed to guide practitioners in evaluating key release readiness aspects such as performance, monitoring, and deployment strategies, aiming to enhance the reliability and effectiveness of LLM-based applications in real-world settings."],"url":"http://arxiv.org/abs/2403.18958v1","category":"cs.SE"}
{"created":"2024-03-27 19:02:13","title":"Moderating Illicit Online Image Promotion for Unsafe User-Generated Content Games Using Large Vision-Language Models","abstract":"Online user-generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment. However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents. Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users. This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content. In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs. We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators. Our in-depth studies reveal a new understanding of this problem and the urgent need for automatically flagging illicit UGCG promotions. We additionally create a cutting-edge system, UGCG-Guard, designed to aid social media platforms in effectively identifying images used for illicit UGCG promotions. This system leverages recently introduced large vision-language models (VLMs) and employs a novel conditional prompting strategy for zero-shot domain adaptation, along with chain-of-thought (CoT) reasoning for contextual identification. UGCG-Guard achieves outstanding results, with an accuracy rate of 94% in detecting these images used for the illicit promotion of such games in real-world scenarios.","sentences":["Online user-generated content games (UGCGs) are increasingly popular among children and adolescents for social interaction and more creative online entertainment.","However, they pose a heightened risk of exposure to explicit content, raising growing concerns for the online safety of children and adolescents.","Despite these concerns, few studies have addressed the issue of illicit image-based promotions of unsafe UGCGs on social media, which can inadvertently attract young users.","This challenge arises from the difficulty of obtaining comprehensive training data for UGCG images and the unique nature of these images, which differ from traditional unsafe content.","In this work, we take the first step towards studying the threat of illicit promotions of unsafe UGCGs.","We collect a real-world dataset comprising 2,924 images that display diverse sexually explicit and violent content used to promote UGCGs by their game creators.","Our in-depth studies reveal a new understanding of this problem and the urgent need for automatically flagging illicit UGCG promotions.","We additionally create a cutting-edge system, UGCG-Guard, designed to aid social media platforms in effectively identifying images used for illicit UGCG promotions.","This system leverages recently introduced large vision-language models (VLMs) and employs a novel conditional prompting strategy for zero-shot domain adaptation, along with chain-of-thought (CoT) reasoning for contextual identification.","UGCG-Guard achieves outstanding results, with an accuracy rate of 94% in detecting these images used for the illicit promotion of such games in real-world scenarios."],"url":"http://arxiv.org/abs/2403.18957v1","category":"cs.CY"}
{"created":"2024-03-27 18:38:39","title":"Reshaping Free-Text Radiology Notes Into Structured Reports With Generative Transformers","abstract":"BACKGROUND: Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use. Recently the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness and information retrieval. We propose a pipeline to extract information from free-text radiology reports, that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma. METHODS: Our work aims to leverage the potential of Natural Language Processing (NLP) and Transformer-based models to deal with automatic SR registry filling. With the availability of 174 radiology reports, we investigate a rule-free generative Question Answering approach based on a domain-specific version of T5 (IT5). Two strategies (batch-truncation and ex-post combination) are implemented to comply with the model's context length limitations. Performance is evaluated in terms of strict accuracy, F1, and format accuracy, and compared with the widely used GPT-3.5 Large Language Model. A 5-point Likert scale questionnaire is used to collect human-expert feedback on the similarity between medical annotations and generated answers. RESULTS: The combination of fine-tuning and batch splitting allows IT5 to achieve notable results; it performs on par with GPT-3.5 albeit its size being a thousand times smaller in terms of parameters. Human-based assessment scores show a high correlation (Spearman's correlation coefficients>0.88, p-values<0.001) with AI performance metrics (F1) and confirm the superior ability of LLMs (i.e., GPT-3.5, 175B of parameters) in generating plausible human-like statements.","sentences":["BACKGROUND:","Radiology reports are typically written in a free-text format, making clinical information difficult to extract and use.","Recently the adoption of structured reporting (SR) has been recommended by various medical societies thanks to the advantages it offers, e.g. standardization, completeness and information retrieval.","We propose a pipeline to extract information from free-text radiology reports, that fits with the items of the reference SR registry proposed by a national society of interventional and medical radiology, focusing on CT staging of patients with lymphoma.","METHODS:","Our work aims to leverage the potential of Natural Language Processing (NLP) and Transformer-based models to deal with automatic SR registry filling.","With the availability of 174 radiology reports, we investigate a rule-free generative Question Answering approach based on a domain-specific version of T5 (IT5).","Two strategies (batch-truncation and ex-post combination) are implemented to comply with the model's context length limitations.","Performance is evaluated in terms of strict accuracy, F1, and format accuracy, and compared with the widely used GPT-3.5 Large Language Model.","A 5-point Likert scale questionnaire is used to collect human-expert feedback on the similarity between medical annotations and generated answers.","RESULTS:","The combination of fine-tuning and batch splitting allows IT5 to achieve notable results; it performs on par with GPT-3.5 albeit its size being a thousand times smaller in terms of parameters.","Human-based assessment scores show a high correlation (Spearman's correlation coefficients>0.88, p-values<0.001) with AI performance metrics (F1) and confirm the superior ability of LLMs (i.e., GPT-3.5, 175B of parameters) in generating plausible human-like statements."],"url":"http://arxiv.org/abs/2403.18938v1","category":"cs.CL"}
{"created":"2024-03-27 18:22:48","title":"Measuring Political Bias in Large Language Models: What Is Said and How It Is Said","abstract":"We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable.","sentences":["We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues.","Existing benchmarks and measures focus on gender and racial biases.","However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications.","In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs.","Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias.","We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable."],"url":"http://arxiv.org/abs/2403.18932v1","category":"cs.CL"}
{"created":"2024-03-27 18:09:55","title":"CPR: Retrieval Augmented Generation for Copyright Protection","abstract":"Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale. However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output. To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models.CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs. In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion scores at inference. We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images. We provide two algorithms for copyright protection, CPR-KL and CPR-Choose. Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward diffusion. We show that our method can be applied to any pre-trained conditional diffusion model, such as Stable Diffusion or unCLIP. In particular, we empirically show that applying CPR on top of unCLIP improves quality and text-to-image alignment of the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning.","sentences":["Retrieval Augmented Generation (RAG) is emerging as a flexible and robust technique to adapt models to private users data without training, to handle credit attribution, and to allow efficient machine unlearning at scale.","However, RAG techniques for image generation may lead to parts of the retrieved samples being copied in the model's output.","To reduce risks of leaking private information contained in the retrieved set, we introduce Copy-Protected generation with Retrieval (CPR), a new method for RAG with strong copyright protection guarantees in a mixed-private setting for diffusion models.","CPR allows to condition the output of diffusion models on a set of retrieved images, while also guaranteeing that unique identifiable information about those example is not exposed in the generated outputs.","In particular, it does so by sampling from a mixture of public (safe) distribution and private (user) distribution by merging their diffusion scores at inference.","We prove that CPR satisfies Near Access Freeness (NAF) which bounds the amount of information an attacker may be able to extract from the generated images.","We provide two algorithms for copyright protection, CPR-KL and CPR-Choose.","Unlike previously proposed rejection-sampling-based NAF methods, our methods enable efficient copyright-protected sampling with a single run of backward diffusion.","We show that our method can be applied to any pre-trained conditional diffusion model, such as Stable Diffusion or unCLIP.","In particular, we empirically show that applying CPR on top of unCLIP improves quality and text-to-image alignment of the generated results (81.4 to 83.17 on TIFA benchmark), while enabling credit attribution, copy-right protection, and deterministic, constant time, unlearning."],"url":"http://arxiv.org/abs/2403.18920v1","category":"cs.CR"}
{"created":"2024-03-27 18:02:49","title":"A Geometric Explanation of the Likelihood OOD Detection Paradox","abstract":"Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources. Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods. This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable. Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass. We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds. We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM. Our method can be applied to normalizing flows and score-based diffusion models, and obtains results which match or surpass state-of-the-art OOD detection benchmarks using the same DGM backbones. Our code is available at https://github.com/layer6ai-labs/dgm_ood_detection.","sentences":["Likelihood-based deep generative models (DGMs) commonly exhibit a puzzling behaviour: when trained on a relatively complex dataset, they assign higher likelihood values to out-of-distribution (OOD) data from simpler sources.","Adding to the mystery, OOD samples are never generated by these DGMs despite having higher likelihoods.","This two-pronged paradox has yet to be conclusively explained, making likelihood-based OOD detection unreliable.","Our primary observation is that high-likelihood regions will not be generated if they contain minimal probability mass.","We demonstrate how this seeming contradiction of large densities yet low probability mass can occur around data confined to low-dimensional manifolds.","We also show that this scenario can be identified through local intrinsic dimension (LID) estimation, and propose a method for OOD detection which pairs the likelihoods and LID estimates obtained from a pre-trained DGM.","Our method can be applied to normalizing flows and score-based diffusion models, and obtains results which match or surpass state-of-the-art OOD detection benchmarks using the same DGM backbones.","Our code is available at https://github.com/layer6ai-labs/dgm_ood_detection."],"url":"http://arxiv.org/abs/2403.18910v1","category":"cs.LG"}
{"created":"2024-03-28 17:59:49","title":"RSMamba: Remote Sensing Image Classification with State Space Model","abstract":"Remote sensing image classification forms the foundation of various understanding tasks, serving a crucial function in remote sensing image interpretation. The recent advancements of Convolutional Neural Networks (CNNs) and Transformers have markedly enhanced classification accuracy. Nonetheless, remote sensing scene classification remains a significant challenge, especially given the complexity and diversity of remote sensing scenarios and the variability of spatiotemporal resolutions. The capacity for whole-image understanding can provide more precise semantic cues for scene discrimination. In this paper, we introduce RSMamba, a novel architecture for remote sensing image classification. RSMamba is based on the State Space Model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba. It integrates the advantages of both a global receptive field and linear modeling complexity. To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to two-dimensional image data, we propose a dynamic multi-path activation mechanism to augment Mamba's capacity to model non-causal data. Notably, RSMamba maintains the inherent modeling mechanism of the vanilla Mamba, yet exhibits superior performance across multiple remote sensing image classification datasets. This indicates that RSMamba holds significant potential to function as the backbone of future visual foundation models. The code will be available at \\url{https://github.com/KyanChen/RSMamba}.","sentences":["Remote sensing image classification forms the foundation of various understanding tasks, serving a crucial function in remote sensing image interpretation.","The recent advancements of Convolutional Neural Networks (CNNs) and Transformers have markedly enhanced classification accuracy.","Nonetheless, remote sensing scene classification remains a significant challenge, especially given the complexity and diversity of remote sensing scenarios and the variability of spatiotemporal resolutions.","The capacity for whole-image understanding can provide more precise semantic cues for scene discrimination.","In this paper, we introduce RSMamba, a novel architecture for remote sensing image classification.","RSMamba is based on the State Space Model (SSM) and incorporates an efficient, hardware-aware design known as the Mamba.","It integrates the advantages of both a global receptive field and linear modeling complexity.","To overcome the limitation of the vanilla Mamba, which can only model causal sequences and is not adaptable to two-dimensional image data, we propose a dynamic multi-path activation mechanism to augment Mamba's capacity to model non-causal data.","Notably, RSMamba maintains the inherent modeling mechanism of the vanilla Mamba, yet exhibits superior performance across multiple remote sensing image classification datasets.","This indicates that RSMamba holds significant potential to function as the backbone of future visual foundation models.","The code will be available at \\url{https://github.com/KyanChen/RSMamba}."],"url":"http://arxiv.org/abs/2403.19654v1","category":"cs.CV"}
{"created":"2024-03-28 17:54:46","title":"Gaussian statistics for left and right eigenvectors of complex non-Hermitian matrices","abstract":"We consider a constant-size subset of left and right eigenvectors of an $N\\times N$ i.i.d. complex non-Hermitian matrix associated with the eigenvalues with pairwise distances at least $N^{-\\frac12+\\epsilon}$. We show that arbitrary constant rank projections of these eigenvectors are Gaussian and jointly independent.","sentences":["We consider a constant-size subset of left and right eigenvectors of an $N\\times N$ i.i.d. complex non-Hermitian matrix associated with the eigenvalues with pairwise distances at least $N^{-\\frac12+\\epsilon}$. We show that arbitrary constant rank projections of these eigenvectors are Gaussian and jointly independent."],"url":"http://arxiv.org/abs/2403.19644v1","category":"math.PR"}
{"created":"2024-03-28 17:49:31","title":"Asymmetric and trial-dependent modeling: the contribution of LIA to SdSV Challenge Task 2","abstract":"The SdSv challenge Task 2 provided an opportunity to assess efficiency and robustness of modern text-independent speaker verification systems. But it also made it possible to test new approaches, capable of taking into account the main issues of this challenge (duration, language, ...). This paper describes the contributions of our laboratory to the speaker recognition field. These contributions highlight two other challenges in addition to short-duration and language: the mismatch between enrollment and test data and the one between subsets of the evaluation trial dataset. The proposed approaches experimentally show their relevance and efficiency on the SdSv evaluation, and could be of interest in many real-life applications.","sentences":["The SdSv challenge Task 2 provided an opportunity to assess efficiency and robustness of modern text-independent speaker verification systems.","But it also made it possible to test new approaches, capable of taking into account the main issues of this challenge (duration, language, ...).","This paper describes the contributions of our laboratory to the speaker recognition field.","These contributions highlight two other challenges in addition to short-duration and language: the mismatch between enrollment and test data and the one between subsets of the evaluation trial dataset.","The proposed approaches experimentally show their relevance and efficiency on the SdSv evaluation, and could be of interest in many real-life applications."],"url":"http://arxiv.org/abs/2403.19634v1","category":"cs.SD"}
{"created":"2024-03-28 17:45:04","title":"Four-dimensional gradient Ricci solitons with (half) nonnegative isotropic curvature","abstract":"This is a sequel to our paper [24], in which we investigated the geometry of 4-dimensional gradient shrinking Ricci solitons with half positive (nonnegative) isotropic curvature. In this paper, we mainly focus on 4-dimensional gradient steady Ricci solitons with nonnegative isotropic curvature (WPIC) or half nonnegative isotropic curvature (half WPIC). In particular, for $4$D complete {\\it ancient solutions} with WPIC, we are able to prove the nonnegativity of the Ricci curvature $Rc\\geq 0$ and bound the curvature tensor $Rm$ by $|Rm| \\leq R$. For 4D gradient steady solitons with WPIC, we obtain a classification result. We also give a partial classification of 4D gradient steady Ricci solitons with half WPIC. Moreover, we obtain a preliminary classification result for 4D complete gradient {\\it expanding Ricci solitons} with WPIC. Finally, motivated by the recent work [60], we improve our earlier results in [24] on 4D gradient {\\it shrinking Ricci solitons} with half PIC or half WPIC, and also provide a characterization of complete gradient K\\\"ahler-Ricci shrinkers in complex dimension two among 4-dimensional gradient Ricci shrinkers.","sentences":["This is a sequel to our paper [24], in which we investigated the geometry of 4-dimensional gradient shrinking Ricci solitons with half positive (nonnegative) isotropic curvature.","In this paper, we mainly focus on 4-dimensional gradient steady Ricci solitons with nonnegative isotropic curvature (WPIC) or half nonnegative isotropic curvature (half WPIC).","In particular, for $4$D complete {\\it ancient solutions} with WPIC, we are able to prove the nonnegativity of the Ricci curvature $Rc\\geq 0$ and bound the curvature tensor $Rm$ by $|Rm| \\leq R$. For 4D gradient steady solitons with WPIC, we obtain a classification result.","We also give a partial classification of 4D gradient steady Ricci solitons with half WPIC.","Moreover, we obtain a preliminary classification result for 4D complete gradient {\\it expanding Ricci solitons} with WPIC.","Finally, motivated by the recent work [60], we improve our earlier results in [24] on 4D gradient {\\it shrinking Ricci solitons} with half PIC or half WPIC, and also provide a characterization of complete gradient K\\\"ahler-Ricci shrinkers in complex dimension two among 4-dimensional gradient Ricci shrinkers."],"url":"http://arxiv.org/abs/2403.19627v1","category":"math.DG"}
{"created":"2024-03-28 17:44:57","title":"Ghost cycles exhibit increased entrainment and richer dynamics in response to external forcing compared to slow-fast systems","abstract":"Many natural, living and engineered systems display oscillations that are characterized by multiple timescales. Typically, such systems are described as slow-fast systems, where the slow dynamics result from a hyperbolic slow manifold that guides the movement of the system trajectories. Recently, we have provided an alternative description in which the slow dynamics result from a non-hyperbolic and Lyapunov-unstable attracting sets from connected dynamical ghosts that form a closed orbit (termed ghost cycles). Here we investigate the response properties of both type of systems to external forcing. Using the classical Van-der-Pol oscillator and two modified versions of this model that correspond to a 1-ghost and a 2-ghost cycle, respectively, we find that ghost cycles are characterized by significant increase especially in the 1:1 entrainment regions as demonstrated by the corresponding Arnold tongues and exhibit richer dynamics (bursting, chaos) in contrast to the classical slow-fast system. Phase plane analysis reveals that these features result from the continuous remodeling of the attractor landscape of the ghost cycles models characteristic for non-autonomous systems, whereas the attractor landscape of the corresponding slow-fast system remains qualitatively unaltered. We propose that systems containing ghost cycles display increased flexibility and responsiveness to continuous environmental changes.","sentences":["Many natural, living and engineered systems display oscillations that are characterized by multiple timescales.","Typically, such systems are described as slow-fast systems, where the slow dynamics result from a hyperbolic slow manifold that guides the movement of the system trajectories.","Recently, we have provided an alternative description in which the slow dynamics result from a non-hyperbolic and Lyapunov-unstable attracting sets from connected dynamical ghosts that form a closed orbit (termed ghost cycles).","Here we investigate the response properties of both type of systems to external forcing.","Using the classical Van-der-Pol oscillator and two modified versions of this model that correspond to a 1-ghost and a 2-ghost cycle, respectively, we find that ghost cycles are characterized by significant increase especially in the 1:1 entrainment regions as demonstrated by the corresponding Arnold tongues and exhibit richer dynamics (bursting, chaos) in contrast to the classical slow-fast system.","Phase plane analysis reveals that these features result from the continuous remodeling of the attractor landscape of the ghost cycles models characteristic for non-autonomous systems, whereas the attractor landscape of the corresponding slow-fast system remains qualitatively unaltered.","We propose that systems containing ghost cycles display increased flexibility and responsiveness to continuous environmental changes."],"url":"http://arxiv.org/abs/2403.19624v1","category":"nlin.AO"}
{"created":"2024-03-28 17:44:41","title":"Generalisation of the Spectral Difference scheme for the diffused-interface five equation model","abstract":"The present work focuses on the generalisation of the Spectral Difference (SD) scheme to the reduced Baer-Nunziato system known as five-equation model for the simulation of two immiscible compressible fluids. This five equation model is considered with the additional Allen-Cahn regularisation to avoid both over-diffusion and over-thinning of the phase field representing the interface. Finally, in order to preserve contact discontinuities, in the reconstruction step of the spectral difference scheme, a change of variables from conservative to primitive is used. This approach is shown to be beneficial in avoiding pressure oscillations at material interfaces. An extensive series of numerical tests are proposed to assess accuracy and robustness of the present method. Both kinematic (Rider-Kothe vortex) and two-phase flow problems (Rayleigh-Taylor instability, shock-droplet interaction, Taylor-Green vortex) are considered.","sentences":["The present work focuses on the generalisation of the Spectral Difference (SD) scheme to the reduced Baer-Nunziato system known as five-equation model for the simulation of two immiscible compressible fluids.","This five equation model is considered with the additional Allen-Cahn regularisation to avoid both over-diffusion and over-thinning of the phase field representing the interface.","Finally, in order to preserve contact discontinuities, in the reconstruction step of the spectral difference scheme, a change of variables from conservative to primitive is used.","This approach is shown to be beneficial in avoiding pressure oscillations at material interfaces.","An extensive series of numerical tests are proposed to assess accuracy and robustness of the present method.","Both kinematic (Rider-Kothe vortex) and two-phase flow problems (Rayleigh-Taylor instability, shock-droplet interaction, Taylor-Green vortex) are considered."],"url":"http://arxiv.org/abs/2403.19623v1","category":"physics.flu-dyn"}
{"created":"2024-03-28 17:40:24","title":"Holomorphically conjugate polynomial automorphisms of C^2 are polynomially conjugate","abstract":"We confirm a conjecture of Friedland and Milnor: if two polynomial automorphisms f and g in Aut(C^2) with dynamical degree >1 are conjugate by some holomorphic diffeomorphism \\phi of C^2, then \\phi is a polynomial automorphism; thus, f and g are conjugate inside Aut(C^2). We also discuss a number of variations on this result.","sentences":["We confirm a conjecture of Friedland and Milnor: if two polynomial automorphisms f and g in Aut(C^2) with dynamical degree >1 are conjugate by some holomorphic diffeomorphism \\phi of C^2, then \\phi is a polynomial automorphism; thus, f and g are conjugate inside Aut(C^2).","We also discuss a number of variations on this result."],"url":"http://arxiv.org/abs/2403.19621v1","category":"math.DS"}
{"created":"2024-03-28 17:35:25","title":"Feedback Optimization of Incentives for Distribution Grid Services","abstract":"Energy prices and net power injection limitations regulate the operations in distribution grids and typically ensure that operational constraints are met. Nevertheless, unexpected or prolonged abnormal events could undermine the grid's functioning. During contingencies, customers could contribute effectively to sustaining the network by providing services. This paper proposes an incentive mechanism that promotes users' active participation by essentially altering the energy pricing rule. The incentives are modeled via a linear function whose parameters can be computed by the system operator (SO) by solving an optimization problem. Feedback-based optimization algorithms are then proposed to seek optimal incentives by leveraging measurements from the grid, even in the case when the SO does not have a full grid and customer information. Numerical simulations on a standard testbed validate the proposed approach.","sentences":["Energy prices and net power injection limitations regulate the operations in distribution grids and typically ensure that operational constraints are met.","Nevertheless, unexpected or prolonged abnormal events could undermine the grid's functioning.","During contingencies, customers could contribute effectively to sustaining the network by providing services.","This paper proposes an incentive mechanism that promotes users' active participation by essentially altering the energy pricing rule.","The incentives are modeled via a linear function whose parameters can be computed by the system operator (SO) by solving an optimization problem.","Feedback-based optimization algorithms are then proposed to seek optimal incentives by leveraging measurements from the grid, even in the case when the SO does not have a full grid and customer information.","Numerical simulations on a standard testbed validate the proposed approach."],"url":"http://arxiv.org/abs/2403.19616v1","category":"eess.SY"}
{"created":"2024-03-28 17:32:20","title":"NELIOTA: New results and updated statistics after 6.5 years of lunar impact flashes monitoring","abstract":"We present results of the NELIOTA campaign for lunar impact flashes observed with the 1.2 m Kryoneri telescope. From August 2019 to August 2023, we report 113 validated and 70 suspected flashes. For the validated flashes, we calculate the physical parameters of the corresponding projectiles, the temperatures developed during the impacts, and the expected crater sizes. For the multiframe flashes we present light curves and thermal evolution plots. Using the whole sample of NELIOTA that encompasses 192 validated flashes in total from 2017, the statistics of the physical parameters of the meteoroids, the peak temperatures of the impacts and the expected crater sizes has been updated. Using this large sample, empirical relations correlating the luminous energies per photometric band were derived and used to roughly estimate the parameters of 92 suspected flashes of the NELIOTA archive. For a typical value of the luminous efficiency, we found that more than the 75% of the impacting meteoroids have masses between 1-200 g, radii between 0.5-3 cm and produced craters up to 3.5 m. 85% of the peak temperatures of the impacts range between 2000 and 4500 K. Statistics regarding the magnitude decline and the cooling rates of the multiframe flashes are also presented. The recalculation of the appearance frequency of meteoroids, lying within the aforementioned ranges of physical parameters, on the Moon yields that the total lunar surface is bombarded with 7.4 sporadic meteoroids/ hour and up to 12.6 meteoroids/hour when the Earth-Moon system passes through a strong meteoroid stream. By extrapolating these rates on Earth, the respective rates for various distances from its surface are calculated and used to estimate the probability of an impact of a meteoroid with a hypothetical infrastructure on the Moon, or with a satellite orbiting Earth for various impact surfaces and duration times of the missions.","sentences":["We present results of the NELIOTA campaign for lunar impact flashes observed with the 1.2 m Kryoneri telescope.","From August 2019 to August 2023, we report 113 validated and 70 suspected flashes.","For the validated flashes, we calculate the physical parameters of the corresponding projectiles, the temperatures developed during the impacts, and the expected crater sizes.","For the multiframe flashes we present light curves and thermal evolution plots.","Using the whole sample of NELIOTA that encompasses 192 validated flashes in total from 2017, the statistics of the physical parameters of the meteoroids, the peak temperatures of the impacts and the expected crater sizes has been updated.","Using this large sample, empirical relations correlating the luminous energies per photometric band were derived and used to roughly estimate the parameters of 92 suspected flashes of the NELIOTA archive.","For a typical value of the luminous efficiency, we found that more than the 75% of the impacting meteoroids have masses between 1-200 g, radii between 0.5-3 cm and produced craters up to 3.5 m. 85% of the peak temperatures of the impacts range between 2000 and 4500 K. Statistics regarding the magnitude decline and the cooling rates of the multiframe flashes are also presented.","The recalculation of the appearance frequency of meteoroids, lying within the aforementioned ranges of physical parameters, on the Moon yields that the total lunar surface is bombarded with 7.4 sporadic meteoroids/ hour and up to 12.6 meteoroids/hour when the Earth-Moon system passes through a strong meteoroid stream.","By extrapolating these rates on Earth, the respective rates for various distances from its surface are calculated and used to estimate the probability of an impact of a meteoroid with a hypothetical infrastructure on the Moon, or with a satellite orbiting Earth for various impact surfaces and duration times of the missions."],"url":"http://arxiv.org/abs/2403.19613v1","category":"astro-ph.EP"}
{"created":"2024-03-28 17:31:23","title":"Nearest Neighbor Classication for Classical Image Upsampling","abstract":"Given a set of ordered pixel data in the form of an image, our goal is to perform upsampling on the data such that: the resulting resolution is improved by some factor, the final result passes the human test, having added new, believable, and realistic information and detail to the image, the time complexity for upscaling is relatively close to that of lossy upscaling implementations.","sentences":["Given a set of ordered pixel data in the form of an image, our goal is to perform upsampling on the data such that: the resulting resolution is improved by some factor, the final result passes the human test, having added new, believable, and realistic information and detail to the image, the time complexity for upscaling is relatively close to that of lossy upscaling implementations."],"url":"http://arxiv.org/abs/2403.19611v1","category":"cs.CV"}
{"created":"2024-03-28 17:26:48","title":"Behavior Trees in Industrial Applications: A Case Study in Underground Explosive Charging","abstract":"In industrial applications Finite State Machines (FSMs) are often used to implement decision making policies for autonomous systems. In recent years, the use of Behavior Trees (BT) as an alternative policy representation has gained considerable attention. The benefits of using BTs over FSMs are modularity and reusability, enabling a system that is easy to extend and modify. However, there exists few published studies on successful implementations of BTs for industrial applications. This paper contributes with the lessons learned from implementing BTs in a complex industrial use case, where a robotic system assembles explosive charges and places them in holes on the rock face. The main result of the paper is that even if it is possible to model the entire system as a BT, combining BTs with FSMs can increase the readability and maintainability of the system. The benefit of such combination is remarked especially in the use case studied in this paper, where the full system cannot run autonomously but human supervision and feedback are needed.","sentences":["In industrial applications Finite State Machines (FSMs) are often used to implement decision making policies for autonomous systems.","In recent years, the use of Behavior Trees (BT) as an alternative policy representation has gained considerable attention.","The benefits of using BTs over FSMs are modularity and reusability, enabling a system that is easy to extend and modify.","However, there exists few published studies on successful implementations of BTs for industrial applications.","This paper contributes with the lessons learned from implementing BTs in a complex industrial use case, where a robotic system assembles explosive charges and places them in holes on the rock face.","The main result of the paper is that even if it is possible to model the entire system as a BT, combining BTs with FSMs can increase the readability and maintainability of the system.","The benefit of such combination is remarked especially in the use case studied in this paper, where the full system cannot run autonomously but human supervision and feedback are needed."],"url":"http://arxiv.org/abs/2403.19602v1","category":"cs.RO"}
{"created":"2024-03-28 17:11:25","title":"Taming the Interactive Particle Langevin Algorithm -- the superlinear case","abstract":"Recent advances in stochastic optimization have yielded the interactive particle Langevin algorithm (IPLA), which leverages the notion of interacting particle systems (IPS) to efficiently sample from approximate posterior densities. This becomes particularly crucial within the framework of Expectation-Maximization (EM), where the E-step is computationally challenging or even intractable. Although prior research has focused on scenarios involving convex cases with gradients of log densities that grow at most linearly, our work extends this framework to include polynomial growth. Taming techniques are employed to produce an explicit discretization scheme that yields a new class of stable, under such non-linearities, algorithms which are called tamed interactive particle Langevin algorithms (tIPLA). We obtain non-asymptotic convergence error estimates in Wasserstein-2 distance for the new class under an optimal rate.","sentences":["Recent advances in stochastic optimization have yielded the interactive particle Langevin algorithm (IPLA), which leverages the notion of interacting particle systems (IPS) to efficiently sample from approximate posterior densities.","This becomes particularly crucial within the framework of Expectation-Maximization (EM), where the E-step is computationally challenging or even intractable.","Although prior research has focused on scenarios involving convex cases with gradients of log densities that grow at most linearly, our work extends this framework to include polynomial growth.","Taming techniques are employed to produce an explicit discretization scheme that yields a new class of stable, under such non-linearities, algorithms which are called tamed interactive particle Langevin algorithms (tIPLA).","We obtain non-asymptotic convergence error estimates in Wasserstein-2 distance for the new class under an optimal rate."],"url":"http://arxiv.org/abs/2403.19587v1","category":"math.PR"}
{"created":"2024-03-28 17:05:51","title":"A nontrivial uniform algebra Dirichlet on its maximal ideal space","abstract":"It is shown that there exists a nontrivial uniform algebra that is Dirichlet on its maximal ideal space and has a dense set of elements that are exponentials. This answers a 64-year-old question of John Wermer and a 16-year-old question of Garth Dales and Joel Feinstein. Our example is P(X) for a certain compact set X in complex Euclidean 2-space ($\\mathbb{C}^2$). It is also shown that there exists a logmodular uniform algebra with proper Shilov boundary but with no nontrivial Gleason parts. This answers a modified form of another 64-year-old question of Wermer.","sentences":["It is shown that there exists a nontrivial uniform algebra that is Dirichlet on its maximal ideal space and has a dense set of elements that are exponentials.","This answers a 64-year-old question of John Wermer and a 16-year-old question of Garth Dales and Joel Feinstein.","Our example is P(X) for a certain compact set X in complex Euclidean 2-space ($\\mathbb{C}^2$).","It is also shown that there exists a logmodular uniform algebra with proper Shilov boundary but with no nontrivial Gleason parts.","This answers a modified form of another 64-year-old question of Wermer."],"url":"http://arxiv.org/abs/2403.19583v1","category":"math.CV"}
{"created":"2024-03-28 17:05:24","title":"Tuning the intrinsic anomalous Hall effect from large to zero in the two ferromagnetic states of the layered SmMn2Ge2","abstract":"The intrinsic anomalous Hall conductivity in a ferromagnetic metal is completely determined by its band structure. Since the spin orientation direction is an important band structure tuning parameter, it is highly desirable to study the anomalous Hall effect in a system with multiple spin reorientation transitions. We study a layered tetragonal room temperature ferromagnet SmMn2Ge2, which gives us the opportunity to measure magneto-transport properties where the long c-axis and the short a-axis can both be magnetically easy axes depending on the temperature range we choose. We show a moderately large completely intrinsic anomalous Hall conductivity (AHC) up to room temperature when the crystal is magnetized along c-axis. Interestingly, the AHC can be tuned to completely extrinsic with extremely large values when the crystal is magnetized along a-axis irrespective of whether the a-axis is magnetically easy or hard axis. The first principles calculations show that nodal line states originate from Mn-d orbitals just below the Fermi energy (EF) in the electronic band structure when the spins are oriented along the c-axis. Intrinsic AHC originates form the Berry curvature effect of the gapped nodal lines in the presence of spin-orbit coupling. AHC almost vanishes when the spins are aligned along the a-axis because nodal line states shift above EF and become unoccupied states.","sentences":["The intrinsic anomalous Hall conductivity in a ferromagnetic metal is completely determined by its band structure.","Since the spin orientation direction is an important band structure tuning parameter, it is highly desirable to study the anomalous Hall effect in a system with multiple spin reorientation transitions.","We study a layered tetragonal room temperature ferromagnet SmMn2Ge2, which gives us the opportunity to measure magneto-transport properties where the long c-axis and the short a-axis can both be magnetically easy axes depending on the temperature range we choose.","We show a moderately large completely intrinsic anomalous Hall conductivity (AHC) up to room temperature when the crystal is magnetized along c-axis.","Interestingly, the AHC can be tuned to completely extrinsic with extremely large values when the crystal is magnetized along a-axis irrespective of whether the a-axis is magnetically easy or hard axis.","The first principles calculations show that nodal line states originate from Mn-d orbitals just below the Fermi energy (EF) in the electronic band structure when the spins are oriented along the c-axis.","Intrinsic AHC originates form the Berry curvature effect of the gapped nodal lines in the presence of spin-orbit coupling.","AHC almost vanishes when the spins are aligned along the a-axis because nodal line states shift above EF and become unoccupied states."],"url":"http://arxiv.org/abs/2403.19581v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-28 16:52:47","title":"GrINd: Grid Interpolation Network for Scattered Observations","abstract":"Predicting the evolution of spatiotemporal physical systems from sparse and scattered observational data poses a significant challenge in various scientific domains. Traditional methods rely on dense grid-structured data, limiting their applicability in scenarios with sparse observations. To address this challenge, we introduce GrINd (Grid Interpolation Network for Scattered Observations), a novel network architecture that leverages the high-performance of grid-based models by mapping scattered observations onto a high-resolution grid using a Fourier Interpolation Layer. In the high-resolution space, a NeuralPDE-class model predicts the system's state at future timepoints using differentiable ODE solvers and fully convolutional neural networks parametrizing the system's dynamics. We empirically evaluate GrINd on the DynaBench benchmark dataset, comprising six different physical systems observed at scattered locations, demonstrating its state-of-the-art performance compared to existing models. GrINd offers a promising approach for forecasting physical systems from sparse, scattered observational data, extending the applicability of deep learning methods to real-world scenarios with limited data availability.","sentences":["Predicting the evolution of spatiotemporal physical systems from sparse and scattered observational data poses a significant challenge in various scientific domains.","Traditional methods rely on dense grid-structured data, limiting their applicability in scenarios with sparse observations.","To address this challenge, we introduce GrINd (Grid Interpolation Network for Scattered Observations), a novel network architecture that leverages the high-performance of grid-based models by mapping scattered observations onto a high-resolution grid using a Fourier Interpolation Layer.","In the high-resolution space, a NeuralPDE-class model predicts the system's state at future timepoints using differentiable ODE solvers and fully convolutional neural networks parametrizing the system's dynamics.","We empirically evaluate GrINd on the DynaBench benchmark dataset, comprising six different physical systems observed at scattered locations, demonstrating its state-of-the-art performance compared to existing models.","GrINd offers a promising approach for forecasting physical systems from sparse, scattered observational data, extending the applicability of deep learning methods to real-world scenarios with limited data availability."],"url":"http://arxiv.org/abs/2403.19570v1","category":"cs.LG"}
{"created":"2024-03-28 16:52:37","title":"The motivic Hecke algebra for PEL Shimura varieties","abstract":"We construct a motivic lift of the action of the Hecke algebra on the cohomology of PEL Shimura varieties $S_K$. To do so, when $S_K$ is associated with a reductive algebraic group $G$ and $V$ is a local system on $S_K$ coming from a $G$-representation, we define a motivic Hecke algebra $\\mathcal{H}^M(G,K)$ as a natural sub-algebra of the endomorphism algebra, in the triangulated category of motives, of the constructible motive associated with $S_K$ and $V$. The algebra $\\mathcal{H}^M(G,K)$ is such that realizations induce an epimorphism from it onto the classical Hecke algebra. We then consider Wildeshaus' theory of interior motives, along with the necessary hypotheses for it to be employed. Whenever those assumptions hold, one gets a Chow motive realizing to interior $V$-valued cohomology of $S_K$, equipped with an action of $\\mathcal{H}^M(G,K)$ as an algebra of correspondences modulo rational equivalence. We give a list of known cases where this applies.","sentences":["We construct a motivic lift of the action of the Hecke algebra on the cohomology of PEL Shimura varieties $S_K$. To do so, when $S_K$ is associated with a reductive algebraic group $G$ and $V$ is a local system on $S_K$ coming from a $G$-representation, we define a motivic Hecke algebra $\\mathcal{H}^M(G,K)$ as a natural sub-algebra of the endomorphism algebra, in the triangulated category of motives, of the constructible motive associated with $S_K$ and $V$.","The algebra $\\mathcal{H}^M(G,K)$ is such that realizations induce an epimorphism from it onto the classical Hecke algebra.","We then consider Wildeshaus' theory of interior motives, along with the necessary hypotheses for it to be employed.","Whenever those assumptions hold, one gets a Chow motive realizing to interior $V$-valued cohomology of $S_K$, equipped with an action of $\\mathcal{H}^M(G,K)$ as an algebra of correspondences modulo rational equivalence.","We give a list of known cases where this applies."],"url":"http://arxiv.org/abs/2403.19568v1","category":"math.AG"}
{"created":"2024-03-28 16:51:19","title":"Poissonian Actions of Polish Groups","abstract":"We define and study Poissonian actions of Polish groups as a framework to Poisson suspensions, characterize them spectrally, and provide a complete characterization of their ergodicity. We further construct 'spatial' Poissonian actions, answering partially a question of Glasner, Tsirelson & Weiss about L\\'evy groups. We also construct for every diffeomorphism group an ergodic free spatial probability preserving actions. This constitutes a new class of Polish groups admitting non-essentially countable orbit equivalence relations, obtaining progress on a problem of Kechris.","sentences":["We define and study Poissonian actions of Polish groups as a framework to Poisson suspensions, characterize them spectrally, and provide a complete characterization of their ergodicity.","We further construct 'spatial' Poissonian actions, answering partially a question of Glasner, Tsirelson & Weiss about L\\'evy groups.","We also construct for every diffeomorphism group an ergodic free spatial probability preserving actions.","This constitutes a new class of Polish groups admitting non-essentially countable orbit equivalence relations, obtaining progress on a problem of Kechris."],"url":"http://arxiv.org/abs/2403.19567v1","category":"math.DS"}
{"created":"2024-03-28 16:38:04","title":"Cross-Attention is Not Always Needed: Dynamic Cross-Attention for Audio-Visual Dimensional Emotion Recognition","abstract":"In video-based emotion recognition, audio and visual modalities are often expected to have a complementary relationship, which is widely explored using cross-attention. However, they may also exhibit weak complementary relationships, resulting in poor representations of audio-visual features, thus degrading the performance of the system. To address this issue, we propose Dynamic Cross-Attention (DCA) that can dynamically select cross-attended or unattended features on the fly based on their strong or weak complementary relationship with each other, respectively. Specifically, a simple yet efficient gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit a strong complementary relationship, otherwise unattended features. We evaluate the performance of the proposed approach on the challenging RECOLA and Aff-Wild2 datasets. We also compare the proposed approach with other variants of cross-attention and show that the proposed model consistently improves the performance on both datasets.","sentences":["In video-based emotion recognition, audio and visual modalities are often expected to have a complementary relationship, which is widely explored using cross-attention.","However, they may also exhibit weak complementary relationships, resulting in poor representations of audio-visual features, thus degrading the performance of the system.","To address this issue, we propose Dynamic Cross-Attention (DCA) that can dynamically select cross-attended or unattended features on the fly based on their strong or weak complementary relationship with each other, respectively.","Specifically, a simple yet efficient gating layer is designed to evaluate the contribution of the cross-attention mechanism and choose cross-attended features only when they exhibit a strong complementary relationship, otherwise unattended features.","We evaluate the performance of the proposed approach on the challenging RECOLA and Aff-Wild2 datasets.","We also compare the proposed approach with other variants of cross-attention and show that the proposed model consistently improves the performance on both datasets."],"url":"http://arxiv.org/abs/2403.19554v1","category":"cs.CV"}
{"created":"2024-03-28 16:37:17","title":"Skoda-Zeriahi type integrability and entropy compactness for some measure with $L^1$-density","abstract":"In this paper, we prove the Skoda-Zeriahi type integrability theorem with respect to some measure with $L^1$-density. In addition, we introduce the log-log threshold in order to detect singularities of K\\\"{a}hler potentials. We prove the positivity of the integrability threshold for such a measure and K\\\"{a}hler potentials with uniform log-log threshold. As an application, we prove the entropy compactness theorem for a family of potential functions of Poincar\\'{e} type K\\\"{a}hler metrics with uniform log-log threshold. The Ohsawa-Takegoshi $L^2$-extension theorem and Skoda-Zeriahi's integrability theorem play a very important role in this paper.","sentences":["In this paper, we prove the Skoda-Zeriahi type integrability theorem with respect to some measure with $L^1$-density.","In addition, we introduce the log-log threshold in order to detect singularities of K\\\"{a}hler potentials.","We prove the positivity of the integrability threshold for such a measure and K\\\"{a}hler potentials with uniform log-log threshold.","As an application, we prove the entropy compactness theorem for a family of potential functions of Poincar\\'{e} type K\\\"{a}hler metrics with uniform log-log threshold.","The Ohsawa-Takegoshi $L^2$-extension theorem and Skoda-Zeriahi's integrability theorem play a very important role in this paper."],"url":"http://arxiv.org/abs/2403.19553v1","category":"math.DG"}
{"created":"2024-03-28 16:34:22","title":"Spectral gap for surfaces of infinite volume with negative curvature","abstract":"We prove that the imaginary parts of scattering resonances for negatively curved asymptotically hyperbolic surfaces are uniformly bounded away from zero and provide a resolvent bound in the resulting resonance-free strip. This provides an essential spectral gap without the pressure condition. This is done by adapting the methods of [arXiv:1004.3361], [arXiv:1012.4391] and [arXiv:2201.08259] and answers a question posed in [arXiv:1504.06589].","sentences":["We prove that the imaginary parts of scattering resonances for negatively curved asymptotically hyperbolic surfaces are uniformly bounded away from zero and provide a resolvent bound in the resulting resonance-free strip.","This provides an essential spectral gap without the pressure condition.","This is done by adapting the methods of [arXiv:1004.3361], [arXiv:1012.4391] and [arXiv:2201.08259] and answers a question posed in [arXiv:1504.06589]."],"url":"http://arxiv.org/abs/2403.19550v1","category":"math.SP"}
{"created":"2024-03-28 16:32:06","title":"GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM","abstract":"Recent advancements in RGB-only dense Simultaneous Localization and Mapping (SLAM) have predominantly utilized grid-based neural implicit encodings and/or struggle to efficiently realize global map and pose consistency. To this end, we propose an efficient RGB-only dense SLAM system using a flexible neural point cloud scene representation that adapts to keyframe poses and depth updates, without needing costly backpropagation. Another critical challenge of RGB-only SLAM is the lack of geometric priors. To alleviate this issue, with the aid of a monocular depth estimator, we introduce a novel DSPO layer for bundle adjustment which optimizes the pose and depth of keyframes along with the scale of the monocular depth. Finally, our system benefits from loop closure and online global bundle adjustment and performs either better or competitive to existing dense neural RGB SLAM methods in tracking, mapping and rendering accuracy on the Replica, TUM-RGBD and ScanNet datasets. The source code will be made available.","sentences":["Recent advancements in RGB-only dense Simultaneous Localization and Mapping (SLAM) have predominantly utilized grid-based neural implicit encodings and/or struggle to efficiently realize global map and pose consistency.","To this end, we propose an efficient RGB-only dense SLAM system using a flexible neural point cloud scene representation that adapts to keyframe poses and depth updates, without needing costly backpropagation.","Another critical challenge of RGB-only SLAM is the lack of geometric priors.","To alleviate this issue, with the aid of a monocular depth estimator, we introduce a novel DSPO layer for bundle adjustment which optimizes the pose and depth of keyframes along with the scale of the monocular depth.","Finally, our system benefits from loop closure and online global bundle adjustment and performs either better or competitive to existing dense neural RGB SLAM methods in tracking, mapping and rendering accuracy on the Replica, TUM-RGBD and ScanNet datasets.","The source code will be made available."],"url":"http://arxiv.org/abs/2403.19549v1","category":"cs.CV"}
{"created":"2024-03-28 16:27:03","title":"Impact of Resin Molecular Weight on Drying Kinetics and Sag of Coatings","abstract":"The work herein investigates the impact of resin molecular weight and solvent choice on drying kinetics and sag velocity in polymer films. These films, ranging in thickness from ~60 micrometer to ~120 micrometer were formulated with 45% by weight polymer resin in one of two solvent packages with different relative evaporation rates (RER). Gravimetry was initially used to track drying kinetics and a one-dimensional diffusion model was utilized to compute the apparent solvent diffusivity. In addition, the film thickness was tracked with optical profilometry. Results from these measurements showed that for fixed molecular weight the drying kinetics increased by approximately two-fold for the high RER solvent, whereas the apparent diffusivity tended to increase with increasing polymer molecular weight. Films formulated from higher molecular weight resins had greater initial viscosities and thicknesses for identical draw down blade clearance. By extension, the higher apparent diffusivities at greater molecular weights were attributed to effects of prolonged evaporation times for the thicker films. The sag velocity was measured through the thickness of the film for these systems at a 5 degree incline using the Variable Angle Inspection Microscope (VAIM). Measurements showed an increase in sag velocity for thinner and less viscous films, which was somewhat surprising both because a thinner film will experience lower gravitational stress and quicker drying times as compared to a thicker film. From these data we conclude that formulating a coating with higher molecular weight resin, although likely to increase drying time, will tend to deter sag because of the large impact of viscosity on these phenomena.","sentences":["The work herein investigates the impact of resin molecular weight and solvent choice on drying kinetics and sag velocity in polymer films.","These films, ranging in thickness from ~60 micrometer to ~120 micrometer were formulated with 45% by weight polymer resin in one of two solvent packages with different relative evaporation rates (RER).","Gravimetry was initially used to track drying kinetics and a one-dimensional diffusion model was utilized to compute the apparent solvent diffusivity.","In addition, the film thickness was tracked with optical profilometry.","Results from these measurements showed that for fixed molecular weight the drying kinetics increased by approximately two-fold for the high RER solvent, whereas the apparent diffusivity tended to increase with increasing polymer molecular weight.","Films formulated from higher molecular weight resins had greater initial viscosities and thicknesses for identical draw down blade clearance.","By extension, the higher apparent diffusivities at greater molecular weights were attributed to effects of prolonged evaporation times for the thicker films.","The sag velocity was measured through the thickness of the film for these systems at a 5 degree incline using the Variable Angle Inspection Microscope (VAIM).","Measurements showed an increase in sag velocity for thinner and less viscous films, which was somewhat surprising both because a thinner film will experience lower gravitational stress and quicker drying times as compared to a thicker film.","From these data we conclude that formulating a coating with higher molecular weight resin, although likely to increase drying time, will tend to deter sag because of the large impact of viscosity on these phenomena."],"url":"http://arxiv.org/abs/2403.19544v1","category":"cond-mat.soft"}
{"created":"2024-03-28 16:25:29","title":"Lattice outlook on $B\\to\u03c1\\ell\\bar\u03bd$ and $B\\to K^\\star \\ell \\ell$","abstract":"Lattice Quantum Chromodynamics (QCD) has significantly contributed to our understanding of the CKM matrix through precise determinations of hadronic matrix elements. With advancements in theoretical methodologies and computational resources, investigations can now extend to processes involving QCD-unstable hadrons such as the $\\rho$ and $K^\\star(892)$. These resonances play vital roles in processes such as weak decays of $B$ mesons, opening new avenues for exploration. Finite-volume lattice QCD techniques involving complex computational methods are used to determine the transition amplitudes. Here, we present preliminary results for $B\\to\\rho\\ell\\bar{\\nu}$.","sentences":["Lattice Quantum Chromodynamics (QCD) has significantly contributed to our understanding of the CKM matrix through precise determinations of hadronic matrix elements.","With advancements in theoretical methodologies and computational resources, investigations can now extend to processes involving QCD-unstable hadrons such as the $\\rho$ and $K^\\star(892)$. These resonances play vital roles in processes such as weak decays of $B$ mesons, opening new avenues for exploration.","Finite-volume lattice QCD techniques involving complex computational methods are used to determine the transition amplitudes.","Here, we present preliminary results for $B\\to\\rho\\ell\\bar{\\nu}$."],"url":"http://arxiv.org/abs/2403.19543v1","category":"hep-lat"}
{"created":"2024-03-28 15:44:20","title":"Quantum Realization of the Finite Element Method","abstract":"This paper presents a quantum algorithm for the solution of prototypical second-order linear elliptic partial differential equations discretized by $d$-linear finite elements on Cartesian grids of a bounded $d$-dimensional domain. An essential step in the construction is a BPX preconditioner, which transforms the linear system into a sufficiently well-conditioned one, making it amenable to quantum computation. We provide a constructive proof demonstrating that our quantum algorithm can compute suitable functionals of the solution to a given tolerance $\\texttt{tol}$ with a complexity linear in $\\texttt{tol}^{-1}$ for a fixed dimension $d$, neglecting logarithmic terms. This complexity is proportional to that of its one-dimensional counterpart and improves previous quantum algorithms by a factor of order $\\texttt{tol}^{-2}$. We also detail the design and implementation of a quantum circuit capable of executing our algorithm, and present simulator results that support the quantum feasibility of the finite element method in the near future, paving the way for quantum computing approaches to a wide range of PDE-related challenges.","sentences":["This paper presents a quantum algorithm for the solution of prototypical second-order linear elliptic partial differential equations discretized by $d$-linear finite elements on Cartesian grids of a bounded $d$-dimensional domain.","An essential step in the construction is a BPX preconditioner, which transforms the linear system into a sufficiently well-conditioned one, making it amenable to quantum computation.","We provide a constructive proof demonstrating that our quantum algorithm can compute suitable functionals of the solution to a given tolerance $\\texttt{tol}$ with a complexity linear in $\\texttt{tol}^{-1}$ for a fixed dimension $d$, neglecting logarithmic terms.","This complexity is proportional to that of its one-dimensional counterpart and improves previous quantum algorithms by a factor of order $\\texttt{tol}^{-2}$. We also detail the design and implementation of a quantum circuit capable of executing our algorithm, and present simulator results that support the quantum feasibility of the finite element method in the near future, paving the way for quantum computing approaches to a wide range of PDE-related challenges."],"url":"http://arxiv.org/abs/2403.19512v1","category":"quant-ph"}
{"created":"2024-03-28 15:41:41","title":"SineNet: Learning Temporal Dynamics in Time-Dependent Partial Differential Equations","abstract":"We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics. While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model's performance. To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves. In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage. We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information. Our method is rigorously tested on multiple PDE datasets, including the Navier-Stokes equations and shallow water equations, showcasing the advantages of our proposed approach over conventional U-Nets with a comparable parameter budget. We further demonstrate that increasing the number of waves in SineNet while maintaining the same number of parameters leads to a monotonically improved performance. The results highlight the effectiveness of SineNet and the potential of our approach in advancing the state-of-the-art in neural PDE solver design. Our code is available as part of AIRS (https://github.com/divelab/AIRS).","sentences":["We consider using deep neural networks to solve time-dependent partial differential equations (PDEs), where multi-scale processing is crucial for modeling complex, time-evolving dynamics.","While the U-Net architecture with skip connections is commonly used by prior studies to enable multi-scale processing, our analysis shows that the need for features to evolve across layers results in temporally misaligned features in skip connections, which limits the model's performance.","To address this limitation, we propose SineNet, consisting of multiple sequentially connected U-shaped network blocks, referred to as waves.","In SineNet, high-resolution features are evolved progressively through multiple stages, thereby reducing the amount of misalignment within each stage.","We furthermore analyze the role of skip connections in enabling both parallel and sequential processing of multi-scale information.","Our method is rigorously tested on multiple PDE datasets, including the Navier-Stokes equations and shallow water equations, showcasing the advantages of our proposed approach over conventional U-Nets with a comparable parameter budget.","We further demonstrate that increasing the number of waves in SineNet while maintaining the same number of parameters leads to a monotonically improved performance.","The results highlight the effectiveness of SineNet and the potential of our approach in advancing the state-of-the-art in neural PDE solver design.","Our code is available as part of AIRS (https://github.com/divelab/AIRS)."],"url":"http://arxiv.org/abs/2403.19507v1","category":"cs.LG"}
{"created":"2024-03-28 15:37:10","title":"LLMs as Academic Reading Companions: Extending HCI Through Synthetic Personae","abstract":"This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning. We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content. The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses. Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study. However, there is potential for overreliance and ethical considerations that warrant continued investigation. By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning. Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing.","sentences":["This position paper argues that large language models (LLMs) constitute promising yet underutilized academic reading companions capable of enhancing learning.","We detail an exploratory study examining Claude.ai from Anthropic, an LLM-based interactive assistant that helps students comprehend complex qualitative literature content.","The study compares quantitative survey data and qualitative interviews assessing outcomes between a control group and an experimental group leveraging Claude.ai over a semester across two graduate courses.","Initial findings demonstrate tangible improvements in reading comprehension and engagement among participants using the AI agent versus unsupported independent study.","However, there is potential for overreliance and ethical considerations that warrant continued investigation.","By documenting an early integration of an LLM reading companion into an educational context, this work contributes pragmatic insights to guide development of synthetic personae supporting learning.","Broader impacts compel policy and industry actions to uphold responsible design in order to maximize benefits of AI integration while prioritizing student wellbeing."],"url":"http://arxiv.org/abs/2403.19506v1","category":"cs.HC"}
{"created":"2024-03-28 15:31:36","title":"RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method","abstract":"Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes. Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data. However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements. For holistic human motion understanding, we present RELI11D, a high-quality multimodal human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera. It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams. Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location. To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy. We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance. Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field.","sentences":["Comprehensive capturing of human motions requires both accurate captures of complex poses and precise localization of the human within scenes.","Most of the HPE datasets and methods primarily rely on RGB, LiDAR, or IMU data.","However, solely using these modalities or a combination of them may not be adequate for HPE, particularly for complex and fast movements.","For holistic human motion understanding, we present RELI11D, a high-quality multimodal human motion dataset involves LiDAR, IMU system, RGB camera, and Event camera.","It records the motions of 10 actors performing 5 sports in 7 scenes, including 3.32 hours of synchronized LiDAR point clouds, IMU measurement data, RGB videos and Event steams.","Through extensive experiments, we demonstrate that the RELI11D presents considerable challenges and opportunities as it contains many rapid and complex motions that require precise location.","To address the challenge of integrating different modalities, we propose LEIR, a multimodal baseline that effectively utilizes LiDAR Point Cloud, Event stream, and RGB through our cross-attention fusion strategy.","We show that LEIR exhibits promising results for rapid motions and daily motions and that utilizing the characteristics of multiple modalities can indeed improve HPE performance.","Both the dataset and source code will be released publicly to the research community, fostering collaboration and enabling further exploration in this field."],"url":"http://arxiv.org/abs/2403.19501v1","category":"cs.CV"}
{"created":"2024-03-28 15:29:19","title":"Client-supervised Federated Learning: Towards One-model-for-all Personalization","abstract":"Personalized Federated Learning (PerFL) is a new machine learning paradigm that delivers personalized models for diverse clients under federated learning settings. Most PerFL methods require extra learning processes on a client to adapt a globally shared model to the client-specific personalized model using its own local data. However, the model adaptation process in PerFL is still an open challenge in the stage of model deployment and test time. This work tackles the challenge by proposing a novel federated learning framework to learn only one robust global model to achieve competitive performance to those personalized models on unseen/test clients in the FL system. Specifically, we design a new Client-Supervised Federated Learning (FedCS) to unravel clients' bias on instances' latent representations so that the global model can learn both client-specific and client-agnostic knowledge. Experimental study shows that the FedCS can learn a robust FL global model for the changing data distributions of unseen/test clients. The FedCS's global model can be directly deployed to the test clients while achieving comparable performance to other personalized FL methods that require model adaptation.","sentences":["Personalized Federated Learning (PerFL) is a new machine learning paradigm that delivers personalized models for diverse clients under federated learning settings.","Most PerFL methods require extra learning processes on a client to adapt a globally shared model to the client-specific personalized model using its own local data.","However, the model adaptation process in PerFL is still an open challenge in the stage of model deployment and test time.","This work tackles the challenge by proposing a novel federated learning framework to learn only one robust global model to achieve competitive performance to those personalized models on unseen/test clients in the FL system.","Specifically, we design a new Client-Supervised Federated Learning (FedCS) to unravel clients' bias on instances' latent representations so that the global model can learn both client-specific and client-agnostic knowledge.","Experimental study shows that the FedCS can learn a robust FL global model for the changing data distributions of unseen/test clients.","The FedCS's global model can be directly deployed to the test clients while achieving comparable performance to other personalized FL methods that require model adaptation."],"url":"http://arxiv.org/abs/2403.19499v1","category":"cs.LG"}
{"created":"2024-03-28 15:22:29","title":"Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment","abstract":"Structural model pruning is a prominent approach used for reducing the computational cost of Convolutional Neural Networks (CNNs) before their deployment on resource-constrained devices. Yet, the majority of proposed ideas require a pretrained model before pruning, which is costly to secure. In this paper, we propose a novel structural pruning approach to jointly learn the weights and structurally prune architectures of CNN models. The core element of our method is a Reinforcement Learning (RL) agent whose actions determine the pruning ratios of the CNN model's layers, and the resulting model's accuracy serves as its reward. We conduct the joint training and pruning by iteratively training the model's weights and the agent's policy, and we regularize the model's weights to align with the selected structure by the agent. The evolving model's weights result in a dynamic reward function for the agent, which prevents using prominent episodic RL methods with stationary environment assumption for our purpose. We address this challenge by designing a mechanism to model the complex changing dynamics of the reward function and provide a representation of it to the RL agent. To do so, we take a learnable embedding for each training epoch and employ a recurrent model to calculate a representation of the changing environment. We train the recurrent model and embeddings using a decoder model to reconstruct observed rewards. Such a design empowers our agent to effectively leverage episodic observations along with the environment representations to learn a proper policy to determine performant sub-networks of the CNN model. Our extensive experiments on CIFAR-10 and ImageNet using ResNets and MobileNets demonstrate the effectiveness of our method.","sentences":["Structural model pruning is a prominent approach used for reducing the computational cost of Convolutional Neural Networks (CNNs) before their deployment on resource-constrained devices.","Yet, the majority of proposed ideas require a pretrained model before pruning, which is costly to secure.","In this paper, we propose a novel structural pruning approach to jointly learn the weights and structurally prune architectures of CNN models.","The core element of our method is a Reinforcement Learning (RL) agent whose actions determine the pruning ratios of the CNN model's layers, and the resulting model's accuracy serves as its reward.","We conduct the joint training and pruning by iteratively training the model's weights and the agent's policy, and we regularize the model's weights to align with the selected structure by the agent.","The evolving model's weights result in a dynamic reward function for the agent, which prevents using prominent episodic RL methods with stationary environment assumption for our purpose.","We address this challenge by designing a mechanism to model the complex changing dynamics of the reward function and provide a representation of it to the RL agent.","To do so, we take a learnable embedding for each training epoch and employ a recurrent model to calculate a representation of the changing environment.","We train the recurrent model and embeddings using a decoder model to reconstruct observed rewards.","Such a design empowers our agent to effectively leverage episodic observations along with the environment representations to learn a proper policy to determine performant sub-networks of the CNN model.","Our extensive experiments on CIFAR-10 and ImageNet using ResNets and MobileNets demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2403.19490v1","category":"cs.CV"}
{"created":"2024-03-28 15:14:08","title":"Adaptive resolution of fine scales in modes of microstructured optical fibers","abstract":"An adaptive algorithm for computing eigenmodes and propagation constants of optical fibers is proposed. The algorithm is built using a dual-weighted residual error estimator. The residuals are based on the eigensystem for leaky hybrid modes obtained from Maxwell equations truncated to a finite domain after a transformation by a perfectly matched layer. The adaptive algorithm is then applied to compute practically interesting modes for multiple fiber microstructures. Emerging microstructured optical fibers are characterized by complex geometrical features in their transverse cross-section. Their leaky modes, useful for confining and propagating light in their cores, often exhibit fine scale features. The adaptive algorithm automatically captures these features without any expert input. The results also show that confinement losses of these modes are captured accurately on the adaptively found meshes.","sentences":["An adaptive algorithm for computing eigenmodes and propagation constants of optical fibers is proposed.","The algorithm is built using a dual-weighted residual error estimator.","The residuals are based on the eigensystem for leaky hybrid modes obtained from Maxwell equations truncated to a finite domain after a transformation by a perfectly matched layer.","The adaptive algorithm is then applied to compute practically interesting modes for multiple fiber microstructures.","Emerging microstructured optical fibers are characterized by complex geometrical features in their transverse cross-section.","Their leaky modes, useful for confining and propagating light in their cores, often exhibit fine scale features.","The adaptive algorithm automatically captures these features without any expert input.","The results also show that confinement losses of these modes are captured accurately on the adaptively found meshes."],"url":"http://arxiv.org/abs/2403.19485v1","category":"math.NA"}
{"created":"2024-03-28 15:14:03","title":"Improved Genetic Algorithm Based on Greedy and Simulated Annealing Ideas for Vascular Robot Ordering Strategy","abstract":"This study presents a comprehensive approach for optimizing the acquisition, utilization, and maintenance of ABLVR vascular robots in healthcare settings. Medical robotics, particularly in vascular treatments, necessitates precise resource allocation and optimization due to the complex nature of robot and operator maintenance. Traditional heuristic methods, though intuitive, often fail to achieve global optimization. To address these challenges, this research introduces a novel strategy, combining mathematical modeling, a hybrid genetic algorithm, and ARIMA time series forecasting. Considering the dynamic healthcare environment, our approach includes a robust resource allocation model for robotic vessels and operators. We incorporate the unique requirements of the adaptive learning process for operators and the maintenance needs of robotic components. The hybrid genetic algorithm, integrating simulated annealing and greedy approaches, efficiently solves the optimization problem. Additionally, ARIMA time series forecasting predicts the demand for vascular robots, further enhancing the adaptability of our strategy. Experimental results demonstrate the superiority of our approach in terms of optimization, transparency, and convergence speed from other state-of-the-art methods.","sentences":["This study presents a comprehensive approach for optimizing the acquisition, utilization, and maintenance of ABLVR vascular robots in healthcare settings.","Medical robotics, particularly in vascular treatments, necessitates precise resource allocation and optimization due to the complex nature of robot and operator maintenance.","Traditional heuristic methods, though intuitive, often fail to achieve global optimization.","To address these challenges, this research introduces a novel strategy, combining mathematical modeling, a hybrid genetic algorithm, and ARIMA time series forecasting.","Considering the dynamic healthcare environment, our approach includes a robust resource allocation model for robotic vessels and operators.","We incorporate the unique requirements of the adaptive learning process for operators and the maintenance needs of robotic components.","The hybrid genetic algorithm, integrating simulated annealing and greedy approaches, efficiently solves the optimization problem.","Additionally, ARIMA time series forecasting predicts the demand for vascular robots, further enhancing the adaptability of our strategy.","Experimental results demonstrate the superiority of our approach in terms of optimization, transparency, and convergence speed from other state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.19484v1","category":"cs.NE"}
{"created":"2024-03-28 14:59:56","title":"Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM","abstract":"Implicit neural representation (INR), in combination with geometric rendering, has recently been employed in real-time dense RGB-D SLAM. Despite active research endeavors being made, there lacks a unified protocol for fair evaluation, impeding the evolution of this area. In this work, we establish, to our knowledge, the first open-source benchmark framework to evaluate the performance of a wide spectrum of commonly used INRs and rendering functions for mapping and localization. The goal of our benchmark is to 1) gain an intuition of how different INRs and rendering functions impact mapping and localization and 2) establish a unified evaluation protocol w.r.t. the design choices that may impact the mapping and localization. With the framework, we conduct a large suite of experiments, offering various insights in choosing the INRs and geometric rendering functions: for example, the dense feature grid outperforms other INRs (e.g. tri-plane and hash grid), even when geometric and color features are jointly encoded for memory efficiency. To extend the findings into the practical scenario, a hybrid encoding strategy is proposed to bring the best of the accuracy and completion from the grid-based and decomposition-based INRs. We further propose explicit hybrid encoding for high-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts the premise on robustness and computation efficiency.","sentences":["Implicit neural representation (INR), in combination with geometric rendering, has recently been employed in real-time dense RGB-D SLAM.","Despite active research endeavors being made, there lacks a unified protocol for fair evaluation, impeding the evolution of this area.","In this work, we establish, to our knowledge, the first open-source benchmark framework to evaluate the performance of a wide spectrum of commonly used INRs and rendering functions for mapping and localization.","The goal of our benchmark is to 1) gain an intuition of how different INRs and rendering functions impact mapping and localization and 2) establish a unified evaluation protocol w.r.t.","the design choices that may impact the mapping and localization.","With the framework, we conduct a large suite of experiments, offering various insights in choosing the INRs and geometric rendering functions: for example, the dense feature grid outperforms other INRs (e.g. tri-plane and hash grid), even when geometric and color features are jointly encoded for memory efficiency.","To extend the findings into the practical scenario, a hybrid encoding strategy is proposed to bring the best of the accuracy and completion from the grid-based and decomposition-based INRs.","We further propose explicit hybrid encoding for high-fidelity dense grid mapping to comply with the RGB-D SLAM system that puts the premise on robustness and computation efficiency."],"url":"http://arxiv.org/abs/2403.19473v1","category":"cs.CV"}
{"created":"2024-03-28 14:51:06","title":"The phase curve of the ultra-hot Jupiter WASP-167b as seen by TESS","abstract":"Ultra-hot Jupiters (UHJs) orbiting pulsating A/F stars represent an important subset of the exoplanetary demographic, as they are excellent candidates for the study of exoplanetary atmospheres, as well as being astrophysical laboratories for the investigation of planet-to-star interactions. We analyse the \\texttt{TESS} (Transiting Exoplanet Survey Satellite) light curve of the WASP-167 system, consisting of an F1V star and a substellar companion on a $\\sim 2.02$ day orbit. We model the combination of the ellipsoidal variability and the Doppler beaming to measure the mass of WASP-167b, and the reflection effect to obtain constraints on the geometric albedo, while placing a special emphasis on noise separation. We implement a basic model to determine the dayside ($T_{\\rm Day}$), nightside ($T_{\\rm Night}$) and intrinsic ($T_{\\rm Internal}$) temperatures of WASP-167b and put a constraint on its Bond albedo. We confirm the transit parameters of the planet seen in the literature. We find that a resonant $\\sim 2P^{-1}$ stellar signal (which may originate from planet-to-star interactions) interferes with the phase curve analysis. After considerate treatment of this signal, we find $M_p = 0.34 \\pm 0.22$~$M_J$. We measure a dayside temperature of $2790 \\pm 100$ K, classifying WASP-167b as an UHJ. We find a $2\\sigma$ upper limit of $0.51$ on its Bond albedo, and determine the geometric albedo at $0.34 \\pm 0.11$ ($1 \\sigma$ uncertainty). With an occultation depth of $106.8 \\pm 27.3$ ppm in the \\texttt{TESS} passband, the UHJ WASP-167b will be an excellent target for atmospheric studies, especially those at thermal wavelength ranges, where the stellar pulsations are expected to be be less influential.","sentences":["Ultra-hot Jupiters (UHJs) orbiting pulsating A/F stars represent an important subset of the exoplanetary demographic, as they are excellent candidates for the study of exoplanetary atmospheres, as well as being astrophysical laboratories for the investigation of planet-to-star interactions.","We analyse the \\texttt{TESS} (Transiting Exoplanet Survey Satellite) light curve of the WASP-167 system, consisting of an F1V star and a substellar companion on a $\\sim 2.02$ day orbit.","We model the combination of the ellipsoidal variability and the Doppler beaming to measure the mass of WASP-167b, and the reflection effect to obtain constraints on the geometric albedo, while placing a special emphasis on noise separation.","We implement a basic model to determine the dayside ($T_{\\rm Day}$), nightside ($T_{\\rm Night}$) and intrinsic ($T_{\\rm Internal}$) temperatures of WASP-167b and put a constraint on its Bond albedo.","We confirm the transit parameters of the planet seen in the literature.","We find that a resonant $\\sim 2P^{-1}$ stellar signal (which may originate from planet-to-star interactions) interferes with the phase curve analysis.","After considerate treatment of this signal, we find $M_p = 0.34 \\pm 0.22$~$M_J$.","We measure a dayside temperature of $2790 \\pm 100$ K, classifying WASP-167b as an UHJ.","We find a $2\\sigma$ upper limit of $0.51$ on its Bond albedo, and determine the geometric albedo at $0.34 \\pm 0.11$ ($1 \\sigma$ uncertainty).","With an occultation depth of $106.8 \\pm 27.3$ ppm in the \\texttt{TESS} passband, the UHJ WASP-167b will be an excellent target for atmospheric studies, especially those at thermal wavelength ranges, where the stellar pulsations are expected to be be less influential."],"url":"http://arxiv.org/abs/2403.19468v1","category":"astro-ph.EP"}
{"created":"2024-03-28 14:38:11","title":"Quantitatively rating galaxy simulations against real observations with anomaly detection","abstract":"Cosmological galaxy formation simulations are powerful tools to understand the complex processes that govern the formation and evolution of galaxies. However, evaluating the realism of these simulations remains a challenge. The two common approaches for evaluating galaxy simulations is either through scaling relations based on a few key physical galaxy properties, or through a set of pre-defined morphological parameters based on galaxy images. This paper proposes a novel image-based method for evaluating the quality of galaxy simulations using unsupervised deep learning anomaly detection techniques. By comparing full galaxy images, our approach can identify and quantify discrepancies between simulated and observed galaxies. As a demonstration, we apply this method to SDSS imaging and NIHAO simulations with different physics models, parameters, and resolution. We further compare the metric of our method to scaling relations as well as morphological parameters. We show that anomaly detection is able to capture similarities and differences between real and simulated objects that scaling relations and morphological parameters are unable to cover, thus indeed providing a new point of view to validate and calibrate cosmological simulations against observed data.","sentences":["Cosmological galaxy formation simulations are powerful tools to understand the complex processes that govern the formation and evolution of galaxies.","However, evaluating the realism of these simulations remains a challenge.","The two common approaches for evaluating galaxy simulations is either through scaling relations based on a few key physical galaxy properties, or through a set of pre-defined morphological parameters based on galaxy images.","This paper proposes a novel image-based method for evaluating the quality of galaxy simulations using unsupervised deep learning anomaly detection techniques.","By comparing full galaxy images, our approach can identify and quantify discrepancies between simulated and observed galaxies.","As a demonstration, we apply this method to SDSS imaging and NIHAO simulations with different physics models, parameters, and resolution.","We further compare the metric of our method to scaling relations as well as morphological parameters.","We show that anomaly detection is able to capture similarities and differences between real and simulated objects that scaling relations and morphological parameters are unable to cover, thus indeed providing a new point of view to validate and calibrate cosmological simulations against observed data."],"url":"http://arxiv.org/abs/2403.19464v1","category":"astro-ph.GA"}
{"created":"2024-03-28 14:26:24","title":"Stabilization of a Class of Large-Scale Systems of Linear Hyperbolic PDEs via Continuum Approximation of Exact Backstepping Kernels","abstract":"We establish that stabilization of a class of linear, hyperbolic partial differential equations (PDEs) with a large (nevertheless finite) number of components, can be achieved via employment of a backstepping-based control law, which is constructed for stabilization of a continuum version (i.e., as the number of components tends to infinity) of the PDE system. This is achieved by proving that the exact backstepping kernels, constructed for stabilization of the large-scale system, can be approximated (in certain sense such that exponential stability is preserved) by the backstepping kernels constructed for stabilization of a continuum version (essentially an infinite ensemble) of the original PDE system. The proof relies on construction of a convergent sequence of backstepping kernels that is defined such that each kernel matches the exact backstepping kernels (derived based on the original, large-scale system), in a piecewise constant manner with respect to an ensemble variable; while showing that they satisfy the continuum backstepping kernel equations. We present a numerical example that reveals that complexity of computation of stabilizing backstepping kernels may not scale with the number of components of the PDE state, when the kernels are constructed on the basis of the continuum version, in contrast to the case in which they are constructed on the basis of the original, large-scale system. In addition, we formally establish the connection between the solutions to the large-scale system and its continuum counterpart. Thus, this approach can be useful for design of computationally tractable, stabilizing backstepping-based control laws for large-scale PDE systems.","sentences":["We establish that stabilization of a class of linear, hyperbolic partial differential equations (PDEs) with a large (nevertheless finite) number of components, can be achieved via employment of a backstepping-based control law, which is constructed for stabilization of a continuum version (i.e., as the number of components tends to infinity) of the PDE system.","This is achieved by proving that the exact backstepping kernels, constructed for stabilization of the large-scale system, can be approximated (in certain sense such that exponential stability is preserved) by the backstepping kernels constructed for stabilization of a continuum version (essentially an infinite ensemble) of the original PDE system.","The proof relies on construction of a convergent sequence of backstepping kernels that is defined such that each kernel matches the exact backstepping kernels (derived based on the original, large-scale system), in a piecewise constant manner with respect to an ensemble variable; while showing that they satisfy the continuum backstepping kernel equations.","We present a numerical example that reveals that complexity of computation of stabilizing backstepping kernels may not scale with the number of components of the PDE state, when the kernels are constructed on the basis of the continuum version, in contrast to the case in which they are constructed on the basis of the original, large-scale system.","In addition, we formally establish the connection between the solutions to the large-scale system and its continuum counterpart.","Thus, this approach can be useful for design of computationally tractable, stabilizing backstepping-based control laws for large-scale PDE systems."],"url":"http://arxiv.org/abs/2403.19455v1","category":"math.OC"}
{"created":"2024-03-28 14:22:36","title":"The submodularity of the covolume function in global function fields","abstract":"In this paper, we study the submodularity of the covolume function in global function fields. The submodular property is often needed in the study of homogeneous dynamics, especially to define a Margulis function. We proved that the covolume function is submodular when the class group of the global function field is trivial.","sentences":["In this paper, we study the submodularity of the covolume function in global function fields.","The submodular property is often needed in the study of homogeneous dynamics, especially to define a Margulis function.","We proved that the covolume function is submodular when the class group of the global function field is trivial."],"url":"http://arxiv.org/abs/2403.19453v1","category":"math.NT"}
{"created":"2024-03-28 14:11:40","title":"A Novel Stochastic Transformer-based Approach for Post-Traumatic Stress Disorder Detection using Audio Recording of Clinical Interviews","abstract":"Post-traumatic stress disorder (PTSD) is a mental disorder that can be developed after witnessing or experiencing extremely traumatic events. PTSD can affect anyone, regardless of ethnicity, or culture. An estimated one in every eleven people will experience PTSD during their lifetime. The Clinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians (PCL-C) interviews are gold standards in the diagnosis of PTSD. These questionnaires can be fooled by the subject's responses. This work proposes a deep learning-based approach that achieves state-of-the-art performances for PTSD detection using audio recordings during clinical interviews. Our approach is based on MFCC low-level features extracted from audio recordings of clinical interviews, followed by deep high-level learning using a Stochastic Transformer. Our proposed approach achieves state-of-the-art performances with an RMSE of 2.92 on the eDAIC dataset thanks to the stochastic depth, stochastic deep learning layers, and stochastic activation function.","sentences":["Post-traumatic stress disorder (PTSD) is a mental disorder that can be developed after witnessing or experiencing extremely traumatic events.","PTSD can affect anyone, regardless of ethnicity, or culture.","An estimated one in every eleven people will experience PTSD during their lifetime.","The Clinician-Administered PTSD Scale (CAPS) and the PTSD Check List for Civilians (PCL-C) interviews are gold standards in the diagnosis of PTSD.","These questionnaires can be fooled by the subject's responses.","This work proposes a deep learning-based approach that achieves state-of-the-art performances for PTSD detection using audio recordings during clinical interviews.","Our approach is based on MFCC low-level features extracted from audio recordings of clinical interviews, followed by deep high-level learning using a Stochastic Transformer.","Our proposed approach achieves state-of-the-art performances with an RMSE of 2.92 on the eDAIC dataset thanks to the stochastic depth, stochastic deep learning layers, and stochastic activation function."],"url":"http://arxiv.org/abs/2403.19441v1","category":"cs.SD"}
{"created":"2024-03-28 14:11:40","title":"Exploiting Individual Graph Structures to Enhance Ecological Momentary Assessment (EMA) Forecasting","abstract":"In the evolving field of psychopathology, the accurate assessment and forecasting of data derived from Ecological Momentary Assessment (EMA) is crucial. EMA offers contextually-rich psychopathological measurements over time, that practically lead to Multivariate Time Series (MTS) data. Thus, many challenges arise in analysis from the temporal complexities inherent in emotional, behavioral, and contextual EMA data as well as their inter-dependencies. To address both of these aspects, this research investigates the performance of Recurrent and Temporal Graph Neural Networks (GNNs). Overall, GNNs, by incorporating additional information from graphs reflecting the inner relationships between the variables, notably enhance the results by decreasing the Mean Squared Error (MSE) to 0.84 compared to the baseline LSTM model at 1.02. Therefore, the effect of constructing graphs with different characteristics on GNN performance is also explored. Additionally, GNN-learned graphs, which are dynamically refined during the training process, were evaluated. Using such graphs showed a similarly good performance. Thus, graph learning proved also promising for other GNN methods, potentially refining the pre-defined graphs.","sentences":["In the evolving field of psychopathology, the accurate assessment and forecasting of data derived from Ecological Momentary Assessment (EMA) is crucial.","EMA offers contextually-rich psychopathological measurements over time, that practically lead to Multivariate Time Series (MTS) data.","Thus, many challenges arise in analysis from the temporal complexities inherent in emotional, behavioral, and contextual EMA data as well as their inter-dependencies.","To address both of these aspects, this research investigates the performance of Recurrent and Temporal Graph Neural Networks (GNNs).","Overall, GNNs, by incorporating additional information from graphs reflecting the inner relationships between the variables, notably enhance the results by decreasing the Mean Squared Error (MSE) to 0.84 compared to the baseline LSTM model at 1.02.","Therefore, the effect of constructing graphs with different characteristics on GNN performance is also explored.","Additionally, GNN-learned graphs, which are dynamically refined during the training process, were evaluated.","Using such graphs showed a similarly good performance.","Thus, graph learning proved also promising for other GNN methods, potentially refining the pre-defined graphs."],"url":"http://arxiv.org/abs/2403.19442v1","category":"cs.LG"}
{"created":"2024-03-28 14:07:40","title":"Dynamic Analyses of Contagion Risk and Module Evolution on the SSE A-Shares Market Based on Minimum Information Entropy","abstract":"The interactive effect is significant in the Chinese stock market, exacerbating the abnormal market volatilities and risk contagion. Based on daily stock returns in the Shanghai Stock Exchange (SSE) A-shares, this paper divides the period between 2005 and 2018 into eight bull and bear market stages to investigate interactive patterns in the Chinese financial market. We employ the LASSO method to construct the stock network and further use the Map Equation method to analyze the evolution of modules in the SSE A-shares market. Empirical results show: (1) The connected effect is more significant in bear markets than bull markets; (2) A system module can be found in the network during the first four stages, and the industry aggregation effect leads to module differentiation in the last four stages; (3) Some stocks have leading effects on others throughout eight periods, and medium- and small-cap stocks with poor financial conditions are more likely to become risk sources, especially in bear markets. Our conclusions are beneficial to improving investment strategies and making regulatory policies.","sentences":["The interactive effect is significant in the Chinese stock market, exacerbating the abnormal market volatilities and risk contagion.","Based on daily stock returns in the Shanghai Stock Exchange (SSE) A-shares, this paper divides the period between 2005 and 2018 into eight bull and bear market stages to investigate interactive patterns in the Chinese financial market.","We employ the LASSO method to construct the stock network and further use the Map Equation method to analyze the evolution of modules in the SSE A-shares market.","Empirical results show: (1) The connected effect is more significant in bear markets than bull markets; (2) A system module can be found in the network during the first four stages, and the industry aggregation effect leads to module differentiation in the last four stages; (3) Some stocks have leading effects on others throughout eight periods, and medium- and small-cap stocks with poor financial conditions are more likely to become risk sources, especially in bear markets.","Our conclusions are beneficial to improving investment strategies and making regulatory policies."],"url":"http://arxiv.org/abs/2403.19439v1","category":"econ.EM"}
{"created":"2024-03-28 14:03:45","title":"ATMOSPHERIX: III- Estimating the C/O ratio and molecular dynamics at the limbs of WASP-76 b with SPIRou","abstract":"Measuring the abundances of C- and O-bearing species in exoplanet atmospheres enables us to constrain the C/O ratio, that contains indications about the planet formation history. With a wavelength coverage going from 0.95 to 2.5 microns, the high-resolution (R$\\sim$70 000) spectropolarimeter SPIRou can detect spectral lines of major bearers of C and O in exoplanets. Here we present our study of SPIRou transmission spectra of WASP-76 b acquired for the ATMOSPHERIX program. We applied the publicly available data analysis pipeline developed within the ATMOSPHERIX consortium, analysing the data using 1-D models created with the petitRADTRANS code, with and without a grey cloud deck. We report the detection of H$_2$O and CO at a Doppler shift of around -6 km.s$^{-1}$, consistent with previous observations of the planet. Finding a deep cloud deck to be favoured, we measured in mass mixing ratio (MMR) log(H$_2$O)$_{MMR}$ = -4.52 $\\pm$ 0.77 and log(CO)$_{MMR}$ = -3.09 $\\pm$ 1.05 consistent with a sub-solar metallicity to more than 1$\\sigma$. We report 3$\\sigma$ upper limits for the abundances of C$_2$H$_2$, HCN and OH. We estimated a C/O ratio of 0.94 $\\pm$ 0.39 ($\\sim$ 1.7 $\\pm$ 0.7 x solar, with errors indicated corresponding to the 2$\\sigma$ values) for the limbs of WASP-76 b at the pressures probed by SPIRou. We used 1-D ATMO forward models to verify the validity of our estimation. Comparing them to our abundance estimations of H$_2$O and CO, as well as our upper limits for C$_2$H$_2$, HCN and OH, we found that our results were consistent with a C/O ratio between 1 and 2 x solar, and hence with our C/O estimation. Finally, we found indications of asymmetry for both H$_2$O and CO when investigating the dynamics of their signatures, pointing to a complex scenario involving possibly both a temperature difference between limbs and clouds being behind the asymmetry this planet is best known for.","sentences":["Measuring the abundances of C- and O-bearing species in exoplanet atmospheres enables us to constrain the C/O ratio, that contains indications about the planet formation history.","With a wavelength coverage going from 0.95 to 2.5 microns, the high-resolution (R$\\sim$70 000) spectropolarimeter SPIRou can detect spectral lines of major bearers of C and O in exoplanets.","Here we present our study of SPIRou transmission spectra of WASP-76 b acquired for the ATMOSPHERIX program.","We applied the publicly available data analysis pipeline developed within the ATMOSPHERIX consortium, analysing the data using 1-D models created with the petitRADTRANS code, with and without a grey cloud deck.","We report the detection of H$_2$O and CO at a Doppler shift of around -6 km.s$^{-1}$, consistent with previous observations of the planet.","Finding a deep cloud deck to be favoured, we measured in mass mixing ratio (MMR) log(H$_2$O)$_{MMR}$ = -4.52 $\\pm$ 0.77 and log(CO)$_{MMR}$ = -3.09 $\\pm$ 1.05 consistent with a sub-solar metallicity to more than 1$\\sigma$. We report 3$\\sigma$ upper limits for the abundances of C$_2$H$_2$, HCN and OH.","We estimated a C/O ratio of 0.94 $\\pm$ 0.39 ($\\sim$ 1.7 $\\pm$ 0.7 x solar, with errors indicated corresponding to the 2$\\sigma$ values) for the limbs of WASP-76 b at the pressures probed by SPIRou.","We used 1-D ATMO forward models to verify the validity of our estimation.","Comparing them to our abundance estimations of H$_2$O and CO, as well as our upper limits for C$_2$H$_2$, HCN and OH, we found that our results were consistent with a C/O ratio between 1 and 2 x solar, and hence with our C/O estimation.","Finally, we found indications of asymmetry for both H$_2$O and CO when investigating the dynamics of their signatures, pointing to a complex scenario involving possibly both a temperature difference between limbs and clouds being behind the asymmetry this planet is best known for."],"url":"http://arxiv.org/abs/2403.19434v1","category":"astro-ph.EP"}
{"created":"2024-03-28 13:59:28","title":"Coexistence of non-Hermitian skin effect and extended states in one-dimensional nonreciprocal lattices","abstract":"We study the one-dimensional non-Hermitian lattices with staggered onsite modulations and nonreciprocal hopping up to the next-nearest-neighboring (NNN) sites. Due to the NNN nonreciprocity, the non-Hermitian skin effect (NHSE) in the system under open boundary conditions (OBC) can be energy-dependent, and there will be NHSE edges in the eigenenergy spectrum, which separates the eigenstates localized at the opposite ends of the lattice. We find that the interplay between the nonreciprocal hopping and onsite modulations can reverse the direction of the skin effect and modify the position of the NHSE edge. Moreover, by tuning the system parameters, some of the eigenstates under OBC will become fully extended with the corresponding eigenenergies being imaginary under both open and periodic boundary conditions. Thus, the extended states can coexist with the NHSH in the same system. The NHSE can even be completely dissolved with all the eigenstates being extended when the modulation is imaginary. Our work unveils the intricate interplay between onsite modulations and nonreciprocal hopping in non-Hermitian systems.","sentences":["We study the one-dimensional non-Hermitian lattices with staggered onsite modulations and nonreciprocal hopping up to the next-nearest-neighboring (NNN) sites.","Due to the NNN nonreciprocity, the non-Hermitian skin effect (NHSE) in the system under open boundary conditions (OBC) can be energy-dependent, and there will be NHSE edges in the eigenenergy spectrum, which separates the eigenstates localized at the opposite ends of the lattice.","We find that the interplay between the nonreciprocal hopping and onsite modulations can reverse the direction of the skin effect and modify the position of the NHSE edge.","Moreover, by tuning the system parameters, some of the eigenstates under OBC will become fully extended with the corresponding eigenenergies being imaginary under both open and periodic boundary conditions.","Thus, the extended states can coexist with the NHSH in the same system.","The NHSE can even be completely dissolved with all the eigenstates being extended when the modulation is imaginary.","Our work unveils the intricate interplay between onsite modulations and nonreciprocal hopping in non-Hermitian systems."],"url":"http://arxiv.org/abs/2403.19430v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 13:36:00","title":"A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization","abstract":"Event cameras exhibit remarkable attributes such as high dynamic range, asynchronicity, and low latency, making them highly suitable for vision tasks that involve high-speed motion in challenging lighting conditions. These cameras implicitly capture movement and depth information in events, making them appealing sensors for Camera Pose Relocalization (CPR) tasks. Nevertheless, existing CPR networks based on events neglect the pivotal fine-grained temporal information in events, resulting in unsatisfactory performance. Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices. In this paper, we introduce PEPNet, a simple and effective point-based network designed to regress six degrees of freedom (6-DOFs) event camera poses. We rethink the relationship between the event camera and CPR tasks, leveraging the raw Point Cloud directly as network input to harness the high-temporal resolution and inherent sparsity of events. PEPNet is adept at abstracting the spatial and implicit temporal features through hierarchical structure and explicit temporal features by Attentive Bi-directional Long Short-Term Memory (A-Bi-LSTM). By employing a carefully crafted lightweight design, PEPNet delivers state-of-the-art (SOTA) performance on both indoor and outdoor datasets with meager computational resources. Specifically, PEPNet attains a significant 38% and 33% performance improvement on the random split IJRR and M3ED datasets, respectively. Moreover, the lightweight design version PEPNet$_{tiny}$ accomplishes results comparable to the SOTA while employing a mere 0.5% of the parameters.","sentences":["Event cameras exhibit remarkable attributes such as high dynamic range, asynchronicity, and low latency, making them highly suitable for vision tasks that involve high-speed motion in challenging lighting conditions.","These cameras implicitly capture movement and depth information in events, making them appealing sensors for Camera Pose Relocalization (CPR) tasks.","Nevertheless, existing CPR networks based on events neglect the pivotal fine-grained temporal information in events, resulting in unsatisfactory performance.","Moreover, the energy-efficient features are further compromised by the use of excessively complex models, hindering efficient deployment on edge devices.","In this paper, we introduce PEPNet, a simple and effective point-based network designed to regress six degrees of freedom (6-DOFs) event camera poses.","We rethink the relationship between the event camera and CPR tasks, leveraging the raw Point Cloud directly as network input to harness the high-temporal resolution and inherent sparsity of events.","PEPNet is adept at abstracting the spatial and implicit temporal features through hierarchical structure and explicit temporal features by Attentive Bi-directional Long Short-Term Memory (A-Bi-LSTM).","By employing a carefully crafted lightweight design, PEPNet delivers state-of-the-art (SOTA) performance on both indoor and outdoor datasets with meager computational resources.","Specifically, PEPNet attains a significant 38% and 33% performance improvement on the random split IJRR and M3ED datasets, respectively.","Moreover, the lightweight design version PEPNet$_{tiny}$ accomplishes results comparable to the SOTA while employing a mere 0.5% of the parameters."],"url":"http://arxiv.org/abs/2403.19412v1","category":"cs.CV"}
{"created":"2024-03-28 13:28:08","title":"Efficient rational approximation of optical response functions with the AAA algorithm","abstract":"We introduce a theoretical framework for the rational approximation of optical response functions in resonant photonic systems. The framework is based on the AAA algorithm and further allows to solve the underlying nonlinear eigenproblems and to efficiently model sensitivities. An adaptive sampling strategy exploits the predominance of resonances in the physical response. We investigate a chiral metasurface and show that the chiroptical response on parameter variations can be accurately modeled in the vicinity of the relevant resonance frequencies.","sentences":["We introduce a theoretical framework for the rational approximation of optical response functions in resonant photonic systems.","The framework is based on the AAA algorithm and further allows to solve the underlying nonlinear eigenproblems and to efficiently model sensitivities.","An adaptive sampling strategy exploits the predominance of resonances in the physical response.","We investigate a chiral metasurface and show that the chiroptical response on parameter variations can be accurately modeled in the vicinity of the relevant resonance frequencies."],"url":"http://arxiv.org/abs/2403.19404v1","category":"physics.optics"}
{"created":"2024-03-28 13:26:48","title":"Non-real zeros of derivatives in the unit disc","abstract":"The main result establishes an estimate for the growth of a real meromorphic function $f$ on the unit disc $\\Delta$ such that: (i) at least one of $f$ and $1/f$ has finitely many poles and non-real zeros in $\\Delta$; (ii)~$f^{(k)}$ has finitely many non-real zeros in $\\Delta$, for some $k \\geq 2$.","sentences":["The main result establishes an estimate for the growth of a real meromorphic function $f$ on the unit disc $\\Delta$ such that: (i) at least one of $f$ and $1/f$ has finitely many poles and non-real zeros in $\\Delta$; (ii)~$f^{(k)}$ has finitely many non-real zeros in $\\Delta$, for some $k \\geq 2$."],"url":"http://arxiv.org/abs/2403.19403v1","category":"math.CV"}
{"created":"2024-03-28 13:26:31","title":"V2X Enabled Emergency Vehicle Alert System","abstract":"Today's major concern in traffic management systems includes time-efficient emergency transports. The awareness of environment and vehicle information is necessary for the emergency vehicles as well as the surrounding commercial vehicles that might be driven by inexperienced drivers to act accordingly if they both interact. The information exchange should be quick and accurate along with how much interactive the alerting system is with the drivers. Therefore, technologies like V2X-based alert systems can deal with such emergency situations and hence prevent potential health or social hazards. An alerting system as a part of a smart-connected city is proposed in this paper. The Dedicated Short Range Communication (DSRC) based system has tried to cover the major domain of information about misbehaving vehicles, any pedestrians on the road, and information about the emergency vehicle itself. The commercial vehicle also will have a similar alert system as an application of V2V and V2I. Further in this paper, a realtime monitoring system was developed using grafana dashboard which will be installed in the area's base station to monitor the vehicles in that area.","sentences":["Today's major concern in traffic management systems includes time-efficient emergency transports.","The awareness of environment and vehicle information is necessary for the emergency vehicles as well as the surrounding commercial vehicles that might be driven by inexperienced drivers to act accordingly if they both interact.","The information exchange should be quick and accurate along with how much interactive the alerting system is with the drivers.","Therefore, technologies like V2X-based alert systems can deal with such emergency situations and hence prevent potential health or social hazards.","An alerting system as a part of a smart-connected city is proposed in this paper.","The Dedicated Short Range Communication (DSRC) based system has tried to cover the major domain of information about misbehaving vehicles, any pedestrians on the road, and information about the emergency vehicle itself.","The commercial vehicle also will have a similar alert system as an application of V2V and V2I. Further in this paper, a realtime monitoring system was developed using grafana dashboard which will be installed in the area's base station to monitor the vehicles in that area."],"url":"http://arxiv.org/abs/2403.19402v1","category":"eess.SP"}
{"created":"2024-03-28 13:18:49","title":"Clustering MOOC Programming Solutions to Diversify Their Presentation to Students","abstract":"In many MOOCs, whenever a student completes a programming task, they can see previous solutions of other students to find potentially different ways of solving the problem and learn new coding constructs. However, a lot of MOOCs simply show the most recent solutions, disregarding their diversity or quality.   To solve this novel problem, we adapted the existing plagiarism detection tool JPlag to Python submissions on Hyperskill, a popular MOOC platform. However, due to the tool's inner algorithm, it fully processed only 46 out of 867 studied tasks. Therefore, we developed our own tool called Rhubarb. This tool first standardizes solutions that are algorithmically the same, then calculates the structure-aware edit distance between them, and then applies clustering. Finally, it selects one example from each of the largest clusters, taking into account their code quality. Rhubarb was able to handle all 867 tasks successfully.   We compared approaches on a set of 59 tasks that both tools could process. Eight experts rated the selected solutions based on diversity, code quality, and usefulness. The default platform approach of selecting recent submissions received on average 3.12 out of 5, JPlag - 3.77, Rhubarb - 3.50. Since in the real MOOC, it is imperative to process everything, we created a system that uses JPlag on the 5.3% of tasks it fully processes and Rhubarb on the remaining 94.7%.","sentences":["In many MOOCs, whenever a student completes a programming task, they can see previous solutions of other students to find potentially different ways of solving the problem and learn new coding constructs.","However, a lot of MOOCs simply show the most recent solutions, disregarding their diversity or quality.   ","To solve this novel problem, we adapted the existing plagiarism detection tool JPlag to Python submissions on Hyperskill, a popular MOOC platform.","However, due to the tool's inner algorithm, it fully processed only 46 out of 867 studied tasks.","Therefore, we developed our own tool called Rhubarb.","This tool first standardizes solutions that are algorithmically the same, then calculates the structure-aware edit distance between them, and then applies clustering.","Finally, it selects one example from each of the largest clusters, taking into account their code quality.","Rhubarb was able to handle all 867 tasks successfully.   ","We compared approaches on a set of 59 tasks that both tools could process.","Eight experts rated the selected solutions based on diversity, code quality, and usefulness.","The default platform approach of selecting recent submissions received on average 3.12 out of 5, JPlag - 3.77, Rhubarb - 3.50.","Since in the real MOOC, it is imperative to process everything, we created a system that uses JPlag on the 5.3% of tasks it fully processes and Rhubarb on the remaining 94.7%."],"url":"http://arxiv.org/abs/2403.19398v1","category":"cs.SE"}
{"created":"2024-03-28 12:41:01","title":"The image of random analytic functions: coverage of the complex plane via branching processes","abstract":"We consider the range of random analytic functions with finite radius of convergence. We show that any unbounded random Taylor series with rotationally invariant coefficients has dense image in the plane. We moreover show that if in addition the coefficients are complex Gaussian with sufficiently regular variances, then the image is the whole complex plane. We do this by exploiting an approximate connection between the coverage problem and spatial branching processes. This answers a long-standing open question of J.-P. Kahane, with sufficient regularity.","sentences":["We consider the range of random analytic functions with finite radius of convergence.","We show that any unbounded random Taylor series with rotationally invariant coefficients has dense image in the plane.","We moreover show that if in addition the coefficients are complex Gaussian with sufficiently regular variances, then the image is the whole complex plane.","We do this by exploiting an approximate connection between the coverage problem and spatial branching processes.","This answers a long-standing open question of J.-P. Kahane, with sufficient regularity."],"url":"http://arxiv.org/abs/2403.19380v1","category":"math.PR"}
{"created":"2024-03-28 12:36:31","title":"A noise-tolerant, resource-saving probabilistic binary neural network implemented by the SOT-MRAM compute-in-memory system","abstract":"We report a spin-orbit torque(SOT) magnetoresistive random-access memory(MRAM)-based probabilistic binary neural network(PBNN) for resource-saving and hardware noise-tolerant computing applications. With the presence of thermal fluctuation, the non-destructive SOT-driven magnetization switching characteristics lead to a random weight matrix with controllable probability distribution. In the meanwhile, the proposed CIM architecture allows for the concurrent execution of the probabilistic vector-matrix multiplication (PVMM) and binarization. Furthermore, leveraging the effectiveness of random binary cells to propagate multi-bit probabilistic information, our SOT-MRAM-based PBNN system achieves a 97.78\\% classification accuracy under a 7.01\\% weight variation on the MNIST database through 10 sampling cycles, and the number of bit-level computation operations is reduced by a factor of 6.9 compared to that of the full-precision LeNet-5 network. Our work provides a compelling framework for the design of reliable neural networks tailored to the applications with low power consumption and limited computational resources.","sentences":["We report a spin-orbit torque(SOT) magnetoresistive random-access memory(MRAM)-based probabilistic binary neural network(PBNN) for resource-saving and hardware noise-tolerant computing applications.","With the presence of thermal fluctuation, the non-destructive SOT-driven magnetization switching characteristics lead to a random weight matrix with controllable probability distribution.","In the meanwhile, the proposed CIM architecture allows for the concurrent execution of the probabilistic vector-matrix multiplication (PVMM) and binarization.","Furthermore, leveraging the effectiveness of random binary cells to propagate multi-bit probabilistic information, our SOT-MRAM-based PBNN system achieves a 97.78\\% classification accuracy under a 7.01\\% weight variation on the MNIST database through 10 sampling cycles, and the number of bit-level computation operations is reduced by a factor of 6.9 compared to that of the full-precision LeNet-5 network.","Our work provides a compelling framework for the design of reliable neural networks tailored to the applications with low power consumption and limited computational resources."],"url":"http://arxiv.org/abs/2403.19374v1","category":"cs.ET"}
{"created":"2024-03-28 12:35:46","title":"Robustness of type-II Dirac cones in biphenylene-based structures","abstract":"The electronic properties of one- and two-dimensional biphenylene-based systems, such as nanoribbons and bilayers, are studied within a unified approach. Besides the bilayer with direct (AA) stacking, we present two additional symmetric stackings for bilayer biphenylene that we denote by AB, by analogy with bilayer graphene, and AX, which can be derived by a small translation (slip) from the AA bilayer, with distinct band structures. We combine first-principles calculations with a tight-binding model to provide a realistic effective description of these structures. Our approach provides a global framework to characterize and analyze the robustness of the type-II Dirac cone within these structures, captures the variations caused by different stackings, and highlights important symmetries inherent in the biphenylene nanoribbon Dirac cones and edge states.","sentences":["The electronic properties of one- and two-dimensional biphenylene-based systems, such as nanoribbons and bilayers, are studied within a unified approach.","Besides the bilayer with direct (AA) stacking, we present two additional symmetric stackings for bilayer biphenylene that we denote by AB, by analogy with bilayer graphene, and AX, which can be derived by a small translation (slip) from the AA bilayer, with distinct band structures.","We combine first-principles calculations with a tight-binding model to provide a realistic effective description of these structures.","Our approach provides a global framework to characterize and analyze the robustness of the type-II Dirac cone within these structures, captures the variations caused by different stackings, and highlights important symmetries inherent in the biphenylene nanoribbon Dirac cones and edge states."],"url":"http://arxiv.org/abs/2403.19373v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 12:35:36","title":"Emergent predictability in microbial ecosystems","abstract":"Microbial ecosystems carry out essential functions for global climate, human health, and industry. These complex communities exhibit a surprising amount of functionally relevant diversity at all levels of taxonomic resolution, presenting a significant challenge for most modeling frameworks. A long-standing hope of theoretical ecology is that some patterns might persist despite community complexity -- or perhaps even emerge because of it. A deeper understanding of such \"emergent simplicity\" could enable new approaches for predicting the behaviors of the complex ecosystems in nature. However, most examples described so far afford limited predictive power, as they focused on reproducibility rather than prediction. Here, we propose an information-theoretic framework for defining, nuancing and quantifying emergent simplicity in empirical data based on the ability of simple models to predict community-level functional properties. Applying this framework to two published datasets, we demonstrate that the majority of properties measured across both experiments exhibit robust evidence of emergent predictability: surprisingly, as community richness increases, simple compositional descriptions become more predictive. We show that this behavior is not typical within the standard modeling frameworks of theoretical ecology, and argue that improving our ability to predict and control natural microbial communities will require a shift of focus: away from complexity of _ecosystems_, and towards prediction complexity of _properties_ of ecosystems.","sentences":["Microbial ecosystems carry out essential functions for global climate, human health, and industry.","These complex communities exhibit a surprising amount of functionally relevant diversity at all levels of taxonomic resolution, presenting a significant challenge for most modeling frameworks.","A long-standing hope of theoretical ecology is that some patterns might persist despite community complexity -- or perhaps even emerge because of it.","A deeper understanding of such \"emergent simplicity\" could enable new approaches for predicting the behaviors of the complex ecosystems in nature.","However, most examples described so far afford limited predictive power, as they focused on reproducibility rather than prediction.","Here, we propose an information-theoretic framework for defining, nuancing and quantifying emergent simplicity in empirical data based on the ability of simple models to predict community-level functional properties.","Applying this framework to two published datasets, we demonstrate that the majority of properties measured across both experiments exhibit robust evidence of emergent predictability: surprisingly, as community richness increases, simple compositional descriptions become more predictive.","We show that this behavior is not typical within the standard modeling frameworks of theoretical ecology, and argue that improving our ability to predict and control natural microbial communities will require a shift of focus: away from complexity of _ecosystems_, and towards prediction complexity of _properties_ of ecosystems."],"url":"http://arxiv.org/abs/2403.19372v1","category":"q-bio.PE"}
{"created":"2024-03-28 12:34:29","title":"Cell Electropermeabilization Modeling via Multiple Traces Formulation and Time Semi-Implicit Coupling","abstract":"We simulate the electrical response of multiple disjoint biological 3D cells in the electropermeabilization process. Instead of solving the boundary value problem in the volume, we reduce it to a system of boundary integrals equations with nonlinear dynamics on the cell membranes via a coupling the local Multiple Traces Formulation with a time semi-implicit scheme. Spatially, boundary unknowns are approximated by spherical harmonics, thereby allowing for spectral convergence rates for suitable time steps. Numerical results are provided to validate our claims.","sentences":["We simulate the electrical response of multiple disjoint biological 3D cells in the electropermeabilization process.","Instead of solving the boundary value problem in the volume, we reduce it to a system of boundary integrals equations with nonlinear dynamics on the cell membranes via a coupling the local Multiple Traces Formulation with a time semi-implicit scheme.","Spatially, boundary unknowns are approximated by spherical harmonics, thereby allowing for spectral convergence rates for suitable time steps.","Numerical results are provided to validate our claims."],"url":"http://arxiv.org/abs/2403.19371v1","category":"cs.CE"}
{"created":"2024-03-28 12:31:08","title":"Cloudy with a Chance of Cyberattacks: Dangling Resources Abuse on Cloud Platforms","abstract":"Recent works showed that it is feasible to hijack resources on cloud platforms. In such hijacks, attackers can take over released resources that belong to legitimate organizations. It was proposed that adversaries could abuse these resources to carry out attacks against customers of the hijacked services, e.g., through malware distribution. However, to date, no research has confirmed the existence of these attacks. We identify, for the first time, real-life hijacks of cloud resources. This yields a number of surprising and important insights. First, contrary to previous assumption that attackers primarily target IP addresses, our findings reveal that the type of resource is not the main consideration in a hijack. Attackers focus on hijacking records that allow them to determine the resource by entering freetext. The costs and overhead of hijacking such records are much lower than those of hijacking IP addresses, which are randomly selected from a large pool. Second, identifying hijacks poses a substantial challenge. Monitoring resource changes, e.g., changes in content, is insufficient, since such changes could also be legitimate. Retrospective analysis of digital assets to identify hijacks is also arduous due to the immense volume of data involved and the absence of indicators to search for. To address this challenge, we develop a novel approach that involves analyzing data from diverse sources to effectively differentiate between malicious and legitimate modifications. Our analysis has revealed 20,904 instances of hijacked resources on popular cloud platforms. While some hijacks are short-lived (up to 15 days), 1/3 persist for more than 65 days. We study how attackers abuse the hijacked resources and find that, in contrast to the threats considered in previous work, the majority of the abuse (75%) is blackhat search engine optimization.","sentences":["Recent works showed that it is feasible to hijack resources on cloud platforms.","In such hijacks, attackers can take over released resources that belong to legitimate organizations.","It was proposed that adversaries could abuse these resources to carry out attacks against customers of the hijacked services, e.g., through malware distribution.","However, to date, no research has confirmed the existence of these attacks.","We identify, for the first time, real-life hijacks of cloud resources.","This yields a number of surprising and important insights.","First, contrary to previous assumption that attackers primarily target IP addresses, our findings reveal that the type of resource is not the main consideration in a hijack.","Attackers focus on hijacking records that allow them to determine the resource by entering freetext.","The costs and overhead of hijacking such records are much lower than those of hijacking IP addresses, which are randomly selected from a large pool.","Second, identifying hijacks poses a substantial challenge.","Monitoring resource changes, e.g., changes in content, is insufficient, since such changes could also be legitimate.","Retrospective analysis of digital assets to identify hijacks is also arduous due to the immense volume of data involved and the absence of indicators to search for.","To address this challenge, we develop a novel approach that involves analyzing data from diverse sources to effectively differentiate between malicious and legitimate modifications.","Our analysis has revealed 20,904 instances of hijacked resources on popular cloud platforms.","While some hijacks are short-lived (up to 15 days), 1/3 persist for more than 65 days.","We study how attackers abuse the hijacked resources and find that, in contrast to the threats considered in previous work, the majority of the abuse (75%) is blackhat search engine optimization."],"url":"http://arxiv.org/abs/2403.19368v1","category":"cs.NI"}
{"created":"2024-03-28 12:25:29","title":"Probing Causation Dynamics in Quantum Chains near Criticality","abstract":"Distinguishing causation from correlation is crucial and requires careful consideration. In this study, we utilize a recent quantum extension of Liang information to investigate causation in quantum chains across their phase diagram. Our analysis encompasses two distinct scenarios: (i) the Aubry-Andr$\\'e$-Harper model, characterized by a spectrum-wide phase transition, and (ii) the Anisotropic Transverse Field Ising (ANNNI) model, which demonstrates a ground state transition. We discern a notable shift in causation behavior across the critical point with each case exhibiting distinct hallmarks, different from correlation measures. Especially, in the latter case, we observe maximum causation in the ordered phase just preceding the critical point.","sentences":["Distinguishing causation from correlation is crucial and requires careful consideration.","In this study, we utilize a recent quantum extension of Liang information to investigate causation in quantum chains across their phase diagram.","Our analysis encompasses two distinct scenarios: (i) the Aubry-Andr$\\'e$-Harper model, characterized by a spectrum-wide phase transition, and (ii) the Anisotropic Transverse Field Ising (ANNNI) model, which demonstrates a ground state transition.","We discern a notable shift in causation behavior across the critical point with each case exhibiting distinct hallmarks, different from correlation measures.","Especially, in the latter case, we observe maximum causation in the ordered phase just preceding the critical point."],"url":"http://arxiv.org/abs/2403.19364v1","category":"quant-ph"}
{"created":"2024-03-28 12:18:50","title":"Low Phase Noise, Record-High Power Optical Parametric Oscillator Tunable from 2.7-4.7 \u03bcm for Metrology Applications","abstract":"Within the domain of optical frequency comb systems operating in the mid-infrared, extensive exploration has been undertaken regarding critical parameters such as stabilization, coherence, or spectral tunability. Despite this, certain essential parameters remain inadequately addressed, particularly concerning the light source prerequisites for advanced spectroscopy techniques operating in the 3-5 ${\\mu}$m spectral region. Specifically, the necessity for high simultaneous stability of average power, spectral shape, and temporal properties, alongside requirements for excellent beam quality and high powers of several Watts, emerges for applications like cavity-enhanced Doppler-free saturation spectroscopy. Existing systems fall short of meeting these rigorous criteria. This study delves into the metrology aspects of an optical parametric oscillator system, with a particular emphasis on its suitability for powerhungry metrology applications. Notably, the highest average power reported in the 3-5 ${\\mu}$m region, reaching 10.3W for the idler output at 3.1 ${\\mu}$m, is achieved. Additionally, new aspects, like the analysis of idler phase noise and beam quality and the onset of higher order modes are discussed. These findings represent a significant advancement towards the realization of highly-stable, metrologygrade frequency combs in the mid-infrared, thereby facilitating precision spectroscopy techniques previously constrained by light source average powers and quality limitations.","sentences":["Within the domain of optical frequency comb systems operating in the mid-infrared, extensive exploration has been undertaken regarding critical parameters such as stabilization, coherence, or spectral tunability.","Despite this, certain essential parameters remain inadequately addressed, particularly concerning the light source prerequisites for advanced spectroscopy techniques operating in the 3-5 ${\\mu}$m spectral region.","Specifically, the necessity for high simultaneous stability of average power, spectral shape, and temporal properties, alongside requirements for excellent beam quality and high powers of several Watts, emerges for applications like cavity-enhanced Doppler-free saturation spectroscopy.","Existing systems fall short of meeting these rigorous criteria.","This study delves into the metrology aspects of an optical parametric oscillator system, with a particular emphasis on its suitability for powerhungry metrology applications.","Notably, the highest average power reported in the 3-5 ${\\mu}$m region, reaching 10.3W for the idler output at 3.1 ${\\mu}$m, is achieved.","Additionally, new aspects, like the analysis of idler phase noise and beam quality and the onset of higher order modes are discussed.","These findings represent a significant advancement towards the realization of highly-stable, metrologygrade frequency combs in the mid-infrared, thereby facilitating precision spectroscopy techniques previously constrained by light source average powers and quality limitations."],"url":"http://arxiv.org/abs/2403.19360v1","category":"physics.optics"}
{"created":"2024-03-28 12:08:47","title":"A Software-Defined Networking Solution for Interconnecting Network Functions in Service-Based Architectures","abstract":"Mobile core networks handle critical control functions for delivering services in modern cellular networks. Traditional point-to-point architectures, where network functions are directly connected through standardized interfaces, are being substituted by service-based architectures (SBAs), where core functionalities are finer-grained microservices decoupled from the underlying infrastructure. In this way, network functions and services can be distributed, with scaling and fail-over mechanisms, and can be dynamically deployed, updated, or removed to support slicing. A myriad of network functions can be deployed or removed according to traffic flows, thereby increasing the complexity of connection management. In this context, 3GPP Release 16 defines the service communication proxy (SCP) as a unified communication interface for a set of network functions. In this paper, we propose a novel software-defined networking (SDN)-based solution with the same role for a service mesh architecture where network functions can be deployed anywhere in the infrastructure. We demonstrated its efficiency in comparison with alternative architectures.","sentences":["Mobile core networks handle critical control functions for delivering services in modern cellular networks.","Traditional point-to-point architectures, where network functions are directly connected through standardized interfaces, are being substituted by service-based architectures (SBAs), where core functionalities are finer-grained microservices decoupled from the underlying infrastructure.","In this way, network functions and services can be distributed, with scaling and fail-over mechanisms, and can be dynamically deployed, updated, or removed to support slicing.","A myriad of network functions can be deployed or removed according to traffic flows, thereby increasing the complexity of connection management.","In this context, 3GPP Release 16 defines the service communication proxy (SCP) as a unified communication interface for a set of network functions.","In this paper, we propose a novel software-defined networking (SDN)-based solution with the same role for a service mesh architecture where network functions can be deployed anywhere in the infrastructure.","We demonstrated its efficiency in comparison with alternative architectures."],"url":"http://arxiv.org/abs/2403.19353v1","category":"cs.NI"}
{"created":"2024-03-28 12:02:17","title":"Gain-Only Neural Operator Approximators of PDE Backstepping Controllers","abstract":"For the recently introduced deep learning-powered approach to PDE backstepping control, we present an advancement applicable across all the results developed thus far: approximating the control gain function only (a function of one variable), rather than the entire kernel function of the backstepping transformation (a function of two variables). We introduce this idea on a couple benchmark (unstable) PDEs, hyperbolic and parabolic. We alter the approach of quantifying the effect of the approximation error by replacing a backstepping transformation that employs the approximated kernel (suitable for adaptive control) by a transformation that employs the exact kernel (suitable for gain scheduling). A major simplification in the target system arises, with the perturbation due to the approximation shifting from the domain to the boundary condition. This results in a significant difference in the Lyapunov analysis, which nevertheless results in a guarantee of the stability being retained with the simplified approximation approach. The approach of approximating only the control gain function simplifies the operator being approximated and the training of its neural approximation, with an expected reduction in the neural network size. The price for the savings in approximation is paid through a somewhat more intricate Lyapunov analysis, in higher Sobolev spaces for some PDEs, as well as some restrictions on initial conditions that result from higher Sobolev spaces. While the proposed approach appears inapplicable to uses in adaptive control, it is almost certainly applicable in gain scheduling applications of neural operator-approximated PDE backstepping controllers.","sentences":["For the recently introduced deep learning-powered approach to PDE backstepping control, we present an advancement applicable across all the results developed thus far: approximating the control gain function only (a function of one variable), rather than the entire kernel function of the backstepping transformation (a function of two variables).","We introduce this idea on a couple benchmark (unstable) PDEs, hyperbolic and parabolic.","We alter the approach of quantifying the effect of the approximation error by replacing a backstepping transformation that employs the approximated kernel (suitable for adaptive control) by a transformation that employs the exact kernel (suitable for gain scheduling).","A major simplification in the target system arises, with the perturbation due to the approximation shifting from the domain to the boundary condition.","This results in a significant difference in the Lyapunov analysis, which nevertheless results in a guarantee of the stability being retained with the simplified approximation approach.","The approach of approximating only the control gain function simplifies the operator being approximated and the training of its neural approximation, with an expected reduction in the neural network size.","The price for the savings in approximation is paid through a somewhat more intricate Lyapunov analysis, in higher Sobolev spaces for some PDEs, as well as some restrictions on initial conditions that result from higher Sobolev spaces.","While the proposed approach appears inapplicable to uses in adaptive control, it is almost certainly applicable in gain scheduling applications of neural operator-approximated PDE backstepping controllers."],"url":"http://arxiv.org/abs/2403.19344v1","category":"eess.SY"}
{"created":"2024-03-28 11:58:32","title":"An efficient multiscale multigrid preconditioner for Darcy flow in high-contrast media","abstract":"In this paper, we develop a multigrid preconditioner to solve Darcy flow in highly heterogeneous porous media. The key component of the preconditioner is to construct a sequence of nested subspaces $W_{\\mathcal{L}}\\subset W_{\\mathcal{L}-1}\\subset\\cdots\\subset W_1=W_h$. An appropriate spectral problem is defined in the space of $W_{i-1}$, then the eigenfunctions of the spectral problems are utilized to form $W_i$. The preconditioner is applied to solve a positive semidefinite linear system which results from discretizing the Darcy flow equation with the lowest order Raviart-Thomas spaces and adopting a trapezoidal quadrature rule. Theoretical analysis and numerical investigations of this preconditioner will be presented. In particular, we will consider several typical highly heterogeneous permeability fields whose resolutions are up to $1024^3$ and examine the computational performance of the preconditioner in several aspects, such as strong scalability, weak scalability, and robustness against the contrast of the media. We also demonstrate an application of this preconditioner for solving a two-phase flow benchmark problem.","sentences":["In this paper, we develop a multigrid preconditioner to solve Darcy flow in highly heterogeneous porous media.","The key component of the preconditioner is to construct a sequence of nested subspaces $W_{\\mathcal{L}}\\subset W_{\\mathcal{L}-1}\\subset\\cdots\\subset W_1=W_h$. An appropriate spectral problem is defined in the space of $W_{i-1}$, then the eigenfunctions of the spectral problems are utilized to form $W_i$. The preconditioner is applied to solve a positive semidefinite linear system which results from discretizing the Darcy flow equation with the lowest order Raviart-Thomas spaces and adopting a trapezoidal quadrature rule.","Theoretical analysis and numerical investigations of this preconditioner will be presented.","In particular, we will consider several typical highly heterogeneous permeability fields whose resolutions are up to $1024^3$ and examine the computational performance of the preconditioner in several aspects, such as strong scalability, weak scalability, and robustness against the contrast of the media.","We also demonstrate an application of this preconditioner for solving a two-phase flow benchmark problem."],"url":"http://arxiv.org/abs/2403.19342v1","category":"math.NA"}
{"created":"2024-03-28 11:46:55","title":"Learning a Formally Verified Control Barrier Function in Stochastic Environment","abstract":"Safety is a fundamental requirement of control systems. Control Barrier Functions (CBFs) are proposed to ensure the safety of the control system by constructing safety filters or synthesizing control inputs. However, the safety guarantee and performance of safe controllers rely on the construction of valid CBFs. Inspired by universal approximatability, CBFs are represented by neural networks, known as neural CBFs (NCBFs). This paper presents an algorithm for synthesizing formally verified continuous-time neural Control Barrier Functions in stochastic environments in a single step. The proposed training process ensures efficacy across the entire state space with only a finite number of data points by constructing a sample-based learning framework for Stochastic Neural CBFs (SNCBFs). Our methodology eliminates the need for post hoc verification by enforcing Lipschitz bounds on the neural network, its Jacobian, and Hessian terms. We demonstrate the effectiveness of our approach through case studies on the inverted pendulum system and obstacle avoidance in autonomous driving, showcasing larger safe regions compared to baseline methods.","sentences":["Safety is a fundamental requirement of control systems.","Control Barrier Functions (CBFs) are proposed to ensure the safety of the control system by constructing safety filters or synthesizing control inputs.","However, the safety guarantee and performance of safe controllers rely on the construction of valid CBFs.","Inspired by universal approximatability, CBFs are represented by neural networks, known as neural CBFs (NCBFs).","This paper presents an algorithm for synthesizing formally verified continuous-time neural Control Barrier Functions in stochastic environments in a single step.","The proposed training process ensures efficacy across the entire state space with only a finite number of data points by constructing a sample-based learning framework for Stochastic Neural CBFs (SNCBFs).","Our methodology eliminates the need for post hoc verification by enforcing Lipschitz bounds on the neural network, its Jacobian, and Hessian terms.","We demonstrate the effectiveness of our approach through case studies on the inverted pendulum system and obstacle avoidance in autonomous driving, showcasing larger safe regions compared to baseline methods."],"url":"http://arxiv.org/abs/2403.19332v1","category":"cs.RO"}
{"created":"2024-03-28 11:41:59","title":"RootInteractive tool for multidimensional statistical analysis, machine learning and analytical model validation","abstract":"The ALICE experiment at CERN LHC is specifically designed for investigating heavy ion collisions. The upgraded ALICE accommodates a tenfold increase in PbPb luminosity and a two-order of magnitude surge in minimum bias events. To address the challenges of high detector occupancy and event pile-ups, advanced multidimensional data analysis techniques, including machine learning (ML), are indispensable. Despite ML popularity, the complexity of its models presents interpretation challenges, and oversimplification in analysis often leads to inaccuracies.   Our objective was to develop RootInteractive, a tool for multidimensional statistical analysis. This tool simplifies data analysis across dimensions, visualizes functions with uncertainties, and validates assumptions and approximations. In RootInteractive, it is crucial to easily define the functional composition of analytical parametric and non-parametric functions, exploit symmetries, and define multidimensional invariant functions and corresponding alarms.   RootInteractive adopts a declarative programming paradigm, ensuring userfriendliness for experts, students, and educators. It facilitates interactive visualization, n-dimensional histogramming/projection, and information extraction on both Python,C++ server and client. Data compression, datasets with O(10 to 7) entries and O(25) attributes can be interactively analyzed in a browser with O(0.500-1 GB) size. Representative downsampling and reweighting/pre-aggregation enable the effective analysis of one year of ALICE data for various purposes.","sentences":["The ALICE experiment at CERN LHC is specifically designed for investigating heavy ion collisions.","The upgraded ALICE accommodates a tenfold increase in PbPb luminosity and a two-order of magnitude surge in minimum bias events.","To address the challenges of high detector occupancy and event pile-ups, advanced multidimensional data analysis techniques, including machine learning (ML), are indispensable.","Despite ML popularity, the complexity of its models presents interpretation challenges, and oversimplification in analysis often leads to inaccuracies.   ","Our objective was to develop RootInteractive, a tool for multidimensional statistical analysis.","This tool simplifies data analysis across dimensions, visualizes functions with uncertainties, and validates assumptions and approximations.","In RootInteractive, it is crucial to easily define the functional composition of analytical parametric and non-parametric functions, exploit symmetries, and define multidimensional invariant functions and corresponding alarms.   ","RootInteractive adopts a declarative programming paradigm, ensuring userfriendliness for experts, students, and educators.","It facilitates interactive visualization, n-dimensional histogramming/projection, and information extraction on both Python,C++ server and client.","Data compression, datasets with O(10 to 7) entries and O(25) attributes can be interactively analyzed in a browser with O(0.500-1 GB) size.","Representative downsampling and reweighting/pre-aggregation enable the effective analysis of one year of ALICE data for various purposes."],"url":"http://arxiv.org/abs/2403.19330v1","category":"hep-ex"}
{"created":"2024-03-28 11:12:33","title":"Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction","abstract":"Scene reconstruction from multi-view images is a fundamental problem in computer vision and graphics. Recent neural implicit surface reconstruction methods have achieved high-quality results; however, editing and manipulating the 3D geometry of reconstructed scenes remains challenging due to the absence of naturally decomposed object entities and complex object/background compositions. In this paper, we present Total-Decom, a novel method for decomposed 3D reconstruction with minimal human interaction. Our approach seamlessly integrates the Segment Anything Model (SAM) with hybrid implicit-explicit neural surface representations and a mesh-based region-growing technique for accurate 3D object decomposition. Total-Decom requires minimal human annotations while providing users with real-time control over the granularity and quality of decomposition. We extensively evaluate our method on benchmark datasets and demonstrate its potential for downstream applications, such as animation and scene editing. The code is available at \\href{https://github.com/CVMI-Lab/Total-Decom.git}{https://github.com/CVMI-Lab/Total-Decom.git}.","sentences":["Scene reconstruction from multi-view images is a fundamental problem in computer vision and graphics.","Recent neural implicit surface reconstruction methods have achieved high-quality results; however, editing and manipulating the 3D geometry of reconstructed scenes remains challenging due to the absence of naturally decomposed object entities and complex object/background compositions.","In this paper, we present Total-Decom, a novel method for decomposed 3D reconstruction with minimal human interaction.","Our approach seamlessly integrates the Segment Anything Model (SAM) with hybrid implicit-explicit neural surface representations and a mesh-based region-growing technique for accurate 3D object decomposition.","Total-Decom requires minimal human annotations while providing users with real-time control over the granularity and quality of decomposition.","We extensively evaluate our method on benchmark datasets and demonstrate its potential for downstream applications, such as animation and scene editing.","The code is available at \\href{https://github.com/CVMI-Lab/Total-Decom.git}{https://github.com/CVMI-Lab/Total-Decom.git}."],"url":"http://arxiv.org/abs/2403.19314v1","category":"cs.CV"}
{"created":"2024-03-28 10:56:38","title":"MRNaB: Mixed Reality-based Robot Navigation Interface using Optical-see-through MR-beacon","abstract":"Recent advancements in robotics have led to the development of numerous interfaces to enhance the intuitiveness of robot navigation. However, the reliance on traditional 2D displays imposes limitations on the simultaneous visualization of information. Mixed Reality (MR) technology addresses this issue by enhancing the dimensionality of information visualization, allowing users to perceive multiple pieces of information concurrently. This paper proposes Mixed reality-based robot navigation interface using an optical-see-through MR-beacon (MRNaB), a novel approach that incorporates an MR-beacon, situated atop the real-world environment, to function as a signal transmitter for robot navigation. This MR-beacon is designed to be persistent, eliminating the need for repeated navigation inputs for the same location. Our system is mainly constructed into four primary functions: \"Add\", \"Move\", \"Delete\", and \"Select\". These allow for the addition of a MR-beacon, location movement, its deletion, and the selection of MR-beacon for navigation purposes, respectively. The effectiveness of the proposed method was then validated through experiments by comparing it with the traditional 2D system. As the result, MRNaB was proven to increase the performance of the user when doing navigation to a certain place subjectively and objectively. For additional material, please check: https://mertcookimg.github.io/mrnab","sentences":["Recent advancements in robotics have led to the development of numerous interfaces to enhance the intuitiveness of robot navigation.","However, the reliance on traditional 2D displays imposes limitations on the simultaneous visualization of information.","Mixed Reality (MR) technology addresses this issue by enhancing the dimensionality of information visualization, allowing users to perceive multiple pieces of information concurrently.","This paper proposes Mixed reality-based robot navigation interface using an optical-see-through MR-beacon (MRNaB), a novel approach that incorporates an MR-beacon, situated atop the real-world environment, to function as a signal transmitter for robot navigation.","This MR-beacon is designed to be persistent, eliminating the need for repeated navigation inputs for the same location.","Our system is mainly constructed into four primary functions: \"Add\", \"Move\", \"Delete\", and \"Select\".","These allow for the addition of a MR-beacon, location movement, its deletion, and the selection of MR-beacon for navigation purposes, respectively.","The effectiveness of the proposed method was then validated through experiments by comparing it with the traditional 2D system.","As the result, MRNaB was proven to increase the performance of the user when doing navigation to a certain place subjectively and objectively.","For additional material, please check:","https://mertcookimg.github.io/mrnab"],"url":"http://arxiv.org/abs/2403.19310v1","category":"cs.RO"}
{"created":"2024-03-28 10:51:20","title":"Improving performance of contour integral-based nonlinear eigensolvers with infinite GMRES","abstract":"In this work, the infinite GMRES algorithm, recently proposed by Correnty et al., is employed in contour integral-based nonlinear eigensolvers, avoiding the computation of costly factorizations at each quadrature node to solve the linear systems efficiently. Several techniques are applied to make the infinite GMRES memory-friendly, computationally efficient, and numerically stable in practice. More specifically, we analyze the relationship between polynomial eigenvalue problems and their scaled linearizations, and provide a novel weighting strategy which can significantly accelerate the convergence of infinite GMRES in this particular context. We also adopt the technique of TOAR to infinite GMRES to reduce the memory footprint. Theoretical analysis and numerical experiments are provided to illustrate the efficiency of the proposed algorithm.","sentences":["In this work, the infinite GMRES algorithm, recently proposed by Correnty et al., is employed in contour integral-based nonlinear eigensolvers, avoiding the computation of costly factorizations at each quadrature node to solve the linear systems efficiently.","Several techniques are applied to make the infinite GMRES memory-friendly, computationally efficient, and numerically stable in practice.","More specifically, we analyze the relationship between polynomial eigenvalue problems and their scaled linearizations, and provide a novel weighting strategy which can significantly accelerate the convergence of infinite GMRES in this particular context.","We also adopt the technique of TOAR to infinite GMRES to reduce the memory footprint.","Theoretical analysis and numerical experiments are provided to illustrate the efficiency of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.19309v1","category":"math.NA"}
{"created":"2024-03-28 10:45:20","title":"A Deep Redshift Survey of the Perseus Cluster: Spatial Distribution and Kinematics of Galaxies","abstract":"We study the global kinematics of the Perseus galaxy cluster (Abell 426) at redshift z = 0.017 using a large sample of galaxies from our new MMT/Hectospec spectroscopic observation for this cluster. The sample includes 1447 galaxies with measured redshifts within 60' from the cluster center (1148 from this MMT/Hectospec program and 299 from the literature). The resulting spectroscopic completeness is 67% at r-band apparent magnitude $r_{\\rm{Petro, 0}}\\leq 18.0$ within 60' from the cluster center. To identify cluster member galaxies in this sample, we develop a new open-source Python package, CausticSNUpy. This code implements the algorithm of the caustic technique and yields 418 member galaxies within 60' of the cluster. We study the cluster using this sample of member galaxies. The cluster shows no significant signal of global rotation. A statistical test shows that the cluster does not have a noticeable substructure within 30'. We find two central regions where the X-ray emitting intracluster medium and galaxies show significant velocity differences ($>7\\sigma$). On a large scale, however, the overall morphology and kinematics between the intracluster medium and galaxies agree well. Our results suggest that the Perseus cluster is a relaxed system and has not experienced a recent merger.","sentences":["We study the global kinematics of the Perseus galaxy cluster (Abell 426) at redshift z = 0.017 using a large sample of galaxies from our new MMT/Hectospec spectroscopic observation for this cluster.","The sample includes 1447 galaxies with measured redshifts within 60' from the cluster center (1148 from this MMT/Hectospec program and 299 from the literature).","The resulting spectroscopic completeness is 67% at r-band apparent magnitude $r_{\\rm{Petro, 0}}\\leq 18.0$ within 60' from the cluster center.","To identify cluster member galaxies in this sample, we develop a new open-source Python package, CausticSNUpy.","This code implements the algorithm of the caustic technique and yields 418 member galaxies within 60' of the cluster.","We study the cluster using this sample of member galaxies.","The cluster shows no significant signal of global rotation.","A statistical test shows that the cluster does not have a noticeable substructure within 30'.","We find two central regions where the X-ray emitting intracluster medium and galaxies show significant velocity differences ($>7\\sigma$).","On a large scale, however, the overall morphology and kinematics between the intracluster medium and galaxies agree well.","Our results suggest that the Perseus cluster is a relaxed system and has not experienced a recent merger."],"url":"http://arxiv.org/abs/2403.19307v1","category":"astro-ph.GA"}
{"created":"2024-03-28 10:38:13","title":"Post Quantum Cryptography & its Comparison with Classical Cryptography","abstract":"Cryptography plays a pivotal role in safeguarding sensitive information and facilitating secure communication. Classical cryptography relies on mathematical computations, whereas quantum cryptography operates on the principles of quantum mechanics, offering a new frontier in secure communication. Quantum cryptographic systems introduce novel dimensions to security, capable of detecting and thwarting eavesdropping attempts. By contrasting quantum cryptography with its classical counterpart, it becomes evident how quantum mechanics revolutionizes the landscape of secure communication.","sentences":["Cryptography plays a pivotal role in safeguarding sensitive information and facilitating secure communication.","Classical cryptography relies on mathematical computations, whereas quantum cryptography operates on the principles of quantum mechanics, offering a new frontier in secure communication.","Quantum cryptographic systems introduce novel dimensions to security, capable of detecting and thwarting eavesdropping attempts.","By contrasting quantum cryptography with its classical counterpart, it becomes evident how quantum mechanics revolutionizes the landscape of secure communication."],"url":"http://arxiv.org/abs/2403.19299v1","category":"cs.CR"}
{"created":"2024-03-28 10:33:29","title":"Quantum asymptotic amplitude for quantum oscillatory systems from the Koopman operator viewpoint","abstract":"We have recently proposed a fully quantum-mechanical definition of the asymptotic phase for quantum nonlinear oscillators, which is also applicable in the strong quantum regime [Kato and Nakao 2022 Chaos 32 063133]. In this study, we propose a definition of the quantum asymptotic amplitude for quantum oscillatory systems, which extends naturally the definition of the asymptotic amplitude for classical nonlinear oscillators on the basis of the Koopman operator theory. We introduce the asymptotic amplitude for quantum oscillatory systems by using the eigenoperator of the backward Liouville operator associated with the largest non-zero real eigenvalue. Using examples of the quantum van der Pol oscillator with the quantum Kerr effect, exhibiting quantum limit-cycle oscillations, and the quantum van der Pol model with the quantum squeezing and degenerate parametric oscillator with nonlinear damping, exhibiting quantum noise-induced oscillations, we illustrate that the proposed quantum asymptotic amplitude appropriately yields isostable amplitude values that decay exponentially with a constant rate.","sentences":["We have recently proposed a fully quantum-mechanical definition of the asymptotic phase for quantum nonlinear oscillators, which is also applicable in the strong quantum regime","[Kato and Nakao 2022 Chaos 32 063133].","In this study, we propose a definition of the quantum asymptotic amplitude for quantum oscillatory systems, which extends naturally the definition of the asymptotic amplitude for classical nonlinear oscillators on the basis of the Koopman operator theory.","We introduce the asymptotic amplitude for quantum oscillatory systems by using the eigenoperator of the backward Liouville operator associated with the largest non-zero real eigenvalue.","Using examples of the quantum van der Pol oscillator with the quantum Kerr effect, exhibiting quantum limit-cycle oscillations, and the quantum van der Pol model with the quantum squeezing and degenerate parametric oscillator with nonlinear damping, exhibiting quantum noise-induced oscillations, we illustrate that the proposed quantum asymptotic amplitude appropriately yields isostable amplitude values that decay exponentially with a constant rate."],"url":"http://arxiv.org/abs/2403.19297v1","category":"nlin.AO"}
{"created":"2024-03-28 10:33:21","title":"Theoretical analysis of chemical reactions using a variational quantum eigensolver method without specifying molecular charge","abstract":"Quantum chemical calculations have attracted much attention as a practical application of quantum computing. Quantum computers can prepare superpositions of electronic states with various numbers of electrons on qubits. This special feature could be used to construct an efficient method for analyzing the structural variations of molecules and chemical reactions involving changes in molecular charge. The present work demonstrates a variational quantum eigensolver (VQE) algorithm based on a cost function ($L_{cost}$) having the same form as the grand potential of the grand canonical ensemble of electrons. The chemical potential of the electrons ($w$) is used as an input to these VQE calculations, whereas the molecular charge is not specified in advance but rather is a physical quantity that results from the calculations. Calculations involving model systems are carried out to show the viability of this new approach. Calculations for typical electron-donating and electron-accepting molecules using this technique yielded cationic, neutral or anionic species depending on the value of $w$. Models representing the adsorption of water or ammonia on copper-based catalysts predicted that oxidation would be associated with such adsorption. The molecular structures in which such reactions occurred were found to be dependent on the catalyst model, the adsorbed molecular species, and the value of $w$. These results arise because the electronic state that gives the lowest $L_{cost}$ value depends on the value of $w$ and the molecular structure. This behaviour was successfully simulated by the present VQE calculations.","sentences":["Quantum chemical calculations have attracted much attention as a practical application of quantum computing.","Quantum computers can prepare superpositions of electronic states with various numbers of electrons on qubits.","This special feature could be used to construct an efficient method for analyzing the structural variations of molecules and chemical reactions involving changes in molecular charge.","The present work demonstrates a variational quantum eigensolver (VQE) algorithm based on a cost function ($L_{cost}$) having the same form as the grand potential of the grand canonical ensemble of electrons.","The chemical potential of the electrons ($w$) is used as an input to these VQE calculations, whereas the molecular charge is not specified in advance but rather is a physical quantity that results from the calculations.","Calculations involving model systems are carried out to show the viability of this new approach.","Calculations for typical electron-donating and electron-accepting molecules using this technique yielded cationic, neutral or anionic species depending on the value of $w$. Models representing the adsorption of water or ammonia on copper-based catalysts predicted that oxidation would be associated with such adsorption.","The molecular structures in which such reactions occurred were found to be dependent on the catalyst model, the adsorbed molecular species, and the value of $w$. These results arise because the electronic state that gives the lowest $L_{cost}$ value depends on the value of $w$ and the molecular structure.","This behaviour was successfully simulated by the present VQE calculations."],"url":"http://arxiv.org/abs/2403.19296v1","category":"physics.chem-ph"}
{"created":"2024-03-28 10:29:57","title":"Adaptive Preload Control of Cable-Driven Parallel Robots for Handling Task","abstract":"This paper presents a method for dynamic adjustment of cable preloads based on the actuation redundancy of \\acp{CDPR}, which allows increasing or decreasing the platform stiffness depending on task requirements. This is achieved by computing preload parameters with an extended nullspace formulation of the kinematics. The method facilitates the operator's ability to specify a defined preload within the operation space. The algorithms are implemented in a real-time environment, allowing for the use of optimization in hybrid position-force control. To validate the effectiveness of this approach, a simulation study is performed, and the obtained results are compared to existing methods. Furthermore, the method is investigated experimentally and compared with the conventional position-controlled operation of a cable robot. The results demonstrate the feasibility of adaptively adjusting cable preloads during platform motion and manipulation of additional objects.","sentences":["This paper presents a method for dynamic adjustment of cable preloads based on the actuation redundancy of \\acp{CDPR}, which allows increasing or decreasing the platform stiffness depending on task requirements.","This is achieved by computing preload parameters with an extended nullspace formulation of the kinematics.","The method facilitates the operator's ability to specify a defined preload within the operation space.","The algorithms are implemented in a real-time environment, allowing for the use of optimization in hybrid position-force control.","To validate the effectiveness of this approach, a simulation study is performed, and the obtained results are compared to existing methods.","Furthermore, the method is investigated experimentally and compared with the conventional position-controlled operation of a cable robot.","The results demonstrate the feasibility of adaptively adjusting cable preloads during platform motion and manipulation of additional objects."],"url":"http://arxiv.org/abs/2403.19293v1","category":"cs.RO"}
{"created":"2024-03-28 10:27:50","title":"Non-reciprocal alignment induces asymmetric clustering in active repulsive mixtures","abstract":"Heterogeneity is a ubiquitous feature in many biological and synthetic active matter systems that are inherently out of equilibrium. In addition to conservative interactions between active constituents, a non-equilibrium environment often induces effective non-reciprocal (NR) couplings. The full consequences, especially for systems with order parameters of different symmetries, still remain elusive. Here, we study a minimal active NR mixture exhibiting both, polar ordering and clustering using a combination of hydrodynamic theory, linear stability analysis, particle-based simulations and fluctuation analysis. We show that NR alignment interactions have profound influence on the density dynamics already far below the threshold related to spontaneous time dependency of polarization dynamics. In particular, NR alignment alone induces asymmetrical clustering, and thus, partial demixing with single-species clusters chasing more dilute accumulations of the other species. Extremely large NR alignment eventually leads to a disappearance of clustered states.","sentences":["Heterogeneity is a ubiquitous feature in many biological and synthetic active matter systems that are inherently out of equilibrium.","In addition to conservative interactions between active constituents, a non-equilibrium environment often induces effective non-reciprocal (NR) couplings.","The full consequences, especially for systems with order parameters of different symmetries, still remain elusive.","Here, we study a minimal active NR mixture exhibiting both, polar ordering and clustering using a combination of hydrodynamic theory, linear stability analysis, particle-based simulations and fluctuation analysis.","We show that NR alignment interactions have profound influence on the density dynamics already far below the threshold related to spontaneous time dependency of polarization dynamics.","In particular, NR alignment alone induces asymmetrical clustering, and thus, partial demixing with single-species clusters chasing more dilute accumulations of the other species.","Extremely large NR alignment eventually leads to a disappearance of clustered states."],"url":"http://arxiv.org/abs/2403.19291v1","category":"cond-mat.soft"}
{"created":"2024-03-28 10:19:28","title":"Entanglement in multinucleon transfer reactions","abstract":"Nuclear reactions present an interesting case for studies of the time-evolution of entanglement between complex quantum systems. In this work, the time-dependent nuclear density functional theory is employed to explore entanglement in multinucleon transfer reactions. As an illustrative example, for the reaction $^{40}$Ca $+$ $^{208}$Pb at $E_{\\rm lab} = 249$ MeV, in the interval of impact parameters $4.65-7.40$ fm, and the relativistic density functional PC-PK1, we compute the von Neumann entropies, entanglement between fragments, nucleon-number fluctuations, and Shannon entropy for the nucleon-number observable. A simple linear correlation is established between the entanglement and nucleon-number fluctuation of the final fragments. The entanglement between the fragments can be related to the corresponding excitation energies and angular momenta. The relationship between the von Neumann entropy and the Shannon entropy for the nucleon-number observable is analyzed, as well as the time-evolution of the entanglement (nucleon-number fluctuation). The entanglement is also calculated for a range of incident energies and it is shown how, depending on the impact parameter, the entanglement increases with the collision energy.","sentences":["Nuclear reactions present an interesting case for studies of the time-evolution of entanglement between complex quantum systems.","In this work, the time-dependent nuclear density functional theory is employed to explore entanglement in multinucleon transfer reactions.","As an illustrative example, for the reaction $^{40}$Ca $+$ $^{208}$Pb at $E_{\\rm lab} = 249$ MeV, in the interval of impact parameters $4.65-7.40$ fm, and the relativistic density functional PC-PK1, we compute the von Neumann entropies, entanglement between fragments, nucleon-number fluctuations, and Shannon entropy for the nucleon-number observable.","A simple linear correlation is established between the entanglement and nucleon-number fluctuation of the final fragments.","The entanglement between the fragments can be related to the corresponding excitation energies and angular momenta.","The relationship between the von Neumann entropy and the Shannon entropy for the nucleon-number observable is analyzed, as well as the time-evolution of the entanglement (nucleon-number fluctuation).","The entanglement is also calculated for a range of incident energies and it is shown how, depending on the impact parameter, the entanglement increases with the collision energy."],"url":"http://arxiv.org/abs/2403.19288v1","category":"nucl-th"}
{"created":"2024-03-28 10:04:15","title":"On potentials whose level sets are orbits","abstract":"A level orbit of a mechanical Hamiltonian system is a solution of Newton equation that is contained in a level set of the potential energy. In 2003, Mark Levi asked for a characterization of the smooth potential energy functions on the plane with the property that any point on the plane lies on a level orbit; we call such functions Levi potentials. The basic examples are the radial monotone increasing smooth functions. In this paper we show that any Levi potential that is analytic or has totally path-disconnected critical set must be radial. Nevertheless, we show that every compact convex subset of the plane is the critical set of a Levi potential. A crucial observation for these theorems is that, outside the critical set, the family of level sets of a Levi potential forms a solution of the inverse curvature flow.","sentences":["A level orbit of a mechanical Hamiltonian system is a solution of Newton equation that is contained in a level set of the potential energy.","In 2003, Mark Levi asked for a characterization of the smooth potential energy functions on the plane with the property that any point on the plane lies on a level orbit; we call such functions Levi potentials.","The basic examples are the radial monotone increasing smooth functions.","In this paper we show that any Levi potential that is analytic or has totally path-disconnected critical set must be radial.","Nevertheless, we show that every compact convex subset of the plane is the critical set of a Levi potential.","A crucial observation for these theorems is that, outside the critical set, the family of level sets of a Levi potential forms a solution of the inverse curvature flow."],"url":"http://arxiv.org/abs/2403.19281v1","category":"math.DG"}
{"created":"2024-03-28 10:03:23","title":"Quantum-thermodynamic enhancements in continuous thermal machines require energetic coherence","abstract":"Quantum coherence has been shown to impact the operational capabilities of quantum systems performing thermodynamic tasks in a significant way, and yet the possibility of genuine coherence-enhanced thermodynamic operation remains unclear. Here we show that only the presence of energetic coherence -- coherence between levels with different energies -- in steady-state quantum thermal machines can lead to genuine thermodynamic advantage. On the other hand, engines showing coherence between degenerate levels, or subjected to noise-induced coherence, are shown to be systematically outperformed by classical stochastic engines using exactly the same set of (incoherent) resources. We illustrate our results with three prototypical models of heat engines and refrigerators and employ multi-objective optimization techniques to characterize quantum-enhanced regimes in connection with the thermodynamic uncertainty relation.","sentences":["Quantum coherence has been shown to impact the operational capabilities of quantum systems performing thermodynamic tasks in a significant way, and yet the possibility of genuine coherence-enhanced thermodynamic operation remains unclear.","Here we show that only the presence of energetic coherence -- coherence between levels with different energies -- in steady-state quantum thermal machines can lead to genuine thermodynamic advantage.","On the other hand, engines showing coherence between degenerate levels, or subjected to noise-induced coherence, are shown to be systematically outperformed by classical stochastic engines using exactly the same set of (incoherent) resources.","We illustrate our results with three prototypical models of heat engines and refrigerators and employ multi-objective optimization techniques to characterize quantum-enhanced regimes in connection with the thermodynamic uncertainty relation."],"url":"http://arxiv.org/abs/2403.19280v1","category":"quant-ph"}
{"created":"2024-03-28 10:01:35","title":"Enhanced Bayesian Personalized Ranking for Robust Hard Negative Sampling in Recommender Systems","abstract":"In implicit collaborative filtering, hard negative mining techniques are developed to accelerate and enhance the recommendation model learning. However, the inadvertent selection of false negatives remains a major concern in hard negative sampling, as these false negatives can provide incorrect information and mislead the model learning. To date, only a small number of studies have been committed to solve the false negative problem, primarily focusing on designing sophisticated sampling algorithms to filter false negatives. In contrast, this paper shifts its focus to refining the loss function. We find that the original Bayesian Personalized Ranking (BPR), initially designed for uniform negative sampling, is inadequate in adapting to hard sampling scenarios. Hence, we introduce an enhanced Bayesian Personalized Ranking objective, named as Hard-BPR, which is specifically crafted for dynamic hard negative sampling to mitigate the influence of false negatives. This method is simple yet efficient for real-world deployment. Extensive experiments conducted on three real-world datasets demonstrate the effectiveness and robustness of our approach, along with the enhanced ability to distinguish false negatives.","sentences":["In implicit collaborative filtering, hard negative mining techniques are developed to accelerate and enhance the recommendation model learning.","However, the inadvertent selection of false negatives remains a major concern in hard negative sampling, as these false negatives can provide incorrect information and mislead the model learning.","To date, only a small number of studies have been committed to solve the false negative problem, primarily focusing on designing sophisticated sampling algorithms to filter false negatives.","In contrast, this paper shifts its focus to refining the loss function.","We find that the original Bayesian Personalized Ranking (BPR), initially designed for uniform negative sampling, is inadequate in adapting to hard sampling scenarios.","Hence, we introduce an enhanced Bayesian Personalized Ranking objective, named as Hard-BPR, which is specifically crafted for dynamic hard negative sampling to mitigate the influence of false negatives.","This method is simple yet efficient for real-world deployment.","Extensive experiments conducted on three real-world datasets demonstrate the effectiveness and robustness of our approach, along with the enhanced ability to distinguish false negatives."],"url":"http://arxiv.org/abs/2403.19276v1","category":"cs.IR"}
{"created":"2024-03-28 09:49:38","title":"On the Performance of Low-complexity Decoders of LDPC and Polar Codes","abstract":"Efficient decoding is crucial to high-throughput and low-power wireless communication scenarios. A theoretical analysis of the performance-complexity tradeoff toward low-complexity decoding is required for a better understanding of the fundamental limits in the above-mentioned scenarios. This study aims to explore the performance of decoders with complexity constraints. Specifically, we investigate the performance of LDPC codes with different numbers of belief-propagation iterations and the performance of polar codes with an SSC decoder. We found that the asymptotic error rates of both polar codes and LDPC codes are functions of complexity $T$ and code length $N$, in the form of $2^{-a2^{b\\frac{T}{N}}}$, where $a$ and $b$ are constants that depend on channel and coding schemes. Our analysis reveals the different performance-complexity tradeoffs for LDPC and polar codes. The results indicate that if one aims to further enhance the decoding efficiency for LDPC codes, the key lies in how to efficiently pass messages on the factor graph. In terms of decoding efficiency, polar codes asymptotically outperform $(J, K)$-regular LDPC codes with a code rate $R \\le 1-\\frac{J(J-1)}{2^J+(J-1)}$ in the low-complexity regime $(T \\le O(NlogN))$.","sentences":["Efficient decoding is crucial to high-throughput and low-power wireless communication scenarios.","A theoretical analysis of the performance-complexity tradeoff toward low-complexity decoding is required for a better understanding of the fundamental limits in the above-mentioned scenarios.","This study aims to explore the performance of decoders with complexity constraints.","Specifically, we investigate the performance of LDPC codes with different numbers of belief-propagation iterations and the performance of polar codes with an SSC decoder.","We found that the asymptotic error rates of both polar codes and LDPC codes are functions of complexity $T$ and code length $N$, in the form of $2^{-a2^{b\\frac{T}{N}}}$, where $a$ and $b$ are constants that depend on channel and coding schemes.","Our analysis reveals the different performance-complexity tradeoffs for LDPC and polar codes.","The results indicate that if one aims to further enhance the decoding efficiency for LDPC codes, the key lies in how to efficiently pass messages on the factor graph.","In terms of decoding efficiency, polar codes asymptotically outperform $(J, K)$-regular LDPC codes with a code rate $R \\le 1-\\frac{J(J-1)}{2^J+(J-1)}$ in the low-complexity regime $(T \\le O(NlogN))$."],"url":"http://arxiv.org/abs/2403.19266v1","category":"cs.IT"}
{"created":"2024-03-28 09:32:43","title":"J-CRe3: A Japanese Conversation Dataset for Real-world Reference Resolution","abstract":"Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users. In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views. To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3). Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home. The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames. These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations. We also constructed an experimental model and clarified the challenges in multimodal reference resolution tasks.","sentences":["Understanding expressions that refer to the physical world is crucial for such human-assisting systems in the real world, as robots that must perform actions that are expected by users.","In real-world reference resolution, a system must ground the verbal information that appears in user interactions to the visual information observed in egocentric views.","To this end, we propose a multimodal reference resolution task and construct a Japanese Conversation dataset for Real-world Reference Resolution (J-CRe3).","Our dataset contains egocentric video and dialogue audio of real-world conversations between two people acting as a master and an assistant robot at home.","The dataset is annotated with crossmodal tags between phrases in the utterances and the object bounding boxes in the video frames.","These tags include indirect reference relations, such as predicate-argument structures and bridging references as well as direct reference relations.","We also constructed an experimental model and clarified the challenges in multimodal reference resolution tasks."],"url":"http://arxiv.org/abs/2403.19259v1","category":"cs.CL"}
{"created":"2024-03-28 09:20:15","title":"Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning","abstract":"Effective agent coordination is crucial in cooperative Multi-Agent Reinforcement Learning (MARL). While agent cooperation can be represented by graph structures, prevailing graph learning methods in MARL are limited. They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient graphs that foster redundant or detrimental information exchanges. Additionally, high computational demands for action-pair calculations in dense graphs impede scalability. To address these challenges, we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for MARL. The LTS-CG leverages agents' historical observations to calculate an agent-pair probability matrix, where a sparse graph is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty. The computational complexity of this procedure is only related to the number of agents. This graph learning process is further augmented by two innovative characteristics: Predict-Future, which enables agents to foresee upcoming observations, and Infer-Present, ensuring a thorough grasp of the environmental context from limited data. These features allow LTS-CG to construct temporal graphs from historical and real-time information, promoting knowledge exchange during policy learning and effective collaboration. Graph learning and agent training occur simultaneously in an end-to-end manner. Our demonstrated results on the StarCraft II benchmark underscore LTS-CG's superior performance.","sentences":["Effective agent coordination is crucial in cooperative Multi-Agent Reinforcement Learning (MARL).","While agent cooperation can be represented by graph structures, prevailing graph learning methods in MARL are limited.","They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient graphs that foster redundant or detrimental information exchanges.","Additionally, high computational demands for action-pair calculations in dense graphs impede scalability.","To address these challenges, we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for MARL.","The LTS-CG leverages agents' historical observations to calculate an agent-pair probability matrix, where a sparse graph is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty.","The computational complexity of this procedure is only related to the number of agents.","This graph learning process is further augmented by two innovative characteristics: Predict-Future, which enables agents to foresee upcoming observations, and Infer-Present, ensuring a thorough grasp of the environmental context from limited data.","These features allow LTS-CG to construct temporal graphs from historical and real-time information, promoting knowledge exchange during policy learning and effective collaboration.","Graph learning and agent training occur simultaneously in an end-to-end manner.","Our demonstrated results on the StarCraft II benchmark underscore LTS-CG's superior performance."],"url":"http://arxiv.org/abs/2403.19253v1","category":"cs.LG"}
{"created":"2024-03-28 09:18:09","title":"Unveiling the Cosmic Cradle: clustering and massive star formation in the enigmatic Galactic bubble N59","abstract":"In this paper, we have conducted an investigation focused on a segment of the $Spitzer$ mid-infrared bubble N59, specifically referred to as R1 within our study. Situated in the inner Galactic plane, this region stands out for its hosting of five 6.7 GHz methanol masers, as well as numerous compact H II regions, massive clumps, filaments, and prominent bright rims. As 6.7 GHz masers are closely linked to the initial phases of high-mass star formation, exploring regions that exhibit a high abundance of these maser detections provides an opportunity to investigate relatively young massive star-forming sites. To characterize the R1 region comprehensively, we utilize multi-wavelength (archival) data from optical to radio wavelengths, together with $^{13}$CO and C$^{18}$O data. Utilizing the $Gaia$ DR3 data, we estimate the distance towards the bubble to be $4.66 \\pm 0.70$ kpc. By combining near-infrared (NIR) and mid-infrared (MIR) data, we identify 12 Class I and 8 Class II sources within R1. Furthermore, spectral energy distribution (SED) analysis of selected sources reveals the presence of four embedded high-mass sources with masses ranging from 8.70-14.20 M$_\\odot$. We also identified several O and B-type stars from radio continuum analysis. Our molecular study uncovers two distinct molecular clouds in the region, which, although spatially close, occupy different regions in velocity space. We also find indications of a potential hub-filament system fostering star formation within the confines of R1. Finally, we propose that the feedback from the H II regions has led to the formation of prominent Bright Rimmed Clouds (BRC) within our region of interest.","sentences":["In this paper, we have conducted an investigation focused on a segment of the $Spitzer$ mid-infrared bubble N59, specifically referred to as R1 within our study.","Situated in the inner Galactic plane, this region stands out for its hosting of five 6.7 GHz methanol masers, as well as numerous compact H II regions, massive clumps, filaments, and prominent bright rims.","As 6.7 GHz masers are closely linked to the initial phases of high-mass star formation, exploring regions that exhibit a high abundance of these maser detections provides an opportunity to investigate relatively young massive star-forming sites.","To characterize the R1 region comprehensively, we utilize multi-wavelength (archival) data from optical to radio wavelengths, together with $^{13}$CO and C$^{18}$O data.","Utilizing the $Gaia$ DR3 data, we estimate the distance towards the bubble to be $4.66 \\pm 0.70$ kpc.","By combining near-infrared (NIR) and mid-infrared (MIR) data, we identify 12 Class I and 8 Class II sources within R1.","Furthermore, spectral energy distribution (SED) analysis of selected sources reveals the presence of four embedded high-mass sources with masses ranging from 8.70-14.20 M$_\\odot$. We also identified several O and B-type stars from radio continuum analysis.","Our molecular study uncovers two distinct molecular clouds in the region, which, although spatially close, occupy different regions in velocity space.","We also find indications of a potential hub-filament system fostering star formation within the confines of R1.","Finally, we propose that the feedback from the H II regions has led to the formation of prominent Bright Rimmed Clouds (BRC) within our region of interest."],"url":"http://arxiv.org/abs/2403.19252v1","category":"astro-ph.GA"}
{"created":"2024-03-28 09:17:27","title":"Arbitrary State Transition of Open Qubit System Based on Switching Control","abstract":"We present a switching control strategy based on Lyapunov control for arbitrary state transitions in open qubit systems. With coherent vector representation, we propose a switching control strategy, which can prevent the state of the qubit from entering invariant sets and singular value sets, effectively driving the system ultimately to a sufficiently small neighborhood of target states. In comparison to existing works, this control strategy relaxes the strict constraints on system models imposed by special target states. Furthermore, we identify conditions under which the open qubit system achieves finite-time stability (FTS) and finite-time contractive stability (FTCS), respectively. This represents a critical improvement in quantum state transitions, especially considering the asymptotic stability of arbitrary target states is unattainable in open quantum systems. The effectiveness of our proposed method is convincingly demonstrated through its application in a qubit system affected by various types of decoherence, including amplitude, dephasing and polarization decoherence.","sentences":["We present a switching control strategy based on Lyapunov control for arbitrary state transitions in open qubit systems.","With coherent vector representation, we propose a switching control strategy, which can prevent the state of the qubit from entering invariant sets and singular value sets, effectively driving the system ultimately to a sufficiently small neighborhood of target states.","In comparison to existing works, this control strategy relaxes the strict constraints on system models imposed by special target states.","Furthermore, we identify conditions under which the open qubit system achieves finite-time stability (FTS) and finite-time contractive stability (FTCS), respectively.","This represents a critical improvement in quantum state transitions, especially considering the asymptotic stability of arbitrary target states is unattainable in open quantum systems.","The effectiveness of our proposed method is convincingly demonstrated through its application in a qubit system affected by various types of decoherence, including amplitude, dephasing and polarization decoherence."],"url":"http://arxiv.org/abs/2403.19251v1","category":"quant-ph"}
{"created":"2024-03-28 09:11:51","title":"Optical tools for laser machining along six orders of magnitude","abstract":"We present an overview on the development and characterization of multiscale laser processing optics for versatile material modifications across more than six orders of magnitude. Starting with solutions for micromachining we present high-NA microscope objectives creating sub-wavelength material modifications on macroscopic scales with highest peak intensities. Moving on to the millimeter range, the adaptability and scalability of scanning optics is examined for large-area machining. Finally, we explore line beam optics in the meter range, evaluating their use in uniform material processing using average powers above 100kW. This study provides an insight into the design and performance characteristics of such optics and demonstrates their potential in advanced laser processing.","sentences":["We present an overview on the development and characterization of multiscale laser processing optics for versatile material modifications across more than six orders of magnitude.","Starting with solutions for micromachining we present high-NA microscope objectives creating sub-wavelength material modifications on macroscopic scales with highest peak intensities.","Moving on to the millimeter range, the adaptability and scalability of scanning optics is examined for large-area machining.","Finally, we explore line beam optics in the meter range, evaluating their use in uniform material processing using average powers above 100kW.","This study provides an insight into the design and performance characteristics of such optics and demonstrates their potential in advanced laser processing."],"url":"http://arxiv.org/abs/2403.19250v1","category":"physics.optics"}
{"created":"2024-03-28 09:06:23","title":"MPXGAT: An Attention based Deep Learning Model for Multiplex Graphs Embedding","abstract":"Graph representation learning has rapidly emerged as a pivotal field of study. Despite its growing popularity, the majority of research has been confined to embedding single-layer graphs, which fall short in representing complex systems with multifaceted relationships. To bridge this gap, we introduce MPXGAT, an innovative attention-based deep learning model tailored to multiplex graph embedding. Leveraging the robustness of Graph Attention Networks (GATs), MPXGAT captures the structure of multiplex networks by harnessing both intra-layer and inter-layer connections. This exploitation facilitates accurate link prediction within and across the network's multiple layers. Our comprehensive experimental evaluation, conducted on various benchmark datasets, confirms that MPXGAT consistently outperforms state-of-the-art competing algorithms.","sentences":["Graph representation learning has rapidly emerged as a pivotal field of study.","Despite its growing popularity, the majority of research has been confined to embedding single-layer graphs, which fall short in representing complex systems with multifaceted relationships.","To bridge this gap, we introduce MPXGAT, an innovative attention-based deep learning model tailored to multiplex graph embedding.","Leveraging the robustness of Graph Attention Networks (GATs), MPXGAT captures the structure of multiplex networks by harnessing both intra-layer and inter-layer connections.","This exploitation facilitates accurate link prediction within and across the network's multiple layers.","Our comprehensive experimental evaluation, conducted on various benchmark datasets, confirms that MPXGAT consistently outperforms state-of-the-art competing algorithms."],"url":"http://arxiv.org/abs/2403.19246v1","category":"cs.LG"}
{"created":"2024-03-28 08:45:56","title":"Regularized dynamical parametric approximation","abstract":"This paper studies the numerical approximation of evolution equations by nonlinear parametrizations $u(t)=\\Phi(q(t))$ with time-dependent parameters $q(t)$, which are to be determined in the computation. The motivation comes from approximations in quantum dynamics by multiple Gaussians and approximations of various dynamical problems by tensor networks and neural networks. In all these cases, the parametrization is typically irregular: the derivative $\\Phi'(q)$ can have arbitrarily small singular values and may have varying rank. We derive approximation results for a regularized approach in the time-continuous case as well as in time-discretized cases. With a suitable choice of the regularization parameter and the time stepsize, the approach can be successfully applied in irregular situations, even though it runs counter to the basic principle in numerical analysis to avoid solving ill-posed subproblems when aiming for a stable algorithm. Numerical experiments with sums of Gaussians for approximating quantum dynamics and with neural networks for approximating the flow map of a system of ordinary differential equations illustrate and complement the theoretical results.","sentences":["This paper studies the numerical approximation of evolution equations by nonlinear parametrizations $u(t)=\\Phi(q(t))$ with time-dependent parameters $q(t)$, which are to be determined in the computation.","The motivation comes from approximations in quantum dynamics by multiple Gaussians and approximations of various dynamical problems by tensor networks and neural networks.","In all these cases, the parametrization is typically irregular: the derivative $\\Phi'(q)$ can have arbitrarily small singular values and may have varying rank.","We derive approximation results for a regularized approach in the time-continuous case as well as in time-discretized cases.","With a suitable choice of the regularization parameter and the time stepsize, the approach can be successfully applied in irregular situations, even though it runs counter to the basic principle in numerical analysis to avoid solving ill-posed subproblems when aiming for a stable algorithm.","Numerical experiments with sums of Gaussians for approximating quantum dynamics and with neural networks for approximating the flow map of a system of ordinary differential equations illustrate and complement the theoretical results."],"url":"http://arxiv.org/abs/2403.19234v1","category":"math.NA"}
{"created":"2024-03-28 08:44:36","title":"AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search","abstract":"Training-free network architecture search (NAS) aims to discover high-performing networks with zero-cost proxies, capturing network characteristics related to the final performance. However, network rankings estimated by previous training-free NAS methods have shown weak correlations with the performance. To address this issue, we propose AZ-NAS, a novel approach that leverages the ensemble of various zero-cost proxies to enhance the correlation between a predicted ranking of networks and the ground truth substantially in terms of the performance. To achieve this, we introduce four novel zero-cost proxies that are complementary to each other, analyzing distinct traits of architectures in the views of expressivity, progressivity, trainability, and complexity. The proxy scores can be obtained simultaneously within a single forward and backward pass, making an overall NAS process highly efficient. In order to integrate the rankings predicted by our proxies effectively, we introduce a non-linear ranking aggregation method that highlights the networks highly-ranked consistently across all the proxies. Experimental results conclusively demonstrate the efficacy and efficiency of AZ-NAS, outperforming state-of-the-art methods on standard benchmarks, all while maintaining a reasonable runtime cost.","sentences":["Training-free network architecture search (NAS) aims to discover high-performing networks with zero-cost proxies, capturing network characteristics related to the final performance.","However, network rankings estimated by previous training-free NAS methods have shown weak correlations with the performance.","To address this issue, we propose AZ-NAS, a novel approach that leverages the ensemble of various zero-cost proxies to enhance the correlation between a predicted ranking of networks and the ground truth substantially in terms of the performance.","To achieve this, we introduce four novel zero-cost proxies that are complementary to each other, analyzing distinct traits of architectures in the views of expressivity, progressivity, trainability, and complexity.","The proxy scores can be obtained simultaneously within a single forward and backward pass, making an overall NAS process highly efficient.","In order to integrate the rankings predicted by our proxies effectively, we introduce a non-linear ranking aggregation method that highlights the networks highly-ranked consistently across all the proxies.","Experimental results conclusively demonstrate the efficacy and efficiency of AZ-NAS, outperforming state-of-the-art methods on standard benchmarks, all while maintaining a reasonable runtime cost."],"url":"http://arxiv.org/abs/2403.19232v1","category":"cs.CV"}
{"created":"2024-03-28 08:43:24","title":"Optomechanical cavities based on epitaxial GaP on nominally (001)-oriented Si","abstract":"Gallium phosphide (GaP) has recently received considerable attention as a suitable material for building photonic integrated circuits due to its remarkable optical and piezoelectric properties. Usually, GaP is grown epitaxially on III-V substrates to keep its crystallinity and later transferred to silicon wafers for further processing. Here, an alternative promising route for the fabrication of optomechanical (OM) cavities on GaP epitaxially grown on nominally (001)-oriented Si is introduced by using a two-step process consisting of a low-temperature etching of GaP followed by selective etching of the underneath silicon. The low-temperature (-30 $^o$C) during the dry-etching of GaP hinders the lateral etching rate, preserving the pattern with a deviation between the design and the pattern in the GaP layer lower than 5 %, avoiding the complex process of transferring and bonding a GaP wafer to a silicon-on-insulator wafer. To demonstrate the quality and feasibility of the proposed fabrication route, suspended OM cavities are fabricated and experimentally characterized. The cavities show optical quality factors between 10$^3$ and 10$^4$, and localized mechanical resonances at frequencies around 3.1 GHz. Both optical and mechanical resonances are close to those previously reported on crystalline GaP structures. These results suggest a simple and low-cost way to build GaP-based photonic devices directly integrated on industry-standard Si(001) photonic wafers.","sentences":["Gallium phosphide (GaP) has recently received considerable attention as a suitable material for building photonic integrated circuits due to its remarkable optical and piezoelectric properties.","Usually, GaP is grown epitaxially on III-V substrates to keep its crystallinity and later transferred to silicon wafers for further processing.","Here, an alternative promising route for the fabrication of optomechanical (OM) cavities on GaP epitaxially grown on nominally (001)-oriented Si is introduced by using a two-step process consisting of a low-temperature etching of GaP followed by selective etching of the underneath silicon.","The low-temperature (-30 $^o$C) during the dry-etching of GaP hinders the lateral etching rate, preserving the pattern with a deviation between the design and the pattern in the GaP layer lower than 5 %, avoiding the complex process of transferring and bonding a GaP wafer to a silicon-on-insulator wafer.","To demonstrate the quality and feasibility of the proposed fabrication route, suspended OM cavities are fabricated and experimentally characterized.","The cavities show optical quality factors between 10$^3$ and 10$^4$, and localized mechanical resonances at frequencies around 3.1 GHz.","Both optical and mechanical resonances are close to those previously reported on crystalline GaP structures.","These results suggest a simple and low-cost way to build GaP-based photonic devices directly integrated on industry-standard Si(001) photonic wafers."],"url":"http://arxiv.org/abs/2403.19230v1","category":"physics.optics"}
{"created":"2024-03-28 08:43:14","title":"Duality between ${\\rm U}\\times{\\rm U}$ and ${\\rm O}\\times{\\rm USp}$ theories from $\\hat{A}_{3}=\\hat{D}_{3}$","abstract":"We study the duality between the ABJ(M) theory at Chern-Simons level $k=4$ and the orientifold ABJ theory at Chern-Simons level $k=1$ by using the $S^{3}$ partition function. The partition function can be computed using the supersymmetric localization in terms of a matrix model, and we derive an ideal Fermi gas system by applying the Fermi gas formalism to the matrix model. By using this formalism, we show that the matrix model can be embedded in the partition function of $\\hat{A}_{3}$ or $\\hat{D}_{3}$ quiver theories, and the equality of these quiver theories, which comes from $\\hat{A}_{3}=\\hat{D}_{3}$, leads to exact relations of the matrix models. These exact relations can be used to check the identification of the flavor symmetries. The perfect agreement between the matrix models also implies that no decoupled sector is needed.","sentences":["We study the duality between the ABJ(M) theory at Chern-Simons level $k=4$ and the orientifold ABJ theory at Chern-Simons level $k=1$ by using the $S^{3}$ partition function.","The partition function can be computed using the supersymmetric localization in terms of a matrix model, and we derive an ideal Fermi gas system by applying the Fermi gas formalism to the matrix model.","By using this formalism, we show that the matrix model can be embedded in the partition function of $\\hat{A}_{3}$ or $\\hat{D}_{3}$ quiver theories, and the equality of these quiver theories, which comes from $\\hat{A}_{3}=\\hat{D}_{3}$, leads to exact relations of the matrix models.","These exact relations can be used to check the identification of the flavor symmetries.","The perfect agreement between the matrix models also implies that no decoupled sector is needed."],"url":"http://arxiv.org/abs/2403.19228v1","category":"hep-th"}
{"created":"2024-03-28 08:23:13","title":"Convolutional network learning of self-consistent electron density via grid-projected atomic fingerprints","abstract":"The self-consistent field (SCF) generation of the three-dimensional (3D) electron density distribution ($\\rho$) represents a fundamental aspect of density functional theory (DFT) and related first-principles calculations, and how one can shorten or bypass the SCF loop represents a critical question from both practical and fundamental standpoints. Herein, a machine learning strategy DeepSCF is presented in which the map between the SCF $\\rho$ and the initial guess density ($\\rho_0$) constructed by the summation of neutral atomic densities is learned using 3D convolutional neural networks (CNNs). High accuracy and transferability of DeepSCF are achieved by expanding the input features to include atomic fingerprints beyond $\\rho_0$ and encoding them on a 3D grid. The prediction of the residual density ($\\delta\\rho$) rather than $\\rho$ itself is targeted, and, since $\\delta\\rho$ corresponds to chemical bonding information, a dataset of small-sized organic molecules featuring diverse bonding characters is adopted. After enhancing the fidelity of the method by subjecting the atomic geometries in the dataset to random strains and rotations, the effectiveness of DeepSCF is finally demonstrated using a complex large carbon nanotube-based DNA sequencer model. This work evidences that the nearsightedness in electronic structures can be optimally represented via the local connectivity in CNNs.","sentences":["The self-consistent field (SCF) generation of the three-dimensional (3D) electron density distribution ($\\rho$) represents a fundamental aspect of density functional theory (DFT) and related first-principles calculations, and how one can shorten or bypass the SCF loop represents a critical question from both practical and fundamental standpoints.","Herein, a machine learning strategy DeepSCF is presented in which the map between the SCF $\\rho$ and the initial guess density ($\\rho_0$) constructed by the summation of neutral atomic densities is learned using 3D convolutional neural networks (CNNs).","High accuracy and transferability of DeepSCF are achieved by expanding the input features to include atomic fingerprints beyond $\\rho_0$ and encoding them on a 3D grid.","The prediction of the residual density ($\\delta\\rho$) rather than $\\rho$ itself is targeted, and, since $\\delta\\rho$ corresponds to chemical bonding information, a dataset of small-sized organic molecules featuring diverse bonding characters is adopted.","After enhancing the fidelity of the method by subjecting the atomic geometries in the dataset to random strains and rotations, the effectiveness of DeepSCF is finally demonstrated using a complex large carbon nanotube-based DNA sequencer model.","This work evidences that the nearsightedness in electronic structures can be optimally represented via the local connectivity in CNNs."],"url":"http://arxiv.org/abs/2403.19214v1","category":"physics.comp-ph"}
{"created":"2024-03-28 08:21:56","title":"Learning Multiple Representations with Inconsistency-Guided Detail Regularization for Mask-Guided Matting","abstract":"Mask-guided matting networks have achieved significant improvements and have shown great potential in practical applications in recent years. However, simply learning matting representation from synthetic and lack-of-real-world-diversity matting data, these approaches tend to overfit low-level details in wrong regions, lack generalization to objects with complex structures and real-world scenes such as shadows, as well as suffer from interference of background lines or textures. To address these challenges, in this paper, we propose a novel auxiliary learning framework for mask-guided matting models, incorporating three auxiliary tasks: semantic segmentation, edge detection, and background line detection besides matting, to learn different and effective representations from different types of data and annotations. Our framework and model introduce the following key aspects: (1) to learn real-world adaptive semantic representation for objects with diverse and complex structures under real-world scenes, we introduce extra semantic segmentation and edge detection tasks on more diverse real-world data with segmentation annotations; (2) to avoid overfitting on low-level details, we propose a module to utilize the inconsistency between learned segmentation and matting representations to regularize detail refinement; (3) we propose a novel background line detection task into our auxiliary learning framework, to suppress interference of background lines or textures. In addition, we propose a high-quality matting benchmark, Plant-Mat, to evaluate matting methods on complex structures. Extensively quantitative and qualitative results show that our approach outperforms state-of-the-art mask-guided methods.","sentences":["Mask-guided matting networks have achieved significant improvements and have shown great potential in practical applications in recent years.","However, simply learning matting representation from synthetic and lack-of-real-world-diversity matting data, these approaches tend to overfit low-level details in wrong regions, lack generalization to objects with complex structures and real-world scenes such as shadows, as well as suffer from interference of background lines or textures.","To address these challenges, in this paper, we propose a novel auxiliary learning framework for mask-guided matting models, incorporating three auxiliary tasks: semantic segmentation, edge detection, and background line detection besides matting, to learn different and effective representations from different types of data and annotations.","Our framework and model introduce the following key aspects: (1) to learn real-world adaptive semantic representation for objects with diverse and complex structures under real-world scenes, we introduce extra semantic segmentation and edge detection tasks on more diverse real-world data with segmentation annotations; (2) to avoid overfitting on low-level details, we propose a module to utilize the inconsistency between learned segmentation and matting representations to regularize detail refinement; (3) we propose a novel background line detection task into our auxiliary learning framework, to suppress interference of background lines or textures.","In addition, we propose a high-quality matting benchmark, Plant-Mat, to evaluate matting methods on complex structures.","Extensively quantitative and qualitative results show that our approach outperforms state-of-the-art mask-guided methods."],"url":"http://arxiv.org/abs/2403.19213v1","category":"cs.CV"}
{"created":"2024-03-28 08:04:01","title":"The Lorentz force at work: multi-phase magnetohydrodynamics throughout a flare lifespan","abstract":"The hour-long, gradual phase of solar flares is well-observed across the electromagnetic spectrum, demonstrating many multi-phase aspects, where cold condensations form within the heated post-flare system, but a complete three-dimensional (3D) model is lacking. Using a state-of-the-art 3D magnetohydrodynamic simulation, we identify the key role played by the Lorentz force through the entire flare lifespan, and show that slow variations in the post-flare magnetic field achieve the bulk of the energy release. Synthetic images in multiple passbands closely match flare observations, and we quantify the role of conductive, radiative and Lorentz force work contributions from flare onset to decay. This highlights how the non-force-free nature of the magnetic topology is crucial to trigger Rayleigh-Taylor dynamics, observed as waving coronal rays in extreme ultraviolet observations. Our C-class solar flare reproduces multi-phase aspects such as post-flare coronal rain. In agreement with observations, we find strands of cooler plasma forming spontaneously by catastrophic cooling, leading to cool plasma draining down the post-flare loops. As there is force balance between magnetic pressure and tension and the plasma pressure in gradual-phase flare loops, this has potential for coronal seismology to decipher the magnetic field strength variation from observations.","sentences":["The hour-long, gradual phase of solar flares is well-observed across the electromagnetic spectrum, demonstrating many multi-phase aspects, where cold condensations form within the heated post-flare system, but a complete three-dimensional (3D) model is lacking.","Using a state-of-the-art 3D magnetohydrodynamic simulation, we identify the key role played by the Lorentz force through the entire flare lifespan, and show that slow variations in the post-flare magnetic field achieve the bulk of the energy release.","Synthetic images in multiple passbands closely match flare observations, and we quantify the role of conductive, radiative and Lorentz force work contributions from flare onset to decay.","This highlights how the non-force-free nature of the magnetic topology is crucial to trigger Rayleigh-Taylor dynamics, observed as waving coronal rays in extreme ultraviolet observations.","Our C-class solar flare reproduces multi-phase aspects such as post-flare coronal rain.","In agreement with observations, we find strands of cooler plasma forming spontaneously by catastrophic cooling, leading to cool plasma draining down the post-flare loops.","As there is force balance between magnetic pressure and tension and the plasma pressure in gradual-phase flare loops, this has potential for coronal seismology to decipher the magnetic field strength variation from observations."],"url":"http://arxiv.org/abs/2403.19204v1","category":"astro-ph.SR"}
{"created":"2024-03-28 07:52:59","title":"Hamiltonian mechanics of \"magnetic'' solitons in two-component Bose-Einstein condensates","abstract":"We consider motion of a \"magnetic'' soliton in two-component condensates along a non-uniform and time-dependent backgrounds in framework of the Hamiltonian mechanics. Our approach is based on generalization of Stokes' remark that soliton's velocity is related with its inverse half-width by the dispersion law for linear waves continued to the region of complex wave numbers. We obtain expressions for the canonical momentum and the Hamiltonian as functions of soliton's velocity and transform the Hamilton equations to the Newton-like equation. The theory is illustrated by several examples of concrete soliton's dynamics.","sentences":["We consider motion of a \"magnetic'' soliton in two-component condensates along a non-uniform and time-dependent backgrounds in framework of the Hamiltonian mechanics.","Our approach is based on generalization of Stokes' remark that soliton's velocity is related with its inverse half-width by the dispersion law for linear waves continued to the region of complex wave numbers.","We obtain expressions for the canonical momentum and the Hamiltonian as functions of soliton's velocity and transform the Hamilton equations to the Newton-like equation.","The theory is illustrated by several examples of concrete soliton's dynamics."],"url":"http://arxiv.org/abs/2403.19199v1","category":"nlin.PS"}
{"created":"2024-03-28 07:52:56","title":"Phase coexistence in a weakly stochastic reaction-diffusion system","abstract":"We investigate phase coexistence in a weakly stochastic reaction-diffusion system without assuming a continuum description. Concretely, for $(2N+1)$ diffusion-coupled vessels in which a chemical reaction exhibiting bistability occurs, we derive a condition for the phase coexistence in the limit $N \\to \\infty$. We then find that the phase coexistence condition depends on the rate of hopping between neighboring vessels. The conditions in the high- and low-hopping-rate limits are expressed in terms of two different potentials which are determined from the chemical reaction model in a single vessel.","sentences":["We investigate phase coexistence in a weakly stochastic reaction-diffusion system without assuming a continuum description.","Concretely, for $(2N+1)$ diffusion-coupled vessels in which a chemical reaction exhibiting bistability occurs, we derive a condition for the phase coexistence in the limit $N \\to \\infty$. We then find that the phase coexistence condition depends on the rate of hopping between neighboring vessels.","The conditions in the high- and low-hopping-rate limits are expressed in terms of two different potentials which are determined from the chemical reaction model in a single vessel."],"url":"http://arxiv.org/abs/2403.19198v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-28 07:48:09","title":"Efficient Nonlinear Model Predictive Control by Leveraging Linear Parameter-Varying Embedding and Sequential Quadratic Programming","abstract":"In this study, we are concerned with nonlinear model predictive control (NMPC) schemes that, through the linear parameter-varying (LPV) formulation, nonlinear systems can be embedded and with a sequential quadratic program (SQP) can provide efficient solutions for the NMPC. We revisit the different constrained optimization formulations known as simultaneous and sequential approaches tailored with the LPV predictor, constituting, in general, a nonlinear program (NLP). The derived NLPs are represented through the Lagrangian formulation, which enforces the Karush-Kuhn-Tucker (KKT) optimality conditions for the optimization problem to be solvable. The main novelty suggests that the problem can still be efficiently solvable with a significantly lower computational load by approximating certain terms to reduce the computational burden. The proposed method is compared with other state-of-the-art approaches on standard performance measures. Moreover, we provide convergence analysis that can assert further theoretical guarantees in control, such as stability and recursive feasibility. Finally, the method is tested through well-studied control benchmarks such as the forced Van der Pol oscillator and the dynamic unicycle.","sentences":["In this study, we are concerned with nonlinear model predictive control (NMPC) schemes that, through the linear parameter-varying (LPV) formulation, nonlinear systems can be embedded and with a sequential quadratic program (SQP) can provide efficient solutions for the NMPC.","We revisit the different constrained optimization formulations known as simultaneous and sequential approaches tailored with the LPV predictor, constituting, in general, a nonlinear program (NLP).","The derived NLPs are represented through the Lagrangian formulation, which enforces the Karush-Kuhn-Tucker (KKT) optimality conditions for the optimization problem to be solvable.","The main novelty suggests that the problem can still be efficiently solvable with a significantly lower computational load by approximating certain terms to reduce the computational burden.","The proposed method is compared with other state-of-the-art approaches on standard performance measures.","Moreover, we provide convergence analysis that can assert further theoretical guarantees in control, such as stability and recursive feasibility.","Finally, the method is tested through well-studied control benchmarks such as the forced Van der Pol oscillator and the dynamic unicycle."],"url":"http://arxiv.org/abs/2403.19195v1","category":"math.OC"}
{"created":"2024-03-28 07:42:06","title":"Superfluid Oscillator Circuit with Quantum Current Regulator","abstract":"We examine the properties of atomic current in a superfluid oscillating circuit consisting of a mesoscopic channel that connects two reservoirs of a Bose-Einstein condensate. We investigate the presence of a critical current in the channel and examine how the amplitude of the oscillations in the number imbalance between the two reservoirs varies with system parameters. In addition to highlighting that the dissipative resistance stems from the formation of vortex pairs, we also illustrate the role of these vortex pairs as a quantum current regulator. The dissipation strength is discrete based on the number imbalance, which corresponds to the emergence of vortex pairs in the system. Our findings indicate that the circuit demonstrates characteristics of both voltage-limiting and current-limiting mechanisms. To model the damping behavior of the atomic superfluid circuit, we develop an equivalent LC oscillator circuit with a quantum current regulator.","sentences":["We examine the properties of atomic current in a superfluid oscillating circuit consisting of a mesoscopic channel that connects two reservoirs of a Bose-Einstein condensate.","We investigate the presence of a critical current in the channel and examine how the amplitude of the oscillations in the number imbalance between the two reservoirs varies with system parameters.","In addition to highlighting that the dissipative resistance stems from the formation of vortex pairs, we also illustrate the role of these vortex pairs as a quantum current regulator.","The dissipation strength is discrete based on the number imbalance, which corresponds to the emergence of vortex pairs in the system.","Our findings indicate that the circuit demonstrates characteristics of both voltage-limiting and current-limiting mechanisms.","To model the damping behavior of the atomic superfluid circuit, we develop an equivalent LC oscillator circuit with a quantum current regulator."],"url":"http://arxiv.org/abs/2403.19191v1","category":"quant-ph"}
{"created":"2024-03-28 07:30:28","title":"Optimization hardness constrains ecological transients","abstract":"Living systems operate far from equilibrium, yet few general frameworks provide global bounds on biological transients. Here, we frame equilibration in complex ecosystems as the process of solving an analogue optimization problem. We show that functional redundancies among species in a complex foodweb lead to harder ill-conditioned problems, manifesting as slow modes on a nearly-degenerate solution landscape. Common dimensionality reduction methods precondition ecological dynamics, by isolating fast relaxation from slow solving timescales that exhibit transient chaos for harder problem instances. In evolutionary simulations, we show that selection for steady-state diversity produces ill-conditioning, an effect quantifiable using complexity bounds for constrained optimization. Our results demonstrate the physical toll of computational constraints on biological dynamics.","sentences":["Living systems operate far from equilibrium, yet few general frameworks provide global bounds on biological transients.","Here, we frame equilibration in complex ecosystems as the process of solving an analogue optimization problem.","We show that functional redundancies among species in a complex foodweb lead to harder ill-conditioned problems, manifesting as slow modes on a nearly-degenerate solution landscape.","Common dimensionality reduction methods precondition ecological dynamics, by isolating fast relaxation from slow solving timescales that exhibit transient chaos for harder problem instances.","In evolutionary simulations, we show that selection for steady-state diversity produces ill-conditioning, an effect quantifiable using complexity bounds for constrained optimization.","Our results demonstrate the physical toll of computational constraints on biological dynamics."],"url":"http://arxiv.org/abs/2403.19186v1","category":"physics.bio-ph"}
{"created":"2024-03-28 07:28:53","title":"Deep CSI Compression for Dual-Polarized Massive MIMO Channels with Disentangled Representation Learning","abstract":"Channel state information (CSI) feedback is critical for achieving the promised advantages of enhancing spectral and energy efficiencies in massive multiple-input multiple-output (MIMO) wireless communication systems. Deep learning (DL)-based methods have been proven effective in reducing the required signaling overhead for CSI feedback. In practical dual-polarized MIMO scenarios, channels in the vertical and horizontal polarization directions tend to exhibit high polarization correlation. To fully exploit the inherent propagation similarity within dual-polarized channels, we propose a disentangled representation neural network (NN) for CSI feedback, referred to as DiReNet. The proposed DiReNet disentangles dual-polarized CSI into three components: polarization-shared information, vertical polarization-specific information, and horizontal polarization-specific information. This disentanglement of dual-polarized CSI enables the minimization of information redundancy caused by the polarization correlation and improves the performance of CSI compression and recovery. Additionally, flexible quantization and network extension schemes are designed. Consequently, our method provides a pragmatic solution for CSI feedback to harness the physical MIMO polarization as a priori information. Our experimental results show that the performance of our proposed DiReNet surpasses that of existing DL-based networks, while also effectively reducing the number of network parameters by nearly one third.","sentences":["Channel state information (CSI) feedback is critical for achieving the promised advantages of enhancing spectral and energy efficiencies in massive multiple-input multiple-output (MIMO) wireless communication systems.","Deep learning (DL)-based methods have been proven effective in reducing the required signaling overhead for CSI feedback.","In practical dual-polarized MIMO scenarios, channels in the vertical and horizontal polarization directions tend to exhibit high polarization correlation.","To fully exploit the inherent propagation similarity within dual-polarized channels, we propose a disentangled representation neural network (NN) for CSI feedback, referred to as DiReNet.","The proposed DiReNet disentangles dual-polarized CSI into three components: polarization-shared information, vertical polarization-specific information, and horizontal polarization-specific information.","This disentanglement of dual-polarized CSI enables the minimization of information redundancy caused by the polarization correlation and improves the performance of CSI compression and recovery.","Additionally, flexible quantization and network extension schemes are designed.","Consequently, our method provides a pragmatic solution for CSI feedback to harness the physical MIMO polarization as a priori information.","Our experimental results show that the performance of our proposed DiReNet surpasses that of existing DL-based networks, while also effectively reducing the number of network parameters by nearly one third."],"url":"http://arxiv.org/abs/2403.19185v1","category":"cs.IT"}
{"created":"2024-03-28 07:22:16","title":"Make Large Language Model a Better Ranker","abstract":"The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed. However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms. These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models. While some studies have delved into list-wise approaches, they fall short in ranking tasks. This shortfall is attributed to the misalignment between the objectives of ranking and language generation. To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO). ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems. A key feature of ALRO is the introduction of soft lambda loss, an adaptation of lambda loss tailored to suit language generation tasks. Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference. Our evaluative studies reveal that ALRO outperforms existing embedding-based recommendation methods and the existing LLM-based recommendation baselines, highlighting its efficacy.","sentences":["The evolution of Large Language Models (LLMs) has significantly enhanced capabilities across various fields, leading to a paradigm shift in how Recommender Systems (RSs) are conceptualized and developed.","However, existing research primarily focuses on point-wise and pair-wise recommendation paradigms.","These approaches prove inefficient in LLM-based recommenders due to the high computational cost of utilizing Large Language Models.","While some studies have delved into list-wise approaches, they fall short in ranking tasks.","This shortfall is attributed to the misalignment between the objectives of ranking and language generation.","To this end, this paper introduces the Language Model Framework with Aligned Listwise Ranking Objectives (ALRO).","ALRO is designed to bridge the gap between the capabilities of LLMs and the nuanced requirements of ranking tasks within recommender systems.","A key feature of ALRO is the introduction of soft lambda loss, an adaptation of lambda loss tailored to suit language generation tasks.","Additionally, ALRO incorporates a permutation-sensitive learning mechanism that addresses position bias, a prevalent issue in generative models, without imposing additional computational burdens during inference.","Our evaluative studies reveal that ALRO outperforms existing embedding-based recommendation methods and the existing LLM-based recommendation baselines, highlighting its efficacy."],"url":"http://arxiv.org/abs/2403.19181v1","category":"cs.IR"}
{"created":"2024-03-28 07:16:31","title":"A Multi-hop Secure UWOC assisted Local Area Network for UIoT and Underwater Monitoring","abstract":"Underwater environment is substantially less explored territory as compared to earth surface due to lack of robust underwater communication infrastructure. For Internet of Underwater things connectivity, underwater wireless optical communication can play a vital role, compared to conventional radio frequency communication, due to longer range, high data rate, low latency, and unregulated bandwidth. This study proposes underwater wireless optical communication driven local area network UWOC LAN, comprised of multiple network nodes with optical transceivers. Moreover, the temperature sensor data is encapsulated with individual authentication identity to enhance the security of the framework at the user end. The proposed system is evaluated in a specially designed water tank of 4 meters. The proposed system evaluation analysis shows that the system can transmit underwater temperature data reliably in real time. The proposed secure UWOC LAN is tested within a communication range of 16 meters by incorporating multi hop connectivity to monitor the underwater environment.","sentences":["Underwater environment is substantially less explored territory as compared to earth surface due to lack of robust underwater communication infrastructure.","For Internet of Underwater things connectivity, underwater wireless optical communication can play a vital role, compared to conventional radio frequency communication, due to longer range, high data rate, low latency, and unregulated bandwidth.","This study proposes underwater wireless optical communication driven local area network UWOC LAN, comprised of multiple network nodes with optical transceivers.","Moreover, the temperature sensor data is encapsulated with individual authentication identity to enhance the security of the framework at the user end.","The proposed system is evaluated in a specially designed water tank of 4 meters.","The proposed system evaluation analysis shows that the system can transmit underwater temperature data reliably in real time.","The proposed secure UWOC LAN is tested within a communication range of 16 meters by incorporating multi hop connectivity to monitor the underwater environment."],"url":"http://arxiv.org/abs/2403.19180v1","category":"eess.SP"}
{"created":"2024-03-28 07:09:46","title":"Environmental monitoring using orbital angular momentum mode decomposition enhanced machine learning","abstract":"Atmospheric interaction with light has been an area of fascination for many researchers over the last century. Environmental conditions, such as temperature and wind speed, heavily influence the complex and rapidly varying optical distortions propagating optical fields experience. The continuous random phase fluctuations commonly make deciphering the exact origins of specific optical aberrations challenging. The generation of eddies is a major contributor to atmospheric turbulence, similar in geometric structure to optical vortices that sit at the centre of OAM beams. Decomposing the received optical fields into OAM provides a unique spatial similarity that can be used to analyse turbulent channels. In this work, we present a novel mode decomposition assisted machine learning approach that reveals trainable features in the distortions of vortex beams that allow for effective environmental monitoring. This novel technique can be used reliably with Support Vector Machine regression models to measure temperature variations of 0.49C and wind speed variations of 0.029 m/s over a 36m experimental turbulent free-space channels with controllable and verifiable temperature and wind speed with short 3s measurement. The predictable nature of these findings could indicate the presence of an underlying physical relationship between environmental conditions that lead to specific eddy formation and the OAM spiral spectra.","sentences":["Atmospheric interaction with light has been an area of fascination for many researchers over the last century.","Environmental conditions, such as temperature and wind speed, heavily influence the complex and rapidly varying optical distortions propagating optical fields experience.","The continuous random phase fluctuations commonly make deciphering the exact origins of specific optical aberrations challenging.","The generation of eddies is a major contributor to atmospheric turbulence, similar in geometric structure to optical vortices that sit at the centre of OAM beams.","Decomposing the received optical fields into OAM provides a unique spatial similarity that can be used to analyse turbulent channels.","In this work, we present a novel mode decomposition assisted machine learning approach that reveals trainable features in the distortions of vortex beams that allow for effective environmental monitoring.","This novel technique can be used reliably with Support Vector Machine regression models to measure temperature variations of 0.49C and wind speed variations of 0.029 m/s over a 36m experimental turbulent free-space channels with controllable and verifiable temperature and wind speed with short 3s measurement.","The predictable nature of these findings could indicate the presence of an underlying physical relationship between environmental conditions that lead to specific eddy formation and the OAM spiral spectra."],"url":"http://arxiv.org/abs/2403.19179v1","category":"physics.optics"}
{"created":"2024-03-28 06:51:59","title":"Design and Evaluation of a DC Microgrid Testbed for DER Integration and Power Management","abstract":"This paper presents a DC microgrid testbed setup that consists of various Distributed Energy Resources (DERs) including solar Photovoltaics (PV), supercapacitors for voltage regulation, and Battery Energy Storage Systems (BESS). The DC microgrid accommodates both non-flexible and flexible loads which can be dynamically adjusted based on PV power availability. The integration of the setup with the Hyphae Autonomous Power Interchange System (APIS) framework automates energy transfer within the BESS, ensuring efficient power management and optimizing the overall efficiency of the DC microgrid. Furthermore, the setup is validated in terms of the efficacy of the proposed model via real-time simulation, facilitated by the Speedgoat baseline real-time target Hardware-in-the-Loop (HIL) machine. The results demonstrate the model's adeptness in efficiently managing power sharing, emphasizing the capabilities of the DC microgrid setup in terms of performance and reliability in dynamic energy scenarios as well as enhancing the resilience of the grid amidst PV uncertainties.","sentences":["This paper presents a DC microgrid testbed setup that consists of various Distributed Energy Resources (DERs) including solar Photovoltaics (PV), supercapacitors for voltage regulation, and Battery Energy Storage Systems (BESS).","The DC microgrid accommodates both non-flexible and flexible loads which can be dynamically adjusted based on PV power availability.","The integration of the setup with the Hyphae Autonomous Power Interchange System (APIS) framework automates energy transfer within the BESS, ensuring efficient power management and optimizing the overall efficiency of the DC microgrid.","Furthermore, the setup is validated in terms of the efficacy of the proposed model via real-time simulation, facilitated by the Speedgoat baseline real-time target Hardware-in-the-Loop (HIL) machine.","The results demonstrate the model's adeptness in efficiently managing power sharing, emphasizing the capabilities of the DC microgrid setup in terms of performance and reliability in dynamic energy scenarios as well as enhancing the resilience of the grid amidst PV uncertainties."],"url":"http://arxiv.org/abs/2403.19176v1","category":"eess.SY"}
{"created":"2024-03-28 06:28:47","title":"Tunable Superconducting Magnetic Levitation with Self-Stability","abstract":"Magnetic levitation based on the flux pinning nature of type II superconductors has the merit of self-stability, making it appealing for applications such as high speed bearings, maglev trains, space generators, etc. However, such levitation systems physically rely on the superconductor pre-capturing magnetic flux (i.e. field cooling process) before establishing the levitation state which is nonadjustable afterwards. Moreover, practical type II superconductors in the levitation system inevitably suffer from various sources of energy losses, leading to continuous levitation force decay. These intrinsic drawbacks make superconducting maglev inflexible and impractical for long term operation. Here we propose and demonstrate a new form of superconducting maglev which is tunable and with self-stability. The maglev system uses a closed-loop type II superconducting coil to lock flux of a magnet, establishing self-stable levitation between the two objects. A flux pump is used to modulate the total magnetic flux of the coil without breaking its superconductivity, thus flexibly tuning levitation force and height meanwhile maintaining self-stability. For the first time, we experimentally demonstrate a self-stable type II superconducting maglev system which is able to: counteract long term levitation force decay, adjust levitation force and equilibrium position, and establish levitation under zero field cooling condition. These breakthroughs may bridge the gap between demonstrations and practical applications of type II superconducting maglevs.","sentences":["Magnetic levitation based on the flux pinning nature of type II superconductors has the merit of self-stability, making it appealing for applications such as high speed bearings, maglev trains, space generators, etc.","However, such levitation systems physically rely on the superconductor pre-capturing magnetic flux (i.e. field cooling process) before establishing the levitation state which is nonadjustable afterwards.","Moreover, practical type II superconductors in the levitation system inevitably suffer from various sources of energy losses, leading to continuous levitation force decay.","These intrinsic drawbacks make superconducting maglev inflexible and impractical for long term operation.","Here we propose and demonstrate a new form of superconducting maglev which is tunable and with self-stability.","The maglev system uses a closed-loop type II superconducting coil to lock flux of a magnet, establishing self-stable levitation between the two objects.","A flux pump is used to modulate the total magnetic flux of the coil without breaking its superconductivity, thus flexibly tuning levitation force and height meanwhile maintaining self-stability.","For the first time, we experimentally demonstrate a self-stable type II superconducting maglev system which is able to: counteract long term levitation force decay, adjust levitation force and equilibrium position, and establish levitation under zero field cooling condition.","These breakthroughs may bridge the gap between demonstrations and practical applications of type II superconducting maglevs."],"url":"http://arxiv.org/abs/2403.19168v1","category":"eess.SY"}
{"created":"2024-03-28 06:13:54","title":"Fast and faithful interpolation of numerical relativity surrogate waveforms using meshfree approximation","abstract":"Several theoretical waveform models have been developed over the years to capture the gravitational wave emission from the dynamical evolution of compact binary systems of neutron stars and black holes. As ground-based detectors improve their sensitivity at low frequencies, the real-time computation of these waveforms can become computationally expensive, exacerbating the steep cost of rapidly reconstructing source parameters using Bayesian methods. This paper describes an efficient numerical algorithm for generating high-fidelity interpolated compact binary waveforms at an arbitrary point in the signal manifold by leveraging computational linear algebra techniques such as singular value decomposition and meshfree approximation. The results are presented for the time-domain \\texttt{NRHybSur3dq8} inspiral-merger-ringdown (IMR) waveform model that is fine tuned to numerical relativity simulations and parameterized by the two component-masses and two aligned spins. For demonstration, we target a specific region of the intrinsic parameter space inspired by the previously inferred parameters of the \\texttt{GW200311\\_115853} event -- a binary black hole system whose merger was recorded by the network of advanced-LIGO and Virgo detectors during the third observation run. We show that the meshfree interpolated waveforms can be evaluated in $\\sim 2.3$ ms, which is about $\\times 38$ faster than its brute-force (frequency-domain tapered) implementation in the \\textsc{PyCBC} software package at a median accuracy of $\\sim \\mathcal{O}(10^{-5})$. The algorithm is computationally efficient and scales favourably with an increasing number of dimensions of the parameter space. This technique may find use in rapid parameter estimation and source reconstruction studies.","sentences":["Several theoretical waveform models have been developed over the years to capture the gravitational wave emission from the dynamical evolution of compact binary systems of neutron stars and black holes.","As ground-based detectors improve their sensitivity at low frequencies, the real-time computation of these waveforms can become computationally expensive, exacerbating the steep cost of rapidly reconstructing source parameters using Bayesian methods.","This paper describes an efficient numerical algorithm for generating high-fidelity interpolated compact binary waveforms at an arbitrary point in the signal manifold by leveraging computational linear algebra techniques such as singular value decomposition and meshfree approximation.","The results are presented for the time-domain \\texttt{NRHybSur3dq8} inspiral-merger-ringdown (IMR) waveform model that is fine tuned to numerical relativity simulations and parameterized by the two component-masses and two aligned spins.","For demonstration, we target a specific region of the intrinsic parameter space inspired by the previously inferred parameters of the \\texttt{GW200311\\_115853} event -- a binary black hole system whose merger was recorded by the network of advanced-LIGO and Virgo detectors during the third observation run.","We show that the meshfree interpolated waveforms can be evaluated in $\\sim 2.3$ ms, which is about $\\times 38$ faster than its brute-force (frequency-domain tapered) implementation in the \\textsc{PyCBC} software package at a median accuracy of $\\sim \\mathcal{O}(10^{-5})$. The algorithm is computationally efficient and scales favourably with an increasing number of dimensions of the parameter space.","This technique may find use in rapid parameter estimation and source reconstruction studies."],"url":"http://arxiv.org/abs/2403.19162v1","category":"gr-qc"}
{"created":"2024-03-28 05:42:07","title":"Correlation functions between singular values and eigenvalues","abstract":"Exploiting the explicit bijection between the density of singular values and the density of eigenvalues for bi-unitarily invariant complex random matrix ensembles of finite matrix size we aim at finding the induced probability measure on $j$ eigenvalues and $k$ singular values that we coin $j,k$-point correlation measure. We fully derive all $j,k$-point correlation measures in the simplest cases for one- and two-dimensional matrices. For $n>2$, we find a general formula for the $1,1$-point correlation measure. This formula reduces drastically when assuming the singular values are drawn from a polynomial ensemble, yielding an explicit formula in terms of the kernel corresponding to the singular value statistics. These expressions simplify even further when the singular values are drawn from a P\\'{o}lya ensemble and extend known results between their eigenvalue and singular value statistics.","sentences":["Exploiting the explicit bijection between the density of singular values and the density of eigenvalues for bi-unitarily invariant complex random matrix ensembles of finite matrix size we aim at finding the induced probability measure on $j$ eigenvalues and $k$ singular values that we coin $j,k$-point correlation measure.","We fully derive all $j,k$-point correlation measures in the simplest cases for one- and two-dimensional matrices.","For $n>2$, we find a general formula for the $1,1$-point correlation measure.","This formula reduces drastically when assuming the singular values are drawn from a polynomial ensemble, yielding an explicit formula in terms of the kernel corresponding to the singular value statistics.","These expressions simplify even further when the singular values are drawn from a P\\'{o}lya ensemble and extend known results between their eigenvalue and singular value statistics."],"url":"http://arxiv.org/abs/2403.19157v1","category":"math.PR"}
{"created":"2024-03-28 05:24:59","title":"Curvature formulas and curvature strict positivity of direct image bundles","abstract":"In this paper, we consider the curvature strict positivity of direct image bundles (vector bundles) associated to a strictly pseudoconvex family of bounded domains. The main result is that the curvature of the direct image bundle (vector bundles) associated to a strictly pseudoconvex family of bounded domains is strictly positive in the sense of Nakano even if the curvature of the original vector bundle is just Nakano positive. Based on our (I and my coauthors) previous results, this result further demonstrates that strictly pseudoconvex domains and pseudoconvex domains have very different geometric properties. To consider the curvature strict positivity, we will first construct the curvature formulas for vector bundles, then the curvature strict positivity of a strictly pseudoconvex family of bounded domains will be a simple consequence of it. As applications to convex analysis, we get a corresponding version of Pr\\'ekopa's Theorem, i.e., we get the strict positivity of a strictly convex family of bounded domains.","sentences":["In this paper, we consider the curvature strict positivity of direct image bundles (vector bundles) associated to a strictly pseudoconvex family of bounded domains.","The main result is that the curvature of the direct image bundle (vector bundles) associated to a strictly pseudoconvex family of bounded domains is strictly positive in the sense of Nakano even if the curvature of the original vector bundle is just Nakano positive.","Based on our (I and my coauthors) previous results, this result further demonstrates that strictly pseudoconvex domains and pseudoconvex domains have very different geometric properties.","To consider the curvature strict positivity, we will first construct the curvature formulas for vector bundles, then the curvature strict positivity of a strictly pseudoconvex family of bounded domains will be a simple consequence of it.","As applications to convex analysis, we get a corresponding version of Pr\\'ekopa's Theorem, i.e., we get the strict positivity of a strictly convex family of bounded domains."],"url":"http://arxiv.org/abs/2403.19152v1","category":"math.CV"}
{"created":"2024-03-28 04:56:07","title":"Resilience-Oriented Operation of Micro-Grids in both Grid-Connected and Isolated Conditions within Sustainable Active Distribution Networks","abstract":"Due to the increasing occurrence of natural disasters, importance of maintaining sustainable energy for cities and society is felt more than ever. On the other hand, power loss reduction is a challenging issue of active distribution networks (ADNs). In this paper, a new convex optimization model is proposed with two objective functions including energy loss reduction in normal operating mode and system load shedding minimization in critical conditions after the occurrence of natural disasters. This purpose is fulfilled through optimal allocation of distributed generation (DG) units from both conventional and renewable types as well as energy storage systems (ESSs). In addition, a new formulation has been derived to form optimal micro-grids (MGs) aiming at energy loss reduction in normal operating condition and resiliency index improvement under emergency situations. The developed model is implemented in GAMS software and the studies have been tested and analyzed on the IEEE 33-bus system. The results verify the effectiveness of the proposed method in terms of energy loss reduction as well as resilience enhancement in extreme operation condition following severe disruptions in the system.","sentences":["Due to the increasing occurrence of natural disasters, importance of maintaining sustainable energy for cities and society is felt more than ever.","On the other hand, power loss reduction is a challenging issue of active distribution networks (ADNs).","In this paper, a new convex optimization model is proposed with two objective functions including energy loss reduction in normal operating mode and system load shedding minimization in critical conditions after the occurrence of natural disasters.","This purpose is fulfilled through optimal allocation of distributed generation (DG) units from both conventional and renewable types as well as energy storage systems (ESSs).","In addition, a new formulation has been derived to form optimal micro-grids (MGs) aiming at energy loss reduction in normal operating condition and resiliency index improvement under emergency situations.","The developed model is implemented in GAMS software and the studies have been tested and analyzed on the IEEE 33-bus system.","The results verify the effectiveness of the proposed method in terms of energy loss reduction as well as resilience enhancement in extreme operation condition following severe disruptions in the system."],"url":"http://arxiv.org/abs/2403.19147v1","category":"eess.SY"}
{"created":"2024-03-28 04:49:13","title":"Improving the Bit Complexity of Communication for Distributed Convex Optimization","abstract":"We consider the communication complexity of some fundamental convex optimization problems in the point-to-point (coordinator) and blackboard communication models. We strengthen known bounds for approximately solving linear regression, $p$-norm regression (for $1\\leq p\\leq 2$), linear programming, minimizing the sum of finitely many convex nonsmooth functions with varying supports, and low rank approximation; for a number of these fundamental problems our bounds are nearly optimal, as proven by our lower bounds.   Among our techniques, we use the notion of block leverage scores, which have been relatively unexplored in this context, as well as dropping all but the ``middle\" bits in Richardson-style algorithms. We also introduce a new communication problem for accurately approximating inner products and establish a lower bound using the spherical Radon transform. Our lower bound can be used to show the first separation of linear programming and linear systems in the distributed model when the number of constraints is polynomial, addressing an open question in prior work.","sentences":["We consider the communication complexity of some fundamental convex optimization problems in the point-to-point (coordinator) and blackboard communication models.","We strengthen known bounds for approximately solving linear regression, $p$-norm regression (for $1\\leq p\\leq 2$), linear programming, minimizing the sum of finitely many convex nonsmooth functions with varying supports, and low rank approximation; for a number of these fundamental problems our bounds are nearly optimal, as proven by our lower bounds.   ","Among our techniques, we use the notion of block leverage scores, which have been relatively unexplored in this context, as well as dropping all but the ``middle\" bits in Richardson-style algorithms.","We also introduce a new communication problem for accurately approximating inner products and establish a lower bound using the spherical Radon transform.","Our lower bound can be used to show the first separation of linear programming and linear systems in the distributed model when the number of constraints is polynomial, addressing an open question in prior work."],"url":"http://arxiv.org/abs/2403.19146v1","category":"cs.DS"}
{"created":"2024-03-28 04:35:27","title":"Tiny Graph Neural Networks for Radio Resource Management","abstract":"The surge in demand for efficient radio resource management has necessitated the development of sophisticated yet compact neural network architectures. In this paper, we introduce a novel approach to Graph Neural Networks (GNNs) tailored for radio resource management by presenting a new architecture: the Low Rank Message Passing Graph Neural Network (LR-MPGNN). The cornerstone of LR-MPGNN is the implementation of a low-rank approximation technique that substitutes the conventional linear layers with their low-rank counterparts. This innovative design significantly reduces the model size and the number of parameters. We evaluate the performance of the proposed LR-MPGNN model based on several key metrics: model size, number of parameters, weighted sum rate of the communication system, and the distribution of eigenvalues of weight matrices. Our extensive evaluations demonstrate that the LR-MPGNN model achieves a sixtyfold decrease in model size, and the number of model parameters can be reduced by up to 98%. Performance-wise, the LR-MPGNN demonstrates robustness with a marginal 2% reduction in the best-case scenario in the normalized weighted sum rate compared to the original MPGNN model. Additionally, the distribution of eigenvalues of the weight matrices in the LR-MPGNN model is more uniform and spans a wider range, suggesting a strategic redistribution of weights.","sentences":["The surge in demand for efficient radio resource management has necessitated the development of sophisticated yet compact neural network architectures.","In this paper, we introduce a novel approach to Graph Neural Networks (GNNs) tailored for radio resource management by presenting a new architecture: the Low Rank Message Passing Graph Neural Network (LR-MPGNN).","The cornerstone of LR-MPGNN is the implementation of a low-rank approximation technique that substitutes the conventional linear layers with their low-rank counterparts.","This innovative design significantly reduces the model size and the number of parameters.","We evaluate the performance of the proposed LR-MPGNN model based on several key metrics: model size, number of parameters, weighted sum rate of the communication system, and the distribution of eigenvalues of weight matrices.","Our extensive evaluations demonstrate that the LR-MPGNN model achieves a sixtyfold decrease in model size, and the number of model parameters can be reduced by up to 98%.","Performance-wise, the LR-MPGNN demonstrates robustness with a marginal 2% reduction in the best-case scenario in the normalized weighted sum rate compared to the original MPGNN model.","Additionally, the distribution of eigenvalues of the weight matrices in the LR-MPGNN model is more uniform and spans a wider range, suggesting a strategic redistribution of weights."],"url":"http://arxiv.org/abs/2403.19143v1","category":"cs.LG"}
{"created":"2024-03-28 04:30:07","title":"A Tulu Resource for Machine Translation","abstract":"We present the first parallel dataset for English-Tulu translation. Tulu, classified within the South Dravidian linguistic family branch, is predominantly spoken by approximately 2.5 million individuals in southwestern India. Our dataset is constructed by integrating human translations into the multilingual machine translation resource FLORES-200. Furthermore, we use this dataset for evaluation purposes in developing our English-Tulu machine translation model. For the model's training, we leverage resources available for related South Dravidian languages. We adopt a transfer learning approach that exploits similarities between high-resource and low-resource languages. This method enables the training of a machine translation system even in the absence of parallel data between the source and target language, thereby overcoming a significant obstacle in machine translation development for low-resource languages. Our English-Tulu system, trained without using parallel English-Tulu data, outperforms Google Translate by 19 BLEU points (in September 2023). The dataset and code are available here: https://github.com/manunarayanan/Tulu-NMT.","sentences":["We present the first parallel dataset for English-Tulu translation.","Tulu, classified within the South Dravidian linguistic family branch, is predominantly spoken by approximately 2.5 million individuals in southwestern India.","Our dataset is constructed by integrating human translations into the multilingual machine translation resource FLORES-200.","Furthermore, we use this dataset for evaluation purposes in developing our English-Tulu machine translation model.","For the model's training, we leverage resources available for related South Dravidian languages.","We adopt a transfer learning approach that exploits similarities between high-resource and low-resource languages.","This method enables the training of a machine translation system even in the absence of parallel data between the source and target language, thereby overcoming a significant obstacle in machine translation development for low-resource languages.","Our English-Tulu system, trained without using parallel English-Tulu data, outperforms Google Translate by 19 BLEU points (in September 2023).","The dataset and code are available here: https://github.com/manunarayanan/Tulu-NMT."],"url":"http://arxiv.org/abs/2403.19142v1","category":"cs.CL"}
{"created":"2024-03-28 04:27:54","title":"Glassy Dynamics in a Molecular Liquid","abstract":"A universal dynamical crossover temperature, Tcr, in glassy liquids, associated with the {\\alpha}-\\b{eta} bifurcation temperature, TB, has been observed in dielectric spectroscopy and other experiments. Tcr lies significantly above the glass transition temperature. Here, we introduce a new class of glass-forming liquids, binary mixtures of prolate and oblate ellipsoids. This model system exhibits sharp thermodynamic and dynamic anomalies, such as the specific heat jump during heating and a sharp variation in the thermal expansion coefficient around a temperature identified as the glass transition temperature, Tg. The same temperature is obtained from the fit of the calculated relaxation times to the Vogel-Fulcher-Tammann (VFT) form. As the temperature is lowered, the single peak rotational relaxation spectrum splits into two at a temperature TB significantly above the estimated Tg. Similar bifurcation is also observed in the distribution of short-to-intermediate time translational diffusion. Interrogation of the two peaks reveals a lower extent of dynamic heterogeneity in the population of the faster mode. We observe an unexpected appearance of a sharp peak in the product of rotational relaxation time {\\tau}2 and diffusion constant D at a temperature Tcr, close to TB, but above the glass transition temperature. Additionally, we coarse-grain the system into cubic boxes, each containing, on average, ~62 particles, to study the average dynamical properties. Clear evidence of large-scale sudden changes in the diffusion coefficient and rotational correlation time signals first-order transitions between low and high-mobility domains.","sentences":["A universal dynamical crossover temperature, Tcr, in glassy liquids, associated with the {\\alpha}-\\b{eta} bifurcation temperature, TB, has been observed in dielectric spectroscopy and other experiments.","Tcr lies significantly above the glass transition temperature.","Here, we introduce a new class of glass-forming liquids, binary mixtures of prolate and oblate ellipsoids.","This model system exhibits sharp thermodynamic and dynamic anomalies, such as the specific heat jump during heating and a sharp variation in the thermal expansion coefficient around a temperature identified as the glass transition temperature, Tg.","The same temperature is obtained from the fit of the calculated relaxation times to the Vogel-Fulcher-Tammann (VFT) form.","As the temperature is lowered, the single peak rotational relaxation spectrum splits into two at a temperature TB significantly above the estimated Tg.","Similar bifurcation is also observed in the distribution of short-to-intermediate time translational diffusion.","Interrogation of the two peaks reveals a lower extent of dynamic heterogeneity in the population of the faster mode.","We observe an unexpected appearance of a sharp peak in the product of rotational relaxation time {\\tau}2 and diffusion constant D at a temperature Tcr, close to TB, but above the glass transition temperature.","Additionally, we coarse-grain the system into cubic boxes, each containing, on average, ~62 particles, to study the average dynamical properties.","Clear evidence of large-scale sudden changes in the diffusion coefficient and rotational correlation time signals first-order transitions between low and high-mobility domains."],"url":"http://arxiv.org/abs/2403.19141v1","category":"cond-mat.soft"}
{"created":"2024-03-28 04:20:22","title":"Symbiotic Control of Uncertain Dynamical Systems: Harnessing Synergy Between Fixed-Gain Control and Adaptive Learning Architectures","abstract":"Both fixed-gain control and adaptive learning architectures aim to mitigate the effects of uncertainties. In particular, fixed-gain control offers more predictable closed-loop system behavior but requires the knowledge of uncertainty bounds. In contrast, while adaptive learning does not necessarily require such knowledge, it often results in less predictable closed-loop system behavior compared to fixed-gain control. To this end, this paper presents a novel symbiotic control framework that offers the strengths of fixed-gain control and adaptive learning architectures. Specifically, this framework synergistically integrates these architectures to mitigate the effects of uncertainties in a more predictable manner as compared to adaptive learning alone and it does not require any knowledge on such uncertainties. Both parametric and nonparametric uncertainties are considered, where we utilize neural networks to approximate the unknown uncertainty basis for the latter case. Counterintuitively, the proposed framework has the ability to achieve a desired level of closed-loop system behavior even with an insufficient number of neurons (e.g., when the neural network approximation error is large) or in the face of injudiciously selected adaptive learning parameters (e.g., high leakage term parameters).","sentences":["Both fixed-gain control and adaptive learning architectures aim to mitigate the effects of uncertainties.","In particular, fixed-gain control offers more predictable closed-loop system behavior but requires the knowledge of uncertainty bounds.","In contrast, while adaptive learning does not necessarily require such knowledge, it often results in less predictable closed-loop system behavior compared to fixed-gain control.","To this end, this paper presents a novel symbiotic control framework that offers the strengths of fixed-gain control and adaptive learning architectures.","Specifically, this framework synergistically integrates these architectures to mitigate the effects of uncertainties in a more predictable manner as compared to adaptive learning alone and it does not require any knowledge on such uncertainties.","Both parametric and nonparametric uncertainties are considered, where we utilize neural networks to approximate the unknown uncertainty basis for the latter case.","Counterintuitively, the proposed framework has the ability to achieve a desired level of closed-loop system behavior even with an insufficient number of neurons (e.g., when the neural network approximation error is large) or in the face of injudiciously selected adaptive learning parameters (e.g., high leakage term parameters)."],"url":"http://arxiv.org/abs/2403.19139v1","category":"eess.SY"}
{"created":"2024-03-28 04:02:25","title":"Meta-Heuristic Fronthaul Bit Allocation for Cell-free Massive MIMO Systems","abstract":"Limited capacity of fronthaul links in a cell-free massive multiple-input multiple-output (MIMO) system can cause quantization errors at a central processing unit (CPU) during data transmission, complicating the centralized rate optimization problem. Addressing this challenge, we propose a harmony search (HS)-based algorithm that renders the combinatorial non-convex problem tractable. One of the distinctive features of our algorithm is its hierarchical structure: it first allocates resources at the access point (AP) level and subsequently optimizes for user equipment (UE), ensuring a more efficient and structured approach to resource allocation. Our proposed algorithm deals with rigorous conditions, such as asymmetric fronthaul bit allocation and distinct quantization error levels at each AP, which were not considered in previous works. We derive a closed-form expression of signal-to-interference-plusnoise ratio (SINR), in which additive quantization noise model (AQNM) based distortion error is taken into account, to define the mathematical expression of spectral efficiency (SE) for each UE. Also, we provide analyses on computational complexity and convergence to investigate the practicality of proposed algorithm. By leveraging various performance metrics such as total SE and max-min fairness, we demonstrate that the proposed algorithm can adaptively optimize the fronthaul bit allocation depending on system requirements. Finally, simulation results show that the proposed algorithm can achieve satisfactory performance while maintaining low computational complexity, as compared to the exhaustive search method","sentences":["Limited capacity of fronthaul links in a cell-free massive multiple-input multiple-output (MIMO) system can cause quantization errors at a central processing unit (CPU) during data transmission, complicating the centralized rate optimization problem.","Addressing this challenge, we propose a harmony search (HS)-based algorithm that renders the combinatorial non-convex problem tractable.","One of the distinctive features of our algorithm is its hierarchical structure: it first allocates resources at the access point (AP) level and subsequently optimizes for user equipment (UE), ensuring a more efficient and structured approach to resource allocation.","Our proposed algorithm deals with rigorous conditions, such as asymmetric fronthaul bit allocation and distinct quantization error levels at each AP, which were not considered in previous works.","We derive a closed-form expression of signal-to-interference-plusnoise ratio (SINR), in which additive quantization noise model (AQNM) based distortion error is taken into account, to define the mathematical expression of spectral efficiency (SE) for each UE.","Also, we provide analyses on computational complexity and convergence to investigate the practicality of proposed algorithm.","By leveraging various performance metrics such as total SE and max-min fairness, we demonstrate that the proposed algorithm can adaptively optimize the fronthaul bit allocation depending on system requirements.","Finally, simulation results show that the proposed algorithm can achieve satisfactory performance while maintaining low computational complexity, as compared to the exhaustive search method"],"url":"http://arxiv.org/abs/2403.19132v1","category":"eess.SP"}
{"created":"2024-03-28 03:55:56","title":"Gamu Blue: A Practical Tool for Game Theory Security Equilibria","abstract":"The application of game theory in cybersecurity enables strategic analysis, adversarial modeling, and optimal decision-making to address security threats' complex and dynamic nature. Previous studies by Abraham et al. and Bi\\c{c}er et al. presented various definitions of equilibria to examine the security aspects of games involving multiple parties. Nonetheless, these definitions lack practical and easy-to-use implementations. Our primary contribution is addressing this gap by developing Gamu Blue, an easy-to-use tool with implementations for computing the equilibria definitions including k-resiliency, l-repellence, t-immunity, (l, t)-resistance, and m-stability.","sentences":["The application of game theory in cybersecurity enables strategic analysis, adversarial modeling, and optimal decision-making to address security threats' complex and dynamic nature.","Previous studies by Abraham et al. and Bi\\c{c}er et al. presented various definitions of equilibria to examine the security aspects of games involving multiple parties.","Nonetheless, these definitions lack practical and easy-to-use implementations.","Our primary contribution is addressing this gap by developing Gamu Blue, an easy-to-use tool with implementations for computing the equilibria definitions including k-resiliency, l-repellence, t-immunity, (l, t)-resistance, and m-stability."],"url":"http://arxiv.org/abs/2403.19130v1","category":"cs.GT"}
{"created":"2024-03-28 03:51:14","title":"OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition","abstract":"Recently, visually-situated text parsing (VsTP) has experienced notable advancements, driven by the increasing demand for automated document understanding and the emergence of Generative Large Language Models (LLMs) capable of processing document-based questions. Various methods have been proposed to address the challenging problem of VsTP. However, due to the diversified targets and heterogeneous schemas, previous works usually design task-specific architectures and objectives for individual tasks, which inadvertently leads to modal isolation and complex workflow. In this paper, we propose a unified paradigm for parsing visually-situated text across diverse scenarios. Specifically, we devise a universal model, called OmniParser, which can simultaneously handle three typical visually-situated text parsing tasks: text spotting, key information extraction, and table recognition. In OmniParser, all tasks share the unified encoder-decoder architecture, the unified objective: point-conditioned text generation, and the unified input & output representation: prompt & structured sequences. Extensive experiments demonstrate that the proposed OmniParser achieves state-of-the-art (SOTA) or highly competitive performances on 7 datasets for the three visually-situated text parsing tasks, despite its unified, concise design. The code is available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery.","sentences":["Recently, visually-situated text parsing (VsTP) has experienced notable advancements, driven by the increasing demand for automated document understanding and the emergence of Generative Large Language Models (LLMs) capable of processing document-based questions.","Various methods have been proposed to address the challenging problem of VsTP.","However, due to the diversified targets and heterogeneous schemas, previous works usually design task-specific architectures and objectives for individual tasks, which inadvertently leads to modal isolation and complex workflow.","In this paper, we propose a unified paradigm for parsing visually-situated text across diverse scenarios.","Specifically, we devise a universal model, called OmniParser, which can simultaneously handle three typical visually-situated text parsing tasks: text spotting, key information extraction, and table recognition.","In OmniParser, all tasks share the unified encoder-decoder architecture, the unified objective: point-conditioned text generation, and the unified input & output representation: prompt & structured sequences.","Extensive experiments demonstrate that the proposed OmniParser achieves state-of-the-art (SOTA) or highly competitive performances on 7 datasets for the three visually-situated text parsing tasks, despite its unified, concise design.","The code is available at https://github.com/AlibabaResearch/AdvancedLiterateMachinery."],"url":"http://arxiv.org/abs/2403.19128v1","category":"cs.CV"}
{"created":"2024-03-28 03:47:19","title":"Decentralizing Coherent Joint Transmission Precoding via Fast ADMM with Deterministic Equivalents","abstract":"Inter-cell interference (ICI) suppression is critical for multi-cell multi-user networks. In this paper, we investigate advanced precoding techniques for coordinated multi-point (CoMP) with downlink coherent joint transmission, an effective approach for ICI suppression. Different from the centralized precoding schemes that require frequent information exchange among the cooperating base stations, we propose a decentralized scheme to minimize the total power consumption. In particular, based on the covariance matrices of global channel state information, we estimate the ICI bounds via the deterministic equivalents and decouple the original design problem into sub-problems, each of which can be solved in a decentralized manner. To solve the sub-problems at each base station, we develop a low-complexity solver based on the alternating direction method of multipliers (ADMM) in conjunction with the convex-concave procedure (CCCP). Simulation results demonstrate the effectiveness of our proposed decentralized precoding scheme, which achieves performance similar to the optimal centralized precoding scheme. Besides, our proposed ADMM solver can substantially reduce the computational complexity, while maintaining outstanding performance.","sentences":["Inter-cell interference (ICI) suppression is critical for multi-cell multi-user networks.","In this paper, we investigate advanced precoding techniques for coordinated multi-point (CoMP) with downlink coherent joint transmission, an effective approach for ICI suppression.","Different from the centralized precoding schemes that require frequent information exchange among the cooperating base stations, we propose a decentralized scheme to minimize the total power consumption.","In particular, based on the covariance matrices of global channel state information, we estimate the ICI bounds via the deterministic equivalents and decouple the original design problem into sub-problems, each of which can be solved in a decentralized manner.","To solve the sub-problems at each base station, we develop a low-complexity solver based on the alternating direction method of multipliers (ADMM) in conjunction with the convex-concave procedure (CCCP).","Simulation results demonstrate the effectiveness of our proposed decentralized precoding scheme, which achieves performance similar to the optimal centralized precoding scheme.","Besides, our proposed ADMM solver can substantially reduce the computational complexity, while maintaining outstanding performance."],"url":"http://arxiv.org/abs/2403.19127v1","category":"eess.SP"}
{"created":"2024-03-28 03:41:05","title":"Harnessing Data for Accelerating Model Predictive Control by Constraint Removal","abstract":"Model predictive control (MPC) solves a receding-horizon optimization problem in real-time, which can be computationally demanding when there are thousands of constraints. To accelerate online computation of MPC, we utilize data to adaptively remove the constraints while maintaining the MPC policy unchanged. Specifically, we design the removal rule based on the Lipschitz continuity of the MPC policy. This removal rule can use the information of historical data according to the Lipschitz constant and the distance between the current state and historical states. In particular, we provide the explicit expression for calculating the Lipschitz constant by the model parameters. Finally, simulations are performed to validate the effectiveness of the proposed method.","sentences":["Model predictive control (MPC) solves a receding-horizon optimization problem in real-time, which can be computationally demanding when there are thousands of constraints.","To accelerate online computation of MPC, we utilize data to adaptively remove the constraints while maintaining the MPC policy unchanged.","Specifically, we design the removal rule based on the Lipschitz continuity of the MPC policy.","This removal rule can use the information of historical data according to the Lipschitz constant and the distance between the current state and historical states.","In particular, we provide the explicit expression for calculating the Lipschitz constant by the model parameters.","Finally, simulations are performed to validate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2403.19126v1","category":"eess.SY"}
{"created":"2024-03-28 03:36:45","title":"A generic reduction theory for Fermi sea topology in metallic systems","abstract":"Fermi sea can host exotic quantum topology, which determines the conductance quantization in metals and is characterized by the Euler characteristic $\\chi_F$. Here, we propose a generic reduction theory for the Fermi sea topology in $d$-dimensional metallic systems, showing that $\\chi_F$ can be identified by the feature of reduced critical points on Fermi surfaces, with theoretical simplicity and observational intuitiveness. We also reveal a striking connection between the Fermi sea topology and the gapped band topology, in which $\\chi_F$ exactly equals to the topological invariant of the gapped topological phases by using an ingenious mapping. This nontrivial result provides a potential method to probe $\\chi_F$ through $\\mathbb{Z}$-classified topological superconductors and further paves the way for studying the Fermi sea topology via the gapped topological systems. Our work promotes a deep understanding and simple detection for $\\chi_F$, which shall facilitate the discovery of other novel physical effects caused by the Fermi sea topology in the future.","sentences":["Fermi sea can host exotic quantum topology, which determines the conductance quantization in metals and is characterized by the Euler characteristic $\\chi_F$. Here, we propose a generic reduction theory for the Fermi sea topology in $d$-dimensional metallic systems, showing that $\\chi_F$ can be identified by the feature of reduced critical points on Fermi surfaces, with theoretical simplicity and observational intuitiveness.","We also reveal a striking connection between the Fermi sea topology and the gapped band topology, in which $\\chi_F$ exactly equals to the topological invariant of the gapped topological phases by using an ingenious mapping.","This nontrivial result provides a potential method to probe $\\chi_F$ through $\\mathbb{Z}$-classified topological superconductors and further paves the way for studying the Fermi sea topology via the gapped topological systems.","Our work promotes a deep understanding and simple detection for $\\chi_F$, which shall facilitate the discovery of other novel physical effects caused by the Fermi sea topology in the future."],"url":"http://arxiv.org/abs/2403.19125v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 03:35:00","title":"PoCo: A Self-Supervised Approach via Polar Transformation Based Progressive Contrastive Learning for Ophthalmic Disease Diagnosis","abstract":"Automatic ophthalmic disease diagnosis on fundus images is important in clinical practice. However, due to complex fundus textures and limited annotated data, developing an effective automatic method for this problem is still challenging. In this paper, we present a self-supervised method via polar transformation based progressive contrastive learning, called PoCo, for ophthalmic disease diagnosis. Specifically, we novelly inject the polar transformation into contrastive learning to 1) promote contrastive learning pre-training to be faster and more stable and 2) naturally capture task-free and rotation-related textures, which provides insights into disease recognition on fundus images. Beneficially, simple normal translation-invariant convolution on transformed images can equivalently replace the complex rotation-invariant and sector convolution on raw images. After that, we develop a progressive contrastive learning method to efficiently utilize large unannotated images and a novel progressive hard negative sampling scheme to gradually reduce the negative sample number for efficient training and performance enhancement. Extensive experiments on three public ophthalmic disease datasets show that our PoCo achieves state-of-the-art performance with good generalization ability, validating that our method can reduce annotation efforts and provide reliable diagnosis. Codes are available at \\url{https://github.com/wjh892521292/PoCo}.","sentences":["Automatic ophthalmic disease diagnosis on fundus images is important in clinical practice.","However, due to complex fundus textures and limited annotated data, developing an effective automatic method for this problem is still challenging.","In this paper, we present a self-supervised method via polar transformation based progressive contrastive learning, called PoCo, for ophthalmic disease diagnosis.","Specifically, we novelly inject the polar transformation into contrastive learning to 1) promote contrastive learning pre-training to be faster and more stable and 2) naturally capture task-free and rotation-related textures, which provides insights into disease recognition on fundus images.","Beneficially, simple normal translation-invariant convolution on transformed images can equivalently replace the complex rotation-invariant and sector convolution on raw images.","After that, we develop a progressive contrastive learning method to efficiently utilize large unannotated images and a novel progressive hard negative sampling scheme to gradually reduce the negative sample number for efficient training and performance enhancement.","Extensive experiments on three public ophthalmic disease datasets show that our PoCo achieves state-of-the-art performance with good generalization ability, validating that our method can reduce annotation efforts and provide reliable diagnosis.","Codes are available at \\url{https://github.com/wjh892521292/PoCo}."],"url":"http://arxiv.org/abs/2403.19124v1","category":"cs.CV"}
{"created":"2024-03-28 03:32:22","title":"Schr\u00f6dingerisation based computationally stable algorithms for ill-posed problems in partial differential equations","abstract":"We introduce a simple and stable computational method for ill-posed partial differential equation (PDE) problems. The method is based on Schr\\\"odingerisation, introduced in [S. Jin, N. Liu and Y. Yu, Phys. Rev. A, 108 (2023), 032603], which maps all linear PDEs into Schr\\\"odinger-type equations in one higher dimension, for quantum simulations of these PDEs. Although the original problem is ill-posed, the Schr\\\"odingerized equations are Hamiltonian systems and time-reversible, allowing stable computation both forward and backward in time. The original variable can be recovered by data from suitably chosen domain in the extended dimension. We will use the backward heat equation and the linear convection equation with imaginary wave speed as examples. Error analysis of these algorithms are conducted and verified numerically. The methods apply to both classical and quantum computers, and we also layout the quantum algorithms for these methods.","sentences":["We introduce a simple and stable computational method for ill-posed partial differential equation (PDE) problems.","The method is based on Schr\\\"odingerisation, introduced in [S. Jin, N. Liu and Y. Yu, Phys.","Rev. A, 108 (2023), 032603], which maps all linear PDEs into Schr\\\"odinger-type equations in one higher dimension, for quantum simulations of these PDEs.","Although the original problem is ill-posed, the Schr\\\"odingerized equations are Hamiltonian systems and time-reversible, allowing stable computation both forward and backward in time.","The original variable can be recovered by data from suitably chosen domain in the extended dimension.","We will use the backward heat equation and the linear convection equation with imaginary wave speed as examples.","Error analysis of these algorithms are conducted and verified numerically.","The methods apply to both classical and quantum computers, and we also layout the quantum algorithms for these methods."],"url":"http://arxiv.org/abs/2403.19123v1","category":"math.NA"}
{"created":"2024-03-28 03:30:25","title":"Safety-Critical Planning and Control for Dynamic Obstacle Avoidance Using Control Barrier Functions","abstract":"Dynamic obstacle avoidance is a challenging topic for optimal control and optimization-based trajectory planning problems, especially when in a tight environment. Many existing works use control barrier functions (CBFs) to enforce safety constraints within control systems. Inside these works, CBFs are usually formulated under model predictive control (MPC) framework to anticipate future states and make informed decisions, or integrated with path planning algorithms as a safety enhancement tool. However, these approaches usually require knowledge of the obstacle boundary equations or have very slow computational efficiency. In this paper, we propose a novel framework to the iterative MPC with discrete-time CBFs (DCBFs) to generate a collision-free trajectory. The DCBFs are obtained from convex polyhedra generated in sequential grid maps, without the need to know the boundary equations of obstacles. Additionally, a path planning algorithm is incorporated into this framework to ensure the global optimality of the generated trajectory. We demonstrate through numerical examples that our framework enables a unicycle robot to safely and efficiently navigate through tight and dynamically changing environments, tackling both convex and nonconvex obstacles with remarkable computing efficiency and reliability in control and trajectory generation.","sentences":["Dynamic obstacle avoidance is a challenging topic for optimal control and optimization-based trajectory planning problems, especially when in a tight environment.","Many existing works use control barrier functions (CBFs) to enforce safety constraints within control systems.","Inside these works, CBFs are usually formulated under model predictive control (MPC) framework to anticipate future states and make informed decisions, or integrated with path planning algorithms as a safety enhancement tool.","However, these approaches usually require knowledge of the obstacle boundary equations or have very slow computational efficiency.","In this paper, we propose a novel framework to the iterative MPC with discrete-time CBFs (DCBFs) to generate a collision-free trajectory.","The DCBFs are obtained from convex polyhedra generated in sequential grid maps, without the need to know the boundary equations of obstacles.","Additionally, a path planning algorithm is incorporated into this framework to ensure the global optimality of the generated trajectory.","We demonstrate through numerical examples that our framework enables a unicycle robot to safely and efficiently navigate through tight and dynamically changing environments, tackling both convex and nonconvex obstacles with remarkable computing efficiency and reliability in control and trajectory generation."],"url":"http://arxiv.org/abs/2403.19122v1","category":"cs.RO"}
{"created":"2024-03-28 03:24:31","title":"Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications -- Part III: Multi-Target Tracking","abstract":"As a next-generation wireless technology, the in-band full-duplex (IBFD) transmission enables simultaneous transmission and reception of signals over the same frequency, thereby doubling spectral efficiency. Further, a continuous up-scaling of wireless network carrier frequencies arising from ever-increasing data traffic is driving research on integrated sensing and communications (ISAC) systems. In this context, we study the co-design of common waveforms, precoders, and filters for an IBFD multi-user (MU) multiple-input multiple-output (MIMO) communications with a distributed MIMO radar. This paper, along with companion papers (Part I and II), proposes a comprehensive MRMC framework that addresses all these challenges. In the companion papers, we developed signal processing and joint design algorithms for this distributed system. In this paper, we tackle multi-target detection, localization, and tracking. This co-design problem that includes practical MU-MIMO constraints on power and quality-of-service is highly non-convex. We propose a low-complexity procedure based on Barzilai-Borwein gradient algorithm to obtain the design parameters and mixed-integer linear program for distributed target localization. Numerical experiments demonstrate the feasibility and accuracy of multi-target sensing of the distributed FD ISAC system. Finally, we localize and track multiple targets by adapting the joint probabilistic data association and extended Kalman filter for this system.","sentences":["As a next-generation wireless technology, the in-band full-duplex (IBFD) transmission enables simultaneous transmission and reception of signals over the same frequency, thereby doubling spectral efficiency.","Further, a continuous up-scaling of wireless network carrier frequencies arising from ever-increasing data traffic is driving research on integrated sensing and communications (ISAC) systems.","In this context, we study the co-design of common waveforms, precoders, and filters for an IBFD multi-user (MU) multiple-input multiple-output (MIMO) communications with a distributed MIMO radar.","This paper, along with companion papers (Part I and II), proposes a comprehensive MRMC framework that addresses all these challenges.","In the companion papers, we developed signal processing and joint design algorithms for this distributed system.","In this paper, we tackle multi-target detection, localization, and tracking.","This co-design problem that includes practical MU-MIMO constraints on power and quality-of-service is highly non-convex.","We propose a low-complexity procedure based on Barzilai-Borwein gradient algorithm to obtain the design parameters and mixed-integer linear program for distributed target localization.","Numerical experiments demonstrate the feasibility and accuracy of multi-target sensing of the distributed FD ISAC system.","Finally, we localize and track multiple targets by adapting the joint probabilistic data association and extended Kalman filter for this system."],"url":"http://arxiv.org/abs/2403.19120v1","category":"cs.IT"}
{"created":"2024-03-28 03:24:19","title":"Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User MIMO Communications -- Part II: Joint Precoder, Radar Code, and Receive Filters Design","abstract":"We address the challenge of spectral sharing between a statistical multiple-input multiple-output (MIMO) radar and an in-band full-duplex (IBFD) multi-user MIMO (MU-MIMO) communications system operating simultaneously in the same frequency band. Existing research on joint MIMO-radar-MIMO-communications (MRMC) systems has limitations, such as focusing on colocated MIMO radars, half-duplex MIMO communications, single-user scenarios, neglecting practical constraints, or employing separate transmit/receive units for MRMC coexistence. This paper, along with companion papers (Part I and III), proposes a comprehensive MRMC framework that addresses all these challenges. In the previous companion paper (Part I), we presented signal processing techniques for a distributed IBFD MRMC system. In this paper, we introduce joint design of statistical MIMO radar codes, uplink/downlink precoders, and corresponding receive filters using a novel metric called compounded-and-weighted sum mutual information. To solve the resulting highly non-convex problem, we employ a combination of block coordinate descent (BCD) and alternating projection methods. Numerical experiments show convergence of our algorithm, mitigation of uplink interference, and stable data rates under varying noise levels, channel estimate imperfections, and self-interference. The subsequent companion paper (Part III) extends the discussion to multiple targets and evaluates the tracking performance of our MRMC system.","sentences":["We address the challenge of spectral sharing between a statistical multiple-input multiple-output (MIMO) radar and an in-band full-duplex (IBFD) multi-user MIMO (MU-MIMO) communications system operating simultaneously in the same frequency band.","Existing research on joint MIMO-radar-MIMO-communications (MRMC) systems has limitations, such as focusing on colocated MIMO radars, half-duplex MIMO communications, single-user scenarios, neglecting practical constraints, or employing separate transmit/receive units for MRMC coexistence.","This paper, along with companion papers (Part I and III), proposes a comprehensive MRMC framework that addresses all these challenges.","In the previous companion paper (Part I), we presented signal processing techniques for a distributed IBFD MRMC system.","In this paper, we introduce joint design of statistical MIMO radar codes, uplink/downlink precoders, and corresponding receive filters using a novel metric called compounded-and-weighted sum mutual information.","To solve the resulting highly non-convex problem, we employ a combination of block coordinate descent (BCD) and alternating projection methods.","Numerical experiments show convergence of our algorithm, mitigation of uplink interference, and stable data rates under varying noise levels, channel estimate imperfections, and self-interference.","The subsequent companion paper (Part III) extends the discussion to multiple targets and evaluates the tracking performance of our MRMC system."],"url":"http://arxiv.org/abs/2403.19119v1","category":"cs.IT"}
{"created":"2024-03-28 03:23:46","title":"Non-Abelian observable-geometric phases and the Riemann zeros","abstract":"The Hilbert-P\\'{o}lya conjecture asserts that the imaginary parts of the nontrivial zeros of the Riemann zeta function (the Riemann zeros) are the eigenvalues of a self-adjoint operator (a quantum mechanical Hamiltonian, in the physical sense), as a promising approach to prove the Riemann hypothesis (cf.\\cite{SH2011}). Instead of the eigenvalues, in this paper we consider observable-geometric phases as the realization of the Riemann zeros in a periodically driven quantum system, which were introduced in \\cite{Chen2020} for the study of geometric quantum computation. To this end, we further introduce the notion of non-Abelian observable-geometric phases, involving which we give an approach to finding a physical system to study the Riemann zeros. Since the observable-geometric phases are connected with the geometry of the observable space according to the evolution of the Heisenberg equation, this sheds some light on the investigation of the Riemann hypothesis.","sentences":["The Hilbert-P\\'{o}lya conjecture asserts that the imaginary parts of the nontrivial zeros of the Riemann zeta function (the Riemann zeros) are the eigenvalues of a self-adjoint operator (a quantum mechanical Hamiltonian, in the physical sense), as a promising approach to prove the Riemann hypothesis (cf.\\cite{SH2011}).","Instead of the eigenvalues, in this paper we consider observable-geometric phases as the realization of the Riemann zeros in a periodically driven quantum system, which were introduced in \\cite{Chen2020} for the study of geometric quantum computation.","To this end, we further introduce the notion of non-Abelian observable-geometric phases, involving which we give an approach to finding a physical system to study the Riemann zeros.","Since the observable-geometric phases are connected with the geometry of the observable space according to the evolution of the Heisenberg equation, this sheds some light on the investigation of the Riemann hypothesis."],"url":"http://arxiv.org/abs/2403.19118v1","category":"quant-ph"}
{"created":"2024-03-28 03:11:38","title":"HiRoPE: Length Extrapolation for Code Models","abstract":"Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper. Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences. Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code. HiRoPE offers easy integration into existing LLMs without extra training costs. Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion. We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field. Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding. Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length.","sentences":["Addressing the limitation of context length in large language models for code-related tasks is the primary focus of this paper.","Existing LLMs are constrained by their pre-trained context lengths, leading to performance issues in handling long complex code sequences.","Inspired by how human programmers navigate code, we introduce Hierarchical Rotary Position Embedding (HiRoPE), a novel approach that enhances the traditional rotary position embedding into a hierarchical format based on the hierarchical structure of source code.","HiRoPE offers easy integration into existing LLMs without extra training costs.","Our method is extensively evaluated with various LLMs, demonstrating stable performance in tasks such as language modeling and long code completion.","We also introduce a new long code understanding task with real-world code projects, in hopes of promoting further development in this code-related field.","Theoretically and experimentally, we find that HiRoPE also addresses the out-of-distribution issue in position encoding.","Our HiRoPE significantly expands the context length capabilities of LLMs, enabling inference at lengths exponentially greater than the training length."],"url":"http://arxiv.org/abs/2403.19115v1","category":"cs.SE"}
{"created":"2024-03-28 03:06:14","title":"Remarks on $J$-tame inflation","abstract":"We give a complete and self-contained exposition of the $J$-tame inflation lemma: Given any tame almost complex structure $J$ on a symplectic $4$-manifold $(M,\\omega)$, and given any compact, embedded, $J$-holomorphic submanifold $Z$, it is always possible to construct a deformation of symplectic forms $\\omega_t$ in classes $[\\omega_t]=[\\omega]+t\\mathrm{PD}{Z}$, for $0\\leq t$ less than an upper bound $0<T$ that only depends on the self-intersection $Z\\cdot Z$. The original proofs of this fact make the unwarranted assumption that one can find a family of normal planes along $Z$ that is both $J$ invariant and $\\omega$-orthogonal to $TZ$ -- which amounts, in effect, to assuming the compatibility of $J$ and $\\omega$ along $Z$. We explain how the original constructions can be adapted to avoid this assumption when $Z$ has nonpositive self-intersection, and we discuss the difficulties with this line of argument in general to establish the full inflation when $Z$ has positive self-intersection. We overcome this problem by proving a `preparation lemma', which states that prior to inflation, one can isotop $\\omega$ within its cohomology class to a new form that still tames $J$ and which is compatible with $J$ along the submanifold $Z$.","sentences":["We give a complete and self-contained exposition of the $J$-tame inflation lemma: Given any tame almost complex structure $J$ on a symplectic $4$-manifold $(M,\\omega)$, and given any compact, embedded, $J$-holomorphic submanifold $Z$, it is always possible to construct a deformation of symplectic forms $\\omega_t$ in classes $[\\omega_t]=[\\omega]+t\\mathrm{PD}{Z}$, for $0\\leq t$ less than an upper bound $0<T$ that only depends on the self-intersection $Z\\cdot Z$.","The original proofs of this fact make the unwarranted assumption that one can find a family of normal planes along $Z$ that is both $J$ invariant and $\\omega$-orthogonal to $TZ$ -- which amounts, in effect, to assuming the compatibility of $J$ and $\\omega$ along $Z$. We explain how the original constructions can be adapted to avoid this assumption when $Z$ has nonpositive self-intersection, and we discuss the difficulties with this line of argument in general to establish the full inflation when $Z$ has positive self-intersection.","We overcome this problem by proving a `preparation lemma', which states that prior to inflation, one can isotop $\\omega$ within its cohomology class to a new form that still tames $J$ and which is compatible with $J$ along the submanifold $Z$."],"url":"http://arxiv.org/abs/2403.19110v1","category":"math.SG"}
{"created":"2024-03-28 03:03:19","title":"Enhancing Evolutionary Solver Efficiency for NP Hard Single Machine Scheduling Problems","abstract":"The study explores the optimization of evolutionary solver parameters for minimizing total tardiness in single machine scheduling, an NP-hard problem with zero ready times included. It investigates various parameter combinations, including population sizes, mutation rates, and a constant convergence rate, both above and below default values. The aim is to enhance the solver's effectiveness in addressing this complex challenge. The findings contribute to improving scheduling efficiency in manufacturing and operations management contexts.","sentences":["The study explores the optimization of evolutionary solver parameters for minimizing total tardiness in single machine scheduling, an NP-hard problem with zero ready times included.","It investigates various parameter combinations, including population sizes, mutation rates, and a constant convergence rate, both above and below default values.","The aim is to enhance the solver's effectiveness in addressing this complex challenge.","The findings contribute to improving scheduling efficiency in manufacturing and operations management contexts."],"url":"http://arxiv.org/abs/2403.19109v1","category":"cs.CE"}
{"created":"2024-03-28 02:40:43","title":"Pilot Signal and Channel Estimator Co-Design for Hybrid-Field XL-MIMO","abstract":"This paper addresses the intricate task of hybrid-field channel estimation in extremely large-scale MIMO (XL-MIMO) systems, critical for the progression of 6G communications. Within these systems, comprising a line-of-sight (LoS) channel component alongside far-field and near-field scattering channel components, our objective is to tackle the channel estimation challenge. We encounter two central hurdles for ensuring dependable sparse channel recovery: the design of pilot signals and channel estimators tailored for hybrid-field communications. To overcome the first challenge, we propose a method to derive optimal pilot signals, aimed at minimizing the mutual coherence of the sensing matrix within the context of compressive sensing (CS) problems. These optimal signals are derived using the alternating direction method of multipliers (ADMM), ensuring robust performance in sparse channel recovery. Additionally, leveraging the acquired optimal pilot signal, we introduce a two-stage channel estimation approach that sequentially estimates the LoS channel component and the hybrid-field scattering channel components. Simulation results attest to the superiority of our co-designed approach for pilot signal and channel estimation over conventional CS-based methods, providing more reliable sparse channel recovery in practical scenarios.","sentences":["This paper addresses the intricate task of hybrid-field channel estimation in extremely large-scale MIMO (XL-MIMO) systems, critical for the progression of 6G communications.","Within these systems, comprising a line-of-sight (LoS) channel component alongside far-field and near-field scattering channel components, our objective is to tackle the channel estimation challenge.","We encounter two central hurdles for ensuring dependable sparse channel recovery: the design of pilot signals and channel estimators tailored for hybrid-field communications.","To overcome the first challenge, we propose a method to derive optimal pilot signals, aimed at minimizing the mutual coherence of the sensing matrix within the context of compressive sensing (CS) problems.","These optimal signals are derived using the alternating direction method of multipliers (ADMM), ensuring robust performance in sparse channel recovery.","Additionally, leveraging the acquired optimal pilot signal, we introduce a two-stage channel estimation approach that sequentially estimates the LoS channel component and the hybrid-field scattering channel components.","Simulation results attest to the superiority of our co-designed approach for pilot signal and channel estimation over conventional CS-based methods, providing more reliable sparse channel recovery in practical scenarios."],"url":"http://arxiv.org/abs/2403.19105v1","category":"cs.IT"}
{"created":"2024-03-28 17:32:58","title":"SA-GS: Scale-Adaptive Gaussian Splatting for Training-Free Anti-Aliasing","abstract":"In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS). While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free. Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field's anti-alising performance. The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time. As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing. Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency. In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies. When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing. This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting. Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting. Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated. Our codes, data and models are available at https://github.com/zsy1987/SA-GS.","sentences":["In this paper, we present a Scale-adaptive method for Anti-aliasing Gaussian Splatting (SA-GS).","While the state-of-the-art method Mip-Splatting needs modifying the training procedure of Gaussian splatting, our method functions at test-time and is training-free.","Specifically, SA-GS can be applied to any pretrained Gaussian splatting field as a plugin to significantly improve the field's anti-alising performance.","The core technique is to apply 2D scale-adaptive filters to each Gaussian during test time.","As pointed out by Mip-Splatting, observing Gaussians at different frequencies leads to mismatches between the Gaussian scales during training and testing.","Mip-Splatting resolves this issue using 3D smoothing and 2D Mip filters, which are unfortunately not aware of testing frequency.","In this work, we show that a 2D scale-adaptive filter that is informed of testing frequency can effectively match the Gaussian scale, thus making the Gaussian primitive distribution remain consistent across different testing frequencies.","When scale inconsistency is eliminated, sampling rates smaller than the scene frequency result in conventional jaggedness, and we propose to integrate the projected 2D Gaussian within each pixel during testing.","This integration is actually a limiting case of super-sampling, which significantly improves anti-aliasing performance over vanilla Gaussian Splatting.","Through extensive experiments using various settings and both bounded and unbounded scenes, we show SA-GS performs comparably with or better than Mip-Splatting.","Note that super-sampling and integration are only effective when our scale-adaptive filtering is activated.","Our codes, data and models are available at https://github.com/zsy1987/SA-GS."],"url":"http://arxiv.org/abs/2403.19615v1","category":"cs.CV"}
{"created":"2024-03-28 17:28:24","title":"Positivity violations in marginal structural survival models with time-dependent confounding: a simulation study on IPTW-estimator performance","abstract":"In longitudinal observational studies, marginal structural models (MSMs) are a class of causal models used to analyze the effect of an exposure on the (survival) outcome of interest while accounting for exposure-affected time-dependent confounding. In the applied literature, inverse probability of treatment weighting (IPTW) has been widely adopted to estimate MSMs. An essential assumption for IPTW-based MSMs is the positivity assumption, which ensures that each individual in the population has a non-zero probability of receiving each exposure level within confounder strata. Positivity, along with consistency, conditional exchangeability, and correct specification of the weighting model, is crucial for valid causal inference through IPTW-based MSMs but is often overlooked compared to confounding bias. Positivity violations can arise from subjects having a zero probability of being exposed/unexposed (strict violations) or near-zero probabilities due to sampling variability (near violations). This article discusses the effect of violations in the positivity assumption on the estimates from IPTW-based MSMs. Building on the algorithms for simulating longitudinal survival data from MSMs by Havercroft and Didelez (2012) and Keogh et al. (2021), systematic simulations under strict/near positivity violations are performed. Various scenarios are explored by varying (i) the size of the confounder interval in which positivity violations arise, (ii) the sample size, (iii) the weight truncation strategy, and (iv) the subject's propensity to follow the protocol violation rule. This study underscores the importance of assessing positivity violations in IPTW-based MSMs to ensure robust and reliable causal inference in survival analyses.","sentences":["In longitudinal observational studies, marginal structural models (MSMs) are a class of causal models used to analyze the effect of an exposure on the (survival) outcome of interest while accounting for exposure-affected time-dependent confounding.","In the applied literature, inverse probability of treatment weighting (IPTW) has been widely adopted to estimate MSMs.","An essential assumption for IPTW-based MSMs is the positivity assumption, which ensures that each individual in the population has a non-zero probability of receiving each exposure level within confounder strata.","Positivity, along with consistency, conditional exchangeability, and correct specification of the weighting model, is crucial for valid causal inference through IPTW-based MSMs but is often overlooked compared to confounding bias.","Positivity violations can arise from subjects having a zero probability of being exposed/unexposed (strict violations) or near-zero probabilities due to sampling variability (near violations).","This article discusses the effect of violations in the positivity assumption on the estimates from IPTW-based MSMs.","Building on the algorithms for simulating longitudinal survival data from MSMs by Havercroft and Didelez (2012) and Keogh et al. (2021), systematic simulations under strict/near positivity violations are performed.","Various scenarios are explored by varying (i) the size of the confounder interval in which positivity violations arise, (ii) the sample size, (iii) the weight truncation strategy, and (iv) the subject's propensity to follow the protocol violation rule.","This study underscores the importance of assessing positivity violations in IPTW-based MSMs to ensure robust and reliable causal inference in survival analyses."],"url":"http://arxiv.org/abs/2403.19606v1","category":"stat.ME"}
{"created":"2024-03-28 17:27:44","title":"More on Black Holes Perceiving the Dark Dimension","abstract":"In the last two years the dark dimension scenario has emerged as focal point of many research interests. In particular, it functions as a stepping stone to address the cosmological hierarchy problem and provides a colosseum for dark matter contenders. We reexamine the possibility that primordial black holes (PBHs) perceiving the dark dimension could constitute all of the dark matter in the universe. We re-assess limits on the abundance of PBHs as dark matter candidates from $\\gamma$-ray emission resulting from Hawking evaporation. We re-evaluate constraints from the diffuse $\\gamma$-ray emission in the direction of the Galactic center which offer the best and most solid upper limits on the dark matter fraction composed of PBHs. The revised mass range which allows PBHs to assemble all cosmological dark matter is estimated to be $10^{15} \\alt M_{\\rm BH}/{\\rm g} \\alt 10^{21}$. We demonstrate that due to the constraints from $\\gamma$-ray emission, quantum corrections due to the speculative memory burden effect do not modify this mass range. We also investigate the main characteristics of PBHs which are localized in the bulk. We show that PBHs localized in the bulk can make all cosmological dark matter if $10^{11} \\alt M_{\\rm BH}/{\\rm g} \\alt 10^{21}$. Finally, we comment on the black holes that could be produced if one advocates a space with two boundaries for the dark dimension.","sentences":["In the last two years the dark dimension scenario has emerged as focal point of many research interests.","In particular, it functions as a stepping stone to address the cosmological hierarchy problem and provides a colosseum for dark matter contenders.","We reexamine the possibility that primordial black holes (PBHs) perceiving the dark dimension could constitute all of the dark matter in the universe.","We re-assess limits on the abundance of PBHs as dark matter candidates from $\\gamma$-ray emission resulting from Hawking evaporation.","We re-evaluate constraints from the diffuse $\\gamma$-ray emission in the direction of the Galactic center which offer the best and most solid upper limits on the dark matter fraction composed of PBHs.","The revised mass range which allows PBHs to assemble all cosmological dark matter is estimated to be $10^{15} \\alt M_{\\rm BH}/{\\rm g} \\alt 10^{21}$.","We demonstrate that due to the constraints from $\\gamma$-ray emission, quantum corrections due to the speculative memory burden effect do not modify this mass range.","We also investigate the main characteristics of PBHs which are localized in the bulk.","We show that PBHs localized in the bulk can make all cosmological dark matter if $10^{11} \\alt M_{\\rm BH}/{\\rm g} \\alt 10^{21}$.","Finally, we comment on the black holes that could be produced if one advocates a space with two boundaries for the dark dimension."],"url":"http://arxiv.org/abs/2403.19604v1","category":"hep-th"}
{"created":"2024-03-28 17:15:35","title":"Reproducibility Made Easy: A Tool for Methodological Transparency and Efficient Standardized Reporting based on the proposed MRSinMRS Consensus","abstract":"A recent expert consensus found that non-standard reporting in MRS studies led to poor reproducibility. In order to address this, MRSinMRS guidelines were introduced; however, because of the disparate nomenclature and data formats, adoption has been slow. To get around this problem, REMY, a toolbox that supports major vendor formats, was created. By efficiently filling in important fields in the MRSinMRS table, it improves reproducibility. Even with certain hardware-related restrictions, REMY makes a substantial contribution to the completion of acquisition parameters, which facilitates reporting. Its compatibility and user-friendly interface should promote widespread adoption of MRSinMRS, raising the caliber of MRS research.","sentences":["A recent expert consensus found that non-standard reporting in MRS studies led to poor reproducibility.","In order to address this, MRSinMRS guidelines were introduced; however, because of the disparate nomenclature and data formats, adoption has been slow.","To get around this problem, REMY, a toolbox that supports major vendor formats, was created.","By efficiently filling in important fields in the MRSinMRS table, it improves reproducibility.","Even with certain hardware-related restrictions, REMY makes a substantial contribution to the completion of acquisition parameters, which facilitates reporting.","Its compatibility and user-friendly interface should promote widespread adoption of MRSinMRS, raising the caliber of MRS research."],"url":"http://arxiv.org/abs/2403.19594v1","category":"physics.med-ph"}
{"created":"2024-03-28 17:08:58","title":"TOGS: Gaussian Splatting with Temporal Opacity Offset for Real-Time 4D DSA Rendering","abstract":"Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels. It plays a significant role in the diagnosis of cerebrovascular diseases. Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions. The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed. To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA. We introduce an opacity offset table for each Gaussian to model the temporal variations in the radiance of the contrast agent. By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined. This enables us to render the 2D DSA image at that specific moment. Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios. During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model. The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art reconstruction quality under the same number of training views. Additionally, it enables real-time rendering while maintaining low storage overhead. The code will be publicly available.","sentences":["Four-dimensional Digital Subtraction Angiography (4D DSA) is a medical imaging technique that provides a series of 2D images captured at different stages and angles during the process of contrast agent filling blood vessels.","It plays a significant role in the diagnosis of cerebrovascular diseases.","Improving the rendering quality and speed under sparse sampling is important for observing the status and location of lesions.","The current methods exhibit inadequate rendering quality in sparse views and suffer from slow rendering speed.","To overcome these limitations, we propose TOGS, a Gaussian splatting method with opacity offset over time, which can effectively improve the rendering quality and speed of 4D DSA.","We introduce an opacity offset table for each Gaussian to model the temporal variations in the radiance of the contrast agent.","By interpolating the opacity offset table, the opacity variation of the Gaussian at different time points can be determined.","This enables us to render the 2D DSA image at that specific moment.","Additionally, we introduced a Smooth loss term in the loss function to mitigate overfitting issues that may arise in the model when dealing with sparse view scenarios.","During the training phase, we randomly prune Gaussians, thereby reducing the storage overhead of the model.","The experimental results demonstrate that compared to previous methods, this model achieves state-of-the-art reconstruction quality under the same number of training views.","Additionally, it enables real-time rendering while maintaining low storage overhead.","The code will be publicly available."],"url":"http://arxiv.org/abs/2403.19586v1","category":"cs.CV"}
{"created":"2024-03-28 16:57:13","title":"Measurement of double-differential cross sections for mesonless charged-current muon neutrino interactions on argon with final-state protons using the MicroBooNE detector","abstract":"Charged-current neutrino interactions with final states containing zero mesons and at least one proton are of high interest for current and future accelerator-based neutrino oscillation experiments. Using the Booster Neutrino Beam and the MicroBooNE detector at Fermi National Accelerator Laboratory, we have obtained the first double-differential cross section measurements of this channel for muon neutrino scattering on an argon target with a proton momentum threshold of 0.25 GeV/c. We also report a flux-averaged total cross section of $\\sigma = (11.8 \\pm 1.2) \\times 10^{-38}$ cm$^2$ / Ar and several single-differential measurements which extend and improve upon previous results. Statistical and systematic uncertainties are quantified with a full treatment of correlations across 359 kinematic bins, including correlations between distributions describing different observables. The resulting data set provides the most detailed information obtained to date for testing models of mesonless neutrino-argon scattering.","sentences":["Charged-current neutrino interactions with final states containing zero mesons and at least one proton are of high interest for current and future accelerator-based neutrino oscillation experiments.","Using the Booster Neutrino Beam and the MicroBooNE detector at Fermi National Accelerator Laboratory, we have obtained the first double-differential cross section measurements of this channel for muon neutrino scattering on an argon target with a proton momentum threshold of 0.25 GeV/c.","We also report a flux-averaged total cross section of $\\sigma = (11.8 \\pm 1.2)","\\times 10^{-38}$","cm$^2$ / Ar and several single-differential measurements which extend and improve upon previous results.","Statistical and systematic uncertainties are quantified with a full treatment of correlations across 359 kinematic bins, including correlations between distributions describing different observables.","The resulting data set provides the most detailed information obtained to date for testing models of mesonless neutrino-argon scattering."],"url":"http://arxiv.org/abs/2403.19574v1","category":"hep-ex"}
{"created":"2024-03-28 16:47:12","title":"Extremality as a Consistency Condition on Subregion Duality","abstract":"In JT gravity coupled to a CFT, I argue without using the path integral that the entanglement wedge of a boundary region is bounded by a quantum extremal surface (QES). For any candidate not bounded by a QES, a unitary in the complement can make reconstruction within the candidate inconsistent with boundary causality. The case without islands is a direct consequence of subregion duality, and the case with islands can also be dealt with with a stronger assumption.","sentences":["In JT gravity coupled to a CFT, I argue without using the path integral that the entanglement wedge of a boundary region is bounded by a quantum extremal surface (QES).","For any candidate not bounded by a QES, a unitary in the complement can make reconstruction within the candidate inconsistent with boundary causality.","The case without islands is a direct consequence of subregion duality, and the case with islands can also be dealt with with a stronger assumption."],"url":"http://arxiv.org/abs/2403.19562v1","category":"hep-th"}
{"created":"2024-03-28 16:36:15","title":"Probing the dark Universe with gravitational waves","abstract":"Gravitational waves (GW) are expected to interact with dark energy and dark matter, affecting their propagation on cosmological scales. In order to model this interaction, we derive a gauge invariant effective equation and action valid for all GWs polarizations, based on encoding the effects of the interaction of GWs at different order in perturbations, in a polarization, frequency and time dependent effective speed. The invariance of perturbations under time dependent conformal transformations and the gauge invariance of the GWs allow to obtain the unitary gauge effective action in any conformally related frame, making transparent the relation between Einstein and Jordan frame. The propagation time and luminosity distance of different GWs polarizations allow to probe at different frequencies and redshift the dark Universe, which act as an effective medium, whose physical properties can be modeled by the GWs effective speed.","sentences":["Gravitational waves (GW) are expected to interact with dark energy and dark matter, affecting their propagation on cosmological scales.","In order to model this interaction, we derive a gauge invariant effective equation and action valid for all GWs polarizations, based on encoding the effects of the interaction of GWs at different order in perturbations, in a polarization, frequency and time dependent effective speed.","The invariance of perturbations under time dependent conformal transformations and the gauge invariance of the GWs allow to obtain the unitary gauge effective action in any conformally related frame, making transparent the relation between Einstein and Jordan frame.","The propagation time and luminosity distance of different GWs polarizations allow to probe at different frequencies and redshift the dark Universe, which act as an effective medium, whose physical properties can be modeled by the GWs effective speed."],"url":"http://arxiv.org/abs/2403.19552v1","category":"astro-ph.CO"}
{"created":"2024-03-28 16:20:30","title":"A third-order low-regularity trigonometric integrator for the semilinear Klein-Gordon equation","abstract":"In this paper, we propose and analyze a novel third-order low-regularity trigonometric integrator for the semilinear Klein-Gordon equation in the $d$-dimensional space with $d=1,2,3$. The integrator is constructed based on the full use of Duhamel's formula and the technique of twisted function to the trigonometric integrals. Rigorous error estimates are presented and the proposed method is shown to have third-order accuracy in the energy space under a weak regularity requirement in $H^{2}\\times H^{1}$. A numerical experiment shows that the proposed third-order low-regularity integrator is much more accurate than the well-known exponential integrators of order three for approximating the Klein-Gordon equation with nonsmooth solutions.","sentences":["In this paper, we propose and analyze a novel third-order low-regularity trigonometric integrator for the semilinear Klein-Gordon equation in the $d$-dimensional space with $d=1,2,3$. The integrator is constructed based on the full use of Duhamel's formula and the technique of twisted function to the trigonometric integrals.","Rigorous error estimates are presented and the proposed method is shown to have third-order accuracy in the energy space under a weak regularity requirement in $H^{2}\\times H^{1}$.","A numerical experiment shows that the proposed third-order low-regularity integrator is much more accurate than the well-known exponential integrators of order three for approximating the Klein-Gordon equation with nonsmooth solutions."],"url":"http://arxiv.org/abs/2403.19540v1","category":"math.NA"}
{"created":"2024-03-28 16:13:22","title":"De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts","abstract":"Data-Free Knowledge Distillation (DFKD) is a promising task to train high-performance small models to enhance actual deployment without relying on the original training data. Existing methods commonly avoid relying on private data by utilizing synthetic or sampled data. However, a long-overlooked issue is that the severe distribution shifts between their substitution and original data, which manifests as huge differences in the quality of images and class proportions. The harmful shifts are essentially the confounder that significantly causes performance bottlenecks. To tackle the issue, this paper proposes a novel perspective with causal inference to disentangle the student models from the impact of such shifts. By designing a customized causal graph, we first reveal the causalities among the variables in the DFKD task. Subsequently, we propose a Knowledge Distillation Causal Intervention (KDCI) framework based on the backdoor adjustment to de-confound the confounder. KDCI can be flexibly combined with most existing state-of-the-art baselines. Experiments in combination with six representative DFKD methods demonstrate the effectiveness of our KDCI, which can obviously help existing methods under almost all settings, \\textit{e.g.}, improving the baseline by up to 15.54\\% accuracy on the CIFAR-100 dataset.","sentences":["Data-Free Knowledge Distillation (DFKD) is a promising task to train high-performance small models to enhance actual deployment without relying on the original training data.","Existing methods commonly avoid relying on private data by utilizing synthetic or sampled data.","However, a long-overlooked issue is that the severe distribution shifts between their substitution and original data, which manifests as huge differences in the quality of images and class proportions.","The harmful shifts are essentially the confounder that significantly causes performance bottlenecks.","To tackle the issue, this paper proposes a novel perspective with causal inference to disentangle the student models from the impact of such shifts.","By designing a customized causal graph, we first reveal the causalities among the variables in the DFKD task.","Subsequently, we propose a Knowledge Distillation Causal Intervention (KDCI) framework based on the backdoor adjustment to de-confound the confounder.","KDCI can be flexibly combined with most existing state-of-the-art baselines.","Experiments in combination with six representative DFKD methods demonstrate the effectiveness of our KDCI, which can obviously help existing methods under almost all settings, \\textit{e.g.}, improving the baseline by up to 15.54\\% accuracy on the CIFAR-100 dataset."],"url":"http://arxiv.org/abs/2403.19539v1","category":"cs.CV"}
{"created":"2024-03-28 16:06:14","title":"Measuring Solar Neutrinos in the SNO+ Detector","abstract":"The SNO+ experiment is a large multi-purpose neutrino detector, currently filled with liquid scintillator. For the first time in a single experiment, SNO+ is able to measure the neutrino oscillation parameters $\\theta_{12}$ and $\\Delta m^{2}_{21}$ simultaneously through both reactor anti-neutrinos and $^{8}B$ solar neutrinos. The latter approach is demonstrated here, with an analysis of an initial 80 days of scintillator phase data. A Bayesian statistical approach via Markov Chain Monte Carlo is used, allowing for the simultaneous fitting of the oscillation parameters, $^{8}B$ neutrino flux, background components with constraints, and systematic uncertainties. The neutrino oscillation parameter $\\theta_{12}$ was measured to be $38.9^{\\circ+8.0^{\\circ}}_{-7.9^{\\circ}}$, assuming the current global fit flux of $^{8}B$ solar neutrinos. This is consistent with the current global fit result for $\\theta_{12}$. A sensitivity study shows that this measurement is statistics-limited, and precision could be improved by a factor of two with two years of livetime, assuming the same backgrounds and selections.","sentences":["The SNO+ experiment is a large multi-purpose neutrino detector, currently filled with liquid scintillator.","For the first time in a single experiment, SNO+ is able to measure the neutrino oscillation parameters $\\theta_{12}$ and $\\Delta m^{2}_{21}$ simultaneously through both reactor anti-neutrinos and $^{8}B$ solar neutrinos.","The latter approach is demonstrated here, with an analysis of an initial 80 days of scintillator phase data.","A Bayesian statistical approach via Markov Chain Monte Carlo is used, allowing for the simultaneous fitting of the oscillation parameters, $^{8}B$ neutrino flux, background components with constraints, and systematic uncertainties.","The neutrino oscillation parameter $\\theta_{12}$ was measured to be $38.9^{\\circ+8.0^{\\circ}}_{-7.9^{\\circ}}$, assuming the current global fit flux of $^{8}B$ solar neutrinos.","This is consistent with the current global fit result for $\\theta_{12}$. A sensitivity study shows that this measurement is statistics-limited, and precision could be improved by a factor of two with two years of livetime, assuming the same backgrounds and selections."],"url":"http://arxiv.org/abs/2403.19532v1","category":"hep-ex"}
{"created":"2024-03-28 16:05:22","title":"Gauge Theories With Infinite Multiplets of Fermions","abstract":"We study the coupling constant renormalization of gauge theories with an infinite multiplet of fermions, using the zeta function method to make sense of the infinite sums over fermions. If the gauge group K is the maximal compact subgroup of a simple non-compact group G, such infinite multiplets can arise naturally, as reductions of discrete series unitary representations of G. The example K=U(1) and G=SU(1,1) will be studied in detail. Surprisingly, there are abelian gauge theories which are asymptotically free; and others that are UV finite.","sentences":["We study the coupling constant renormalization of gauge theories with an infinite multiplet of fermions, using the zeta function method to make sense of the infinite sums over fermions.","If the gauge group K is the maximal compact subgroup of a simple non-compact group G, such infinite multiplets can arise naturally, as reductions of discrete series unitary representations of G.","The example K=U(1) and G=SU(1,1) will be studied in detail.","Surprisingly, there are abelian gauge theories which are asymptotically free; and others that are UV finite."],"url":"http://arxiv.org/abs/2403.19528v1","category":"hep-th"}
{"created":"2024-03-28 16:01:05","title":"Logic and Languages of Higher-Dimensional Automata","abstract":"In this paper we study finite higher-dimensional automata (HDAs) from the logical point of view. Languages of HDAs are sets of finite bounded-width interval pomsets with interfaces (iiPoms<=k) closed under order extension. We prove that languages of HDAs are MSO-definable. For the converse, we show that the order extensions of MSO-definable sets of iiPoms<=k are languages of HDAs. As a consequence, unlike the case of all pomsets, order extension of MSO-definable sets of iiPoms<=k is also MSO-definable.","sentences":["In this paper we study finite higher-dimensional automata (HDAs) from the logical point of view.","Languages of HDAs are sets of finite bounded-width interval pomsets with interfaces (iiPoms<=k) closed under order extension.","We prove that languages of HDAs are MSO-definable.","For the converse, we show that the order extensions of MSO-definable sets of iiPoms<=k are languages of HDAs.","As a consequence, unlike the case of all pomsets, order extension of MSO-definable sets of iiPoms<=k is also MSO-definable."],"url":"http://arxiv.org/abs/2403.19526v1","category":"cs.FL"}
{"created":"2024-03-28 15:43:38","title":"On the Robustness of LDP Protocols for Numerical Attributes under Data Poisoning Attacks","abstract":"Recent studies reveal that local differential privacy (LDP) protocols are vulnerable to data poisoning attacks where an attacker can manipulate the final estimate on the server by leveraging the characteristics of LDP and sending carefully crafted data from a small fraction of controlled local clients. This vulnerability raises concerns regarding the robustness and reliability of LDP in hostile environments.   In this paper, we conduct a systematic investigation of the robustness of state-of-the-art LDP protocols for numerical attributes, i.e., categorical frequency oracles (CFOs) with binning and consistency, and distribution reconstruction. We evaluate protocol robustness through an attack-driven approach and propose new metrics for cross-protocol attack gain measurement. The results indicate that Square Wave and CFO-based protocols in the Server setting are more robust against the attack compared to the CFO-based protocols in the User setting. Our evaluation also unfolds new relationships between LDP security and its inherent design choices. We found that the hash domain size in local-hashing-based LDP has a profound impact on protocol robustness beyond the well-known effect on utility. Further, we propose a zero-shot attack detection by leveraging the rich reconstructed distribution information. The experiment show that our detection significantly improves the existing methods and effectively identifies data manipulation in challenging scenarios.","sentences":["Recent studies reveal that local differential privacy (LDP) protocols are vulnerable to data poisoning attacks where an attacker can manipulate the final estimate on the server by leveraging the characteristics of LDP and sending carefully crafted data from a small fraction of controlled local clients.","This vulnerability raises concerns regarding the robustness and reliability of LDP in hostile environments.   ","In this paper, we conduct a systematic investigation of the robustness of state-of-the-art LDP protocols for numerical attributes, i.e., categorical frequency oracles (CFOs) with binning and consistency, and distribution reconstruction.","We evaluate protocol robustness through an attack-driven approach and propose new metrics for cross-protocol attack gain measurement.","The results indicate that Square Wave and CFO-based protocols in the Server setting are more robust against the attack compared to the CFO-based protocols in the User setting.","Our evaluation also unfolds new relationships between LDP security and its inherent design choices.","We found that the hash domain size in local-hashing-based LDP has a profound impact on protocol robustness beyond the well-known effect on utility.","Further, we propose a zero-shot attack detection by leveraging the rich reconstructed distribution information.","The experiment show that our detection significantly improves the existing methods and effectively identifies data manipulation in challenging scenarios."],"url":"http://arxiv.org/abs/2403.19510v1","category":"cs.CR"}
{"created":"2024-03-28 15:27:34","title":"Surface-based parcellation and vertex-wise analysis of ultra high-resolution ex vivo 7 tesla MRI in neurodegenerative diseases","abstract":"Magnetic resonance imaging (MRI) is the standard modality to understand human brain structure and function in vivo (antemortem). Decades of research in human neuroimaging has led to the widespread development of methods and tools to provide automated volume-based segmentations and surface-based parcellations which help localize brain functions to specialized anatomical regions. Recently ex vivo (postmortem) imaging of the brain has opened-up avenues to study brain structure at sub-millimeter ultra high-resolution revealing details not possible to observe with in vivo MRI. Unfortunately, there has been limited methodological development in ex vivo MRI primarily due to lack of datasets and limited centers with such imaging resources. Therefore, in this work, we present one-of-its-kind dataset of 82 ex vivo T2w whole brain hemispheres MRI at 0.3 mm isotropic resolution spanning Alzheimer's disease and related dementias. We adapted and developed a fast and easy-to-use automated surface-based pipeline to parcellate, for the first time, ultra high-resolution ex vivo brain tissue at the native subject space resolution using the Desikan-Killiany-Tourville (DKT) brain atlas. This allows us to perform vertex-wise analysis in the template space and thereby link morphometry measures with pathology measurements derived from histology. We will open-source our dataset docker container, Jupyter notebooks for ready-to-use out-of-the-box set of tools and command line options to advance ex vivo MRI clinical brain imaging research on the project webpage.","sentences":["Magnetic resonance imaging (MRI) is the standard modality to understand human brain structure and function in vivo (antemortem).","Decades of research in human neuroimaging has led to the widespread development of methods and tools to provide automated volume-based segmentations and surface-based parcellations which help localize brain functions to specialized anatomical regions.","Recently ex vivo (postmortem) imaging of the brain has opened-up avenues to study brain structure at sub-millimeter ultra high-resolution revealing details not possible to observe with in vivo MRI.","Unfortunately, there has been limited methodological development in ex vivo MRI primarily due to lack of datasets and limited centers with such imaging resources.","Therefore, in this work, we present one-of-its-kind dataset of 82 ex vivo T2w whole brain hemispheres MRI at 0.3 mm isotropic resolution spanning Alzheimer's disease and related dementias.","We adapted and developed a fast and easy-to-use automated surface-based pipeline to parcellate, for the first time, ultra high-resolution ex vivo brain tissue at the native subject space resolution using the Desikan-Killiany-Tourville (DKT) brain atlas.","This allows us to perform vertex-wise analysis in the template space and thereby link morphometry measures with pathology measurements derived from histology.","We will open-source our dataset docker container, Jupyter notebooks for ready-to-use out-of-the-box set of tools and command line options to advance ex vivo MRI clinical brain imaging research on the project webpage."],"url":"http://arxiv.org/abs/2403.19497v1","category":"cs.CV"}
{"created":"2024-03-28 14:15:29","title":"Long-range Phase Coherence and Tunable Second Order $\u03c6_0$-Josephson Effect in a Dirac Semimetal $1T-PtTe_2$","abstract":"Superconducting diode effects have recently attracted much attention for their potential applications in superconducting logic circuits. Several mechanisms such as magneto-chiral effects, finite momentum Cooper pairing, asymmetric edge currents have been proposed to give rise to a supercurrent diode effect in different materials. In this work, we establish the presence of a large intrinsic Josephson diode effect in a type-II Dirac semimetal $1T-PtTe_2$ facilitated by its helical spin-momentum locking and distinguish it from other extrinsic effects. The magnitude of the Josephson diode effect is shown to be directly correlated to the large second-harmonic component of the supercurrent that is induced by the significant contribution of the topological spin-momentum locked states that promote coherent Andreev processes in the junction. We denote such junctions, where the relative phase between the two harmonics corresponding to charge transfers of $2e$ and $4e$ can be tuned by a magnetic field, as second order ${\\phi}_0$-junctions. The direct correspondence between the second harmonic supercurrent component and the diode effect in $1T-PtTe_2$ junctions makes topological semimetals with high transparency an ideal platform to study and implement the Josephson diode effect, while also enabling further research on higher order supercurrent transport in Josephson junctions.","sentences":["Superconducting diode effects have recently attracted much attention for their potential applications in superconducting logic circuits.","Several mechanisms such as magneto-chiral effects, finite momentum Cooper pairing, asymmetric edge currents have been proposed to give rise to a supercurrent diode effect in different materials.","In this work, we establish the presence of a large intrinsic Josephson diode effect in a type-II Dirac semimetal $1T-PtTe_2$ facilitated by its helical spin-momentum locking and distinguish it from other extrinsic effects.","The magnitude of the Josephson diode effect is shown to be directly correlated to the large second-harmonic component of the supercurrent that is induced by the significant contribution of the topological spin-momentum locked states that promote coherent Andreev processes in the junction.","We denote such junctions, where the relative phase between the two harmonics corresponding to charge transfers of $2e$ and $4e$ can be tuned by a magnetic field, as second order ${\\phi}_0$-junctions.","The direct correspondence between the second harmonic supercurrent component and the diode effect in $1T-PtTe_2$ junctions makes topological semimetals with high transparency an ideal platform to study and implement the Josephson diode effect, while also enabling further research on higher order supercurrent transport in Josephson junctions."],"url":"http://arxiv.org/abs/2403.19445v1","category":"cond-mat.supr-con"}
{"created":"2024-03-28 13:59:02","title":"Transport properties through alternating borophene and graphene superlattices","abstract":"The electronic transport properties of two junctions (BGB, GBG) made of borophene (B) and graphene (G) are studied. Using the transfer matrix method with Chebyshev polynomials, we have studied single and multiple barriers in a superlattice configuration. We showed that a single barrier exhibits remarkable tilted transport properties, with perfect transmission observed for both junctions under normal incidence. We found that robust superlattice transmission is maintained for multiple barriers, particularly in the BGB junction. It turns out that by varying the incident energy, many gaps appear in the transmission probability. The number, width, and position of these transmission gaps can be manipulated by adjusting the number of cells, incident angle, and barrier characteristics. For diffuse transport, we observed significant variations in conductance and the Fano factor, highlighting the sensitivity of these junctions to the physical parameters. We showed different behaviors between BGB and GBG junctions, particularly with respect to the response of conductance and Fano factor when barrier height varies. For ballistic transport, we have seen that the minimum conductivity is related to the maximum Fano factor, demonstrating their control under specific conditions of the physical parameters. Analysis of the length ratio (geometric factor) revealed some remarkable patterns, where conductivity and the Fano factor converged to certain values as the ratio approached infinity.","sentences":["The electronic transport properties of two junctions (BGB, GBG) made of borophene (B) and graphene (G) are studied.","Using the transfer matrix method with Chebyshev polynomials, we have studied single and multiple barriers in a superlattice configuration.","We showed that a single barrier exhibits remarkable tilted transport properties, with perfect transmission observed for both junctions under normal incidence.","We found that robust superlattice transmission is maintained for multiple barriers, particularly in the BGB junction.","It turns out that by varying the incident energy, many gaps appear in the transmission probability.","The number, width, and position of these transmission gaps can be manipulated by adjusting the number of cells, incident angle, and barrier characteristics.","For diffuse transport, we observed significant variations in conductance and the Fano factor, highlighting the sensitivity of these junctions to the physical parameters.","We showed different behaviors between BGB and GBG junctions, particularly with respect to the response of conductance and Fano factor when barrier height varies.","For ballistic transport, we have seen that the minimum conductivity is related to the maximum Fano factor, demonstrating their control under specific conditions of the physical parameters.","Analysis of the length ratio (geometric factor) revealed some remarkable patterns, where conductivity and the Fano factor converged to certain values as the ratio approached infinity."],"url":"http://arxiv.org/abs/2403.19429v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 13:58:05","title":"Burst Super-Resolution with Diffusion Models for Improving Perceptual Quality","abstract":"While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image. In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry. Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries. Such high-fidelity images can be reconstructed by diffusion models. However, prior SR methods using the diffusion model are not properly optimized for the burst SR task. Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR. In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the diffusion model. This reverse process from the intermediate step 1) skips diffusion steps for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures. Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics. Code: https://github.com/placerkyo/BSRD","sentences":["While burst LR images are useful for improving the SR image quality compared with a single LR image, prior SR networks accepting the burst LR images are trained in a deterministic manner, which is known to produce a blurry SR image.","In addition, it is difficult to perfectly align the burst LR images, making the SR image more blurry.","Since such blurry images are perceptually degraded, we aim to reconstruct the sharp high-fidelity boundaries.","Such high-fidelity images can be reconstructed by diffusion models.","However, prior SR methods using the diffusion model are not properly optimized for the burst SR task.","Specifically, the reverse process starting from a random sample is not optimized for image enhancement and restoration methods, including burst SR.","In our proposed method, on the other hand, burst LR features are used to reconstruct the initial burst SR image that is fed into an intermediate step in the diffusion model.","This reverse process from the intermediate step 1) skips diffusion steps for reconstructing the global structure of the image and 2) focuses on steps for refining detailed textures.","Our experimental results demonstrate that our method can improve the scores of the perceptual quality metrics.","Code: https://github.com/placerkyo/BSRD"],"url":"http://arxiv.org/abs/2403.19428v1","category":"cs.CV"}
{"created":"2024-03-28 13:31:10","title":"Kitaev Interactions Through an Extended Superexchange Pathway in the jeff = 1/2 Ru3+ Honeycomb Magnet, RuP3SiO11","abstract":"Magnetic materials are composed of the simple building blocks of magnetic moments on a crystal lattice that interact via short-range magnetic exchange interactions. Yet from these simple building blocks emerges a remarkable diversity of magnetic states. Some of these, such as ferromagnetism, are familiar in our everyday lives, while others reveal the deep quantum mechanical origins of magnetism. A prime example of the latter are quantum spin liquid (QSL) states in which -- unlike in a ferromagnet where magnetic moments are driven by their exchange interactions to adopt long-range order -- magnetic moments remain disordered at low temperatures but are simultaneously correlated over long length scales through quantum entanglement. A particularly promising theoretical model of a QSL is the Kitaev model, composed of unusual bond-dependent exchange interactions between magnetic moments on a honeycomb lattice. However, the Kitaev QSL is extremely challenging to realise experimentally as it is unstable to competing exchange interactions and crystal lattice perturbations that inevitably arise in real materials. This makes it essential to understand the relationship between the structure and interactions that may give rise to Kitaev interactions in new candidate materials. Here we show that the material requirements for the Kitaev QSL survive for an extended pseudo-edge-sharing superexchange pathway of Ru3+ 4d5 octahedra within the honeycomb layers of the inorganic framework solid, RuP3SiO11. Through materials synthesis and structural characterisation, resonant inelastic X-ray and neutron scattering experiments, we confirm the requisite jeff = 1/2 state of Ru3+ in RuP3SiO11 and resolve the hierarchy of exchange interactions that provide experimental access to an otherwise unexplored region of the extended Kitaev phase diagram.","sentences":["Magnetic materials are composed of the simple building blocks of magnetic moments on a crystal lattice that interact via short-range magnetic exchange interactions.","Yet from these simple building blocks emerges a remarkable diversity of magnetic states.","Some of these, such as ferromagnetism, are familiar in our everyday lives, while others reveal the deep quantum mechanical origins of magnetism.","A prime example of the latter are quantum spin liquid (QSL) states in which -- unlike in a ferromagnet where magnetic moments are driven by their exchange interactions to adopt long-range order -- magnetic moments remain disordered at low temperatures but are simultaneously correlated over long length scales through quantum entanglement.","A particularly promising theoretical model of a QSL is the Kitaev model, composed of unusual bond-dependent exchange interactions between magnetic moments on a honeycomb lattice.","However, the Kitaev QSL is extremely challenging to realise experimentally as it is unstable to competing exchange interactions and crystal lattice perturbations that inevitably arise in real materials.","This makes it essential to understand the relationship between the structure and interactions that may give rise to Kitaev interactions in new candidate materials.","Here we show that the material requirements for the Kitaev QSL survive for an extended pseudo-edge-sharing superexchange pathway of Ru3+ 4d5 octahedra within the honeycomb layers of the inorganic framework solid, RuP3SiO11.","Through materials synthesis and structural characterisation, resonant inelastic X-ray and neutron scattering experiments, we confirm the requisite jeff = 1/2 state of Ru3+ in RuP3SiO11 and resolve the hierarchy of exchange interactions that provide experimental access to an otherwise unexplored region of the extended Kitaev phase diagram."],"url":"http://arxiv.org/abs/2403.19406v1","category":"cond-mat.str-el"}
{"created":"2024-03-28 13:11:50","title":"Kernel entropy estimation for linear processes II","abstract":"Let $X=\\{X_n: n\\in \\mathbb{N}\\}$ be a linear process with bounded probability density function $f(x)$. Under certain conditions, we use the kernel estimator \\[ \\frac{2}{n(n-1)h_n} \\sum_{1\\le i<j\\le n}K\\Big(\\frac{X_i-X_j}{h_n}\\Big) \\] to estimate the quadratic functional of $\\int_{\\mathbb{R}}f^2(x)dx$ of the linear process $X=\\{X_n: n\\in \\mathbb{N}\\}$ and improve the corresponding results in [4].","sentences":["Let $X=\\{X_n: n\\in \\mathbb{N}\\}$ be a linear process with bounded probability density function $f(x)$. Under certain conditions, we use the kernel estimator \\[ \\frac{2}{n(n-1)h_n} \\sum_{1\\le i<j\\le n}K\\Big(\\frac{X_i-X_j}{h_n}\\Big) \\] to estimate the quadratic functional of $\\int_{\\mathbb{R}}f^2(x)dx$ of the linear process $X=\\{X_n: n\\in \\mathbb{N}\\}$ and improve the corresponding results in [4]."],"url":"http://arxiv.org/abs/2403.19395v1","category":"math.ST"}
{"created":"2024-03-28 12:08:16","title":"The SNO+ Journey to 0\u03bd\u03b2\u03b2","abstract":"SNO+ is a large multipurpose experiment with the ultimate goal of searching for the neutrinoless double beta decay in $^{130}\\mathrm{Te}$. After a commissioning phase with water as the target medium, during which acquired data allowed for measurements of solar neutrinos and the detection of reactor antineutrinos, SNO+ is now filled with 780 tonnes of liquid scintillator. The higher light yield of the scintillator enhances the physics capabilities of the experiment, and a physics program including reactor, geo and solar neutrinos is currently underway. The water and unloaded scintillator phases provide crucial commissioning milestones in preparation for the tellurium loading, such as calibrating the detector and making extensive background constraint measurements as components of the final scintillator cocktail are gradually added. In a first phase, 3900~kg of natural tellurium (0.5\\%) will be added to the scintillator for a predicted sensitivity of about $2\\times20^{26}$ years (90\\% CL) with 3 years of livetime. Higher tellurium loading will follow for predicted sensitivities above $1\\times10^{27}$ years (3\\% loading).","sentences":["SNO+ is a large multipurpose experiment with the ultimate goal of searching for the neutrinoless double beta decay in $^{130}\\mathrm{Te}$. After a commissioning phase with water as the target medium, during which acquired data allowed for measurements of solar neutrinos and the detection of reactor antineutrinos, SNO+ is now filled with 780 tonnes of liquid scintillator.","The higher light yield of the scintillator enhances the physics capabilities of the experiment, and a physics program including reactor, geo and solar neutrinos is currently underway.","The water and unloaded scintillator phases provide crucial commissioning milestones in preparation for the tellurium loading, such as calibrating the detector and making extensive background constraint measurements as components of the final scintillator cocktail are gradually added.","In a first phase, 3900~kg of natural tellurium (0.5\\%) will be added to the scintillator for a predicted sensitivity of about $2\\times20^{26}$ years (90\\% CL) with 3 years of livetime.","Higher tellurium loading will follow for predicted sensitivities above $1\\times10^{27}$ years (3\\% loading)."],"url":"http://arxiv.org/abs/2403.19351v1","category":"hep-ex"}
{"created":"2024-03-28 11:41:12","title":"Simulating Relational Event Histories -- Why and How","abstract":"Many important social phenomena result from repeated interactions among individuals over time such as email exchanges in an organization, or face-to-face interactions in a classroom. Insights into the mechanisms underlying the dynamics of these interactions can be achieved through simulations of networks on a fine temporal granularity. In this paper, we present statistical frameworks to simulate relational event networks under dyadic and actor-oriented relational event models. These simulators have a broad applicability in temporal social network research such as model fit assessment, theory building, network intervention planning, making predictions, understanding the impact of network structures, to name a few. We show this in three extensive applications. First, it is shown why simulation-based techniques are crucial for relational event model assessment, for example to investigate how past events affect future interactions in the network. Second, we demonstrate how simulation techniques contribute to a better understanding of the longevity of network interventions. Third, we show how simulation techniques are important when building and extending theories about social phenomena such as understanding social identity dynamics using optimal distinctiveness theory.","sentences":["Many important social phenomena result from repeated interactions among individuals over time such as email exchanges in an organization, or face-to-face interactions in a classroom.","Insights into the mechanisms underlying the dynamics of these interactions can be achieved through simulations of networks on a fine temporal granularity.","In this paper, we present statistical frameworks to simulate relational event networks under dyadic and actor-oriented relational event models.","These simulators have a broad applicability in temporal social network research such as model fit assessment, theory building, network intervention planning, making predictions, understanding the impact of network structures, to name a few.","We show this in three extensive applications.","First, it is shown why simulation-based techniques are crucial for relational event model assessment, for example to investigate how past events affect future interactions in the network.","Second, we demonstrate how simulation techniques contribute to a better understanding of the longevity of network interventions.","Third, we show how simulation techniques are important when building and extending theories about social phenomena such as understanding social identity dynamics using optimal distinctiveness theory."],"url":"http://arxiv.org/abs/2403.19329v1","category":"cs.SI"}
{"created":"2024-03-28 10:05:58","title":"Existence of solutions for a class of Kirchhoff-type equations with indefinite potential","abstract":"In this paper, we consider the existence of solutions of the following Kirchhoff-type problem \\[   \\left\\{   \\begin{array}   [c]{ll}   -\\left(a+b\\int_{\\mathbb{R}^3}|\\nabla u|^2dx\\right)\\Delta u+ V(x)u=f(x,u),~{\\rm{in}}~ \\mathbb{R}^{3},\\\\   u\\in H^1(\\mathbb{R}^3),   \\end{array} \\right. \\] where $a,b$ are postive constants, and the potential $V(x)$ is continuous and indefinite in sign. Under some suitable assumptions on $V(x)$ and $f$, we obtain the existence of solutions by the Symmetric Mountain Pass Theorem.","sentences":["In this paper, we consider the existence of solutions of the following Kirchhoff-type problem \\[   \\left\\{   \\begin{array}   ","[c]{ll}   -\\left(a+b\\int_{\\mathbb{R}^3}|\\nabla u|^2dx\\right)\\Delta u+ V(x)u=f(x,u),~{\\rm{in}}~ \\mathbb{R}^{3},\\\\   u\\in H^1(\\mathbb{R}^3),   \\end{array} \\right.","\\] where $a,b$ are postive constants, and the potential $V(x)$ is continuous and indefinite in sign.","Under some suitable assumptions on $V(x)$ and $f$, we obtain the existence of solutions by the Symmetric Mountain Pass Theorem."],"url":"http://arxiv.org/abs/2403.19284v1","category":"math.AP"}
{"created":"2024-03-28 09:56:51","title":"Mil2: Efficient Cloth Simulation Using Non-distance Barriers and Subspace Reuse","abstract":"Mil2 pushes the performance of high-resolution cloth simulation, making the simulation interactive (in milliseconds) for models with one million degrees of freedom (DOFs) while keeping every triangle untangled. The guarantee of being penetration-free is inspired by the interior-point method, which converts the inequality constraints to barrier potentials. Nevertheless, we propose a major overhaul of this modality by defining a novel and simple barrier formulation which does not depend on the distance between mesh primitives. Such a non-distance barrier model allows a new way to integrate collision detection into the simulation pipeline. Another contributor to the performance boost comes from the so-called subspace reuse strategy. This is based on the observation that low-frequency strain vibrations are near orthogonal to the deformation induced by collisions or self-collisions, often of high frequency. Subspace reuse then takes care of low-frequency residuals, while high-frequency residuals can also be effectively smoothed by GPU-based iterative solvers. We show that our method outperforms existing fast cloth simulators by nearly one order while keeping the entire simulation penetration-free and producing high-equality animations of high-resolution models.","sentences":["Mil2 pushes the performance of high-resolution cloth simulation, making the simulation interactive (in milliseconds) for models with one million degrees of freedom (DOFs) while keeping every triangle untangled.","The guarantee of being penetration-free is inspired by the interior-point method, which converts the inequality constraints to barrier potentials.","Nevertheless, we propose a major overhaul of this modality by defining a novel and simple barrier formulation which does not depend on the distance between mesh primitives.","Such a non-distance barrier model allows a new way to integrate collision detection into the simulation pipeline.","Another contributor to the performance boost comes from the so-called subspace reuse strategy.","This is based on the observation that low-frequency strain vibrations are near orthogonal to the deformation induced by collisions or self-collisions, often of high frequency.","Subspace reuse then takes care of low-frequency residuals, while high-frequency residuals can also be effectively smoothed by GPU-based iterative solvers.","We show that our method outperforms existing fast cloth simulators by nearly one order while keeping the entire simulation penetration-free and producing high-equality animations of high-resolution models."],"url":"http://arxiv.org/abs/2403.19272v1","category":"cs.GR"}
{"created":"2024-03-28 08:52:30","title":"Fluctuations of the additive martingales related to super-Brownian motion","abstract":"Let $(W_{t}(\\lambda))_{t\\ge 0}$, parametrized by $\\lambda\\in\\mathbb{R}$, be the additive martingale related to a supercritical super-Brownian motion on the real line and let $W_{\\infty}(\\lambda)$ be its limit. Under a natural condition for the martingale limit to be non-degenerate, we investigate the rate at which the martingale approaches its limit. Indeed, assuming certain moment conditions on the branching mechanism, we show that the tail martingale $W_{\\infty}(\\lambda)-W_{t}(\\lambda)$, properly normalized, converges in distribution to a non-degenerate random variable, and we identify the limit laws. We find that, for parameters with small absolute value, the fluctuations are affected by the behaviour of the branching mechanism $\\psi$ around $0$. In fact, we prove that, in the case of small $|\\lambda|$, when $\\psi$ is secondly differentiable at $0$, the limit laws are scale mixtures of the standard normal laws, and when $\\psi$ is `stable-like' near $0$ in some proper sense, the limit laws are scale mixtures of the stable laws. However, the effect of the branching mechanism is limited in the case of large $|\\lambda|$. In the latter case, we show that the fluctuations and limit laws are determined by the limiting extremal process of the super-Brownian motion.","sentences":["Let $(W_{t}(\\lambda))_{t\\ge 0}$, parametrized by $\\lambda\\in\\mathbb{R}$, be the additive martingale related to a supercritical super-Brownian motion on the real line and let $W_{\\infty}(\\lambda)$ be its limit.","Under a natural condition for the martingale limit to be non-degenerate, we investigate the rate at which the martingale approaches its limit.","Indeed, assuming certain moment conditions on the branching mechanism, we show that the tail martingale $W_{\\infty}(\\lambda)-W_{t}(\\lambda)$, properly normalized, converges in distribution to a non-degenerate random variable, and we identify the limit laws.","We find that, for parameters with small absolute value, the fluctuations are affected by the behaviour of the branching mechanism $\\psi$ around $0$.","In fact, we prove that, in the case of small $|\\lambda|$, when $\\psi$ is secondly differentiable at $0$, the limit laws are scale mixtures of the standard normal laws, and when $\\psi$ is `stable-like' near $0$ in some proper sense, the limit laws are scale mixtures of the stable laws.","However, the effect of the branching mechanism is limited in the case of large $|\\lambda|$. In the latter case, we show that the fluctuations and limit laws are determined by the limiting extremal process of the super-Brownian motion."],"url":"http://arxiv.org/abs/2403.19239v1","category":"math.PR"}
{"created":"2024-03-28 08:38:43","title":"Emotion Neural Transducer for Fine-Grained Speech Emotion Recognition","abstract":"The mainstream paradigm of speech emotion recognition (SER) is identifying the single emotion label of the entire utterance. This line of works neglect the emotion dynamics at fine temporal granularity and mostly fail to leverage linguistic information of speech signal explicitly. In this paper, we propose Emotion Neural Transducer for fine-grained speech emotion recognition with automatic speech recognition (ASR) joint training. We first extend typical neural transducer with emotion joint network to construct emotion lattice for fine-grained SER. Then we propose lattice max pooling on the alignment lattice to facilitate distinguishing emotional and non-emotional frames. To adapt fine-grained SER to transducer inference manner, we further make blank, the special symbol of ASR, serve as underlying emotion indicator as well, yielding Factorized Emotion Neural Transducer. For typical utterance-level SER, our ENT models outperform state-of-the-art methods on IEMOCAP in low word error rate. Experiments on IEMOCAP and the latest speech emotion diarization dataset ZED also demonstrate the superiority of fine-grained emotion modeling. Our code is available at https://github.com/ECNU-Cross-Innovation-Lab/ENT.","sentences":["The mainstream paradigm of speech emotion recognition (SER) is identifying the single emotion label of the entire utterance.","This line of works neglect the emotion dynamics at fine temporal granularity and mostly fail to leverage linguistic information of speech signal explicitly.","In this paper, we propose Emotion Neural Transducer for fine-grained speech emotion recognition with automatic speech recognition (ASR) joint training.","We first extend typical neural transducer with emotion joint network to construct emotion lattice for fine-grained SER.","Then we propose lattice max pooling on the alignment lattice to facilitate distinguishing emotional and non-emotional frames.","To adapt fine-grained SER to transducer inference manner, we further make blank, the special symbol of ASR, serve as underlying emotion indicator as well, yielding Factorized Emotion Neural Transducer.","For typical utterance-level SER, our ENT models outperform state-of-the-art methods on IEMOCAP in low word error rate.","Experiments on IEMOCAP and the latest speech emotion diarization dataset ZED also demonstrate the superiority of fine-grained emotion modeling.","Our code is available at https://github.com/ECNU-Cross-Innovation-Lab/ENT."],"url":"http://arxiv.org/abs/2403.19224v1","category":"cs.SD"}
{"created":"2024-03-28 08:35:53","title":"Forecast of CMB TB and EB correlations for AliCPT-1","abstract":"The correlations between T, E modes and B modes in cosmic microwave background (CMB) radiation, which are expected to vanish under parity symmetry, have become a sensitive probe of the new physics beyond the standard model. In this paper, we forecast the estimation of TB and EB cross power spectra using NILC and cILC on AliCPT-1 simulations. We find that, NILC performs better than cILC on TB and EB correlations in light of its lower uncertainties. In terms of the birefringence angle estimation without assuming systematic errors, the combination of CMB TB and EB spectrum from NILC cleaned simulations could reach a sensitivity of $-0.049^\\circ<\\beta<0.056^\\circ$ (95% CL).","sentences":["The correlations between T, E modes and B modes in cosmic microwave background (CMB) radiation, which are expected to vanish under parity symmetry, have become a sensitive probe of the new physics beyond the standard model.","In this paper, we forecast the estimation of TB and EB cross power spectra using NILC and cILC on AliCPT-1 simulations.","We find that, NILC performs better than cILC on TB and EB correlations in light of its lower uncertainties.","In terms of the birefringence angle estimation without assuming systematic errors, the combination of CMB TB and EB spectrum from NILC cleaned simulations could reach a sensitivity of $-0.049^\\circ<\\beta<0.056^\\circ$ (95% CL)."],"url":"http://arxiv.org/abs/2403.19222v1","category":"astro-ph.CO"}
{"created":"2024-03-28 08:32:14","title":"Collaborative Knowledge Infusion for Low-resource Stance Detection","abstract":"Stance detection is the view towards a specific target by a given context (\\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often needed to assist stance detection models in understanding the target well and making detection correctly. However, prevailing works for knowledge-infused stance detection predominantly incorporate target knowledge from a singular source that lacks knowledge verification in limited domain knowledge. The low-resource training data further increases the challenge for the data-driven large models in this task. To address those challenges, we propose a collaborative knowledge infusion approach for low-resource stance detection tasks, employing a combination of aligned knowledge enhancement and efficient parameter learning techniques. Specifically, our stance detection approach leverages target background knowledge collaboratively from different knowledge sources with the help of knowledge alignment. Additionally, we also introduce the parameter-efficient collaborative adaptor with a staged optimization algorithm, which collaboratively addresses the challenges associated with low-resource stance detection tasks from both network structure and learning perspectives. To assess the effectiveness of our method, we conduct extensive experiments on three public stance detection datasets, including low-resource and cross-target settings. The results demonstrate significant performance improvements compared to the existing stance detection approaches.","sentences":["Stance detection is the view towards a specific target by a given context (\\textit{e.g.} tweets, commercial reviews).","Target-related knowledge is often needed to assist stance detection models in understanding the target well and making detection correctly.","However, prevailing works for knowledge-infused stance detection predominantly incorporate target knowledge from a singular source that lacks knowledge verification in limited domain knowledge.","The low-resource training data further increases the challenge for the data-driven large models in this task.","To address those challenges, we propose a collaborative knowledge infusion approach for low-resource stance detection tasks, employing a combination of aligned knowledge enhancement and efficient parameter learning techniques.","Specifically, our stance detection approach leverages target background knowledge collaboratively from different knowledge sources with the help of knowledge alignment.","Additionally, we also introduce the parameter-efficient collaborative adaptor with a staged optimization algorithm, which collaboratively addresses the challenges associated with low-resource stance detection tasks from both network structure and learning perspectives.","To assess the effectiveness of our method, we conduct extensive experiments on three public stance detection datasets, including low-resource and cross-target settings.","The results demonstrate significant performance improvements compared to the existing stance detection approaches."],"url":"http://arxiv.org/abs/2403.19219v1","category":"cs.CL"}
{"created":"2024-03-28 08:18:58","title":"Note on the complete moment convergence for moving average process of a class of random variables under sub-linear expectations","abstract":"In this paper, the complete moment convergence for the partial sums of moving average processes $\\{X_n=\\sum_{i=-\\infty}^{\\infty}a_iY_{i+n},n\\ge 1\\}$ is proved under some proper conditions, where $\\{Y_i,-\\infty<i<\\infty\\}$ is a doubly sequence of identically distributed, negatively dependent random variables under sub-linear expectations and $\\{a_i,-\\infty<i<\\infty\\}$ is an absolutely summable sequence of real numbers. The results established in sub-linear expectation spaces generalize the corresponding ones in probability space.","sentences":["In this paper, the complete moment convergence for the partial sums of moving average processes $\\{X_n=\\sum_{i=-\\infty}^{\\infty}a_iY_{i+n},n\\ge 1\\}$ is proved under some proper conditions, where $\\{Y_i,-\\infty<i<\\infty\\}$ is a doubly sequence of identically distributed, negatively dependent random variables under sub-linear expectations and $\\{a_i,-\\infty<i<\\infty\\}$ is an absolutely summable sequence of real numbers.","The results established in sub-linear expectation spaces generalize the corresponding ones in probability space."],"url":"http://arxiv.org/abs/2403.19209v1","category":"math.PR"}
{"created":"2024-03-28 07:57:09","title":"Monitoring the Convergence Speed of PDHG to Find Better Primal and Dual Step Sizes","abstract":"Primal-dual algorithms for the resolution of convex-concave saddle point problems usually come with one or several step size parameters. Within the range where convergence is guaranteed, choosing well the step size can make the difference between a slow or a fast algorithm. A usual way to adaptively set step sizes is to ensure that there is a fair balance between primal and dual variable's amount of change. In this work, we show how to find even better step sizes for the primal-dual hybrid gradient. Getting inspiration from quadratic problems, we base our method on a spectral radius estimation procedure and try to minimize this spectral radius, which is directly related to the rate of convergence. Building on power iterations, we could produce spectral radius estimates that are always smaller than 1 and work also in the case of conjugate principal eigenvalues. For strongly convex quadratics, we show that our step size rule yields an algorithm as fast as inertial gradient descent. Moreover, since our spectral radius estimates only rely on residual norms, our method can be readily adapted to more general convex-concave saddle point problems. In a second part, we extend these results to a randomized version of PDHG called PURE-CD. We design a statistical test to compare observed convergence rates and decide whether a step size is better than another. Numerical experiments on least squares, sparse SVM, TV-L1 denoising and TV-L2 denoising problems support our findings.","sentences":["Primal-dual algorithms for the resolution of convex-concave saddle point problems usually come with one or several step size parameters.","Within the range where convergence is guaranteed, choosing well the step size can make the difference between a slow or a fast algorithm.","A usual way to adaptively set step sizes is to ensure that there is a fair balance between primal and dual variable's amount of change.","In this work, we show how to find even better step sizes for the primal-dual hybrid gradient.","Getting inspiration from quadratic problems, we base our method on a spectral radius estimation procedure and try to minimize this spectral radius, which is directly related to the rate of convergence.","Building on power iterations, we could produce spectral radius estimates that are always smaller than 1 and work also in the case of conjugate principal eigenvalues.","For strongly convex quadratics, we show that our step size rule yields an algorithm as fast as inertial gradient descent.","Moreover, since our spectral radius estimates only rely on residual norms, our method can be readily adapted to more general convex-concave saddle point problems.","In a second part, we extend these results to a randomized version of PDHG called PURE-CD.","We design a statistical test to compare observed convergence rates and decide whether a step size is better than another.","Numerical experiments on least squares, sparse SVM, TV-L1 denoising and TV-L2 denoising problems support our findings."],"url":"http://arxiv.org/abs/2403.19202v1","category":"math.OC"}
{"created":"2024-03-28 07:54:01","title":"Cell-Free MIMO Perceptive Mobile Networks: Cloud vs. Edge Processing","abstract":"Perceptive mobile networks implement sensing and communication by reusing existing cellular infrastructure. Cell-free multiple-input multiple-output, thanks to the cooperation among distributed access points, supports the deployment of multistatic radar sensing, while providing high spectral efficiency for data communication services. To this end, the distributed access points communicate over fronthaul links with a central processing unit acting as a cloud processor. This work explores four different types of PMN uplink solutions based on Cell-free multiple-input multiple-output, in which the sensing and decoding functionalities are carried out at either cloud or edge. Accordingly, we investigate and compare joint cloud-based decoding and sensing (CDCS), hybrid cloud-based decoding and edge-based sensing (CDES), hybrid edge-based decoding and cloud-based sensing (EDCS) and edge-based decoding and sensing (EDES). In all cases, we target a unified design problem formulation whereby the fronthaul quantization of signals received in the training and data phases are jointly designed to maximize the achievable rate under sensing requirements and fronthaul capacity constraints. Via numerical results, the four implementation scenarios are compared as a function of the available fronthaul resources by highlighting the relative merits of edge- and cloud-based sensing and communications. This study provides guidelines on the optimal functional allocation in fronthaul-constrained networks implementing integrated sensing and communications.","sentences":["Perceptive mobile networks implement sensing and communication by reusing existing cellular infrastructure.","Cell-free multiple-input multiple-output, thanks to the cooperation among distributed access points, supports the deployment of multistatic radar sensing, while providing high spectral efficiency for data communication services.","To this end, the distributed access points communicate over fronthaul links with a central processing unit acting as a cloud processor.","This work explores four different types of PMN uplink solutions based on Cell-free multiple-input multiple-output, in which the sensing and decoding functionalities are carried out at either cloud or edge.","Accordingly, we investigate and compare joint cloud-based decoding and sensing (CDCS), hybrid cloud-based decoding and edge-based sensing (CDES), hybrid edge-based decoding and cloud-based sensing (EDCS) and edge-based decoding and sensing (EDES).","In all cases, we target a unified design problem formulation whereby the fronthaul quantization of signals received in the training and data phases are jointly designed to maximize the achievable rate under sensing requirements and fronthaul capacity constraints.","Via numerical results, the four implementation scenarios are compared as a function of the available fronthaul resources by highlighting the relative merits of edge- and cloud-based sensing and communications.","This study provides guidelines on the optimal functional allocation in fronthaul-constrained networks implementing integrated sensing and communications."],"url":"http://arxiv.org/abs/2403.19200v1","category":"cs.IT"}
{"created":"2024-03-28 07:48:27","title":"What Is a Good Imputation Under MAR Missingness?","abstract":"Missing values pose a persistent challenge in modern data science. Consequently, there is an ever-growing number of publications introducing new imputation methods in various fields. The present paper attempts to take a step back and provide a more systematic analysis: Starting from an in-depth discussion of the Missing at Random (MAR) condition for nonparametric imputation, we first develop an identification result, showing that the widely used Multiple Imputation by Chained Equations (MICE) approach indeed identifies the right conditional distributions. This result, together with two illuminating examples, allows us to propose four essential properties a successful MICE imputation method should meet, thus enabling a more principled evaluation of existing methods and more targeted development of new methods. In particular, we introduce a new method that meets 3 out of the 4 criteria. We then discuss and refine ways to rank imputation methods, even in the challenging setting when the true underlying values are not available. The result is a powerful, easy-to-use scoring algorithm to rank missing value imputations under MAR missingness.","sentences":["Missing values pose a persistent challenge in modern data science.","Consequently, there is an ever-growing number of publications introducing new imputation methods in various fields.","The present paper attempts to take a step back and provide a more systematic analysis: Starting from an in-depth discussion of the Missing at Random (MAR) condition for nonparametric imputation, we first develop an identification result, showing that the widely used Multiple Imputation by Chained Equations (MICE) approach indeed identifies the right conditional distributions.","This result, together with two illuminating examples, allows us to propose four essential properties a successful MICE imputation method should meet, thus enabling a more principled evaluation of existing methods and more targeted development of new methods.","In particular, we introduce a new method that meets 3 out of the 4 criteria.","We then discuss and refine ways to rank imputation methods, even in the challenging setting when the true underlying values are not available.","The result is a powerful, easy-to-use scoring algorithm to rank missing value imputations under MAR missingness."],"url":"http://arxiv.org/abs/2403.19196v1","category":"math.ST"}
{"created":"2024-03-28 06:51:08","title":"Toward Practical Benchmarks of Ising Machines: A Case Study on the Quadratic Knapsack Problem","abstract":"Combinatorial optimization has wide applications from industry to natural science. Ising machines bring an emerging computing paradigm for efficiently solving a combinatorial optimization problem by searching a ground state of a given Ising model. Current cutting-edge Ising machines achieve fast sampling of near-optimal solutions of the max-cut problem. However, for problems with additional constraint conditions, their advantages have been hardly shown due to difficulties in handling the constraints. The performance of Ising machines on such problems heavily depends on encoding methods of constraints into penalties, but the optimal choice is non-trivial. In this work, we focus on benchmarks of Ising machines on the quadratic knapsack problem (QKP). To bring out their practical performance, we propose to exploit the problem structure upon using Ising machines. Specifically, we apply fast two-stage post-processing to the outputs of Ising machines, which makes handling the constraint easier. Simulation on medium-sized test instances shows that the proposed method substantially improves the solving performance of Ising machines and the improvement is robust to a choice of the encoding methods. We evaluate an Ising machine called Amplify Annealing Engine with the proposed method and found that it achieves comparable results with existing heuristics.","sentences":["Combinatorial optimization has wide applications from industry to natural science.","Ising machines bring an emerging computing paradigm for efficiently solving a combinatorial optimization problem by searching a ground state of a given Ising model.","Current cutting-edge Ising machines achieve fast sampling of near-optimal solutions of the max-cut problem.","However, for problems with additional constraint conditions, their advantages have been hardly shown due to difficulties in handling the constraints.","The performance of Ising machines on such problems heavily depends on encoding methods of constraints into penalties, but the optimal choice is non-trivial.","In this work, we focus on benchmarks of Ising machines on the quadratic knapsack problem (QKP).","To bring out their practical performance, we propose to exploit the problem structure upon using Ising machines.","Specifically, we apply fast two-stage post-processing to the outputs of Ising machines, which makes handling the constraint easier.","Simulation on medium-sized test instances shows that the proposed method substantially improves the solving performance of Ising machines and the improvement is robust to a choice of the encoding methods.","We evaluate an Ising machine called Amplify Annealing Engine with the proposed method and found that it achieves comparable results with existing heuristics."],"url":"http://arxiv.org/abs/2403.19175v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-28 06:24:04","title":"Evaluating Fair Feature Selection in Machine Learning for Healthcare","abstract":"With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk. We explore algorithmic fairness from the perspective of feature selection. Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups. To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups. We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error. We tested our approach on three publicly available healthcare datasets. On all three datasets, we observed improvements in fairness metrics coupled with a minimal degradation of balanced accuracy. Our approach addresses both distributive and procedural fairness within the fair machine learning context.","sentences":["With the universal adoption of machine learning in healthcare, the potential for the automation of societal biases to further exacerbate health disparities poses a significant risk.","We explore algorithmic fairness from the perspective of feature selection.","Traditional feature selection methods identify features for better decision making by removing resource-intensive, correlated, or non-relevant features but overlook how these factors may differ across subgroups.","To counter these issues, we evaluate a fair feature selection method that considers equal importance to all demographic groups.","We jointly considered a fairness metric and an error metric within the feature selection process to ensure a balance between minimizing both bias and global classification error.","We tested our approach on three publicly available healthcare datasets.","On all three datasets, we observed improvements in fairness metrics coupled with a minimal degradation of balanced accuracy.","Our approach addresses both distributive and procedural fairness within the fair machine learning context."],"url":"http://arxiv.org/abs/2403.19165v1","category":"cs.LG"}
{"created":"2024-03-28 06:07:15","title":"Improving Vietnamese-English Medical Machine Translation","abstract":"Machine translation for Vietnamese-English in the medical domain is still an under-explored research area. In this paper, we introduce MedEV -- a high-quality Vietnamese-English parallel dataset constructed specifically for the medical domain, comprising approximately 360K sentence pairs. We conduct extensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo), state-of-the-art Vietnamese-English neural machine translation models and pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV dataset. Experimental results show that the best performance is achieved by fine-tuning \"vinai-translate\" for each translation direction. We publicly release our dataset to promote further research.","sentences":["Machine translation for Vietnamese-English in the medical domain is still an under-explored research area.","In this paper, we introduce MedEV -- a high-quality Vietnamese-English parallel dataset constructed specifically for the medical domain, comprising approximately 360K sentence pairs.","We conduct extensive experiments comparing Google Translate, ChatGPT (gpt-3.5-turbo), state-of-the-art Vietnamese-English neural machine translation models and pre-trained bilingual/multilingual sequence-to-sequence models on our new MedEV dataset.","Experimental results show that the best performance is achieved by fine-tuning \"vinai-translate\" for each translation direction.","We publicly release our dataset to promote further research."],"url":"http://arxiv.org/abs/2403.19161v1","category":"cs.CL"}
{"created":"2024-03-28 06:03:47","title":"Disentangling Length from Quality in Direct Preference Optimization","abstract":"Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models. However, RLHF is know to exploit biases in human preferences, such as verbosity. A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective. A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO). Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting. Our work makes several contributions. For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstrapping. We then develop a principled but simple regularization strategy that prevents length exploitation, while still maintaining improvements in model quality. We demonstrate these effects across datasets on summarization and dialogue, where we achieve up to 20\\% improvement in win rates when controlling for length, despite the GPT4 judge's well-known verbosity bias.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has been a crucial component in the recent success of Large Language Models.","However, RLHF is know to exploit biases in human preferences, such as verbosity.","A well-formatted and eloquent answer is often more highly rated by users, even when it is less helpful and objective.","A number of approaches have been developed to control those biases in the classical RLHF literature, but the problem remains relatively under-explored for Direct Alignment Algorithms such as Direct Preference Optimization (DPO).","Unlike classical RLHF, DPO does not train a separate reward model or use reinforcement learning directly, so previous approaches developed to control verbosity cannot be directly applied to this setting.","Our work makes several contributions.","For the first time, we study the length problem in the DPO setting, showing significant exploitation in DPO and linking it to out-of-distribution bootstrapping.","We then develop a principled but simple regularization strategy that prevents length exploitation, while still maintaining improvements in model quality.","We demonstrate these effects across datasets on summarization and dialogue, where we achieve up to 20\\% improvement in win rates when controlling for length, despite the GPT4 judge's well-known verbosity bias."],"url":"http://arxiv.org/abs/2403.19159v1","category":"cs.CL"}
{"created":"2024-03-28 05:44:48","title":"Uncertainty-Aware Deep Video Compression with Ensembles","abstract":"Deep learning-based video compression is a challenging task, and many previous state-of-the-art learning-based video codecs use optical flows to exploit the temporal correlation between successive frames and then compress the residual error. Although these two-stage models are end-to-end optimized, the epistemic uncertainty in the motion estimation and the aleatoric uncertainty from the quantization operation lead to errors in the intermediate representations and introduce artifacts in the reconstructed frames. This inherent flaw limits the potential for higher bit rate savings. To address this issue, we propose an uncertainty-aware video compression model that can effectively capture the predictive uncertainty with deep ensembles. Additionally, we introduce an ensemble-aware loss to encourage the diversity among ensemble members and investigate the benefits of incorporating adversarial training in the video compression task. Experimental results on 1080p sequences show that our model can effectively save bits by more than 20% compared to DVC Pro.","sentences":["Deep learning-based video compression is a challenging task, and many previous state-of-the-art learning-based video codecs use optical flows to exploit the temporal correlation between successive frames and then compress the residual error.","Although these two-stage models are end-to-end optimized, the epistemic uncertainty in the motion estimation and the aleatoric uncertainty from the quantization operation lead to errors in the intermediate representations and introduce artifacts in the reconstructed frames.","This inherent flaw limits the potential for higher bit rate savings.","To address this issue, we propose an uncertainty-aware video compression model that can effectively capture the predictive uncertainty with deep ensembles.","Additionally, we introduce an ensemble-aware loss to encourage the diversity among ensemble members and investigate the benefits of incorporating adversarial training in the video compression task.","Experimental results on 1080p sequences show that our model can effectively save bits by more than 20% compared to DVC Pro."],"url":"http://arxiv.org/abs/2403.19158v1","category":"cs.CV"}
{"created":"2024-03-28 05:37:29","title":"Information Disturbance Tradeoff in Bidirectional QKD","abstract":"Making use of the Quantum Network formalism of \\textit{Phys. Rev. A,} \\textbf{82} (2010) 062305, we present the case for quantum networks with finite outcomes, more specifically one which could distinguish only between specific unitary operators in a given basis for operators. Despite its simplicity, we proceed to build a network derived from the optimal strategy in \\textit{Phys. Rev. A,} \\textbf{82} (2010) 062305 and show that the information-disturbance tradeoff in distinguishing between two operators acting on qubits, selected from mutually unbiased unitary bases is equal to the case of estimating an operator selected randomly from the set of SU($2$) based on the Haar measure. This suggests that such strategies in distinguishing between mutually unbiased operators is not any easier than estimating an operator derived from an infinite set. We then show how this network can be used as a natural attack strategy against a bidirectional quantum cryptographic protocol.","sentences":["Making use of the Quantum Network formalism of \\textit{Phys. Rev. A,} \\textbf{82} (2010) 062305, we present the case for quantum networks with finite outcomes, more specifically one which could distinguish only between specific unitary operators in a given basis for operators.","Despite its simplicity, we proceed to build a network derived from the optimal strategy in \\textit{Phys. Rev. A,} \\textbf{82} (2010) 062305 and show that the information-disturbance tradeoff in distinguishing between two operators acting on qubits, selected from mutually unbiased unitary bases is equal to the case of estimating an operator selected randomly from the set of SU($2$) based on the Haar measure.","This suggests that such strategies in distinguishing between mutually unbiased operators is not any easier than estimating an operator derived from an infinite set.","We then show how this network can be used as a natural attack strategy against a bidirectional quantum cryptographic protocol."],"url":"http://arxiv.org/abs/2403.19156v1","category":"quant-ph"}
{"created":"2024-03-28 05:35:24","title":"First Landau Level Physics in Second Moir\u00e9 Band of $2.1^\\circ$ Twisted Bilayer MoTe${}_2$","abstract":"The recent experimental discovery of the fractional quantum spin Hall effect in twisted bilayer MoTe$_2$ at a twist angle of 2.1 degrees highlights the unique properties of its second moir\\'e band. Inspired by this finding, we conduct a comprehensive theoretical investigation of the half-filled second moir\\'e band at this angle, utilizing an effective continuum model. Our analysis reveals that the band not only exhibits characteristics akin to the first Landau level, $K=\\int_\\mathrm{BZ}\\mathrm{d}^2\\mathbf{k}\\:\\mathrm{tr}\\:\\eta(\\mathbf{k}) \\approx 3$, but also that its projected Coulomb interaction strikingly mirrors the Haldane pseudopotentials of the first Landau level. They together strongly indicate the potential emergence of a non-Abelian fractional quantum anomalous Hall state at the half-filling under the Coulomb interactions. By performing exact diagonalization calculations, we validate this hypothesis and construct a global phase diagram around this non-Abelian fractional quantum anomalous Hall states. We also introduce a novel metric of 1LL-ness of a band, which quantitatively measures the alignment of the projected Coulomb interaction with the Haldane pseudopotentials in Landau levels. This metric is then compared with the global phase diagram including the non-Abelian fractional quantum anomalous Hall state, revealing its utility in predicting the parameter region of the 1LL-ness. Finally, we discuss the potential implications on experiments.","sentences":["The recent experimental discovery of the fractional quantum spin Hall effect in twisted bilayer MoTe$_2$ at a twist angle of 2.1 degrees highlights the unique properties of its second moir\\'e band.","Inspired by this finding, we conduct a comprehensive theoretical investigation of the half-filled second moir\\'e band at this angle, utilizing an effective continuum model.","Our analysis reveals that the band not only exhibits characteristics akin to the first Landau level, $K=\\int_\\mathrm{BZ}\\mathrm{d}^2\\mathbf{k}\\:\\mathrm{tr}\\:\\eta(\\mathbf{k})","\\approx 3$, but also that its projected Coulomb interaction strikingly mirrors the Haldane pseudopotentials of the first Landau level.","They together strongly indicate the potential emergence of a non-Abelian fractional quantum anomalous Hall state at the half-filling under the Coulomb interactions.","By performing exact diagonalization calculations, we validate this hypothesis and construct a global phase diagram around this non-Abelian fractional quantum anomalous Hall states.","We also introduce a novel metric of 1LL-ness of a band, which quantitatively measures the alignment of the projected Coulomb interaction with the Haldane pseudopotentials in Landau levels.","This metric is then compared with the global phase diagram including the non-Abelian fractional quantum anomalous Hall state, revealing its utility in predicting the parameter region of the 1LL-ness.","Finally, we discuss the potential implications on experiments."],"url":"http://arxiv.org/abs/2403.19155v1","category":"cond-mat.str-el"}
{"created":"2024-03-28 04:15:58","title":"CLAP4CLIP: Continual Learning with Probabilistic Finetuning for Vision-Language Models","abstract":"Continual learning (CL) aims to help deep neural networks to learn new knowledge while retaining what has been learned. Recently, pre-trained vision-language models such as CLIP, with powerful generalization ability, have been gaining traction as practical CL candidates. However, the domain mismatch between the pre-training and the downstream CL tasks calls for finetuning of the CLIP on the latter. The deterministic nature of the existing finetuning methods makes them overlook the many possible interactions across the modalities and deems them unsafe for high-risk CL tasks requiring reliable uncertainty estimation. To address these, our work proposes Continual LeArning with Probabilistic finetuning (CLAP). CLAP develops probabilistic modeling over task-specific modules with visual-guided text features, providing more reliable fine-tuning in CL. It further alleviates forgetting by exploiting the rich pre-trained knowledge of CLIP for weight initialization and distribution regularization of task-specific modules. Cooperating with the diverse range of existing prompting methods, CLAP can surpass the predominant deterministic finetuning approaches for CL with CLIP. Lastly, we study the superior uncertainty estimation abilities of CLAP for novel data detection and exemplar selection within CL setups. Our code is available at \\url{https://github.com/srvCodes/clap4clip}.","sentences":["Continual learning (CL) aims to help deep neural networks to learn new knowledge while retaining what has been learned.","Recently, pre-trained vision-language models such as CLIP, with powerful generalization ability, have been gaining traction as practical CL candidates.","However, the domain mismatch between the pre-training and the downstream CL tasks calls for finetuning of the CLIP on the latter.","The deterministic nature of the existing finetuning methods makes them overlook the many possible interactions across the modalities and deems them unsafe for high-risk CL tasks requiring reliable uncertainty estimation.","To address these, our work proposes Continual LeArning with Probabilistic finetuning (CLAP).","CLAP develops probabilistic modeling over task-specific modules with visual-guided text features, providing more reliable fine-tuning in CL.","It further alleviates forgetting by exploiting the rich pre-trained knowledge of CLIP for weight initialization and distribution regularization of task-specific modules.","Cooperating with the diverse range of existing prompting methods, CLAP can surpass the predominant deterministic finetuning approaches for CL with CLIP.","Lastly, we study the superior uncertainty estimation abilities of CLAP for novel data detection and exemplar selection within CL setups.","Our code is available at \\url{https://github.com/srvCodes/clap4clip}."],"url":"http://arxiv.org/abs/2403.19137v1","category":"cs.CV"}
{"created":"2024-03-28 03:55:04","title":"Stable Object Placing using Curl and Diff Features of Vision-based Tactile Sensors","abstract":"Ensuring stable object placement is crucial to prevent objects from toppling over, breaking, or causing spills. When an object makes initial contact to a surface, and some force is exerted, the moment of rotation caused by the instability of the object's placing can cause the object to rotate in a certain direction (henceforth referred to as direction of corrective rotation). Existing methods often employ a Force/Torque (F/T) sensor to estimate the direction of corrective rotation by detecting the moment of rotation as a torque. However, its effectiveness may be hampered by sensor noise and the tension of the external wiring of robot cables. To address these issues, we propose a method for stable object placing using GelSights, vision-based tactile sensors, as an alternative to F/T sensors. Our method estimates the direction of corrective rotation of objects using the displacement of the black dot pattern on the elastomeric surface of GelSight. We calculate the Curl from vector analysis, indicative of the rotational field magnitude and direction of the displacement of the black dots pattern. Simultaneously, we calculate the difference (Diff) of displacement between the left and right fingers' GelSight's black dots. Then, the robot can manipulate the objects' pose using Curl and Diff features, facilitating stable placing. Across experiments, handling 18 differently characterized objects, our method achieves precise placing accuracy (less than 1-degree error) in nearly 100% of cases. An accompanying video is available at the following link: https://youtu.be/fQbmCksVHlU","sentences":["Ensuring stable object placement is crucial to prevent objects from toppling over, breaking, or causing spills.","When an object makes initial contact to a surface, and some force is exerted, the moment of rotation caused by the instability of the object's placing can cause the object to rotate in a certain direction (henceforth referred to as direction of corrective rotation).","Existing methods often employ a Force/Torque (F/T) sensor to estimate the direction of corrective rotation by detecting the moment of rotation as a torque.","However, its effectiveness may be hampered by sensor noise and the tension of the external wiring of robot cables.","To address these issues, we propose a method for stable object placing using GelSights, vision-based tactile sensors, as an alternative to F/T sensors.","Our method estimates the direction of corrective rotation of objects using the displacement of the black dot pattern on the elastomeric surface of GelSight.","We calculate the Curl from vector analysis, indicative of the rotational field magnitude and direction of the displacement of the black dots pattern.","Simultaneously, we calculate the difference (Diff) of displacement between the left and right fingers' GelSight's black dots.","Then, the robot can manipulate the objects' pose using Curl and Diff features, facilitating stable placing.","Across experiments, handling 18 differently characterized objects, our method achieves precise placing accuracy (less than 1-degree error) in nearly 100% of cases.","An accompanying video is available at the following link: https://youtu.be/fQbmCksVHlU"],"url":"http://arxiv.org/abs/2403.19129v1","category":"cs.RO"}
{"created":"2024-03-28 03:07:23","title":"Uncover the Premeditated Attacks: Detecting Exploitable Reentrancy Vulnerabilities by Identifying Attacker Contracts","abstract":"Reentrancy, a notorious vulnerability in smart contracts, has led to millions of dollars in financial loss. However, current smart contract vulnerability detection tools suffer from a high false positive rate in identifying contracts with reentrancy vulnerabilities. Moreover, only a small portion of the detected reentrant contracts can actually be exploited by hackers, making these tools less effective in securing the Ethereum ecosystem in practice.   In this paper, we propose BlockWatchdog, a tool that focuses on detecting reentrancy vulnerabilities by identifying attacker contracts. These attacker contracts are deployed by hackers to exploit vulnerable contracts automatically. By focusing on attacker contracts, BlockWatchdog effectively detects truly exploitable reentrancy vulnerabilities by identifying reentrant call flow. Additionally, BlockWatchdog is capable of detecting new types of reentrancy vulnerabilities caused by poor designs when using ERC tokens or user-defined interfaces, which cannot be detected by current rule-based tools. We implement BlockWatchdog using cross-contract static dataflow techniques based on attack logic obtained from an empirical study that analyzes attacker contracts from 281 attack incidents. BlockWatchdog is evaluated on 421,889 Ethereum contract bytecodes and identifies 113 attacker contracts that target 159 victim contracts, leading to the theft of Ether and tokens valued at approximately 908.6 million USD. Notably, only 18 of the identified 159 victim contracts can be reported by current reentrancy detection tools.","sentences":["Reentrancy, a notorious vulnerability in smart contracts, has led to millions of dollars in financial loss.","However, current smart contract vulnerability detection tools suffer from a high false positive rate in identifying contracts with reentrancy vulnerabilities.","Moreover, only a small portion of the detected reentrant contracts can actually be exploited by hackers, making these tools less effective in securing the Ethereum ecosystem in practice.   ","In this paper, we propose BlockWatchdog, a tool that focuses on detecting reentrancy vulnerabilities by identifying attacker contracts.","These attacker contracts are deployed by hackers to exploit vulnerable contracts automatically.","By focusing on attacker contracts, BlockWatchdog effectively detects truly exploitable reentrancy vulnerabilities by identifying reentrant call flow.","Additionally, BlockWatchdog is capable of detecting new types of reentrancy vulnerabilities caused by poor designs when using ERC tokens or user-defined interfaces, which cannot be detected by current rule-based tools.","We implement BlockWatchdog using cross-contract static dataflow techniques based on attack logic obtained from an empirical study that analyzes attacker contracts from 281 attack incidents.","BlockWatchdog is evaluated on 421,889 Ethereum contract bytecodes and identifies 113 attacker contracts that target 159 victim contracts, leading to the theft of Ether and tokens valued at approximately 908.6 million USD.","Notably, only 18 of the identified 159 victim contracts can be reported by current reentrancy detection tools."],"url":"http://arxiv.org/abs/2403.19112v1","category":"cs.CR"}
{"created":"2024-03-28 02:29:24","title":"Quantifying the uncertainty in the time-redshift relationship","abstract":"The age of the Universe at a given redshift is a fundamental relationship in cosmology. For many years, the uncertainties in it were dauntingly large, close to a factor of 2. In this age of precision cosmology, they are now at the percent level and dominated solely by the Hubble constant. The uncertainties due to the parameters that describe the cosmological model are must less important. In decreasing order they are: uncertainty due to the dark energy equation-of-state parameter $w$, at most 0.9%; uncertainty due to the matter density $\\Omega_M$, at most 0.5% and uncertainty due to the curvature parameter $\\Omega_k$, at most 0.07%.","sentences":["The age of the Universe at a given redshift is a fundamental relationship in cosmology.","For many years, the uncertainties in it were dauntingly large, close to a factor of 2.","In this age of precision cosmology, they are now at the percent level and dominated solely by the Hubble constant.","The uncertainties due to the parameters that describe the cosmological model are must less important.","In decreasing order they are: uncertainty due to the dark energy equation-of-state parameter $w$, at most 0.9%; uncertainty due to the matter density $\\Omega_M$, at most 0.5% and uncertainty due to the curvature parameter $\\Omega_k$, at most 0.07%."],"url":"http://arxiv.org/abs/2403.19100v1","category":"astro-ph.CO"}
{"created":"2024-03-28 02:12:49","title":"Learning From Correctness Without Prompting Makes LLM Efficient Reasoner","abstract":"Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content. One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools). In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts. The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors. This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits. Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning performance with reduced token consumption.","sentences":["Large language models (LLMs) have demonstrated outstanding performance across various tasks, yet they still exhibit limitations such as hallucination, unfaithful reasoning, and toxic content.","One potential approach to mitigate these issues is learning from human or external feedback (e.g. tools).","In this paper, we introduce an intrinsic self-correct reasoning framework for LLMs that eliminates the need for human feedback, external tools, and handcraft prompts.","The proposed framework, based on a multi-step reasoning paradigm \\textbf{Le}arning from \\textbf{Co}rrectness (\\textsc{LeCo}), improves reasoning performance without needing to learn from errors.","This paradigm prioritizes learning from correct reasoning steps, and a unique method to measure confidence for each reasoning step based on generation logits.","Experimental results across various multi-step reasoning tasks demonstrate the effectiveness of the framework in improving reasoning performance with reduced token consumption."],"url":"http://arxiv.org/abs/2403.19094v1","category":"cs.CL"}
{"created":"2024-03-28 01:34:12","title":"MaterialsMap: A CALPHAD-Based Tool to Design Composition Pathways through feasibility map for Desired Dissimilar Materials, demonstrated with RSW Joining of Ag-Al-Cu","abstract":"Assembly of dissimilar metals can be achieved by different methods, for example, casting, welding, and additive manufacturing (AM). However, undesired phases formed in liquid-phase assembling processes due to solute segregation during solidification diminish mechanical and other properties of the processed parts. In the present work, an open-source software named MaterialsMap, has been developed based on the CALculation of Phase Diagrams (CALPHAD) approach. The primary objective of MaterialsMap is to facilitate the design of an optimal composition pathway for assembling dissimilar alloys with liquid-phases based on the formation of desired and undesired phases along the pathway. In MaterialsMap, equilibrium thermodynamic calculations are used to predict equilibrium phases formed at slow cooling rate, while Scheil-Gulliver simulations are employed to predict non-equilibrium phases formed during rapid cooling. By combining these two simulations, MaterialsMap offers a thorough guide for understanding phase formation in various manufacturing processes, assisting users in making informed decisions during material selection and production. As a demonstration of this approach, a compositional pathway was designed from pure Al to pure Cu through Ag using MaterialsMap. The design was experimentally verified using resistance spot welding (RSW).","sentences":["Assembly of dissimilar metals can be achieved by different methods, for example, casting, welding, and additive manufacturing (AM).","However, undesired phases formed in liquid-phase assembling processes due to solute segregation during solidification diminish mechanical and other properties of the processed parts.","In the present work, an open-source software named MaterialsMap, has been developed based on the CALculation of Phase Diagrams (CALPHAD) approach.","The primary objective of MaterialsMap is to facilitate the design of an optimal composition pathway for assembling dissimilar alloys with liquid-phases based on the formation of desired and undesired phases along the pathway.","In MaterialsMap, equilibrium thermodynamic calculations are used to predict equilibrium phases formed at slow cooling rate, while Scheil-Gulliver simulations are employed to predict non-equilibrium phases formed during rapid cooling.","By combining these two simulations, MaterialsMap offers a thorough guide for understanding phase formation in various manufacturing processes, assisting users in making informed decisions during material selection and production.","As a demonstration of this approach, a compositional pathway was designed from pure Al to pure Cu through Ag using MaterialsMap.","The design was experimentally verified using resistance spot welding (RSW)."],"url":"http://arxiv.org/abs/2403.19084v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-28 01:05:06","title":"MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models","abstract":"Different from a unimodal model whose input is from a single modality, the input (called multi-modal input) of a multi-modal model is from multiple modalities such as image, 3D points, audio, text, etc. Similar to unimodal models, many existing studies show that a multi-modal model is also vulnerable to adversarial perturbation, where an attacker could add small perturbation to all modalities of a multi-modal input such that the multi-modal model makes incorrect predictions for it. Existing certified defenses are mostly designed for unimodal models, which achieve sub-optimal certified robustness guarantees when extended to multi-modal models as shown in our experimental results. In our work, we propose MMCert, the first certified defense against adversarial attacks to a multi-modal model. We derive a lower bound on the performance of our MMCert under arbitrary adversarial attacks with bounded perturbations to both modalities (e.g., in the context of auto-driving, we bound the number of changed pixels in both RGB image and depth image). We evaluate our MMCert using two benchmark datasets: one for the multi-modal road segmentation task and the other for the multi-modal emotion recognition task. Moreover, we compare our MMCert with a state-of-the-art certified defense extended from unimodal models. Our experimental results show that our MMCert outperforms the baseline.","sentences":["Different from a unimodal model whose input is from a single modality, the input (called multi-modal input) of a multi-modal model is from multiple modalities such as image, 3D points, audio, text, etc. Similar to unimodal models, many existing studies show that a multi-modal model is also vulnerable to adversarial perturbation, where an attacker could add small perturbation to all modalities of a multi-modal input such that the multi-modal model makes incorrect predictions for it.","Existing certified defenses are mostly designed for unimodal models, which achieve sub-optimal certified robustness guarantees when extended to multi-modal models as shown in our experimental results.","In our work, we propose MMCert, the first certified defense against adversarial attacks to a multi-modal model.","We derive a lower bound on the performance of our MMCert under arbitrary adversarial attacks with bounded perturbations to both modalities (e.g., in the context of auto-driving, we bound the number of changed pixels in both RGB image and depth image).","We evaluate our MMCert using two benchmark datasets: one for the multi-modal road segmentation task and the other for the multi-modal emotion recognition task.","Moreover, we compare our MMCert with a state-of-the-art certified defense extended from unimodal models.","Our experimental results show that our MMCert outperforms the baseline."],"url":"http://arxiv.org/abs/2403.19080v1","category":"cs.CV"}
{"created":"2024-03-27 23:59:23","title":"GENESIS-RL: GEnerating Natural Edge-cases with Systematic Integration of Safety considerations and Reinforcement Learning","abstract":"In the rapidly evolving field of autonomous systems, the safety and reliability of the system components are fundamental requirements. These components are often vulnerable to complex and unforeseen environments, making natural edge-case generation essential for enhancing system resilience. This paper presents GENESIS-RL, a novel framework that leverages system-level safety considerations and reinforcement learning techniques to systematically generate naturalistic edge cases. By simulating challenging conditions that mimic the real-world situations, our framework aims to rigorously test entire system's safety and reliability. Although demonstrated within the autonomous driving application, our methodology is adaptable across diverse autonomous systems. Our experimental validation, conducted on high-fidelity simulator underscores the overall effectiveness of this framework.","sentences":["In the rapidly evolving field of autonomous systems, the safety and reliability of the system components are fundamental requirements.","These components are often vulnerable to complex and unforeseen environments, making natural edge-case generation essential for enhancing system resilience.","This paper presents GENESIS-RL, a novel framework that leverages system-level safety considerations and reinforcement learning techniques to systematically generate naturalistic edge cases.","By simulating challenging conditions that mimic the real-world situations, our framework aims to rigorously test entire system's safety and reliability.","Although demonstrated within the autonomous driving application, our methodology is adaptable across diverse autonomous systems.","Our experimental validation, conducted on high-fidelity simulator underscores the overall effectiveness of this framework."],"url":"http://arxiv.org/abs/2403.19062v1","category":"eess.SY"}
{"created":"2024-03-27 23:49:22","title":"Equity in Healthcare: Analyzing Disparities in Machine Learning Predictions of Diabetic Patient Readmissions","abstract":"This study investigates how machine learning (ML) models can predict hospital readmissions for diabetic patients fairly and accurately across different demographics (age, gender, race). We compared models like Deep Learning, Generalized Linear Models, Gradient Boosting Machines (GBM), and Naive Bayes. GBM stood out with an F1-score of 84.3% and accuracy of 82.2%, accurately predicting readmissions across demographics. A fairness analysis was conducted across all the models. GBM minimized disparities in predictions, achieving balanced results across genders and races. It showed low False Discovery Rates (FDR) (6-7%) and False Positive Rates (FPR) (5%) for both genders. Additionally, FDRs remained low for racial groups, such as African Americans (8%) and Asians (7%). Similarly, FPRs were consistent across age groups (4%) for both patients under 40 and those above 40, indicating its precision and ability to reduce bias. These findings emphasize the importance of choosing ML models carefully to ensure both accuracy and fairness for all patients. By showcasing effectiveness of various models with fairness metrics, this study promotes personalized medicine and the need for fair ML algorithms in healthcare. This can ultimately reduce disparities and improve outcomes for diabetic patients of all backgrounds.","sentences":["This study investigates how machine learning (ML) models can predict hospital readmissions for diabetic patients fairly and accurately across different demographics (age, gender, race).","We compared models like Deep Learning, Generalized Linear Models, Gradient Boosting Machines (GBM), and Naive Bayes.","GBM stood out with an F1-score of 84.3% and accuracy of 82.2%, accurately predicting readmissions across demographics.","A fairness analysis was conducted across all the models.","GBM minimized disparities in predictions, achieving balanced results across genders and races.","It showed low False Discovery Rates (FDR) (6-7%) and False Positive Rates (FPR) (5%) for both genders.","Additionally, FDRs remained low for racial groups, such as African Americans (8%) and Asians (7%).","Similarly, FPRs were consistent across age groups (4%) for both patients under 40 and those above 40, indicating its precision and ability to reduce bias.","These findings emphasize the importance of choosing ML models carefully to ensure both accuracy and fairness for all patients.","By showcasing effectiveness of various models with fairness metrics, this study promotes personalized medicine and the need for fair ML algorithms in healthcare.","This can ultimately reduce disparities and improve outcomes for diabetic patients of all backgrounds."],"url":"http://arxiv.org/abs/2403.19057v1","category":"cs.LG"}
{"created":"2024-03-27 22:26:50","title":"Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE","abstract":"Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows connecting two adjacent time series points to point in the same direction, while the Edge Length Loss (ELL) penalizes arrows - which effectively represent time gaps in the visualized embedding - based on their length. Both loss terms are differentiable and can be easily incorporated into existing dimensionality reduction techniques. By promoting local directionality of the directed edges, our procedure produces more temporally meaningful and less cluttered visualizations. We demonstrate the effectiveness of our approach on a toy dataset and two real-world datasets.","sentences":["Many real-world data sets contain a temporal component or involve transitions from state to state.","For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges.","Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns.","To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed.","The Directional Coherence Loss (DCL) encourages nearby arrows connecting two adjacent time series points to point in the same direction, while the Edge Length Loss (ELL) penalizes arrows - which effectively represent time gaps in the visualized embedding - based on their length.","Both loss terms are differentiable and can be easily incorporated into existing dimensionality reduction techniques.","By promoting local directionality of the directed edges, our procedure produces more temporally meaningful and less cluttered visualizations.","We demonstrate the effectiveness of our approach on a toy dataset and two real-world datasets."],"url":"http://arxiv.org/abs/2403.19040v1","category":"cs.LG"}
{"created":"2024-03-27 22:15:53","title":"MiMiC: A High-Performance Framework for Multiscale Molecular Dynamics Simulations","abstract":"MiMiC is a framework for performing multiscale simulations, where individual subsystems are handled at different resolutions and/or levels of theory by loosely coupled external programs. To make it highly efficient and flexible, we adopt an interoperable approach based on a multiple-program multiple-data paradigm, serving as an intermediary responsible for fast data exchange and interactions between the subsystems. The main goal of MiMiC is to avoid interfering with the underlying parallelization of the external programs, including the operability on hybrid architectures (e.g., CPU/GPU), and keep their setup and execution as close as possible to the original. At the moment, MiMiC offers an efficient implementation of electrostatic embedding QM/MM that has demonstrated unprecedented parallel scaling in simulations of large biomolecules using CPMD and GROMACS as QM and MM engines, respectively. However, as it is designed for high flexibility with general multiscale models in mind, it can be straightforwardly extended beyond QM/MM. In this article, we illustrate the software design and the features of the framework, which make it a compelling choice for multiscale simulations in the upcoming era of exascale high-performance computing.","sentences":["MiMiC is a framework for performing multiscale simulations, where individual subsystems are handled at different resolutions and/or levels of theory by loosely coupled external programs.","To make it highly efficient and flexible, we adopt an interoperable approach based on a multiple-program multiple-data paradigm, serving as an intermediary responsible for fast data exchange and interactions between the subsystems.","The main goal of MiMiC is to avoid interfering with the underlying parallelization of the external programs, including the operability on hybrid architectures (e.g., CPU/GPU), and keep their setup and execution as close as possible to the original.","At the moment, MiMiC offers an efficient implementation of electrostatic embedding QM/MM that has demonstrated unprecedented parallel scaling in simulations of large biomolecules using CPMD and GROMACS as QM and MM engines, respectively.","However, as it is designed for high flexibility with general multiscale models in mind, it can be straightforwardly extended beyond QM/MM.","In this article, we illustrate the software design and the features of the framework, which make it a compelling choice for multiscale simulations in the upcoming era of exascale high-performance computing."],"url":"http://arxiv.org/abs/2403.19035v1","category":"physics.chem-ph"}
{"created":"2024-03-27 22:10:27","title":"Locational Marginal Prices Obey DC Circuit Laws","abstract":"Electricity markets often utilize the DC approximation of the AC power flow equations to facilitate solving an otherwise complex nonconvex optimization problem. These DC power flow equations have analogies to DC circuit laws such as Kirchhoff's Laws, resulting in an intuitive understanding of power flows under this model. Variables derived from the Lagrangian dual of the DC optimal power flow problem, such as locational marginal prices (LMPs) and congestion cost, are less intuitive without an understanding of optimization theory. Even with this understanding, LMP behavior, such as the conditions in which negative prices occur or the impact of individual congested lines on network-wide LMPs, remain somewhat mysterious. In this paper, we show that prices also obey DC circuit laws, which can help facilitate an intuitive understanding of their behavior and relationships throughout a network without explicitly understanding duality. In particular, prices can be modeled as voltages, and their differences can be modeled as flows, allowing for a physical interpretation of prices. This analogy also lends itself to the use of well-understood DC circuit concepts such as superposition and Kirchhoff's Laws, which can further facilitate a clearer understanding of price behavior.","sentences":["Electricity markets often utilize the DC approximation of the AC power flow equations to facilitate solving an otherwise complex nonconvex optimization problem.","These DC power flow equations have analogies to DC circuit laws such as Kirchhoff's Laws, resulting in an intuitive understanding of power flows under this model.","Variables derived from the Lagrangian dual of the DC optimal power flow problem, such as locational marginal prices (LMPs) and congestion cost, are less intuitive without an understanding of optimization theory.","Even with this understanding, LMP behavior, such as the conditions in which negative prices occur or the impact of individual congested lines on network-wide LMPs, remain somewhat mysterious.","In this paper, we show that prices also obey DC circuit laws, which can help facilitate an intuitive understanding of their behavior and relationships throughout a network without explicitly understanding duality.","In particular, prices can be modeled as voltages, and their differences can be modeled as flows, allowing for a physical interpretation of prices.","This analogy also lends itself to the use of well-understood DC circuit concepts such as superposition and Kirchhoff's Laws, which can further facilitate a clearer understanding of price behavior."],"url":"http://arxiv.org/abs/2403.19032v1","category":"math.OC"}
{"created":"2024-03-27 21:58:19","title":"Symmetry criteria for the equality of interior and exterior shape factors","abstract":"Lienhard (2019) reported that the shape factor of the interior of a simply-connected region ($\\Omega$) is equal to that of its exterior ($\\mathbb{R}^2\\backslash\\Omega$) under the same boundary conditions. In that study, numerical examples supported the claim in particular cases; for example, it was shown that for certain boundary conditions on circles and squares, the conjecture holds. In the present paper, we show that the conjecture is not generally true, unless some additional condition is met. We proceed by elucidating why the conjecture does in fact hold in all of the examples analysed by Lienhard. We thus deduce a simple criterion which, when satisfied, ensures the equality of interior and exterior shape factors in general. Our criterion notably relies on a beautiful and little-known symmetry method due to Hersch (1982) which we introduce in a tutorial manner.","sentences":["Lienhard (2019) reported that the shape factor of the interior of a simply-connected region ($\\Omega$) is equal to that of its exterior ($\\mathbb{R}^2\\backslash\\Omega$) under the same boundary conditions.","In that study, numerical examples supported the claim in particular cases; for example, it was shown that for certain boundary conditions on circles and squares, the conjecture holds.","In the present paper, we show that the conjecture is not generally true, unless some additional condition is met.","We proceed by elucidating why the conjecture does in fact hold in all of the examples analysed by Lienhard.","We thus deduce a simple criterion which, when satisfied, ensures the equality of interior and exterior shape factors in general.","Our criterion notably relies on a beautiful and little-known symmetry method due to Hersch (1982) which we introduce in a tutorial manner."],"url":"http://arxiv.org/abs/2403.19030v1","category":"physics.class-ph"}
{"created":"2024-03-27 21:57:18","title":"Nonlinear Model Predictive Control for Enhanced Navigation of Autonomous Surface Vessels","abstract":"This article proposes an approach for collision avoidance, path following, and anti-grounding of autonomous surface vessels under consideration of environmental forces based on Nonlinear Model Predictive Control (NMPC). Artificial Potential Fields (APFs) set the foundation for the cost function of the optimal control problem in terms of collision avoidance and anti-grounding. Depending on the risk of a collision given by the resulting force of the APFs, the controller optimizes regarding an adapted heading and travel speed by additionally following a desired path. For this purpose, nonlinear vessel dynamics are used for the NMPC. To extend the situational awareness concerning environmental disturbances impacted by wind, waves, and sea currents, a nonlinear disturbance observer is coupled to the entire NMPC scheme, allowing for the correction of an incorrect vessel motion due to external forces. In addition, the most essential rules according to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) are considered. The results of the simulations show that the proposed framework can control an autonomous surface vessel under various challenging scenarios, including environmental disturbances, to avoid collisions and follow desired paths.","sentences":["This article proposes an approach for collision avoidance, path following, and anti-grounding of autonomous surface vessels under consideration of environmental forces based on Nonlinear Model Predictive Control (NMPC).","Artificial Potential Fields (APFs) set the foundation for the cost function of the optimal control problem in terms of collision avoidance and anti-grounding.","Depending on the risk of a collision given by the resulting force of the APFs, the controller optimizes regarding an adapted heading and travel speed by additionally following a desired path.","For this purpose, nonlinear vessel dynamics are used for the NMPC.","To extend the situational awareness concerning environmental disturbances impacted by wind, waves, and sea currents, a nonlinear disturbance observer is coupled to the entire NMPC scheme, allowing for the correction of an incorrect vessel motion due to external forces.","In addition, the most essential rules according to the Convention on the International Regulations for Preventing Collisions at Sea (COLREGs) are considered.","The results of the simulations show that the proposed framework can control an autonomous surface vessel under various challenging scenarios, including environmental disturbances, to avoid collisions and follow desired paths."],"url":"http://arxiv.org/abs/2403.19028v1","category":"eess.SY"}
{"created":"2024-03-27 21:24:20","title":"WALT3D: Generating Realistic Training Data from Time-Lapse Imagery for Reconstructing Dynamic Objects under Occlusion","abstract":"Current methods for 2D and 3D object understanding struggle with severe occlusions in busy urban environments, partly due to the lack of large-scale labeled ground-truth annotations for learning occlusion. In this work, we introduce a novel framework for automatically generating a large, realistic dataset of dynamic objects under occlusions using freely available time-lapse imagery. By leveraging off-the-shelf 2D (bounding box, segmentation, keypoint) and 3D (pose, shape) predictions as pseudo-groundtruth, unoccluded 3D objects are identified automatically and composited into the background in a clip-art style, ensuring realistic appearances and physically accurate occlusion configurations. The resulting clip-art image with pseudo-groundtruth enables efficient training of object reconstruction methods that are robust to occlusions. Our method demonstrates significant improvements in both 2D and 3D reconstruction, particularly in scenarios with heavily occluded objects like vehicles and people in urban scenes.","sentences":["Current methods for 2D and 3D object understanding struggle with severe occlusions in busy urban environments, partly due to the lack of large-scale labeled ground-truth annotations for learning occlusion.","In this work, we introduce a novel framework for automatically generating a large, realistic dataset of dynamic objects under occlusions using freely available time-lapse imagery.","By leveraging off-the-shelf 2D (bounding box, segmentation, keypoint) and 3D (pose, shape) predictions as pseudo-groundtruth, unoccluded 3D objects are identified automatically and composited into the background in a clip-art style, ensuring realistic appearances and physically accurate occlusion configurations.","The resulting clip-art image with pseudo-groundtruth enables efficient training of object reconstruction methods that are robust to occlusions.","Our method demonstrates significant improvements in both 2D and 3D reconstruction, particularly in scenarios with heavily occluded objects like vehicles and people in urban scenes."],"url":"http://arxiv.org/abs/2403.19022v1","category":"cs.CV"}
{"created":"2024-03-27 21:14:17","title":"Thelxino\u00eb: Recognizing Human Emotions Using Pupillometry and Machine Learning","abstract":"In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxino\\\"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for future advancements in virtual touch technology.","sentences":["In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry.","We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VR generated data.","Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR).","By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it.","This research contributes significantly to the Thelxino\\\"e framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions.","Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for future advancements in virtual touch technology."],"url":"http://arxiv.org/abs/2403.19014v1","category":"cs.LG"}
{"created":"2024-03-27 21:06:26","title":"Sequential Inference of Hospitalization ElectronicHealth Records Using Probabilistic Models","abstract":"In the dynamic hospital setting, decision support can be a valuable tool for improving patient outcomes. Data-driven inference of future outcomes is challenging in this dynamic setting, where long sequences such as laboratory tests and medications are updated frequently. This is due in part to heterogeneity of data types and mixed-sequence types contained in variable length sequences. In this work we design a probabilistic unsupervised model for multiple arbitrary-length sequences contained in hospitalization Electronic Health Record (EHR) data. The model uses a latent variable structure and captures complex relationships between medications, diagnoses, laboratory tests, neurological assessments, and medications. It can be trained on original data, without requiring any lossy transformations or time binning. Inference algorithms are derived that use partial data to infer properties of the complete sequences, including their length and presence of specific values. We train this model on data from subjects receiving medical care in the Kaiser Permanente Northern California integrated healthcare delivery system. The results are evaluated against held-out data for predicting the length of sequences and presence of Intensive Care Unit (ICU) in hospitalization bed sequences. Our method outperforms a baseline approach, showing that in these experiments the trained model captures information in the sequences that is informative of their future values.","sentences":["In the dynamic hospital setting, decision support can be a valuable tool for improving patient outcomes.","Data-driven inference of future outcomes is challenging in this dynamic setting, where long sequences such as laboratory tests and medications are updated frequently.","This is due in part to heterogeneity of data types and mixed-sequence types contained in variable length sequences.","In this work we design a probabilistic unsupervised model for multiple arbitrary-length sequences contained in hospitalization Electronic Health Record (EHR) data.","The model uses a latent variable structure and captures complex relationships between medications, diagnoses, laboratory tests, neurological assessments, and medications.","It can be trained on original data, without requiring any lossy transformations or time binning.","Inference algorithms are derived that use partial data to infer properties of the complete sequences, including their length and presence of specific values.","We train this model on data from subjects receiving medical care in the Kaiser Permanente Northern California integrated healthcare delivery system.","The results are evaluated against held-out data for predicting the length of sequences and presence of Intensive Care Unit (ICU) in hospitalization bed sequences.","Our method outperforms a baseline approach, showing that in these experiments the trained model captures information in the sequences that is informative of their future values."],"url":"http://arxiv.org/abs/2403.19011v1","category":"q-bio.QM"}
{"created":"2024-03-27 21:04:45","title":"Gaussian Process-based Traversability Analysis for Terrain Mapless Navigation","abstract":"Efficient navigation through uneven terrain remains a challenging endeavor for autonomous robots. We propose a new geometric-based uneven terrain mapless navigation framework combining a Sparse Gaussian Process (SGP) local map with a Rapidly-Exploring Random Tree* (RRT*) planner. Our approach begins with the generation of a high-resolution SGP local map, providing an interpolated representation of the robot's immediate environment. This map captures crucial environmental variations, including height, uncertainties, and slope characteristics. Subsequently, we construct a traversability map based on the SGP representation to guide our planning process. The RRT* planner efficiently generates real-time navigation paths, avoiding untraversable terrain in pursuit of the goal. This combination of SGP-based terrain interpretation and RRT* planning enables ground robots to safely navigate environments with varying elevations and steep obstacles. We evaluate the performance of our proposed approach through robust simulation testing, highlighting its effectiveness in achieving safe and efficient navigation compared to existing methods.","sentences":["Efficient navigation through uneven terrain remains a challenging endeavor for autonomous robots.","We propose a new geometric-based uneven terrain mapless navigation framework combining a Sparse Gaussian Process (SGP) local map with a Rapidly-Exploring Random Tree* (RRT*) planner.","Our approach begins with the generation of a high-resolution SGP local map, providing an interpolated representation of the robot's immediate environment.","This map captures crucial environmental variations, including height, uncertainties, and slope characteristics.","Subsequently, we construct a traversability map based on the SGP representation to guide our planning process.","The RRT* planner efficiently generates real-time navigation paths, avoiding untraversable terrain in pursuit of the goal.","This combination of SGP-based terrain interpretation and RRT* planning enables ground robots to safely navigate environments with varying elevations and steep obstacles.","We evaluate the performance of our proposed approach through robust simulation testing, highlighting its effectiveness in achieving safe and efficient navigation compared to existing methods."],"url":"http://arxiv.org/abs/2403.19010v1","category":"cs.RO"}
{"created":"2024-03-27 20:59:49","title":"Policy iteration for discrete-time systems with discounted costs: stability and near-optimality guarantees","abstract":"Given a discounted cost, we study deterministic discrete-time systems whose inputs are generated by policy iteration (PI). We provide novel near-optimality and stability properties, while allowing for non stabilizing initial policies. That is, we first give novel bounds on the mismatch between the value function generated by PI and the optimal value function, which are less conservative in general than those encountered in the dynamic programming literature for the considered class of systems. Then, we show that the system in closed-loop with policies generated by PI are stabilizing under mild conditions, after a finite (and known) number of iterations.","sentences":["Given a discounted cost, we study deterministic discrete-time systems whose inputs are generated by policy iteration (PI).","We provide novel near-optimality and stability properties, while allowing for non stabilizing initial policies.","That is, we first give novel bounds on the mismatch between the value function generated by PI and the optimal value function, which are less conservative in general than those encountered in the dynamic programming literature for the considered class of systems.","Then, we show that the system in closed-loop with policies generated by PI are stabilizing under mild conditions, after a finite (and known) number of iterations."],"url":"http://arxiv.org/abs/2403.19007v1","category":"math.OC"}
{"created":"2024-03-27 20:58:45","title":"Ensuring Safe Autonomy: Navigating the Future of Autonomous Vehicles","abstract":"Autonomous driving vehicles provide a vast potential for realizing use cases in the on-road and off-road domains. Consequently, remarkable solutions exist to autonomous systems' environmental perception and control. Nevertheless, proof of safety remains an open challenge preventing such machinery from being introduced to markets and deployed in real world. Traditional approaches for safety assurance of autonomously driving vehicles often lead to underperformance due to conservative safety assumptions that cannot handle the overall complexity. Besides, the more sophisticated safety systems rely on the vehicle's perception systems. However, perception is often unreliable due to uncertainties resulting from disturbances or the lack of context incorporation for data interpretation. Accordingly, this paper illustrates the potential of a modular, self-adaptive autonomy framework with integrated dynamic risk management to overcome the abovementioned drawbacks.","sentences":["Autonomous driving vehicles provide a vast potential for realizing use cases in the on-road and off-road domains.","Consequently, remarkable solutions exist to autonomous systems' environmental perception and control.","Nevertheless, proof of safety remains an open challenge preventing such machinery from being introduced to markets and deployed in real world.","Traditional approaches for safety assurance of autonomously driving vehicles often lead to underperformance due to conservative safety assumptions that cannot handle the overall complexity.","Besides, the more sophisticated safety systems rely on the vehicle's perception systems.","However, perception is often unreliable due to uncertainties resulting from disturbances or the lack of context incorporation for data interpretation.","Accordingly, this paper illustrates the potential of a modular, self-adaptive autonomy framework with integrated dynamic risk management to overcome the abovementioned drawbacks."],"url":"http://arxiv.org/abs/2403.19006v1","category":"cs.RO"}
{"created":"2024-03-27 20:56:02","title":"Discrete Poincar\u00e9 inequality and Discrete Trace inequality in Piece-wise Polynomial Hybridizable Spaces","abstract":"In this paper, we establish discrete versions of the Poincar\\'e and trace inequalities for hybridizable finite element spaces. These spaces are made of piecewise polynomial functions defined both within the interiors of elements and across all faces in a mesh's skeleton, serving as the basis for both the hybridizable discontinuous Galerkin (HDG) and hybrid high-order (HHO) methods. Additionally, we present a specific adaptation of these inequalities for the HDG method and apply them to demonstrate the stability of the related numerical schemes for second-order elliptic equations under the minimal regularity assumptions for the source term and boundary data.","sentences":["In this paper, we establish discrete versions of the Poincar\\'e and trace inequalities for hybridizable finite element spaces.","These spaces are made of piecewise polynomial functions defined both within the interiors of elements and across all faces in a mesh's skeleton, serving as the basis for both the hybridizable discontinuous Galerkin (HDG) and hybrid high-order (HHO) methods.","Additionally, we present a specific adaptation of these inequalities for the HDG method and apply them to demonstrate the stability of the related numerical schemes for second-order elliptic equations under the minimal regularity assumptions for the source term and boundary data."],"url":"http://arxiv.org/abs/2403.19004v1","category":"math.NA"}
{"created":"2024-03-27 20:39:36","title":"Deciding Boolean Separation Logic via Small Models (Technical Report)","abstract":"We present a novel decision procedure for a fragment of separation logic (SL) with arbitrary nesting of separating conjunctions with boolean conjunctions, disjunctions, and guarded negations together with a support for the most common variants of linked lists. Our method is based on a model-based translation to SMT for which we introduce several optimisations$\\unicode{x2013}$the most important of them is based on bounding the size of predicate instantiations within models of larger formulae, which leads to a much more efficient translation of SL formulae to SMT. Through a series of experiments, we show that, on the frequently used symbolic heap fragment, our decision procedure is competitive with other existing approaches, and it can outperform them outside the symbolic heap fragment. Moreover, our decision procedure can also handle some formulae for which no decision procedure has been implemented so far.","sentences":["We present a novel decision procedure for a fragment of separation logic (SL) with arbitrary nesting of separating conjunctions with boolean conjunctions, disjunctions, and guarded negations together with a support for the most common variants of linked lists.","Our method is based on a model-based translation to SMT for which we introduce several optimisations$\\unicode{x2013}$the most important of them is based on bounding the size of predicate instantiations within models of larger formulae, which leads to a much more efficient translation of SL formulae to SMT.","Through a series of experiments, we show that, on the frequently used symbolic heap fragment, our decision procedure is competitive with other existing approaches, and it can outperform them outside the symbolic heap fragment.","Moreover, our decision procedure can also handle some formulae for which no decision procedure has been implemented so far."],"url":"http://arxiv.org/abs/2403.18999v1","category":"cs.LO"}
{"created":"2024-03-27 20:24:07","title":"Tractography with T1-weighted MRI and associated anatomical constraints on clinical quality diffusion MRI","abstract":"Diffusion MRI (dMRI) streamline tractography, the gold standard for in vivo estimation of brain white matter (WM) pathways, has long been considered indicative of macroscopic relationships with WM microstructure. However, recent advances in tractography demonstrated that convolutional recurrent neural networks (CoRNN) trained with a teacher-student framework have the ability to learn and propagate streamlines directly from T1 and anatomical contexts. Training for this network has previously relied on high-resolution dMRI. In this paper, we generalize the training mechanism to traditional clinical resolution data, which allows generalizability across sensitive and susceptible study populations. We train CoRNN on a small subset of the Baltimore Longitudinal Study of Aging (BLSA), which better resembles clinical protocols. Then, we define a metric, termed the epsilon ball seeding method, to compare T1 tractography and traditional diffusion tractography at the streamline level. Under this metric, T1 tractography generated by CoRNN reproduces diffusion tractography with approximately two millimeters of error.","sentences":["Diffusion MRI (dMRI) streamline tractography, the gold standard for in vivo estimation of brain white matter (WM) pathways, has long been considered indicative of macroscopic relationships with WM microstructure.","However, recent advances in tractography demonstrated that convolutional recurrent neural networks (CoRNN) trained with a teacher-student framework have the ability to learn and propagate streamlines directly from T1 and anatomical contexts.","Training for this network has previously relied on high-resolution dMRI.","In this paper, we generalize the training mechanism to traditional clinical resolution data, which allows generalizability across sensitive and susceptible study populations.","We train CoRNN on a small subset of the Baltimore Longitudinal Study of Aging (BLSA), which better resembles clinical protocols.","Then, we define a metric, termed the epsilon ball seeding method, to compare T1 tractography and traditional diffusion tractography at the streamline level.","Under this metric, T1 tractography generated by CoRNN reproduces diffusion tractography with approximately two millimeters of error."],"url":"http://arxiv.org/abs/2403.18992v1","category":"eess.IV"}
{"created":"2024-03-27 20:15:18","title":"Turbulence properties and kinetic signatures of electron in Kelvin-Helmholtz waves during a geomagnetic storm","abstract":"We present a comprehensive study of Magnetospheric Multiscale (MMS) spacecraft encounter with KHI during a geomagnetic storm, focusing on elucidating key turbulence properties and reconnection signatures observed at the edges of KH vortices. The spectral slope for electric field stays approximately constant for frequencies below the ion cyclotron frequency and exhibits a break around the lower hybrid frequency, indicating wave activity. Furthermore, MMS observes a current sheet accompanied by intense electron jets and features consistent with strong guide-field asymmetric reconnection across the magnetopause. Substantial agyrotropy (by a factor of 10) in electron distribution functions is observed in the reconnecting current sheet and at the edges of KH. Our observation presents a multi-scale view into KH turbulence under strongly driven conditions and into the dynamics occurring at electron dissipation scales.","sentences":["We present a comprehensive study of Magnetospheric Multiscale (MMS) spacecraft encounter with KHI during a geomagnetic storm, focusing on elucidating key turbulence properties and reconnection signatures observed at the edges of KH vortices.","The spectral slope for electric field stays approximately constant for frequencies below the ion cyclotron frequency and exhibits a break around the lower hybrid frequency, indicating wave activity.","Furthermore, MMS observes a current sheet accompanied by intense electron jets and features consistent with strong guide-field asymmetric reconnection across the magnetopause.","Substantial agyrotropy (by a factor of 10) in electron distribution functions is observed in the reconnecting current sheet and at the edges of KH.","Our observation presents a multi-scale view into KH turbulence under strongly driven conditions and into the dynamics occurring at electron dissipation scales."],"url":"http://arxiv.org/abs/2403.18990v1","category":"physics.space-ph"}
{"created":"2024-03-27 20:08:01","title":"Reconfigurable multiplex setup for high throughput electrical characterisation at cryogenic temperature","abstract":"In this paper, we present a reconfigurable multiplex (MUX) setup that increases the throughput of electrical characterisation at cryogenic temperature. The setup separates the MUX circuitry from quantum device under test (qDUT), allowing qDUT chips to be exchanged easily and MUX chips to be reused. To interface with different types of qDUTs, board-level designs are incorporated to allow interconnects flexibly routed into different topology. MUXs are built based on a multiple level selective gating (MLSG) scheme, where the number of multiplexed output channels (interconnects) is exponentially dependent on the number of control lines. In the prototype setup presented in this paper, with 14 out of 44 existing wires from room temperature, 4 MUXs at cryogenic temperature can supply in total 128 interconnects to interface with qDUTs. We validate the MUX setup operation and assess the various limits existed by measuring k$\\Omega$ resistors made of $\\mu$m-size graphene ribbons. We further demonstrate the setup by performing charge transport measurement on 128 nm-size graphene quantum devices in a single cooling down.","sentences":["In this paper, we present a reconfigurable multiplex (MUX) setup that increases the throughput of electrical characterisation at cryogenic temperature.","The setup separates the MUX circuitry from quantum device under test (qDUT), allowing qDUT chips to be exchanged easily and MUX chips to be reused.","To interface with different types of qDUTs, board-level designs are incorporated to allow interconnects flexibly routed into different topology.","MUXs are built based on a multiple level selective gating (MLSG) scheme, where the number of multiplexed output channels (interconnects) is exponentially dependent on the number of control lines.","In the prototype setup presented in this paper, with 14 out of 44 existing wires from room temperature, 4 MUXs at cryogenic temperature can supply in total 128 interconnects to interface with qDUTs.","We validate the MUX setup operation and assess the various limits existed by measuring k$\\Omega$ resistors made of $\\mu$m-size graphene ribbons.","We further demonstrate the setup by performing charge transport measurement on 128 nm-size graphene quantum devices in a single cooling down."],"url":"http://arxiv.org/abs/2403.18987v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-27 20:00:30","title":"Insights on the dip of fault zones in Southern California from modeling of seismicity with anisotropic point processes","abstract":"Accurate models of fault zone geometry are important for scientific and hazard applications. While seismicity can provide high-resolution point measurements of fault geometry, extrapolating these measurements to volumes may involve making strong assumptions. This is particularly problematic in distributed fault zones, which are commonly observed in immature faulting regions. In this study, we focus on characterizing the dip of fault zones in Southern California with the goal of improving fault models. We introduce a novel technique from spatial point process theory to quantify the orientation of persistent surficial features in seismicity, even when embedded in wide shear zones. The technique makes relatively mild assumptions about fault geometry and is formulated with the goal of determining the dip of a fault zone at depth. The method is applied to 11 prominent seismicity regions in Southern California. Overall, the results compare favorably with the geometry models provided by the SCEC Community Fault Model and other focused regional studies. More specifically, we find evidence that the Southern San Andreas and San Jacinto fault zones are both northeast dipping at seismogenic depths at the length scales of 1.0-4.0 km. In addition, we find more limited evidence for some depth dependent variations in dip that suggest a listric geometry. The developed technique can provide an independent source of information from seismicity to augment existing fault geometry models.","sentences":["Accurate models of fault zone geometry are important for scientific and hazard applications.","While seismicity can provide high-resolution point measurements of fault geometry, extrapolating these measurements to volumes may involve making strong assumptions.","This is particularly problematic in distributed fault zones, which are commonly observed in immature faulting regions.","In this study, we focus on characterizing the dip of fault zones in Southern California with the goal of improving fault models.","We introduce a novel technique from spatial point process theory to quantify the orientation of persistent surficial features in seismicity, even when embedded in wide shear zones.","The technique makes relatively mild assumptions about fault geometry and is formulated with the goal of determining the dip of a fault zone at depth.","The method is applied to 11 prominent seismicity regions in Southern California.","Overall, the results compare favorably with the geometry models provided by the SCEC Community Fault Model and other focused regional studies.","More specifically, we find evidence that the Southern San Andreas and San Jacinto fault zones are both northeast dipping at seismogenic depths at the length scales of 1.0-4.0 km.","In addition, we find more limited evidence for some depth dependent variations in dip that suggest a listric geometry.","The developed technique can provide an independent source of information from seismicity to augment existing fault geometry models."],"url":"http://arxiv.org/abs/2403.18982v1","category":"physics.geo-ph"}
{"created":"2024-03-27 19:43:45","title":"A Novel Corpus of Annotated Medical Imaging Reports and Information Extraction Results Using BERT-based Language Models","abstract":"Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others. Radiologists interpret these complex, unstructured images and articulate their assessments through narrative reports that remain largely unstructured. This unstructured narrative must be converted into a structured semantic representation to facilitate secondary applications such as retrospective analyses or clinical decision support. Here, we introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which includes 609 annotated radiology reports from three imaging modality types: Computed Tomography, Magnetic Resonance Imaging, and Positron Emission Tomography-Computed Tomography. Reports were annotated using an event-based schema that captures clinical indications, lesions, and medical problems. Each event consists of a trigger and multiple arguments, and a majority of the argument types, including anatomy, normalize the spans to pre-defined concepts to facilitate secondary use. CAMIR uniquely combines a granular event structure and concept normalization. To extract CAMIR events, we explored two BERT (Bi-directional Encoder Representation from Transformers)-based architectures, including an existing architecture (mSpERT) that jointly extracts all event information and a multi-step approach (PL-Marker++) that we augmented for the CAMIR schema.","sentences":["Medical imaging is critical to the diagnosis, surveillance, and treatment of many health conditions, including oncological, neurological, cardiovascular, and musculoskeletal disorders, among others.","Radiologists interpret these complex, unstructured images and articulate their assessments through narrative reports that remain largely unstructured.","This unstructured narrative must be converted into a structured semantic representation to facilitate secondary applications such as retrospective analyses or clinical decision support.","Here, we introduce the Corpus of Annotated Medical Imaging Reports (CAMIR), which includes 609 annotated radiology reports from three imaging modality types: Computed Tomography, Magnetic Resonance Imaging, and Positron Emission Tomography-Computed Tomography.","Reports were annotated using an event-based schema that captures clinical indications, lesions, and medical problems.","Each event consists of a trigger and multiple arguments, and a majority of the argument types, including anatomy, normalize the spans to pre-defined concepts to facilitate secondary use.","CAMIR uniquely combines a granular event structure and concept normalization.","To extract CAMIR events, we explored two BERT (Bi-directional Encoder Representation from Transformers)-based architectures, including an existing architecture (mSpERT) that jointly extracts all event information and a multi-step approach (PL-Marker++) that we augmented for the CAMIR schema."],"url":"http://arxiv.org/abs/2403.18975v1","category":"cs.CL"}
{"created":"2024-03-27 19:43:07","title":"Granular gases under resetting","abstract":"We investigate the granular temperatures in force-free granular gases under exponential resetting. When a resetting event occurs, the granular temperature attains its initial value, while between the resetting events it cools down. We show that the granular system attains a non-equilibrium steady state value and study the dependence of average granular temperature on the resetting rate. Our theory may help to explain the behavior of non-periodically driven granular systems.","sentences":["We investigate the granular temperatures in force-free granular gases under exponential resetting.","When a resetting event occurs, the granular temperature attains its initial value, while between the resetting events it cools down.","We show that the granular system attains a non-equilibrium steady state value and study the dependence of average granular temperature on the resetting rate.","Our theory may help to explain the behavior of non-periodically driven granular systems."],"url":"http://arxiv.org/abs/2403.18974v1","category":"cond-mat.soft"}
{"created":"2024-03-27 19:42:01","title":"Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition","abstract":"We present Conformal Intent Classification and Clarification (CICC), a framework for fast and accurate intent classification for task-oriented dialogue systems. The framework turns heuristic uncertainty scores of any intent classifier into a clarification question that is guaranteed to contain the true intent at a pre-defined confidence level. By disambiguating between a small number of likely intents, the user query can be resolved quickly and accurately. Additionally, we propose to augment the framework for out-of-scope detection. In a comparative evaluation using seven intent recognition datasets we find that CICC generates small clarification questions and is capable of out-of-scope detection. CICC can help practitioners and researchers substantially in improving the user experience of dialogue agents with specific clarification questions.","sentences":["We present Conformal Intent Classification and Clarification (CICC), a framework for fast and accurate intent classification for task-oriented dialogue systems.","The framework turns heuristic uncertainty scores of any intent classifier into a clarification question that is guaranteed to contain the true intent at a pre-defined confidence level.","By disambiguating between a small number of likely intents, the user query can be resolved quickly and accurately.","Additionally, we propose to augment the framework for out-of-scope detection.","In a comparative evaluation using seven intent recognition datasets we find that CICC generates small clarification questions and is capable of out-of-scope detection.","CICC can help practitioners and researchers substantially in improving the user experience of dialogue agents with specific clarification questions."],"url":"http://arxiv.org/abs/2403.18973v1","category":"cs.CL"}
{"created":"2024-03-27 19:36:36","title":"Concurrent level set and fiber orientation optimization of composite structures","abstract":"By adjusting both the structural shape and fiber orientation, this research aims to optimize the design of Fiber Reinforced Composite structures. The structural geometry is represented by a level set function, which is approximated by quadratic B-spline functions. The fiber orientation field is parameterized with quadratic/cubic B-splines on hierarchically refined meshes. Different levels for B-spline mesh refinement for the level set and fiber orientation fields are studied to obtain a smooth fiber layout. To facilitate FRC manufacturing, the parallel alignment, and smoothness of fiber paths are enforced by introducing penalty terms referred to as \"misalignment penalty and curvature penalty\", which are incorporated into the optimization process. A geometric interpretation of the penalties is provided. The material behavior of the FRCs is modeled by the Mori-Tanaka homogenization scheme and the macroscopic structure response is modeled by linear elasticity under static mutiloading conditions. The Governing equations are discretized by a Heaviside-enriched eXtended IsoGeometric Analysis to avoid the need to generate conformal meshes. Instabilities in XIGA are mitigated by the facet-oriented ghost stabilization technique. This work considers mass and strain energy in the formulation of the optimization objective, along with misalignment and curvature penalties and additional regularization terms. Constraints are imposed on the volume of the structure. The resulting optimization problems are solved by a gradient-based algorithm. The design sensitivities are computed by the adjoint method. Numerical examples demonstrate with two-dimensional and three-dimensional configurations that the proposed method is efficient in simultaneously optimizing the macroscopic shape and the fiber layout while improving manufacturability by promoting parallel and smooth fiber paths.","sentences":["By adjusting both the structural shape and fiber orientation, this research aims to optimize the design of Fiber Reinforced Composite structures.","The structural geometry is represented by a level set function, which is approximated by quadratic B-spline functions.","The fiber orientation field is parameterized with quadratic/cubic B-splines on hierarchically refined meshes.","Different levels for B-spline mesh refinement for the level set and fiber orientation fields are studied to obtain a smooth fiber layout.","To facilitate FRC manufacturing, the parallel alignment, and smoothness of fiber paths are enforced by introducing penalty terms referred to as \"misalignment penalty and curvature penalty\", which are incorporated into the optimization process.","A geometric interpretation of the penalties is provided.","The material behavior of the FRCs is modeled by the Mori-Tanaka homogenization scheme and the macroscopic structure response is modeled by linear elasticity under static mutiloading conditions.","The Governing equations are discretized by a Heaviside-enriched eXtended IsoGeometric Analysis to avoid the need to generate conformal meshes.","Instabilities in XIGA are mitigated by the facet-oriented ghost stabilization technique.","This work considers mass and strain energy in the formulation of the optimization objective, along with misalignment and curvature penalties and additional regularization terms.","Constraints are imposed on the volume of the structure.","The resulting optimization problems are solved by a gradient-based algorithm.","The design sensitivities are computed by the adjoint method.","Numerical examples demonstrate with two-dimensional and three-dimensional configurations that the proposed method is efficient in simultaneously optimizing the macroscopic shape and the fiber layout while improving manufacturability by promoting parallel and smooth fiber paths."],"url":"http://arxiv.org/abs/2403.18971v1","category":"math.NA"}
{"created":"2024-03-27 19:32:27","title":"Stabilization of linear Port-Hamiltonian Descriptor Systems via Output Feedback","abstract":"The structure preserving stabilization of (possibly non-regular) linear port-Hamiltonian descriptor (pHDAE) systems by output feedback is discussed. While for general descriptor systems the characterization when there exist output feedbacks that lead to an asymptotically stable closed loop system is a very hard and partially open problem, for systems in pHDAE representation this problem can be completely solved. Necessary and sufficient conditions are presented that guarantee that there exist a proportional output feedback such that the resulting closed-loop port-Hamiltonian descriptor system is (robustly) asymptotically stable. For this it is also necessary that the output feedback also makes the problem regular and of index at most one. A complete characterization when this is possible is presented as well.","sentences":["The structure preserving stabilization of (possibly non-regular) linear port-Hamiltonian descriptor (pHDAE) systems by output feedback is discussed.","While for general descriptor systems the characterization when there exist output feedbacks that lead to an asymptotically stable closed loop system is a very hard and partially open problem, for systems in pHDAE representation this problem can be completely solved.","Necessary and sufficient conditions are presented that guarantee that there exist a proportional output feedback such that the resulting closed-loop port-Hamiltonian descriptor system is (robustly) asymptotically stable.","For this it is also necessary that the output feedback also makes the problem regular and of index at most one.","A complete characterization when this is possible is presented as well."],"url":"http://arxiv.org/abs/2403.18967v1","category":"math.OC"}
{"created":"2024-03-27 19:11:53","title":"Spatial confounding under infill asymptotics","abstract":"The estimation of regression parameters in spatially referenced data plays a crucial role across various scientific domains. A common approach involves employing an additive regression model to capture the relationship between observations and covariates, accounting for spatial variability not explained by the covariates through a Gaussian random field. While theoretical analyses of such models have predominantly focused on prediction and covariance parameter inference, recent attention has shifted towards understanding the theoretical properties of regression coefficient estimates, particularly in the context of spatial confounding. This article studies the effect of misspecified covariates, in particular when the misspecification changes the smoothness. We analyze the theoretical properties of the generalize least-square estimator under infill asymptotics, and show that the estimator can have counter-intuitive properties. In particular, the estimated regression coefficients can converge to zero as the number of observations increases, despite high correlations between observations and covariates. Perhaps even more surprising, the estimates can diverge to infinity under certain conditions. Through an application to temperature and precipitation data, we show that both behaviors can be observed for real data. Finally, we propose a simple fix to the problem by adding a smoothing step in the regression.","sentences":["The estimation of regression parameters in spatially referenced data plays a crucial role across various scientific domains.","A common approach involves employing an additive regression model to capture the relationship between observations and covariates, accounting for spatial variability not explained by the covariates through a Gaussian random field.","While theoretical analyses of such models have predominantly focused on prediction and covariance parameter inference, recent attention has shifted towards understanding the theoretical properties of regression coefficient estimates, particularly in the context of spatial confounding.","This article studies the effect of misspecified covariates, in particular when the misspecification changes the smoothness.","We analyze the theoretical properties of the generalize least-square estimator under infill asymptotics, and show that the estimator can have counter-intuitive properties.","In particular, the estimated regression coefficients can converge to zero as the number of observations increases, despite high correlations between observations and covariates.","Perhaps even more surprising, the estimates can diverge to infinity under certain conditions.","Through an application to temperature and precipitation data, we show that both behaviors can be observed for real data.","Finally, we propose a simple fix to the problem by adding a smoothing step in the regression."],"url":"http://arxiv.org/abs/2403.18961v1","category":"math.ST"}
{"created":"2024-03-27 19:10:32","title":"Robust In-Hand Manipulation with Extrinsic Contacts","abstract":"We present in-hand manipulation tasks where a robot moves an object in grasp, maintains its external contact mode with the environment, and adjusts its in-hand pose simultaneously. The proposed manipulation task leads to complex contact interactions which can be very susceptible to uncertainties in kinematic and physical parameters. Therefore, we propose a robust in-hand manipulation method, which consists of two parts. First, an in-gripper mechanics model that computes a na\\\"ive motion cone assuming all parameters are precise. Then, a robust planning method refines the motion cone to maintain desired contact mode regardless of parametric errors. Real-world experiments were conducted to illustrate the accuracy of the mechanics model and the effectiveness of the robust planning framework in the presence of kinematics parameter errors.","sentences":["We present in-hand manipulation tasks where a robot moves an object in grasp, maintains its external contact mode with the environment, and adjusts its in-hand pose simultaneously.","The proposed manipulation task leads to complex contact interactions which can be very susceptible to uncertainties in kinematic and physical parameters.","Therefore, we propose a robust in-hand manipulation method, which consists of two parts.","First, an in-gripper mechanics model that computes a na\\\"ive motion cone assuming all parameters are precise.","Then, a robust planning method refines the motion cone to maintain desired contact mode regardless of parametric errors.","Real-world experiments were conducted to illustrate the accuracy of the mechanics model and the effectiveness of the robust planning framework in the presence of kinematics parameter errors."],"url":"http://arxiv.org/abs/2403.18960v1","category":"cs.RO"}
{"created":"2024-03-27 18:59:46","title":"Characterizing Controllability and Observability for Systems with Locality, Communication, and Actuation Constraints","abstract":"This paper presents a closed-form notion of controllability and observability for systems with communication delays, actuation delays, and locality constraints. The formulation reduces to classical notions of controllability and observability in the unconstrained setting. As a consequence of our formulation, we show that the addition of locality and communication constraints may not affect the controllability and observability of the system, and we provide an efficient sufficient condition under which this phenomenon occurs. This contrasts with actuation and sensing delays, which cause a gradual loss of controllability and observability as the delays increase. We illustrate our results using linearized swing equations for the power grid, showing how actuation delay and locality constraints affect controllability.","sentences":["This paper presents a closed-form notion of controllability and observability for systems with communication delays, actuation delays, and locality constraints.","The formulation reduces to classical notions of controllability and observability in the unconstrained setting.","As a consequence of our formulation, we show that the addition of locality and communication constraints may not affect the controllability and observability of the system, and we provide an efficient sufficient condition under which this phenomenon occurs.","This contrasts with actuation and sensing delays, which cause a gradual loss of controllability and observability as the delays increase.","We illustrate our results using linearized swing equations for the power grid, showing how actuation delay and locality constraints affect controllability."],"url":"http://arxiv.org/abs/2403.18956v1","category":"math.OC"}
{"created":"2024-03-27 18:57:15","title":"JWST Photometric Time-Delay and Magnification Measurements for the Triply-Imaged Type Ia \"Supernova H0pe\" at z = 1.78","abstract":"Supernova (SN) H0pe is a gravitationally lensed, triply-imaged, Type Ia SN (SN Ia) discovered in James Webb Space Telescope imaging of the PLCK G165.7+67.0 cluster of galaxies. Well-observed multiply-imaged SNe provide a rare opportunity to constrain the Hubble constant ($H_0$), by measuring the relative time delay between the images and modeling the foreground mass distribution. SN H0pe is located at $z=1.783$, and is the first SN Ia with sufficient light curve sampling and long enough time delays for an $H_0$ inference. Here we present photometric time-delay measurements and SN properties of SN H0pe. Using JWST/NIRCam photometry we measure time delays of $\\Delta t_{ab}=-116.6^{+10.8}_{-9.3}$ and $\\Delta t_{cb}=-48.6^{+3.6}_{-4.0}$ observer-frame days relative to the last image to arrive (image 2b; all uncertainties are $1\\sigma$), which corresponds to a $\\sim5.6\\%$ uncertainty contribution for $H_0$ assuming $70 \\rm{km s^{-1} Mpc^{-1}}$. We also constrain the absolute magnification of each image to $\\mu_{a}=4.3^{+1.6}_{-1.8}$, $\\mu_{b}=7.6^{+3.6}_{-2.6}$, $\\mu_{c}=6.4^{+1.6}_{-1.5}$ by comparing the observed peak near-IR magnitude of SN H0pe to the non-lensed population of SNe Ia.","sentences":["Supernova (SN) H0pe is a gravitationally lensed, triply-imaged, Type Ia SN (SN Ia) discovered in James Webb Space Telescope imaging of the PLCK G165.7+67.0 cluster of galaxies.","Well-observed multiply-imaged SNe provide a rare opportunity to constrain the Hubble constant ($H_0$), by measuring the relative time delay between the images and modeling the foreground mass distribution.","SN H0pe is located at $z=1.783$, and is the first SN Ia with sufficient light curve sampling and long enough time delays for an $H_0$ inference.","Here we present photometric time-delay measurements and SN properties of SN H0pe.","Using JWST/NIRCam photometry we measure time delays of $\\Delta t_{ab}=-116.6^{+10.8}_{-9.3}$ and $\\Delta t_{cb}=-48.6^{+3.6}_{-4.0}$ observer-frame days relative to the last image to arrive (image 2b; all uncertainties are $1\\sigma$), which corresponds to a $\\sim5.6\\%$ uncertainty contribution for $H_0$ assuming $70 \\rm{km s^{-1} Mpc^{-1}}$.","We also constrain the absolute magnification of each image to $\\mu_{a}=4.3^{+1.6}_{-1.8}$, $\\mu_{b}=7.6^{+3.6}_{-2.6}$, $\\mu_{c}=6.4^{+1.6}_{-1.5}$ by comparing the observed peak near-IR magnitude of SN H0pe to the non-lensed population of SNe Ia."],"url":"http://arxiv.org/abs/2403.18954v1","category":"astro-ph.CO"}
{"created":"2024-03-27 18:56:39","title":"Supernova Simulations","abstract":"Magnetohydrodynamic simulations of core-collapse supernovae have become increasingly mature and important in recent years. Magnetic fields take center stage in scenarios for explaining hypernova explosions, but are now also considered in supernova theory more broadly as an important factor even in neutrino-driven explosions, especially in the context of neutron star birth properties. Here we present an overview of simulation approaches currently used for magnetohydrodynamic supernova simulations and sketch essential physical concepts for understanding the role of magnetic fields in supernovae of slowly or rapidly rotating massive stars. We review progress on simulations of neutrino-driven supernovae, magnetorotational supernovae, and the relevant field amplification processes. Recent results on the nucleosynthesis and gravitational wave emission from magnetorotational supernovae are also discussed. We highlight efforts to provide better initial conditions for magnetohydrodynamic supernova models by simulating short phases of the progenitor evolution in 3D to address uncertainties in the treatment of rotation and magnetic fields in current stellar evolution models.","sentences":["Magnetohydrodynamic simulations of core-collapse supernovae have become increasingly mature and important in recent years.","Magnetic fields take center stage in scenarios for explaining hypernova explosions, but are now also considered in supernova theory more broadly as an important factor even in neutrino-driven explosions, especially in the context of neutron star birth properties.","Here we present an overview of simulation approaches currently used for magnetohydrodynamic supernova simulations and sketch essential physical concepts for understanding the role of magnetic fields in supernovae of slowly or rapidly rotating massive stars.","We review progress on simulations of neutrino-driven supernovae, magnetorotational supernovae, and the relevant field amplification processes.","Recent results on the nucleosynthesis and gravitational wave emission from magnetorotational supernovae are also discussed.","We highlight efforts to provide better initial conditions for magnetohydrodynamic supernova models by simulating short phases of the progenitor evolution in 3D to address uncertainties in the treatment of rotation and magnetic fields in current stellar evolution models."],"url":"http://arxiv.org/abs/2403.18952v1","category":"astro-ph.HE"}
{"created":"2024-03-27 18:35:26","title":"Neural Post-Einsteinian Framework for Efficient Theory-Agnostic Tests of General Relativity with Gravitational Waves","abstract":"The parametrized post-Einsteinian (ppE) framework and its variants are widely used to probe gravity through gravitational-wave tests that apply to a large class of theories beyond general relativity. However, the ppE framework is not truly theory-agnostic as it only captures certain types of deviations from general relativity: those that admit a post-Newtonian series representation in the inspiral of coalescencing compact objects. Moreover, each type of deviation in the ppE framework has to be tested separately, making the whole process computationally inefficient and expensive, possibly obscuring the theoretical interpretation of potential deviations that could be detected in the future. We here present the neural post-Einsteinian (npE) framework, an extension of the ppE formalism that overcomes the above weaknesses using deep-learning neural networks. The core of the npE framework is a variantional autoencoder that maps the discrete ppE theories into a continuous latent space in a well-organized manner. This design enables the npE framework to test many theories simultaneously and to select the theory that best describes the observation in a single parameter estimation run. The smooth extension of the ppE parametrization also allows for more general types of deviations to be searched for with the npE model. We showcase the application of the new npE framework to future tests of general relativity with the fifth observing run of the LIGO-Virgo-KAGRA collaboration. In particular, the npE framework is demonstrated to efficiently explore modifications to general relativity beyond what can be mapped by the ppE framework, including modifications coming from higher-order curvature corrections to the Einstein-Hilbert action at high post-Newtonian order, and dark-photon interactions in possibly hidden sectors of matter that do not admit a post-Newtonian representation.","sentences":["The parametrized post-Einsteinian (ppE) framework and its variants are widely used to probe gravity through gravitational-wave tests that apply to a large class of theories beyond general relativity.","However, the ppE framework is not truly theory-agnostic as it only captures certain types of deviations from general relativity: those that admit a post-Newtonian series representation in the inspiral of coalescencing compact objects.","Moreover, each type of deviation in the ppE framework has to be tested separately, making the whole process computationally inefficient and expensive, possibly obscuring the theoretical interpretation of potential deviations that could be detected in the future.","We here present the neural post-Einsteinian (npE) framework, an extension of the ppE formalism that overcomes the above weaknesses using deep-learning neural networks.","The core of the npE framework is a variantional autoencoder that maps the discrete ppE theories into a continuous latent space in a well-organized manner.","This design enables the npE framework to test many theories simultaneously and to select the theory that best describes the observation in a single parameter estimation run.","The smooth extension of the ppE parametrization also allows for more general types of deviations to be searched for with the npE model.","We showcase the application of the new npE framework to future tests of general relativity with the fifth observing run of the LIGO-Virgo-KAGRA collaboration.","In particular, the npE framework is demonstrated to efficiently explore modifications to general relativity beyond what can be mapped by the ppE framework, including modifications coming from higher-order curvature corrections to the Einstein-Hilbert action at high post-Newtonian order, and dark-photon interactions in possibly hidden sectors of matter that do not admit a post-Newtonian representation."],"url":"http://arxiv.org/abs/2403.18936v1","category":"gr-qc"}
{"created":"2024-03-27 18:16:24","title":"Interval Effect Algebras and Holevo Instruments","abstract":"This article begins with a study of convex effect-state spaces. We point out that such spaces are equivalent to interval effect algebras that generate an ordered linear space and possess an order-determining set of states. We then discuss operations and instruments on interval effect algebras under the assumption of an unrestrictive condition. Effects measured by operations and sequential products of effects relative to operations are considered. Observables are introduced and coexistence of effects are discussed. We also present properties of sequential products of observables and conditioning of observables related to instruments. The final section is devoted to Holevo instruments. Pure and mixed Holevo operations are defined and extended to instruments. The Holevo sequential product of two observables is defined and the marginals of these products are computed. We define the commutant of two effects and derive its properties. Examples are given that illustrate the properties of previously presented concepts.","sentences":["This article begins with a study of convex effect-state spaces.","We point out that such spaces are equivalent to interval effect algebras that generate an ordered linear space and possess an order-determining set of states.","We then discuss operations and instruments on interval effect algebras under the assumption of an unrestrictive condition.","Effects measured by operations and sequential products of effects relative to operations are considered.","Observables are introduced and coexistence of effects are discussed.","We also present properties of sequential products of observables and conditioning of observables related to instruments.","The final section is devoted to Holevo instruments.","Pure and mixed Holevo operations are defined and extended to instruments.","The Holevo sequential product of two observables is defined and the marginals of these products are computed.","We define the commutant of two effects and derive its properties.","Examples are given that illustrate the properties of previously presented concepts."],"url":"http://arxiv.org/abs/2403.18925v1","category":"quant-ph"}
{"created":"2024-03-27 18:13:16","title":"Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D","abstract":"In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets. At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images. However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging. Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization. In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions. We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method. In many cases, we even outperform state-of-the-art methods specialized for the task in question. Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization.","sentences":["In recent years, there has been an explosion of 2D vision models for numerous tasks such as semantic segmentation, style transfer or scene editing, enabled by large-scale 2D image datasets.","At the same time, there has been renewed interest in 3D scene representations such as neural radiance fields from multi-view images.","However, the availability of 3D or multiview data is still substantially limited compared to 2D image datasets, making extending 2D vision models to 3D data highly desirable but also very challenging.","Indeed, extending a single 2D vision operator like scene editing to 3D typically requires a highly creative method specialized to that task and often requires per-scene optimization.","In this paper, we ask the question of whether any 2D vision model can be lifted to make 3D consistent predictions.","We answer this question in the affirmative; our new Lift3D method trains to predict unseen views on feature spaces generated by a few visual models (i.e. DINO and CLIP), but then generalizes to novel vision operators and tasks, such as style transfer, super-resolution, open vocabulary segmentation and image colorization; for some of these tasks, there is no comparable previous 3D method.","In many cases, we even outperform state-of-the-art methods specialized for the task in question.","Moreover, Lift3D is a zero-shot method, in the sense that it requires no task-specific training, nor scene-specific optimization."],"url":"http://arxiv.org/abs/2403.18922v1","category":"cs.CV"}
{"created":"2024-03-27 18:12:24","title":"SMOF: Streaming Modern CNNs on FPGAs with Smart Off-Chip Eviction","abstract":"Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks. However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets. In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip. However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures. The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources. The proposed mechanism is incorporated into an existing toolflow, expanding the design space by utilising off-chip memory as a buffer. This enables the mapping of such modern CNNs to devices with limited on-chip memory, under the streaming architecture design approach. SMOF has demonstrated the capacity to deliver competitive and, in some cases, state-of-the-art performance across a spectrum of computer vision tasks, achieving up to 10.65 X throughput improvement compared to previous works.","sentences":["Convolutional Neural Networks (CNNs) have demonstrated their effectiveness in numerous vision tasks.","However, their high processing requirements necessitate efficient hardware acceleration to meet the application's performance targets.","In the space of FPGAs, streaming-based dataflow architectures are often adopted by users, as significant performance gains can be achieved through layer-wise pipelining and reduced off-chip memory access by retaining data on-chip.","However, modern topologies, such as the UNet, YOLO, and X3D models, utilise long skip connections, requiring significant on-chip storage and thus limiting the performance achieved by such system architectures.","The paper addresses the above limitation by introducing weight and activation eviction mechanisms to off-chip memory along the computational pipeline, taking into account the available compute and memory resources.","The proposed mechanism is incorporated into an existing toolflow, expanding the design space by utilising off-chip memory as a buffer.","This enables the mapping of such modern CNNs to devices with limited on-chip memory, under the streaming architecture design approach.","SMOF has demonstrated the capacity to deliver competitive and, in some cases, state-of-the-art performance across a spectrum of computer vision tasks, achieving up to 10.65 X throughput improvement compared to previous works."],"url":"http://arxiv.org/abs/2403.18921v1","category":"cs.AR"}
{"created":"2024-03-27 18:09:13","title":"A fundamental correlative spectroscopic study on LixNiO2 and NaNiO2","abstract":"The intimate correlation between the local atomic arrangement and electronic states in Li-ion battery cathode materials plays a crucial role in determining their electrochemical properties, including capacity, cycling stability, and rate capability. Despite almost 30 years of research efforts on high performance cathodes based on Ni rich layered oxides, there is still no consensus on LiNiO2 local atomic and electronic structure. Ni sites could be either Jahn-Teller distorted or bond disproportionated and the role of Ni and oxygen in the charge compensation mechanism remains unclear. In this study, we compare the local and electronic structure of LiNiO2 and NaNiO2, a long-range Jahn-Teller system, using a novel approach which aims at correlating the results from bulk spectroscopy techniques, particularly under operando conditions, obtained on standard samples to ensure sample interoperability and enhance the reliability and robustness of our results. Despite being a site-selective and local technique, XAS is unable to discriminate between the proposed scenarios, as confirmed also by theoretical calculations. On the contrary, Raman spectroscopy show local structural differences between monoclinic distorted NaNiO2 and rhombohedral LiNiO2. Additionally, HAXPES confirms the presence of multiple formal oxidation states for Ni, and RIXS data provides evidence of 3d8 states, confirming the negative charge transfer character of Ni and some degree of bond disproportionation in LiNiO2. Regarding the charge compensation mechanism, XRS and RIXS support the participation of oxygen holes in the redox activity, while Raman spectroscopy does not detect molecular oxygen. By combing several high-fidelity spectroscopy datasets, this study shows the value of correlative characterization workflows to provide insights into complex structural-electrochemical relationships.","sentences":["The intimate correlation between the local atomic arrangement and electronic states in Li-ion battery cathode materials plays a crucial role in determining their electrochemical properties, including capacity, cycling stability, and rate capability.","Despite almost 30 years of research efforts on high performance cathodes based on Ni rich layered oxides, there is still no consensus on LiNiO2 local atomic and electronic structure.","Ni sites could be either Jahn-Teller distorted or bond disproportionated and the role of Ni and oxygen in the charge compensation mechanism remains unclear.","In this study, we compare the local and electronic structure of LiNiO2 and NaNiO2, a long-range Jahn-Teller system, using a novel approach which aims at correlating the results from bulk spectroscopy techniques, particularly under operando conditions, obtained on standard samples to ensure sample interoperability and enhance the reliability and robustness of our results.","Despite being a site-selective and local technique, XAS is unable to discriminate between the proposed scenarios, as confirmed also by theoretical calculations.","On the contrary, Raman spectroscopy show local structural differences between monoclinic distorted NaNiO2 and rhombohedral LiNiO2.","Additionally, HAXPES confirms the presence of multiple formal oxidation states for Ni, and RIXS data provides evidence of 3d8 states, confirming the negative charge transfer character of Ni and some degree of bond disproportionation in LiNiO2.","Regarding the charge compensation mechanism, XRS and RIXS support the participation of oxygen holes in the redox activity, while Raman spectroscopy does not detect molecular oxygen.","By combing several high-fidelity spectroscopy datasets, this study shows the value of correlative characterization workflows to provide insights into complex structural-electrochemical relationships."],"url":"http://arxiv.org/abs/2403.18919v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 18:08:31","title":"Modelling the Raft Distributed Consensus Protocol in mCRL2","abstract":"The consensus problem is a fundamental problem in distributed systems. It involves a set of actors, or entities, that need to agree on some values or decisions. The Raft algorithm is a solution to the consensus problem that has gained widespread popularity as an easy-to-understand and implement alternative to Lamport's Paxos algorithm. In this paper we discuss a formalisation of the Raft algorithm and its associated correctness properties in the mCRL2 specification language.","sentences":["The consensus problem is a fundamental problem in distributed systems.","It involves a set of actors, or entities, that need to agree on some values or decisions.","The Raft algorithm is a solution to the consensus problem that has gained widespread popularity as an easy-to-understand and implement alternative to Lamport's Paxos algorithm.","In this paper we discuss a formalisation of the Raft algorithm and its associated correctness properties in the mCRL2 specification language."],"url":"http://arxiv.org/abs/2403.18916v1","category":"cs.LO"}
{"created":"2024-03-27 18:08:14","title":"PLOT-TAL -- Prompt Learning with Optimal Transport for Few-Shot Temporal Action Localization","abstract":"This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning. Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos. Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport. This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting. Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data. Our experiments demonstrate significant improvements in action localization accuracy and robustness in few-shot settings on the standard challenging datasets of THUMOS-14 and EpicKitchens100, highlighting the efficacy of our multi-prompt optimal transport approach in overcoming the challenges of conventional few-shot TAL methods.","sentences":["This paper introduces a novel approach to temporal action localization (TAL) in few-shot learning.","Our work addresses the inherent limitations of conventional single-prompt learning methods that often lead to overfitting due to the inability to generalize across varying contexts in real-world videos.","Recognizing the diversity of camera views, backgrounds, and objects in videos, we propose a multi-prompt learning framework enhanced with optimal transport.","This design allows the model to learn a set of diverse prompts for each action, capturing general characteristics more effectively and distributing the representation to mitigate the risk of overfitting.","Furthermore, by employing optimal transport theory, we efficiently align these prompts with action features, optimizing for a comprehensive representation that adapts to the multifaceted nature of video data.","Our experiments demonstrate significant improvements in action localization accuracy and robustness in few-shot settings on the standard challenging datasets of THUMOS-14 and EpicKitchens100, highlighting the efficacy of our multi-prompt optimal transport approach in overcoming the challenges of conventional few-shot TAL methods."],"url":"http://arxiv.org/abs/2403.18915v1","category":"cs.CV"}
{"created":"2024-03-27 18:03:20","title":"Emulation Techniques for Scenario and Classical Control Design of Tokamak Plasmas","abstract":"The optimisation of scenarios and design of real-time-control in tokamaks, especially for machines still in design phase, requires a comprehensive exploration of solutions to the Grad-Shafranov (GS) equation over a high-dimensional space of plasma and coil parameters. Emulators can bypass the numerical issues in the GS equation, if a large enough library of equilibria is available. We train an ensemble of neural networks to emulate the typical shape-control targets (separatrix at midplane, X-points, divertor strike point, flux expansion, poloidal beta) as a function of plasma parameters and active coil currents for the range of plasma configurations relevant to spherical tokamaks with a super-X divertor, with percent-level accuracy. This allows a quick calculation of the classical-control shape matrices, potentially allowing real-time calculation at any point in a shot with sub-ms latency. We devise a hyperparameter sampler to select the optimal network architectures and quantify uncertainties on the model predictions. To generate the relevant training set, we devise a Markov-Chain Monte Carlo algorithm to produce large libraries of forward Grad-Shafranov solutions without the need for user intervention. The algorithm promotes equilibria with desirable properties, while avoiding parameter combinations resulting in problematic profiles or numerical issues in the integration of the GS equation.","sentences":["The optimisation of scenarios and design of real-time-control in tokamaks, especially for machines still in design phase, requires a comprehensive exploration of solutions to the Grad-Shafranov (GS) equation over a high-dimensional space of plasma and coil parameters.","Emulators can bypass the numerical issues in the GS equation, if a large enough library of equilibria is available.","We train an ensemble of neural networks to emulate the typical shape-control targets (separatrix at midplane, X-points, divertor strike point, flux expansion, poloidal beta) as a function of plasma parameters and active coil currents for the range of plasma configurations relevant to spherical tokamaks with a super-X divertor, with percent-level accuracy.","This allows a quick calculation of the classical-control shape matrices, potentially allowing real-time calculation at any point in a shot with sub-ms latency.","We devise a hyperparameter sampler to select the optimal network architectures and quantify uncertainties on the model predictions.","To generate the relevant training set, we devise a Markov-Chain Monte Carlo algorithm to produce large libraries of forward Grad-Shafranov solutions without the need for user intervention.","The algorithm promotes equilibria with desirable properties, while avoiding parameter combinations resulting in problematic profiles or numerical issues in the integration of the GS equation."],"url":"http://arxiv.org/abs/2403.18912v1","category":"physics.plasm-ph"}
{"created":"2024-03-27 18:01:17","title":"Open system dynamics in interacting quantum field theories","abstract":"A quantum system that interacts with an environment generally undergoes non-unitary evolution described by a non-Markovian or Markovian master equation. In this paper, we construct the non-Markovian Redfield master equation for a quantum scalar field that interacts with a second field through a bilinear or nonlinear interaction on a Minkowski background. We use the resulting master equation to set up coupled differential equations that can be solved to obtain the equal-time two-point function of the system field. We show how the equations simplify under various approximations including the Markovian limit, and argue that the Redfield equation-based solution provides a perturbative resummation to the standard second order Dyson series result. For the bilinear interaction, we explicitly show that the Redfield solution is closer to the exact solution compared to the perturbation theory-based one. Further, the environment correlation function is oscillatory and non-decaying in this case, making the Markovian master equation a poor approximation. For the nonlinear interaction, on the other hand, the environment correlation function is sharply peaked and the Redfield solution matches that obtained using a Markovian master equation in the late-time limit.","sentences":["A quantum system that interacts with an environment generally undergoes non-unitary evolution described by a non-Markovian or Markovian master equation.","In this paper, we construct the non-Markovian Redfield master equation for a quantum scalar field that interacts with a second field through a bilinear or nonlinear interaction on a Minkowski background.","We use the resulting master equation to set up coupled differential equations that can be solved to obtain the equal-time two-point function of the system field.","We show how the equations simplify under various approximations including the Markovian limit, and argue that the Redfield equation-based solution provides a perturbative resummation to the standard second order Dyson series result.","For the bilinear interaction, we explicitly show that the Redfield solution is closer to the exact solution compared to the perturbation theory-based one.","Further, the environment correlation function is oscillatory and non-decaying in this case, making the Markovian master equation a poor approximation.","For the nonlinear interaction, on the other hand, the environment correlation function is sharply peaked and the Redfield solution matches that obtained using a Markovian master equation in the late-time limit."],"url":"http://arxiv.org/abs/2403.18907v1","category":"hep-th"}
{"created":"2024-03-27 18:00:04","title":"Toward Low-latency Iterative Decoding of QLDPC Codes Under Circuit-Level Noise","abstract":"We introduce a sliding window decoder based on belief propagation (BP) with guided decimation for the purposes of decoding quantum low-density parity-check codes in the presence of circuit-level noise. Windowed decoding keeps the decoding complexity reasonable when, as is typically the case, repeated rounds of syndrome extraction are required to decode. Within each window, we employ several rounds of BP with decimation of the variable node that we expect to be the most likely to flip in each round, Furthermore, we employ ensemble decoding to keep both decimation options (guesses) open in a small number of chosen rounds. We term the resulting decoder BP with guided decimation guessing (GDG). Applied to bivariate bicycle codes, GDG achieves a similar logical error rate as BP with an additional OSD post-processing stage (BP+OSD) and combination-sweep of order 10. For a window size of three syndrome cycles, a multi-threaded CPU implementation of GDG achieves a worst-case decoding latency of 3ms per window for the [[144,12,12]] code.","sentences":["We introduce a sliding window decoder based on belief propagation (BP) with guided decimation for the purposes of decoding quantum low-density parity-check codes in the presence of circuit-level noise.","Windowed decoding keeps the decoding complexity reasonable when, as is typically the case, repeated rounds of syndrome extraction are required to decode.","Within each window, we employ several rounds of BP with decimation of the variable node that we expect to be the most likely to flip in each round, Furthermore, we employ ensemble decoding to keep both decimation options (guesses) open in a small number of chosen rounds.","We term the resulting decoder BP with guided decimation guessing (GDG).","Applied to bivariate bicycle codes, GDG achieves a similar logical error rate as BP with an additional OSD post-processing stage (BP+OSD) and combination-sweep of order 10.","For a window size of three syndrome cycles, a multi-threaded CPU implementation of GDG achieves a worst-case decoding latency of 3ms per window for the [[144,12,12]] code."],"url":"http://arxiv.org/abs/2403.18901v1","category":"quant-ph"}
{"created":"2024-03-27 18:00:01","title":"High-resolution Spectroscopic Reconnaissance of a Temperate Sub-Neptune","abstract":"The study of temperate sub-Neptunes is the new frontier in exoplanetary science. A major development in the past year has been the first detection of carbon-bearing molecules in the atmosphere of a temperate sub-Neptune, K2-18 b, a possible Hycean world, with the James Webb Space Telescope (JWST). The JWST is poised to characterise the atmospheres of several other such planets with important implications for planetary processes in the temperate regime. Meanwhile, ground-based high-resolution spectroscopy has been highly successful in detecting chemical signatures of giant exoplanets, though low-mass planets have remained elusive. In the present work, we report the atmospheric reconnaissance of a temperate sub-Neptune using ground-based high-resolution transmission spectroscopy. The long orbital period and the low systemic velocity results in a low planetary radial velocity during transit, making this system a valuable testbed for high-resolution spectroscopy of temperate sub-Neptunes. We observe high-resolution time-series spectroscopy in the H- and K-bands during the planetary transit with the IGRINS instrument (R$\\sim$45,000) on Gemini-South. Using observations from a single transit we find marginal evidence (2.2$\\sigma$) for the presence of methane (CH$_4$) in the atmosphere and no evidence for ammonia (NH$_3$) despite its strong detectability for a cloud-free H$_2$-rich atmosphere. We assess our findings using injection tests with different atmospheric scenarios, and find them to be consistent with a high CH$_4$/NH$_3$ ratio and/or the presence of high-altitude clouds. Our results demonstrate the capability of Gemini-S/IGRINS for atmospheric characterization of temperate sub-Neptunes, and the complementarity between space- and ground-based facilities in this planetary regime.","sentences":["The study of temperate sub-Neptunes is the new frontier in exoplanetary science.","A major development in the past year has been the first detection of carbon-bearing molecules in the atmosphere of a temperate sub-Neptune, K2-18 b, a possible Hycean world, with the James Webb Space Telescope (JWST).","The JWST is poised to characterise the atmospheres of several other such planets with important implications for planetary processes in the temperate regime.","Meanwhile, ground-based high-resolution spectroscopy has been highly successful in detecting chemical signatures of giant exoplanets, though low-mass planets have remained elusive.","In the present work, we report the atmospheric reconnaissance of a temperate sub-Neptune using ground-based high-resolution transmission spectroscopy.","The long orbital period and the low systemic velocity results in a low planetary radial velocity during transit, making this system a valuable testbed for high-resolution spectroscopy of temperate sub-Neptunes.","We observe high-resolution time-series spectroscopy in the H- and K-bands during the planetary transit with the IGRINS instrument (R$\\sim$45,000) on Gemini-South.","Using observations from a single transit we find marginal evidence (2.2$\\sigma$) for the presence of methane (CH$_4$) in the atmosphere and no evidence for ammonia (NH$_3$) despite its strong detectability for a cloud-free H$_2$-rich atmosphere.","We assess our findings using injection tests with different atmospheric scenarios, and find them to be consistent with a high CH$_4$/NH$_3$ ratio and/or the presence of high-altitude clouds.","Our results demonstrate the capability of Gemini-S/IGRINS for atmospheric characterization of temperate sub-Neptunes, and the complementarity between space- and ground-based facilities in this planetary regime."],"url":"http://arxiv.org/abs/2403.18891v1","category":"astro-ph.EP"}
{"created":"2024-03-27 18:00:00","title":"Estimating Galaxy Parameters with Self-Organizing Maps and the Effect of Missing Data","abstract":"The current and upcoming large data volume galaxy surveys require the use of machine learning techniques to maximize their scientific return. This study explores the use of Self-Organizing Maps (SOMs) to estimate galaxy parameters with a focus on handling cases of missing data and providing realistic probability distribution functions for the parameters. We train a SOM with a simulated mass-limited lightcone assuming a ugrizYJHKs+IRAC dataset, mimicking the Hyper Suprime-Cam (HSC) Deep joint dataset. For parameter estimation, we derive SOM likelihood surfaces considering photometric errors to derive total (statistical and systematic) uncertainties. We explore the effects of missing data including which bands are particular critical to the accuracy of the derived parameters. We demonstrate that the parameter recovery is significantly better when the missing bands are \"filled-in\" rather than if they are completely omitted. We propose a practical method for such recovery of missing data.","sentences":["The current and upcoming large data volume galaxy surveys require the use of machine learning techniques to maximize their scientific return.","This study explores the use of Self-Organizing Maps (SOMs) to estimate galaxy parameters with a focus on handling cases of missing data and providing realistic probability distribution functions for the parameters.","We train a SOM with a simulated mass-limited lightcone assuming a ugrizYJHKs+IRAC dataset, mimicking the Hyper Suprime-Cam (HSC) Deep joint dataset.","For parameter estimation, we derive SOM likelihood surfaces considering photometric errors to derive total (statistical and systematic) uncertainties.","We explore the effects of missing data including which bands are particular critical to the accuracy of the derived parameters.","We demonstrate that the parameter recovery is significantly better when the missing bands are \"filled-in\" rather than if they are completely omitted.","We propose a practical method for such recovery of missing data."],"url":"http://arxiv.org/abs/2403.18888v1","category":"astro-ph.GA"}
{"created":"2024-03-28 16:24:44","title":"Cosmological inference from combining Planck and ACT cluster counts","abstract":"We have adapted the Planck cluster likelihood in such a way that it can be applied to the sample of clusters detected by the Atacama Cosmology Telescope (ACT). Applying it to the 2016 sample from Planck and the 2018 sample from ACT we find, by fixing the cosmology using CMB observations and the cluster model adopted by Planck, that the mass bias required by the two are $1-b_{\\rm Planck}=0.61\\pm 0.03$ and $1-b_{\\rm ACT}=0.75\\pm 0.06$. These are broadly in agreement but hint that the model could be adapted to reach a better agreement. By normalizing the cluster model using weak lensing observations, we find evidence for either evolution in the cluster model, quantified by the cluster modeling parameter describing redshift dependence $\\beta=0.86 \\pm 0.07$ using an updated CCCP-based normalization, or evolution in the cosmological model quantified by the dark energy equation of state parameter $w=-0.82 \\pm 0.07$.","sentences":["We have adapted the Planck cluster likelihood in such a way that it can be applied to the sample of clusters detected by the Atacama Cosmology Telescope (ACT).","Applying it to the 2016 sample from Planck and the 2018 sample from ACT we find, by fixing the cosmology using CMB observations and the cluster model adopted by Planck, that the mass bias required by the two are $1-b_{\\rm Planck}=0.61\\pm 0.03$ and $1-b_{\\rm ACT}=0.75\\pm 0.06$.","These are broadly in agreement but hint that the model could be adapted to reach a better agreement.","By normalizing the cluster model using weak lensing observations, we find evidence for either evolution in the cluster model, quantified by the cluster modeling parameter describing redshift dependence $\\beta=0.86 \\pm 0.07$ using an updated CCCP-based normalization, or evolution in the cosmological model quantified by the dark energy equation of state parameter $w=-0.82 \\pm 0.07$."],"url":"http://arxiv.org/abs/2403.19542v1","category":"astro-ph.CO"}
{"created":"2024-03-28 12:18:15","title":"Coordinated Allocation of Radio Resources to Wi-Fi and Cellular Technologies in Shared Unlicensed Frequencies","abstract":"Wireless connectivity is essential for industrial production processes and workflow management. Moreover, the connectivity requirements of industrial devices, which are usually long-term investments, are diverse and require different radio interfaces. In this regard, the 3GPP has studied how to support heterogeneous radio access technologies (RATs) such as Wi-Fi and unlicensed cellular technologies in 5G core networks. In some cases, these technologies coexist in the same spectrum. Dynamic spectrum sharing (DSS), which has already been proven to increase spectrum efficiency in licensed bands, can also be applied to this scenario. In this paper, we propose two solutions for mobile network operators (MNOs) or service providers to dynamically divide (multiplex) the radio resources of a shared channel between a Wi-Fi basic service set (BSS) and one or several carriers of scheduled wireless networks, such as cellular technologies, with a configurable level of sharing granularity. These solutions do not require modifications to the current commercial off-the-shelf (COTS) end devices. We adapt the existing IEEE 802.11 procedures to notify the Wi-Fi stations that they must share channels with different access networks. We demonstrate that our dynamic sharing proposals are also advantageous over direct coexistence and evaluate each of them quantitatively and qualitatively to determine when one or the other is preferable. The evaluation is particularized for IEEE 802.11ac and long-term evolution (LTE) license assisted access (LAA), but the solutions can be easily extended to 5G new radio-unlicensed (5G NR-U) or to any other wireless technology in which the network side schedules end device transmissions.","sentences":["Wireless connectivity is essential for industrial production processes and workflow management.","Moreover, the connectivity requirements of industrial devices, which are usually long-term investments, are diverse and require different radio interfaces.","In this regard, the 3GPP has studied how to support heterogeneous radio access technologies (RATs) such as Wi-Fi and unlicensed cellular technologies in 5G core networks.","In some cases, these technologies coexist in the same spectrum.","Dynamic spectrum sharing (DSS), which has already been proven to increase spectrum efficiency in licensed bands, can also be applied to this scenario.","In this paper, we propose two solutions for mobile network operators (MNOs) or service providers to dynamically divide (multiplex) the radio resources of a shared channel between a Wi-Fi basic service set (BSS) and one or several carriers of scheduled wireless networks, such as cellular technologies, with a configurable level of sharing granularity.","These solutions do not require modifications to the current commercial off-the-shelf (COTS) end devices.","We adapt the existing IEEE 802.11 procedures to notify the Wi-Fi stations that they must share channels with different access networks.","We demonstrate that our dynamic sharing proposals are also advantageous over direct coexistence and evaluate each of them quantitatively and qualitatively to determine when one or the other is preferable.","The evaluation is particularized for IEEE 802.11ac and long-term evolution (LTE) license assisted access (LAA), but the solutions can be easily extended to 5G new radio-unlicensed (5G NR-U) or to any other wireless technology in which the network side schedules end device transmissions."],"url":"http://arxiv.org/abs/2403.19359v1","category":"cs.NI"}
{"created":"2024-03-28 11:33:02","title":"MedBN: Robust Test-Time Adaptation against Malicious Test Samples","abstract":"Test-time adaptation (TTA) has emerged as a promising solution to address performance decay due to unforeseen distribution shifts between training and test data. While recent TTA methods excel in adapting to test data variations, such adaptability exposes a model to vulnerability against malicious examples, an aspect that has received limited attention. Previous studies have uncovered security vulnerabilities within TTA even when a small proportion of the test batch is maliciously manipulated. In response to the emerging threat, we propose median batch normalization (MedBN), leveraging the robustness of the median for statistics estimation within the batch normalization layer during test-time inference. Our method is algorithm-agnostic, thus allowing seamless integration with existing TTA frameworks. Our experimental results on benchmark datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently demonstrate that MedBN outperforms existing approaches in maintaining robust performance across different attack scenarios, encompassing both instant and cumulative attacks. Through extensive experiments, we show that our approach sustains the performance even in the absence of attacks, achieving a practical balance between robustness and performance.","sentences":["Test-time adaptation (TTA) has emerged as a promising solution to address performance decay due to unforeseen distribution shifts between training and test data.","While recent TTA methods excel in adapting to test data variations, such adaptability exposes a model to vulnerability against malicious examples, an aspect that has received limited attention.","Previous studies have uncovered security vulnerabilities within TTA even when a small proportion of the test batch is maliciously manipulated.","In response to the emerging threat, we propose median batch normalization (MedBN), leveraging the robustness of the median for statistics estimation within the batch normalization layer during test-time inference.","Our method is algorithm-agnostic, thus allowing seamless integration with existing TTA frameworks.","Our experimental results on benchmark datasets, including CIFAR10-C, CIFAR100-C and ImageNet-C, consistently demonstrate that MedBN outperforms existing approaches in maintaining robust performance across different attack scenarios, encompassing both instant and cumulative attacks.","Through extensive experiments, we show that our approach sustains the performance even in the absence of attacks, achieving a practical balance between robustness and performance."],"url":"http://arxiv.org/abs/2403.19326v1","category":"cs.LG"}
{"created":"2024-03-28 11:26:45","title":"Rigid and shaky hard link diagrams","abstract":"In this study of the Reidemeister moves within the classical knot theory, we focus on hard diagrams of knots and links, categorizing them as either rigid or shaky based on their adaptability to certain moves. We establish that every link possesses a diagram that is a rigid hard diagram and we provide an upper limit for the number of crossings in such diagrams. Furthermore, we investigate rigid hard diagrams for specific knots or links to determine their rigid hard index. In the topic of shaky hard diagrams, we demonstrate the existence of such diagrams for the unknot and unlink, regardless of the number of components, and present examples of shaky hard diagrams.","sentences":["In this study of the Reidemeister moves within the classical knot theory, we focus on hard diagrams of knots and links, categorizing them as either rigid or shaky based on their adaptability to certain moves.","We establish that every link possesses a diagram that is a rigid hard diagram and we provide an upper limit for the number of crossings in such diagrams.","Furthermore, we investigate rigid hard diagrams for specific knots or links to determine their rigid hard index.","In the topic of shaky hard diagrams, we demonstrate the existence of such diagrams for the unknot and unlink, regardless of the number of components, and present examples of shaky hard diagrams."],"url":"http://arxiv.org/abs/2403.19323v1","category":"math.GT"}
{"created":"2024-03-28 10:02:08","title":"CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection","abstract":"Domain adaptive object detection aims to adapt detection models to domains where annotated data is unavailable. Existing methods have been proposed to address the domain gap using the semi-supervised student-teacher framework. However, a fundamental issue arises from the class imbalance in the labelled training set, which can result in inaccurate pseudo-labels. The relationship between classes, especially where one class is a majority and the other minority, has a large impact on class bias. We propose Class-Aware Teacher (CAT) to address the class bias issue in the domain adaptation setting. In our work, we approximate the class relationships with our Inter-Class Relation module (ICRm) and exploit it to reduce the bias within the model. In this way, we are able to apply augmentations to highly related classes, both inter- and intra-domain, to boost the performance of minority classes while having minimal impact on majority classes. We further reduce the bias by implementing a class-relation weight to our classification loss. Experiments conducted on various datasets and ablation studies show that our method is able to address the class bias in the domain adaptation setting. On the Cityscapes to Foggy Cityscapes dataset, we attained a 52.5 mAP, a substantial improvement over the 51.2 mAP achieved by the state-of-the-art method.","sentences":["Domain adaptive object detection aims to adapt detection models to domains where annotated data is unavailable.","Existing methods have been proposed to address the domain gap using the semi-supervised student-teacher framework.","However, a fundamental issue arises from the class imbalance in the labelled training set, which can result in inaccurate pseudo-labels.","The relationship between classes, especially where one class is a majority and the other minority, has a large impact on class bias.","We propose Class-Aware Teacher (CAT) to address the class bias issue in the domain adaptation setting.","In our work, we approximate the class relationships with our Inter-Class Relation module (ICRm) and exploit it to reduce the bias within the model.","In this way, we are able to apply augmentations to highly related classes, both inter- and intra-domain, to boost the performance of minority classes while having minimal impact on majority classes.","We further reduce the bias by implementing a class-relation weight to our classification loss.","Experiments conducted on various datasets and ablation studies show that our method is able to address the class bias in the domain adaptation setting.","On the Cityscapes to Foggy Cityscapes dataset, we attained a 52.5 mAP, a substantial improvement over the 51.2 mAP achieved by the state-of-the-art method."],"url":"http://arxiv.org/abs/2403.19278v1","category":"cs.CV"}
{"created":"2024-03-28 08:58:20","title":"Sine Activated Low-Rank Matrices for Parameter Efficient Learning","abstract":"Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling. This demonstrates the wide-ranging potential and efficiency of our proposed technique.","sentences":["Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning.","These techniques significantly lower the number of parameters, striking a balance between compactness and performance.","However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts.","In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process.","This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy.","Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful application in Vision Transformers (ViT), Large Language Models (LLMs), Neural Radiance Fields (NeRF), and 3D shape modeling.","This demonstrates the wide-ranging potential and efficiency of our proposed technique."],"url":"http://arxiv.org/abs/2403.19243v1","category":"cs.LG"}
{"created":"2024-03-28 08:36:54","title":"Computing large deviation rate functions of entropy production for diffusion processes in the vanishing-noise limit and high dimensions by an interacting particle method","abstract":"We study an interacting particle method (IPM) for computing the large deviation rate function of entropy production for diffusion processes, with emphasis on the vanishing-noise limit and high dimensions. The crucial ingredient to obtain the rate function is the computation of the principal eigenvalue $\\lambda$ of elliptic, non-self-adjoint operators. We show that this principal eigenvalue can be approximated in terms of the spectral radius of a discretized evolution operator obtained from an operator splitting scheme and an Euler--Maruyama scheme with a small time step size, and we show that this spectral radius can be accessed through a large number of iterations of this discretized semigroup, suitable for the IPM. The IPM applies naturally to problems in unbounded domains, scales easily to high dimensions, and adapts to singular behaviors in the vanishing-noise limit. We show numerical examples in dimensions up to 16. The numerical results show that our numerical approximation of $\\lambda$ converges to the analytical vanishing-noise limit with a fixed number of particles and a fixed time step size. Our paper appears to be the first one to obtain numerical results of principal eigenvalue problems for non-self-adjoint operators in such high dimensions.","sentences":["We study an interacting particle method (IPM) for computing the large deviation rate function of entropy production for diffusion processes, with emphasis on the vanishing-noise limit and high dimensions.","The crucial ingredient to obtain the rate function is the computation of the principal eigenvalue $\\lambda$ of elliptic, non-self-adjoint operators.","We show that this principal eigenvalue can be approximated in terms of the spectral radius of a discretized evolution operator obtained from an operator splitting scheme and an Euler--Maruyama scheme with a small time step size, and we show that this spectral radius can be accessed through a large number of iterations of this discretized semigroup, suitable for the IPM.","The IPM applies naturally to problems in unbounded domains, scales easily to high dimensions, and adapts to singular behaviors in the vanishing-noise limit.","We show numerical examples in dimensions up to 16.","The numerical results show that our numerical approximation of $\\lambda$ converges to the analytical vanishing-noise limit with a fixed number of particles and a fixed time step size.","Our paper appears to be the first one to obtain numerical results of principal eigenvalue problems for non-self-adjoint operators in such high dimensions."],"url":"http://arxiv.org/abs/2403.19223v1","category":"math.NA"}
{"created":"2024-03-28 08:06:48","title":"From Activation to Initialization: Scaling Insights for Optimizing Neural Fields","abstract":"In the realm of computer vision, Neural Fields have gained prominence as a contemporary tool harnessing neural networks for signal representation. Despite the remarkable progress in adapting these networks to solve a variety of problems, the field still lacks a comprehensive theoretical framework. This article aims to address this gap by delving into the intricate interplay between initialization and activation, providing a foundational basis for the robust optimization of Neural Fields. Our theoretical insights reveal a deep-seated connection among network initialization, architectural choices, and the optimization process, emphasizing the need for a holistic approach when designing cutting-edge Neural Fields.","sentences":["In the realm of computer vision, Neural Fields have gained prominence as a contemporary tool harnessing neural networks for signal representation.","Despite the remarkable progress in adapting these networks to solve a variety of problems, the field still lacks a comprehensive theoretical framework.","This article aims to address this gap by delving into the intricate interplay between initialization and activation, providing a foundational basis for the robust optimization of Neural Fields.","Our theoretical insights reveal a deep-seated connection among network initialization, architectural choices, and the optimization process, emphasizing the need for a holistic approach when designing cutting-edge Neural Fields."],"url":"http://arxiv.org/abs/2403.19205v1","category":"cs.CV"}
{"created":"2024-03-28 02:59:45","title":"Local smoothing for the Hermite wave equation","abstract":"We show local smoothing estimates in $L^p$-spaces for solutions to the Hermite wave equation. For this purpose, we obtain a parametrix given by a Fourier Integral Operator, which we linearize. This leads us to analyze local smoothing estimates for solutions to Klein-Gordon equations. We show $\\ell^2$-decoupling estimates adapted to the mass parameter to obtain local smoothing with essentially sharp derivative loss. In one dimension as consequence of square function estimates, we obtain estimates sharp up to endpoints. Finally, we elaborate on the implications of local smoothing estimates for Hermite Bochner--Riesz means.","sentences":["We show local smoothing estimates in $L^p$-spaces for solutions to the Hermite wave equation.","For this purpose, we obtain a parametrix given by a Fourier Integral Operator, which we linearize.","This leads us to analyze local smoothing estimates for solutions to Klein-Gordon equations.","We show $\\ell^2$-decoupling estimates adapted to the mass parameter to obtain local smoothing with essentially sharp derivative loss.","In one dimension as consequence of square function estimates, we obtain estimates sharp up to endpoints.","Finally, we elaborate on the implications of local smoothing estimates for Hermite Bochner--Riesz means."],"url":"http://arxiv.org/abs/2403.19108v1","category":"math.AP"}
{"created":"2024-03-28 01:00:08","title":"A Real-Time Framework for Domain-Adaptive Underwater Object Detection with Image Enhancement","abstract":"In recent years, significant progress has been made in the field of underwater image enhancement (UIE). However, its practical utility for high-level vision tasks, such as underwater object detection (UOD) in Autonomous Underwater Vehicles (AUVs), remains relatively unexplored. It may be attributed to several factors: (1) Existing methods typically employ UIE as a pre-processing step, which inevitably introduces considerable computational overhead and latency. (2) The process of enhancing images prior to training object detectors may not necessarily yield performance improvements. (3) The complex underwater environments can induce significant domain shifts across different scenarios, seriously deteriorating the UOD performance. To address these challenges, we introduce EnYOLO, an integrated real-time framework designed for simultaneous UIE and UOD with domain-adaptation capability. Specifically, both the UIE and UOD task heads share the same network backbone and utilize a lightweight design. Furthermore, to ensure balanced training for both tasks, we present a multi-stage training strategy aimed at consistently enhancing their performance. Additionally, we propose a novel domain-adaptation strategy to align feature embeddings originating from diverse underwater environments. Comprehensive experiments demonstrate that our framework not only achieves state-of-the-art (SOTA) performance in both UIE and UOD tasks, but also shows superior adaptability when applied to different underwater scenarios. Our efficiency analysis further highlights the substantial potential of our framework for onboard deployment.","sentences":["In recent years, significant progress has been made in the field of underwater image enhancement (UIE).","However, its practical utility for high-level vision tasks, such as underwater object detection (UOD) in Autonomous Underwater Vehicles (AUVs), remains relatively unexplored.","It may be attributed to several factors: (1) Existing methods typically employ UIE as a pre-processing step, which inevitably introduces considerable computational overhead and latency.","(2) The process of enhancing images prior to training object detectors may not necessarily yield performance improvements.","(3) The complex underwater environments can induce significant domain shifts across different scenarios, seriously deteriorating the UOD performance.","To address these challenges, we introduce EnYOLO, an integrated real-time framework designed for simultaneous UIE and UOD with domain-adaptation capability.","Specifically, both the UIE and UOD task heads share the same network backbone and utilize a lightweight design.","Furthermore, to ensure balanced training for both tasks, we present a multi-stage training strategy aimed at consistently enhancing their performance.","Additionally, we propose a novel domain-adaptation strategy to align feature embeddings originating from diverse underwater environments.","Comprehensive experiments demonstrate that our framework not only achieves state-of-the-art (SOTA) performance in both UIE and UOD tasks, but also shows superior adaptability when applied to different underwater scenarios.","Our efficiency analysis further highlights the substantial potential of our framework for onboard deployment."],"url":"http://arxiv.org/abs/2403.19079v1","category":"cs.CV"}
{"created":"2024-03-28 00:48:08","title":"Blockchains, MEV and the knapsack problem: a primer","abstract":"In this paper, we take a close look at a problem labeled maximal extractable value (MEV), which arises in a blockchain due to the ability of a block producer to manipulate the order of transactions within a block. Indeed, blockchains such as Ethereum have spent considerable resources addressing this issue and have redesigned the block production process to account for MEV. This paper provides an overview of the MEV problem and tracks how Ethereum has adapted to its presence. A vital aspect of the block building exercise is that it is a variant of the knapsack problem. Consequently, this paper highlights the role of designing auctions to fill a knapsack--or knapsack auctions--in alleviating the MEV problem. Overall, this paper presents a survey of the main issues and an accessible primer for researchers and students wishing to explore the economics of block building and MEV further.","sentences":["In this paper, we take a close look at a problem labeled maximal extractable value (MEV), which arises in a blockchain due to the ability of a block producer to manipulate the order of transactions within a block.","Indeed, blockchains such as Ethereum have spent considerable resources addressing this issue and have redesigned the block production process to account for MEV.","This paper provides an overview of the MEV problem and tracks how Ethereum has adapted to its presence.","A vital aspect of the block building exercise is that it is a variant of the knapsack problem.","Consequently, this paper highlights the role of designing auctions to fill a knapsack--or knapsack auctions--in alleviating the MEV problem.","Overall, this paper presents a survey of the main issues and an accessible primer for researchers and students wishing to explore the economics of block building and MEV further."],"url":"http://arxiv.org/abs/2403.19077v1","category":"econ.GN"}
{"created":"2024-03-28 00:14:53","title":"Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach","abstract":"Parameter-efficient fine-tuning for pre-trained Vision Transformers aims to adeptly tailor a model to downstream tasks by learning a minimal set of new adaptation parameters while preserving the frozen majority of pre-trained parameters. Striking a balance between retaining the generalizable representation capacity of the pre-trained model and acquiring task-specific features poses a key challenge. Currently, there is a lack of focus on guiding this delicate trade-off. In this study, we approach the problem from the perspective of Singular Value Decomposition (SVD) of pre-trained parameter matrices, providing insights into the tuning dynamics of existing methods. Building upon this understanding, we propose a Residual-based Low-Rank Rescaling (RLRR) fine-tuning strategy. This strategy not only enhances flexibility in parameter tuning but also ensures that new parameters do not deviate excessively from the pre-trained model through a residual design. Extensive experiments demonstrate that our method achieves competitive performance across various downstream image classification tasks, all while maintaining comparable new parameters. We believe this work takes a step forward in offering a unified perspective for interpreting existing methods and serves as motivation for the development of new approaches that move closer to effectively considering the crucial trade-off mentioned above. Our code is available at \\href{https://github.com/zstarN70/RLRR.git}{https://github.com/zstarN70/RLRR.git}.","sentences":["Parameter-efficient fine-tuning for pre-trained Vision Transformers aims to adeptly tailor a model to downstream tasks by learning a minimal set of new adaptation parameters while preserving the frozen majority of pre-trained parameters.","Striking a balance between retaining the generalizable representation capacity of the pre-trained model and acquiring task-specific features poses a key challenge.","Currently, there is a lack of focus on guiding this delicate trade-off.","In this study, we approach the problem from the perspective of Singular Value Decomposition (SVD) of pre-trained parameter matrices, providing insights into the tuning dynamics of existing methods.","Building upon this understanding, we propose a Residual-based Low-Rank Rescaling (RLRR) fine-tuning strategy.","This strategy not only enhances flexibility in parameter tuning but also ensures that new parameters do not deviate excessively from the pre-trained model through a residual design.","Extensive experiments demonstrate that our method achieves competitive performance across various downstream image classification tasks, all while maintaining comparable new parameters.","We believe this work takes a step forward in offering a unified perspective for interpreting existing methods and serves as motivation for the development of new approaches that move closer to effectively considering the crucial trade-off mentioned above.","Our code is available at \\href{https://github.com/zstarN70/RLRR.git}{https://github.com/zstarN70/RLRR.git}."],"url":"http://arxiv.org/abs/2403.19067v1","category":"cs.CV"}
{"created":"2024-03-28 00:03:54","title":"Instruction-based Hypergraph Pretraining","abstract":"Pretraining has been widely explored to augment the adaptability of graph learning models to transfer knowledge from large datasets to a downstream task, such as link prediction or classification. However, the gap between training objectives and the discrepancy between data distributions in pretraining and downstream tasks hinders the transfer of the pretrained knowledge. Inspired by instruction-based prompts widely used in pretrained language models, we introduce instructions into graph pretraining. In this paper, we propose a novel pretraining framework named Instruction-based Hypergraph Pretraining. To overcome the discrepancy between pretraining and downstream tasks, text-based instructions are applied to provide explicit guidance on specific tasks for representation learning. Compared to learnable prompts, whose effectiveness depends on the quality and the diversity of training data, text-based instructions intrinsically encapsulate task information and support the model to generalize beyond the structure seen during pretraining. To capture high-order relations with task information in a context-aware manner, a novel prompting hypergraph convolution layer is devised to integrate instructions into information propagation in hypergraphs. Extensive experiments conducted on three public datasets verify the superiority of IHP in various scenarios.","sentences":["Pretraining has been widely explored to augment the adaptability of graph learning models to transfer knowledge from large datasets to a downstream task, such as link prediction or classification.","However, the gap between training objectives and the discrepancy between data distributions in pretraining and downstream tasks hinders the transfer of the pretrained knowledge.","Inspired by instruction-based prompts widely used in pretrained language models, we introduce instructions into graph pretraining.","In this paper, we propose a novel pretraining framework named Instruction-based Hypergraph Pretraining.","To overcome the discrepancy between pretraining and downstream tasks, text-based instructions are applied to provide explicit guidance on specific tasks for representation learning.","Compared to learnable prompts, whose effectiveness depends on the quality and the diversity of training data, text-based instructions intrinsically encapsulate task information and support the model to generalize beyond the structure seen during pretraining.","To capture high-order relations with task information in a context-aware manner, a novel prompting hypergraph convolution layer is devised to integrate instructions into information propagation in hypergraphs.","Extensive experiments conducted on three public datasets verify the superiority of IHP in various scenarios."],"url":"http://arxiv.org/abs/2403.19063v1","category":"cs.IR"}
{"created":"2024-03-27 20:53:15","title":"Finding Birkhoff Averages via Adaptive Filtering","abstract":"In many applications, one is interested in classifying trajectories of Hamiltonian systems as invariant tori, islands, or chaos. The convergence rate of ergodic Birkhoff averages can be used to categorize these regions, but many iterations of the return map are needed to implement this directly. Recently, it has been shown that a weighted Birkhoff average can be used to accelerate the convergence, resulting in a useful method for categorizing trajectories.   In this paper, we show how a modified version the reduced rank extrapolation method (named Birkhoff RRE) can also be used to find optimal weights for the weighted average with a single linear least-squares solve.Using these, we classify trajectories with fewer iterations of the map than the standard weighted Birkhoff average. Furthermore, for the islands and invariant circles, a subsequent eigenvalue problem gives the number of islands and the rotation number. Using these numbers, we find Fourier parameterizations of invariant circles and islands. We show examples of Birkhoff RRE on the standard map and on magnetic field line dynamics.","sentences":["In many applications, one is interested in classifying trajectories of Hamiltonian systems as invariant tori, islands, or chaos.","The convergence rate of ergodic Birkhoff averages can be used to categorize these regions, but many iterations of the return map are needed to implement this directly.","Recently, it has been shown that a weighted Birkhoff average can be used to accelerate the convergence, resulting in a useful method for categorizing trajectories.   ","In this paper, we show how a modified version the reduced rank extrapolation method (named Birkhoff RRE) can also be used to find optimal weights for the weighted average with a single linear least-squares solve.","Using these, we classify trajectories with fewer iterations of the map than the standard weighted Birkhoff average.","Furthermore, for the islands and invariant circles, a subsequent eigenvalue problem gives the number of islands and the rotation number.","Using these numbers, we find Fourier parameterizations of invariant circles and islands.","We show examples of Birkhoff RRE on the standard map and on magnetic field line dynamics."],"url":"http://arxiv.org/abs/2403.19003v1","category":"math.DS"}
{"created":"2024-03-27 20:24:18","title":"Parallel refreshed cryogenic charge-locking array with low power dissipation","abstract":"To build a large scale quantum circuit comprising millions of cryogenic qubits will require an efficient way to supply large numbers of classic control signals. Given the limited number of direct connections allowed from room temperature, multiple level of signal multiplexing becomes essential. The stacking of hardware to accomplish this task is highly dependent on the lowest level implementation of control electronics, of which an open question is the feasibility of mK integration. Such integration is preferred for signal transmission and wire interconnection, provided it is not limited by the large power dissipation involved. Novel cryogenic electronics that prioritises power efficiency has to be developed to meet the tight thermal budget. In this paper, we present a power efficient approach to implement charge-locking array.","sentences":["To build a large scale quantum circuit comprising millions of cryogenic qubits will require an efficient way to supply large numbers of classic control signals.","Given the limited number of direct connections allowed from room temperature, multiple level of signal multiplexing becomes essential.","The stacking of hardware to accomplish this task is highly dependent on the lowest level implementation of control electronics, of which an open question is the feasibility of mK integration.","Such integration is preferred for signal transmission and wire interconnection, provided it is not limited by the large power dissipation involved.","Novel cryogenic electronics that prioritises power efficiency has to be developed to meet the tight thermal budget.","In this paper, we present a power efficient approach to implement charge-locking array."],"url":"http://arxiv.org/abs/2403.18993v1","category":"quant-ph"}
{"created":"2024-03-27 18:38:55","title":"Increasing the raw contrast of VLT/SPHERE with dark hole techniques III. Broadband reference differential imaging of HR\\,4796 using a four-quadrant phase mask","abstract":"Imaging exoplanetary systems is essential to characterizing exoplanetary systems and to studying planet-disk interactions to understand planet formation. Such imaging in the visible and near-infrared is challenging because these objects are very faint relative to their star and only fractions of an arcsecond away. Coronagraphic instruments have already allowed the imaging of a few exoplanets, but their performance is limited by wavefront aberrations. Adaptive optics systems partly compensate for the Earth's atmosphere turbulence, but they cannot fully control the wavefront. Some of the starlight leaks through the coronagraph and forms speckles in the image. Focal plane wavefront control, used as a second stage after the adaptive optics system, can minimize the speckle intensity within an area called the dark hole. We demonstrated the on-sky performance of dark hole techniques, pairwise probing coupled with electric field conjugation, using the apodized pupil Lyot coronagraph of the VLT/SPHERE instrument. In this paper, we probe their performance using the SPHERE four-quadrant phase mask coronagraph. We demonstrate the interest of combining dark hole techniques and reference differential imaging (RDI). We create a dark hole on-sky in the narrow band around~$1.7\\,\\mu$m observing HR\\,4796. We then record broadband images of HR\\,4796 and a reference star at the H band. The dark hole techniques improve the H-band detection limit by a factor of three. The dark hole is stable from one star to a nearby star enabling RDI. This stability offers two new strategies of observation. First, one can quickly create a dark hole observing a bright star before pointing to a faint target star. Furthermore, one can couple dark hole techniques and RDI. A very interesting point is that the performance of these methods does not depend on the astrophysical signal.","sentences":["Imaging exoplanetary systems is essential to characterizing exoplanetary systems and to studying planet-disk interactions to understand planet formation.","Such imaging in the visible and near-infrared is challenging because these objects are very faint relative to their star and only fractions of an arcsecond away.","Coronagraphic instruments have already allowed the imaging of a few exoplanets, but their performance is limited by wavefront aberrations.","Adaptive optics systems partly compensate for the Earth's atmosphere turbulence, but they cannot fully control the wavefront.","Some of the starlight leaks through the coronagraph and forms speckles in the image.","Focal plane wavefront control, used as a second stage after the adaptive optics system, can minimize the speckle intensity within an area called the dark hole.","We demonstrated the on-sky performance of dark hole techniques, pairwise probing coupled with electric field conjugation, using the apodized pupil Lyot coronagraph of the VLT/SPHERE instrument.","In this paper, we probe their performance using the SPHERE four-quadrant phase mask coronagraph.","We demonstrate the interest of combining dark hole techniques and reference differential imaging (RDI).","We create a dark hole on-sky in the narrow band around~$1.7\\,\\mu$m observing HR\\,4796.","We then record broadband images of HR\\,4796 and a reference star at the H band.","The dark hole techniques improve the H-band detection limit by a factor of three.","The dark hole is stable from one star to a nearby star enabling RDI.","This stability offers two new strategies of observation.","First, one can quickly create a dark hole observing a bright star before pointing to a faint target star.","Furthermore, one can couple dark hole techniques and RDI.","A very interesting point is that the performance of these methods does not depend on the astrophysical signal."],"url":"http://arxiv.org/abs/2403.18939v1","category":"astro-ph.IM"}
{"created":"2024-03-27 17:59:21","title":"Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning","abstract":"Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts. However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decides to reuse or add adapter modules on demand in continual learning, depending on whether drastic distribution shift that could not be handled by existing modules is detected at different representation levels. We design each adapter module to consist of an adapter and a representation descriptor, specifically, implemented as an autoencoder. The representation descriptor functions as a distributional shift indicator during training and triggers adapter expansion. For better usage of the adapters, an expandable weighting router is learned jointly for mixture of adapter outputs. By comparing with vision-transformer-based continual learning adaptation methods, we demonstrate that the proposed framework outperforms the state-of-the-art without memory rehearsal.","sentences":["Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge.","While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts.","However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility.","The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning.","We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decides to reuse or add adapter modules on demand in continual learning, depending on whether drastic distribution shift that could not be handled by existing modules is detected at different representation levels.","We design each adapter module to consist of an adapter and a representation descriptor, specifically, implemented as an autoencoder.","The representation descriptor functions as a distributional shift indicator during training and triggers adapter expansion.","For better usage of the adapters, an expandable weighting router is learned jointly for mixture of adapter outputs.","By comparing with vision-transformer-based continual learning adaptation methods, we demonstrate that the proposed framework outperforms the state-of-the-art without memory rehearsal."],"url":"http://arxiv.org/abs/2403.18886v1","category":"cs.LG"}
{"created":"2024-03-28 17:13:47","title":"Genetic Quantization-Aware Approximation for Non-Linear Operations in Transformers","abstract":"Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs. Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization. This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness. The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models. Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alternatives. Code is available at https:// github.com/PingchengDong/GQA-LUT.","sentences":["Non-linear functions are prevalent in Transformers and their lightweight variants, incurring substantial and frequently underestimated hardware costs.","Previous state-of-the-art works optimize these operations by piece-wise linear approximation and store the parameters in look-up tables (LUT), but most of them require unfriendly high-precision arithmetics such as FP/INT 32 and lack consideration of integer-only INT quantization.","This paper proposed a genetic LUT-Approximation algorithm namely GQA-LUT that can automatically determine the parameters with quantization awareness.","The results demonstrate that GQA-LUT achieves negligible degradation on the challenging semantic segmentation task for both vanilla and linear Transformer models.","Besides, proposed GQA-LUT enables the employment of INT8-based LUT-Approximation that achieves an area savings of 81.3~81.7% and a power reduction of 79.3~80.2% compared to the high-precision FP/INT 32 alternatives.","Code is available at https:// github.com/PingchengDong/GQA-LUT."],"url":"http://arxiv.org/abs/2403.19591v1","category":"cs.LG"}
{"created":"2024-03-28 17:12:39","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","abstract":"This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures. We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities. Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures. We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts. Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage. Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation. Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs. Our code is available at https://github.com/naver-ai/rdnet.","sentences":["This paper revives Densely Connected Convolutional Networks (DenseNets) and reveals the underrated effectiveness over predominant ResNet-style architectures.","We believe DenseNets' potential was overlooked due to untouched training methods and traditional design elements not fully revealing their capabilities.","Our pilot study shows dense connections through concatenation are strong, demonstrating that DenseNets can be revitalized to compete with modern architectures.","We methodically refine suboptimal components - architectural adjustments, block redesign, and improved training recipes towards widening DenseNets and boosting memory efficiency while keeping concatenation shortcuts.","Our models, employing simple architectural elements, ultimately surpass Swin Transformer, ConvNeXt, and DeiT-III - key architectures in the residual learning lineage.","Furthermore, our models exhibit near state-of-the-art performance on ImageNet-1K, competing with the very recent models and downstream tasks, ADE20k semantic segmentation, and COCO object detection/instance segmentation.","Finally, we provide empirical analyses that uncover the merits of the concatenation over additive shortcuts, steering a renewed preference towards DenseNet-style designs.","Our code is available at https://github.com/naver-ai/rdnet."],"url":"http://arxiv.org/abs/2403.19588v1","category":"cs.CV"}
{"created":"2024-03-28 16:11:35","title":"Complete Integrability of the Problem of Full Statistics of Nonstationary Mass Transfer in the Simple Inclusion Process","abstract":"The Simple Inclusion Process (SIP) interpolates between two well-known lattice gas models: the independent random walkers and the Kipnis-Marchiro-Presutti model. Here we study large deviations of nonstationary mass transfer in the SIP at long times in one dimension. We suppose that $N\\gg 1$ particles start from a single lattice site, and we are interested in the full probability density $\\mathcal{P}(M,N,T)$ of observing $M$ particles, $0\\leq M\\leq N$, at $x>0$ at a specified time $T\\gg 1$. At large times, the probability distribution has a large-deviation behavior, $-\\ln \\mathcal{P}(M,N,T) \\simeq \\sqrt{T} s(M/N,N/\\sqrt{T})$. We determine the rate function $s$ exactly by uncovering and utilizing complete integrability, by the inverse scattering method, of the underlying equations of the macroscopic fluctuation theory. We also analyze different asymptotic limits of the rate function $s$.","sentences":["The Simple Inclusion Process (SIP) interpolates between two well-known lattice gas models: the independent random walkers and the Kipnis-Marchiro-Presutti model.","Here we study large deviations of nonstationary mass transfer in the SIP at long times in one dimension.","We suppose that $N\\gg 1$ particles start from a single lattice site, and we are interested in the full probability density $\\mathcal{P}(M,N,T)$ of observing $M$ particles, $0\\leq M\\leq N$, at $x>0$ at a specified time $T\\gg 1$.","At large times, the probability distribution has a large-deviation behavior, $-\\ln \\mathcal{P}(M,N,T)","\\simeq \\sqrt{T} s(M/N,N/\\sqrt{T})$.","We determine the rate function $s$ exactly by uncovering and utilizing complete integrability, by the inverse scattering method, of the underlying equations of the macroscopic fluctuation theory.","We also analyze different asymptotic limits of the rate function $s$."],"url":"http://arxiv.org/abs/2403.19536v1","category":"cond-mat.stat-mech"}
{"created":"2024-03-28 15:58:15","title":"twoPhaseInterTrackFoam: an OpenFOAM module for Arbitrary Lagrangian/Eulerian Interface Tracking with Surfactants and Subgrid-Scale Modeling","abstract":"We provide an implementation of the unstructured Finite-Volume Arbitrary Lagrangian / Eulerian (ALE) Interface-Tracking method for simulating incompressible, immiscible two-phase flows as an OpenFOAM module. In addition to interface-tracking capabilities that include tracking of two fluid phases, an implementation of a Subgrid-Scale (SGS) modeling framework for increased accuracy when simulating sharp boundary layers is enclosed. The SGS modeling framework simplifies embedding subgrid-scale profiles into the unstructured Finite Volume discretization. Our design of the SGS model library significantly simplifies adding new SGS models and applying SGS modeling to Partial Differential Equations (PDEs) in OpenFOAM.","sentences":["We provide an implementation of the unstructured Finite-Volume Arbitrary Lagrangian / Eulerian (ALE) Interface-Tracking method for simulating incompressible, immiscible two-phase flows as an OpenFOAM module.","In addition to interface-tracking capabilities that include tracking of two fluid phases, an implementation of a Subgrid-Scale (SGS) modeling framework for increased accuracy when simulating sharp boundary layers is enclosed.","The SGS modeling framework simplifies embedding subgrid-scale profiles into the unstructured Finite Volume discretization.","Our design of the SGS model library significantly simplifies adding new SGS models and applying SGS modeling to Partial Differential Equations (PDEs) in OpenFOAM."],"url":"http://arxiv.org/abs/2403.19523v1","category":"physics.comp-ph"}
{"created":"2024-03-28 15:48:16","title":"XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold","abstract":"We propose XScale-NVS for high-fidelity cross-scale novel view synthesis of real-world large-scale scenes. Existing representations based on explicit surface suffer from discretization resolution or UV distortion, while implicit volumetric representations lack scalability for large scenes due to the dispersed weight distribution and surface ambiguity. In light of the above challenges, we introduce hash featurized manifold, a novel hash-based featurization coupled with a deferred neural rendering framework. This approach fully unlocks the expressivity of the representation by explicitly concentrating the hash entries on the 2D manifold, thus effectively representing highly detailed contents independent of the discretization resolution. We also introduce a novel dataset, namely GigaNVS, to benchmark cross-scale, high-resolution novel view synthesis of realworld large-scale scenes. Our method significantly outperforms competing baselines on various real-world scenes, yielding an average LPIPS that is 40% lower than prior state-of-the-art on the challenging GigaNVS benchmark. Please see our project page at: xscalenvs.github.io.","sentences":["We propose XScale-NVS for high-fidelity cross-scale novel view synthesis of real-world large-scale scenes.","Existing representations based on explicit surface suffer from discretization resolution or UV distortion, while implicit volumetric representations lack scalability for large scenes due to the dispersed weight distribution and surface ambiguity.","In light of the above challenges, we introduce hash featurized manifold, a novel hash-based featurization coupled with a deferred neural rendering framework.","This approach fully unlocks the expressivity of the representation by explicitly concentrating the hash entries on the 2D manifold, thus effectively representing highly detailed contents independent of the discretization resolution.","We also introduce a novel dataset, namely GigaNVS, to benchmark cross-scale, high-resolution novel view synthesis of realworld large-scale scenes.","Our method significantly outperforms competing baselines on various real-world scenes, yielding an average LPIPS that is 40% lower than prior state-of-the-art on the challenging GigaNVS benchmark.","Please see our project page at: xscalenvs.github.io."],"url":"http://arxiv.org/abs/2403.19517v1","category":"cs.CV"}
{"created":"2024-03-28 15:28:20","title":"Giant High-order Nonlinear and Nonreciprocal Electrical Transports Induced by Valley Flipping in Bernal Bilayer Graphene","abstract":"We investigate the electrical transport properties of the mini-valley polarized state proposed recently in slightly doped Bernal Bilayer Graphene (BLG) in large electric displacement fields. By minimizing the Hartree-Fock energy functional, we first confirm the appearance of mini-valley polarized phase. At the low carrier doping regime, the 1-pocket state will be stabilized where only one of the trigonal-wrapping-induced Fermi pockets near the atomic-valley center is filled. Then we study the electrical transport of the 1-pocket state by solving the Boltzmann equation. We find that the valley polarization could be easily flopped by an in-plane electrical field, which will lead to hysteresis loop in the direct current (DC) $I-V$ curves. Such irreversible current responses in the DC limit will directly induce strong nonlinear and nonreciprocal alternating current (AC) responses, which has been already observed in the recent experiments on BLG.","sentences":["We investigate the electrical transport properties of the mini-valley polarized state proposed recently in slightly doped Bernal Bilayer Graphene (BLG) in large electric displacement fields.","By minimizing the Hartree-Fock energy functional, we first confirm the appearance of mini-valley polarized phase.","At the low carrier doping regime, the 1-pocket state will be stabilized where only one of the trigonal-wrapping-induced Fermi pockets near the atomic-valley center is filled.","Then we study the electrical transport of the 1-pocket state by solving the Boltzmann equation.","We find that the valley polarization could be easily flopped by an in-plane electrical field, which will lead to hysteresis loop in the direct current (DC) $I-V$ curves.","Such irreversible current responses in the DC limit will directly induce strong nonlinear and nonreciprocal alternating current (AC) responses, which has been already observed in the recent experiments on BLG."],"url":"http://arxiv.org/abs/2403.19498v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 15:27:13","title":"CoherentGS: Sparse Novel View Synthesis with Coherent 3D Gaussians","abstract":"The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS). The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality. Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views. To address this issue, we propose regularized optimization and depth-based initialization. Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space. We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization. Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively. With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function. To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view. We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes.","sentences":["The field of 3D reconstruction from images has rapidly evolved in the past few years, first with the introduction of Neural Radiance Field (NeRF) and more recently with 3D Gaussian Splatting (3DGS).","The latter provides a significant edge over NeRF in terms of the training and inference speed, as well as the reconstruction quality.","Although 3DGS works well for dense input images, the unstructured point-cloud like representation quickly overfits to the more challenging setup of extremely sparse input images (e.g., 3 images), creating a representation that appears as a jumble of needles from novel views.","To address this issue, we propose regularized optimization and depth-based initialization.","Our key idea is to introduce a structured Gaussian representation that can be controlled in 2D image space.","We then constraint the Gaussians, in particular their position, and prevent them from moving independently during optimization.","Specifically, we introduce single and multiview constraints through an implicit convolutional decoder and a total variation loss, respectively.","With the coherency introduced to the Gaussians, we further constrain the optimization through a flow-based loss function.","To support our regularized optimization, we propose an approach to initialize the Gaussians using monocular depth estimates at each input view.","We demonstrate significant improvements compared to the state-of-the-art sparse-view NeRF-based approaches on a variety of scenes."],"url":"http://arxiv.org/abs/2403.19495v1","category":"cs.CV"}
{"created":"2024-03-28 15:26:36","title":"Natural convection in a vertical channel. Part 1. Wavenumber interaction and Eckhaus instability in a narrow domain","abstract":"Convection in a vertical channel subjected to a horizontal temperature gradient is numerically investigated. Previous numerical simulations reveal a variety of behaviors: steady, time-periodic and chaotic. We extend previous numerical investigations by constructing stable and unstable branches of equilibria and periodic orbits of the underlying Oberbeck-Boussinesq equations by parametric continuation. In a narrow domain of vertical aspect ratio ten, the observed flow structures are dominated by the competition between three and four co-rotating rolls. We identify the invariant solution branches underlying the observed flow dynamics, characterize the bifurcations creating those branches and link the observed bifurcation scenarios to the symmetries of the primary branches. Specifically, we show that D4 symmetry dictates that several intermediate branches bifurcate simultaneously from the four-roll branch, while D3 symmetry requires that the intersection of these intermediate branches with the three-roll branch be transcritical. We observe other manifestations of the competition between three and four rolls, in which the symmetry in time or in the transverse direction is broken, leading to limit cycles or wavy rolls, respectively. Our work highlights the interest of combining numerical simulations, bifurcation theory, and group theory, in order to understand the transitions between and origin of flow patterns.","sentences":["Convection in a vertical channel subjected to a horizontal temperature gradient is numerically investigated.","Previous numerical simulations reveal a variety of behaviors: steady, time-periodic and chaotic.","We extend previous numerical investigations by constructing stable and unstable branches of equilibria and periodic orbits of the underlying Oberbeck-Boussinesq equations by parametric continuation.","In a narrow domain of vertical aspect ratio ten, the observed flow structures are dominated by the competition between three and four co-rotating rolls.","We identify the invariant solution branches underlying the observed flow dynamics, characterize the bifurcations creating those branches and link the observed bifurcation scenarios to the symmetries of the primary branches.","Specifically, we show that D4 symmetry dictates that several intermediate branches bifurcate simultaneously from the four-roll branch, while D3 symmetry requires that the intersection of these intermediate branches with the three-roll branch be transcritical.","We observe other manifestations of the competition between three and four rolls, in which the symmetry in time or in the transverse direction is broken, leading to limit cycles or wavy rolls, respectively.","Our work highlights the interest of combining numerical simulations, bifurcation theory, and group theory, in order to understand the transitions between and origin of flow patterns."],"url":"http://arxiv.org/abs/2403.19493v1","category":"physics.flu-dyn"}
{"created":"2024-03-28 15:09:07","title":"Lp -cohomology and the geometry of p-harmonic forms","abstract":"In this note we describe basic geometric properties of p-harmonic forms and p-coclosed forms and use them to reprove vanishing theorems of Pansu and new injectivity theorems for the Lp -cohomology of simply connected, pinched negatively curved manifolds. We also provide a partial resolution of a conjecture of Gromov on the vanishing of Lp -cohomology on symmetric spaces.","sentences":["In this note we describe basic geometric properties of p-harmonic forms and p-coclosed forms and use them to reprove vanishing theorems of Pansu and new injectivity theorems for the Lp -cohomology of simply connected, pinched negatively curved manifolds.","We also provide a partial resolution of a conjecture of Gromov on the vanishing of Lp -cohomology on symmetric spaces."],"url":"http://arxiv.org/abs/2403.19481v1","category":"math.DG"}
{"created":"2024-03-28 15:01:58","title":"SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks","abstract":"Scene graphs have been recently introduced into 3D spatial understanding as a comprehensive representation of the scene. The alignment between 3D scene graphs is the first step of many downstream tasks such as scene graph aided point cloud registration, mosaicking, overlap checking, and robot navigation. In this work, we treat 3D scene graph alignment as a partial graph-matching problem and propose to solve it with a graph neural network. We reuse the geometric features learned by a point cloud registration method and associate the clustered point-level geometric features with the node-level semantic feature via our designed feature fusion module. Partial matching is enabled by using a learnable method to select the top-k similar node pairs. Subsequent downstream tasks such as point cloud registration are achieved by running a pre-trained registration network within the matched regions. We further propose a point-matching rescoring method, that uses the node-wise alignment of the 3D scene graph to reweight the matching candidates from a pre-trained point cloud registration method. It reduces the false point correspondences estimated especially in low-overlapping cases. Experiments show that our method improves the alignment accuracy by 10~20% in low-overlap and random transformation scenarios and outperforms the existing work in multiple downstream tasks.","sentences":["Scene graphs have been recently introduced into 3D spatial understanding as a comprehensive representation of the scene.","The alignment between 3D scene graphs is the first step of many downstream tasks such as scene graph aided point cloud registration, mosaicking, overlap checking, and robot navigation.","In this work, we treat 3D scene graph alignment as a partial graph-matching problem and propose to solve it with a graph neural network.","We reuse the geometric features learned by a point cloud registration method and associate the clustered point-level geometric features with the node-level semantic feature via our designed feature fusion module.","Partial matching is enabled by using a learnable method to select the top-k similar node pairs.","Subsequent downstream tasks such as point cloud registration are achieved by running a pre-trained registration network within the matched regions.","We further propose a point-matching rescoring method, that uses the node-wise alignment of the 3D scene graph to reweight the matching candidates from a pre-trained point cloud registration method.","It reduces the false point correspondences estimated especially in low-overlapping cases.","Experiments show that our method improves the alignment accuracy by 10~20% in low-overlap and random transformation scenarios and outperforms the existing work in multiple downstream tasks."],"url":"http://arxiv.org/abs/2403.19474v1","category":"cs.CV"}
{"created":"2024-03-28 14:54:31","title":"Deep decomposition method for the limited aperture inverse obstacle scattering problem","abstract":"In this paper, we consider a deep learning approach to the limited aperture inverse obstacle scattering problem. It is well known that traditional deep learning relies solely on data, which may limit its performance for the inverse problem when only indirect observation data and a physical model are available. A fundamental question arises in light of these limitations: is it possible to enable deep learning to work on inverse problems without labeled data and to be aware of what it is learning? This work proposes a deep decomposition method (DDM) for such purposes, which does not require ground truth labels. It accomplishes this by providing physical operators associated with the scattering model to the neural network architecture. Additionally, a deep learning based data completion scheme is implemented in DDM to prevent distorting the solution of the inverse problem for limited aperture data. Furthermore, apart from addressing the ill-posedness imposed by the inverse problem itself, DDM is a physics-aware machine learning technique that can have interpretability property. The convergence result of DDM is theoretically proven. Numerical experiments are presented to demonstrate the validity of the proposed DDM even when the incident and observation apertures are extremely limited.","sentences":["In this paper, we consider a deep learning approach to the limited aperture inverse obstacle scattering problem.","It is well known that traditional deep learning relies solely on data, which may limit its performance for the inverse problem when only indirect observation data and a physical model are available.","A fundamental question arises in light of these limitations: is it possible to enable deep learning to work on inverse problems without labeled data and to be aware of what it is learning?","This work proposes a deep decomposition method (DDM) for such purposes, which does not require ground truth labels.","It accomplishes this by providing physical operators associated with the scattering model to the neural network architecture.","Additionally, a deep learning based data completion scheme is implemented in DDM to prevent distorting the solution of the inverse problem for limited aperture data.","Furthermore, apart from addressing the ill-posedness imposed by the inverse problem itself, DDM is a physics-aware machine learning technique that can have interpretability property.","The convergence result of DDM is theoretically proven.","Numerical experiments are presented to demonstrate the validity of the proposed DDM even when the incident and observation apertures are extremely limited."],"url":"http://arxiv.org/abs/2403.19470v1","category":"math.NA"}
{"created":"2024-03-28 13:37:52","title":"Carleman estimates for space semi-discrete approximations of one-dimensional stochastic parabolic equation and its applications","abstract":"In this paper, we study discrete Carleman estimates for space semi-discrete approximations of one-dimensional stochastic parabolic equation. As applications of these discrete Carleman estimates, we apply them to study two inverse problems for the spatial semi-discrete stochastic parabolic equations, including a discrete inverse random source problem and a discrete Cauchy problem. We firstly establish two Carleman estimates for a one-dimensional semi-discrete stochastic parabolic equation, one for homogeneous boundary and the other for non-homogeneous boundary. Then we apply these two estimates separately to derive two stability results. The first one is the Lipschitz stability for the discrete inverse random source problem. The second one is the H\\\"{o}lder stability for the discrete Cauchy problem.","sentences":["In this paper, we study discrete Carleman estimates for space semi-discrete approximations of one-dimensional stochastic parabolic equation.","As applications of these discrete Carleman estimates, we apply them to study two inverse problems for the spatial semi-discrete stochastic parabolic equations, including a discrete inverse random source problem and a discrete Cauchy problem.","We firstly establish two Carleman estimates for a one-dimensional semi-discrete stochastic parabolic equation, one for homogeneous boundary and the other for non-homogeneous boundary.","Then we apply these two estimates separately to derive two stability results.","The first one is the Lipschitz stability for the discrete inverse random source problem.","The second one is the H\\\"{o}lder stability for the discrete Cauchy problem."],"url":"http://arxiv.org/abs/2403.19413v1","category":"math.PR"}
{"created":"2024-03-28 11:32:53","title":"Bayesian inference of the dense matter equation of state built upon extended Skyrme interactions","abstract":"The non-relativistic model of nuclear matter with Brussels extended Skyrme interactions is employed in order to build, within a Bayesian approach, models for the dense matter equation of state (EOS). In addition to a minimal set of constraints on nuclear empirical parameters; the density behavior of energy per particle in pure neutron matter (PNM); a lower limit on the maximum neutron star (NS) mass, we require that the Fermi velocity of neutrons ($v_{\\mathrm{F;\\,n}}$) in PNM and symmetric nuclear matter (SNM) with densities up to $0.8~\\mathrm{fm}^{-3}$ does not exceed the speed of light. The latter condition is imposed in order to cure a deficiency present in many Skyrme interactions [Duan and Urban, Phys. Rev. C 108, 025813 (2023)]. We illustrate the importance of this constraint for the posterior distributions. Some of our models are subjected to constraints on the density dependence of neutron (nucleon) Landau effective mass in PNM (SNM), too. The impact of various sets of constraints on the behaviors of nuclear matter and NSs is discussed in detail. When compared with models built upon standard Skyrme interactions, it turns out that those based on the extended interactions are stiffer if Fermi velocity is disregarded, but softer if it is taken into account.","sentences":["The non-relativistic model of nuclear matter with Brussels extended Skyrme interactions is employed in order to build, within a Bayesian approach, models for the dense matter equation of state (EOS).","In addition to a minimal set of constraints on nuclear empirical parameters; the density behavior of energy per particle in pure neutron matter (PNM); a lower limit on the maximum neutron star (NS) mass, we require that the Fermi velocity of neutrons ($v_{\\mathrm{F;\\,n}}$) in PNM and symmetric nuclear matter (SNM) with densities up to $0.8~\\mathrm{fm}^{-3}$ does not exceed the speed of light.","The latter condition is imposed in order to cure a deficiency present in many Skyrme interactions","[Duan and Urban, Phys.","Rev. C 108, 025813 (2023)].","We illustrate the importance of this constraint for the posterior distributions.","Some of our models are subjected to constraints on the density dependence of neutron (nucleon)","Landau effective mass in PNM (SNM), too.","The impact of various sets of constraints on the behaviors of nuclear matter and NSs is discussed in detail.","When compared with models built upon standard Skyrme interactions, it turns out that those based on the extended interactions are stiffer if Fermi velocity is disregarded, but softer if it is taken into account."],"url":"http://arxiv.org/abs/2403.19325v1","category":"nucl-th"}
{"created":"2024-03-28 10:37:09","title":"A unified SHTC multiphase model of continuum mechanics","abstract":"In this paper, we present a unified nonequilibrium model of continuum mechanics for compressible multiphase flows. The model, which is formulated within the framework of Symmetric Hyperbolic Thermodynamically Compatible (SHTC) equations, can describe the arbitrary number of phases that can be heat-conducting inviscid and viscous fluids}, as well as elastoplastic solids. The phases are allowed to have different velocities, pressures, temperatures, and shear stresses, while the material interfaces are treated as diffuse interfaces with the volume fraction playing the role of the interface field. To relate our model to other multiphase approaches, we reformulate the SHTC governing equations in terms of the phase state parameters and put them in the form of Baer-Nunziato-type models. It is the Baer-Nunziato form of the SHTC equations which is then solved numerically using a robust second-order path-conservative MUSCL-Hancock finite volume method on Cartesian meshes. Due to the fact that the obtained governing equations are very challenging, we restrict our numerical examples to a simplified version of the model, focusing on the isentropic limit for three-phase mixtures. To address the stiffness properties of the relaxation source terms present in the model, the implemented scheme incorporates a semi-analytical time integration method specifically designed for the non-linear stiff source terms governing the strain relaxation. The validation process involves a wide range of benchmarks and several applications for compressible multiphase problems. Notably, results are presented for multiphase flows in all the relaxation limit cases of the model, including inviscid and viscous Newtonian fluids, as well as non-linear hyperelastic and elastoplastic solids.","sentences":["In this paper, we present a unified nonequilibrium model of continuum mechanics for compressible multiphase flows.","The model, which is formulated within the framework of Symmetric Hyperbolic Thermodynamically Compatible (SHTC) equations, can describe the arbitrary number of phases that can be heat-conducting inviscid and viscous fluids}, as well as elastoplastic solids.","The phases are allowed to have different velocities, pressures, temperatures, and shear stresses, while the material interfaces are treated as diffuse interfaces with the volume fraction playing the role of the interface field.","To relate our model to other multiphase approaches, we reformulate the SHTC governing equations in terms of the phase state parameters and put them in the form of Baer-Nunziato-type models.","It is the Baer-Nunziato form of the SHTC equations which is then solved numerically using a robust second-order path-conservative MUSCL-Hancock finite volume method on Cartesian meshes.","Due to the fact that the obtained governing equations are very challenging, we restrict our numerical examples to a simplified version of the model, focusing on the isentropic limit for three-phase mixtures.","To address the stiffness properties of the relaxation source terms present in the model, the implemented scheme incorporates a semi-analytical time integration method specifically designed for the non-linear stiff source terms governing the strain relaxation.","The validation process involves a wide range of benchmarks and several applications for compressible multiphase problems.","Notably, results are presented for multiphase flows in all the relaxation limit cases of the model, including inviscid and viscous Newtonian fluids, as well as non-linear hyperelastic and elastoplastic solids."],"url":"http://arxiv.org/abs/2403.19298v1","category":"math.NA"}
{"created":"2024-03-28 10:29:12","title":"Deep Learning-based Modulation Classification of Practical OFDM Signals for Spectrum Sensing","abstract":"In this study, the modulation of symbols on OFDM subcarriers is classified for transmissions following Wi-Fi~6 and 5G downlink specifications. First, our approach estimates the OFDM symbol duration and cyclic prefix length based on the cyclic autocorrelation function. We propose a feature extraction algorithm characterizing the modulation of OFDM signals, which includes removing the effects of a synchronization error. The obtained feature is converted into a 2D histogram of phase and amplitude and this histogram is taken as input to a convolutional neural network (CNN)-based classifier. The classifier does not require prior knowledge of protocol-specific information such as Wi-Fi preamble or resource allocation of 5G physical channels. The classifier's performance, evaluated using synthetic and real-world measured over-the-air (OTA) datasets, achieves a minimum accuracy of 97\\% accuracy with OTA data when SNR is above the value required for data transmission.","sentences":["In this study, the modulation of symbols on OFDM subcarriers is classified for transmissions following Wi-Fi~6 and 5G downlink specifications.","First, our approach estimates the OFDM symbol duration and cyclic prefix length based on the cyclic autocorrelation function.","We propose a feature extraction algorithm characterizing the modulation of OFDM signals, which includes removing the effects of a synchronization error.","The obtained feature is converted into a 2D histogram of phase and amplitude and this histogram is taken as input to a convolutional neural network (CNN)-based classifier.","The classifier does not require prior knowledge of protocol-specific information such as Wi-Fi preamble or resource allocation of 5G physical channels.","The classifier's performance, evaluated using synthetic and real-world measured over-the-air (OTA) datasets, achieves a minimum accuracy of 97\\% accuracy with OTA data when SNR is above the value required for data transmission."],"url":"http://arxiv.org/abs/2403.19292v1","category":"cs.NI"}
{"created":"2024-03-28 09:54:32","title":"Liouville Theorem for $k-$curvature equation with fully nonlinear boundary in half space","abstract":"We obtain the Liouville theorem for constant $k$-curvature $\\sigma_{k}(A_{g})$ in $\\mathbb{R}_{+}^{n}$ with constant $\\mathcal{B}_{k}^{g}$ curvature on $\\partial\\mathbb{R}_{+}^{n}$, where $\\mathcal{B}_{k}^{g}$ is derived from the variational functional for $\\sigma_{k}(A_{g})$, and specially represents the boundary term in the Gauss-Bonnet-Chern formula for $k=n/2$.","sentences":["We obtain the Liouville theorem for constant $k$-curvature $\\sigma_{k}(A_{g})$ in $\\mathbb{R}_{+}^{n}$ with constant $\\mathcal{B}_{k}^{g}$ curvature on $\\partial\\mathbb{R}_{+}^{n}$, where $\\mathcal{B}_{k}^{g}$ is derived from the variational functional for $\\sigma_{k}(A_{g})$, and specially represents the boundary term in the Gauss-Bonnet-Chern formula for $k=n/2$."],"url":"http://arxiv.org/abs/2403.19268v1","category":"math.DG"}
{"created":"2024-03-28 08:44:08","title":"Numerical approximations of a lattice Boltzmann scheme with a family of partial differential equations","abstract":"Is it possible to consider a lattice Boltzmann scheme as an approximation of a partial differential equation? For a nonhomogeneous advection problem in one spatial dimension, we propose equivalent partial differential equations at various orders. We compare the lattice Boltzmann results and a spectral approximation of the differential equations. No simple correlation is obtained for a stationary problem. For an unsteady situation, we show that the initialization scheme of the microscopic moments plays a crucial role.","sentences":["Is it possible to consider a lattice Boltzmann scheme as an approximation of a partial differential equation?","For a nonhomogeneous advection problem in one spatial dimension, we propose equivalent partial differential equations at various orders.","We compare the lattice Boltzmann results and a spectral approximation of the differential equations.","No simple correlation is obtained for a stationary problem.","For an unsteady situation, we show that the initialization scheme of the microscopic moments plays a crucial role."],"url":"http://arxiv.org/abs/2403.19231v1","category":"math.NA"}
{"created":"2024-03-28 08:41:17","title":"Gyrokinetic limit of the 2D Hartree equation in a large magnetic field","abstract":"We study the dynamics of two-dimensional interacting fermions submitted to a homogeneous transverse magnetic field. We consider a large magnetic field regime, with the gap between Landau levels set to the same order as that of potential energy contributions. Within the mean-field approximation, i.e. starting from Hartree's equation for the first reduced density matrix, we derive a drift equation for the particle density. We use vortex coherent states and the associated Husimi function to define a semi-classical density almost satisfying the limiting equation. We then deduce convergence of the density of the true Hartree solution by a Dobrushin-type stability estimate for the limiting equation.","sentences":["We study the dynamics of two-dimensional interacting fermions submitted to a homogeneous transverse magnetic field.","We consider a large magnetic field regime, with the gap between Landau levels set to the same order as that of potential energy contributions.","Within the mean-field approximation, i.e. starting from Hartree's equation for the first reduced density matrix, we derive a drift equation for the particle density.","We use vortex coherent states and the associated Husimi function to define a semi-classical density almost satisfying the limiting equation.","We then deduce convergence of the density of the true Hartree solution by a Dobrushin-type stability estimate for the limiting equation."],"url":"http://arxiv.org/abs/2403.19226v1","category":"math.AP"}
{"created":"2024-03-28 08:32:13","title":"A piecewise neural network method for solving large interval solution to initial value problem of ordinary differential equations","abstract":"Various traditional numerical methods for solving initial value problems of differential equations often produce local solutions near the initial value point, despite the problems having larger interval solutions. Even current popular neural network algorithms or deep learning methods cannot guarantee yielding large interval solutions for these problems. In this paper, we propose a piecewise neural network approach to obtain a large interval numerical solution for initial value problems of differential equations. In this method, we first divide the solution interval, on which the initial problem is to be solved, into several smaller intervals. Neural networks with a unified structure are then employed on each sub-interval to solve the related sub-problems. By assembling these neural network solutions, a piecewise expression of the large interval solution to the problem is constructed, referred to as the piecewise neural network solution. The continuous differentiability of the solution over the entire interval, except for finite points, is proven through theoretical analysis and employing a parameter transfer technique. Additionally, a parameter transfer and multiple rounds of pre-training technique are utilized to enhance the accuracy of the approximation solution. Compared with existing neural network algorithms, this method does not increase the network size and training data scale for training the network on each sub-domain. Finally, several numerical experiments are presented to demonstrate the efficiency of the proposed algorithm.","sentences":["Various traditional numerical methods for solving initial value problems of differential equations often produce local solutions near the initial value point, despite the problems having larger interval solutions.","Even current popular neural network algorithms or deep learning methods cannot guarantee yielding large interval solutions for these problems.","In this paper, we propose a piecewise neural network approach to obtain a large interval numerical solution for initial value problems of differential equations.","In this method, we first divide the solution interval, on which the initial problem is to be solved, into several smaller intervals.","Neural networks with a unified structure are then employed on each sub-interval to solve the related sub-problems.","By assembling these neural network solutions, a piecewise expression of the large interval solution to the problem is constructed, referred to as the piecewise neural network solution.","The continuous differentiability of the solution over the entire interval, except for finite points, is proven through theoretical analysis and employing a parameter transfer technique.","Additionally, a parameter transfer and multiple rounds of pre-training technique are utilized to enhance the accuracy of the approximation solution.","Compared with existing neural network algorithms, this method does not increase the network size and training data scale for training the network on each sub-domain.","Finally, several numerical experiments are presented to demonstrate the efficiency of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.19218v1","category":"math.NA"}
{"created":"2024-03-28 08:08:53","title":"LV-CTC: Non-autoregressive ASR with CTC and latent variable models","abstract":"Non-autoregressive (NAR) models for automatic speech recognition (ASR) aim to achieve high accuracy and fast inference by simplifying the autoregressive (AR) generation process of conventional models. Connectionist temporal classification (CTC) is one of the key techniques used in NAR ASR models. In this paper, we propose a new model combining CTC and a latent variable model, which is one of the state-of-the-art models in the neural machine translation research field. A new neural network architecture and formulation specialized for ASR application are introduced. In the proposed model, CTC alignment is assumed to be dependent on the latent variables that are expected to capture dependencies between tokens. Experimental results on a 100 hours subset of Librispeech corpus showed the best recognition accuracy among CTC-based NAR models. On the TED-LIUM2 corpus, the best recognition accuracy is achieved including AR E2E models with faster inference speed.","sentences":["Non-autoregressive (NAR) models for automatic speech recognition (ASR) aim to achieve high accuracy and fast inference by simplifying the autoregressive (AR) generation process of conventional models.","Connectionist temporal classification (CTC) is one of the key techniques used in NAR ASR models.","In this paper, we propose a new model combining CTC and a latent variable model, which is one of the state-of-the-art models in the neural machine translation research field.","A new neural network architecture and formulation specialized for ASR application are introduced.","In the proposed model, CTC alignment is assumed to be dependent on the latent variables that are expected to capture dependencies between tokens.","Experimental results on a 100 hours subset of Librispeech corpus showed the best recognition accuracy among CTC-based NAR models.","On the TED-LIUM2 corpus, the best recognition accuracy is achieved including AR E2E models with faster inference speed."],"url":"http://arxiv.org/abs/2403.19207v1","category":"eess.AS"}
{"created":"2024-03-28 08:08:33","title":"CogniDot: Vasoactivity-based Cognitive Load Monitoring with a Miniature On-skin Sensor","abstract":"Vascular activities offer valuable signatures for psychological monitoring applications. We present CogniDot, an affordable, miniature skin sensor placed on the temporal area on the head that senses cognitive loads with a single-pixel color sensor. With its energy-efficient design, bio-compatible adhesive, and compact size (22mm diameter, 8.5mm thickness), it is ideal for long-term monitoring of mind status. We showed in detail the hardware design of our sensor. The user study results with 12 participants show that CogniDot can accurately differentiate between three levels of cognitive loads with a within-user accuracy of 97%. We also discuss its potential for broader applications.","sentences":["Vascular activities offer valuable signatures for psychological monitoring applications.","We present CogniDot, an affordable, miniature skin sensor placed on the temporal area on the head that senses cognitive loads with a single-pixel color sensor.","With its energy-efficient design, bio-compatible adhesive, and compact size (22mm diameter, 8.5mm thickness), it is ideal for long-term monitoring of mind status.","We showed in detail the hardware design of our sensor.","The user study results with 12 participants show that CogniDot can accurately differentiate between three levels of cognitive loads with a within-user accuracy of 97%.","We also discuss its potential for broader applications."],"url":"http://arxiv.org/abs/2403.19206v1","category":"cs.HC"}
{"created":"2024-03-28 06:32:42","title":"Static Manifolds with Boundary and Rigidity of Scalar Curvature and Mean Curvature","abstract":"On a compact manifold with boundary, the map consisting of the scalar curvature in the interior and the mean curvature on the boundary is a local surjection at generic metrics. Moreover, this result may be localized to compact subdomains in an arbitrary Riemannian manifold with boundary. The non-generic case (also called non-generic domains) corresponds to static manifolds with boundary. We discuss their geometric properties, which also work as the necessary conditions of non-generic metrics. In space forms and the Schwarzschild manifold, we classify simple non-generic domains (with only one boundary component) and show their connection with rigidity theorems and the Schwarzschild photon sphere.","sentences":["On a compact manifold with boundary, the map consisting of the scalar curvature in the interior and the mean curvature on the boundary is a local surjection at generic metrics.","Moreover, this result may be localized to compact subdomains in an arbitrary Riemannian manifold with boundary.","The non-generic case (also called non-generic domains) corresponds to static manifolds with boundary.","We discuss their geometric properties, which also work as the necessary conditions of non-generic metrics.","In space forms and the Schwarzschild manifold, we classify simple non-generic domains (with only one boundary component) and show their connection with rigidity theorems and the Schwarzschild photon sphere."],"url":"http://arxiv.org/abs/2403.19169v1","category":"math.DG"}
{"created":"2024-03-28 06:18:12","title":"D'OH: Decoder-Only random Hypernetworks for Implicit Neural Representations","abstract":"Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers. We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy. Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the initialization of run-time deep implicit functions for single-instance signals through a Decoder-Only randomly projected Hypernetwork (D'OH). By directly changing the dimension of a latent code to approximate a target implicit neural architecture, we provide a natural way to vary the memory footprint of neural representations without the costly need for neural architecture search on a space of alternative low-rate structures.","sentences":["Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals.","Their attractiveness stems from their ability to compactly represent signals with little to no off-line training data.","Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal.","In this paper, we explore the hypothesis that additional compression can be achieved by leveraging the redundancies that exist between layers.","We propose to use a novel run-time decoder-only hypernetwork - that uses no offline training data - to better model this cross-layer parameter redundancy.","Previous applications of hyper-networks with deep implicit functions have applied feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on.","We instead present a strategy for the initialization of run-time deep implicit functions for single-instance signals through a Decoder-Only randomly projected Hypernetwork (D'OH).","By directly changing the dimension of a latent code to approximate a target implicit neural architecture, we provide a natural way to vary the memory footprint of neural representations without the costly need for neural architecture search on a space of alternative low-rate structures."],"url":"http://arxiv.org/abs/2403.19163v1","category":"cs.LG"}
{"created":"2024-03-28 05:18:38","title":"Sounds waves and fluctuations in one-dimensional supersolids","abstract":"We examine the low-energy excitations of a dilute supersolid state of matter with a one-dimensional crystal structure. A hydrodynamic description is developed based on a Lagrangian, incorporating generalized elastic parameters derived from ground state calculations. The predictions of the hydrodynamic theory are validated against solutions of the Bogoliubov-de Gennes equations, by comparing the speeds of sound, density fluctuations, and phase fluctuations of the two gapless bands. Our results are presented for two distinct supersolid models: a dipolar Bose-Einstein condensate in an infinite tube and a dilute Bose gas of atoms with soft-core interactions. Characteristic energy scales are identified, highlighting that these two models approximately realize the bulk incompressible and rigid lattice supersolid limits.","sentences":["We examine the low-energy excitations of a dilute supersolid state of matter with a one-dimensional crystal structure.","A hydrodynamic description is developed based on a Lagrangian, incorporating generalized elastic parameters derived from ground state calculations.","The predictions of the hydrodynamic theory are validated against solutions of the Bogoliubov-de Gennes equations, by comparing the speeds of sound, density fluctuations, and phase fluctuations of the two gapless bands.","Our results are presented for two distinct supersolid models: a dipolar Bose-Einstein condensate in an infinite tube and a dilute Bose gas of atoms with soft-core interactions.","Characteristic energy scales are identified, highlighting that these two models approximately realize the bulk incompressible and rigid lattice supersolid limits."],"url":"http://arxiv.org/abs/2403.19151v1","category":"cond-mat.quant-gas"}
{"created":"2024-03-28 04:19:01","title":"Bertrand types of regular curves and Bertrand framed curves in the Euclidean 3-space","abstract":"A Bertrand (respectively, Mannheim) curve is a space curve whose principal normal line is the same as the principal normal (respectively, bi-normal) line of another curve. By definition, another curve is a parallel curve with respect to the direction of the principal normal vector. In this paper, we consider the other cases, that is, a space curve whose tangent (or, principal normal, bi-normal) line is the same as the tangent (or, principal normal, bi-normal) line of another curve, respectively. We say that a Bertrand type curve if there exists such another curve. We clarify that the existence conditions of Bertrand type curves in all cases. There are times when the Bertrand type curve does not exist. On the other hand, since the another curve may have singular points, we also consider curves with singular points. As smooth curves with singular points, it is useful to use the framed curves in the Euclidean space. Then we define and investigate Bertrand framed curves. We also clarify that the existence conditions of the Bertrand framed curves in all cases.","sentences":["A Bertrand (respectively, Mannheim) curve is a space curve whose principal normal line is the same as the principal normal (respectively, bi-normal) line of another curve.","By definition, another curve is a parallel curve with respect to the direction of the principal normal vector.","In this paper, we consider the other cases, that is, a space curve whose tangent (or, principal normal, bi-normal) line is the same as the tangent (or, principal normal, bi-normal) line of another curve, respectively.","We say that a Bertrand type curve if there exists such another curve.","We clarify that the existence conditions of Bertrand type curves in all cases.","There are times when the Bertrand type curve does not exist.","On the other hand, since the another curve may have singular points, we also consider curves with singular points.","As smooth curves with singular points, it is useful to use the framed curves in the Euclidean space.","Then we define and investigate Bertrand framed curves.","We also clarify that the existence conditions of the Bertrand framed curves in all cases."],"url":"http://arxiv.org/abs/2403.19138v1","category":"math.DG"}
{"created":"2024-03-28 04:12:18","title":"Quantum focusing conjecture in two-dimensional evaporating black holes","abstract":"We consider the quantum focusing conjecture (QFC) for two-dimensional evaporating black holes. The QFC is closely related to the behavior of the generalized entropy -- the sum of the area entropy for a given co-dimension two surface and the entanglement entropy for quantum fields outside the area. In the context of the black hole evaporation, the entanglement entropy of the Hawking radiation is decreasing after the Page time, and therefore it is not obvious whether the QFC holds in the black hole evaporation process especially after the Page time. One of the present authors previously addressed this problem in a four-dimensional spherically symmetric dynamical black hole model and showed that the QFC is satisfied. However the background spacetime considered was approximated by the Vaidya metric, and quantum effects of matters in the semiclassical regime is not fully taken into consideration. It remains to be seen if the QFC in fact holds for exact solutions of the semiclassical Einstein equations. In this paper, we address this problem in a two-dimensional dynamical black hole of the Russo-Susskind-Thorlacius (RST) model, which allows us to solve the semiclassical equations of motion exactly. We first give a suitable definition of the quantum expansion in two-dimensions and then prove that the QFC is satisfied for evaporating black holes in the RST model with the island formation taken into account.","sentences":["We consider the quantum focusing conjecture (QFC) for two-dimensional evaporating black holes.","The QFC is closely related to the behavior of the generalized entropy -- the sum of the area entropy for a given co-dimension two surface and the entanglement entropy for quantum fields outside the area.","In the context of the black hole evaporation, the entanglement entropy of the Hawking radiation is decreasing after the Page time, and therefore it is not obvious whether the QFC holds in the black hole evaporation process especially after the Page time.","One of the present authors previously addressed this problem in a four-dimensional spherically symmetric dynamical black hole model and showed that the QFC is satisfied.","However the background spacetime considered was approximated by the Vaidya metric, and quantum effects of matters in the semiclassical regime is not fully taken into consideration.","It remains to be seen if the QFC in fact holds for exact solutions of the semiclassical Einstein equations.","In this paper, we address this problem in a two-dimensional dynamical black hole of the Russo-Susskind-Thorlacius (RST) model, which allows us to solve the semiclassical equations of motion exactly.","We first give a suitable definition of the quantum expansion in two-dimensions and then prove that the QFC is satisfied for evaporating black holes in the RST model with the island formation taken into account."],"url":"http://arxiv.org/abs/2403.19136v1","category":"hep-th"}
{"created":"2024-03-28 04:05:08","title":"Long-time dynamics of a competition model with nonlocal diffusion and free boundaries: Chances of successful invasion","abstract":"This is a continuation of our work \\cite{dns-part1} to investigate the long-time dynamics of a two species competition model of Lotka-Volterra type with nonlocal diffusions, where the territory (represented by the real line $\\R$) of a native species with density $v(t,x)$, is invaded by a competitor with density $u(t,x)$, via two fronts, $x=g(t)$ on the left and $x=h(t)$ on the right. So the population range of $u$ is the evolving interval $[g(t), h(t)]$ and the reaction-diffusion equation for $u$ has two free boundaries, with $g(t)$ decreasing in $t$ and $h(t)$ increasing in $t$. Let $h_\\infty:=h(\\infty)\\leq \\infty$ and $g_\\infty:=g(\\infty)\\geq -\\infty$. In \\cite{dns-part1}, we obtained detailed descriptions of the long-time dynamics of the model according to whether $h_\\infty-g_\\infty$ is $\\infty$ or finite. In the latter case, we demonstrated in what sense the invader $u$ vanishes in the long run and $v$ survives the invasion, while in the former case, we obtained a rather satisfactory description of the long-time asymptotic limits of $u(t,x)$ and $v(t,x)$ when the parameter $k$ in the model is less than 1. In the current paper, we obtain sharp criteria to distinguish the case $h_\\infty-g_\\infty=\\infty$ from the case $h_\\infty-g_\\infty$ is finite. Moreover, for the case $k\\geq 1$ and $u$ is a weak competitor, we obtain biologically meaningful conditions that guarantee the vanishing of the invader $u$, and reveal chances for $u$ to invade successfully. In particular, we demonstrate that both $h_\\infty=\\infty=-g_\\infty$ and $h_\\infty=\\infty$ but $g_\\infty$ is finite are possible; the latter seems to be the first example for this kind of population models, with either local or nonlocal diffusion.","sentences":["This is a continuation of our work \\cite{dns-part1} to investigate the long-time dynamics of a two species competition model of Lotka-Volterra type with nonlocal diffusions, where the territory (represented by the real line $\\R$) of a native species with density $v(t,x)$, is invaded by a competitor with density $u(t,x)$, via two fronts, $x=g(t)$ on the left and $x=h(t)$ on the right.","So the population range of $u$ is the evolving interval $[g(t), h(t)]$ and the reaction-diffusion equation for $u$ has two free boundaries, with $g(t)$ decreasing in $t$ and $h(t)$ increasing in $t$. Let $h_\\infty:=h(\\infty)\\leq \\infty$ and $g_\\infty:=g(\\infty)\\geq -\\infty$.","In \\cite{dns-part1}, we obtained detailed descriptions of the long-time dynamics of the model according to whether $h_\\infty-g_\\infty$ is $\\infty$ or finite.","In the latter case, we demonstrated in what sense the invader $u$ vanishes in the long run and $v$ survives the invasion, while in the former case, we obtained a rather satisfactory description of the long-time asymptotic limits of $u(t,x)$ and $v(t,x)$ when the parameter $k$ in the model is less than 1.","In the current paper, we obtain sharp criteria to distinguish the case $h_\\infty-g_\\infty=\\infty$ from the case $h_\\infty-g_\\infty$ is finite.","Moreover, for the case $k\\geq 1$ and $u$ is a weak competitor, we obtain biologically meaningful conditions that guarantee the vanishing of the invader $u$, and reveal chances for $u$ to invade successfully.","In particular, we demonstrate that both $h_\\infty=\\infty=-g_\\infty$ and $h_\\infty=\\infty$ but $g_\\infty$ is finite are possible; the latter seems to be the first example for this kind of population models, with either local or nonlocal diffusion."],"url":"http://arxiv.org/abs/2403.19134v1","category":"math.AP"}
{"created":"2024-03-28 03:59:59","title":"Long-time dynamics of a competition model with nonlocal diffusion and free boundaries: Vanishing and spreading of the invader","abstract":"In this work, we investigate the long-time dynamics of a two species competition model of Lotka-Volterra type with nonlocal diffusions. One of the species, with density $v(t,x)$, is assumed to be a native in the environment (represented by the real line $\\R$), while the other species, with density $u(t,x)$, is an invading species which invades the territory of $v$ with two fronts, $x=g(t)$ on the left and $x=h(t)$ on the right. So the population range of $u$ is the evolving interval $[g(t), h(t)]$ and the reaction-diffusion equation for $u$ has two free boundaries, with $g(t)$ decreasing in $t$ and $h(t)$ increasing in $t$, and the limits $h_\\infty:=h(\\infty)\\leq \\infty$ and $g_\\infty:=g(\\infty)\\geq -\\infty$ thus always exist. We obtain detailed descriptions of the long-time dynamics of the model according to whether $h_\\infty-g_\\infty$ is $\\infty$ or finite. In the latter case, we reveal in what sense the invader $u$ vanishes in the long run and $v$ survives the invasion, while in the former case, we obtain a rather satisfactory description of the long-time asymptotic limit for both $u(t,x)$ and $v(t,x)$ when a certain parameter $k$ in the model is less than 1. This research is continued in a separate work, where sharp criteria are obtained to distinguish the case $h_\\infty-g_\\infty=\\infty$ from the case $h_\\infty-g_\\infty$ is finite, and new phenomena are revealed for the case $k\\geq 1$. The techniques developed in this paper should have applications to other models with nonlocal diffusion and free boundaries.","sentences":["In this work, we investigate the long-time dynamics of a two species competition model of Lotka-Volterra type with nonlocal diffusions.","One of the species, with density $v(t,x)$, is assumed to be a native in the environment (represented by the real line $\\R$), while the other species, with density $u(t,x)$, is an invading species which invades the territory of $v$ with two fronts, $x=g(t)$ on the left and $x=h(t)$ on the right.","So the population range of $u$ is the evolving interval $[g(t), h(t)]$ and the reaction-diffusion equation for $u$ has two free boundaries, with $g(t)$ decreasing in $t$ and $h(t)$ increasing in $t$, and the limits $h_\\infty:=h(\\infty)\\leq \\infty$ and $g_\\infty:=g(\\infty)\\geq -\\infty$ thus always exist.","We obtain detailed descriptions of the long-time dynamics of the model according to whether $h_\\infty-g_\\infty$ is $\\infty$ or finite.","In the latter case, we reveal in what sense the invader $u$ vanishes in the long run and $v$ survives the invasion, while in the former case, we obtain a rather satisfactory description of the long-time asymptotic limit for both $u(t,x)$ and $v(t,x)$ when a certain parameter $k$ in the model is less than 1.","This research is continued in a separate work, where sharp criteria are obtained to distinguish the case $h_\\infty-g_\\infty=\\infty$ from the case $h_\\infty-g_\\infty$ is finite, and new phenomena are revealed for the case $k\\geq 1$.","The techniques developed in this paper should have applications to other models with nonlocal diffusion and free boundaries."],"url":"http://arxiv.org/abs/2403.19131v1","category":"math.AP"}
{"created":"2024-03-28 02:43:44","title":"Branching problem of tensoring two Verma modules and its application to differential symmetry breaking operators","abstract":"Kobayashi-Pevzner discovered in [Selecta Math., 2016] that the failure of the multiplicity-one property in the fusion rule of Verma modules of sl2 occurs exactly when the Rankin-Cohen bracket vanishes, and 1classified all the corresponding parameters. In this paper we provide yet another characterization for these parameters, and give a precise description of indecomposable components of the tensor product. Furthermore, we discuss when the tensor products of two Verma modules are isomorphic to each other for semisimple Lie algebras g.","sentences":["Kobayashi-Pevzner discovered in [Selecta Math., 2016] that the failure of the multiplicity-one property in the fusion rule of Verma modules of sl2 occurs exactly when the Rankin-Cohen bracket vanishes, and 1classified all the corresponding parameters.","In this paper we provide yet another characterization for these parameters, and give a precise description of indecomposable components of the tensor product.","Furthermore, we discuss when the tensor products of two Verma modules are isomorphic to each other for semisimple Lie algebras g."],"url":"http://arxiv.org/abs/2403.19106v1","category":"math.RT"}
{"created":"2024-03-28 02:25:12","title":"Optimizing Quantum Convolutional Neural Network Architectures for Arbitrary Data Dimension","abstract":"Quantum convolutional neural networks (QCNNs) represent a promising approach in quantum machine learning, paving new directions for both quantum and classical data analysis. This approach is particularly attractive due to the absence of the barren plateau problem, a fundamental challenge in training quantum neural networks (QNNs), and its feasibility. However, a limitation arises when applying QCNNs to classical data. The network architecture is most natural when the number of input qubits is a power of two, as this number is reduced by a factor of two in each pooling layer. The number of input qubits determines the dimensions (i.e. the number of features) of the input data that can be processed, restricting the applicability of QCNN algorithms to real-world data. To address this issue, we propose a QCNN architecture capable of handling arbitrary input data dimensions while optimizing the allocation of quantum resources such as ancillary qubits and quantum gates. This optimization is not only important for minimizing computational resources, but also essential in noisy intermediate-scale quantum (NISQ) computing, as the size of the quantum circuits that can be executed reliably is limited. Through numerical simulations, we benchmarked the classification performance of various QCNN architectures when handling arbitrary input data dimensions on the MNIST and Breast Cancer datasets. The results validate that the proposed QCNN architecture achieves excellent classification performance while utilizing a minimal resource overhead, providing an optimal solution when reliable quantum computation is constrained by noise and imperfections.","sentences":["Quantum convolutional neural networks (QCNNs) represent a promising approach in quantum machine learning, paving new directions for both quantum and classical data analysis.","This approach is particularly attractive due to the absence of the barren plateau problem, a fundamental challenge in training quantum neural networks (QNNs), and its feasibility.","However, a limitation arises when applying QCNNs to classical data.","The network architecture is most natural when the number of input qubits is a power of two, as this number is reduced by a factor of two in each pooling layer.","The number of input qubits determines the dimensions (i.e. the number of features) of the input data that can be processed, restricting the applicability of QCNN algorithms to real-world data.","To address this issue, we propose a QCNN architecture capable of handling arbitrary input data dimensions while optimizing the allocation of quantum resources such as ancillary qubits and quantum gates.","This optimization is not only important for minimizing computational resources, but also essential in noisy intermediate-scale quantum (NISQ) computing, as the size of the quantum circuits that can be executed reliably is limited.","Through numerical simulations, we benchmarked the classification performance of various QCNN architectures when handling arbitrary input data dimensions on the MNIST and Breast Cancer datasets.","The results validate that the proposed QCNN architecture achieves excellent classification performance while utilizing a minimal resource overhead, providing an optimal solution when reliable quantum computation is constrained by noise and imperfections."],"url":"http://arxiv.org/abs/2403.19099v1","category":"quant-ph"}
{"created":"2024-03-28 01:54:27","title":"A Stabilized Physics Informed Neural Networks Method for Wave Equations","abstract":"In this article, we propose a novel Stabilized Physics Informed Neural Networks method (SPINNs) for solving wave equations. In general, this method not only demonstrates theoretical convergence but also exhibits higher efficiency compared to the original PINNs. By replacing the $L^2$ norm with $H^1$ norm in the learning of initial condition and boundary condition, we theoretically proved that the error of solution can be upper bounded by the risk in SPINNs. Based on this, we decompose the error of SPINNs into approximation error, statistical error and optimization error. Furthermore, by applying the approximating theory of $ReLU^3$ networks and the learning theory on Rademacher complexity, covering number and pseudo-dimension of neural networks, we present a systematical non-asymptotic convergence analysis on our method, which shows that the error of SPINNs can be well controlled if the number of training samples, depth and width of the deep neural networks have been appropriately chosen. Two illustrative numerical examples on 1-dimensional and 2-dimensional wave equations demonstrate that SPINNs can achieve a faster and better convergence than classical PINNs method.","sentences":["In this article, we propose a novel Stabilized Physics Informed Neural Networks method (SPINNs) for solving wave equations.","In general, this method not only demonstrates theoretical convergence but also exhibits higher efficiency compared to the original PINNs.","By replacing the $L^2$ norm with $H^1$ norm in the learning of initial condition and boundary condition, we theoretically proved that the error of solution can be upper bounded by the risk in SPINNs.","Based on this, we decompose the error of SPINNs into approximation error, statistical error and optimization error.","Furthermore, by applying the approximating theory of $ReLU^3$ networks and the learning theory on Rademacher complexity, covering number and pseudo-dimension of neural networks, we present a systematical non-asymptotic convergence analysis on our method, which shows that the error of SPINNs can be well controlled if the number of training samples, depth and width of the deep neural networks have been appropriately chosen.","Two illustrative numerical examples on 1-dimensional and 2-dimensional wave equations demonstrate that SPINNs can achieve a faster and better convergence than classical PINNs method."],"url":"http://arxiv.org/abs/2403.19090v1","category":"math.NA"}
{"created":"2024-03-28 01:50:54","title":"A Framework for Time-Varying Optimization via Derivative Estimation","abstract":"Optimization algorithms have a rich and fundamental relationship with ordinary differential equations given by its continuous-time limit. When the cost function varies with time -- typically in response to a dynamically changing environment -- online optimization becomes a continuous-time trajectory tracking problem. To accommodate these time variations, one typically requires some inherent knowledge about their nature such as a time derivative.   In this paper, we propose a novel construction and analysis of a continuous-time derivative estimation scheme based on \"dirty-derivatives\", and show how it naturally interfaces with continuous-time optimization algorithms using the language of ISS (Input-to-State Stability). More generally, we show how a simple Lyapunov redesign technique leads to provable suboptimality guarantees when composing this estimator with any well-behaved optimization algorithm for time-varying costs.","sentences":["Optimization algorithms have a rich and fundamental relationship with ordinary differential equations given by its continuous-time limit.","When the cost function varies with time -- typically in response to a dynamically changing environment -- online optimization becomes a continuous-time trajectory tracking problem.","To accommodate these time variations, one typically requires some inherent knowledge about their nature such as a time derivative.   ","In this paper, we propose a novel construction and analysis of a continuous-time derivative estimation scheme based on \"dirty-derivatives\", and show how it naturally interfaces with continuous-time optimization algorithms using the language of ISS (Input-to-State Stability).","More generally, we show how a simple Lyapunov redesign technique leads to provable suboptimality guarantees when composing this estimator with any well-behaved optimization algorithm for time-varying costs."],"url":"http://arxiv.org/abs/2403.19088v1","category":"math.OC"}
{"created":"2024-03-28 01:48:36","title":"Type problem, the first eigenvalue and Hardy inequalities","abstract":"In this paper, we study the relationship between the type problem and the asymptotic behaviour of the first (Dirichlet) eigenvalues $\\lambda_1(B_r)$ of ``balls'' $B_r:=\\{\\rho<r\\}$ on a complete Riemannian manifold $M$ as $r\\rightarrow +\\infty$, where $\\rho$ is a Lipschitz continuous exhaustion function with $|\\nabla\\rho|\\leq1$ a.e. on $M$. We obtain several sharp results. First, if for all $r>r_0$ \\[ r^2 \\lambda_1(B_r)\\ge \\gamma>0, \\] we obtain a sharp estimate of the volume growth: $|B_r|\\ge cr^{\\mu(\\gamma)}.$ Moreover when $\\gamma>j_0^2\\approx 5.784$, where $j_0$ denotes the first positive zero of the Bessel function $J_0$, then $M$ is hyperbolic and we have a Hardy type inequality. In the case where $r_0=0$, a sharp Hardy type inequality holds. These spectral conditions are satisfied if one assumes that $\\Delta\\rho^2\\geq2\\mu(\\gamma)>0$. In particular, when $\\inf_M\\Delta\\rho^2>4$, $M$ is hyperbolic and we get a sharp Hardy type inequality. Related results for finite volume case are also studied.","sentences":["In this paper, we study the relationship between the type problem and the asymptotic behaviour of the first (Dirichlet) eigenvalues $\\lambda_1(B_r)$ of ``balls'' $B_r:=\\{\\rho<r\\}$ on a complete Riemannian manifold $M$ as $r\\rightarrow +\\infty$, where $\\rho$ is a Lipschitz continuous exhaustion function with $|\\nabla\\rho|\\leq1$ a.e.","on $M$. We obtain several sharp results.","First, if for all $r>r_0$ \\[ r^2 \\lambda_1(B_r)\\ge \\gamma>0, \\] we obtain a sharp estimate of the volume growth: $|B_r|\\ge cr^{\\mu(\\gamma)}.$ Moreover when $\\gamma>j_0^2\\approx 5.784$, where $j_0$ denotes the first positive zero of the Bessel function $J_0$, then $M$ is hyperbolic and we have a Hardy type inequality.","In the case where $r_0=0$, a sharp Hardy type inequality holds.","These spectral conditions are satisfied if one assumes that $\\Delta\\rho^2\\geq2\\mu(\\gamma)>0$. In particular, when $\\inf_M\\Delta\\rho^2>4$, $M$ is hyperbolic and we get a sharp Hardy type inequality.","Related results for finite volume case are also studied."],"url":"http://arxiv.org/abs/2403.19086v1","category":"math.DG"}
{"created":"2024-03-28 00:20:17","title":"Stability of solutions of the porous medium equation with growth with respect to the diffusion exponent","abstract":"We consider a macroscopic model for the growth of living tissues incorporating pressure-driven dispersal and pressure-modulated proliferation. Assuming a power-law relation between the mechanical pressure and the cell density, the model can be expressed as the porous medium equation with a growth term. We prove H\\\"older continuous dependence of the solutions of the model on the diffusion exponent. The main difficulty lies in the degeneracy of the porous medium equations at vacuum. To deal with this issue, we first regularise the equation by shifting the initial data away from zero and then optimise the stability estimate derived in the regular setting.","sentences":["We consider a macroscopic model for the growth of living tissues incorporating pressure-driven dispersal and pressure-modulated proliferation.","Assuming a power-law relation between the mechanical pressure and the cell density, the model can be expressed as the porous medium equation with a growth term.","We prove H\\\"older continuous dependence of the solutions of the model on the diffusion exponent.","The main difficulty lies in the degeneracy of the porous medium equations at vacuum.","To deal with this issue, we first regularise the equation by shifting the initial data away from zero and then optimise the stability estimate derived in the regular setting."],"url":"http://arxiv.org/abs/2403.19070v1","category":"math.AP"}
{"created":"2024-03-27 23:29:31","title":"Computing the spectrum and pseudospectrum of infinite-volume operators from local patches","abstract":"We show how the spectrum of normal discrete short-range infinite-volume operators can be approximated with two-sided error control using only data from finite-sized local patches. As a corollary, we prove the computability of the spectrum of such infinite-volume operators with the additional property of finite local complexity and provide an explicit algorithm. Such operators appear in many applications, e.g. as discretizations of differential operators on unbounded domains or as so-called tight-binding Hamiltonians in solid state physics. For a large class of such operators, our result allows for the first time to establish computationally also the absence of spectrum, i.e. the existence and the size of spectral gaps. We extend our results to the $\\varepsilon$-pseudospectrum of non-normal operators, proving that also the pseudospectrum of such operators is computable.","sentences":["We show how the spectrum of normal discrete short-range infinite-volume operators can be approximated with two-sided error control using only data from finite-sized local patches.","As a corollary, we prove the computability of the spectrum of such infinite-volume operators with the additional property of finite local complexity and provide an explicit algorithm.","Such operators appear in many applications, e.g. as discretizations of differential operators on unbounded domains or as so-called tight-binding Hamiltonians in solid state physics.","For a large class of such operators, our result allows for the first time to establish computationally also the absence of spectrum, i.e. the existence and the size of spectral gaps.","We extend our results to the $\\varepsilon$-pseudospectrum of non-normal operators, proving that also the pseudospectrum of such operators is computable."],"url":"http://arxiv.org/abs/2403.19055v1","category":"math.SP"}
{"created":"2024-03-27 23:04:51","title":"Phase-space representation of coherent states generated through SUSY QM for tilted anisotropic Dirac materials","abstract":"In this paper, we examine the electron interaction within tilted anisotropic Dirac materials when subjected to external electric and magnetic fields possessing translational symmetry. Specifically, we focus on a distinct non-zero electric field magnitude, enabling the separation of the differential equation system inherent in the eigenvalue problem. Subsequently, employing supersymmetric quantum mechanics facilitates the determination of eigenstates and eigenvalues corresponding to the Hamiltonian operator. To delve into a semi-classical analysis of the system, we identify a set of coherent states. Finally, we assess the characteristics of these states using fidelity and the phase-space representation through the Wigner function.","sentences":["In this paper, we examine the electron interaction within tilted anisotropic Dirac materials when subjected to external electric and magnetic fields possessing translational symmetry.","Specifically, we focus on a distinct non-zero electric field magnitude, enabling the separation of the differential equation system inherent in the eigenvalue problem.","Subsequently, employing supersymmetric quantum mechanics facilitates the determination of eigenstates and eigenvalues corresponding to the Hamiltonian operator.","To delve into a semi-classical analysis of the system, we identify a set of coherent states.","Finally, we assess the characteristics of these states using fidelity and the phase-space representation through the Wigner function."],"url":"http://arxiv.org/abs/2403.19048v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-27 22:36:02","title":"Illicit object detection in X-ray images using Vision Transformers","abstract":"Illicit object detection is a critical task performed at various high-security locations, including airports, train stations, subways, and ports. The continuous and tedious work of examining thousands of X-ray images per hour can be mentally taxing. Thus, Deep Neural Networks (DNNs) can be used to automate the X-ray image analysis process, improve efficiency and alleviate the security officers' inspection burden. The neural architectures typically utilized in relevant literature are Convolutional Neural Networks (CNNs), with Vision Transformers (ViTs) rarely employed. In order to address this gap, this paper conducts a comprehensive evaluation of relevant ViT architectures on illicit item detection in X-ray images. This study utilizes both Transformer and hybrid backbones, such as SWIN and NextViT, and detectors, such as DINO and RT-DETR. The results demonstrate the remarkable accuracy of the DINO Transformer detector in the low-data regime, the impressive real-time performance of YOLOv8, and the effectiveness of the hybrid NextViT backbone.","sentences":["Illicit object detection is a critical task performed at various high-security locations, including airports, train stations, subways, and ports.","The continuous and tedious work of examining thousands of X-ray images per hour can be mentally taxing.","Thus, Deep Neural Networks (DNNs) can be used to automate the X-ray image analysis process, improve efficiency and alleviate the security officers' inspection burden.","The neural architectures typically utilized in relevant literature are Convolutional Neural Networks (CNNs), with Vision Transformers (ViTs) rarely employed.","In order to address this gap, this paper conducts a comprehensive evaluation of relevant ViT architectures on illicit item detection in X-ray images.","This study utilizes both Transformer and hybrid backbones, such as SWIN and NextViT, and detectors, such as DINO and RT-DETR.","The results demonstrate the remarkable accuracy of the DINO Transformer detector in the low-data regime, the impressive real-time performance of YOLOv8, and the effectiveness of the hybrid NextViT backbone."],"url":"http://arxiv.org/abs/2403.19043v1","category":"cs.CV"}
{"created":"2024-03-27 22:31:07","title":"Orchestrating Mixed-Criticality Cloud Workloads in Reconfigurable Manufacturing Systems","abstract":"The adoption of cloud computing technologies in the industry is paving the way to new manufacturing paradigms. In this paper we propose a model to optimize the orchestration of workloads with differentiated criticality levels on a cloud-enabled factory floor. Preliminary results show that it is possible to optimize the guarantees to deployed jobs without penalizing the number of schedulable jobs. We indicate future research paths to quantitatively evaluate job isolation.","sentences":["The adoption of cloud computing technologies in the industry is paving the way to new manufacturing paradigms.","In this paper we propose a model to optimize the orchestration of workloads with differentiated criticality levels on a cloud-enabled factory floor.","Preliminary results show that it is possible to optimize the guarantees to deployed jobs without penalizing the number of schedulable jobs.","We indicate future research paths to quantitatively evaluate job isolation."],"url":"http://arxiv.org/abs/2403.19042v1","category":"cs.DC"}
{"created":"2024-03-27 22:21:02","title":"Cosmic Neutrino Decoupling and its Observable Imprints: Insights from Entropic-Dual Transport","abstract":"Very different processes characterize the decoupling of neutrinos to form the cosmic neutrino background (C$\\nu$B) and the much later decoupling of photons from thermal equilibrium to form the cosmic microwave background (CMB). The C$\\nu$B emerges from the fuzzy, energy-dependent neutrinosphere and encodes the physics operating in the early universe in the temperature range $T\\sim 10\\,{\\rm MeV}$ to $T\\sim10\\,{\\rm keV}$. This is the epoch where beyond Standard Model (BSM) physics may be influential in setting the light element abundances and the necessarily distorted fossil neutrino energy spectra. Here we use techniques honed in extensive CMB studies to analyze the C$\\nu$B as calculated in detailed neutrino energy transport and nuclear reaction simulations. Our moment method, relative entropy, and differential visibility approach can leverage future high precision CMB and primordial abundance measurements to provide new insights into the C$\\nu$B and any BSM physics it encodes. We demonstrate that the evolution of the energy spectrum of the C$\\nu$B throughout the weak decoupling epoch is accurately captured in the Standard Model by only three parameters per species, a non-trivial conclusion given the deviation from thermal equilibrium. Furthermore, we can interpret each of the three parameters as physical characteristics of a non-equilibrium system. The success of our compact description within the Standard Model motivates its use also in BSM scenarios. We demonstrate how observations of primordial light element abundances can be used to place constraints on the C$\\nu$B energy spectrum, deriving response functions that can be applied for general C$\\nu$B spectral distortions. Combined with the description of those deviations that we develop here, our methods provide a convenient and powerful framework to constrain the impact of BSM physics on the C$\\nu$B.","sentences":["Very different processes characterize the decoupling of neutrinos to form the cosmic neutrino background (C$\\nu$B) and the much later decoupling of photons from thermal equilibrium to form the cosmic microwave background (CMB).","The C$\\nu$B emerges from the fuzzy, energy-dependent neutrinosphere and encodes the physics operating in the early universe in the temperature range $T\\sim 10\\,{\\rm MeV}$ to $T\\sim10\\,{\\rm keV}$.","This is the epoch where beyond Standard Model (BSM) physics may be influential in setting the light element abundances and the necessarily distorted fossil neutrino energy spectra.","Here we use techniques honed in extensive CMB studies to analyze the C$\\nu$B as calculated in detailed neutrino energy transport and nuclear reaction simulations.","Our moment method, relative entropy, and differential visibility approach can leverage future high precision CMB and primordial abundance measurements to provide new insights into the C$\\nu$B and any BSM physics it encodes.","We demonstrate that the evolution of the energy spectrum of the C$\\nu$B throughout the weak decoupling epoch is accurately captured in the Standard Model by only three parameters per species, a non-trivial conclusion given the deviation from thermal equilibrium.","Furthermore, we can interpret each of the three parameters as physical characteristics of a non-equilibrium system.","The success of our compact description within the Standard Model motivates its use also in BSM scenarios.","We demonstrate how observations of primordial light element abundances can be used to place constraints on the C$\\nu$B energy spectrum, deriving response functions that can be applied for general C$\\nu$B spectral distortions.","Combined with the description of those deviations that we develop here, our methods provide a convenient and powerful framework to constrain the impact of BSM physics on the C$\\nu$B."],"url":"http://arxiv.org/abs/2403.19038v1","category":"astro-ph.CO"}
{"created":"2024-03-27 22:13:27","title":"Note on Klein-Nishina effect in strong-field QED: the case of nonlinear Compton scattering","abstract":"Suitably normalized differential probabilities of one-photon emission in external electromagnetic fields are compared to quantify the transit of nonlinear Compton scattering to linear Compton scattering, described by the Klein-Nishina formula, and to constant crossed field treatment. The known Klein-Nishina suppression at large energies is further enforced by increasing field intensity. In view of the Ritus-Narozhny conjecture, we demonstrate that different paths in the field intensity vs. energy plane towards large values of the quantum non-linearity parameter $\\chi$ facilitate significantly different asymptotic dependencies, both in the Klein-Nishina regime and the constant crossed field regime and in between.","sentences":["Suitably normalized differential probabilities of one-photon emission in external electromagnetic fields are compared to quantify the transit of nonlinear Compton scattering to linear Compton scattering, described by the Klein-Nishina formula, and to constant crossed field treatment.","The known Klein-Nishina suppression at large energies is further enforced by increasing field intensity.","In view of the Ritus-Narozhny conjecture, we demonstrate that different paths in the field intensity vs. energy plane towards large values of the quantum non-linearity parameter $\\chi$ facilitate significantly different asymptotic dependencies, both in the Klein-Nishina regime and the constant crossed field regime and in between."],"url":"http://arxiv.org/abs/2403.19034v1","category":"hep-ph"}
{"created":"2024-03-27 20:32:04","title":"Quantum to Classical Neural Network Transfer Learning Applied to Drug Toxicity Prediction","abstract":"Toxicity is a roadblock that prevents an inordinate number of drugs from being used in potentially life-saving applications. Deep learning provides a promising solution to finding ideal drug candidates; however, the vastness of chemical space coupled with the underlying $\\mathcal{O}(n^3)$ matrix multiplication means these efforts quickly become computationally demanding. To remedy this, we present a hybrid quantum-classical neural network for predicting drug toxicity, utilizing a quantum circuit design that mimics classical neural behavior by explicitly calculating matrix products with complexity $\\mathcal{O}(n^2)$. Leveraging the Hadamard test for efficient inner product estimation rather than the conventionally used swap test, we reduce the number qubits by half and remove the need for quantum phase estimation. Directly computing matrix products quantum mechanically allows for learnable weights to be transferred from a quantum to a classical device for further training. We apply our framework to the Tox21 dataset and show that it achieves commensurate predictive accuracy to the model's fully classical $\\mathcal{O}(n^3)$ analog. Additionally, we demonstrate the model continues to learn, without disruption, once transferred to a fully classical architecture. We believe combining the quantum advantage of reduced complexity and the classical advantage of noise-free calculation will pave the way to more scalable machine learning models.","sentences":["Toxicity is a roadblock that prevents an inordinate number of drugs from being used in potentially life-saving applications.","Deep learning provides a promising solution to finding ideal drug candidates; however, the vastness of chemical space coupled with the underlying $\\mathcal{O}(n^3)$ matrix multiplication means these efforts quickly become computationally demanding.","To remedy this, we present a hybrid quantum-classical neural network for predicting drug toxicity, utilizing a quantum circuit design that mimics classical neural behavior by explicitly calculating matrix products with complexity $\\mathcal{O}(n^2)$. Leveraging the Hadamard test for efficient inner product estimation rather than the conventionally used swap test, we reduce the number qubits by half and remove the need for quantum phase estimation.","Directly computing matrix products quantum mechanically allows for learnable weights to be transferred from a quantum to a classical device for further training.","We apply our framework to the Tox21 dataset and show that it achieves commensurate predictive accuracy to the model's fully classical $\\mathcal{O}(n^3)$ analog.","Additionally, we demonstrate the model continues to learn, without disruption, once transferred to a fully classical architecture.","We believe combining the quantum advantage of reduced complexity and the classical advantage of noise-free calculation will pave the way to more scalable machine learning models."],"url":"http://arxiv.org/abs/2403.18997v1","category":"quant-ph"}
{"created":"2024-03-27 20:07:46","title":"The Sun's differential rotation is controlled by high-latitude baroclinically unstable inertial modes","abstract":"Rapidly rotating fluids have a rotation profile which depends only on the distance from the rotation axis, in accordance with the Taylor-Proudman theorem. Although the Sun was expected to be such a body, helioseismology showed that the rotation rate in the convection zone is closer to constant on radii. It has been postulated that this deviation is due to the poles being warmer than the equator by a few degrees. Using numerical simulations, we show that the pole-to-equator temperature difference cannot exceed 7 Kelvin as a result of the back-reaction of the high-latitude baroclinically unstable inertial modes. The observed amplitudes of the modes further indicate that this maximum temperature difference is reached in the Sun. We conclude that the Sun's latitudinal differential rotation reaches its maximum allowed value.","sentences":["Rapidly rotating fluids have a rotation profile which depends only on the distance from the rotation axis, in accordance with the Taylor-Proudman theorem.","Although the Sun was expected to be such a body, helioseismology showed that the rotation rate in the convection zone is closer to constant on radii.","It has been postulated that this deviation is due to the poles being warmer than the equator by a few degrees.","Using numerical simulations, we show that the pole-to-equator temperature difference cannot exceed 7 Kelvin as a result of the back-reaction of the high-latitude baroclinically unstable inertial modes.","The observed amplitudes of the modes further indicate that this maximum temperature difference is reached in the Sun.","We conclude that the Sun's latitudinal differential rotation reaches its maximum allowed value."],"url":"http://arxiv.org/abs/2403.18986v1","category":"astro-ph.SR"}
{"created":"2024-03-27 20:01:57","title":"Extension method in Dirichlet spaces with sub-Gaussian estimates and applications to regularity of jump processes on fractals","abstract":"We investigate regularity properties of some non-local equations defined on Dirichlet spaces equipped with sub-gaussian estimates for the heat kernel associated to the generator. We prove that weak solutions for homogeneous equations involving pure powers of the generator are actually H\\\"older continuous and satisfy an Harnack inequality. Our methods are based on a version of the Caffarelli-Silvestre extension method which is valid in any Dirichlet space and our results complement the existing literature on solutions of PDEs on classes of Dirichlet spaces such as fractals.","sentences":["We investigate regularity properties of some non-local equations defined on Dirichlet spaces equipped with sub-gaussian estimates for the heat kernel associated to the generator.","We prove that weak solutions for homogeneous equations involving pure powers of the generator are actually H\\\"older continuous and satisfy an Harnack inequality.","Our methods are based on a version of the Caffarelli-Silvestre extension method which is valid in any Dirichlet space and our results complement the existing literature on solutions of PDEs on classes of Dirichlet spaces such as fractals."],"url":"http://arxiv.org/abs/2403.18984v1","category":"math.AP"}
{"created":"2024-03-27 19:49:45","title":"Semiclassical wave-packets for weakly nonlinear Schr\u00f6dinger equations with rotation","abstract":"We consider semiclassically scaled, weakly nonlinear Schr\\\"odinger equations with external confining potentials and additional angular-momentum rotation term. This type of model arises in the Gross-Pitaevskii theory of trapped, rotating quantum gases. We construct asymptotic solutions in the form of semiclassical wave-packets, which are concentrated in both space and in frequency around an classical Hamiltonian phase-space flow. The rotation term is thereby seen to alter this flow, but not the corresponding classical action.","sentences":["We consider semiclassically scaled, weakly nonlinear Schr\\\"odinger equations with external confining potentials and additional angular-momentum rotation term.","This type of model arises in the Gross-Pitaevskii theory of trapped, rotating quantum gases.","We construct asymptotic solutions in the form of semiclassical wave-packets, which are concentrated in both space and in frequency around an classical Hamiltonian phase-space flow.","The rotation term is thereby seen to alter this flow, but not the corresponding classical action."],"url":"http://arxiv.org/abs/2403.18977v1","category":"math.AP"}
{"created":"2024-03-27 18:39:18","title":"Transfer matrix analysis of non-hermitian Hamiltonians: asymptotic spectra and topological eigenvalues","abstract":"Transfer matrix techniques are used to {provide} a new proof of Widom's results on the asymptotic spectral theory of finite block Toeplitz matrices. Furthermore, a rigorous treatment of {the} skin effect, spectral outliers, the generalized Brillouin zone and the bulk-boundary correspondence in such systems is given. This covers chiral Hamiltonians with topological eigenvalues close to zero, but no line-gap.","sentences":["Transfer matrix techniques are used to {provide} a new proof of Widom's results on the asymptotic spectral theory of finite block Toeplitz matrices.","Furthermore, a rigorous treatment of {the} skin effect, spectral outliers, the generalized Brillouin zone and the bulk-boundary correspondence in such systems is given.","This covers chiral Hamiltonians with topological eigenvalues close to zero, but no line-gap."],"url":"http://arxiv.org/abs/2403.18942v1","category":"math-ph"}
{"created":"2024-03-27 18:21:41","title":"Local topology for periodic Hamiltonians and fuzzy tori","abstract":"A variety of local index formulas is constructed for quantum Hamiltonians with periodic boundary conditions. All dimensions of physical space as well as many symmetry constraints are covered, notably one-dimensional systems in Class DIII as well as two- and three-dimensional systems in Class AII. The constructions are based on several periodic variations of the spectral localizer and are rooted in the existence of underlying fuzzy tori. For these latter, a general invariant theory is developed.","sentences":["A variety of local index formulas is constructed for quantum Hamiltonians with periodic boundary conditions.","All dimensions of physical space as well as many symmetry constraints are covered, notably one-dimensional systems in Class DIII as well as two- and three-dimensional systems in Class AII.","The constructions are based on several periodic variations of the spectral localizer and are rooted in the existence of underlying fuzzy tori.","For these latter, a general invariant theory is developed."],"url":"http://arxiv.org/abs/2403.18931v1","category":"math-ph"}
{"created":"2024-03-27 18:17:23","title":"Optimal Coherent Quantum Phase Estimation via Tapering","abstract":"Quantum phase estimation is one of the fundamental primitives that underpins many quantum algorithms, including quantum amplitude estimation, the HHL algorithm for solving linear systems of equations, and quantum principal component analysis. Due to its significance as a subroutine, in this work, we study the coherent version of the phase estimation problem, where given an arbitrary input state and black-box access to unitaries $U$ and controlled-$U$, the goal is to estimate the phases of $U$ in superposition. Unlike most existing phase estimation algorithms, which employ intermediary measurements steps that inevitably destroy coherence, only a couple of algorithms, including the well-known standard quantum phase estimation algorithm, consider this coherent setting. In this work, we propose an improved version of this standard algorithm that utilizes tapering/window functions. Our algorithm, which we call tapered quantum phase estimation algorithm, achieves the optimal query complexity (total number of calls to $U$ and controlled-$U$) without requiring the use of a computationally expensive quantum sorting network for median computation, which the standard algorithm uses to boost the success probability arbitrarily close to one. We also show that the tapering functions that we use are optimal by formulating optimization problems with different optimization criteria. Beyond the asymptotic regime, we also provide non-asymptotic query complexity of our algorithm, as it is crucial for practical implementation. Finally, we also propose an efficient algorithm that prepares the quantum state corresponding to the optimal tapering function.","sentences":["Quantum phase estimation is one of the fundamental primitives that underpins many quantum algorithms, including quantum amplitude estimation, the HHL algorithm for solving linear systems of equations, and quantum principal component analysis.","Due to its significance as a subroutine, in this work, we study the coherent version of the phase estimation problem, where given an arbitrary input state and black-box access to unitaries $U$ and controlled-$U$, the goal is to estimate the phases of $U$ in superposition.","Unlike most existing phase estimation algorithms, which employ intermediary measurements steps that inevitably destroy coherence, only a couple of algorithms, including the well-known standard quantum phase estimation algorithm, consider this coherent setting.","In this work, we propose an improved version of this standard algorithm that utilizes tapering/window functions.","Our algorithm, which we call tapered quantum phase estimation algorithm, achieves the optimal query complexity (total number of calls to $U$ and controlled-$U$) without requiring the use of a computationally expensive quantum sorting network for median computation, which the standard algorithm uses to boost the success probability arbitrarily close to one.","We also show that the tapering functions that we use are optimal by formulating optimization problems with different optimization criteria.","Beyond the asymptotic regime, we also provide non-asymptotic query complexity of our algorithm, as it is crucial for practical implementation.","Finally, we also propose an efficient algorithm that prepares the quantum state corresponding to the optimal tapering function."],"url":"http://arxiv.org/abs/2403.18927v1","category":"quant-ph"}
{"created":"2024-03-27 18:16:05","title":"Sum of terms of recurrence sequences in the solution sets of generalized Pell equations","abstract":"Let $(X_{k})_{k\\geq 1}$ and $(Y_k)_{k\\geq 1}$ be the sequence of $X$ and $Y$-coordinates of the positive integer solutions $(x, y)$ of the equation $x^2 - dy^2 = t$. In this paper we completely describe those recurrence sequences such that sums of two terms recurrence sequences in the solution sets of generalized Pell equations are infinitely many. Further, we give an upper bound for the number of such terms when there are only finitely many of them. This work is motivated by the recent paper Hajdu and Sebesty\\'en (Int. J. Number Theory 18 (2022), 1605-1612).","sentences":["Let $(X_{k})_{k\\geq 1}$ and $(Y_k)_{k\\geq 1}$ be the sequence of $X$ and $Y$-coordinates of the positive integer solutions $(x, y)$ of the equation $x^2 - dy^2 = t$. In this paper we completely describe those recurrence sequences such that sums of two terms recurrence sequences in the solution sets of generalized Pell equations are infinitely many.","Further, we give an upper bound for the number of such terms when there are only finitely many of them.","This work is motivated by the recent paper Hajdu and Sebesty\\'en (Int.","J. Number Theory 18 (2022), 1605-1612)."],"url":"http://arxiv.org/abs/2403.18924v1","category":"math.NT"}
{"created":"2024-03-27 18:02:34","title":"Can spinodal decomposition occur during decompression-induced vesiculation of magma?","abstract":"Volcanic eruptions are driven by decompression-induced vesiculation of volatiles in magma. Its initial phase has long been described as nucleation. Recently, it was proposed that spinodal decomposition (SD; an energetically spontaneous phase separation without forming a distinct interface) may occur during magma vesiculation. This suggestion is currently based only on qualitative textural observations of the products of decompression experiments. In this study, I used a simple thermodynamic approach to quantitatively investigate whether SD can occur during magma vesiculation. Using the previous water solubility data, I plotted the binodal and spinodal curves on the chemical composition-pressure plane for several hydrous magmas, treating them as two-component symmetric regular solutions of silicate and water. The spinodal curves were much lower than the binodal curves at pressures sufficiently below the second critical endpoints. In addition, the final pressure of all the decompression experiments performed to date fell between these two curves. This suggests that SD is unlikely to occur in the pressure range of magmatic processes in the continental crust or at realistic decompression rates i.e. magma vesiculation results from nucleation, as previously suggested. To test this thermodynamic approach, I estimated the microscopic surface tension between the melt and bubble nucleus in previous decompression experiments by substituting the spinodal pressure into the nonclassical nucleation theory (non-CNT) equation for the dependence of the surface tension on the degree of supersaturation. The resulting values were significantly more scattered than those obtained by the conventional method: inversion of the experimental bubble number density using the classical nucleation theory (CNT) formula. Thus, it is difficult to judge whether the non-CNT equation is appropriate for magma systems.","sentences":["Volcanic eruptions are driven by decompression-induced vesiculation of volatiles in magma.","Its initial phase has long been described as nucleation.","Recently, it was proposed that spinodal decomposition (SD; an energetically spontaneous phase separation without forming a distinct interface) may occur during magma vesiculation.","This suggestion is currently based only on qualitative textural observations of the products of decompression experiments.","In this study, I used a simple thermodynamic approach to quantitatively investigate whether SD can occur during magma vesiculation.","Using the previous water solubility data, I plotted the binodal and spinodal curves on the chemical composition-pressure plane for several hydrous magmas, treating them as two-component symmetric regular solutions of silicate and water.","The spinodal curves were much lower than the binodal curves at pressures sufficiently below the second critical endpoints.","In addition, the final pressure of all the decompression experiments performed to date fell between these two curves.","This suggests that SD is unlikely to occur in the pressure range of magmatic processes in the continental crust or at realistic decompression rates i.e. magma vesiculation results from nucleation, as previously suggested.","To test this thermodynamic approach, I estimated the microscopic surface tension between the melt and bubble nucleus in previous decompression experiments by substituting the spinodal pressure into the nonclassical nucleation theory (non-CNT) equation for the dependence of the surface tension on the degree of supersaturation.","The resulting values were significantly more scattered than those obtained by the conventional method: inversion of the experimental bubble number density using the classical nucleation theory (CNT) formula.","Thus, it is difficult to judge whether the non-CNT equation is appropriate for magma systems."],"url":"http://arxiv.org/abs/2403.18909v1","category":"physics.geo-ph"}
{"created":"2024-03-27 18:00:01","title":"The Boundary Proposal","abstract":"One of the leading ideas for the beginning of the Universe is the Hartle-Hawking `No-Boundary Proposal.' Since the Cobordism Conjecture claims that any spacetime allows for a dynamical boundary, we suggest that one may equally well consider a `Boundary Proposal'. Specifically, the corresponding euclidean instanton is a sphere with two holes around north and south pole cut out. Analogously to the Hartle-Hawking proposal, the sphere is then cut in two at the equator and half of it is dropped. The equator is glued to an expanding Lorentzian de Sitter space, implementing a beginning of the Universe with a spacelike spherical boundary at its earliest moment. This process is in principle on equal footing with the one based on the no-boundary instanton. In fact, if the Linde-Vilenkin sign choice is used, this `Boundary' creation process may even dominate. An intriguing implication arises if tensionless end-of-the-world branes, as familiar from type-IIA or M-theory, are available: Analogously to the Boundary Proposal, one may then be able to create a compact, flat torus universe from nothing, without any exponential suppression or enhancement factors.","sentences":["One of the leading ideas for the beginning of the Universe is the Hartle-Hawking `No-Boundary Proposal.'","Since the Cobordism Conjecture claims that any spacetime allows for a dynamical boundary, we suggest that one may equally well consider a `Boundary Proposal'.","Specifically, the corresponding euclidean instanton is a sphere with two holes around north and south pole cut out.","Analogously to the Hartle-Hawking proposal, the sphere is then cut in two at the equator and half of it is dropped.","The equator is glued to an expanding Lorentzian de Sitter space, implementing a beginning of the Universe with a spacelike spherical boundary at its earliest moment.","This process is in principle on equal footing with the one based on the no-boundary instanton.","In fact, if the Linde-Vilenkin sign choice is used, this `Boundary' creation process may even dominate.","An intriguing implication arises if tensionless end-of-the-world branes, as familiar from type-IIA or M-theory, are available: Analogously to the Boundary Proposal, one may then be able to create a compact, flat torus universe from nothing, without any exponential suppression or enhancement factors."],"url":"http://arxiv.org/abs/2403.18892v1","category":"hep-th"}
{"created":"2024-03-28 17:52:24","title":"Siamese Vision Transformers are Scalable Audio-visual Learners","abstract":"Traditional audio-visual methods rely on independent audio and visual backbones, which is costly and not scalable. In this work, we investigate using an audio-visual siamese network (AVSiam) for efficient and scalable audio-visual pretraining. Our framework uses a single shared vision transformer backbone to process audio and visual inputs, improving its parameter efficiency, reducing the GPU memory footprint, and allowing us to scale our method to larger datasets and model sizes. We pretrain our model using a contrastive audio-visual matching objective with a multi-ratio random masking scheme, which enables our model to process larger audio-visual instance batches, helpful for contrastive learning. Unlike prior audio-visual methods, our method can robustly handle audio, visual, and audio-visual inputs with a single shared ViT backbone. Furthermore, despite using the shared backbone for both modalities, AVSiam achieves competitive or even better results than prior methods on AudioSet and VGGSound for audio-visual classification and retrieval. Our code is available at https://github.com/GenjiB/AVSiam","sentences":["Traditional audio-visual methods rely on independent audio and visual backbones, which is costly and not scalable.","In this work, we investigate using an audio-visual siamese network (AVSiam) for efficient and scalable audio-visual pretraining.","Our framework uses a single shared vision transformer backbone to process audio and visual inputs, improving its parameter efficiency, reducing the GPU memory footprint, and allowing us to scale our method to larger datasets and model sizes.","We pretrain our model using a contrastive audio-visual matching objective with a multi-ratio random masking scheme, which enables our model to process larger audio-visual instance batches, helpful for contrastive learning.","Unlike prior audio-visual methods, our method can robustly handle audio, visual, and audio-visual inputs with a single shared ViT backbone.","Furthermore, despite using the shared backbone for both modalities, AVSiam achieves competitive or even better results than prior methods on AudioSet and VGGSound for audio-visual classification and retrieval.","Our code is available at https://github.com/GenjiB/AVSiam"],"url":"http://arxiv.org/abs/2403.19638v1","category":"cs.CV"}
{"created":"2024-03-28 17:45:03","title":"Top-$k$ Classification and Cardinality-Aware Prediction","abstract":"We present a detailed study of top-$k$ classification, the task of predicting the $k$ most probable classes for an input, extending beyond single-class prediction. We demonstrate that several prevalent surrogate loss functions in multi-class classification, such as comp-sum and constrained losses, are supported by $H$-consistency bounds with respect to the top-$k$ loss. These bounds guarantee consistency in relation to the hypothesis set $H$, providing stronger guarantees than Bayes-consistency due to their non-asymptotic and hypothesis-set specific nature. To address the trade-off between accuracy and cardinality $k$, we further introduce cardinality-aware loss functions through instance-dependent cost-sensitive learning. For these functions, we derive cost-sensitive comp-sum and constrained surrogate losses, establishing their $H$-consistency bounds and Bayes-consistency. Minimizing these losses leads to new cardinality-aware algorithms for top-$k$ classification. We report the results of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN datasets demonstrating the effectiveness and benefit of these algorithms.","sentences":["We present a detailed study of top-$k$ classification, the task of predicting the $k$ most probable classes for an input, extending beyond single-class prediction.","We demonstrate that several prevalent surrogate loss functions in multi-class classification, such as comp-sum and constrained losses, are supported by $H$-consistency bounds with respect to the top-$k$ loss.","These bounds guarantee consistency in relation to the hypothesis set $H$, providing stronger guarantees than Bayes-consistency due to their non-asymptotic and hypothesis-set specific nature.","To address the trade-off between accuracy and cardinality $k$, we further introduce cardinality-aware loss functions through instance-dependent cost-sensitive learning.","For these functions, we derive cost-sensitive comp-sum and constrained surrogate losses, establishing their $H$-consistency bounds and Bayes-consistency.","Minimizing these losses leads to new cardinality-aware algorithms for top-$k$ classification.","We report the results of extensive experiments on CIFAR-100, ImageNet, CIFAR-10, and SVHN datasets demonstrating the effectiveness and benefit of these algorithms."],"url":"http://arxiv.org/abs/2403.19625v1","category":"cs.LG"}
{"created":"2024-03-28 17:32:01","title":"ILPO-NET: Network for the invariant recognition of arbitrary volumetric patterns in 3D","abstract":"Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis. Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations. While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research. Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions. Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIST. Beyond these demonstrations, ILPO-Net's rotational invariance paves the way for other applications across multiple disciplines. Our code is publicly available at https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet.","sentences":["Effective recognition of spatial patterns and learning their hierarchy is crucial in modern spatial data analysis.","Volumetric data applications seek techniques ensuring invariance not only to shifts but also to pattern rotations.","While traditional methods can readily achieve translational invariance, rotational invariance possesses multiple challenges and remains an active area of research.","Here, we present ILPO-Net (Invariant to Local Patterns Orientation Network), a novel approach that handles arbitrarily shaped patterns with the convolutional operation inherently invariant to local spatial pattern orientations using the Wigner matrix expansions.","Our architecture seamlessly integrates the new convolution operator and, when benchmarked on diverse volumetric datasets such as MedMNIST and CATH, demonstrates superior performance over the baselines with significantly reduced parameter counts - up to 1000 times fewer in the case of MedMNIST.","Beyond these demonstrations, ILPO-Net's rotational invariance paves the way for other applications across multiple disciplines.","Our code is publicly available at https://gricad-gitlab.univ-grenoble-alpes.fr/GruLab/ILPONet."],"url":"http://arxiv.org/abs/2403.19612v1","category":"cs.CV"}
{"created":"2024-03-28 17:04:07","title":"The Bad Batches: Enhancing Self-Supervised Learning in Image Classification Through Representative Batch Curation","abstract":"The pursuit of learning robust representations without human supervision is a longstanding challenge. The recent advancements in self-supervised contrastive learning approaches have demonstrated high performance across various representation learning challenges. However, current methods depend on the random transformation of training examples, resulting in some cases of unrepresentative positive pairs that can have a large impact on learning. This limitation not only impedes the convergence of the learning process but the robustness of the learnt representation as well as requiring larger batch sizes to improve robustness to such bad batches. This paper attempts to alleviate the influence of false positive and false negative pairs by employing pairwise similarity calculations through the Fr\\'echet ResNet Distance (FRD), thereby obtaining robust representations from unlabelled data. The effectiveness of the proposed method is substantiated by empirical results, where a linear classifier trained on self-supervised contrastive representations achieved an impressive 87.74\\% top-1 accuracy on STL10 and 99.31\\% on the Flower102 dataset. These results emphasize the potential of the proposed approach in pushing the boundaries of the state-of-the-art in self-supervised contrastive learning, particularly for image classification tasks.","sentences":["The pursuit of learning robust representations without human supervision is a longstanding challenge.","The recent advancements in self-supervised contrastive learning approaches have demonstrated high performance across various representation learning challenges.","However, current methods depend on the random transformation of training examples, resulting in some cases of unrepresentative positive pairs that can have a large impact on learning.","This limitation not only impedes the convergence of the learning process but the robustness of the learnt representation as well as requiring larger batch sizes to improve robustness to such bad batches.","This paper attempts to alleviate the influence of false positive and false negative pairs by employing pairwise similarity calculations through the Fr\\'echet ResNet Distance (FRD), thereby obtaining robust representations from unlabelled data.","The effectiveness of the proposed method is substantiated by empirical results, where a linear classifier trained on self-supervised contrastive representations achieved an impressive 87.74\\% top-1 accuracy on STL10 and 99.31\\% on the Flower102 dataset.","These results emphasize the potential of the proposed approach in pushing the boundaries of the state-of-the-art in self-supervised contrastive learning, particularly for image classification tasks."],"url":"http://arxiv.org/abs/2403.19579v1","category":"cs.CV"}
{"created":"2024-03-28 16:44:20","title":"Exploring Communication Dynamics: Eye-tracking Analysis in Pair Programming of Computer Science Education","abstract":"Pair programming is widely recognized as an effective educational tool in computer science that promotes collaborative learning and mirrors real-world work dynamics. However, communication breakdowns within pairs significantly challenge this learning process. In this study, we use eye-tracking data recorded during pair programming sessions to study communication dynamics between various pair programming roles across different student, expert, and mixed group cohorts containing 19 participants. By combining eye-tracking data analysis with focus group interviews and questionnaires, we provide insights into communication's multifaceted nature in pair programming. Our findings highlight distinct eye-tracking patterns indicating changes in communication skills across group compositions, with participants prioritizing code exploration over communication, especially during challenging tasks. Further, students showed a preference for pairing with experts, emphasizing the importance of understanding group formation in pair programming scenarios. These insights emphasize the importance of understanding group dynamics and enhancing communication skills through pair programming for successful outcomes in computer science education.","sentences":["Pair programming is widely recognized as an effective educational tool in computer science that promotes collaborative learning and mirrors real-world work dynamics.","However, communication breakdowns within pairs significantly challenge this learning process.","In this study, we use eye-tracking data recorded during pair programming sessions to study communication dynamics between various pair programming roles across different student, expert, and mixed group cohorts containing 19 participants.","By combining eye-tracking data analysis with focus group interviews and questionnaires, we provide insights into communication's multifaceted nature in pair programming.","Our findings highlight distinct eye-tracking patterns indicating changes in communication skills across group compositions, with participants prioritizing code exploration over communication, especially during challenging tasks.","Further, students showed a preference for pairing with experts, emphasizing the importance of understanding group formation in pair programming scenarios.","These insights emphasize the importance of understanding group dynamics and enhancing communication skills through pair programming for successful outcomes in computer science education."],"url":"http://arxiv.org/abs/2403.19560v1","category":"cs.HC"}
{"created":"2024-03-28 16:12:01","title":"Multi-product maximal covering second-level facility location problem","abstract":"This paper introduces a new hierarchical facility location model with three levels: first-level facilities which manufacture different products, second-level facilities which act as warehouses and a third-level consisting of the clients who demand the products that have been manufactured in the first level and stored in the second level. In this model, called multi-product maximal covering second-level facility location problem (SL-MCFLP), the aim is to decide the location of the second-level facilities and the products to be stored in each of them maximizing the overall clients' satisfaction with respect their coverage. To deal with this model, we introduce a Mixed Integer Linear Program (MILP) which is reinforced by some families of valid inequalities. Since some of these families have an exponential number of constraints, separation algorithms are proposed. In addition, three variants of a matheuristic procedure are developed. Computational studies are included, showing the potentials and limits of the formulation and the effectiveness of the heuristic.","sentences":["This paper introduces a new hierarchical facility location model with three levels: first-level facilities which manufacture different products, second-level facilities which act as warehouses and a third-level consisting of the clients who demand the products that have been manufactured in the first level and stored in the second level.","In this model, called multi-product maximal covering second-level facility location problem (SL-MCFLP), the aim is to decide the location of the second-level facilities and the products to be stored in each of them maximizing the overall clients' satisfaction with respect their coverage.","To deal with this model, we introduce a Mixed Integer Linear Program (MILP) which is reinforced by some families of valid inequalities.","Since some of these families have an exponential number of constraints, separation algorithms are proposed.","In addition, three variants of a matheuristic procedure are developed.","Computational studies are included, showing the potentials and limits of the formulation and the effectiveness of the heuristic."],"url":"http://arxiv.org/abs/2403.19537v1","category":"math.OC"}
{"created":"2024-03-28 15:57:20","title":"Model Stock: All we need is just a few fine-tuned models","abstract":"This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model. We demonstrate the efficacy of Model Stock with fine-tuned models based upon pre-trained CLIP architectures, achieving remarkable performance on both ID and OOD tasks on the standard benchmarks, all while barely bringing extra computational demands. Our code and pre-trained models are available at https://github.com/naver-ai/model-stock.","sentences":["This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance.","Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy.","Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space.","Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training.","Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models.","This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a more optimized-averaged model.","We demonstrate the efficacy of Model Stock with fine-tuned models based upon pre-trained CLIP architectures, achieving remarkable performance on both ID and OOD tasks on the standard benchmarks, all while barely bringing extra computational demands.","Our code and pre-trained models are available at https://github.com/naver-ai/model-stock."],"url":"http://arxiv.org/abs/2403.19522v1","category":"cs.LG"}
{"created":"2024-03-28 15:47:13","title":"Maximum Likelihood Estimation on Stochastic Blockmodels for Directed Graph Clustering","abstract":"This paper studies the directed graph clustering problem through the lens of statistics, where we formulate clustering as estimating underlying communities in the directed stochastic block model (DSBM). We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed graph structure. In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed graph statistics: edge density and edge orientation. Building on this new formulation of directed clustering, we introduce two efficient and interpretable directed clustering algorithms, a spectral clustering algorithm and a semidefinite programming based clustering algorithm. We provide a theoretical upper bound on the number of misclustered vertices of the spectral clustering algorithm using tools from matrix perturbation theory. We compare, both quantitatively and qualitatively, our proposed algorithms with existing directed clustering methods on both synthetic and real-world data, thus providing further ground to our theoretical contributions.","sentences":["This paper studies the directed graph clustering problem through the lens of statistics, where we formulate clustering as estimating underlying communities in the directed stochastic block model (DSBM).","We conduct the maximum likelihood estimation (MLE) on the DSBM and thereby ascertain the most probable community assignment given the observed graph structure.","In addition to the statistical point of view, we further establish the equivalence between this MLE formulation and a novel flow optimization heuristic, which jointly considers two important directed graph statistics: edge density and edge orientation.","Building on this new formulation of directed clustering, we introduce two efficient and interpretable directed clustering algorithms, a spectral clustering algorithm and a semidefinite programming based clustering algorithm.","We provide a theoretical upper bound on the number of misclustered vertices of the spectral clustering algorithm using tools from matrix perturbation theory.","We compare, both quantitatively and qualitatively, our proposed algorithms with existing directed clustering methods on both synthetic and real-world data, thus providing further ground to our theoretical contributions."],"url":"http://arxiv.org/abs/2403.19516v1","category":"stat.ML"}
{"created":"2024-03-28 15:29:30","title":"Tensor Network-Constrained Kernel Machines as Gaussian Processes","abstract":"Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings. In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters. We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters. We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction. We thereby establish a connection between TN-constrained kernel machines and GPs.","sentences":["Tensor Networks (TNs) have recently been used to speed up kernel machines by constraining the model weights, yielding exponential computational and storage savings.","In this paper we prove that the outputs of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained kernel machines recover a Gaussian Process (GP), which we fully characterize, when placing i.i.d. priors over their parameters.","We analyze the convergence of both CPD and TT-constrained models, and show how TT yields models exhibiting more GP behavior compared to CPD, for the same number of model parameters.","We empirically observe this behavior in two numerical experiments where we respectively analyze the convergence to the GP and the performance at prediction.","We thereby establish a connection between TN-constrained kernel machines and GPs."],"url":"http://arxiv.org/abs/2403.19500v1","category":"cs.LG"}
{"created":"2024-03-28 15:26:38","title":"Regression with Multi-Expert Deferral","abstract":"Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts. While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space. In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts. We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function. We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds. These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set-specific. Our framework is versatile, applying to multiple experts, accommodating any bounded regression losses, addressing both instance-dependent and label-dependent costs, and supporting both single-stage and two-stage methods. A by-product is that our single-stage formulation includes the recent regression with abstention framework (Cheng et al., 2023) as a special case, where only a single expert, the squared loss and a label-independent cost are considered. Minimizing our proposed loss functions directly leads to novel algorithms for regression with deferral. We report the results of extensive experiments showing the effectiveness of our proposed algorithms.","sentences":["Learning to defer with multiple experts is a framework where the learner can choose to defer the prediction to several experts.","While this problem has received significant attention in classification contexts, it presents unique challenges in regression due to the infinite and continuous nature of the label space.","In this work, we introduce a novel framework of regression with deferral, which involves deferring the prediction to multiple experts.","We present a comprehensive analysis for both the single-stage scenario, where there is simultaneous learning of predictor and deferral functions, and the two-stage scenario, which involves a pre-trained predictor with a learned deferral function.","We introduce new surrogate loss functions for both scenarios and prove that they are supported by $H$-consistency bounds.","These bounds provide consistency guarantees that are stronger than Bayes consistency, as they are non-asymptotic and hypothesis set-specific.","Our framework is versatile, applying to multiple experts, accommodating any bounded regression losses, addressing both instance-dependent and label-dependent costs, and supporting both single-stage and two-stage methods.","A by-product is that our single-stage formulation includes the recent regression with abstention framework (Cheng et al., 2023) as a special case, where only a single expert, the squared loss and a label-independent cost are considered.","Minimizing our proposed loss functions directly leads to novel algorithms for regression with deferral.","We report the results of extensive experiments showing the effectiveness of our proposed algorithms."],"url":"http://arxiv.org/abs/2403.19494v1","category":"cs.LG"}
{"created":"2024-03-28 15:02:28","title":"A theoretical framework for the design and analysis of computational thinking problems in education","abstract":"The field of computational thinking education has grown in recent years as researchers and educators have sought to develop and assess students' computational thinking abilities. While much of the research in this area has focused on defining computational thinking, the competencies it involves and how to assess them in teaching and learning contexts, this work takes a different approach. We provide a more situated perspective on computational thinking, focusing on the types of problems that require computational thinking skills to be solved and the features that support these processes. We develop a framework for analysing existing computational thinking problems in an educational context. We conduct a comprehensive literature review to identify prototypical activities from areas where computational thinking is typically pursued in education. We identify the main components and characteristics of these activities, along with their influence on activating computational thinking competencies. The framework provides a catalogue of computational thinking skills that can be used to understand the relationship between problem features and competencies activated. This study contributes to the field of computational thinking education by offering a tool for evaluating and revising existing problems to activate specific skills and for assisting in designing new problems that target the development of particular competencies. The results of this study may be of interest to researchers and educators working in computational thinking education.","sentences":["The field of computational thinking education has grown in recent years as researchers and educators have sought to develop and assess students' computational thinking abilities.","While much of the research in this area has focused on defining computational thinking, the competencies it involves and how to assess them in teaching and learning contexts, this work takes a different approach.","We provide a more situated perspective on computational thinking, focusing on the types of problems that require computational thinking skills to be solved and the features that support these processes.","We develop a framework for analysing existing computational thinking problems in an educational context.","We conduct a comprehensive literature review to identify prototypical activities from areas where computational thinking is typically pursued in education.","We identify the main components and characteristics of these activities, along with their influence on activating computational thinking competencies.","The framework provides a catalogue of computational thinking skills that can be used to understand the relationship between problem features and competencies activated.","This study contributes to the field of computational thinking education by offering a tool for evaluating and revising existing problems to activate specific skills and for assisting in designing new problems that target the development of particular competencies.","The results of this study may be of interest to researchers and educators working in computational thinking education."],"url":"http://arxiv.org/abs/2403.19475v1","category":"cs.HC"}
{"created":"2024-03-28 14:15:50","title":"EDA-Driven Preprocessing for SAT Solving","abstract":"Effective formulation of problems into Conjunctive Normal Form (CNF) is critical in modern Boolean Satisfiability (SAT) solving for optimizing solver performance. Addressing the limitations of existing methods, our Electronic Design Automation (EDA)-driven preprocessing framework introduces a novel methodology for preparing SAT instances, leveraging both circuit and CNF formats for enhanced flexibility and efficiency. Central to our approach is the integration of a new logic synthesis technique, guided by a reinforcement learning agent, and a novel cost-customized LUT mapping strategy, enabling efficient handling of diverse SAT challenges. By transforming the SAT competition benchmarks into circuit instances, our framework demonstrates substantial performance improvements, as evidenced by a 52.42% reduction on average compared to solving directly. Moreover, our framework achieves a remarkable 96.14% runtime reduction on average for a set of logic equivalence checking problems that exhibit inherent circuit structures. These results highlight the effectiveness and versatility of our approach in handling both CNF and circuit instances. The code is available at https://github.com/cure-lab/EDA4SAT.","sentences":["Effective formulation of problems into Conjunctive Normal Form (CNF) is critical in modern Boolean Satisfiability (SAT) solving for optimizing solver performance.","Addressing the limitations of existing methods, our Electronic Design Automation (EDA)-driven preprocessing framework introduces a novel methodology for preparing SAT instances, leveraging both circuit and CNF formats for enhanced flexibility and efficiency.","Central to our approach is the integration of a new logic synthesis technique, guided by a reinforcement learning agent, and a novel cost-customized LUT mapping strategy, enabling efficient handling of diverse SAT challenges.","By transforming the SAT competition benchmarks into circuit instances, our framework demonstrates substantial performance improvements, as evidenced by a 52.42% reduction on average compared to solving directly.","Moreover, our framework achieves a remarkable 96.14% runtime reduction on average for a set of logic equivalence checking problems that exhibit inherent circuit structures.","These results highlight the effectiveness and versatility of our approach in handling both CNF and circuit instances.","The code is available at https://github.com/cure-lab/EDA4SAT."],"url":"http://arxiv.org/abs/2403.19446v1","category":"cs.LO"}
{"created":"2024-03-28 13:55:51","title":"Echo-chambers and Idea Labs: Communication Styles on Twitter","abstract":"This paper investigates the communication styles and structures of Twitter (X) communities within the vaccination context. While mainstream research primarily focuses on the echo-chamber phenomenon, wherein certain ideas are reinforced and participants are isolated from opposing opinions, this study reveals the presence of diverse communication styles across various communities. In addition to the communities exhibiting echo-chamber behavior, this research uncovers communities with distinct communication patterns. By shedding light on the nuanced nature of communication within social networks, this study emphasizes the significance of understanding the diversity of perspectives within online communities.","sentences":["This paper investigates the communication styles and structures of Twitter (X) communities within the vaccination context.","While mainstream research primarily focuses on the echo-chamber phenomenon, wherein certain ideas are reinforced and participants are isolated from opposing opinions, this study reveals the presence of diverse communication styles across various communities.","In addition to the communities exhibiting echo-chamber behavior, this research uncovers communities with distinct communication patterns.","By shedding light on the nuanced nature of communication within social networks, this study emphasizes the significance of understanding the diversity of perspectives within online communities."],"url":"http://arxiv.org/abs/2403.19423v1","category":"cs.SI"}
{"created":"2024-03-28 10:31:23","title":"FlowDepth: Decoupling Optical Flow for Self-Supervised Monocular Depth Estimation","abstract":"Self-supervised multi-frame methods have currently achieved promising results in depth estimation. However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption. Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images. To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level. Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem. For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem. Experimental results on the KITTI and Cityscapes datasets show that our method outperforms the state-of-the-art methods.","sentences":["Self-supervised multi-frame methods have currently achieved promising results in depth estimation.","However, these methods often suffer from mismatch problems due to the moving objects, which break the static assumption.","Additionally, unfairness can occur when calculating photometric errors in high-freq or low-texture regions of the images.","To address these issues, existing approaches use additional semantic priori black-box networks to separate moving objects and improve the model only at the loss level.","Therefore, we propose FlowDepth, where a Dynamic Motion Flow Module (DMFM) decouples the optical flow by a mechanism-based approach and warps the dynamic regions thus solving the mismatch problem.","For the unfairness of photometric errors caused by high-freq and low-texture regions, we use Depth-Cue-Aware Blur (DCABlur) and Cost-Volume sparsity loss respectively at the input and the loss level to solve the problem.","Experimental results on the KITTI and Cityscapes datasets show that our method outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2403.19294v1","category":"cs.CV"}
{"created":"2024-03-28 10:13:34","title":"Going Beyond Word Matching: Syntax Improves In-context Example Selection for Machine Translation","abstract":"In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task. How to select informative examples remains an open issue. Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge. In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance. In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria. Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions.","sentences":["In-context learning (ICL) is the trending prompting strategy in the era of large language models (LLMs), where a few examples are demonstrated to evoke LLMs' power for a given task.","How to select informative examples remains an open issue.","Previous works on in-context example selection for machine translation (MT) focus on superficial word-level features while ignoring deep syntax-level knowledge.","In this paper, we propose a syntax-based in-context example selection method for MT, by computing the syntactic similarity between dependency trees using Polynomial Distance.","In addition, we propose an ensemble strategy combining examples selected by both word-level and syntax-level criteria.","Experimental results between English and 6 common languages indicate that syntax can effectively enhancing ICL for MT, obtaining the highest COMET scores on 11 out of 12 translation directions."],"url":"http://arxiv.org/abs/2403.19285v1","category":"cs.CL"}
{"created":"2024-03-28 10:05:57","title":"Ungrammatical-syntax-based In-context Example Selection for Grammatical Error Correction","abstract":"In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs' potency across various tasks. However, applying LLMs to grammatical error correction (GEC) is still a challenging task. In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC. Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input. Additionally, we carry out a two-stage process to further improve the quality of selection results. On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs. This indicates that for a syntax-oriented task like GEC, paying more attention to syntactic information can effectively boost LLMs' performance. Our code will be publicly available after the publication of this paper.","sentences":["In the era of large language models (LLMs), in-context learning (ICL) stands out as an effective prompting strategy that explores LLMs' potency across various tasks.","However, applying LLMs to grammatical error correction (GEC) is still a challenging task.","In this paper, we propose a novel ungrammatical-syntax-based in-context example selection strategy for GEC.","Specifically, we measure similarity of sentences based on their syntactic structures with diverse algorithms, and identify optimal ICL examples sharing the most similar ill-formed syntax to the test input.","Additionally, we carry out a two-stage process to further improve the quality of selection results.","On benchmark English GEC datasets, empirical results show that our proposed ungrammatical-syntax-based strategies outperform commonly-used word-matching or semantics-based methods with multiple LLMs.","This indicates that for a syntax-oriented task like GEC, paying more attention to syntactic information can effectively boost LLMs' performance.","Our code will be publicly available after the publication of this paper."],"url":"http://arxiv.org/abs/2403.19283v1","category":"cs.CL"}
{"created":"2024-03-28 08:54:40","title":"RTracker: Recoverable Tracking via PN Tree Structured Memory","abstract":"Existing tracking methods mainly focus on learning better target representation or developing more robust prediction models to improve tracking performance. While tracking performance has significantly improved, the target loss issue occurs frequently due to tracking failures, complete occlusion, or out-of-view situations. However, considerably less attention is paid to the self-recovery issue of tracking methods, which is crucial for practical applications. To this end, we propose a recoverable tracking framework, RTracker, that uses a tree-structured memory to dynamically associate a tracker and a detector to enable self-recovery ability. Specifically, we propose a Positive-Negative Tree-structured memory to chronologically store and maintain positive and negative target samples. Upon the PN tree memory, we develop corresponding walking rules for determining the state of the target and define a set of control flows to unite the tracker and the detector in different tracking scenarios. Our core idea is to use the support samples of positive and negative target categories to establish a relative distance-based criterion for a reliable assessment of target loss. The favorable performance in comparison against the state-of-the-art methods on numerous challenging benchmarks demonstrates the effectiveness of the proposed algorithm.","sentences":["Existing tracking methods mainly focus on learning better target representation or developing more robust prediction models to improve tracking performance.","While tracking performance has significantly improved, the target loss issue occurs frequently due to tracking failures, complete occlusion, or out-of-view situations.","However, considerably less attention is paid to the self-recovery issue of tracking methods, which is crucial for practical applications.","To this end, we propose a recoverable tracking framework, RTracker, that uses a tree-structured memory to dynamically associate a tracker and a detector to enable self-recovery ability.","Specifically, we propose a Positive-Negative Tree-structured memory to chronologically store and maintain positive and negative target samples.","Upon the PN tree memory, we develop corresponding walking rules for determining the state of the target and define a set of control flows to unite the tracker and the detector in different tracking scenarios.","Our core idea is to use the support samples of positive and negative target categories to establish a relative distance-based criterion for a reliable assessment of target loss.","The favorable performance in comparison against the state-of-the-art methods on numerous challenging benchmarks demonstrates the effectiveness of the proposed algorithm."],"url":"http://arxiv.org/abs/2403.19242v1","category":"cs.CV"}
{"created":"2024-03-28 06:22:45","title":"RecDiffusion: Rectangling for Image Stitching with Diffusion Models","abstract":"Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing. To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts. To overcome these issues, we introduce a novel diffusion-based learning framework, \\textbf{RecDiffusion}, for image stitching rectangling. This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched image's irregular borders to a geometrically corrected intermediary. Followed by Content Diffusion Models (CDM) for image detail refinement. Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM. Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks. Code is released at https://github.com/lhaippp/RecDiffusion.","sentences":["Image stitching from different captures often results in non-rectangular boundaries, which is often considered unappealing.","To solve non-rectangular boundaries, current solutions involve cropping, which discards image content, inpainting, which can introduce unrelated content, or warping, which can distort non-linear features and introduce artifacts.","To overcome these issues, we introduce a novel diffusion-based learning framework, \\textbf{RecDiffusion}, for image stitching rectangling.","This framework combines Motion Diffusion Models (MDM) to generate motion fields, effectively transitioning from the stitched image's irregular borders to a geometrically corrected intermediary.","Followed by Content Diffusion Models (CDM) for image detail refinement.","Notably, our sampling process utilizes a weighted map to identify regions needing correction during each iteration of CDM.","Our RecDiffusion ensures geometric accuracy and overall visual appeal, surpassing all previous methods in both quantitative and qualitative measures when evaluated on public benchmarks.","Code is released at https://github.com/lhaippp/RecDiffusion."],"url":"http://arxiv.org/abs/2403.19164v1","category":"cs.CV"}
{"created":"2024-03-28 03:10:39","title":"Top Leaderboard Ranking = Top Coding Proficiency, Always? EvoEval: Evolving Coding Benchmarks via LLM","abstract":"LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation. To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks. However, prior benchmarks contain only a very limited set of problems, both in quantity and variety. Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data. Such limitations inevitably lead us to inquire: Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs? To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities. Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval. Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks. Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition. EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code. We have open-sourced our benchmarks, tools, and complete LLM generations at https://github.com/evo-eval/evoeval","sentences":["LLMs have become the go-to choice for code generation tasks, with an exponential increase in the training, development, and usage of LLMs specifically for code generation.","To evaluate the ability of LLMs on code, both academic and industry practitioners rely on popular handcrafted benchmarks.","However, prior benchmarks contain only a very limited set of problems, both in quantity and variety.","Further, due to popularity and age, many benchmarks are prone to data leakage where example solutions can be readily found on the web and thus potentially in training data.","Such limitations inevitably lead us to inquire:","Is the leaderboard performance on existing benchmarks reliable and comprehensive enough to measure the program synthesis ability of LLMs?","To address this, we introduce EvoEval -- a program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities.","Our study on 51 LLMs shows that compared to the high performance obtained on standard benchmarks like HumanEval, there is a significant drop in performance (on average 39.4%) when using EvoEval.","Additionally, the decrease in performance can range from 19.6% to 47.7%, leading to drastic ranking changes amongst LLMs and showing potential overfitting of existing benchmarks.","Furthermore, we showcase various insights, including the brittleness of instruction-following models when encountering rewording or subtle changes as well as the importance of learning problem composition and decomposition.","EvoEval not only provides comprehensive benchmarks, but can be used to further evolve arbitrary problems to keep up with advances and the ever-changing landscape of LLMs for code.","We have open-sourced our benchmarks, tools, and complete LLM generations at https://github.com/evo-eval/evoeval"],"url":"http://arxiv.org/abs/2403.19114v1","category":"cs.SE"}
{"created":"2024-03-28 02:51:33","title":"Synthetic Medical Imaging Generation with Generative Adversarial Networks For Plain Radiographs","abstract":"In medical imaging, access to data is commonly limited due to patient privacy restrictions and the issue that it can be difficult to acquire enough data in the case of rare diseases.[1] The purpose of this investigation was to develop a reusable open-source synthetic image generation pipeline, the GAN Image Synthesis Tool (GIST), that is easy to use as well as easy to deploy. The pipeline helps to improve and standardize AI algorithms in the digital health space by generating high quality synthetic image data that is not linked to specific patients. Its image generation capabilities include the ability to generate imaging of pathologies or injuries with low incidence rates. This improvement of digital health AI algorithms could improve diagnostic accuracy, aid in patient care, decrease medicolegal claims, and ultimately decrease the overall cost of healthcare. The pipeline builds on existing Generative Adversarial Networks (GANs) algorithms, and preprocessing and evaluation steps were included for completeness. For this work, we focused on ensuring the pipeline supports radiography, with a focus on synthetic knee and elbow x-ray images. In designing the pipeline, we evaluated the performance of current GAN architectures, studying the performance on available x-ray data. We show that the pipeline is capable of generating high quality and clinically relevant images based on a lay person's evaluation and the Fr\\'echet Inception Distance (FID) metric.","sentences":["In medical imaging, access to data is commonly limited due to patient privacy restrictions and the issue that it can be difficult to acquire enough data in the case of rare diseases.[1]","The purpose of this investigation was to develop a reusable open-source synthetic image generation pipeline, the GAN Image Synthesis Tool (GIST), that is easy to use as well as easy to deploy.","The pipeline helps to improve and standardize AI algorithms in the digital health space by generating high quality synthetic image data that is not linked to specific patients.","Its image generation capabilities include the ability to generate imaging of pathologies or injuries with low incidence rates.","This improvement of digital health AI algorithms could improve diagnostic accuracy, aid in patient care, decrease medicolegal claims, and ultimately decrease the overall cost of healthcare.","The pipeline builds on existing Generative Adversarial Networks (GANs) algorithms, and preprocessing and evaluation steps were included for completeness.","For this work, we focused on ensuring the pipeline supports radiography, with a focus on synthetic knee and elbow x-ray images.","In designing the pipeline, we evaluated the performance of current GAN architectures, studying the performance on available x-ray data.","We show that the pipeline is capable of generating high quality and clinically relevant images based on a lay person's evaluation and the Fr\\'echet Inception Distance (FID) metric."],"url":"http://arxiv.org/abs/2403.19107v1","category":"cs.CV"}
{"created":"2024-03-28 02:39:45","title":"CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation","abstract":"In the field of 3D object detection for autonomous driving, LiDAR-Camera (LC) fusion is the top-performing sensor configuration. Still, LiDAR is relatively high cost, which hinders adoption of this technology for consumer automobiles. Alternatively, camera and radar are commonly deployed on vehicles already on the road today, but performance of Camera-Radar (CR) fusion falls behind LC fusion. In this work, we propose Camera-Radar Knowledge Distillation (CRKD) to bridge the performance gap between LC and CR detectors with a novel cross-modality KD framework. We use the Bird's-Eye-View (BEV) representation as the shared feature space to enable effective knowledge distillation. To accommodate the unique cross-modality KD path, we propose four distillation losses to help the student learn crucial features from the teacher model. We present extensive evaluations on the nuScenes dataset to demonstrate the effectiveness of the proposed CRKD framework. The project page for CRKD is https://song-jingyu.github.io/CRKD.","sentences":["In the field of 3D object detection for autonomous driving, LiDAR-Camera (LC) fusion is the top-performing sensor configuration.","Still, LiDAR is relatively high cost, which hinders adoption of this technology for consumer automobiles.","Alternatively, camera and radar are commonly deployed on vehicles already on the road today, but performance of Camera-Radar (CR) fusion falls behind LC fusion.","In this work, we propose Camera-Radar Knowledge Distillation (CRKD) to bridge the performance gap between LC and CR detectors with a novel cross-modality KD framework.","We use the Bird's-Eye-View (BEV) representation as the shared feature space to enable effective knowledge distillation.","To accommodate the unique cross-modality KD path, we propose four distillation losses to help the student learn crucial features from the teacher model.","We present extensive evaluations on the nuScenes dataset to demonstrate the effectiveness of the proposed CRKD framework.","The project page for CRKD is https://song-jingyu.github.io/CRKD."],"url":"http://arxiv.org/abs/2403.19104v1","category":"cs.CV"}
{"created":"2024-03-28 02:20:03","title":"SCALE: Constructing Structured Natural Language Comment Trees for Software Vulnerability Detection","abstract":"Recently, there has been a growing interest in automatic software vulnerability detection. Pre-trained model-based approaches have demonstrated superior performance than other Deep Learning (DL)-based approaches in detecting vulnerabilities. However, the existing pre-trained model-based approaches generally employ code sequences as input during prediction, and may ignore vulnerability-related structural information, as reflected in the following two aspects. First, they tend to fail to infer the semantics of the code statements with complex logic such as those containing multiple operators and pointers. Second, they are hard to comprehend various code execution sequences, which is essential for precise vulnerability detection.   To mitigate the challenges, we propose a Structured Natural Language Comment tree-based vulnerAbiLity dEtection framework based on the pre-trained models, named SCALE. The proposed Structured Natural Language Comment Tree (SCT) integrates the semantics of code statements with code execution sequences based on the Abstract Syntax Trees (ASTs). Specifically, SCALE comprises three main modules: (1) Comment Tree Construction, which aims at enhancing the model's ability to infer the semantics of code statements by first incorporating Large Language Models (LLMs) for comment generation and then adding the comment node to ASTs. (2) Structured Natural Language Comment Tree Construction}, which aims at explicitly involving code execution sequence by combining the code syntax templates with the comment tree. (3) SCT-Enhanced Representation, which finally incorporates the constructed SCTs for well capturing vulnerability patterns.","sentences":["Recently, there has been a growing interest in automatic software vulnerability detection.","Pre-trained model-based approaches have demonstrated superior performance than other Deep Learning (DL)-based approaches in detecting vulnerabilities.","However, the existing pre-trained model-based approaches generally employ code sequences as input during prediction, and may ignore vulnerability-related structural information, as reflected in the following two aspects.","First, they tend to fail to infer the semantics of the code statements with complex logic such as those containing multiple operators and pointers.","Second, they are hard to comprehend various code execution sequences, which is essential for precise vulnerability detection.   ","To mitigate the challenges, we propose a Structured Natural Language Comment tree-based vulnerAbiLity dEtection framework based on the pre-trained models, named SCALE.","The proposed Structured Natural Language Comment Tree (SCT) integrates the semantics of code statements with code execution sequences based on the Abstract Syntax Trees (ASTs).","Specifically, SCALE comprises three main modules: (1) Comment Tree Construction, which aims at enhancing the model's ability to infer the semantics of code statements by first incorporating Large Language Models (LLMs) for comment generation and then adding the comment node to ASTs.","(2) Structured Natural Language Comment Tree Construction}, which aims at explicitly involving code execution sequence by combining the code syntax templates with the comment tree.","(3) SCT-Enhanced Representation, which finally incorporates the constructed SCTs for well capturing vulnerability patterns."],"url":"http://arxiv.org/abs/2403.19096v1","category":"cs.SE"}
{"created":"2024-03-28 00:34:01","title":"Efficient Preference Elicitation in Iterative Combinatorial Auctions with Many Participants","abstract":"We study the problem of achieving high efficiency in iterative combinatorial auctions (ICAs). ICAs are a kind of combinatorial auction where the auctioneer interacts with bidders to gather their valuation information using a limited number of queries, aiming for efficient allocation. Preference elicitation, a process that incrementally asks bidders to value bundles while refining the outcome allocation, is a commonly used technique in ICAs. Recently, the integration of machine learning (ML) into ICAs has significantly improved preference elicitation. This approach employs ML models that match the number of bidders, estimating each bidder's valuation functions based on their reported valuations. However, most current studies train a separate model for each bidder, which can be inefficient when there are numerous bidders with similar valuation functions and a limited number of available queries. In this study, we introduce a multi-task learning method to learn valuation functions more efficiently. Specifically, we propose to share model parameters during training to grasp the intrinsic relationships between valuations. We assess the performance of our method using a spectrum auction simulator. The findings demonstrate that our method achieves higher efficiency than existing methods, especially in scenarios with many bidders and items but a limited number of maximum queries.","sentences":["We study the problem of achieving high efficiency in iterative combinatorial auctions (ICAs).","ICAs are a kind of combinatorial auction where the auctioneer interacts with bidders to gather their valuation information using a limited number of queries, aiming for efficient allocation.","Preference elicitation, a process that incrementally asks bidders to value bundles while refining the outcome allocation, is a commonly used technique in ICAs.","Recently, the integration of machine learning (ML) into ICAs has significantly improved preference elicitation.","This approach employs ML models that match the number of bidders, estimating each bidder's valuation functions based on their reported valuations.","However, most current studies train a separate model for each bidder, which can be inefficient when there are numerous bidders with similar valuation functions and a limited number of available queries.","In this study, we introduce a multi-task learning method to learn valuation functions more efficiently.","Specifically, we propose to share model parameters during training to grasp the intrinsic relationships between valuations.","We assess the performance of our method using a spectrum auction simulator.","The findings demonstrate that our method achieves higher efficiency than existing methods, especially in scenarios with many bidders and items but a limited number of maximum queries."],"url":"http://arxiv.org/abs/2403.19075v1","category":"cs.GT"}
{"created":"2024-03-27 22:25:27","title":"Expanding Density-Correlation Machine Learning Representations for Anisotropic Coarse-Grained Particles","abstract":"Physics-based, atom-centered machine learning (ML) representations have been instrumental to the effective integration of ML within the atomistic simulation community. Many of these representations build off the idea of atoms as having spherical, or isotropic, interactions. In many communities, there is often a need to represent groups of atoms, either to increase the computational efficiency of simulation via coarse-graining or to understand molecular influences on system behavior. In such cases, atom-centered representations will have limited utility, as groups of atoms may not be well-approximated as spheres. In this work, we extend the popular Smooth Overlap of Atomic Positions (SOAP) ML representation for systems consisting of non-spherical anisotropic particles or clusters of atoms. We show the power of this anisotropic extension of SOAP, which we deem \\AniSOAP, in accurately characterizing liquid crystal systems and predicting the energetics of Gay-Berne ellipsoids and coarse-grained benzene crystals. With our study of these prototypical anisotropic systems, we derive fundamental insights into how molecular shape influences mesoscale behavior and explain how to reincorporate important atom-atom interactions typically not captured by coarse-grained models. Moving forward, we propose \\AniSOAP as a flexible, unified framework for coarse-graining in complex, multiscale simulation.","sentences":["Physics-based, atom-centered machine learning (ML) representations have been instrumental to the effective integration of ML within the atomistic simulation community.","Many of these representations build off the idea of atoms as having spherical, or isotropic, interactions.","In many communities, there is often a need to represent groups of atoms, either to increase the computational efficiency of simulation via coarse-graining or to understand molecular influences on system behavior.","In such cases, atom-centered representations will have limited utility, as groups of atoms may not be well-approximated as spheres.","In this work, we extend the popular Smooth Overlap of Atomic Positions (SOAP) ML representation for systems consisting of non-spherical anisotropic particles or clusters of atoms.","We show the power of this anisotropic extension of SOAP, which we deem \\AniSOAP, in accurately characterizing liquid crystal systems and predicting the energetics of Gay-Berne ellipsoids and coarse-grained benzene crystals.","With our study of these prototypical anisotropic systems, we derive fundamental insights into how molecular shape influences mesoscale behavior and explain how to reincorporate important atom-atom interactions typically not captured by coarse-grained models.","Moving forward, we propose \\AniSOAP as a flexible, unified framework for coarse-graining in complex, multiscale simulation."],"url":"http://arxiv.org/abs/2403.19039v1","category":"physics.comp-ph"}
{"created":"2024-03-27 21:02:15","title":"Towards Sustainable SecureML: Quantifying Carbon Footprint of Adversarial Machine Learning","abstract":"The widespread adoption of machine learning (ML) across various industries has raised sustainability concerns due to its substantial energy usage and carbon emissions. This issue becomes more pressing in adversarial ML, which focuses on enhancing model security against different network-based attacks. Implementing defenses in ML systems often necessitates additional computational resources and network security measures, exacerbating their environmental impacts. In this paper, we pioneer the first investigation into adversarial ML's carbon footprint, providing empirical evidence connecting greater model robustness to higher emissions. Addressing the critical need to quantify this trade-off, we introduce the Robustness Carbon Trade-off Index (RCTI). This novel metric, inspired by economic elasticity principles, captures the sensitivity of carbon emissions to changes in adversarial robustness. We demonstrate the RCTI through an experiment involving evasion attacks, analyzing the interplay between robustness against attacks, performance, and carbon emissions.","sentences":["The widespread adoption of machine learning (ML) across various industries has raised sustainability concerns due to its substantial energy usage and carbon emissions.","This issue becomes more pressing in adversarial ML, which focuses on enhancing model security against different network-based attacks.","Implementing defenses in ML systems often necessitates additional computational resources and network security measures, exacerbating their environmental impacts.","In this paper, we pioneer the first investigation into adversarial ML's carbon footprint, providing empirical evidence connecting greater model robustness to higher emissions.","Addressing the critical need to quantify this trade-off, we introduce the Robustness Carbon Trade-off Index (RCTI).","This novel metric, inspired by economic elasticity principles, captures the sensitivity of carbon emissions to changes in adversarial robustness.","We demonstrate the RCTI through an experiment involving evasion attacks, analyzing the interplay between robustness against attacks, performance, and carbon emissions."],"url":"http://arxiv.org/abs/2403.19009v1","category":"cs.LG"}
{"created":"2024-03-27 20:30:01","title":"Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models","abstract":"Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging. However, the lack of detailed investigations into the performance of explainability methods on these models is widening the gap between their development and safe deployment. In this work, we analyze the performance of various explainable AI methods on a vision-language model, MedCLIP, to demystify its inner workings. We also provide a simple methodology to overcome the shortcomings of these methods. Our work offers a different new perspective on the explainability of a recent well-known VLM in the medical domain and our assessment method is generalizable to other current and possible future VLMs.","sentences":["Explaining Deep Learning models is becoming increasingly important in the face of daily emerging multimodal models, particularly in safety-critical domains like medical imaging.","However, the lack of detailed investigations into the performance of explainability methods on these models is widening the gap between their development and safe deployment.","In this work, we analyze the performance of various explainable AI methods on a vision-language model, MedCLIP, to demystify its inner workings.","We also provide a simple methodology to overcome the shortcomings of these methods.","Our work offers a different new perspective on the explainability of a recent well-known VLM in the medical domain and our assessment method is generalizable to other current and possible future VLMs."],"url":"http://arxiv.org/abs/2403.18996v1","category":"cs.CV"}
{"created":"2024-03-27 18:39:11","title":"A Data-Driven Search For Mid-Infrared Excesses Among Five Million Main-Sequence FGK Stars","abstract":"Stellar infrared excesses can indicate various phenomena of interest, from protoplanetary disks to debris disks, or (more speculatively) techno-signatures along the lines of Dyson spheres. In this paper, we conduct a large search for such excesses, designed as a data-driven contextual anomaly detection pipeline. We focus our search on FGK stars close to the main sequence to favour non-young host stars. We look for excess in the mid-infrared, unlocking a large sample to search in while favouring extreme IR excess akin to the ones produced by Extreme Debris Disks (EDD). We combine observations from ESA Gaia DR3, 2MASS, and the unWISE of NASA WISE, and create a catalogue of 4,898,812 stars with $G < 16$ mag. We consider a star to have an excess if it is substantially brighter in $W1$ and $W2$ bands than what is predicted from an ensemble of machine-learning models trained on the data, taking optical and near-infrared information as input features. We apply a set of additional cuts (derived from the ML models and the objects' astronomical features) to avoid false-positive and identify a set of 53 objects (a rate of $1.1\\times 10^{-5}$), including one previously identified EDD candidate. Typical infrared-excess fractional luminosities we find are in the range 0.005 to 0.1, consistent with known EDDs.","sentences":["Stellar infrared excesses can indicate various phenomena of interest, from protoplanetary disks to debris disks, or (more speculatively) techno-signatures along the lines of Dyson spheres.","In this paper, we conduct a large search for such excesses, designed as a data-driven contextual anomaly detection pipeline.","We focus our search on FGK stars close to the main sequence to favour non-young host stars.","We look for excess in the mid-infrared, unlocking a large sample to search in while favouring extreme IR excess akin to the ones produced by Extreme Debris Disks (EDD).","We combine observations from ESA Gaia DR3, 2MASS, and the unWISE of NASA WISE, and create a catalogue of 4,898,812 stars with $G < 16$ mag.","We consider a star to have an excess if it is substantially brighter in $W1$ and $W2$ bands than what is predicted from an ensemble of machine-learning models trained on the data, taking optical and near-infrared information as input features.","We apply a set of additional cuts (derived from the ML models and the objects' astronomical features) to avoid false-positive and identify a set of 53 objects (a rate of $1.1\\times 10^{-5}$), including one previously identified EDD candidate.","Typical infrared-excess fractional luminosities we find are in the range 0.005 to 0.1, consistent with known EDDs."],"url":"http://arxiv.org/abs/2403.18941v1","category":"astro-ph.EP"}
{"created":"2024-03-27 18:09:05","title":"Sliced Online Model Checking for Optimizing the Beam Scheduling Problem in Robotic Radiation Therapy","abstract":"In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered.   We address this problem of dynamically ordering the beams by applying a model checking paradigm to select feasible beams. Since breathing patterns are complex and change rapidly, any offline model would be too imprecise. Thus, model checking must be conducted online, predicting the patient's current breathing pattern for a short amount of time and checking which beams can be delivered safely. Monitoring the treatment delivery online provides the option to reschedule beams dynamically in order to avoid pausing and hence to reduce treatment time.   While human breathing patterns are complex and may change rapidly, we need a model which can be verified quickly and use approximation by a superposition of sine curves. Further, we simplify the 3D breathing motion into separate 1D models. We compensate the simplification by adding noise inside the model itself. In turn, we synchronize between the multiple models representing the different spatial directions, the treatment simulation, and corresponding verification queries.   Our preliminary results show a 16.02 % to 37.21 % mean improvement on the idle time compared to a static beam schedule, depending on an additional safety margin. Note that an additional safety margin around the ultrasound robot can decrease idle times but also compromises plan quality by limiting the range of available beam directions. In contrast, the approach using online model checking maintains the plan quality. Further, we compare to a naive machine learning approach that does not achieve its goals while being harder to reason about.","sentences":["In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient.","Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion.","However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it.","While this can be avoided by pausing the beam delivery, the treatment time would increase.","Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time.","However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered.   ","We address this problem of dynamically ordering the beams by applying a model checking paradigm to select feasible beams.","Since breathing patterns are complex and change rapidly, any offline model would be too imprecise.","Thus, model checking must be conducted online, predicting the patient's current breathing pattern for a short amount of time and checking which beams can be delivered safely.","Monitoring the treatment delivery online provides the option to reschedule beams dynamically in order to avoid pausing and hence to reduce treatment time.   ","While human breathing patterns are complex and may change rapidly, we need a model which can be verified quickly and use approximation by a superposition of sine curves.","Further, we simplify the 3D breathing motion into separate 1D models.","We compensate the simplification by adding noise inside the model itself.","In turn, we synchronize between the multiple models representing the different spatial directions, the treatment simulation, and corresponding verification queries.   ","Our preliminary results show a 16.02 % to 37.21 % mean improvement on the idle time compared to a static beam schedule, depending on an additional safety margin.","Note that an additional safety margin around the ultrasound robot can decrease idle times but also compromises plan quality by limiting the range of available beam directions.","In contrast, the approach using online model checking maintains the plan quality.","Further, we compare to a naive machine learning approach that does not achieve its goals while being harder to reason about."],"url":"http://arxiv.org/abs/2403.18918v1","category":"cs.CE"}
{"created":"2024-03-28 17:32:41","title":"Optimizing Josephson Junction Reproducibility in 30 kV E-beam Lithography: Analysis of Backscattered Electron Distribution","abstract":"This paper explores methods to enhance the reproducibility of Josephson junctions, crucial elements in superconducting quantum technologies, when employing the Dolan technique in 30 kV e-beam processes. The study explores the influence of dose distribution along the bridge area on reproducibility, addressing challenges related to fabrication sensitivity. Experimental methods include E-beam lithography, with electron trajectory simulations shedding light on backscattered electron behavior. We demonstrate the fabrication of different junction geometries, revealing that some geometries significantly improve reproducibility by resulting in a more homogeneous dose distribution over the junction area.","sentences":["This paper explores methods to enhance the reproducibility of Josephson junctions, crucial elements in superconducting quantum technologies, when employing the Dolan technique in 30 kV e-beam processes.","The study explores the influence of dose distribution along the bridge area on reproducibility, addressing challenges related to fabrication sensitivity.","Experimental methods include E-beam lithography, with electron trajectory simulations shedding light on backscattered electron behavior.","We demonstrate the fabrication of different junction geometries, revealing that some geometries significantly improve reproducibility by resulting in a more homogeneous dose distribution over the junction area."],"url":"http://arxiv.org/abs/2403.19614v1","category":"quant-ph"}
{"created":"2024-03-28 13:56:52","title":"A multi-step calibration strategy for reliable parameter determination of salt rock mechanics constitutive models","abstract":"Renewable hydrogen storage in salt caverns requires fast injection and production rates to cope with the imbalance between energy production and consumption. Such operational conditions raise concerns about the mechanical stability of salt caverns. Choosing an appropriate constitutive model for salt mechanics is an important step in investigating this issue, and many constitutive models with several parameters have been presented in the literature. However, a robust calibration strategy to reliably determine which model and which parameter set represent the given rock, based on stress-strain data, remains an unsolved challenge. For the first time in the community, we present a multi-step strategy to determine a single parameter set based on many deformation datasets for salt rocks. Towards this end, we first develop a comprehensive constitutive model able to capture all relevant nonlinear deformation physics of transient, reverse, and steady-state creep. The determination of the single set of representative material parameters is achieved by framing the calibration process as an optimization problem, for which the global PSO algorithm is employed. Dynamic data integration is achieved by a multi-step calibration strategy for a situation where experiments are included one at a time, as they become available. Additionally, our calibration strategy is made flexible to account for mild heterogeneity between rock samples, resulting in a single set of parameters that is representative of the deformation datasets. As a rigorous mathematical analysis and the lack of relevant experimental datasets, we consider a wide range of synthetic experimental data, inspired by the existing sparse relevant data in the literature. The results of our performance analyses show that the proposed calibration strategy is robust and accuracy is improved as more experiments are included for calibration.","sentences":["Renewable hydrogen storage in salt caverns requires fast injection and production rates to cope with the imbalance between energy production and consumption.","Such operational conditions raise concerns about the mechanical stability of salt caverns.","Choosing an appropriate constitutive model for salt mechanics is an important step in investigating this issue, and many constitutive models with several parameters have been presented in the literature.","However, a robust calibration strategy to reliably determine which model and which parameter set represent the given rock, based on stress-strain data, remains an unsolved challenge.","For the first time in the community, we present a multi-step strategy to determine a single parameter set based on many deformation datasets for salt rocks.","Towards this end, we first develop a comprehensive constitutive model able to capture all relevant nonlinear deformation physics of transient, reverse, and steady-state creep.","The determination of the single set of representative material parameters is achieved by framing the calibration process as an optimization problem, for which the global PSO algorithm is employed.","Dynamic data integration is achieved by a multi-step calibration strategy for a situation where experiments are included one at a time, as they become available.","Additionally, our calibration strategy is made flexible to account for mild heterogeneity between rock samples, resulting in a single set of parameters that is representative of the deformation datasets.","As a rigorous mathematical analysis and the lack of relevant experimental datasets, we consider a wide range of synthetic experimental data, inspired by the existing sparse relevant data in the literature.","The results of our performance analyses show that the proposed calibration strategy is robust and accuracy is improved as more experiments are included for calibration."],"url":"http://arxiv.org/abs/2403.19426v1","category":"physics.geo-ph"}
{"created":"2024-03-28 12:51:58","title":"pyMSER -- An open-source library for automatic equilibration detection in molecular simulations","abstract":"Automated molecular simulations are used extensively for predicting material properties. Typically, these simulations exhibit two regimes: a dynamic equilibration part, followed by a steady state. For extracting observable properties, the simulations must first reach a steady state so that thermodynamic averages can be taken. However, as equilibration depends on simulation conditions, predicting the optimal number of simulation steps a priori is impossible. Here, we demonstrate the application of the Marginal Standard Error Rule (MSER) for automatically identifying the optimal truncation point in Grand Canonical Monte Carlo (GCMC) simulations. This novel automatic procedure determines the point in which steady state is reached, ensuring that figures-of-merits are extracted in an objective, accurate, and reproducible fashion. In the case of GCMC simulations of gas adsorption in metal-organic frameworks, we find that this methodology reduces the computational cost by up to 90%. As MSER statistics are independent of the simulation method that creates the data, this library is, in principle, applicable to any time series analysis in which equilibration truncation is required. The open-source Python implementation of our method, pyMSER, is publicly available for reuse and validation at https://github.com/IBM/pymser.","sentences":["Automated molecular simulations are used extensively for predicting material properties.","Typically, these simulations exhibit two regimes: a dynamic equilibration part, followed by a steady state.","For extracting observable properties, the simulations must first reach a steady state so that thermodynamic averages can be taken.","However, as equilibration depends on simulation conditions, predicting the optimal number of simulation steps a priori is impossible.","Here, we demonstrate the application of the Marginal Standard Error Rule (MSER) for automatically identifying the optimal truncation point in Grand Canonical Monte Carlo (GCMC) simulations.","This novel automatic procedure determines the point in which steady state is reached, ensuring that figures-of-merits are extracted in an objective, accurate, and reproducible fashion.","In the case of GCMC simulations of gas adsorption in metal-organic frameworks, we find that this methodology reduces the computational cost by up to 90%.","As MSER statistics are independent of the simulation method that creates the data, this library is, in principle, applicable to any time series analysis in which equilibration truncation is required.","The open-source Python implementation of our method, pyMSER, is publicly available for reuse and validation at https://github.com/IBM/pymser."],"url":"http://arxiv.org/abs/2403.19387v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 12:40:41","title":"Optimal Pilot Design for OTFS in Linear Time-Varying Channels","abstract":"This paper investigates the positioning of the pilot symbols, as well as the power distribution between the pilot and the communication symbols in the OTFS modulation scheme. We analyze the pilot placements that minimize the mean squared error (MSE) in estimating the channel taps. In addition, we optimize the average channel capacity by adjusting the power balance. We show that this leads to a significant increase in average capacity. The results provide valuable guidance for designing the OTFS parameters to achieve maximum capacity. Numerical simulations are performed to validate the findings.","sentences":["This paper investigates the positioning of the pilot symbols, as well as the power distribution between the pilot and the communication symbols in the OTFS modulation scheme.","We analyze the pilot placements that minimize the mean squared error (MSE) in estimating the channel taps.","In addition, we optimize the average channel capacity by adjusting the power balance.","We show that this leads to a significant increase in average capacity.","The results provide valuable guidance for designing the OTFS parameters to achieve maximum capacity.","Numerical simulations are performed to validate the findings."],"url":"http://arxiv.org/abs/2403.19379v1","category":"eess.SP"}
{"created":"2024-03-28 12:38:48","title":"Cleaning data with Swipe","abstract":"The repair problem for functional dependencies is the problem where an input database needs to be modified such that all functional dependencies are satisfied and the difference with the original database is minimal. The output database is then called an optimal repair. If the allowed modifications are value updates, finding an optimal repair is NP-hard. A well-known approach to find approximations of optimal repairs builds a Chase tree in which each internal node resolves violations of one functional dependency and leaf nodes represent repairs. A key property of this approach is that controlling the branching factor of the Chase tree allows to control the trade-off between repair quality and computational efficiency. In this paper, we explore an extreme variant of this idea in which the Chase tree has only one path. To construct this path, we first create a partition of attributes such that classes can be repaired sequentially. We repair each class only once and do so by fixing the order in which dependencies are repaired. This principle is called priority repairing and we provide a simple heuristic to determine priority. The algorithms for attribute partitioning and priority repair are combined in the Swipe algorithm. An empirical study on four real-life data sets shows that Swipe is in the range of one to three orders of magnitude faster than multi-sequence Chase-based approaches, whereas the quality of repairs is comparable or better. Moreover, we provide a scalability analysis of the Swipe algorithm.","sentences":["The repair problem for functional dependencies is the problem where an input database needs to be modified such that all functional dependencies are satisfied and the difference with the original database is minimal.","The output database is then called an optimal repair.","If the allowed modifications are value updates, finding an optimal repair is NP-hard.","A well-known approach to find approximations of optimal repairs builds a Chase tree in which each internal node resolves violations of one functional dependency and leaf nodes represent repairs.","A key property of this approach is that controlling the branching factor of the Chase tree allows to control the trade-off between repair quality and computational efficiency.","In this paper, we explore an extreme variant of this idea in which the Chase tree has only one path.","To construct this path, we first create a partition of attributes such that classes can be repaired sequentially.","We repair each class only once and do so by fixing the order in which dependencies are repaired.","This principle is called priority repairing and we provide a simple heuristic to determine priority.","The algorithms for attribute partitioning and priority repair are combined in the Swipe algorithm.","An empirical study on four real-life data sets shows that Swipe is in the range of one to three orders of magnitude faster than multi-sequence Chase-based approaches, whereas the quality of repairs is comparable or better.","Moreover, we provide a scalability analysis of the Swipe algorithm."],"url":"http://arxiv.org/abs/2403.19378v1","category":"cs.DB"}
{"created":"2024-03-28 11:49:50","title":"Three-dimensional shape and connectivity of physical networks","abstract":"Data describing the three-dimensional structure of physical networks is increasingly available, leading to a surge of interest in network science to explore the relationship between the shape and connectivity of physical networks. We contribute to this effort by standardizing and analyzing 15 data sets from different domains. Treating junction points as nodes and connections between them as links, we divide the networks into three categories: lattice-like networks, trees, and linked trees. We find that the degree distribution of physical networks is bounded, with most nodes having degree one or three. Characterizing the physical properties of links, we show that links have an elongated shape and tend to follow a nearly straight trajectory, while a small fraction of links follow a winding path. These typical node and link properties must be reflected by physical network models. We also measure how confined a link is in space by comparing its trajectory to a randomized null-model, showing that links that are central in the abstract network tend to be physically confined by their neighbors. The fact that the shape and connectivity of the physical networks are intertwined highlights that their three-dimensional layout must be taken into account to understand the evolution and function of physical networks.","sentences":["Data describing the three-dimensional structure of physical networks is increasingly available, leading to a surge of interest in network science to explore the relationship between the shape and connectivity of physical networks.","We contribute to this effort by standardizing and analyzing 15 data sets from different domains.","Treating junction points as nodes and connections between them as links, we divide the networks into three categories: lattice-like networks, trees, and linked trees.","We find that the degree distribution of physical networks is bounded, with most nodes having degree one or three.","Characterizing the physical properties of links, we show that links have an elongated shape and tend to follow a nearly straight trajectory, while a small fraction of links follow a winding path.","These typical node and link properties must be reflected by physical network models.","We also measure how confined a link is in space by comparing its trajectory to a randomized null-model, showing that links that are central in the abstract network tend to be physically confined by their neighbors.","The fact that the shape and connectivity of the physical networks are intertwined highlights that their three-dimensional layout must be taken into account to understand the evolution and function of physical networks."],"url":"http://arxiv.org/abs/2403.19333v1","category":"physics.soc-ph"}
{"created":"2024-03-28 10:33:18","title":"The $\\ell_1$ double-bubble problem in three dimensions","abstract":"We characterize the unique minimizer of the three-dimensional double-bubble problem with respect to the $\\ell_1$-norm for volume ratios between $1/2$ and $2$.","sentences":["We characterize the unique minimizer of the three-dimensional double-bubble problem with respect to the $\\ell_1$-norm for volume ratios between $1/2$ and $2$."],"url":"http://arxiv.org/abs/2403.19295v1","category":"math.AP"}
{"created":"2024-03-28 07:28:45","title":"A Constrained Spectral Approximation of Subgrid-Scale Orography on Unstructured Grids","abstract":"The representation of subgrid-scale orography is a challenge in the physical parameterization of orographic gravity-wave sources in weather forecasting. A significant hurdle is encoding as much physical information with as simple a representation as possible. Other issues include scale awareness, i.e., the orographic representation has to change according to the grid cell size and usability on unstructured geodesic grids with non-quadrilateral grid cells. This work introduces a novel spectral analysis method approximating a scale-aware spectrum of subgrid-scale orography on unstructured geodesic grids. The dimension of the physical orographic data is reduced by more than two orders of magnitude in its spectral representation. Simultaneously, the power of the approximated spectrum is close to the physical value. The method is based on well-known least-squares spectral analyses. However, it is robust to the choice of the free parameters, and tuning the algorithm is generally unnecessary. Numerical experiments involving an idealized setup show that this novel spectral analysis performs significantly better than a straightforward least-squares spectral analysis in representing the physical energy of a spectrum. Studies involving real-world topographic data are conducted, and reasonable error scores within $\\pm 10\\%$ error relative to the maximum physical quantity of interest are achieved across different grid sizes and background wind speeds. The deterministic behavior of the method is investigated along with its principal capabilities and potential biases, and it is shown that the error scores can be iteratively improved if an optimization target is known. Discussions on the method's limitations and broader applicability conclude this work.","sentences":["The representation of subgrid-scale orography is a challenge in the physical parameterization of orographic gravity-wave sources in weather forecasting.","A significant hurdle is encoding as much physical information with as simple a representation as possible.","Other issues include scale awareness, i.e., the orographic representation has to change according to the grid cell size and usability on unstructured geodesic grids with non-quadrilateral grid cells.","This work introduces a novel spectral analysis method approximating a scale-aware spectrum of subgrid-scale orography on unstructured geodesic grids.","The dimension of the physical orographic data is reduced by more than two orders of magnitude in its spectral representation.","Simultaneously, the power of the approximated spectrum is close to the physical value.","The method is based on well-known least-squares spectral analyses.","However, it is robust to the choice of the free parameters, and tuning the algorithm is generally unnecessary.","Numerical experiments involving an idealized setup show that this novel spectral analysis performs significantly better than a straightforward least-squares spectral analysis in representing the physical energy of a spectrum.","Studies involving real-world topographic data are conducted, and reasonable error scores within $\\pm 10\\%$ error relative to the maximum physical quantity of interest are achieved across different grid sizes and background wind speeds.","The deterministic behavior of the method is investigated along with its principal capabilities and potential biases, and it is shown that the error scores can be iteratively improved if an optimization target is known.","Discussions on the method's limitations and broader applicability conclude this work."],"url":"http://arxiv.org/abs/2403.19184v1","category":"physics.ao-ph"}
{"created":"2024-03-28 07:22:18","title":"Single-Crystal Growth and Characterization of Cuprate Superconductor (Hg,Re)Ba2Ca2Cu3O8+\u03b4","abstract":"We grew (Hg,Re)Ba2Ca2Cu3O8+{\\delta} ((Hg,Re)1223) single crystals with good reproducibility via the single-step flux method using monoxides as raw materials. A double-sealing method using a thick-walled quartz tube and a stainless-steel container was adopted for explosion protection. The maximum crystal size was approximately 1 mm x 1 mm in the ab plane and 0.04 mm in thickness. The crystal was square-shaped, reflecting the tetragonal crystal structure of (Hg,Re)1223. Magnetic susceptibility measurements indicated a critical temperature of 130 K. The in-plane resistivity exhibited a linear temperature dependence, indicating that the sample was close to optimal doping level. The out-of-plane resistivity was also measured, and the anisotropy parameter was 250-650 at 300 K.","sentences":["We grew (Hg,Re)Ba2Ca2Cu3O8+{\\delta} ((Hg,Re)1223) single crystals with good reproducibility via the single-step flux method using monoxides as raw materials.","A double-sealing method using a thick-walled quartz tube and a stainless-steel container was adopted for explosion protection.","The maximum crystal size was approximately 1 mm x 1 mm in the ab plane and 0.04 mm in thickness.","The crystal was square-shaped, reflecting the tetragonal crystal structure of (Hg,Re)1223.","Magnetic susceptibility measurements indicated a critical temperature of 130 K. The in-plane resistivity exhibited a linear temperature dependence, indicating that the sample was close to optimal doping level.","The out-of-plane resistivity was also measured, and the anisotropy parameter was 250-650 at 300 K."],"url":"http://arxiv.org/abs/2403.19182v1","category":"cond-mat.supr-con"}
{"created":"2024-03-28 02:22:13","title":"Topological Optimal Transport for Geometric Cycle Matching","abstract":"Topological data analysis is a powerful tool for describing topological signatures in real world data. An important challenge in topological data analysis is matching significant topological signals across distinct systems. In geometry and probability theory, optimal transport formalises notions of distance and matchings between distributions and structured objects. We propose to combine these approaches, constructing a mathematical framework for optimal transport-based matchings of topological features. Building upon recent advances in the domains of persistent homology and optimal transport for hypergraphs, we develop a transport-based methodology for topological data processing. We define measure topological networks, which integrate both geometric and topological information about a system, introduce a distance on the space of these objects, and study its metric properties, showing that it induces a geodesic metric space of non-negative curvature. The resulting Topological Optimal Transport (TpOT) framework provides a transport model on point clouds that minimises topological distortion while simultaneously yielding a geometrically informed matching between persistent homology cycles.","sentences":["Topological data analysis is a powerful tool for describing topological signatures in real world data.","An important challenge in topological data analysis is matching significant topological signals across distinct systems.","In geometry and probability theory, optimal transport formalises notions of distance and matchings between distributions and structured objects.","We propose to combine these approaches, constructing a mathematical framework for optimal transport-based matchings of topological features.","Building upon recent advances in the domains of persistent homology and optimal transport for hypergraphs, we develop a transport-based methodology for topological data processing.","We define measure topological networks, which integrate both geometric and topological information about a system, introduce a distance on the space of these objects, and study its metric properties, showing that it induces a geodesic metric space of non-negative curvature.","The resulting Topological Optimal Transport (TpOT) framework provides a transport model on point clouds that minimises topological distortion while simultaneously yielding a geometrically informed matching between persistent homology cycles."],"url":"http://arxiv.org/abs/2403.19097v1","category":"math.AT"}
{"created":"2024-03-28 00:32:04","title":"Cardinality Constraints in Single-Leader-Multi-Follower games","abstract":"This work explores bilevel problems in the context of cardinality constraints. More specifically Single-Leader-Multi-Follower games (SLMFG) involving cardinality constraints are considered in two different configurations: one with the cardinality constraint at the leader's level and a mixed structure in which the cardinality constraint is split between leader and followers problem. We prove existence results in both cases and provided equivalent reformulations allowing the numerical treatment of these complex problems. The obtained results are illustrated thanks to an application to a facility location problem.","sentences":["This work explores bilevel problems in the context of cardinality constraints.","More specifically Single-Leader-Multi-Follower games (SLMFG) involving cardinality constraints are considered in two different configurations: one with the cardinality constraint at the leader's level and a mixed structure in which the cardinality constraint is split between leader and followers problem.","We prove existence results in both cases and provided equivalent reformulations allowing the numerical treatment of these complex problems.","The obtained results are illustrated thanks to an application to a facility location problem."],"url":"http://arxiv.org/abs/2403.19074v1","category":"math.OC"}
{"created":"2024-03-27 21:22:23","title":"The sticky particle dynamics of the 1D pressureless Euler-alignment system as a gradient flow","abstract":"We show how the sticky dynamics for the one-dimensional pressureless Euler-alignment system can be obtained as an $L^2$-gradient flow of a convex functional. This is analogous to the Lagrangian evolution introduced by Natile and Savar\\'{e} for the pressureless Euler system, and by Brenier et al. for the corresponding system with a self-interacting force field. Our Lagrangian evolution can be seen as the limit of sticky particle Cucker-Smale dynamics, similar to the solutions obtained by Leslie and Tan from a corresponding scalar balance law, and provides us with a uniquely determined distributional solution of the original system in the space of probability measures with quadratic moments and corresponding square-integrable velocities. Moreover, we show that the gradient flow also provides an entropy solution to the balance law of Leslie and Tan, and how their results on cluster formation follow naturally from (non-)convexity properties of the so-called natural velocity of the flow.","sentences":["We show how the sticky dynamics for the one-dimensional pressureless Euler-alignment system can be obtained as an $L^2$-gradient flow of a convex functional.","This is analogous to the Lagrangian evolution introduced by Natile and Savar\\'{e} for the pressureless Euler system, and by Brenier et al. for the corresponding system with a self-interacting force field.","Our Lagrangian evolution can be seen as the limit of sticky particle Cucker-Smale dynamics, similar to the solutions obtained by Leslie and Tan from a corresponding scalar balance law, and provides us with a uniquely determined distributional solution of the original system in the space of probability measures with quadratic moments and corresponding square-integrable velocities.","Moreover, we show that the gradient flow also provides an entropy solution to the balance law of Leslie and Tan, and how their results on cluster formation follow naturally from (non-)convexity properties of the so-called natural velocity of the flow."],"url":"http://arxiv.org/abs/2403.19020v1","category":"math.AP"}
{"created":"2024-03-27 21:19:44","title":"Efficient global estimation of conditional-value-at-risk through stochastic kriging and extreme value theory","abstract":"We consider the problem of evaluating risk for a system that is modeled by a complex stochastic simulation with many possible input parameter values. Two sources of computational burden can be identified: the effort associated with extensive simulation runs required to accurately represent the tail of the loss distribution for each set of parameter values, and the computational cost of evaluating multiple candidate parameter values. The former concern can be addressed by using Extreme Value Theory (EVT) estimations, which specifically concentrate on the tails. Meta-modeling approaches are often used to tackle the latter concern. In this paper, we propose a framework for constructing a particular meta-modeling framework, stochastic kriging, that is based on EVT-based estimation for a class of coherent measures of risk. The proposed approach requires an efficient estimator of the intrinsic variance, and so we derive an EVT-based expression for it. It then allows us to avoid multiple replications of the risk measure in each design point, which was required in similar previously proposed approaches, resulting in a substantial reduction in computational effort. We then perform a case study, outlining promising use cases, and conditions when the EVT-based approach outperforms simpler empirical estimators.","sentences":["We consider the problem of evaluating risk for a system that is modeled by a complex stochastic simulation with many possible input parameter values.","Two sources of computational burden can be identified: the effort associated with extensive simulation runs required to accurately represent the tail of the loss distribution for each set of parameter values, and the computational cost of evaluating multiple candidate parameter values.","The former concern can be addressed by using Extreme Value Theory (EVT) estimations, which specifically concentrate on the tails.","Meta-modeling approaches are often used to tackle the latter concern.","In this paper, we propose a framework for constructing a particular meta-modeling framework, stochastic kriging, that is based on EVT-based estimation for a class of coherent measures of risk.","The proposed approach requires an efficient estimator of the intrinsic variance, and so we derive an EVT-based expression for it.","It then allows us to avoid multiple replications of the risk measure in each design point, which was required in similar previously proposed approaches, resulting in a substantial reduction in computational effort.","We then perform a case study, outlining promising use cases, and conditions when the EVT-based approach outperforms simpler empirical estimators."],"url":"http://arxiv.org/abs/2403.19018v1","category":"stat.ME"}
{"created":"2024-03-27 21:19:38","title":"Pole Placement and Feedback Stabilization for Discrete Linear Ensemble Systems","abstract":"We consider discrete ensembles of linear, scalar control systems with single-inputs. Assuming that all the individual systems are unstable, we investigate whether there exist linear feedback control laws that can asymptotically stabilize the ensemble system. We provide necessary/sufficient conditions for feasibility of pole placement in the left half plane and for feedback stabilizability of the ensemble systems.","sentences":["We consider discrete ensembles of linear, scalar control systems with single-inputs.","Assuming that all the individual systems are unstable, we investigate whether there exist linear feedback control laws that can asymptotically stabilize the ensemble system.","We provide necessary/sufficient conditions for feasibility of pole placement in the left half plane and for feedback stabilizability of the ensemble systems."],"url":"http://arxiv.org/abs/2403.19017v1","category":"math.OC"}
{"created":"2024-03-27 21:19:21","title":"Resource Allocation in Large Language Model Integrated 6G Vehicular Networks","abstract":"In the upcoming 6G era, vehicular networks are shifting from simple Vehicle-to-Vehicle (V2V) communication to the more complex Vehicle-to-Everything (V2X) connectivity. At the forefront of this shift is the incorporation of Large Language Models (LLMs) into vehicles. Known for their sophisticated natural language processing abilities, LLMs change how users interact with their vehicles. This integration facilitates voice-driven commands and interactions, departing from the conventional manual control systems. However, integrating LLMs into vehicular systems presents notable challenges. The substantial computational demands and energy requirements of LLMs pose significant challenges, especially in the constrained environment of a vehicle. Additionally, the time-sensitive nature of tasks in vehicular networks adds another layer of complexity. In this paper, we consider an edge computing system where vehicles process the initial layers of LLM computations locally, and offload the remaining LLM computation tasks to the Roadside Units (RSUs), envisioning a vehicular ecosystem where LLM computations seamlessly interact with the ultra-low latency and high-bandwidth capabilities of 6G networks. To balance the trade-off between completion time and energy consumption, we formulate a multi-objective optimization problem to minimize the total cost of the vehicles and RSUs. The problem is then decomposed into two sub-problems, which are solved by sequential quadratic programming (SQP) method and fractional programming technique. The simulation results clearly indicate that the algorithm we have proposed is highly effective in reducing both the completion time and energy consumption of the system.","sentences":["In the upcoming 6G era, vehicular networks are shifting from simple Vehicle-to-Vehicle (V2V) communication to the more complex Vehicle-to-Everything (V2X) connectivity.","At the forefront of this shift is the incorporation of Large Language Models (LLMs) into vehicles.","Known for their sophisticated natural language processing abilities, LLMs change how users interact with their vehicles.","This integration facilitates voice-driven commands and interactions, departing from the conventional manual control systems.","However, integrating LLMs into vehicular systems presents notable challenges.","The substantial computational demands and energy requirements of LLMs pose significant challenges, especially in the constrained environment of a vehicle.","Additionally, the time-sensitive nature of tasks in vehicular networks adds another layer of complexity.","In this paper, we consider an edge computing system where vehicles process the initial layers of LLM computations locally, and offload the remaining LLM computation tasks to the Roadside Units (RSUs), envisioning a vehicular ecosystem where LLM computations seamlessly interact with the ultra-low latency and high-bandwidth capabilities of 6G networks.","To balance the trade-off between completion time and energy consumption, we formulate a multi-objective optimization problem to minimize the total cost of the vehicles and RSUs.","The problem is then decomposed into two sub-problems, which are solved by sequential quadratic programming (SQP) method and fractional programming technique.","The simulation results clearly indicate that the algorithm we have proposed is highly effective in reducing both the completion time and energy consumption of the system."],"url":"http://arxiv.org/abs/2403.19016v1","category":"cs.DC"}
{"created":"2024-03-27 20:28:30","title":"Algebraic Reasoning Meets Automata in Solving Linear Integer Arithmetic","abstract":"We present a new angle on solving quantified linear integer arithmetic based on combining the automata-based approach, where numbers are understood as bitvectors, with ideas from (nowadays prevalent) algebraic approaches, which work directly with numbers. This combination is enabled by a fine-grained version of the duality between automata and arithmetic formulae. In particular, we employ a construction where states of automaton are obtained as derivatives of arithmetic formulae: then every state corresponds to a formula. Optimizations based on techniques and ideas transferred from the world of algebraic methods are used on thousands of automata states, which dramatically amplifies their effect. The merit of this combination of automata with algebraic methods is demonstrated by our prototype implementation being competitive to and even superior to state-of-the-art SMT solvers.","sentences":["We present a new angle on solving quantified linear integer arithmetic based on combining the automata-based approach, where numbers are understood as bitvectors, with ideas from (nowadays prevalent) algebraic approaches, which work directly with numbers.","This combination is enabled by a fine-grained version of the duality between automata and arithmetic formulae.","In particular, we employ a construction where states of automaton are obtained as derivatives of arithmetic formulae: then every state corresponds to a formula.","Optimizations based on techniques and ideas transferred from the world of algebraic methods are used on thousands of automata states, which dramatically amplifies their effect.","The merit of this combination of automata with algebraic methods is demonstrated by our prototype implementation being competitive to and even superior to state-of-the-art SMT solvers."],"url":"http://arxiv.org/abs/2403.18995v1","category":"cs.LO"}
{"created":"2024-03-28 17:59:54","title":"Topologically protected flatness in chiral moir\u00e9 heterostructures","abstract":"The observation of delicate correlated phases in twisted heterostructures of graphene and transition metal dichalcogenides suggests an inherent resilience of moir\\'e flat bands against certain types of disorder. We investigate the robustness of moir\\'e flat bands in the chiral limit of the Bistrizer-MacDonald model, applicable to both platforms in certain limits. We show a drastic difference between the protection of the first magic angle and higher magic angles to chiral symmetric disorder such as random higher moir\\'e potential harmonics arising, for instance, from lattice relaxation. We find that the first magic angle is topologically protected by a topological index theorem, similar to the protection of the zeroth Landau level of Dirac fermions, whose flatness withstands any chiral symmetric perturbation such as non-uniform magnetic fields. Focusing on the first magic angle of twisted bilayer graphene, our analysis reveals a hidden non-local constant of motion that permits the decomposition of the non-abelian gauge field induced by inter-layer tunnelings into two decoupled abelian ones, underscoring a topological mechanism for band flatness. Through numerical simulations, we further show the strikingly different robustness of flat bands across protected (first) and unprotected (higher) magic angles in the presence of various types of disorder and identify the scattering processes that are enhanced or suppressed in the chiral limit. Interestingly, we find that the suppression of disorder broadening persists away from the chiral limit and is further accentuated by isolating a single sublattice polarized flat band in energy. Our analysis suggests the Berry curvature hotspot at the top of the K and K' valence band in the transition metal dichalcogenide monolayers is essential for the stability of its moir\\'e flat bands and their correlated states.","sentences":["The observation of delicate correlated phases in twisted heterostructures of graphene and transition metal dichalcogenides suggests an inherent resilience of moir\\'e flat bands against certain types of disorder.","We investigate the robustness of moir\\'e flat bands in the chiral limit of the Bistrizer-MacDonald model, applicable to both platforms in certain limits.","We show a drastic difference between the protection of the first magic angle and higher magic angles to chiral symmetric disorder such as random higher moir\\'e potential harmonics arising, for instance, from lattice relaxation.","We find that the first magic angle is topologically protected by a topological index theorem, similar to the protection of the zeroth Landau level of Dirac fermions, whose flatness withstands any chiral symmetric perturbation such as non-uniform magnetic fields.","Focusing on the first magic angle of twisted bilayer graphene, our analysis reveals a hidden non-local constant of motion that permits the decomposition of the non-abelian gauge field induced by inter-layer tunnelings into two decoupled abelian ones, underscoring a topological mechanism for band flatness.","Through numerical simulations, we further show the strikingly different robustness of flat bands across protected (first) and unprotected (higher) magic angles in the presence of various types of disorder and identify the scattering processes that are enhanced or suppressed in the chiral limit.","Interestingly, we find that the suppression of disorder broadening persists away from the chiral limit and is further accentuated by isolating a single sublattice polarized flat band in energy.","Our analysis suggests the Berry curvature hotspot at the top of the K and K' valence band in the transition metal dichalcogenide monolayers is essential for the stability of its moir\\'e flat bands and their correlated states."],"url":"http://arxiv.org/abs/2403.19656v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 17:49:37","title":"Radio properties of Green Pea galaxies","abstract":"Green Peas (GPs) are young, compact, star-forming dwarf galaxies, and local (z~0.3) analogues of the early galaxies (z>6) considered to be mainly responsible for the reionisation of the Universe. Recent X-ray observations of GPs have detected high excess emission which cannot be accounted for by star formation alone, and implies presence of an active galactic nucleus (AGN). We employ radio observations to study the radio properties of GPs, build their radio spectral energy distributions, and verify the presence of AGNs. We performed new radio observations of three GPs with the Karl G. Jansky Very Large Array (JVLA) in the L, C, and X bands (1.4, 6 and 10 GHz resp.), supplemented by the data from archival observations and large radio surveys. We also analysed the archival radio data for a larger sample of GPs and blueberry (BBs) galaxies, which are lower-mass and lower-redshift analogues of the GPs. To understand the significance of the radio observations, we assess the detectability of these sources, and compare the detected radio luminosities with the expectations from theoretical and empirical relations. Two of the three targeted GPs were strongly detected ($>10\\sigma$) in the JVLA observations and their fluxes are consistent with star formation, while the third source was undetected. Although a large fraction (~75%) of the sources from the larger archival sample of GPs and BBs could be detected with archival surveys, only a small number (<40%) are detected and their radio luminosity is significantly lower than the expectation from empirical relations. Our results show that the majority of the dwarf galaxy sample is highly underluminous. Especially towards the lower end of galaxy mass and star formation rate (SFR), the radio luminosity-SFR relation deviates from the empirical relations, suggesting that the relations established for larger galaxies may not hold towards the low-mass end.","sentences":["Green Peas (GPs) are young, compact, star-forming dwarf galaxies, and local (z~0.3) analogues of the early galaxies (z>6) considered to be mainly responsible for the reionisation of the Universe.","Recent X-ray observations of GPs have detected high excess emission which cannot be accounted for by star formation alone, and implies presence of an active galactic nucleus (AGN).","We employ radio observations to study the radio properties of GPs, build their radio spectral energy distributions, and verify the presence of AGNs.","We performed new radio observations of three GPs with the Karl G. Jansky Very Large Array (JVLA) in the L, C, and X bands (1.4, 6 and 10 GHz resp.), supplemented by the data from archival observations and large radio surveys.","We also analysed the archival radio data for a larger sample of GPs and blueberry (BBs) galaxies, which are lower-mass and lower-redshift analogues of the GPs.","To understand the significance of the radio observations, we assess the detectability of these sources, and compare the detected radio luminosities with the expectations from theoretical and empirical relations.","Two of the three targeted GPs were strongly detected ($>10\\sigma$) in the JVLA observations and their fluxes are consistent with star formation, while the third source was undetected.","Although a large fraction (~75%) of the sources from the larger archival sample of GPs and BBs could be detected with archival surveys, only a small number (<40%) are detected and their radio luminosity is significantly lower than the expectation from empirical relations.","Our results show that the majority of the dwarf galaxy sample is highly underluminous.","Especially towards the lower end of galaxy mass and star formation rate (SFR), the radio luminosity-SFR relation deviates from the empirical relations, suggesting that the relations established for larger galaxies may not hold towards the low-mass end."],"url":"http://arxiv.org/abs/2403.19635v1","category":"astro-ph.GA"}
{"created":"2024-03-28 17:46:59","title":"The Immortal Dyon Index Is Positive","abstract":"The microstates of supersymmetric black holes in asymptotically flat four-dimensional spacetime are expected to be bosonic due to the spherical symmetry of their horizons. This implies that the index counting the difference between bosonic and fermionic black hole microstates must be positive, as conjectured by Sen in arXiv:1008.4209. We show that the conjecture is satisfied by the index which counts 1/4-BPS states in N = 4 string theory forming single centered black holes, called immortal dyons. The proof relies on the relation between the indexed degeneracies and the coefficients of a family of mock modular forms, thus showing that black holes predict the sign of the Fourier coefficients of mock modular forms.","sentences":["The microstates of supersymmetric black holes in asymptotically flat four-dimensional spacetime are expected to be bosonic due to the spherical symmetry of their horizons.","This implies that the index counting the difference between bosonic and fermionic black hole microstates must be positive, as conjectured by Sen in arXiv:1008.4209.","We show that the conjecture is satisfied by the index which counts 1/4-BPS states in N = 4 string theory forming single centered black holes, called immortal dyons.","The proof relies on the relation between the indexed degeneracies and the coefficients of a family of mock modular forms, thus showing that black holes predict the sign of the Fourier coefficients of mock modular forms."],"url":"http://arxiv.org/abs/2403.19630v1","category":"hep-th"}
{"created":"2024-03-28 17:36:40","title":"Orbital-Selective Spin-Triplet Superconductivity in Infinite-Layer Lanthanum Nickelates","abstract":"The discovery of superconductivity in infinite-layer nickelates has ignited stark interest within the scientific community, particularly regarding its likely unconventional origin. Conflicting magnetotransport measurements report either isotropic or anisotropic suppression of superconductivity in an external magnetic field, with distinct implications for the nature of superconducting order. In order to ensure a most suited model subject to subsequent many-body analysis, we develop a first-principles-guided minimal theory including Ni $d_{x^2-y^2}$, La $d_{3z^2-r^2}$, and La $d_{xy}$ orbitals. Amended by the consideration of orbital-selective pairing formation, which emphasises the correlation state of the Ni $3d_{x^2-y^2}$ orbital, we calculate the superconducting ordering susceptibility mediated by spin fluctuations. We find a parametric competition between even-parity $d$-wave and, in contrast to previous studies, odd-parity $p$-wave pairing, which becomes favorable through a large quasiparticle weight renormalization for Ni $3d_{x^2-y^2}$ electrons. Our findings not only shed light on the distinctiveness of LaNiO$_{2}$ as compared to cuprate superconductors or nickelates of different rare-earth composition but also provoke similarities to other pending candidate odd-parity superconductors.","sentences":["The discovery of superconductivity in infinite-layer nickelates has ignited stark interest within the scientific community, particularly regarding its likely unconventional origin.","Conflicting magnetotransport measurements report either isotropic or anisotropic suppression of superconductivity in an external magnetic field, with distinct implications for the nature of superconducting order.","In order to ensure a most suited model subject to subsequent many-body analysis, we develop a first-principles-guided minimal theory including Ni $d_{x^2-y^2}$, La $d_{3z^2-r^2}$, and La $d_{xy}$ orbitals.","Amended by the consideration of orbital-selective pairing formation, which emphasises the correlation state of the Ni $3d_{x^2-y^2}$ orbital, we calculate the superconducting ordering susceptibility mediated by spin fluctuations.","We find a parametric competition between even-parity $d$-wave and, in contrast to previous studies, odd-parity $p$-wave pairing, which becomes favorable through a large quasiparticle weight renormalization for Ni $3d_{x^2-y^2}$ electrons.","Our findings not only shed light on the distinctiveness of LaNiO$_{2}$ as compared to cuprate superconductors or nickelates of different rare-earth composition but also provoke similarities to other pending candidate odd-parity superconductors."],"url":"http://arxiv.org/abs/2403.19617v1","category":"cond-mat.supr-con"}
{"created":"2024-03-28 17:29:41","title":"Angular dependence in transverse momentum dependent diffractive parton distributions at small-$x$","abstract":"We discuss the angular dependence of the recently proposed transverse momentum dependent quark and gluon diffractive parton distributions at small-$x$. We introduce the difractive versions of the Sivers function and the elliptic gluon Wigner distribution and evaluate them in simple models with gluon saturation and study their geometric scaling properties. We also show that the diffractive version of the linearly polarized gluon distribution identically vanishes to leading order. These distributions enrich the physics opportunities for measuring semi-inclsuive diffractive DIS processes at the future electron-ion colliders.","sentences":["We discuss the angular dependence of the recently proposed transverse momentum dependent quark and gluon diffractive parton distributions at small-$x$. We introduce the difractive versions of the Sivers function and the elliptic gluon Wigner distribution and evaluate them in simple models with gluon saturation and study their geometric scaling properties.","We also show that the diffractive version of the linearly polarized gluon distribution identically vanishes to leading order.","These distributions enrich the physics opportunities for measuring semi-inclsuive diffractive DIS processes at the future electron-ion colliders."],"url":"http://arxiv.org/abs/2403.19609v1","category":"hep-ph"}
{"created":"2024-03-28 17:13:58","title":"New results of $\u03b3$ measurements in ADS and GLW-like decays at LHCb","abstract":"Recent LHCb measurements of the CKM angle $\\gamma$ in ADS and GLW-like decays are presented. One measurement considers $B^{0}\\to D K^{*0}(892)$ decays with two- and four-body $D$-decay final states and another considers $B^{\\pm}\\to D h^{\\pm}$ decays with $D \\to K\\pi\\pi\\pi$ final states. The former supersedes a previous LHCb measurement and was presented for the first time at the 12th Workshop on the CKM Unitarity Triangle. The latter analysis measures the largest CP asymmetry seen at LHCb by splitting the $D$-decay phase space into bins.","sentences":["Recent LHCb measurements of the CKM angle $\\gamma$ in ADS and GLW-like decays are presented.","One measurement considers $B^{0}\\to D K^{*0}(892)$ decays with two- and four-body $D$-decay final states and another considers $B^{\\pm}\\to D h^{\\pm}$ decays with $D \\to K\\pi\\pi\\pi$ final states.","The former supersedes a previous LHCb measurement and was presented for the first time at the 12th Workshop on the CKM Unitarity Triangle.","The latter analysis measures the largest CP asymmetry seen at LHCb by splitting the $D$-decay phase space into bins."],"url":"http://arxiv.org/abs/2403.19592v1","category":"hep-ex"}
{"created":"2024-03-28 17:02:11","title":"A search for super-imposed oscillations to the primordial power spectrum in Planck and SPT-3G 2018 data","abstract":"We search for super-imposed oscillations linearly or logarithmically spaced in Fourier wavenumbers in Planck and South Pole Telescope (SPT-3G) 2018 temperature and polarization data. The SPT-3G temperature and polarization data provide a new window to test these oscillations at high multipoles beyond the Planck angular resolution and sensitivity. We consider both models with a constant and a Gaussian modulated amplitude, which correspond to three and four additional parameters beyond power-law primordial power spectrum for the templates considered, respectively. We find that each of the four models considered can provide an improved fit to Planck data, consistently with previous findings, and to SPT-3G data, always compared to $\\Lambda$CDM. For a constant amplitude of the superimposed oscillations, we find tighter constraints on the amplitude of the oscillations from the combined Planck/SPT-3G data set than in each individual data sets. When the ranges of parameters which provide a better fit to Planck and SPT-3G data overlap, as in the case of Gaussian modulated oscillations, we find a larger $\\Delta \\chi^2$ - $- 17 \\, (-16)$ for logarithmic (linear) oscillations - in a combined Planck/SPT-3G data set than in each individual data sets. These findings will be further tested with upcoming CMB temperature and polarization measurements at high multipoles provided by ongoing ground experiments.","sentences":["We search for super-imposed oscillations linearly or logarithmically spaced in Fourier wavenumbers in Planck and South Pole Telescope (SPT-3G) 2018 temperature and polarization data.","The SPT-3G temperature and polarization data provide a new window to test these oscillations at high multipoles beyond the Planck angular resolution and sensitivity.","We consider both models with a constant and a Gaussian modulated amplitude, which correspond to three and four additional parameters beyond power-law primordial power spectrum for the templates considered, respectively.","We find that each of the four models considered can provide an improved fit to Planck data, consistently with previous findings, and to SPT-3G data, always compared to $\\Lambda$CDM.","For a constant amplitude of the superimposed oscillations, we find tighter constraints on the amplitude of the oscillations from the combined Planck/SPT-3G data set than in each individual data sets.","When the ranges of parameters which provide a better fit to Planck and SPT-3G data overlap, as in the case of Gaussian modulated oscillations, we find a larger $\\Delta \\chi^2$ - $- 17 \\, (-16)$ for logarithmic (linear) oscillations - in a combined Planck/SPT-3G data set than in each individual data sets.","These findings will be further tested with upcoming CMB temperature and polarization measurements at high multipoles provided by ongoing ground experiments."],"url":"http://arxiv.org/abs/2403.19575v1","category":"astro-ph.CO"}
{"created":"2024-03-28 16:12:45","title":"Superintegrability of the monomial Uglov matrix model","abstract":"In this paper we propose a resolution to the problem of $\\beta$-deforming the non-Gaussian monomial matrix models. The naive guess of substituting Schur polynomials with Jack polynomials does not work in that case, therefore, we are forced to look for another basis for superintegrability. We find that the relevant symmetric functions are given by Uglov polynomials, and that the integration measure should also be deformed. The measure appears to be related to the Uglov limit as well, when the quantum parameters $(q,t)$ go to a root of unity. The degree of the root must be equal to the degree of the potential. One cannot derive these results directly, for example, by studying Virasoro constraints. Instead, we use the recently developed techniques of $W$-operators to arrive at the root of unity limit. From the perspective of matrix models this new example demonstrates that even with a rather nontrivial integration measure one can find a superintegrability basis by studying the hidden symmetry of the moduli space of deformations.","sentences":["In this paper we propose a resolution to the problem of $\\beta$-deforming the non-Gaussian monomial matrix models.","The naive guess of substituting Schur polynomials with Jack polynomials does not work in that case, therefore, we are forced to look for another basis for superintegrability.","We find that the relevant symmetric functions are given by Uglov polynomials, and that the integration measure should also be deformed.","The measure appears to be related to the Uglov limit as well, when the quantum parameters $(q,t)$ go to a root of unity.","The degree of the root must be equal to the degree of the potential.","One cannot derive these results directly, for example, by studying Virasoro constraints.","Instead, we use the recently developed techniques of $W$-operators to arrive at the root of unity limit.","From the perspective of matrix models this new example demonstrates that even with a rather nontrivial integration measure one can find a superintegrability basis by studying the hidden symmetry of the moduli space of deformations."],"url":"http://arxiv.org/abs/2403.19538v1","category":"hep-th"}
{"created":"2024-03-28 15:58:43","title":"Joint ALMA/X-ray monitoring of the radio-quiet type 1 AGN IC 4329A","abstract":"The origin of a compact millimeter (mm, 100-250 GHz) emission in radio-quiet active galactic nuclei (RQ AGN) remains debated. Recent studies propose a connection with self-absorbed synchrotron emission from the accretion disk X-ray corona. We present the first joint ALMA ($\\sim$100 GHz) and X-ray (NICER/XMM-Newton/Swift; 2-10 keV) observations of the unobscured RQ AGN, IC 4329A ($z = 0.016$). The time-averaged mm-to-X-ray flux ratio aligns with recently established trends for larger samples (Kawamuro et al. 2022, Ricci et al. 2023), but with a tighter scatter ($\\sim$0.1 dex) compared to previous studies. However, there is no significant correlation on timescales of less than 20 days. The compact mm emission exhibits a spectral index of $-0.23 \\pm 0.18$, remains unresolved with a 13 pc upper limit, and shows no jet signatures. Notably, the mm flux density varies significantly (factor of 3) within 4 days, exceeding the contemporaneous X-ray variability (37% vs. 18%) and showing the largest mm variations ever detected in RQ AGN over daily timescales. The high amplitude variability rules out scenarios of heated dust and thermal free-free emission, pointing toward a synchrotron origin for the mm radiation in a source of $\\sim$1 light day size. While the exact source is not yet certain, an X-ray corona scenario emerges as the most plausible compared to a scaled-down jet or outflow-driven shocks.}","sentences":["The origin of a compact millimeter (mm, 100-250 GHz) emission in radio-quiet active galactic nuclei (RQ AGN) remains debated.","Recent studies propose a connection with self-absorbed synchrotron emission from the accretion disk X-ray corona.","We present the first joint ALMA ($\\sim$100 GHz) and X-ray (NICER/XMM-Newton/Swift; 2-10 keV) observations of the unobscured RQ AGN, IC 4329A ($z = 0.016$).","The time-averaged mm-to-X-ray flux ratio aligns with recently established trends for larger samples (Kawamuro et al. 2022, Ricci et al. 2023), but with a tighter scatter ($\\sim$0.1 dex) compared to previous studies.","However, there is no significant correlation on timescales of less than 20 days.","The compact mm emission exhibits a spectral index of $-0.23 \\pm 0.18$, remains unresolved with a 13 pc upper limit, and shows no jet signatures.","Notably, the mm flux density varies significantly (factor of 3) within 4 days, exceeding the contemporaneous X-ray variability (37% vs. 18%) and showing the largest mm variations ever detected in RQ AGN over daily timescales.","The high amplitude variability rules out scenarios of heated dust and thermal free-free emission, pointing toward a synchrotron origin for the mm radiation in a source of $\\sim$1 light day size.","While the exact source is not yet certain, an X-ray corona scenario emerges as the most plausible compared to a scaled-down jet or outflow-driven shocks.}"],"url":"http://arxiv.org/abs/2403.19524v1","category":"astro-ph.HE"}
{"created":"2024-03-28 15:27:29","title":"Calibration of the Scintillation Timing in SNO+ using In-Situ Backgrounds","abstract":"The SNO+ Collaboration has recently concluded loading its liquid scintillator with PPO, the primary fluor, and the loading of the wavelength shifter, bisMSB, is ongoing. For each stage of the experiment, reliable position and energy reconstruction is essential, and in the face of a changing scintillator cocktail, methods have been developed to rapidly calibrate the SNO+ optical model. To this end, a novel technique for calibrating the time response of the SNO+ liquid scintillator using in-situ backgrounds was developed. By using in-situ coincident backgrounds, it is possible to calibrate both the $\\beta^-$ and $\\alpha$ time responses, as well as facilitate frequent monitoring of the background levels without compromising the radiopurity of the detector. Accurate calibration of the liquid scintillator emission times allows the exploration of time-based particle discrimination, such as between single-site ($\\beta^-$) and multi-site ($\\beta^- \\gamma$) interactions.","sentences":["The SNO+ Collaboration has recently concluded loading its liquid scintillator with PPO, the primary fluor, and the loading of the wavelength shifter, bisMSB, is ongoing.","For each stage of the experiment, reliable position and energy reconstruction is essential, and in the face of a changing scintillator cocktail, methods have been developed to rapidly calibrate the SNO+ optical model.","To this end, a novel technique for calibrating the time response of the SNO+ liquid scintillator using in-situ backgrounds was developed.","By using in-situ coincident backgrounds, it is possible to calibrate both the $\\beta^-$ and $\\alpha$ time responses, as well as facilitate frequent monitoring of the background levels without compromising the radiopurity of the detector.","Accurate calibration of the liquid scintillator emission times allows the exploration of time-based particle discrimination, such as between single-site ($\\beta^-$) and multi-site ($\\beta^- \\gamma$) interactions."],"url":"http://arxiv.org/abs/2403.19496v1","category":"physics.ins-det"}
{"created":"2024-03-28 15:03:45","title":"UVIT/AstroSat observation of TW Hya","abstract":"The paper demonstrates the spectroscopic and photometric capabilities of the Ultra-Violet Imaging Telescope (UVIT) to study T-Tauri stars (TTSs). We present the first UVIT/Far-UV spectrum of a TTS, TW Hya. Based on C IV line luminosity, we estimated accretion luminosity (0.1 $L_\\odot$) and mass accretion rate (2.2 $\\times$ $10^{-8} M_\\odot /yr$) of TW Hya, and compared these values with the accretion luminosity (0.03 $L_\\odot$) and mass accretion rate (0.6 $\\times$ $10^{-8} M_\\odot /yr$) derived from spectral energy distribution (SED). From the SED, we derive best-fitted parameters for TW Hya: $T_{eff}$ = 3900$\\pm$50 K, radius = 1.2$\\pm$0.03 $R_\\odot$, $\\mathrm{log}\\, g = 4.0$ and equivalent black-body temperatures corresponding to accretion luminosity as 14100$\\pm$25 K. The parameters of TW Hya derived from UVIT observations were found to be matched well with the literature. Comparison with IUE spectra also suggests that UVIT can be used to study the spectroscopic variability of young stars. This study proposes leveraging the FUV spectroscopic capabilities of UVIT to contribute to the advancement of upcoming UV spectroscopic missions, including the Indian Spectroscopic Imaging Space Telescope (INSIST).","sentences":["The paper demonstrates the spectroscopic and photometric capabilities of the Ultra-Violet Imaging Telescope (UVIT) to study T-Tauri stars (TTSs).","We present the first UVIT/Far-UV spectrum of a TTS, TW Hya.","Based on C IV line luminosity, we estimated accretion luminosity (0.1 $L_\\odot$) and mass accretion rate (2.2 $\\times$ $10^{-8} M_\\odot /yr$) of TW Hya, and compared these values with the accretion luminosity (0.03 $L_\\odot$) and mass accretion rate (0.6 $\\times$ $10^{-8} M_\\odot /yr$) derived from spectral energy distribution (SED).","From the SED, we derive best-fitted parameters for TW Hya: $T_{eff}$ = 3900$\\pm$50 K, radius = 1.2$\\pm$0.03","$R_\\odot$, $\\mathrm{log}\\, g = 4.0$ and equivalent black-body temperatures corresponding to accretion luminosity as 14100$\\pm$25 K. The parameters of TW Hya derived from UVIT observations were found to be matched well with the literature.","Comparison with IUE spectra also suggests that UVIT can be used to study the spectroscopic variability of young stars.","This study proposes leveraging the FUV spectroscopic capabilities of UVIT to contribute to the advancement of upcoming UV spectroscopic missions, including the Indian Spectroscopic Imaging Space Telescope (INSIST)."],"url":"http://arxiv.org/abs/2403.19478v1","category":"astro-ph.SR"}
{"created":"2024-03-28 14:16:12","title":"The ALMaQUEST Survey XV: The Dependence of the Molecular-to-Atomic Gas Ratios on Resolved Optical Diagnostics","abstract":"The atomic-to-molecular gas conversion is a critical step in the baryon cycle of galaxies, which sets the initial conditions for subsequent star formation and influences the multi-phase interstellar medium. We compiled a sample of 94 nearby galaxies with observations of multi-phase gas contents by utilizing public H I, CO, and optical IFU data from the MaNGA survey together with new FAST H I observations. In agreement with previous results, our sample shows that the global molecular-to-atomic gas ratio ($R_{\\rm mol} \\equiv$ log $M_{\\rm H_2}/M_{\\rm H\\ I}$) is correlated with the global stellar mass surface density $\\mu_*$ with a Kendall's $\\tau$ coefficient of 0.25 and $p < 10^{-3}$, less tightly but still correlated with stellar mass and NUV$-$ r color, and not related to the specific star formation rate (sSFR). The cold gas distribution and kinematics inferred from the H I and CO global profile asymmetry and shape do not significantly rely on $R_{\\rm mol}$. Thanks to the availability of kpc-scale observations of MaNGA, we decompose galaxies into H II, composite, and AGN-dominated regions by using the BPT diagrams. With increasing $R_{\\rm mol}$, the fraction of H II regions within 1.5 effective radius decreases slightly; the density distribution in the spatially resolved BPT diagram also changes significantly, suggesting changes in metallicity and ionization states. Galaxies with high $R_{\\rm mol}$ tend to have high oxygen abundance, both at one effective radius with a Kendall's $\\tau$ coefficient of 0.37 ($p < 10^{-3}$) and their central regions. Among all parameters investigated here, the oxygen abundance at one effective radius has the strongest relation with global $R_{\\rm mol}$, but the dependence of gas conversion on gas distribution and galaxy ionization states is weak.","sentences":["The atomic-to-molecular gas conversion is a critical step in the baryon cycle of galaxies, which sets the initial conditions for subsequent star formation and influences the multi-phase interstellar medium.","We compiled a sample of 94 nearby galaxies with observations of multi-phase gas contents by utilizing public H I, CO, and optical IFU data from the MaNGA survey together with new FAST H I observations.","In agreement with previous results, our sample shows that the global molecular-to-atomic gas ratio ($R_{\\rm mol} \\equiv$ log $M_{\\rm H_2}/M_{\\rm H\\ I}$) is correlated with the global stellar mass surface density $\\mu_*$ with a Kendall's $\\tau$ coefficient of 0.25 and $p < 10^{-3}$, less tightly but still correlated with stellar mass and NUV$-$ r color, and not related to the specific star formation rate (sSFR).","The cold gas distribution and kinematics inferred from the H I and CO global profile asymmetry and shape do not significantly rely on $R_{\\rm mol}$. Thanks to the availability of kpc-scale observations of MaNGA, we decompose galaxies into H II, composite, and AGN-dominated regions by using the BPT diagrams.","With increasing $R_{\\rm mol}$, the fraction of H II regions within 1.5 effective radius decreases slightly; the density distribution in the spatially resolved BPT diagram also changes significantly, suggesting changes in metallicity and ionization states.","Galaxies with high $R_{\\rm mol}$ tend to have high oxygen abundance, both at one effective radius with a Kendall's $\\tau$ coefficient of 0.37 ($p < 10^{-3}$) and their central regions.","Among all parameters investigated here, the oxygen abundance at one effective radius has the strongest relation with global $R_{\\rm mol}$, but the dependence of gas conversion on gas distribution and galaxy ionization states is weak."],"url":"http://arxiv.org/abs/2403.19447v1","category":"astro-ph.GA"}
{"created":"2024-03-28 13:52:06","title":"Predicting Excitation Energies in Warm Dense Matter","abstract":"In a dense plasma environment, the energy levels of an ion shift relative to the isolated ion values. This shift is reflected in the optical spectrum of the plasma and can be measured in, for example, emission experiments. In this work, we use a recently developed method of modeling electronic states in warm dense matter to predict these level energies. In this model, excited state energies are calculated directly by enforcing constrained one-electron occupation factors, thus allowing the calculation of specific transition and ionization energies. This model includes plasma effects self-consistently, so the effect of continuum lowering is included in an ab-initio sense. We use the model to calculate the K-edge and K-alpha energies of solid density magnesium, aluminum, and silicon over a range of temperatures, finding close agreement with experimental results. We also calculate the ionization potential depression (IPD) to compare to widely used models, and investigate the effects of temperature on the lowering of the continuum.","sentences":["In a dense plasma environment, the energy levels of an ion shift relative to the isolated ion values.","This shift is reflected in the optical spectrum of the plasma and can be measured in, for example, emission experiments.","In this work, we use a recently developed method of modeling electronic states in warm dense matter to predict these level energies.","In this model, excited state energies are calculated directly by enforcing constrained one-electron occupation factors, thus allowing the calculation of specific transition and ionization energies.","This model includes plasma effects self-consistently, so the effect of continuum lowering is included in an ab-initio sense.","We use the model to calculate the K-edge and K-alpha energies of solid density magnesium, aluminum, and silicon over a range of temperatures, finding close agreement with experimental results.","We also calculate the ionization potential depression (IPD) to compare to widely used models, and investigate the effects of temperature on the lowering of the continuum."],"url":"http://arxiv.org/abs/2403.19420v1","category":"physics.plasm-ph"}
{"created":"2024-03-28 13:15:08","title":"Persistent Diagram Estimation of Multivariate Piecewise H\u00f6lder-continuous Signals","abstract":"To our knowledge, the analysis of convergence rates for persistent diagram estimation from noisy signals had remained limited to lifting signal estimation results through sup norm (or other functional norm) stability theorems. We believe that moving forward from this approach can lead to considerable gains. We illustrate it in the setting of Gaussian white noise model. We examine from a minimax perspective, the inference of persistent diagram (for sublevel sets filtration). We show that for piecewise H\\\"older-continuous functions, with control over the reach of the discontinuities set, taking the persistent diagram coming from a simple histogram estimator of the signal, permit to achieve the minimax rates known for H\\\"older-continuous functions.","sentences":["To our knowledge, the analysis of convergence rates for persistent diagram estimation from noisy signals had remained limited to lifting signal estimation results through sup norm (or other functional norm) stability theorems.","We believe that moving forward from this approach can lead to considerable gains.","We illustrate it in the setting of Gaussian white noise model.","We examine from a minimax perspective, the inference of persistent diagram (for sublevel sets filtration).","We show that for piecewise H\\\"older-continuous functions, with control over the reach of the discontinuities set, taking the persistent diagram coming from a simple histogram estimator of the signal, permit to achieve the minimax rates known for H\\\"older-continuous functions."],"url":"http://arxiv.org/abs/2403.19396v1","category":"math.ST"}
{"created":"2024-03-28 12:23:13","title":"Dynamic Correlation of Market Connectivity, Risk Spillover and Abnormal Volatility in Stock Price","abstract":"The connectivity of stock markets reflects the information efficiency of capital markets and contributes to interior risk contagion and spillover effects. We compare Shanghai Stock Exchange A-shares (SSE A-shares) during tranquil periods, with high leverage periods associated with the 2015 subprime mortgage crisis. We use Pearson correlations of returns, the maximum strongly connected subgraph, and $3\\sigma$ principle to iteratively determine the threshold value for building a dynamic correlation network of SSE A-shares. Analyses are carried out based on the networking structure, intra-sector connectivity, and node status, identifying several contributions. First, compared with tranquil periods, the SSE A-shares network experiences a more significant small-world and connective effect during the subprime mortgage crisis and the high leverage period in 2015. Second, the finance, energy and utilities sectors have a stronger intra-industry connectivity than other sectors. Third, HUB nodes drive the growth of the SSE A-shares market during bull periods, while stocks have a think-tail degree distribution in bear periods and show distinct characteristics in terms of market value and finance. Granger linear and non-linear causality networks are also considered for the comparison purpose. Studies on the evolution of inter-cycle connectivity in the SSE A-share market may help investors improve portfolios and develop more robust risk management policies.","sentences":["The connectivity of stock markets reflects the information efficiency of capital markets and contributes to interior risk contagion and spillover effects.","We compare Shanghai Stock Exchange A-shares (SSE A-shares) during tranquil periods, with high leverage periods associated with the 2015 subprime mortgage crisis.","We use Pearson correlations of returns, the maximum strongly connected subgraph, and $3\\sigma$ principle to iteratively determine the threshold value for building a dynamic correlation network of SSE A-shares.","Analyses are carried out based on the networking structure, intra-sector connectivity, and node status, identifying several contributions.","First, compared with tranquil periods, the SSE A-shares network experiences a more significant small-world and connective effect during the subprime mortgage crisis and the high leverage period in 2015.","Second, the finance, energy and utilities sectors have a stronger intra-industry connectivity than other sectors.","Third, HUB nodes drive the growth of the SSE A-shares market during bull periods, while stocks have a think-tail degree distribution in bear periods and show distinct characteristics in terms of market value and finance.","Granger linear and non-linear causality networks are also considered for the comparison purpose.","Studies on the evolution of inter-cycle connectivity in the SSE A-share market may help investors improve portfolios and develop more robust risk management policies."],"url":"http://arxiv.org/abs/2403.19363v1","category":"econ.EM"}
{"created":"2024-03-28 12:20:03","title":"GeV gamma-ray emission in the field of young massive star cluster RCW 38","abstract":"We report the detection of gamma-ray emission by the Fermi Large Area Telescope (Fermi-LAT) towards the young massive star cluster RCW 38 in the 1-500 GeV photon energy range. We found spatially extended GeV emission towards the direction of RCW 38, which is best modelled by a Gaussian disc of 0.23$\\deg$ radius with a significance of the extension is $\\sim 11.4 \\sigma$. Furthermore, the spatial correlation with the ionized and molecular gas content favors the hadronic origin of the gamma-ray emission. The gamma-ray spectrum of RCW 38 has a relatively hard photon index of $2.44 \\pm 0.03$, which is similar to other young massive star clusters. We argue that the diffuse GeV gamma-ray emission in this region likely originates from the interaction of accelerated protons in the stellar cluster with the ambient gas.","sentences":["We report the detection of gamma-ray emission by the Fermi Large Area Telescope (Fermi-LAT) towards the young massive star cluster RCW 38 in the 1-500 GeV photon energy range.","We found spatially extended GeV emission towards the direction of RCW 38, which is best modelled by a Gaussian disc of 0.23$\\deg$ radius with a significance of the extension is $\\sim 11.4 \\sigma$.","Furthermore, the spatial correlation with the ionized and molecular gas content favors the hadronic origin of the gamma-ray emission.","The gamma-ray spectrum of RCW 38 has a relatively hard photon index of $2.44 \\pm 0.03$, which is similar to other young massive star clusters.","We argue that the diffuse GeV gamma-ray emission in this region likely originates from the interaction of accelerated protons in the stellar cluster with the ambient gas."],"url":"http://arxiv.org/abs/2403.19362v1","category":"astro-ph.HE"}
{"created":"2024-03-28 11:59:58","title":"Running Coupling and Running Quark Mass Effects on the Elastic Form Factors of Nucleons","abstract":"We study the elastic electric and magnetic form factors of the proton, neutron and the charged roper resonance ($G_E^p$, $G_M^p$, $G_E^n$, $G_M^n$, $G_E^R$ and $G_M^R$) systematically in a constituent quark model. Three ingredients are crucial in this study: i) the mixing of the $|70, ^{2}8,2,0,(\\frac{1}{2})^{\\textmd{+}}\\rangle$ and the $|56, ^{2}8,0,0,(\\frac{1}{2})^{\\textmd{+}}\\rangle$ which produces a nonzero neutron electric form factor. ii) a running coupling constant that soften the form factors. iii) the running quark mass function, $M_q(p^2)$, which is responsible for the decreasing of the $\\mu_p G_E^p(Q^2)/G_M^p(Q^2)$ as $Q^2$ increases. The produced elastic form factors of the proton and neutron match the corresponding experiment values fairly well. Our study shows that $\\mu_p G_E^p(Q^2)/G_M^p(Q^2) \\approx M_q(Q^2/9)/M_q(0)$ upto $Q^2 \\approx 4 \\text{GeV}^2$. We give predictions on the elactic form factors of the roper resonance, the charge and magnetic radius of the roper resonance compared with proton are $r^R_{E}/r^p_{E} \\approx r^R_{M}/r^p_{M} \\approx 1.5$.","sentences":["We study the elastic electric and magnetic form factors of the proton, neutron and the charged roper resonance ($G_E^p$, $G_M^p$, $G_E^n$, $G_M^n$, $G_E^R$ and $G_M^R$) systematically in a constituent quark model.","Three ingredients are crucial in this study: i) the mixing of the $|70, ^{2}8,2,0,(\\frac{1}{2})^{\\textmd{+}}\\rangle$ and the $|56, ^{2}8,0,0,(\\frac{1}{2})^{\\textmd{+}}\\rangle$ which produces a nonzero neutron electric form factor.","ii) a running coupling constant that soften the form factors.","iii) the running quark mass function, $M_q(p^2)$, which is responsible for the decreasing of the $\\mu_p G_E^p(Q^2)/G_M^p(Q^2)$ as $Q^2$ increases.","The produced elastic form factors of the proton and neutron match the corresponding experiment values fairly well.","Our study shows that $\\mu_p G_E^p(Q^2)/G_M^p(Q^2) \\approx M_q(Q^2/9)/M_q(0)$ upto $Q^2 \\approx 4 \\text{GeV}^2$.","We give predictions on the elactic form factors of the roper resonance, the charge and magnetic radius of the roper resonance compared with proton are $r^R_{E}/r^p_{E} \\approx r^R_{M}/r^p_{M}","\\approx 1.5$."],"url":"http://arxiv.org/abs/2403.19343v1","category":"hep-ph"}
{"created":"2024-03-28 11:46:31","title":"An extended Nambu--Jona-Lasinio model for quark and nuclear matters","abstract":"In this work, we extend the two-flavor Nambu--Jona-Lasinio model to one capable of exploring quark and nuclear matter consistently. With an extra term standing for quark-nucleon interactions, nucleons could automatically emerge as color-singlet three-quark entities by following a process similar to mesons. Besides the quark part in mean field approximation, both mesons and nucleons could contribute to the thermodynamic potential thus possibly give rise to quarkyonic matter beyond mean field. In the study, two kinds of \"confining\" couplings are adopted for the new interaction term and two different quark masses are considered for comparison. It turns out that only confined nuclear matter or deconfined quark matter is possible for all the cases at zero temperature, thus quarkyonic matter is not favored at all. Even more strictly, only the case with stronger confinement effect and a smaller quark mass admits a physical first-order phase transition from nuclear matter to quark matter around twice saturation density.","sentences":["In this work, we extend the two-flavor Nambu--Jona-Lasinio model to one capable of exploring quark and nuclear matter consistently.","With an extra term standing for quark-nucleon interactions, nucleons could automatically emerge as color-singlet three-quark entities by following a process similar to mesons.","Besides the quark part in mean field approximation, both mesons and nucleons could contribute to the thermodynamic potential thus possibly give rise to quarkyonic matter beyond mean field.","In the study, two kinds of \"confining\" couplings are adopted for the new interaction term and two different quark masses are considered for comparison.","It turns out that only confined nuclear matter or deconfined quark matter is possible for all the cases at zero temperature, thus quarkyonic matter is not favored at all.","Even more strictly, only the case with stronger confinement effect and a smaller quark mass admits a physical first-order phase transition from nuclear matter to quark matter around twice saturation density."],"url":"http://arxiv.org/abs/2403.19331v1","category":"nucl-th"}
{"created":"2024-03-28 11:11:29","title":"Transport effects of twist-angle disorder in mesoscopic twisted bilayer graphene","abstract":"Magic-angle twisted bilayer graphene is a tunable material with remarkably flat energy bands near the Fermi level, leading to fascinating transport properties and correlated states at low temperatures. However, grown pristine samples of this material tend to break up into landscapes of twist-angle domains, strongly influencing the physical properties of each individual sample. This poses a significant problem to the interpretation and comparison between measurements obtained from different samples. In this work, we study numerically the effects of twist-angle disorder on quantum electron transport in mesoscopic samples of magic-angle twisted bilayer graphene. We find a significant property of twist-angle disorder that distinguishes it from onsite-energy disorder: it leads to an asymmetric broadening of the energy-resolved conductance. The magnitude of the twist-angle variation has a strong effect on conductance, while the number of twist-angle domains is of much lesser significance. We further establish a relationship between the asymmetric broadening and the asymmetric density of states of twisted bilayer graphene at angles smaller than the first magic angle. Our results show that the qualitative differences between the types of disorder in the energy-resolved conductance of twisted bilayer graphene samples can be used to characterize them at temperatures above the critical temperatures of the correlated phases, enabling systematic experimental studies of the effects of the different types of disorders also on the other properties such as the competition of the different types of correlated states appearing at lower temperatures.","sentences":["Magic-angle twisted bilayer graphene is a tunable material with remarkably flat energy bands near the Fermi level, leading to fascinating transport properties and correlated states at low temperatures.","However, grown pristine samples of this material tend to break up into landscapes of twist-angle domains, strongly influencing the physical properties of each individual sample.","This poses a significant problem to the interpretation and comparison between measurements obtained from different samples.","In this work, we study numerically the effects of twist-angle disorder on quantum electron transport in mesoscopic samples of magic-angle twisted bilayer graphene.","We find a significant property of twist-angle disorder that distinguishes it from onsite-energy disorder: it leads to an asymmetric broadening of the energy-resolved conductance.","The magnitude of the twist-angle variation has a strong effect on conductance, while the number of twist-angle domains is of much lesser significance.","We further establish a relationship between the asymmetric broadening and the asymmetric density of states of twisted bilayer graphene at angles smaller than the first magic angle.","Our results show that the qualitative differences between the types of disorder in the energy-resolved conductance of twisted bilayer graphene samples can be used to characterize them at temperatures above the critical temperatures of the correlated phases, enabling systematic experimental studies of the effects of the different types of disorders also on the other properties such as the competition of the different types of correlated states appearing at lower temperatures."],"url":"http://arxiv.org/abs/2403.19313v1","category":"cond-mat.mes-hall"}
{"created":"2024-03-28 10:20:31","title":"Trivalent Feynman Diagrams as a Flag","abstract":"{We exploit the flag and Orlik-Solomon algebra to reconstruct the intersection number of twisted cocycles and results to the bi-adjoint amplitude. Through Studying the pairing of the special flag, we find it results to Cachao-He-Yuan represention of the bi-adjoint amplitude. We identify each standard basis flag with Feynman trivalent graph, one find the geometry interpretation of Bern-Carrasco-Johansson duality discovered by Mizera is the relation of flag with a gap. By define the flag simplex, we also construct the Z-amplitude in the $\\alpha^{\\prime} \\rightarrow 0 $ limit.","sentences":["{We exploit the flag and Orlik-Solomon algebra to reconstruct the intersection number of twisted cocycles and results to the bi-adjoint amplitude.","Through Studying the pairing of the special flag, we find it results to Cachao-He-Yuan represention of the bi-adjoint amplitude.","We identify each standard basis flag with Feynman trivalent graph, one find the geometry interpretation of Bern-Carrasco-Johansson duality discovered by Mizera is the relation of flag with a gap.","By define the flag simplex, we also construct the Z-amplitude in the $\\alpha^{\\prime} \\rightarrow 0 $ limit."],"url":"http://arxiv.org/abs/2403.19290v1","category":"hep-th"}
{"created":"2024-03-28 09:23:02","title":"Impact of a $MoS_2$ monolayer on the nanoscale thermoelastic response of silicon heterostructures","abstract":"Understanding the thermoelastic response of a nanostructure is crucial for the choice of materials and interfaces in electronic devices with improved and tailored transport properties, at the length scales of the present technology. Here we show how the deposition of a $MoS_2$ monolayer can strongly modify the nanoscale thermoelastic dynamics of silicon substrates close to their interface. We achieve this result by creating a transient grating with extreme ultraviolet light, using ultrashort free-electron laser pulses, whose $\\approx$84 nm period is comparable to the size of elements typically used in nanodevices, such as electric contacts and nanowires. The thermoelastic response, featured by coherent acoustic waves and an incoherent relaxation, is tangibly modified by the presence of monolayer $MoS_2$. Namely, we observed a major reduction of the amplitude of the surface mode, which is almost suppressed, while the longitudinal mode is basically unperturbed, aside from a faster decay of the acoustic modulations. We interpret this behavior as a selective modification of the surface elasticity and we discuss the conditions to observe such effect, which might be of immediate relevance for the design of Si-based nanoscale devices.","sentences":["Understanding the thermoelastic response of a nanostructure is crucial for the choice of materials and interfaces in electronic devices with improved and tailored transport properties, at the length scales of the present technology.","Here we show how the deposition of a $MoS_2$ monolayer can strongly modify the nanoscale thermoelastic dynamics of silicon substrates close to their interface.","We achieve this result by creating a transient grating with extreme ultraviolet light, using ultrashort free-electron laser pulses, whose $\\approx$84 nm period is comparable to the size of elements typically used in nanodevices, such as electric contacts and nanowires.","The thermoelastic response, featured by coherent acoustic waves and an incoherent relaxation, is tangibly modified by the presence of monolayer $MoS_2$. Namely, we observed a major reduction of the amplitude of the surface mode, which is almost suppressed, while the longitudinal mode is basically unperturbed, aside from a faster decay of the acoustic modulations.","We interpret this behavior as a selective modification of the surface elasticity and we discuss the conditions to observe such effect, which might be of immediate relevance for the design of Si-based nanoscale devices."],"url":"http://arxiv.org/abs/2403.19255v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-28 08:44:45","title":"High Mobility Charge Transport in a Multicarrier Altermagnet CrSb","abstract":"A newly identified magnetic phase called altermagnet is being actively studied because of its unprecedented spin-dependent phenomena. Among the candidate materials, CrSb has a particularly high transition temperature and a large spin-splitting energy, but its transport properties have remained unexplored. In this study, we report the magnetotransport properties of CrSb measured on single crystals. We found that the Hall resistivity shows a nonlinear dependence on the magnetic field at low temperatures. From symmetry-based considerations, however, this behavior can not be attributed to an anomalous Hall effect, but to a multicarrier effect. A multicarrier fitting to the in-plane conductivity tensor revealed that there are carriers with high mobilities in CrSb, probably because of the presence of Weyl points in the electronic structure.","sentences":["A newly identified magnetic phase called altermagnet is being actively studied because of its unprecedented spin-dependent phenomena.","Among the candidate materials, CrSb has a particularly high transition temperature and a large spin-splitting energy, but its transport properties have remained unexplored.","In this study, we report the magnetotransport properties of CrSb measured on single crystals.","We found that the Hall resistivity shows a nonlinear dependence on the magnetic field at low temperatures.","From symmetry-based considerations, however, this behavior can not be attributed to an anomalous Hall effect, but to a multicarrier effect.","A multicarrier fitting to the in-plane conductivity tensor revealed that there are carriers with high mobilities in CrSb, probably because of the presence of Weyl points in the electronic structure."],"url":"http://arxiv.org/abs/2403.19233v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-28 08:42:48","title":"A new method to determine $H_0$ from cosmological energy-density measurements","abstract":"We introduce a new method for measuring the Hubble parameter from low-redshift large-scale observations that is independent of the comoving sound horizon. The method uses the baryon-to-photon ratio determined by the primordial deuterium abundance, together with Big Bang Nucleosynthesis (BBN) calculations and the present-day CMB temperature to determine the physical baryon density $\\Omega_b h^2$. The baryon fraction $\\Omega_b/\\Omega_m$ is measured using the relative amplitude of the baryonic signature in galaxy clustering measured by the Baryon Oscillation Spectroscopic Survey, scaling the physical baryon density to the physical matter density. The physical density $\\Omega_mh^2$ is then compared with the geometrical density $\\Omega_m$ from Alcock-Paczynski measurements from Baryon Acoustic Oscillations (BAO) and voids, to give $H_0$. Including type Ia supernovae and uncalibrated BAO, we measure $H_0 = 67.1^{+6.3}_{-5.3}$ km s$^{-1}$ Mpc$^{-1}$. We find similar results when varying analysis choices, such as measuring the baryon signature from the reconstructed correlation function, or excluding supernovae or voids. This measurement is currently consistent with both the distance-ladder and CMB $H_0$ determinations, but near-future large-scale structure surveys will obtain 3--4$\\times$ tighter constraints.","sentences":["We introduce a new method for measuring the Hubble parameter from low-redshift large-scale observations that is independent of the comoving sound horizon.","The method uses the baryon-to-photon ratio determined by the primordial deuterium abundance, together with Big Bang Nucleosynthesis (BBN) calculations and the present-day CMB temperature to determine the physical baryon density $\\Omega_b h^2$. The baryon fraction $\\Omega_b/\\Omega_m$ is measured using the relative amplitude of the baryonic signature in galaxy clustering measured by the Baryon Oscillation Spectroscopic Survey, scaling the physical baryon density to the physical matter density.","The physical density $\\Omega_mh^2$ is then compared with the geometrical density $\\Omega_m$ from Alcock-Paczynski measurements from Baryon Acoustic Oscillations (BAO) and voids, to give $H_0$. Including type Ia supernovae and uncalibrated BAO, we measure $H_0 = 67.1^{+6.3}_{-5.3}$ km s$^{-1}$","Mpc$^{-1}$. We find similar results when varying analysis choices, such as measuring the baryon signature from the reconstructed correlation function, or excluding supernovae or voids.","This measurement is currently consistent with both the distance-ladder and CMB $H_0$ determinations, but near-future large-scale structure surveys will obtain 3--4$\\times$ tighter constraints."],"url":"http://arxiv.org/abs/2403.19227v1","category":"astro-ph.CO"}
{"created":"2024-03-28 07:37:54","title":"Prediction and identification of point defect fingerprints in the X-ray photoelectron spectra of TiN$_x$","abstract":"We investigate the effect of selected N and Ti point defects in $B$1 TiN on N 1s and Ti 2p$_{3/2}$ binding energies (BE) by experiments and ab initio calculations. X-ray photoelectron spectroscopy (XPS) measurements of Ti-deficient TiN films reveal additional N 1s spectral components at lower binding energies. Ab initio calculations predict that these components are caused by either Ti vacancies, which induce a N 1s BE shift of $-0.53$ eV in its first N neighbors, and/or N tetrahedral interstitials, which have their N 1s BE shifted by $-1.18$ eV and also shift BE of their first N neighbors by $-0.53$ eV. However, the {\\it ab initio} calculations also reveal that the tetrahedral N interstitial is unstable at room temperature. We, therefore, unambiguously attribute the detected signal to Ti vacancies. Furthermore, the vacancy concentration in Ti-deficient TiN was quantified with XPS supported by ab initio calculations. The largest BE shifts of $-1.53$, $-1.80$ and $-2.28$ eV for Ti 2p$_{3/2}$ electrons are predicted for the Ti tetrahedral, split (10$\\overline{1}$)-aligned and split (111)-aligned interstitial atoms, respectively, and we, therefore, propose XPS could detect them. Other defects such as N vacancy or N split (10$\\overline{1}$)-aligned interstitial introduce smaller N 1s and Ti 2p$_{3/2}$ BE shifts and are unlikely to be detectable experimentally. Our work highlights the potential of ab initio-guided XPS measurements in detecting and quantifying point defects in $B$1 TiN.","sentences":["We investigate the effect of selected N and Ti point defects in $B$1 TiN on N 1s and Ti 2p$_{3/2}$ binding energies (BE) by experiments and ab initio calculations.","X-ray photoelectron spectroscopy (XPS) measurements of Ti-deficient TiN films reveal additional N 1s spectral components at lower binding energies.","Ab initio calculations predict that these components are caused by either Ti vacancies, which induce a N 1s BE shift of $-0.53$ eV in its first N neighbors, and/or N tetrahedral interstitials, which have their N 1s BE shifted by $-1.18$ eV and also shift BE of their first N neighbors by $-0.53$ eV. However, the {\\it ab initio} calculations also reveal that the tetrahedral N interstitial is unstable at room temperature.","We, therefore, unambiguously attribute the detected signal to Ti vacancies.","Furthermore, the vacancy concentration in Ti-deficient TiN was quantified with XPS supported by ab initio calculations.","The largest BE shifts of $-1.53$, $-1.80$ and $-2.28$ eV for Ti 2p$_{3/2}$ electrons are predicted for the Ti tetrahedral, split (10$\\overline{1}$)-aligned and split (111)-aligned interstitial atoms, respectively, and we, therefore, propose XPS could detect them.","Other defects such as N vacancy or N split (10$\\overline{1}$)-aligned interstitial introduce smaller N 1s and Ti 2p$_{3/2}$ BE shifts and are unlikely to be detectable experimentally.","Our work highlights the potential of ab initio-guided XPS measurements in detecting and quantifying point defects in $B$1 TiN."],"url":"http://arxiv.org/abs/2403.19190v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-28 07:32:58","title":"Rydberg exciton states and near-infrared light-emitting diode in monolayer MoTe2 devices","abstract":"Excitons, or bound electron-hole pairs, play a crucial role in the optical response of monolayer, 2H-phase transition-metal dichalcogenides (TMDs). They hold significant promise for the development of novel quantum opto-electronic devices due to their large binding energies and strong spin-orbit coupling. Among the monolayer TMDs, MoTe2 stands out because of its bandgap in the near-infrared (NIR) regime. Here, we report the experimental observation of NIR Rydberg excitons and conduction band-split charged excitons, in high-quality, boron nitride (BN)-encapsulated monolayer MoTe2 devices, probed by photoluminescence and electroluminescence spectroscopy. By employing a graphite bottom gate, we successfully modulate the emission intensity of various excitonic species. Additionally, our device fabrication process within an argon-filled glove box ensures clean TMD/metal electrode interfaces, enabling the construction of p-n junctions near the electrodes. Our work significantly advances our understanding of excitons in monolayer TMDs and contributes to the application of MoTe2 in NIR quantum opto-electronic devices.","sentences":["Excitons, or bound electron-hole pairs, play a crucial role in the optical response of monolayer, 2H-phase transition-metal dichalcogenides (TMDs).","They hold significant promise for the development of novel quantum opto-electronic devices due to their large binding energies and strong spin-orbit coupling.","Among the monolayer TMDs, MoTe2 stands out because of its bandgap in the near-infrared (NIR) regime.","Here, we report the experimental observation of NIR Rydberg excitons and conduction band-split charged excitons, in high-quality, boron nitride (BN)-encapsulated monolayer MoTe2 devices, probed by photoluminescence and electroluminescence spectroscopy.","By employing a graphite bottom gate, we successfully modulate the emission intensity of various excitonic species.","Additionally, our device fabrication process within an argon-filled glove box ensures clean TMD/metal electrode interfaces, enabling the construction of p-n junctions near the electrodes.","Our work significantly advances our understanding of excitons in monolayer TMDs and contributes to the application of MoTe2 in NIR quantum opto-electronic devices."],"url":"http://arxiv.org/abs/2403.19189v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-03-28 07:32:17","title":"Effective codescent morphisms of $n$-quasigroups and $n$-loops","abstract":"Effective codescent morphisms of $n$-quasigroups and of $n$-loops are characterized. To this end, it is proved that, for any $n\\geq 1$, every codescent morphism of $n$-quasigroups (resp. $n$-loops) is effective. This statement generalizes our earlier results on qusigroups and loops. Moreover, it is shown that the elements of the amalgamated free products of $n$-quasigroups (resp. $n$-loops) have unique normal forms, and that the varieties of $n$-quasigroups and $n$-loops satisfy the strong amalgamation property. The latter two statements generalize the corresponding old results on quasigroups and loops by Evans.","sentences":["Effective codescent morphisms of $n$-quasigroups and of $n$-loops are characterized.","To this end, it is proved that, for any $n\\geq 1$, every codescent morphism of $n$-quasigroups (resp.","$n$-loops) is effective.","This statement generalizes our earlier results on qusigroups and loops.","Moreover, it is shown that the elements of the amalgamated free products of $n$-quasigroups (resp.","$n$-loops) have unique normal forms, and that the varieties of $n$-quasigroups and $n$-loops satisfy the strong amalgamation property.","The latter two statements generalize the corresponding old results on quasigroups and loops by Evans."],"url":"http://arxiv.org/abs/2403.19187v1","category":"math.GR"}
{"created":"2024-03-28 06:35:35","title":"X-ray measurement of a high-mass white dwarf and its spin for the intermediate polar IGR J18434-0508","abstract":"IGR J18434-0508 is a Galactic Intermediate Polar (IP) type Cataclysmic Variable (CV) previously classified through optical spectroscopy. The source is already known to have a hard Chandra spectrum. In this paper, we have used follow-up XMM-Newton and NuSTAR observations to measure the white dwarf (WD) mass and spin period. We measure a spin period of P = 304.4 +/- 0.3 s based on the combined MOS1, MOS2, and pn light curve. Although this is twice the optical period found previously, we interpret this value to be the true spin period of the WD. The source has an 8 +/- 2% pulsed fraction in the 0.5-10 keV XMM-Newton data and shows strong dips in the soft energy band (0.5-2 keV). The XMM-Newton and NuSTAR joint spectrum is consistent with a thermal bremsstrahlung continuum model with an additional partial covering factor, reflection, and Fe line Gaussian components. Furthermore, we fit the joint spectrum with the post-shock region \"ipolar\" model which indicates a high WD mass $>$ $\\sim$ 1.36 Msun, approaching the Chandrasekhar limit.","sentences":["IGR J18434-0508 is a Galactic Intermediate Polar (IP) type Cataclysmic Variable (CV) previously classified through optical spectroscopy.","The source is already known to have a hard Chandra spectrum.","In this paper, we have used follow-up XMM-Newton and NuSTAR observations to measure the white dwarf (WD) mass and spin period.","We measure a spin period of P = 304.4 +/- 0.3 s based on the combined MOS1, MOS2, and pn light curve.","Although this is twice the optical period found previously, we interpret this value to be the true spin period of the WD.","The source has an 8 +/- 2% pulsed fraction in the 0.5-10 keV XMM-Newton data and shows strong dips in the soft energy band (0.5-2 keV).","The XMM-Newton and NuSTAR joint spectrum is consistent with a thermal bremsstrahlung continuum model with an additional partial covering factor, reflection, and Fe line Gaussian components.","Furthermore, we fit the joint spectrum with the post-shock region \"ipolar\" model which indicates a high WD mass $>$ $\\sim$ 1.36 Msun, approaching the Chandrasekhar limit."],"url":"http://arxiv.org/abs/2403.19170v1","category":"astro-ph.HE"}
{"created":"2024-03-28 04:03:13","title":"Compression and acceleration of ions by ultra-short ultra-intense azimuthally-polarized light","abstract":"An efficient plasma compression scheme by azimuthally-polarized (AP) light is proposed. An AP light possesses a donut-like intensity pattern, enabling it to compress and accelerate ions toward the optical axis across a wide range of parameters. When the light intensity reaches the relativistic regime of $10^{18}$ $\\mathrm{W}/\\mathrm{cm}^{2}$, and the plasma density is below the critical density, protons can be compressed and accelerated by the toroidal soliton formed by the light. The expansion process of the soliton can be well described by the snow-plow model. Three-dimensional (3D) particle-in-cell (PIC) simulations show that within the soliton regime, despite the ion density surpassing ten times of the critical density, their energy is relatively low for efficient neutron production. When the light intensity increases to $10^{22}$ $\\mathrm{W}/\\mathrm{cm}^{2}$, and the plasma density is tens of the critical density, deuterium ions can be compressed to thousands of the critical density and meanwhile accelerated to the MeV level by a tightly-focused AP light during the hole-boring (HB) process. This process is far more dramatic compared to the soliton regime, and can produce up to $10^{4}$ neutrons in a few light cycles. Moreover, in the subsequent beam-target stage, neutron yield is assessed to reach over $10^{8}$. Finally, we present a comparison with the results by a radially-polarized (RP) light to examine the influence of light polarization.","sentences":["An efficient plasma compression scheme by azimuthally-polarized (AP) light is proposed.","An AP light possesses a donut-like intensity pattern, enabling it to compress and accelerate ions toward the optical axis across a wide range of parameters.","When the light intensity reaches the relativistic regime of $10^{18}$ $\\mathrm{W}/\\mathrm{cm}^{2}$, and the plasma density is below the critical density, protons can be compressed and accelerated by the toroidal soliton formed by the light.","The expansion process of the soliton can be well described by the snow-plow model.","Three-dimensional (3D) particle-in-cell (PIC) simulations show that within the soliton regime, despite the ion density surpassing ten times of the critical density, their energy is relatively low for efficient neutron production.","When the light intensity increases to $10^{22}$ $\\mathrm{W}/\\mathrm{cm}^{2}$, and the plasma density is tens of the critical density, deuterium ions can be compressed to thousands of the critical density and meanwhile accelerated to the MeV level by a tightly-focused AP light during the hole-boring (HB) process.","This process is far more dramatic compared to the soliton regime, and can produce up to $10^{4}$ neutrons in a few light cycles.","Moreover, in the subsequent beam-target stage, neutron yield is assessed to reach over $10^{8}$. Finally, we present a comparison with the results by a radially-polarized (RP) light to examine the influence of light polarization."],"url":"http://arxiv.org/abs/2403.19133v1","category":"physics.plasm-ph"}
{"created":"2024-03-27 23:51:57","title":"Quotients in super-symmetry: formal supergroup case","abstract":"We describe the structure of the quotient $\\mathfrak{G}/\\mathfrak{H}$ of a formal supergroup $\\mathfrak{G}$ by its formal sub-supergroup $\\mathfrak{H}$. This is a consequence which arises as a continuation of the authors' work (partly with M. Hashi) on algebraic/analytic supergoups.The results are presented and proved in terms of super-cocommutative Hopf superalgebras. The notion of co-free super-coalgebras plays a role, in particular.","sentences":["We describe the structure of the quotient $\\mathfrak{G}/\\mathfrak{H}$ of a formal supergroup $\\mathfrak{G}$ by its formal sub-supergroup $\\mathfrak{H}$.","This is a consequence which arises as a continuation of the authors' work (partly with M. Hashi) on algebraic/analytic supergoups.","The results are presented and proved in terms of super-cocommutative Hopf superalgebras.","The notion of co-free super-coalgebras plays a role, in particular."],"url":"http://arxiv.org/abs/2403.19058v1","category":"math.AG"}
{"created":"2024-03-27 22:54:31","title":"Design principles, growth laws, and competition of minimal autocatalysts","abstract":"The apparent difficulty of designing simple autocatalysts that grow exponentially in the absence of enzymes, external drives or ingenious internal mechanisms severely constrains scenarios for the emergence of evolution by natural selection in chemical and physical systems. Here, we systematically analyze these difficulties in the context of one of the simplest and most generic autocatalysts: a dimeric molecule that duplicates by templated ligation. We show that despite its simplicity, such an autocatalyst can achieve exponential growth autonomously. This only requires that the rate of the spontaneous dimerization, the interactions between molecules, and the concentrations of substrates and products are in appropriate ranges. We also show, however, that it is possible to design as simple sub-exponential autocatalysts that have an advantage over exponential autocatalysts when competing for a common resource. We reach these conclusions by developing a general theoretical framework based on kinetic barrier diagrams. Besides challenging commonly accepted assumptions in the field of the origin of life, our results provide a blueprint for the experimental realization of elementary autocatalysts exhibiting a form of natural selection, whether on a molecular or colloidal scale.","sentences":["The apparent difficulty of designing simple autocatalysts that grow exponentially in the absence of enzymes, external drives or ingenious internal mechanisms severely constrains scenarios for the emergence of evolution by natural selection in chemical and physical systems.","Here, we systematically analyze these difficulties in the context of one of the simplest and most generic autocatalysts: a dimeric molecule that duplicates by templated ligation.","We show that despite its simplicity, such an autocatalyst can achieve exponential growth autonomously.","This only requires that the rate of the spontaneous dimerization, the interactions between molecules, and the concentrations of substrates and products are in appropriate ranges.","We also show, however, that it is possible to design as simple sub-exponential autocatalysts that have an advantage over exponential autocatalysts when competing for a common resource.","We reach these conclusions by developing a general theoretical framework based on kinetic barrier diagrams.","Besides challenging commonly accepted assumptions in the field of the origin of life, our results provide a blueprint for the experimental realization of elementary autocatalysts exhibiting a form of natural selection, whether on a molecular or colloidal scale."],"url":"http://arxiv.org/abs/2403.19047v1","category":"q-bio.BM"}
{"created":"2024-03-27 21:57:19","title":"JWST Spectroscopy of SN H0pe: Classification and Time Delays of a Triply-imaged Type Ia Supernova at z = 1.78","abstract":"SN H0pe is a triply imaged supernova (SN) at redshift $z=1.78$ discovered using the James Webb Space Telescope (JWST). In order to classify the SN spectroscopically and measure the relative time delays of its three images (designated A, B, and C), we acquired NIRSpec follow-up spectroscopy spanning 0.6 to 5 microns. From the high signal-to-noise spectra of the two bright images B and C, we first classify the SN, whose spectra most closely match those of SN 1994D and SN 2013dy, as a Type Ia SN. We identify prominent blueshifted absorption features corresponding to Si II $\\lambda6355$ and Ca II H $\\lambda3970$ and K $\\lambda3935$. We next measure the absolute phases of the three images from our spectra, which allows us to constrain their relative time delays. The absolute phases of the three images, determined by fitting the three spectra to Hsiao07 SN templates, are $6.5_{-1.8}^{+2.4}$d, $24.3_{-3.9}^{+3.9}$d, and $50.6_{-15.3}^{+16.1}$d for the brightest to faintest images. These correspond to relative time delays between Image A and Image B and between Image B and Image C of $-122.3_{-43.8}^{+43.7}$d and $49.3_{-14.7}^{+12.2}$d, respectively. The SALT3-NIR model yields phases and time delays consistent with these values. After unblinding, we additionally explored the effect of using Hsiao07 template spectra for simulations through eighty instead of sixty days past maximum, and found a small (11.5 and 1.0 days, respectively) yet statistically insignificant ($\\sim$0.25$\\sigma$ and $\\sim$0.1$\\sigma$) effect on the inferred image delays.","sentences":["SN H0pe is a triply imaged supernova (SN) at redshift $z=1.78$ discovered using the James Webb Space Telescope (JWST).","In order to classify the SN spectroscopically and measure the relative time delays of its three images (designated A, B, and C), we acquired NIRSpec follow-up spectroscopy spanning 0.6 to 5 microns.","From the high signal-to-noise spectra of the two bright images B and C, we first classify the SN, whose spectra most closely match those of SN 1994D and SN 2013dy, as a Type Ia SN.","We identify prominent blueshifted absorption features corresponding to Si II $\\lambda6355$ and Ca II H $\\lambda3970$ and K $\\lambda3935$. We next measure the absolute phases of the three images from our spectra, which allows us to constrain their relative time delays.","The absolute phases of the three images, determined by fitting the three spectra to Hsiao07 SN templates, are $6.5_{-1.8}^{+2.4}$d, $24.3_{-3.9}^{+3.9}$d, and $50.6_{-15.3}^{+16.1}$d for the brightest to faintest images.","These correspond to relative time delays between Image A and Image B and between Image B and Image C of $-122.3_{-43.8}^{+43.7}$d and $49.3_{-14.7}^{+12.2}$d, respectively.","The SALT3-NIR model yields phases and time delays consistent with these values.","After unblinding, we additionally explored the effect of using Hsiao07 template spectra for simulations through eighty instead of sixty days past maximum, and found a small (11.5 and 1.0 days, respectively) yet statistically insignificant ($\\sim$0.25$\\sigma$ and $\\sim$0.1$\\sigma$) effect on the inferred image delays."],"url":"http://arxiv.org/abs/2403.19029v1","category":"astro-ph.GA"}
{"created":"2024-03-27 21:36:05","title":"Analysis of data on $B$ decays into two light vector mesons","abstract":"An important experimental effort has been accomplished in recent years in the measurement of rates, polarization and CP observables in $B$ decays into two light vector mesons. On the theoretical side, after a very consistent effort done within the framework of QCD Factorization, the comparison of the theory with the present experimental data has not been updated, to our knowledge. In the present paper we compare this wealth of data to the theory, in tree color-allowed, tree color-suppressed and pure penguin decays, and present predictions for the observables that have not yet been measured in these decays. In our fits we find acceptable values of $\\chi^2$ for the branching ratios and for the direct CP asymmetries. However, this is not the case for the longitudinal polarization fractions, essentially due to disagreement between theory and experiment for the modes $B_{d,s}\\to K^{*0}\\overline{K}^{*0}$ and $B_d \\to \\rho^+ \\rho^-$. Although we rely on previous work by other authors, we summarize the formalism so that the paper is self-contained and its results can be checked.","sentences":["An important experimental effort has been accomplished in recent years in the measurement of rates, polarization and CP observables in $B$ decays into two light vector mesons.","On the theoretical side, after a very consistent effort done within the framework of QCD Factorization, the comparison of the theory with the present experimental data has not been updated, to our knowledge.","In the present paper we compare this wealth of data to the theory, in tree color-allowed, tree color-suppressed and pure penguin decays, and present predictions for the observables that have not yet been measured in these decays.","In our fits we find acceptable values of $\\chi^2$ for the branching ratios and for the direct CP asymmetries.","However, this is not the case for the longitudinal polarization fractions, essentially due to disagreement between theory and experiment for the modes $B_{d,s}\\to K^{*0}\\overline{K}^{*0}$ and $B_d \\to \\rho^+ \\rho^-$. Although we rely on previous work by other authors, we summarize the formalism so that the paper is self-contained and its results can be checked."],"url":"http://arxiv.org/abs/2403.19025v1","category":"hep-ph"}
{"created":"2024-03-27 21:30:45","title":"Two-sided Lieb-Thirring bounds","abstract":"We prove upper and lower bounds for the number of eigenvalues of semi-bounded Schr\\\"odinger operators in all spatial dimensions. As a corollary, we obtain two-sided estimates for the sum of the negative eigenvalues of atomic Hamiltonians with Kato potentials. Instead of being in terms of the potential itself, as in the usual Lieb-Thirring result, the bounds are in terms of the landscape function, also known as the torsion function, which is a solution of $(-\\Delta + V +M)u_M =1$ in $\\mathbb{R}^d$; here $M\\in\\mathbb{R}$ is chosen so that the operator is positive. We further prove that the infimum of $(u_M^{-1} - M)$ is a lower bound for the ground state energy $E_0$ and derive a simple iteration scheme converging to $E_0$.","sentences":["We prove upper and lower bounds for the number of eigenvalues of semi-bounded Schr\\\"odinger operators in all spatial dimensions.","As a corollary, we obtain two-sided estimates for the sum of the negative eigenvalues of atomic Hamiltonians with Kato potentials.","Instead of being in terms of the potential itself, as in the usual Lieb-Thirring result, the bounds are in terms of the landscape function, also known as the torsion function, which is a solution of $(-\\Delta + V +M)u_M =1$ in $\\mathbb{R}^d$; here $M\\in\\mathbb{R}$ is chosen so that the operator is positive.","We further prove that the infimum of $(u_M^{-1} - M)$ is a lower bound for the ground state energy $E_0$ and derive a simple iteration scheme converging to $E_0$."],"url":"http://arxiv.org/abs/2403.19023v1","category":"math-ph"}
{"created":"2024-03-27 21:17:01","title":"Constraints on Primordial Black Holes from $N$-body simulations of the Eridanus II Stellar Cluster","abstract":"The tidal disruption of old, compact stellar structures provides strong constraints on macroscopic dark matter candidates such as primordial black holes. In view of recent, new observational data on the Eridanus II dwarf galaxy and on its central stellar cluster, we employ, for the first time, $N$-body simulations to assess the impact of compact massive dark matter candidates on the gravitational stability of the cluster. We find evidence that such candidates must be lighter than about one solar mass if they constitute the totality of the dark matter. We additionally derive robust constraints on the fraction of the dark matter in macroscopic objects as a function of mass, by suitably modeling the remainder of the dark matter as standard fluid-like cold dark matter.","sentences":["The tidal disruption of old, compact stellar structures provides strong constraints on macroscopic dark matter candidates such as primordial black holes.","In view of recent, new observational data on the Eridanus II dwarf galaxy and on its central stellar cluster, we employ, for the first time, $N$-body simulations to assess the impact of compact massive dark matter candidates on the gravitational stability of the cluster.","We find evidence that such candidates must be lighter than about one solar mass if they constitute the totality of the dark matter.","We additionally derive robust constraints on the fraction of the dark matter in macroscopic objects as a function of mass, by suitably modeling the remainder of the dark matter as standard fluid-like cold dark matter."],"url":"http://arxiv.org/abs/2403.19015v1","category":"astro-ph.CO"}
{"created":"2024-03-27 21:13:54","title":"Antiferromagnetic domains in a single crystal of the A-type spin-7/2 trigonal topological insulator EuSn$_2$As$_2$","abstract":"EuSn$_2$As$_2$ is a trigonal A-type antiferromagnetic topological insulator with the moments aligned in the $ab$ plane and with a N\\'eel temperature $T_{\\rm N} = 23.5$ K. Here we report that an EuSn$_2$As$_2$ crystal exhibits a broad peak at $H_{\\rm c1} = 1100$ Oe in the field derivative $dM_{ab}/dH$ of the $ab$-plane magnetization $M_{ab}(H)$ at temperature $T=2$ K, demonstrating the presence of trigonal antiferromagnetic domains. We model these $M_{ab}(H,\\,T=2\\,{\\rm K})$ data and obtain the trigonal anisotropy-energy coefficient $K_3$ that is 13.3 and 3.7 times larger than those we previously reported for single crystals of the trigonal compounds EuMg$_2$Sb$_2$ and EuMg$_2$Bi$_2$, respectively.","sentences":["EuSn$_2$As$_2$ is a trigonal A-type antiferromagnetic topological insulator with the moments aligned in the $ab$ plane and with a N\\'eel temperature $T_{\\rm N} = 23.5$ K. Here we report that an EuSn$_2$As$_2$ crystal exhibits a broad peak at $H_{\\rm c1} = 1100$ Oe in the field derivative $dM_{ab}/dH$ of the $ab$-plane magnetization $M_{ab}(H)$ at temperature $T=2$ K, demonstrating the presence of trigonal antiferromagnetic domains.","We model these $M_{ab}(H,\\,T=2\\,{\\rm K})$ data and obtain the trigonal anisotropy-energy coefficient $K_3$ that is 13.3 and 3.7 times larger than those we previously reported for single crystals of the trigonal compounds EuMg$_2$Sb$_2$ and EuMg$_2$Bi$_2$, respectively."],"url":"http://arxiv.org/abs/2403.19013v1","category":"cond-mat.str-el"}
{"created":"2024-03-27 20:19:10","title":"Stau pairs from natural SUSY at high luminosity LHC","abstract":"Natural supersymmetry (SUSY) with light higgsinos is perhaps the most plausible of all weak scale SUSY models while a variety of motivations point to (right) tau sleptons as the lightest of all the sleptons. We examine a SUSY model line with rather light right-staus embedded within natural SUSY. For light stau_1 of a few hundred GeV, then the decays stau_1 -> \\tau\\tchi_{1,2}^0 and \\nu_\\tau\\tchi_1^- occur at comparable rates where the (higgsino-like) \\tchi_1^\\pm and \\tchi_2^0 release only small visible energy: in this case, the expected \\tau^+\\tau^- +\\eslt signature is diminished from usual expectations due to the presence of the nearly invisible decay mode \\ttau_1 -> \\nu_\\tau\\tchi_1^-. However, once m_{\\ttau_1}> ~m(bino), then decays to binos such as \\ttau_1 -> \\tau\\tchi_3^0 open up where \\tchi_3^0 decays to higgsinos plus W^\\pm, Z^0 and h at comparable rates. For these heavier staus, then stau pair production gives rise to diboson+\\eslt events which may contain 0, 1 or 2 additional hard \\tau leptons. From these considerations, we examine the potential for future discovery of tau-slepton pair production at high-luminosity LHC. While we do not find a 5\\sigma HL-LHC discovery reach for 3000 fb^{-1}, we do find a 95\\% CL exclusion reach, ranging between m_{\\ttau_1}:100-450 GeV for m_{\\tchi_1^0}~ 100 GeV. This latter reach disappears for m_{\\tchi_1^0}>~ 200 GeV.","sentences":["Natural supersymmetry (SUSY) with light higgsinos is perhaps the most plausible of all weak scale SUSY models while a variety of motivations point to (right) tau sleptons as the lightest of all the sleptons.","We examine a SUSY model line with rather light right-staus embedded within natural SUSY.","For light stau_1 of a few hundred GeV, then the decays stau_1 -> \\tau\\tchi_{1,2}^0 and \\nu_\\tau\\tchi_1^- occur at comparable rates where the (higgsino-like) \\tchi_1^\\pm and \\tchi_2^0 release only small visible energy: in this case, the expected \\tau^+\\tau^- +\\eslt signature is diminished from usual expectations due to the presence of the nearly invisible decay mode \\ttau_1 -> \\nu_\\tau\\tchi_1^-.","However, once m_{\\ttau_1}> ~m(bino), then decays to binos such as \\ttau_1 -> \\tau\\tchi_3^0 open up where \\tchi_3^0 decays to higgsinos plus W^\\pm, Z^0 and h at comparable rates.","For these heavier staus, then stau pair production gives rise to diboson+\\eslt events which may contain 0, 1 or 2 additional hard \\tau leptons.","From these considerations, we examine the potential for future discovery of tau-slepton pair production at high-luminosity LHC.","While we do not find a 5\\sigma HL-LHC discovery reach for 3000 fb^{-1}, we do find a 95\\% CL exclusion reach, ranging between m_{\\ttau_1}:100-450 GeV for m_{\\tchi_1^0}~ 100 GeV.","This latter reach disappears for m_{\\tchi_1^0}>~ 200 GeV."],"url":"http://arxiv.org/abs/2403.18991v1","category":"hep-ph"}
{"created":"2024-03-27 19:57:20","title":"Revisiting the Vertical Distribution of HI Absorbing Clouds in the Solar Neighborhood","abstract":"The vertical distribution of cold neutral hydrogen (HI) clouds is a constraint on models of the structure, dynamics, and hydrostatic balance of the interstellar medium. In 1978, Crovisier pioneered a method to infer the vertical distribution of HI absorbing clouds in the solar neighborhood. Using data from the Nan\\c{c}ay 21-cm absorption survey, they determine the mean vertical displacement of cold HI clouds, $\\langle|z|\\rangle$. We revisit Crovisier's analysis and explore the consequences of truncating the HI absorption sample in Galactic latitude. For any non-zero latitude limit, we find that the quantity inferred by Crovisier is not the mean vertical displacement but rather a ratio involving higher moments of the vertical distribution. The resultant distribution scale heights are thus ${\\sim}1.5$ to ${\\sim}3$ times smaller than previously determined. In light of this discovery, we develop a Bayesian Monte Carlo Markov Chain method to infer the vertical distribution of HI absorbing clouds. We fit our model to the original Nan\\c{c}ay data and find a vertical distribution moment ratio $\\langle|z|^3\\rangle/\\langle|z|^2\\rangle = 97 \\pm 15\\,\\text{pc}$, which corresponds to a Gaussian scale height $\\sigma_z = 61 \\pm 9\\,\\text{pc}$, an exponential scale height $\\lambda_z = 32 \\pm 5\\,\\text{pc}$, and a rectangular half-width $W_{z, 1/2} = 129 \\pm 20\\,\\text{pc}$. Consistent with recent simulations, the vertical scale height of cold HI clouds appears to remain constant between the inner-Galaxy and the Galactocentric distance of the solar neighborhood. Local fluctuations might explain the large scale height observed at the same Galactocentric distance on the far side of the Galaxy.","sentences":["The vertical distribution of cold neutral hydrogen (HI) clouds is a constraint on models of the structure, dynamics, and hydrostatic balance of the interstellar medium.","In 1978, Crovisier pioneered a method to infer the vertical distribution of HI absorbing clouds in the solar neighborhood.","Using data from the Nan\\c{c}ay 21-cm absorption survey, they determine the mean vertical displacement of cold HI clouds, $\\langle|z|\\rangle$. We revisit Crovisier's analysis and explore the consequences of truncating the HI absorption sample in Galactic latitude.","For any non-zero latitude limit, we find that the quantity inferred by Crovisier is not the mean vertical displacement but rather a ratio involving higher moments of the vertical distribution.","The resultant distribution scale heights are thus ${\\sim}1.5$ to ${\\sim}3$ times smaller than previously determined.","In light of this discovery, we develop a Bayesian Monte Carlo Markov Chain method to infer the vertical distribution of HI absorbing clouds.","We fit our model to the original Nan\\c{c}ay data and find a vertical distribution moment ratio $\\langle|z|^3\\rangle/\\langle|z|^2\\rangle = 97 \\pm 15\\,\\text{pc}$, which corresponds to a Gaussian scale height $\\sigma_z = 61 \\pm 9\\,\\text{pc}$, an exponential scale height $\\lambda_z = 32 \\pm 5\\,\\text{pc}$, and a rectangular half-width $W_{z, 1/2} = 129 \\pm 20\\,\\text{pc}$. Consistent with recent simulations, the vertical scale height of cold HI clouds appears to remain constant between the inner-Galaxy and the Galactocentric distance of the solar neighborhood.","Local fluctuations might explain the large scale height observed at the same Galactocentric distance on the far side of the Galaxy."],"url":"http://arxiv.org/abs/2403.18981v1","category":"astro-ph.GA"}
{"created":"2024-03-27 19:54:20","title":"Complexity of emerging magnetic flux during lifetime of solar ephemeral regions","abstract":"As a relatively active region, ephemeral region (ER) exhibits highly complex pattern of magnetic flux emergence. We aim to study detailed secondary flux emergences (SFEs) which we define as bipoles that they appear close to ERs and finally coalesce with ERs after a period. We study the SFEs during the whole process from emergence to decay of 5 ERs observed by the Helioseismic and Magnetic Imager (HMI) aboard Solar Dynamics Observatory (SDO) . The maximum unsigned magnetic flux for each ER is around $10^{20}$ Mx. Each ER has tens of SFEs with an average emerging magnetic flux of approximately 5$\\times10^{18}$ Mx. The frequency of normalized magnetic flux for all the SFEs follows a power law distribution with an index of -2.08 . The majority of SFEs occur between the positive and negative polarities of ER , and their growth time is concentrated within one hour. The magnetic axis of SFE is found to exhibit a random distribution in the 5 ERs. We suggest that the relationship between SFEs and ERs can be understood by regarding the photospheric magnetic field observations as cross-sections of an emerging magnetic structure. Tracking the ERs' evolution, we propose that these SFEs in ERs may be sequent emergences from the bundle of flux tube of ERs, and that SFEs are partially emerged $\\Omega$-loops.","sentences":["As a relatively active region, ephemeral region (ER) exhibits highly complex pattern of magnetic flux emergence.","We aim to study detailed secondary flux emergences (SFEs) which we define as bipoles that they appear close to ERs and finally coalesce with ERs after a period.","We study the SFEs during the whole process from emergence to decay of 5 ERs observed by the Helioseismic and Magnetic Imager (HMI) aboard Solar Dynamics Observatory (SDO) .","The maximum unsigned magnetic flux for each ER is around $10^{20}$ Mx.","Each ER has tens of SFEs with an average emerging magnetic flux of approximately 5$\\times10^{18}$ Mx.","The frequency of normalized magnetic flux for all the SFEs follows a power law distribution with an index of -2.08 .","The majority of SFEs occur between the positive and negative polarities of ER , and their growth time is concentrated within one hour.","The magnetic axis of SFE is found to exhibit a random distribution in the 5 ERs.","We suggest that the relationship between SFEs and ERs can be understood by regarding the photospheric magnetic field observations as cross-sections of an emerging magnetic structure.","Tracking the ERs' evolution, we propose that these SFEs in ERs may be sequent emergences from the bundle of flux tube of ERs, and that SFEs are partially emerged $\\Omega$-loops."],"url":"http://arxiv.org/abs/2403.18979v1","category":"astro-ph.SR"}
