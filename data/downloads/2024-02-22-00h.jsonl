{"created":"2024-02-20 18:59:55","title":"CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples","abstract":"We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models. In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning. Our work pioneers an approach that addresses these gaps. We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning. We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark. Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V.","sentences":["We propose CounterCurate, a framework to comprehensively improve the visio-linguistic compositional reasoning capability for both contrastive and generative multimodal models.","In particular, we identify two under-explored critical problems: the neglect of the physically grounded reasoning (counting and position understanding) and the potential of using highly capable text and image generation models for semantic counterfactual fine-tuning.","Our work pioneers an approach that addresses these gaps.","We first spotlight the near-chance performance of multimodal models like CLIP and LLaVA in physically grounded compositional reasoning.","We then apply simple data augmentation using a grounded image generation model, GLIGEN, to generate finetuning data, resulting in significant performance improvements: +33% and +37% for CLIP and LLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.","Moreover, we exploit the capabilities of high-performing text generation and image generation models, specifically GPT-4V and DALLE-3, to curate challenging semantic counterfactuals, thereby further enhancing compositional reasoning capabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms GPT-4V."],"url":"http://arxiv.org/abs/2402.13254v1","category":"cs.CV"}
{"created":"2024-02-20 18:59:26","title":"BiMediX: Bilingual Medical Mixture of Experts LLM","abstract":"In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic. Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering. We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations. We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs. Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning. Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference. Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic medical benchmark and 15% on bilingual evaluations across multiple datasets. Our project page with source code and trained model is available at https://github.com/mbzuai-oryx/BiMediX .","sentences":["In this paper, we introduce BiMediX, the first bilingual medical mixture of experts LLM designed for seamless interaction in both English and Arabic.","Our model facilitates a wide range of medical interactions in English and Arabic, including multi-turn chats to inquire about additional details such as patient symptoms and medical history, multiple-choice question answering, and open-ended question answering.","We propose a semi-automated English-to-Arabic translation pipeline with human refinement to ensure high-quality translations.","We also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.","Furthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual instruction set covering 1.3 Million diverse medical interactions, resulting in over 632 million healthcare specialized tokens for instruction tuning.","Our BiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and maintains a 1:2 Arabic-to-English ratio.","Our model outperforms state-of-the-art Med42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively, computed across multiple medical evaluation benchmarks in English, while operating at 8-times faster inference.","Moreover, our BiMediX outperforms the generic Arabic-English bilingual LLM, Jais-30B, by average absolute gains of 10% on our Arabic medical benchmark and 15% on bilingual evaluations across multiple datasets.","Our project page with source code and trained model is available at https://github.com/mbzuai-oryx/BiMediX ."],"url":"http://arxiv.org/abs/2402.13253v1","category":"cs.CL"}
{"created":"2024-02-20 18:58:54","title":"Video ReCap: Recursive Captioning of Hour-Long Videos","abstract":"Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions). However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities. We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels. The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently. We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries. Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema. Data, code, and models are available at: https://sites.google.com/view/vidrecap","sentences":["Most video captioning models are designed to process short video clips of few seconds and output text describing low-level visual concepts (e.g., objects, scenes, atomic actions).","However, most real-world videos last for minutes or hours and have a complex hierarchical structure spanning different temporal granularities.","We propose Video ReCap, a recursive video captioning model that can process video inputs of dramatically different lengths (from 1 second to 2 hours) and output video captions at multiple hierarchy levels.","The recursive video-language architecture exploits the synergy between different video hierarchies and can process hour-long videos efficiently.","We utilize a curriculum learning training scheme to learn the hierarchical structure of videos, starting from clip-level captions describing atomic actions, then focusing on segment-level descriptions, and concluding with generating summaries for hour-long videos.","Furthermore, we introduce Ego4D-HCap dataset by augmenting Ego4D with 8,267 manually collected long-range video summaries.","Our recursive model can flexibly generate captions at different hierarchy levels while also being useful for other complex video understanding tasks, such as VideoQA on EgoSchema.","Data, code, and models are available at: https://sites.google.com/view/vidrecap"],"url":"http://arxiv.org/abs/2402.13250v1","category":"cs.CV"}
{"created":"2024-02-20 18:58:49","title":"TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization","abstract":"Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations. We ask whether these advances carry over to other text summarization domains. We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes. We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences. Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size. On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics. Finally, we conducted an analysis of hallucination types with a curated error taxonomy. We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators.","sentences":["Single document news summarization has seen substantial progress on faithfulness in recent years, driven by research on the evaluation of factual consistency, or hallucinations.","We ask whether these advances carry over to other text summarization domains.","We propose a new evaluation benchmark on topic-focused dialogue summarization, generated by LLMs of varying sizes.","We provide binary sentence-level human annotations of the factual consistency of these summaries along with detailed explanations of factually inconsistent sentences.","Our analysis shows that existing LLMs hallucinate significant amounts of factual errors in the dialogue domain, regardless of the model's size.","On the other hand, when LLMs, including GPT-4, serve as binary factual evaluators, they perform poorly and can be outperformed by prevailing state-of-the-art specialized factuality evaluation metrics.","Finally, we conducted an analysis of hallucination types with a curated error taxonomy.","We find that there are diverse errors and error distributions in model-generated summaries and that non-LLM based metrics can capture all error types better than LLM-based evaluators."],"url":"http://arxiv.org/abs/2402.13249v1","category":"cs.CL"}
{"created":"2024-02-20 18:57:54","title":"On a bijection between a finite group to a non-cyclic group with divisibility of element orders","abstract":"Consider a finite group $G$ of order $n$ with a prime divisor $p$. In this article, we establish, among other results, that if the Sylow $p$-subgroup of $G$ is neither cyclic nor generalized quaternion, then there exists a bijection $f$ from $G$ onto the abelian group $C_{\\frac{n}{p}}\\times C_p$ such that for every element $x$ in $G$, the order of $x$ divides the order of $f(x)$. This resolves Question 1.5 posed in [12].","sentences":["Consider a finite group $G$ of order $n$ with a prime divisor $p$. In this article, we establish, among other results, that if the Sylow $p$-subgroup of $G$ is neither cyclic nor generalized quaternion, then there exists a bijection $f$ from $G$ onto the abelian group","$C_{\\frac{n}{p}}\\times C_p$ such that for every element $x$ in $G$, the order of $x$ divides the order of $f(x)$. This resolves Question 1.5 posed in [12]."],"url":"http://arxiv.org/abs/2402.13247v1","category":"math.GR"}
{"created":"2024-02-20 18:56:26","title":"Game theory of undirected graphical models","abstract":"We model an $n$-player game $X$ in normal form via undirected discrete graphical models where the discrete random variables represent the players and their state spaces are the set of pure strategies. There exists an edge between the vertices of the graphical model $G$ whenever there is a dependency between the associated players. We study the Spohn conditional independence (CI) variety $\\mathcal{V}_{X,\\mathcal{C}}$, which is the intersection of the independence model $\\mathcal{M}_{\\text{global}(G)}$ with the Spohn variety of the game $X$. We prove a conjecture by the first author and Sturmfels that $\\mathcal{V}_{X,\\mathcal{C}}$ is of codimension $n$ in $\\mathcal{M}_{\\mathcal{C}}$ for a generic game $X$ with binary choices. In the case where the undirected graph is a disjoint union of cliques, we analyze certain algebro-geometric features of Spohn CI varieties and prove affine universality theorems.","sentences":["We model an $n$-player game $X$ in normal form via undirected discrete graphical models where the discrete random variables represent the players and their state spaces are the set of pure strategies.","There exists an edge between the vertices of the graphical model $G$ whenever there is a dependency between the associated players.","We study the Spohn conditional independence (CI) variety $\\mathcal{V}_{X,\\mathcal{C}}$, which is the intersection of the independence model $\\mathcal{M}_{\\text{global}(G)}$ with the Spohn variety of the game $X$.","We prove a conjecture by the first author and Sturmfels that $\\mathcal{V}_{X,\\mathcal{C}}$ is of codimension $n$ in $\\mathcal{M}_{\\mathcal{C}}$ for a generic game $X$ with binary choices.","In the case where the undirected graph is a disjoint union of cliques, we analyze certain algebro-geometric features of Spohn CI varieties and prove affine universality theorems."],"url":"http://arxiv.org/abs/2402.13246v1","category":"math.AG"}
{"created":"2024-02-20 18:56:07","title":"Are Fact-Checking Tools Reliable? An Evaluation of Google Fact Check","abstract":"Fact-checking is an important way to combat misinformation on social media, especially during significant social events such as the COVID-19 pandemic and the U.S. presidential elections. In this study, we thoroughly evaluated the performance of Google Fact Check, a search engine specifically for fact-checking results, by analyzing the results returned from Google Fact Check regarding 1,000 false claims about COVID-19. We found that Google Fact Check could not provide sufficient fact-checking information for most false claims, even though the results provided are relatively reliable and helpful. We also found that claims getting different fact-checking verdicts tend to contain different emotional tones, and different sources tend to check claims using dictionary words to different extents and at different lengths. Claims in different descriptions are likely to get different fact-checking results. We aimed to bring up the best practice of fact-checking for the general people based on our analyses.","sentences":["Fact-checking is an important way to combat misinformation on social media, especially during significant social events such as the COVID-19 pandemic and the U.S. presidential elections.","In this study, we thoroughly evaluated the performance of Google Fact Check, a search engine specifically for fact-checking results, by analyzing the results returned from Google Fact Check regarding 1,000 false claims about COVID-19.","We found that Google Fact Check could not provide sufficient fact-checking information for most false claims, even though the results provided are relatively reliable and helpful.","We also found that claims getting different fact-checking verdicts tend to contain different emotional tones, and different sources tend to check claims using dictionary words to different extents and at different lengths.","Claims in different descriptions are likely to get different fact-checking results.","We aimed to bring up the best practice of fact-checking for the general people based on our analyses."],"url":"http://arxiv.org/abs/2402.13244v1","category":"cs.SI"}
{"created":"2024-02-20 18:53:53","title":"Federated Causal Discovery from Heterogeneous Data","abstract":"Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations. This discrepancy has motivated the development of federated causal discovery (FCD) approaches. However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios. In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data. We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients. We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions. These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy. Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models. We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method. The code is available at \\url{https://github.com/lokali/FedCDH.git}.","sentences":["Conventional causal discovery methods rely on centralized data, which is inconsistent with the decentralized nature of data in many real-world situations.","This discrepancy has motivated the development of federated causal discovery (FCD) approaches.","However, existing FCD methods may be limited by their potentially restrictive assumptions of identifiable functional causal models or homogeneous data distributions, narrowing their applicability in diverse scenarios.","In this paper, we propose a novel FCD method attempting to accommodate arbitrary causal models and heterogeneous data.","We first utilize a surrogate variable corresponding to the client index to account for the data heterogeneity across different clients.","We then develop a federated conditional independence test (FCIT) for causal skeleton discovery and establish a federated independent change principle (FICP) to determine causal directions.","These approaches involve constructing summary statistics as a proxy of the raw data to protect data privacy.","Owing to the nonparametric properties, FCIT and FICP make no assumption about particular functional forms, thereby facilitating the handling of arbitrary causal models.","We conduct extensive experiments on synthetic and real datasets to show the efficacy of our method.","The code is available at \\url{https://github.com/lokali/FedCDH.git}."],"url":"http://arxiv.org/abs/2402.13241v1","category":"cs.LG"}
{"created":"2024-02-20 18:49:41","title":"Unlocking Insights: Semantic Search in Jupyter Notebooks","abstract":"Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher's intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in information retrieval. In this paper, we investigate the application of large language models to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks. Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent information.   We demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook's contents, enabling it to effectively handle various types of user queries. Key components of this framework include:   1). A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells. 2). An innovative methodology is devised to address token size limitations that arise with code-type cells. We implement a finer-grained approach to data input, transitioning from the cell level to the function level, effectively resolving these issues.","sentences":["Semantic search, a process aimed at delivering highly relevant search results by comprehending the searcher's intent and the contextual meaning of terms within a searchable dataspace, plays a pivotal role in information retrieval.","In this paper, we investigate the application of large language models to enhance semantic search capabilities, specifically tailored for the domain of Jupyter Notebooks.","Our objective is to retrieve generated outputs, such as figures or tables, associated functions and methods, and other pertinent information.   ","We demonstrate a semantic search framework that achieves a comprehensive semantic understanding of the entire notebook's contents, enabling it to effectively handle various types of user queries.","Key components of this framework include:   1).","A data preprocessor is designed to handle diverse types of cells within Jupyter Notebooks, encompassing both markdown and code cells.","2).","An innovative methodology is devised to address token size limitations that arise with code-type cells.","We implement a finer-grained approach to data input, transitioning from the cell level to the function level, effectively resolving these issues."],"url":"http://arxiv.org/abs/2402.13234v1","category":"cs.IR"}
{"created":"2024-02-20 18:47:56","title":"A Touch, Vision, and Language Dataset for Multimodal Alignment","abstract":"Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model. This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions. As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%). We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder. Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities. Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark. Code and data: https://tactile-vlm.github.io.","sentences":["Touch is an important sensing modality for humans, but it has not yet been incorporated into a multimodal generative language model.","This is partially due to the difficulty of obtaining natural language labels for tactile data and the complexity of aligning tactile readings with both visual observations and language descriptions.","As a step towards bridging that gap, this work introduces a new dataset of 44K in-the-wild vision-touch pairs, with English language labels annotated by humans (10%) and textual pseudo-labels from GPT-4V (90%).","We use this dataset to train a vision-language-aligned tactile encoder for open-vocabulary classification and a touch-vision-language (TVL) model for text generation using the trained encoder.","Results suggest that by incorporating touch, the TVL model improves (+29% classification accuracy) touch-vision-language alignment over existing models trained on any pair of those modalities.","Although only a small fraction of the dataset is human-labeled, the TVL model demonstrates improved visual-tactile understanding over GPT-4V (+12%) and open-source vision-language models (+32%) on a new touch-vision understanding benchmark.","Code and data: https://tactile-vlm.github.io."],"url":"http://arxiv.org/abs/2402.13232v1","category":"cs.CV"}
{"created":"2024-02-20 18:42:34","title":"Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive","abstract":"Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment. Using pairs of preferred and dispreferred data, DPO models the \\textit{relative} probability of picking one response over another. In this work, first we show theoretically that the standard DPO loss can lead to a \\textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases. We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low. Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode. Surprisingly, we also find that DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions. By fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which achieve state-of-the-art open-source performance. Notably, Smaug-72B is nearly 2\\% better than any other open-source model on the HuggingFace Open LLM Leaderboard and becomes the first open-source LLM to surpass an average accuracy of 80\\%.","sentences":["Direct Preference Optimisation (DPO) is effective at significantly improving the performance of large language models (LLMs) on downstream tasks such as reasoning, summarisation, and alignment.","Using pairs of preferred and dispreferred data, DPO models the \\textit{relative} probability of picking one response over another.","In this work, first we show theoretically that the standard DPO loss can lead to a \\textit{reduction} of the model's likelihood of the preferred examples, as long as the relative probability between the preferred and dispreferred classes increases.","We then show empirically that this phenomenon occurs when fine-tuning LLMs on common datasets, especially datasets in which the edit distance between pairs of completions is low.","Using these insights, we design DPO-Positive (DPOP), a new loss function and training procedure which avoids this failure mode.","Surprisingly, we also find that DPOP significantly outperforms DPO across a wide variety of datasets and downstream tasks, including datasets with high edit distances between completions.","By fine-tuning with DPOP, we create and release Smaug-34B and Smaug-72B, which achieve state-of-the-art open-source performance.","Notably, Smaug-72B is nearly 2\\% better than any other open-source model on the HuggingFace Open LLM Leaderboard and becomes the first open-source LLM to surpass an average accuracy of 80\\%."],"url":"http://arxiv.org/abs/2402.13228v1","category":"cs.CL"}
{"created":"2024-02-20 18:37:42","title":"NeRF Solves Undersampled MRI Reconstruction","abstract":"This article presents a novel undersampled magnetic resonance imaging (MRI) technique that leverages the concept of Neural Radiance Field (NeRF). With radial undersampling, the corresponding imaging problem can be reformulated into an image modeling task from sparse-view rendered data; therefore, a high dimensional MR image is obtainable from undersampled $k$-space data by taking advantage of implicit neural representation. A multi-layer perceptron, which is designed to output an image intensity from a spatial coordinate, learns the MR physics-driven rendering relation between given measurement data and desired image. Effective undersampling strategies for high-quality neural representation are investigated. The proposed method serves two benefits: (i) The learning is based fully on single undersampled $k$-space data, not a bunch of measured data and target image sets. It can be used potentially for diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively rare or limited against diversity of clinical images while undersampled reconstruction is highly demanded. (ii) A reconstructed MR image is a scan-specific representation highly adaptive to the given $k$-space measurement. Numerous experiments validate the feasibility and capability of the proposed approach.","sentences":["This article presents a novel undersampled magnetic resonance imaging (MRI) technique that leverages the concept of Neural Radiance Field (NeRF).","With radial undersampling, the corresponding imaging problem can be reformulated into an image modeling task from sparse-view rendered data; therefore, a high dimensional MR image is obtainable from undersampled $k$-space data by taking advantage of implicit neural representation.","A multi-layer perceptron, which is designed to output an image intensity from a spatial coordinate, learns the MR physics-driven rendering relation between given measurement data and desired image.","Effective undersampling strategies for high-quality neural representation are investigated.","The proposed method serves two benefits: (i)","The learning is based fully on single undersampled $k$-space data, not a bunch of measured data and target image sets.","It can be used potentially for diagnostic MR imaging, such as fetal MRI, where data acquisition is relatively rare or limited against diversity of clinical images while undersampled reconstruction is highly demanded.","(ii) A reconstructed MR image is a scan-specific representation highly adaptive to the given $k$-space measurement.","Numerous experiments validate the feasibility and capability of the proposed approach."],"url":"http://arxiv.org/abs/2402.13226v1","category":"eess.IV"}
{"created":"2024-02-20 18:37:19","title":"AgentMD: Empowering Language Agents for Risk Prediction with Large-Scale Clinical Tool Learning","abstract":"Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis. Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality. Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge. In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts. Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs. Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics. At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description. On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy). Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics. In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care.","sentences":["Clinical calculators play a vital role in healthcare by offering accurate evidence-based predictions for various purposes such as prognosis.","Nevertheless, their widespread utilization is frequently hindered by usability challenges, poor dissemination, and restricted functionality.","Augmenting large language models with extensive collections of clinical calculators presents an opportunity to overcome these obstacles and improve workflow efficiency, but the scalability of the manual curation process poses a significant challenge.","In response, we introduce AgentMD, a novel language agent capable of curating and applying clinical calculators across various clinical contexts.","Using the published literature, AgentMD has automatically curated a collection of 2,164 diverse clinical calculators with executable functions and structured documentation, collectively named RiskCalcs.","Manual evaluations show that RiskCalcs tools achieve an accuracy of over 80% on three quality metrics.","At inference time, AgentMD can automatically select and apply the relevant RiskCalcs tools given any patient description.","On the newly established RiskQA benchmark, AgentMD significantly outperforms chain-of-thought prompting with GPT-4 (87.7% vs. 40.9% in accuracy).","Additionally, we also applied AgentMD to real-world clinical notes for analyzing both population-level and risk-level patient characteristics.","In summary, our study illustrates the utility of language agents augmented with clinical calculators for healthcare analytics and patient care."],"url":"http://arxiv.org/abs/2402.13225v1","category":"cs.CL"}
{"created":"2024-02-20 18:37:11","title":"Controlling Large Electric Vehicle Charging Stations via User Behavior Modeling and Stochastic Programming","abstract":"This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs). We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming. The model addresses uncertainties in charging session start and end times, as well as in energy demand. A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction. The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset. The two-stage approach proves robust against early disconnections, considering a more significant number of uncertainty scenarios for optimization. The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline. Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics.","sentences":["This paper introduces an Electric Vehicle Charging Station (EVCS) model that incorporates real-world constraints, such as slot power limitations, contract threshold overruns penalties, or early disconnections of electric vehicles (EVs).","We propose a formulation of the problem of EVCS control under uncertainty, and implement two Multi-Stage Stochastic Programming approaches that leverage user-provided information, namely, Model Predictive Control and Two-Stage Stochastic Programming.","The model addresses uncertainties in charging session start and end times, as well as in energy demand.","A user's behavior model based on a sojourn-time-dependent stochastic process enhances cost reduction while maintaining customer satisfaction.","The benefits of the two proposed methods are showcased against two baselines over a 22-day simulation using a real-world dataset.","The two-stage approach proves robust against early disconnections, considering a more significant number of uncertainty scenarios for optimization.","The algorithm prioritizing user satisfaction over electricity cost achieves a 20% and 36% improvement in two user satisfaction metrics compared to an industry-standard baseline.","Additionally, the algorithm striking the best balance between cost and user satisfaction exhibits a mere 3% relative cost increase compared to the theoretically optimal baseline - for which the nonanticipativity constraint is relaxed - while attaining 94% and 84% of the user satisfaction performance in the two used satisfaction metrics."],"url":"http://arxiv.org/abs/2402.13224v1","category":"math.OC"}
{"created":"2024-02-20 18:33:27","title":"Metallicities and Refined Stellar Parameters for 52 Cool Dwarfs with Transiting Planets and Planet Candidates","abstract":"We collected near-infrared spectra of 65 cool stars with the NASA InfraRed Telescope Facility (IRTF) and analyze them to calculate accurate metallicities and stellar parameters. The sample of 55 M dwarfs and 10 K dwarfs includes 25 systems with confirmed planets and 27 systems with planet candidates identified by the K2 and TESS missions. Three of the 25 confirmed planetary systems host multiple confirmed planets and two of the 27 planet candidate systems host multiple planet candidates. Using the new stellar parameters, we re-fit the K2 and TESS light curves to calculate updated planet properties. In general, our updated stellar properties are more precise than those previously reported and our updated planet properties agree well with those in the literature. Lastly, we briefly examine the relationship between stellar mass, stellar metallicity, and planetary system properties for targets in our sample and for previously characterized planet-hosting low-mass stars. We provide our spectra, stellar parameters, and new planetary fits to the community, expanding the sample available with which to investigate correlations between stellar and planetary properties for low-mass stars.","sentences":["We collected near-infrared spectra of 65 cool stars with the NASA InfraRed Telescope Facility (IRTF) and analyze them to calculate accurate metallicities and stellar parameters.","The sample of 55 M dwarfs and 10 K dwarfs includes 25 systems with confirmed planets and 27 systems with planet candidates identified by the K2 and TESS missions.","Three of the 25 confirmed planetary systems host multiple confirmed planets and two of the 27 planet candidate systems host multiple planet candidates.","Using the new stellar parameters, we re-fit the K2 and TESS light curves to calculate updated planet properties.","In general, our updated stellar properties are more precise than those previously reported and our updated planet properties agree well with those in the literature.","Lastly, we briefly examine the relationship between stellar mass, stellar metallicity, and planetary system properties for targets in our sample and for previously characterized planet-hosting low-mass stars.","We provide our spectra, stellar parameters, and new planetary fits to the community, expanding the sample available with which to investigate correlations between stellar and planetary properties for low-mass stars."],"url":"http://arxiv.org/abs/2402.13223v1","category":"astro-ph.EP"}
{"created":"2024-02-20 18:32:47","title":"RoCode: A Dataset for Measuring Code Intelligence from Problem Definitions in Romanian","abstract":"Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language. However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language. Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs. Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English. In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem. The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models. Through our results and review of related works, we argue for the need to develop code models for languages other than English.","sentences":["Recently, large language models (LLMs) have become increasingly powerful and have become capable of solving a plethora of tasks through proper instructions in natural language.","However, the vast majority of testing suites assume that the instructions are written in English, the de facto prompting language.","Code intelligence and problem solving still remain a difficult task, even for the most advanced LLMs.","Currently, there are no datasets to measure the generalization power for code-generation models in a language other than English.","In this work, we present RoCode, a competitive programming dataset, consisting of 2,642 problems written in Romanian, 11k solutions in C, C++ and Python and comprehensive testing suites for each problem.","The purpose of RoCode is to provide a benchmark for evaluating the code intelligence of language models trained on Romanian / multilingual text as well as a fine-tuning set for pretrained Romanian models.","Through our results and review of related works, we argue for the need to develop code models for languages other than English."],"url":"http://arxiv.org/abs/2402.13222v1","category":"cs.CL"}
{"created":"2024-02-20 18:32:27","title":"CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning","abstract":"Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules. While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials. Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address. Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input. However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text.   We invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K). We define 11 property prediction tasks and 6 structure prediction tasks, which are of special interest for nanomaterial research. We benchmark the performance of a wide array of baseline methods and use these benchmarking results to highlight areas which need future work. To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale -- both on the individual graph level and of the dataset as a whole -- and the only nanomaterials datasets with high structural and elemental diversity.","sentences":["Advances in graph machine learning (ML) have been driven by applications in chemistry as graphs have remained the most expressive representations of molecules.","While early graph ML methods focused primarily on small organic molecules, recently, the scope of graph ML has expanded to include inorganic materials.","Modelling the periodicity and symmetry of inorganic crystalline materials poses unique challenges, which existing graph ML methods are unable to address.","Moving to inorganic nanomaterials increases complexity as the scale of number of nodes within each graph can be broad ($10$ to $10^5$).","The bulk of existing graph ML focuses on characterising molecules and materials by predicting target properties with graphs as input.","However, the most exciting applications of graph ML will be in their generative capabilities, which is currently not at par with other domains such as images or text.   ","We invite the graph ML community to address these open challenges by presenting two new chemically-informed large-scale inorganic (CHILI) nanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M edges) of mono-metallic oxide nanomaterials generated from 12 selected crystal types (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B edges) of nanomaterials generated from experimentally determined crystal structures (CHILI-100K).","We define 11 property prediction tasks and 6 structure prediction tasks, which are of special interest for nanomaterial research.","We benchmark the performance of a wide array of baseline methods and use these benchmarking results to highlight areas which need future work.","To the best of our knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial datasets of this scale -- both on the individual graph level and of the dataset as a whole -- and the only nanomaterials datasets with high structural and elemental diversity."],"url":"http://arxiv.org/abs/2402.13221v1","category":"cs.LG"}
{"created":"2024-02-20 18:31:27","title":"Analyzing Operator States and the Impact of AI-Enhanced Decision Support in Control Rooms: A Human-in-the-Loop Specialized Reinforcement Learning Framework for Intervention Strategies","abstract":"In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and efficiency. The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influence diagrams, a hidden Markov model, and deep reinforcement learning. The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance. Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training. A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye-tracking data, process logs, and responses from questionnaires. The results indicate interesting insights regarding the effectiveness of the approach in aiding decision-making, decreasing perceived workload, and increasing situational awareness for the scenarios considered. Additionally, the results provide valuable insights to compare differences between styles of information gathering when using the system by individual participants. These findings are particularly relevant when predicting the overall performance of the individual participant and their capacity to successfully handle a plant upset and the alarms connected to it using process and human-machine interaction logs in real-time. These predictions enable the development of more effective intervention strategies.","sentences":["In complex industrial and chemical process control rooms, effective decision-making is crucial for safety and efficiency.","The experiments in this paper evaluate the impact and applications of an AI-based decision support system integrated into an improved human-machine interface, using dynamic influence diagrams, a hidden Markov model, and deep reinforcement learning.","The enhanced support system aims to reduce operator workload, improve situational awareness, and provide different intervention strategies to the operator adapted to the current state of both the system and human performance.","Such a system can be particularly useful in cases of information overload when many alarms and inputs are presented all within the same time window, or for junior operators during training.","A comprehensive cross-data analysis was conducted, involving 47 participants and a diverse range of data sources such as smartwatch metrics, eye-tracking data, process logs, and responses from questionnaires.","The results indicate interesting insights regarding the effectiveness of the approach in aiding decision-making, decreasing perceived workload, and increasing situational awareness for the scenarios considered.","Additionally, the results provide valuable insights to compare differences between styles of information gathering when using the system by individual participants.","These findings are particularly relevant when predicting the overall performance of the individual participant and their capacity to successfully handle a plant upset and the alarms connected to it using process and human-machine interaction logs in real-time.","These predictions enable the development of more effective intervention strategies."],"url":"http://arxiv.org/abs/2402.13219v1","category":"cs.AI"}
{"created":"2024-02-20 18:30:51","title":"Spontaneous-emission induced ratchet in atom-optics kicked rotor quantum walks","abstract":"Quantum walks have gained significant attention over the past decades, mainly because of their variety of implementations and applications. Atomic quantum walks are typically subject to spontaneous emissions arising from the control fields. We investigate spontaneous emission in an atom optics kicked rotor quantum walk. Here, spontaneous emission occurs naturally due to the driving by the kicks, and it is generally viewed as a nuisance in the experiment. We find, however, that spontaneous emission may induce asymmetries in an otherwise symmetric quantum walk. Our results underscore the utility of spontaneous emission and the application of the asymmetric evolution in the walker's space, i.e. for the construction of a quantum walk ratchet or for Parrondo-like quantum games. This highlights the potential for reinterpreting seemingly adverse effects as beneficial under certain conditions, thus broadening the scope of quantum walks and their applications.","sentences":["Quantum walks have gained significant attention over the past decades, mainly because of their variety of implementations and applications.","Atomic quantum walks are typically subject to spontaneous emissions arising from the control fields.","We investigate spontaneous emission in an atom optics kicked rotor quantum walk.","Here, spontaneous emission occurs naturally due to the driving by the kicks, and it is generally viewed as a nuisance in the experiment.","We find, however, that spontaneous emission may induce asymmetries in an otherwise symmetric quantum walk.","Our results underscore the utility of spontaneous emission and the application of the asymmetric evolution in the walker's space, i.e. for the construction of a quantum walk ratchet or for Parrondo-like quantum games.","This highlights the potential for reinterpreting seemingly adverse effects as beneficial under certain conditions, thus broadening the scope of quantum walks and their applications."],"url":"http://arxiv.org/abs/2402.13218v1","category":"quant-ph"}
{"created":"2024-02-20 18:29:49","title":"VideoPrism: A Foundational Visual Encoder for Video Understanding","abstract":"We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model. We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts). The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos. We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks.","sentences":["We introduce VideoPrism, a general-purpose video encoder that tackles diverse video understanding tasks with a single frozen model.","We pretrain VideoPrism on a heterogeneous corpus containing 36M high-quality video-caption pairs and 582M video clips with noisy parallel text (e.g., ASR transcripts).","The pretraining approach improves upon masked autoencoding by global-local distillation of semantic video embeddings and a token shuffling scheme, enabling VideoPrism to focus primarily on the video modality while leveraging the invaluable text associated with videos.","We extensively test VideoPrism on four broad groups of video understanding tasks, from web video question answering to CV for science, achieving state-of-the-art performance on 30 out of 33 video understanding benchmarks."],"url":"http://arxiv.org/abs/2402.13217v1","category":"cs.CV"}
{"created":"2024-02-20 18:29:26","title":"Basic Loci of Positive Coxeter Type for $GL_n$","abstract":"Motivated by the problem of giving an explicit description of the basic locus in the reduction of Shimura varieties, G\\\"{o}rtz, He and Nie studied the cases where the basic affine Deligne-Lusztig variety, which serves as its group-theoretic model, is a union of classical Deligne-Lusztig varieties associated to Coxeter elements. In this paper, we study a natural generalization of this stratification in the case of $GL_n$.","sentences":["Motivated by the problem of giving an explicit description of the basic locus in the reduction of Shimura varieties, G\\\"{o}rtz, He and Nie studied the cases where the basic affine Deligne-Lusztig variety, which serves as its group-theoretic model, is a union of classical Deligne-Lusztig varieties associated to Coxeter elements.","In this paper, we study a natural generalization of this stratification in the case of $GL_n$."],"url":"http://arxiv.org/abs/2402.13216v1","category":"math.AG"}
{"created":"2024-02-20 18:24:47","title":"Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&A","abstract":"Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task. For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of softmax probabilities and find similar (but not identical) results.","sentences":["Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem.","We hypothesized that on multiple-choice Q&A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers.","We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&A task.","For the six LLMs with the best Q&A performance, the AUROC derived from the MSP was better than random chance with p < 10^{-4} in 59/60 instances.","Among those six LLMs, the average AUROC ranged from 60% to 69%.","Leveraging these findings, we propose a multiple-choice Q&A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response.","We also run the same experiments with pre-softmax logits instead of softmax probabilities and find similar (but not identical) results."],"url":"http://arxiv.org/abs/2402.13213v1","category":"cs.CL"}
{"created":"2024-02-20 18:22:38","title":"Soft Self-Consistency Improves Language Model Agents","abstract":"Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer. Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers. However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples. This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially. After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion. We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed. Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance. For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld). Finally, we show that Soft-SC can be applied to both open-source and black-box models.","sentences":["Generations from large language models (LLMs) can be improved by sampling and scoring multiple solutions to select a final answer.","Current \"sample and select\" methods such as self-consistency (SC) rely on majority voting to score answers.","However, when tasks have many distinct and valid answers, selection by voting requires a large number of samples.","This makes SC prohibitively expensive for interactive tasks that involve generating multiple actions (answers) sequentially.","After establishing that majority voting fails to provide consistent gains on such tasks, we demonstrate how to increase success rates by softening the scoring criterion.","We introduce Soft Self-Consistency (Soft-SC), which replaces SC's discontinuous scoring with a continuous score computed from model likelihoods, allowing for selection even when actions are sparsely distributed.","Soft-SC improves both performance and efficiency on long-horizon interactive tasks, requiring half as many samples as SC for comparable or better performance.","For a fixed number of samples, Soft-SC leads to a 1.3% increase over SC in absolute success rate on writing bash programs, a 6.6% increase on online shopping (WebShop), and a 4.7% increase for an interactive household game (ALFWorld).","Finally, we show that Soft-SC can be applied to both open-source and black-box models."],"url":"http://arxiv.org/abs/2402.13212v1","category":"cs.CL"}
{"created":"2024-02-20 18:21:32","title":"Can Large Language Models be Good Emotional Supporter? Mitigating Preference Bias on Emotional Support Conversation","abstract":"Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation. Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses. Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support. Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy. Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy. Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters. Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) LLMs alone cannot become good emotional supporters. These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs.","sentences":["Emotional Support Conversation (ESC) is a task aimed at alleviating individuals' emotional distress through daily conversation.","Given its inherent complexity and non-intuitive nature, ESConv dataset incorporates support strategies to facilitate the generation of appropriate responses.","Recently, despite the remarkable conversational ability of large language models (LLMs), previous studies have suggested that they often struggle with providing useful emotional support.","Hence, this work initially analyzes the results of LLMs on ESConv, revealing challenges in selecting the correct strategy and a notable preference for a specific strategy.","Motivated by these, we explore the impact of the inherent preference in LLMs on providing emotional support, and consequently, we observe that exhibiting high preference for specific strategies hinders effective emotional support, aggravating its robustness in predicting the appropriate strategy.","Moreover, we conduct a methodological study to offer insights into the necessary approaches for LLMs to serve as proficient emotional supporters.","Our findings emphasize that (1) low preference for specific strategies hinders the progress of emotional support, (2) external assistance helps reduce preference bias, and (3) LLMs alone cannot become good emotional supporters.","These insights suggest promising avenues for future research to enhance the emotional intelligence of LLMs."],"url":"http://arxiv.org/abs/2402.13211v1","category":"cs.CL"}
{"created":"2024-02-20 18:19:08","title":"How do Hyenas deal with Human Speech? Speech Recognition and Translation with ConfHyena","abstract":"The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity. Consequently, research efforts in the last few years focused on finding more efficient alternatives. Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity. Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs. Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%), which, in most cases, is not statistically significant.","sentences":["The attention mechanism, a cornerstone of state-of-the-art neural models, faces computational hurdles in processing long sequences due to its quadratic complexity.","Consequently, research efforts in the last few years focused on finding more efficient alternatives.","Among them, Hyena (Poli et al., 2023) stands out for achieving competitive results in both language modeling and image classification, while offering sub-quadratic memory and computational complexity.","Building on these promising results, we propose ConfHyena, a Conformer whose encoder self-attentions are replaced with an adaptation of Hyena for speech processing, where the long input sequences cause high computational costs.","Through experiments in automatic speech recognition (for English) and translation (from English into 8 target languages), we show that our best ConfHyena model significantly reduces the training time by 27%, at the cost of minimal quality degradation (~1%), which, in most cases, is not statistically significant."],"url":"http://arxiv.org/abs/2402.13208v1","category":"cs.CL"}
{"created":"2024-02-20 18:15:23","title":"Formulas for the Number of Lines on Complex Projective Hypersurfaces","abstract":"Two formulas for the classical number $C_n$ of lines on a generic hypersurface of degree $2n-3$ in $\\mathbb{CP}^n$ are obtained which substantially differ from Zagier's formula. Schubert calculus leads to an explicit general closed-form formula in terms of binomial coefficients, the Catalan numbers, and elementary symmetric polynomials evaluated at certain integers. This in turn yields $C_n$ as a linear difference recursion relation of unbounded order. Thus, for the sequence of certain linear combinations of $C_n$, a simple generating function is found. Then, a result from random algebraic geometry by Basu, Lerario, Lundberg, and Peterson, that expresses these classical enumerative invariants as proportional to the Bombieri norm of particular polynomial determinants, yields another combinatorial expansion in terms of certain set compositions and block counting. As an example, we compute the 27 lines on a cubic surface and 2875 lines on a quintic threefold. As an application, we reobtain the parity and asymptotic upper bound of the sequence without using Zagier's formula. In consequence, we provide two alternative expressions to the latter for the number of lines in hypersurfaces which may open new directions to study the arithmetic, combinatorial, and analytical properties of this enumerative geometry sequence.","sentences":["Two formulas for the classical number $C_n$ of lines on a generic hypersurface of degree $2n-3$ in $\\mathbb{CP}^n$ are obtained which substantially differ from Zagier's formula.","Schubert calculus leads to an explicit general closed-form formula in terms of binomial coefficients, the Catalan numbers, and elementary symmetric polynomials evaluated at certain integers.","This in turn yields $C_n$ as a linear difference recursion relation of unbounded order.","Thus, for the sequence of certain linear combinations of $C_n$, a simple generating function is found.","Then, a result from random algebraic geometry by Basu, Lerario, Lundberg, and Peterson, that expresses these classical enumerative invariants as proportional to the Bombieri norm of particular polynomial determinants, yields another combinatorial expansion in terms of certain set compositions and block counting.","As an example, we compute the 27 lines on a cubic surface and 2875 lines on a quintic threefold.","As an application, we reobtain the parity and asymptotic upper bound of the sequence without using Zagier's formula.","In consequence, we provide two alternative expressions to the latter for the number of lines in hypersurfaces which may open new directions to study the arithmetic, combinatorial, and analytical properties of this enumerative geometry sequence."],"url":"http://arxiv.org/abs/2402.13206v1","category":"math.CO"}
{"created":"2024-02-20 18:15:11","title":"SONATA: Self-adaptive Evolutionary Framework for Hardware-aware Neural Architecture Search","abstract":"Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency. HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms. However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly. Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front. Addressing these issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware NAS. Our method leverages adaptive evolutionary operators guided by the learned importance of NN design parameters. Specifically, through tree-based surrogate models and a Reinforcement Learning agent, we aspire to gather knowledge on 'How' and 'When' to evolve NN architectures. Comprehensive evaluations across various NAS search spaces and hardware devices on the ImageNet-1k dataset have shown the merit of SONATA with up to 0.25% improvement in accuracy and up to 2.42x gains in latency and energy. Our SONATA has seen up to sim$93.6% Pareto dominance over the native NSGA-II, further stipulating the importance of self-adaptive evolution operators in HW-aware NAS.","sentences":["Recent advancements in Artificial Intelligence (AI), driven by Neural Networks (NN), demand innovative neural architecture designs, particularly within the constrained environments of Internet of Things (IoT) systems, to balance performance and efficiency.","HW-aware Neural Architecture Search (HW-aware NAS) emerges as an attractive strategy to automate the design of NN using multi-objective optimization approaches, such as evolutionary algorithms.","However, the intricate relationship between NN design parameters and HW-aware NAS optimization objectives remains an underexplored research area, overlooking opportunities to effectively leverage this knowledge to guide the search process accordingly.","Furthermore, the large amount of evaluation data produced during the search holds untapped potential for refining the optimization strategy and improving the approximation of the Pareto front.","Addressing these issues, we propose SONATA, a self-adaptive evolutionary algorithm for HW-aware NAS.","Our method leverages adaptive evolutionary operators guided by the learned importance of NN design parameters.","Specifically, through tree-based surrogate models and a Reinforcement Learning agent, we aspire to gather knowledge on 'How' and 'When' to evolve NN architectures.","Comprehensive evaluations across various NAS search spaces and hardware devices on the ImageNet-1k dataset have shown the merit of SONATA with up to 0.25% improvement in accuracy and up to 2.42x gains in latency and energy.","Our SONATA has seen up to sim$93.6","%","Pareto dominance over the native NSGA-II, further stipulating the importance of self-adaptive evolution operators in HW-aware NAS."],"url":"http://arxiv.org/abs/2402.13204v1","category":"cs.NE"}
{"created":"2024-02-20 18:12:59","title":"Quantum Sensing of Antiferromagnetic Magnon Two-Mode Squeezed Vacuum","abstract":"N\\'eel ordered antiferromagnets exhibit two-mode squeezing such that their ground state is a nonclassical superposition of magnon Fock states. Here we theoretically demonstrate that antiferromagnets can couple to spin qubits via direct dispersive interaction stemming from, e.g., interfacial exchange. We demonstrate that this kind of coupling induces a magnon number dependent level splitting of the excited state resulting in multiple system excitation energies. This series of level splittings manifests itself as nontrivial excitation peaks in qubit spectroscopy thereby revealing the underlying nonclassical magnon composition of the antiferromagnetic quantum state. By appropriately choosing the drive or excitation energy, the magnonic state can be controlled via the qubit, suggesting that Fock states of magnon pairs can be generated deterministically. This enables achieving states useful for quantum computing and quantum information science protocols.","sentences":["N\\'eel ordered antiferromagnets exhibit two-mode squeezing such that their ground state is a nonclassical superposition of magnon Fock states.","Here we theoretically demonstrate that antiferromagnets can couple to spin qubits via direct dispersive interaction stemming from, e.g., interfacial exchange.","We demonstrate that this kind of coupling induces a magnon number dependent level splitting of the excited state resulting in multiple system excitation energies.","This series of level splittings manifests itself as nontrivial excitation peaks in qubit spectroscopy thereby revealing the underlying nonclassical magnon composition of the antiferromagnetic quantum state.","By appropriately choosing the drive or excitation energy, the magnonic state can be controlled via the qubit, suggesting that Fock states of magnon pairs can be generated deterministically.","This enables achieving states useful for quantum computing and quantum information science protocols."],"url":"http://arxiv.org/abs/2402.13203v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 18:10:39","title":"Tiny Reinforcement Learning for Quadruped Locomotion using Decision Transformers","abstract":"Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics. For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms. We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms. Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward. Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning. We test our method in simulation using Isaac Gym, a realistic physics simulation environment designed for reinforcement learning. We empirically demonstrate that our method achieves natural looking gaits for Bittle, a resource-constrained quadruped robot. We also run multiple simulations to show the effects of pruning and quantization on the performance of the model. Our results show that quantization (down to 4 bits) and pruning reduce model size by around 30\\% while maintaining a competitive reward, making the model deployable in a resource-constrained system.","sentences":["Resource-constrained robotic platforms are particularly useful for tasks that require low-cost hardware alternatives due to the risk of losing the robot, like in search-and-rescue applications, or the need for a large number of devices, like in swarm robotics.","For this reason, it is crucial to find mechanisms for adapting reinforcement learning techniques to the constraints imposed by lower computational power and smaller memory capacities of these ultra low-cost robotic platforms.","We try to address this need by proposing a method for making imitation learning deployable onto resource-constrained robotic platforms.","Here we cast the imitation learning problem as a conditional sequence modeling task and we train a decision transformer using expert demonstrations augmented with a custom reward.","Then, we compress the resulting generative model using software optimization schemes, including quantization and pruning.","We test our method in simulation using Isaac Gym, a realistic physics simulation environment designed for reinforcement learning.","We empirically demonstrate that our method achieves natural looking gaits for Bittle, a resource-constrained quadruped robot.","We also run multiple simulations to show the effects of pruning and quantization on the performance of the model.","Our results show that quantization (down to 4 bits) and pruning reduce model size by around 30\\% while maintaining a competitive reward, making the model deployable in a resource-constrained system."],"url":"http://arxiv.org/abs/2402.13201v1","category":"cs.RO"}
{"created":"2024-02-20 18:05:55","title":"Quantum Wiretap Channel Coding Assisted by Noisy Correlation","abstract":"We consider the private classical capacity of a quantum wiretap channel, where the users (sender Alice, receiver Bob, and eavesdropper Eve) have access to the resource of a shared quantum state, additionally to their channel inputs and outputs. An extreme case is maximal entanglement or a secret key between Alice and Bob, both of which would allow for onetime padding the message. But here both the wiretap channel and the shared state are general. In the other extreme case that the state is trivial, we recover the wiretap channel and its private capacity [N. Cai, A. Winter and R. W. Yeung, Probl. Inform. Transm. 40(4):318-336, 2004]. We show how to use the given resource state to build a code for secret classical communication. Our main result is a lower bound on the assisted private capacity, which asymptotically meets the multi-letter converse and which encompasses all sorts of previous results as special cases.","sentences":["We consider the private classical capacity of a quantum wiretap channel, where the users (sender Alice, receiver Bob, and eavesdropper Eve) have access to the resource of a shared quantum state, additionally to their channel inputs and outputs.","An extreme case is maximal entanglement or a secret key between Alice and Bob, both of which would allow for onetime padding the message.","But here both the wiretap channel and the shared state are general.","In the other extreme case that the state is trivial, we recover the wiretap channel and its private capacity [N. Cai, A. Winter and R. W. Yeung, Probl.","Inform.","Transm.","40(4):318-336, 2004].","We show how to use the given resource state to build a code for secret classical communication.","Our main result is a lower bound on the assisted private capacity, which asymptotically meets the multi-letter converse and which encompasses all sorts of previous results as special cases."],"url":"http://arxiv.org/abs/2402.13194v1","category":"quant-ph"}
{"created":"2024-02-20 18:03:21","title":"Mutual linearity of nonequilibrium network currents","abstract":"For continuous-time Markov chains and open unimolecular chemical reaction networks, we prove that any two stationary currents are linearly related upon perturbations of a single edge's transition rates, arbitrarily far from equilibrium. We extend the result to non-stationary currents in the frequency domain, provide and discuss an explicit expression for the current-current susceptibility in terms of the network topology, and discuss possible generalizations. In practical scenarios, the mutual linearity relation has predictive power and can be used as a tool for inference or model proof-testing.","sentences":["For continuous-time Markov chains and open unimolecular chemical reaction networks, we prove that any two stationary currents are linearly related upon perturbations of a single edge's transition rates, arbitrarily far from equilibrium.","We extend the result to non-stationary currents in the frequency domain, provide and discuss an explicit expression for the current-current susceptibility in terms of the network topology, and discuss possible generalizations.","In practical scenarios, the mutual linearity relation has predictive power and can be used as a tool for inference or model proof-testing."],"url":"http://arxiv.org/abs/2402.13193v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-20 17:58:43","title":"Force-free identification of minimum-energy pathways and transition states for stochastic electronic structure theories","abstract":"Stochastic electronic structure theories, e.g., Quantum Monte Carlo methods, enable highly accurate total energy calculations which in principle can be used to construct highly accurate potential energy surfaces. However, their stochastic nature poses a challenge to the computation and use of forces and Hessians, which are typically required in algorithms for minimum-energy pathway (MEP) and transition state (TS) identification, such as the nudged-elastic band (NEB) algorithm and its climbing image formulation. Here, we present strategies that utilize the surrogate Hessian line-search method - previously developed for QMC structural optimization - to efficiently identify MEP and TS structures without requiring force calculations at the level of the stochastic electronic structure theory. By modifying the surrogate Hessian algorithm to operate in path-orthogonal subspaces and on saddle points, we show that it is possible to identify MEPs and TSs using a force-free QMC approach. We demonstrate these strategies via two examples, the inversion of the ammonia molecule and an SN2 reaction. We validate our results using Density Functional Theory- and coupled cluster-based NEB calculations. We then introduce a hybrid DFT-QMC approach to compute thermodynamic and kinetic quantities - free energy differences, rate constants, and equilibrium constants - that incorporates stochastically-optimized structures and their energies, and show that this scheme improves upon DFT accuracy. Our methods generalize straightforwardly to other systems and other high-accuracy theories that similarly face challenges computing energy gradients, paving the way for highly accurate PES mapping, transition state determination, and thermodynamic and kinetic calculations, at significantly reduced computational expense.","sentences":["Stochastic electronic structure theories, e.g., Quantum Monte Carlo methods, enable highly accurate total energy calculations which in principle can be used to construct highly accurate potential energy surfaces.","However, their stochastic nature poses a challenge to the computation and use of forces and Hessians, which are typically required in algorithms for minimum-energy pathway (MEP) and transition state (TS) identification, such as the nudged-elastic band (NEB) algorithm and its climbing image formulation.","Here, we present strategies that utilize the surrogate Hessian line-search method - previously developed for QMC structural optimization - to efficiently identify MEP and TS structures without requiring force calculations at the level of the stochastic electronic structure theory.","By modifying the surrogate Hessian algorithm to operate in path-orthogonal subspaces and on saddle points, we show that it is possible to identify MEPs and TSs using a force-free QMC approach.","We demonstrate these strategies via two examples, the inversion of the ammonia molecule and an SN2 reaction.","We validate our results using Density Functional Theory- and coupled cluster-based NEB calculations.","We then introduce a hybrid DFT-QMC approach to compute thermodynamic and kinetic quantities - free energy differences, rate constants, and equilibrium constants - that incorporates stochastically-optimized structures and their energies, and show that this scheme improves upon DFT accuracy.","Our methods generalize straightforwardly to other systems and other high-accuracy theories that similarly face challenges computing energy gradients, paving the way for highly accurate PES mapping, transition state determination, and thermodynamic and kinetic calculations, at significantly reduced computational expense."],"url":"http://arxiv.org/abs/2402.13189v1","category":"physics.chem-ph"}
{"created":"2024-02-20 17:56:24","title":"Question Calibration and Multi-Hop Modeling for Temporal Question Answering","abstract":"Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks. In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention. Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations. (I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities. (II) They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering. To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach. Specifically, We first calibrate the question representation by fusing the question and the time-constrained concepts in KG. Then, we construct the GNN layer to complete multi-hop message passing. Finally, the question representation is combined with the embedding output by the GNN to generate the final prediction. Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset. Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions dataset's complex questions are absolutely improved by 5.1% and 1.2% compared to the best-performing baseline. Moreover, QC-MHM can generate interpretable and trustworthy predictions.","sentences":["Many models that leverage knowledge graphs (KGs) have recently demonstrated remarkable success in question answering (QA) tasks.","In the real world, many facts contained in KGs are time-constrained thus temporal KGQA has received increasing attention.","Despite the fruitful efforts of previous models in temporal KGQA, they still have several limitations.","(I) They adopt pre-trained language models (PLMs) to obtain question representations, while PLMs tend to focus on entity information and ignore entity transfer caused by temporal constraints, and finally fail to learn specific temporal representations of entities.","(II)","They neither emphasize the graph structure between entities nor explicitly model the multi-hop relationship in the graph, which will make it difficult to solve complex multi-hop question answering.","To alleviate this problem, we propose a novel Question Calibration and Multi-Hop Modeling (QC-MHM) approach.","Specifically, We first calibrate the question representation by fusing the question and the time-constrained concepts in KG.","Then, we construct the GNN layer to complete multi-hop message passing.","Finally, the question representation is combined with the embedding output by the GNN to generate the final prediction.","Empirical results verify that the proposed model achieves better performance than the state-of-the-art models in the benchmark dataset.","Notably, the Hits@1 and Hits@10 results of QC-MHM on the CronQuestions dataset's complex questions are absolutely improved by 5.1% and 1.2% compared to the best-performing baseline.","Moreover, QC-MHM can generate interpretable and trustworthy predictions."],"url":"http://arxiv.org/abs/2402.13188v1","category":"cs.CL"}
{"created":"2024-02-20 17:52:12","title":"UniEdit: A Unified Tuning-Free Framework for Video Motion and Appearance Editing","abstract":"Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization). However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored. In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework. To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively. The obtained features are then injected into the main editing path via temporal and spatial self-attention layers. Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods. Our code will be publicly available.","sentences":["Recent advances in text-guided video editing have showcased promising results in appearance editing (e.g., stylization).","However, video motion editing in the temporal dimension (e.g., from eating to waving), which distinguishes video editing from image editing, is underexplored.","In this work, we present UniEdit, a tuning-free framework that supports both video motion and appearance editing by harnessing the power of a pre-trained text-to-video generator within an inversion-then-generation framework.","To realize motion editing while preserving source video content, based on the insights that temporal and spatial self-attention layers encode inter-frame and intra-frame dependency respectively, we introduce auxiliary motion-reference and reconstruction branches to produce text-guided motion and source features respectively.","The obtained features are then injected into the main editing path via temporal and spatial self-attention layers.","Extensive experiments demonstrate that UniEdit covers video motion editing and various appearance editing scenarios, and surpasses the state-of-the-art methods.","Our code will be publicly available."],"url":"http://arxiv.org/abs/2402.13185v1","category":"cs.CV"}
{"created":"2024-02-20 17:49:46","title":"What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents","abstract":"In this study, we introduce \"CosmoAgent,\" an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/agiresearch/AlienAgent.","sentences":["In this study, we introduce \"CosmoAgent,\" an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe.","The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations.","Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation.","Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations.","Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles.","This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts.","We have also released the code and datasets to enable further academic investigation into this interesting area of research.","The code is available at https://github.com/agiresearch/AlienAgent."],"url":"http://arxiv.org/abs/2402.13184v1","category":"cs.CL"}
{"created":"2024-02-20 17:49:43","title":"Robust Model Predictive Control for nonlinear discrete-time systems using iterative time-varying constraint tightening","abstract":"Robust Model Predictive Control (MPC) for nonlinear systems is a problem that poses significant challenges as highlighted by the diversity of approaches proposed in the last decades. Often compromises with respect to computational load, conservatism, generality, or implementation complexity have to be made, and finding an approach that provides the right balance is still a challenge to the research community. This work provides a contribution by proposing a novel shrinking-horizon robust MPC formulation for nonlinear discrete-time systems. By explicitly accounting for how disturbances and linearization errors are propagated through the nonlinear dynamics, a constraint tightening-based formulation is obtained, with guarantees of robust constraint satisfaction. The proposed controller relies on iteratively solving a Nonlinear Program (NLP) to simultaneously optimize system operation and the required constraint tightening. Numerical experiments show the effectiveness of the proposed controller with three different choices of NLP solvers as well as significantly improved computational speed, better scalability, and generally reduced conservatism when compared to an existing technique from the literature.","sentences":["Robust Model Predictive Control (MPC) for nonlinear systems is a problem that poses significant challenges as highlighted by the diversity of approaches proposed in the last decades.","Often compromises with respect to computational load, conservatism, generality, or implementation complexity have to be made, and finding an approach that provides the right balance is still a challenge to the research community.","This work provides a contribution by proposing a novel shrinking-horizon robust MPC formulation for nonlinear discrete-time systems.","By explicitly accounting for how disturbances and linearization errors are propagated through the nonlinear dynamics, a constraint tightening-based formulation is obtained, with guarantees of robust constraint satisfaction.","The proposed controller relies on iteratively solving a Nonlinear Program (NLP) to simultaneously optimize system operation and the required constraint tightening.","Numerical experiments show the effectiveness of the proposed controller with three different choices of NLP solvers as well as significantly improved computational speed, better scalability, and generally reduced conservatism when compared to an existing technique from the literature."],"url":"http://arxiv.org/abs/2402.13183v1","category":"eess.SY"}
{"created":"2024-02-20 17:44:06","title":"Benchmarking Retrieval-Augmented Generation for Medicine","abstract":"While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge. Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted. However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes. To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets. Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work. Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level. Our results show that the combination of various medical corpora and retrievers achieves the best performance. In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG. We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine.","sentences":["While large language models (LLMs) have achieved state-of-the-art performance on a wide range of medical question answering (QA) tasks, they still face challenges with hallucinations and outdated knowledge.","Retrieval-augmented generation (RAG) is a promising solution and has been widely adopted.","However, a RAG system can involve multiple flexible components, and there is a lack of best practices regarding the optimal RAG setting for various medical purposes.","To systematically evaluate such systems, we propose the Medical Information Retrieval-Augmented Generation Evaluation (MIRAGE), a first-of-its-kind benchmark including 7,663 questions from five medical QA datasets.","Using MIRAGE, we conducted large-scale experiments with over 1.8 trillion prompt tokens on 41 combinations of different corpora, retrievers, and backbone LLMs through the MedRAG toolkit introduced in this work.","Overall, MedRAG improves the accuracy of six different LLMs by up to 18% over chain-of-thought prompting, elevating the performance of GPT-3.5 and Mixtral to GPT-4-level.","Our results show that the combination of various medical corpora and retrievers achieves the best performance.","In addition, we discovered a log-linear scaling property and the \"lost-in-the-middle\" effects in medical RAG.","We believe our comprehensive evaluations can serve as practical guidelines for implementing RAG systems for medicine."],"url":"http://arxiv.org/abs/2402.13178v1","category":"cs.CL"}
{"created":"2024-02-20 17:40:59","title":"The Ebb and Flow of Brand Loyalty: A 28-Year Bibliometric and Content Analysis","abstract":"Business research is facing the challenge of scattered knowledge, particularly in the realm of brand loyalty (BL). Although literature reviews on BL exist, they predominantly concentrate on the pre-sent state, neglecting future trends. Therefore, a comprehensive review is imperative to ascertain emerging trends in BL This study employs a bibliometric approach, analyzing 1,468 papers from the Scopus database. Various tools including R software, VOS viewer software, and Publish or Perish are utilized. The aim is to portray the knowledge map, explore the publication years, identify the top authors and their co-occurrence, reliable documents, institutions, subjects, research hotspots, and pioneering countries and universities in the study of BL. The qualitative section of this research identifies gaps and emerging trends in BL through Word Cloud charts, word growth analysis, and a review of highly cited articles from the past four years. Results showed that highly cited articles mention topics such as brand love, consumer-brand identification, and social networks and the U.S. had the most productions in this field. Besides, most citations were related to Keller with 1,173 citations. Furthermore, in the qualitative section, social networks and brand experiences were found to be of interest to researchers in the field. Finally, by introducing the antecedents and consequences of BL, the gaps and emerging trends in BL were identified, so as to present the di-rection of future research in this area.","sentences":["Business research is facing the challenge of scattered knowledge, particularly in the realm of brand loyalty (BL).","Although literature reviews on BL exist, they predominantly concentrate on the pre-sent state, neglecting future trends.","Therefore, a comprehensive review is imperative to ascertain emerging trends in BL","This study employs a bibliometric approach, analyzing 1,468 papers from the Scopus database.","Various tools including R software, VOS viewer software, and Publish or Perish are utilized.","The aim is to portray the knowledge map, explore the publication years, identify the top authors and their co-occurrence, reliable documents, institutions, subjects, research hotspots, and pioneering countries and universities in the study of BL.","The qualitative section of this research identifies gaps and emerging trends in BL through Word Cloud charts, word growth analysis, and a review of highly cited articles from the past four years.","Results showed that highly cited articles mention topics such as brand love, consumer-brand identification, and social networks and the U.S. had the most productions in this field.","Besides, most citations were related to Keller with 1,173 citations.","Furthermore, in the qualitative section, social networks and brand experiences were found to be of interest to researchers in the field.","Finally, by introducing the antecedents and consequences of BL, the gaps and emerging trends in BL were identified, so as to present the di-rection of future research in this area."],"url":"http://arxiv.org/abs/2402.13177v1","category":"econ.GN"}
{"created":"2024-02-20 17:38:37","title":"$h$-Wasserstein barycenters","abstract":"We generalize the notion and theory of Wasserstein barycenters introduced by Agueh and Carlier (2011) from the quadratic cost to general smooth strictly convex costs $h$ with non-degenerate Hessian. We show the equivalence between a coupled two-marginal and a multi-marginal formulation and establish that the multi-marginal optimal plan is unique and of Monge form. To establish the latter result we introduce a new approach which is not based on explicitly solving the optimality system, but instead deriving a quantitative injectivity estimate for the (highly non-injective) map from $N$-point configurations to their $h$-barycenter on the support of an optimal multi-marginal plan.","sentences":["We generalize the notion and theory of Wasserstein barycenters introduced by Agueh and Carlier (2011) from the quadratic cost to general smooth strictly convex costs $h$ with non-degenerate Hessian.","We show the equivalence between a coupled two-marginal and a multi-marginal formulation and establish that the multi-marginal optimal plan is unique and of Monge form.","To establish the latter result we introduce a new approach which is not based on explicitly solving the optimality system, but instead deriving a quantitative injectivity estimate for the (highly non-injective) map from $N$-point configurations to their $h$-barycenter on the support of an optimal multi-marginal plan."],"url":"http://arxiv.org/abs/2402.13176v1","category":"math.AP"}
{"created":"2024-02-20 17:36:56","title":"Geometric structures on the quaternionic unit ball and slice regular M\u00f6bius transformations","abstract":"Building from ideas of hypercomplex analysis on the quaternionic unit ball, we introduce Hermitian, Riemannian and K\\\"ahler-like structures on the latter. These are built from the so-called regular M\\\"obius transformations. Such geometric structures are shown to be natural generalizations of those from the complex setup. Our structures can be considered as more natural, from the analytic viewpoint, than the usual quaternionic hyperbolic geometry. Furthermore, our constructions provide solutions to problems not achieved by hyper-K\\\"ahler and quaternion-K\\\"ahler geometries when applied to the quaternionic unit ball.","sentences":["Building from ideas of hypercomplex analysis on the quaternionic unit ball, we introduce Hermitian, Riemannian and K\\\"ahler-like structures on the latter.","These are built from the so-called regular M\\\"obius transformations.","Such geometric structures are shown to be natural generalizations of those from the complex setup.","Our structures can be considered as more natural, from the analytic viewpoint, than the usual quaternionic hyperbolic geometry.","Furthermore, our constructions provide solutions to problems not achieved by hyper-K\\\"ahler and quaternion-K\\\"ahler geometries when applied to the quaternionic unit ball."],"url":"http://arxiv.org/abs/2402.13175v1","category":"math.CV"}
{"created":"2024-02-20 17:33:40","title":"3D Kinematics Estimation from Video with a Biomechanical Model and Synthetic Training Data","abstract":"Accurate 3D kinematics estimation of human body is crucial in various applications for human health and mobility, such as rehabilitation, injury prevention, and diagnosis, as it helps to understand the biomechanical loading experienced during movement. Conventional marker-based motion capture is expensive in terms of financial investment, time, and the expertise required. Moreover, due to the scarcity of datasets with accurate annotations, existing markerless motion capture methods suffer from challenges including unreliable 2D keypoint detection, limited anatomic accuracy, and low generalization capability. In this work, we propose a novel biomechanics-aware network that directly outputs 3D kinematics from two input views with consideration of biomechanical prior and spatio-temporal information. To train the model, we create synthetic dataset ODAH with accurate kinematics annotations generated by aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal model. Our extensive experiments demonstrate that the proposed approach, only trained on synthetic data, outperforms previous state-of-the-art methods when evaluated across multiple datasets, revealing a promising direction for enhancing video-based human motion capture.","sentences":["Accurate 3D kinematics estimation of human body is crucial in various applications for human health and mobility, such as rehabilitation, injury prevention, and diagnosis, as it helps to understand the biomechanical loading experienced during movement.","Conventional marker-based motion capture is expensive in terms of financial investment, time, and the expertise required.","Moreover, due to the scarcity of datasets with accurate annotations, existing markerless motion capture methods suffer from challenges including unreliable 2D keypoint detection, limited anatomic accuracy, and low generalization capability.","In this work, we propose a novel biomechanics-aware network that directly outputs 3D kinematics from two input views with consideration of biomechanical prior and spatio-temporal information.","To train the model, we create synthetic dataset ODAH with accurate kinematics annotations generated by aligning the body mesh from the SMPL-X model and a full-body OpenSim skeletal model.","Our extensive experiments demonstrate that the proposed approach, only trained on synthetic data, outperforms previous state-of-the-art methods when evaluated across multiple datasets, revealing a promising direction for enhancing video-based human motion capture."],"url":"http://arxiv.org/abs/2402.13172v1","category":"cs.CV"}
{"created":"2024-02-20 17:27:26","title":"The universal thermodynamic properties of Extremely Compact Objects","abstract":"An extremely compact object (ECO) is defined as a quantum object without horizon, whose radius is just a small distance $s$ outside its Schwarzschild radius. We show that any ECO of mass $M$ in $d+1$ dimensions with $s\\ll (M/m_p)^{2/(d-2)(d+1)}l_p$ must have (at leading order) the same thermodynamic properties -- temperature, entropy and radiation rates -- as the corresponding semiclassical black hole of mass $M$. An essential aspect of the argument involves showing that the Tolman-Oppenheimer-Volkoff equation has no consistent solution in the region just outside the ECO surface, unless this region is filled with radiation at the (appropriately blueshifted) Hawking temperature. In string theory it has been found that black hole microstates are fuzzballs -- objects with no horizon -- which are expected to have a radius that is only a little larger than the horizon radius. Thus the arguments of this paper provide a nice closure to the fuzzball paradigm: the absence of a horizon removes the information paradox, and the thermodynamic properties of the semiclassical hole are nonetheless recovered to an excellent approximation.","sentences":["An extremely compact object (ECO) is defined as a quantum object without horizon, whose radius is just a small distance $s$ outside its Schwarzschild radius.","We show that any ECO of mass $M$ in $d+1$ dimensions with $s\\ll (M/m_p)^{2/(d-2)(d+1)}l_p$ must have (at leading order) the same thermodynamic properties -- temperature, entropy and radiation rates -- as the corresponding semiclassical black hole of mass $M$. An essential aspect of the argument involves showing that the Tolman-Oppenheimer-Volkoff equation has no consistent solution in the region just outside the ECO surface, unless this region is filled with radiation at the (appropriately blueshifted)","Hawking temperature.","In string theory it has been found that black hole microstates are fuzzballs -- objects with no horizon -- which are expected to have a radius that is only a little larger than the horizon radius.","Thus the arguments of this paper provide a nice closure to the fuzzball paradigm: the absence of a horizon removes the information paradox, and the thermodynamic properties of the semiclassical hole are nonetheless recovered to an excellent approximation."],"url":"http://arxiv.org/abs/2402.13166v1","category":"hep-th"}
{"created":"2024-02-20 17:24:44","title":"Assessing Correlated Truncation Errors in Modern Nucleon-Nucleon Potentials","abstract":"We test the BUQEYE model of correlated effective field theory (EFT) truncation errors on Reinert, Krebs, and Epelbaum's semi-local momentum-space implementation of the chiral EFT ($\\chi$EFT) expansion of the nucleon-nucleon (NN) potential. This Bayesian model hypothesizes that dimensionless coefficient functions extracted from the order-by-order corrections to NN observables can be treated as draws from a Gaussian process (GP). We combine a variety of graphical and statistical diagnostics to assess when predicted observables have a $\\chi$EFT convergence pattern consistent with the hypothesized GP statistical model. Our conclusions are: First, the BUQEYE model is generally applicable to the potential investigated here, which enables statistically principled estimates of the impact of higher EFT orders on observables. Second, parameters defining the extracted coefficients such as the expansion parameter $Q$ must be well chosen for the coefficients to exhibit a regular convergence pattern -- a property we exploit to obtain posterior distributions for such quantities. Third, the assumption of GP stationarity across lab energy and scattering angle is not generally met; this necessitates adjustments in future work. We provide a workflow and interpretive guide for our analysis framework, and show what can be inferred about probability distributions for $Q$, the EFT breakdown scale $\\Lambda_b$, the scale associated with soft physics in the $\\chi$EFT potential $m_{\\rm eff}$, and the GP hyperparameters. All our results can be reproduced using a publicly available Jupyter notebook, which can be straightforwardly modified to analyze other $\\chi$EFT NN potentials.","sentences":["We test the BUQEYE model of correlated effective field theory (EFT) truncation errors on Reinert, Krebs, and Epelbaum's semi-local momentum-space implementation of the chiral EFT ($\\chi$EFT) expansion of the nucleon-nucleon (NN) potential.","This Bayesian model hypothesizes that dimensionless coefficient functions extracted from the order-by-order corrections to NN observables can be treated as draws from a Gaussian process (GP).","We combine a variety of graphical and statistical diagnostics to assess when predicted observables have a $\\chi$EFT convergence pattern consistent with the hypothesized GP statistical model.","Our conclusions are: First, the BUQEYE model is generally applicable to the potential investigated here, which enables statistically principled estimates of the impact of higher EFT orders on observables.","Second, parameters defining the extracted coefficients such as the expansion parameter $Q$ must be well chosen for the coefficients to exhibit a regular convergence pattern -- a property we exploit to obtain posterior distributions for such quantities.","Third, the assumption of GP stationarity across lab energy and scattering angle is not generally met; this necessitates adjustments in future work.","We provide a workflow and interpretive guide for our analysis framework, and show what can be inferred about probability distributions for $Q$, the EFT breakdown scale $\\Lambda_b$, the scale associated with soft physics in the $\\chi$EFT potential $m_{\\rm eff}$, and the GP hyperparameters.","All our results can be reproduced using a publicly available Jupyter notebook, which can be straightforwardly modified to analyze other $\\chi$EFT NN potentials."],"url":"http://arxiv.org/abs/2402.13165v1","category":"nucl-th"}
{"created":"2024-02-20 17:24:24","title":"Asymptotic quantization on Riemannian manifolds via covering growth estimates","abstract":"The quantization problem looks for best approximations of a probability measure on a given metric space by finitely many points, where the approximation error is measured with respect to the Wasserstein distance. On particular smooth domains, such as $\\mathbb{R}^d$ or complete Riemannian manifolds, the quantization error is known to decay polynomially as the number of points is taken to infinity, provided the measure satisfies an integral condition which controls the amount of mass outside compact sets. On Riemannian manifolds, the existing integral condition involves a quantity measuring the growth of the exponential map, for which the only available estimates are in terms of lower bounds on sectional curvature.   In this paper, we provide a more general integral condition for the asymptotics of the quantization error on Riemannian manifolds, given in terms of the growth of the covering numbers of spheres, which is purely metric in nature and concerns only the large-scale growth of the manifold. We further estimate the covering growth of manifolds in two particular cases, namely lower bounds on the Ricci curvature and geometric group actions by a discrete group of isometries. These estimates can themselves generalize beyond manifolds, and hint at a future treatment of asymptotic quantization also on non-smooth metric measure spaces.","sentences":["The quantization problem looks for best approximations of a probability measure on a given metric space by finitely many points, where the approximation error is measured with respect to the Wasserstein distance.","On particular smooth domains, such as $\\mathbb{R}^d$ or complete Riemannian manifolds, the quantization error is known to decay polynomially as the number of points is taken to infinity, provided the measure satisfies an integral condition which controls the amount of mass outside compact sets.","On Riemannian manifolds, the existing integral condition involves a quantity measuring the growth of the exponential map, for which the only available estimates are in terms of lower bounds on sectional curvature.   ","In this paper, we provide a more general integral condition for the asymptotics of the quantization error on Riemannian manifolds, given in terms of the growth of the covering numbers of spheres, which is purely metric in nature and concerns only the large-scale growth of the manifold.","We further estimate the covering growth of manifolds in two particular cases, namely lower bounds on the Ricci curvature and geometric group actions by a discrete group of isometries.","These estimates can themselves generalize beyond manifolds, and hint at a future treatment of asymptotic quantization also on non-smooth metric measure spaces."],"url":"http://arxiv.org/abs/2402.13164v1","category":"math.MG"}
{"created":"2024-02-20 17:21:18","title":"Higher-order evolution inequalities with Hardy potential on the Kor\u00e1nyi ball","abstract":"We consider a higher order in (time) semilinear evolution inequality posed on the Kor\\'{a}nyi ball under an inhomogeneous Dirichlet-type boundary condition. The problem involves an inverse-square potential $\\lambda/|\\xi|_\\mathbb{H}^2$, where $\\lambda \\geq -(Q-2)^2/4$ and a general weight function $V$ depending on the space variable in front of the power nonlinearity. We first establish a general nonexistence result for the considered problem. Next, in the special case $V(\\xi):=|\\xi|_\\mathbb{H}^a$, $a\\in \\mathbb{R}$, we prove the sharpness of our nonexistence result and show that the problem admits three different critical behaviors according to the value of the parameter $\\lambda$.","sentences":["We consider a higher order in (time) semilinear evolution inequality posed on the Kor\\'{a}nyi ball under an inhomogeneous Dirichlet-type boundary condition.","The problem involves an inverse-square potential $\\lambda/|\\xi|_\\mathbb{H}^2$, where $\\lambda \\geq -(Q-2)^2/4$ and a general weight function $V$ depending on the space variable in front of the power nonlinearity.","We first establish a general nonexistence result for the considered problem.","Next, in the special case $V(\\xi):=|\\xi|_\\mathbb{H}^a$, $a\\in \\mathbb{R}$, we prove the sharpness of our nonexistence result and show that the problem admits three different critical behaviors according to the value of the parameter $\\lambda$."],"url":"http://arxiv.org/abs/2402.13158v1","category":"math.AP"}
{"created":"2024-02-20 17:21:06","title":"Interferometry with few photons","abstract":"Optical phase determination is an important and established tool in diverse fields such as astronomy, biology, or quantum optics. There is increasing interest in using a lower number of total photons. However, different noise sources, such as electronic readout noise in the detector, and shot noise, hamper the phase estimation in regimes of very low illumination. Here we report a study on how the quality of phase determination is affected by these two sources of noise. To that end, we experimentally reconstruct different wavefronts by means of a point diffraction interferometer for different mean intensities of illumination, up to $15\\ \\mathrm{phot/px}$. Our interferometer features a Skipper-CCD sensor, which allows us to reduce the readout noise arbitrarily, thus enabling us to separate the effect of these two sources of noise. For two cases of interest: a spatial qudit encoding phase, consisting of d = 6 uniform phase regions, and a more general continuous phase, we see that reducing the readout noise leads to a clear improvement in the quality of reconstruction. This can be explained by a simple noise model that allows us to predict the expected fidelity of reconstruction and shows excellent agreement with the measurements.","sentences":["Optical phase determination is an important and established tool in diverse fields such as astronomy, biology, or quantum optics.","There is increasing interest in using a lower number of total photons.","However, different noise sources, such as electronic readout noise in the detector, and shot noise, hamper the phase estimation in regimes of very low illumination.","Here we report a study on how the quality of phase determination is affected by these two sources of noise.","To that end, we experimentally reconstruct different wavefronts by means of a point diffraction interferometer for different mean intensities of illumination, up to $15\\ \\mathrm{phot/px}$. Our interferometer features a Skipper-CCD sensor, which allows us to reduce the readout noise arbitrarily, thus enabling us to separate the effect of these two sources of noise.","For two cases of interest: a spatial qudit encoding phase, consisting of d = 6 uniform phase regions, and a more general continuous phase, we see that reducing the readout noise leads to a clear improvement in the quality of reconstruction.","This can be explained by a simple noise model that allows us to predict the expected fidelity of reconstruction and shows excellent agreement with the measurements."],"url":"http://arxiv.org/abs/2402.13157v1","category":"quant-ph"}
{"created":"2024-02-20 17:13:51","title":"Improved error bounds for approximations of high-frequency wave propagation in nonlinear dispersive media","abstract":"High-frequency wave propagation is often modelled by nonlinear Friedrichs systems where both the differential equation and the initial data contain the inverse of a small parameter $\\varepsilon$, which causes oscillations with wavelengths proportional to $\\varepsilon$ in time and space. A prominent example is the Maxwell--Lorentz system, which is a well-established model for the propagation of light in nonlinear media. In diffractive optics, such problems have to be solved on long time intervals with length proportional to $1/\\varepsilon$. Approximating the solution of such a problem numerically with a standard method is hopeless, because traditional methods require an extremely fine resolution in time and space, which entails unacceptable computational costs. A possible alternative is to replace the original problem by a new system of PDEs which is more suitable for numerical computations but still yields a sufficiently accurate approximation. Such models are often based on the \\emph{slowly varying envelope approximation} or generalizations thereof. Results in the literature state that the error of the slowly varying envelope approximation is of $\\mathcal{O}(\\varepsilon)$. In this work, however, we prove that the error is even proportional to $\\varepsilon^2$, which is a substantial improvement, and which explains the error behavior observed in numerical experiments. For a higher-order generalization of the slowly varying envelope approximation we improve the error bound from $\\mathcal{O}(\\varepsilon^2)$ to $\\mathcal{O}(\\varepsilon^3)$. Both proofs are based on a careful analysis of the nonlinear interaction between oscillatory and non-oscillatory error terms, and on \\textit{a priori} bounds for certain ``parts'' of the approximations which are defined by suitable projections.","sentences":["High-frequency wave propagation is often modelled by nonlinear Friedrichs systems where both the differential equation and the initial data contain the inverse of a small parameter $\\varepsilon$, which causes oscillations with wavelengths proportional to $\\varepsilon$ in time and space.","A prominent example is the Maxwell--Lorentz system, which is a well-established model for the propagation of light in nonlinear media.","In diffractive optics, such problems have to be solved on long time intervals with length proportional to $1/\\varepsilon$. Approximating the solution of such a problem numerically with a standard method is hopeless, because traditional methods require an extremely fine resolution in time and space, which entails unacceptable computational costs.","A possible alternative is to replace the original problem by a new system of PDEs which is more suitable for numerical computations but still yields a sufficiently accurate approximation.","Such models are often based on the \\emph{slowly varying envelope approximation} or generalizations thereof.","Results in the literature state that the error of the slowly varying envelope approximation is of $\\mathcal{O}(\\varepsilon)$. In this work, however, we prove that the error is even proportional to $\\varepsilon^2$, which is a substantial improvement, and which explains the error behavior observed in numerical experiments.","For a higher-order generalization of the slowly varying envelope approximation we improve the error bound from $\\mathcal{O}(\\varepsilon^2)$ to $\\mathcal{O}(\\varepsilon^3)$. Both proofs are based on a careful analysis of the nonlinear interaction between oscillatory and non-oscillatory error terms, and on \\textit{a priori} bounds for certain ``parts'' of the approximations which are defined by suitable projections."],"url":"http://arxiv.org/abs/2402.13155v1","category":"math.AP"}
{"created":"2024-02-20 17:06:47","title":"Almost-Tight Bounds on Preserving Cuts in Classes of Submodular Hypergraphs","abstract":"Recently, a number of variants of the notion of cut-preserving hypergraph sparsification have been studied in the literature. These variants include directed hypergraph sparsification, submodular hypergraph sparsification, general notions of approximation including spectral approximations, and more general notions like sketching that can answer cut queries using more general data structures than just sparsifiers. In this work, we provide reductions between these different variants of hypergraph sparsification and establish new upper and lower bounds on the space complexity of preserving their cuts. At a high level, our results use the same general principle, namely, by showing that cuts in one class of hypergraphs can be simulated by cuts in a simpler class of hypergraphs, we can leverage sparsification results for the simpler class of hypergraphs.","sentences":["Recently, a number of variants of the notion of cut-preserving hypergraph sparsification have been studied in the literature.","These variants include directed hypergraph sparsification, submodular hypergraph sparsification, general notions of approximation including spectral approximations, and more general notions like sketching that can answer cut queries using more general data structures than just sparsifiers.","In this work, we provide reductions between these different variants of hypergraph sparsification and establish new upper and lower bounds on the space complexity of preserving their cuts.","At a high level, our results use the same general principle, namely, by showing that cuts in one class of hypergraphs can be simulated by cuts in a simpler class of hypergraphs, we can leverage sparsification results for the simpler class of hypergraphs."],"url":"http://arxiv.org/abs/2402.13151v1","category":"cs.DS"}
{"created":"2024-02-20 17:05:41","title":"On the metric property of quantum Wasserstein divergences","abstract":"Quantum Wasserstein divergences are modified versions of quantum Wasserstein distances defined by channels, and they are conjectured to be genuine metrics on quantum state spaces by De Palma and Trevisan. We prove triangle inequality for quantum Wasserstein divergences for any finite-dimensional quantum system and any quadratic cost operator under the assumption that a particular state involved is pure. We also provide strong numerical evidence suggesting that the triangle inequality holds in general, for an arbitrary choice of states.","sentences":["Quantum Wasserstein divergences are modified versions of quantum Wasserstein distances defined by channels, and they are conjectured to be genuine metrics on quantum state spaces by De Palma and Trevisan.","We prove triangle inequality for quantum Wasserstein divergences for any finite-dimensional quantum system and any quadratic cost operator under the assumption that a particular state involved is pure.","We also provide strong numerical evidence suggesting that the triangle inequality holds in general, for an arbitrary choice of states."],"url":"http://arxiv.org/abs/2402.13150v1","category":"math-ph"}
{"created":"2024-02-20 17:04:06","title":"Defending Jailbreak Prompts via In-Context Adversarial Game","abstract":"Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism.","sentences":["Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications.","However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist.","Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning.","ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks.","Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents.","This continuous improvement process strengthens defenses against newly generated jailbreak prompts.","Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios.","Moreover, ICAG demonstrates remarkable transferability to other LLMs, indicating its potential as a versatile defense mechanism."],"url":"http://arxiv.org/abs/2402.13148v1","category":"cs.LG"}
{"created":"2024-02-20 17:02:48","title":"SubIQ: Inverse Soft-Q Learning for Offline Imitation with Suboptimal Demonstrations","abstract":"We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment. One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces. In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels. Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy. Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution. On the other hand, since sub-optimal sets are much larger, there is a high chance that the imitation policy is trained towards sub-optimal policies. In this paper, to address these issues, we propose a new approach based on inverse soft-Q learning, where a regularization term is added to the training objective, with the aim of aligning the learned rewards with a pre-assigned reward function that allocates higher weights to state-action pairs from expert demonstrations, and lower weights to those from lower expertise levels. On standard benchmarks, our inverse soft-Q learning significantly outperforms other offline IL baselines by a large margin.","sentences":["We consider offline imitation learning (IL), which aims to mimic the expert's behavior from its demonstration without further interaction with the environment.","One of the main challenges in offline IL is dealing with the limited support of expert demonstrations that cover only a small fraction of the state-action spaces.","In this work, we consider offline IL, where expert demonstrations are limited but complemented by a larger set of sub-optimal demonstrations of lower expertise levels.","Most of the existing offline IL methods developed for this setting are based on behavior cloning or distribution matching, where the aim is to match the occupancy distribution of the imitation policy with that of the expert policy.","Such an approach often suffers from over-fitting, as expert demonstrations are limited to accurately represent any occupancy distribution.","On the other hand, since sub-optimal sets are much larger, there is a high chance that the imitation policy is trained towards sub-optimal policies.","In this paper, to address these issues, we propose a new approach based on inverse soft-Q learning, where a regularization term is added to the training objective, with the aim of aligning the learned rewards with a pre-assigned reward function that allocates higher weights to state-action pairs from expert demonstrations, and lower weights to those from lower expertise levels.","On standard benchmarks, our inverse soft-Q learning significantly outperforms other offline IL baselines by a large margin."],"url":"http://arxiv.org/abs/2402.13147v1","category":"cs.LG"}
{"created":"2024-02-20 17:00:59","title":"OLViT: Multi-Modal State Tracking via Attention-Based Embeddings for Video-Grounded Dialog","abstract":"We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker. Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns. OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns. In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds. As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks. Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets.","sentences":["We present the Object Language Video Transformer (OLViT) - a novel model for video dialog operating over a multi-modal attention-based dialog state tracker.","Existing video dialog models struggle with questions requiring both spatial and temporal localization within videos, long-term temporal reasoning, and accurate object tracking across multiple dialog turns.","OLViT addresses these challenges by maintaining a global dialog state based on the output of an Object State Tracker (OST) and a Language State Tracker (LST): while the OST attends to the most important objects within the video, the LST keeps track of the most important linguistic co-references to previous dialog turns.","In stark contrast to previous works, our approach is generic by nature and is therefore capable of learning continuous multi-modal dialog state representations of the most relevant objects and rounds.","As a result, they can be seamlessly integrated into Large Language Models (LLMs) and offer high flexibility in dealing with different datasets and tasks.","Evaluations on the challenging DVD (response classification) and SIMMC 2.1 (response generation) datasets show that OLViT achieves new state-of-the-art performance across both datasets."],"url":"http://arxiv.org/abs/2402.13146v1","category":"cs.CV"}
{"created":"2024-02-20 17:00:41","title":"CMDAG: A Chinese Metaphor Dataset with Annotated Grounds as CoT for Boosting Metaphor Generation","abstract":"Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication. This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc. To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines. These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles. Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles. By integrating \"ground\" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition. We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus. These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research. The code is available in the https://anonymous.4open.science/r/Chinese_Metaphor_Explanation-63F2.","sentences":["Metaphor is a prominent linguistic device in human language and literature, as they add color, imagery, and emphasis to enhance effective communication.","This paper introduces a large-scale high quality annotated Chinese Metaphor Corpus, which comprises around 28K sentences drawn from a diverse range of Chinese literary sources, such as poems, prose, song lyrics, etc.","To ensure the accuracy and consistency of our annotations, we introduce a comprehensive set of guidelines.","These guidelines address the facets of metaphor annotation, including identifying tenors, vehicles, and grounds to handling the complexities of similes, personifications, juxtapositions, and hyperboles.","Breaking tradition, our approach to metaphor generation emphasizes grounds and their distinct features rather than the conventional combination of tenors and vehicles.","By integrating \"ground\" as a CoT (Chain of Thoughts) input, we are able to generate metaphors that resonate more with real-world intuition.","We test generative models such as Belle, Baichuan, and Chinese-alpaca-33B using our annotated corpus.","These models are able to generate creative and fluent metaphor sentences more frequently induced by selected samples from our dataset, demonstrating the value of our corpus for Chinese metaphor research.","The code is available in the https://anonymous.4open.science/r/Chinese_Metaphor_Explanation-63F2."],"url":"http://arxiv.org/abs/2402.13145v1","category":"cs.CL"}
{"created":"2024-02-20 16:59:03","title":"Neural Network Diffusion","abstract":"Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results encourage more exploration on the versatile use of diffusion models.","sentences":["Diffusion models have achieved remarkable success in image and video generation.","In this work, we demonstrate that diffusion models can also \\textit{generate high-performing neural network parameters}.","Our approach is simple, utilizing an autoencoder and a standard latent diffusion model.","The autoencoder extracts latent representations of a subset of the trained network parameters.","A diffusion model is then trained to synthesize these latent parameter representations from random noise.","It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters.","Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost.","Notably, we empirically find that the generated models perform differently with the trained networks.","Our results encourage more exploration on the versatile use of diffusion models."],"url":"http://arxiv.org/abs/2402.13144v1","category":"cs.LG"}
{"created":"2024-02-20 16:56:20","title":"Pr\u00fcfer modules over wild hereditary algebras","abstract":"Let $A$ be a hereditary finite dimensional algebra over an algebraically closed field $k$. A brick is defined as a finitely generated module with $k$ as endomorphism ring. Two non-isomorphic bricks $X,Y$ are said to be orthogonal if $Hom(X,Y)=Hom(Y,X)=0$. In this paper we show that a class $\\mathcal{X}$ of pairwise orthogonal bricks allows to construct Pr\\\"ufer modules. We consider the category $Filt(\\mathcal{X})$ of modules with a filtration in $\\mathcal{X}$ and show that $Filt(\\mathcal{X})$ has enough injective objects. We can construct them by an iteration of the Bongarz construction for an universal short exact sequence. We call the infinite dimensional, indecomposable injective objects in $Filt(\\mathcal{X})$ Pr\\\"ufer modules and show that they share many properties with the Pr\\\"ufer modules over tame hereditary algebras as defined by C. M. Ringel [11]. The results of this paper can also be applied to tame hereditary algebras. The construction gives a strict filtration of the generic module $Q$. We give an alternative proof for the classification of torsion-free divisible modules to show how useful this filtration is.","sentences":["Let $A$ be a hereditary finite dimensional algebra over an algebraically closed field $k$.","A brick is defined as a finitely generated module with $k$ as endomorphism ring.","Two non-isomorphic bricks $X,Y$ are said to be orthogonal if $Hom(X,Y)=Hom(Y,X)=0$.","In this paper we show that a class $\\mathcal{X}$ of pairwise orthogonal bricks allows to construct Pr\\\"ufer modules.","We consider the category $Filt(\\mathcal{X})$ of modules with a filtration in $\\mathcal{X}$ and show that $Filt(\\mathcal{X})$ has enough injective objects.","We can construct them by an iteration of the Bongarz construction for an universal short exact sequence.","We call the infinite dimensional, indecomposable injective objects in $Filt(\\mathcal{X})$ Pr\\\"ufer modules and show that they share many properties with the Pr\\\"ufer modules over tame hereditary algebras as defined by C. M. Ringel [11].","The results of this paper can also be applied to tame hereditary algebras.","The construction gives a strict filtration of the generic module $Q$. We give an alternative proof for the classification of torsion-free divisible modules to show how useful this filtration is."],"url":"http://arxiv.org/abs/2402.13142v1","category":"math.RT"}
{"created":"2024-02-20 16:53:55","title":"Transcendence of generalized Euler-Kronecker constants","abstract":"We introduce some generalizations of the Euler-Kronecker constant of a number field and study their arithmetic nature.","sentences":["We introduce some generalizations of the Euler-Kronecker constant of a number field and study their arithmetic nature."],"url":"http://arxiv.org/abs/2402.13138v1","category":"math.NT"}
{"created":"2024-02-20 16:46:06","title":"Electric Field Evaluation of Reconfigurable Intelligent Surface in Wireless Networks","abstract":"Reconfigurable intelligent surface (RIS) used as infrastructure in wireless networks has been a trend, thanks to its low cost and high flexibility. Working in many ways including reflective mirrors and phase-shifted surfaces, RIS is able to enhance the coverage in communications and provide more degrees of freedom for sensing. However, the key issue lies in how to place RIS in accordance with the regulations for electromagnetic field (EMF) exposure, which requires refined evaluations. In this paper, we first investigate the regulations in terms of E-field. Then, relevant deployment characteristics are evaluated jointly: the minimum distance from the base station (BS) to the RIS, and the minimum height of the RIS are given for a given BS power limit and as function of the number of RIS elements. The ray-tracing simulations verify the correctness of our analysis. Besides, different frequency ranges (FRs) and radiation patterns of RIS elements are investigated. The results show that the EMF exposure risk is negligible when RIS works in the reflective-only (RO) mode. However, when it works in the beamforming (BO) mode, its placement should be well specified based on our analytical framework to comply with the regulations of E-field limit in general public scenarios. Finally, we provide an E-field measurement methodology and low-cost solutions in terms of general wireless networks and 5G standalone networks, which pave the way for real-world evaluation in future work.","sentences":["Reconfigurable intelligent surface (RIS) used as infrastructure in wireless networks has been a trend, thanks to its low cost and high flexibility.","Working in many ways including reflective mirrors and phase-shifted surfaces, RIS is able to enhance the coverage in communications and provide more degrees of freedom for sensing.","However, the key issue lies in how to place RIS in accordance with the regulations for electromagnetic field (EMF) exposure, which requires refined evaluations.","In this paper, we first investigate the regulations in terms of E-field.","Then, relevant deployment characteristics are evaluated jointly: the minimum distance from the base station (BS) to the RIS, and the minimum height of the RIS are given for a given BS power limit and as function of the number of RIS elements.","The ray-tracing simulations verify the correctness of our analysis.","Besides, different frequency ranges (FRs) and radiation patterns of RIS elements are investigated.","The results show that the EMF exposure risk is negligible when RIS works in the reflective-only (RO) mode.","However, when it works in the beamforming (BO) mode, its placement should be well specified based on our analytical framework to comply with the regulations of E-field limit in general public scenarios.","Finally, we provide an E-field measurement methodology and low-cost solutions in terms of general wireless networks and 5G standalone networks, which pave the way for real-world evaluation in future work."],"url":"http://arxiv.org/abs/2402.13132v1","category":"eess.SY"}
{"created":"2024-02-20 16:43:20","title":"Are ELECTRA's Sentence Embeddings Beyond Repair? The Case of Semantic Textual Similarity","abstract":"While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback. In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings. The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS). We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers. We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method. TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset. We extend our analysis to various model sizes and languages. Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, using significantly fewer parameters and a substantially smaller embedding size. Finally, we observe further boosts by combining TMFT with a word similarity task or domain adaptive pre-training.","sentences":["While BERT produces high-quality sentence embeddings, its pre-training computational cost is a significant drawback.","In contrast, ELECTRA delivers a cost-effective pre-training objective and downstream task performance improvements, but not as performant sentence embeddings.","The community tacitly stopped utilizing ELECTRA's sentence embeddings for semantic textual similarity (STS).","We notice a significant drop in performance when using the ELECTRA discriminator's last layer in comparison to earlier layers.","We explore this drop and devise a way to repair ELECTRA's embeddings, proposing a novel truncated model fine-tuning (TMFT) method.","TMFT improves the Spearman correlation coefficient by over 8 points while increasing parameter efficiency on the STS benchmark dataset.","We extend our analysis to various model sizes and languages.","Further, we discover the surprising efficacy of ELECTRA's generator model, which performs on par with BERT, using significantly fewer parameters and a substantially smaller embedding size.","Finally, we observe further boosts by combining TMFT with a word similarity task or domain adaptive pre-training."],"url":"http://arxiv.org/abs/2402.13130v1","category":"cs.CL"}
{"created":"2024-02-20 16:40:47","title":"Non-local time evolution equation with singular integral and its application to traffic flow model","abstract":"We consider an integro-differential equation model for traffic flow which is an extension of the Burgers equation model. To discuss the model, we first examine general settings for integrable integro-differential equations and find that they are obtained through a simple residue formula from integrable eqations in a complex domain. As demonstration of the efficiency of this approach, we list several integrable equations including a difference equation with double singular integral and an equation with elliptic singular integral. Then, we discuss the traffic model with singular integral and show that the model exhibits interaction between free flow region and congested region depending on the parameter of non-locality.","sentences":["We consider an integro-differential equation model for traffic flow which is an extension of the Burgers equation model.","To discuss the model, we first examine general settings for integrable integro-differential equations and find that they are obtained through a simple residue formula from integrable eqations in a complex domain.","As demonstration of the efficiency of this approach, we list several integrable equations including a difference equation with double singular integral and an equation with elliptic singular integral.","Then, we discuss the traffic model with singular integral and show that the model exhibits interaction between free flow region and congested region depending on the parameter of non-locality."],"url":"http://arxiv.org/abs/2402.13128v1","category":"nlin.SI"}
{"created":"2024-02-20 16:39:23","title":"VGMShield: Mitigating Misuse of Video Generative Models","abstract":"With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \\textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \\textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy.   Furthermore, anticipating future generative model improvements, we propose a {\\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal. Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models.","sentences":["With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires.","Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   ","In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation.","We start from \\textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \\textit{tracing} problem, which maps a fake video back to a model that generates it.","Towards these, we propose to leverage pre-trained models that focus on {\\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos.","Through experiments on seven state-of-the-art open-source models, we demonstrate that current models still cannot perfectly handle spatial-temporal relationships, and thus, we can accomplish detection and tracing with nearly perfect accuracy.   ","Furthermore, anticipating future generative model improvements, we propose a {\\it prevention} method that adds invisible perturbations to images to make the generated videos look unreal.","Together with fake video detection and tracing, our multi-faceted set of solutions can effectively mitigate misuse of video generative models."],"url":"http://arxiv.org/abs/2402.13126v1","category":"cs.CR"}
{"created":"2024-02-20 16:38:33","title":"TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning","abstract":"Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge. However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage. Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process. We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions. We also conduct more analysis to show the robustness and reliability of TreeEval. Our code can be accessed via the provided https://github.com/Ashura5/TreeEval.","sentences":["Recently, numerous new benchmarks have been established to evaluate the performance of large language models (LLMs) via either computing a holistic score or employing another LLM as a judge.","However, these approaches suffer from data leakage due to the open access of the benchmark and inflexible evaluation process.","To address this issue, we introduce $\\textbf{TreeEval}$, a benchmark-free evaluation method for LLMs that let a high-performance LLM host an irreproducible evaluation session and essentially avoids the data leakage.","Moreover, this LLM performs as an examiner to raise up a series of questions under a topic with a tree planing strategy, which considers the current evaluation status to decide the next question generation and ensures the completeness and efficiency of the evaluation process.","We evaluate $6$ models of different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately achieved the highest correlation coefficient with AlpacaEval2.0 using only around $45$ questions.","We also conduct more analysis to show the robustness and reliability of TreeEval.","Our code can be accessed via the provided https://github.com/Ashura5/TreeEval."],"url":"http://arxiv.org/abs/2402.13125v1","category":"cs.CL"}
{"created":"2024-02-20 16:36:15","title":"Exploring AI-assisted Ideation and Prototyping for Choreography","abstract":"Choreography creation is a multimodal endeavor, demanding cognitive abilities to develop creative ideas and technical expertise to convert choreographic ideas into physical dance movements. Previous endeavors have sought to reduce the complexities in the choreography creation process in both dimensions. Among them, non-AI-based systems have focused on reinforcing cognitive activities by helping analyze and understand dance movements and augmenting physical capabilities by enhancing body expressivity. On the other hand, AI-based methods have helped the creation of novel choreographic materials with generative AI algorithms. The choreography creation process is constrained by time and requires a rich set of resources to stimulate novel ideas, but the need for iterative prototyping and reduced physical dependence have not been adequately addressed by prior research. Recognizing these challenges and the research gap, we present an innovative AI-based choreography-support system. Our goal is to facilitate rapid ideation by utilizing a generative AI model that can produce diverse and novel dance sequences. The system is designed to support iterative digital dance prototyping through an interactive web-based user interface that enables the editing and modification of generated motion. We evaluated our system by inviting six choreographers to analyze its limitations and benefits and present the evaluation results along with potential directions for future work.","sentences":["Choreography creation is a multimodal endeavor, demanding cognitive abilities to develop creative ideas and technical expertise to convert choreographic ideas into physical dance movements.","Previous endeavors have sought to reduce the complexities in the choreography creation process in both dimensions.","Among them, non-AI-based systems have focused on reinforcing cognitive activities by helping analyze and understand dance movements and augmenting physical capabilities by enhancing body expressivity.","On the other hand, AI-based methods have helped the creation of novel choreographic materials with generative AI algorithms.","The choreography creation process is constrained by time and requires a rich set of resources to stimulate novel ideas, but the need for iterative prototyping and reduced physical dependence have not been adequately addressed by prior research.","Recognizing these challenges and the research gap, we present an innovative AI-based choreography-support system.","Our goal is to facilitate rapid ideation by utilizing a generative AI model that can produce diverse and novel dance sequences.","The system is designed to support iterative digital dance prototyping through an interactive web-based user interface that enables the editing and modification of generated motion.","We evaluated our system by inviting six choreographers to analyze its limitations and benefits and present the evaluation results along with potential directions for future work."],"url":"http://arxiv.org/abs/2402.13123v1","category":"cs.HC"}
{"created":"2024-02-20 16:25:41","title":"Causal and Stable Relativistic Hydrodynamic Fluctuations","abstract":"When two nuclei collide close to the speed of light, a fluid state known as the quark-gluon plasma is formed. Attempts to understand the dynamics of this fluid have generated significant research into dissipative relativistic fluid dynamics. The fluctuation-dissipation theorem implies that any dissipative dynamical system will also experience thermal fluctuations; however, such fluctuations are not typically included in the modeling of the quark-gluon plasma. This work discusses a new method of determining whether a hydrodynamic framework is consistent with thermal fluctuations. We develop a new method for calculating the noise correlator of relativistic hydrodynamic systems and apply it to Israel-Stewart theory in a general hydrodynamic frame.","sentences":["When two nuclei collide close to the speed of light, a fluid state known as the quark-gluon plasma is formed.","Attempts to understand the dynamics of this fluid have generated significant research into dissipative relativistic fluid dynamics.","The fluctuation-dissipation theorem implies that any dissipative dynamical system will also experience thermal fluctuations; however, such fluctuations are not typically included in the modeling of the quark-gluon plasma.","This work discusses a new method of determining whether a hydrodynamic framework is consistent with thermal fluctuations.","We develop a new method for calculating the noise correlator of relativistic hydrodynamic systems and apply it to Israel-Stewart theory in a general hydrodynamic frame."],"url":"http://arxiv.org/abs/2402.13119v1","category":"nucl-th"}
{"created":"2024-02-20 16:17:37","title":"A Survey on Knowledge Distillation of Large Language Models","abstract":"This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral. Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings. Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance. By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts. This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential for more accessible, efficient, and sustainable AI solutions, fostering a more inclusive and equitable landscape in AI advancements. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.","sentences":["This survey presents an in-depth exploration of knowledge distillation (KD) techniques within the realm of Large Language Models (LLMs), spotlighting the pivotal role of KD in transferring sophisticated capabilities from proprietary giants such as GPT-4 to accessible, open-source models like LLaMA and Mistral.","Amidst the evolving AI landscape, this work elucidates the critical disparities between proprietary and open-source LLMs, demonstrating how KD serves as an essential conduit for imbuing the latter with the former's advanced functionalities and nuanced understandings.","Our survey is meticulously structured around three foundational pillars: algorithm, skill, and verticalization -- providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields.","Crucially, the survey navigates the intricate interplay between data augmentation (DA) and KD, illustrating how DA emerges as a powerful paradigm within the KD framework to bolster LLMs' performance.","By leveraging DA to generate context-rich, skill-specific training data, KD transcends traditional boundaries, enabling open-source models to approximate the contextual adeptness, ethical alignment, and deep semantic insights characteristic of their proprietary counterparts.","This work aims to provide an insightful guide for researchers and practitioners, offering a detailed overview of current methodologies in knowledge distillation and proposing future research directions.","By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential for more accessible, efficient, and sustainable AI solutions, fostering a more inclusive and equitable landscape in AI advancements.","An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs."],"url":"http://arxiv.org/abs/2402.13116v1","category":"cs.CL"}
{"created":"2024-02-20 16:17:13","title":"Towards a new model-independent calibration of Gamma-Ray Bursts","abstract":"Current data on baryon acoustic oscillations and Supernovae of Type Ia cover up to $z\\sim2.5$. These low-$z$ observations play a very important role in the determination of cosmological parameters and have been widely used to constrain the $\\Lambda$CDM and models beyond the standard. To extend this investigation to higher redshifts, Gamma-Ray Bursts (GRBs) stand out as one of the most promising observables since can probe the universe up to $z\\sim9.4$. The use of GRB correlations is still a challenge due to the spread in their intrinsic properties. In this work, we propose an innovative and cosmology-independent method of calibration of the so-called 3D Dainotti correlation. We employ state-of-the-art data on Cosmic Chronometers (CCH) at $z\\lesssim2$ and use the Gaussian Processes Bayesian reconstruction tool. To match the CCH redshift range, we select 20 long GRBs in $0.553 \\leq z \\leq 1.96$ from the Platinum sample, which consists of well-defined GRB plateau properties that obey the fundamental plane relation. To ensure the generality of our method, we verify that the choice of priors on the parameters of the Dainotti relation and the modelling of CCH uncertainties and covariance have negligible impact on our results. Moreover, we consider the case in which the redshift evolution of the physical features of the plane is accounted for. We find that the use of CCH allows us to identify a sub-sample of GRBs that adhere even more closely to the fundamental plane relation, with an intrinsic scatter of $\\sigma_{int}=0.20^{+0.03}_{-0.05}$ obtained in this analysis when evolutionary effects are considered. In an epoch in which we strive to reduce uncertainties on the variables of the GRB correlations to tighten constraints on cosmological parameters, we have found a novel model-independent approach to pinpoint a sub-sample that can thus represent a valuable set of standardizable candles.","sentences":["Current data on baryon acoustic oscillations and Supernovae of Type Ia cover up to $z\\sim2.5$. These low-$z$ observations play a very important role in the determination of cosmological parameters and have been widely used to constrain the $\\Lambda$CDM and models beyond the standard.","To extend this investigation to higher redshifts, Gamma-Ray Bursts (GRBs) stand out as one of the most promising observables since can probe the universe up to $z\\sim9.4$. The use of GRB correlations is still a challenge due to the spread in their intrinsic properties.","In this work, we propose an innovative and cosmology-independent method of calibration of the so-called 3D Dainotti correlation.","We employ state-of-the-art data on Cosmic Chronometers (CCH) at $z\\lesssim2$ and use the Gaussian Processes Bayesian reconstruction tool.","To match the CCH redshift range, we select 20 long GRBs in $0.553 \\leq z \\leq 1.96$ from the Platinum sample, which consists of well-defined GRB plateau properties that obey the fundamental plane relation.","To ensure the generality of our method, we verify that the choice of priors on the parameters of the Dainotti relation and the modelling of CCH uncertainties and covariance have negligible impact on our results.","Moreover, we consider the case in which the redshift evolution of the physical features of the plane is accounted for.","We find that the use of CCH allows us to identify a sub-sample of GRBs that adhere even more closely to the fundamental plane relation, with an intrinsic scatter of $\\sigma_{int}=0.20^{+0.03}_{-0.05}$ obtained in this analysis when evolutionary effects are considered.","In an epoch in which we strive to reduce uncertainties on the variables of the GRB correlations to tighten constraints on cosmological parameters, we have found a novel model-independent approach to pinpoint a sub-sample that can thus represent a valuable set of standardizable candles."],"url":"http://arxiv.org/abs/2402.13115v1","category":"astro-ph.CO"}
{"created":"2024-02-20 16:11:59","title":"BuffGraph: Enhancing Class-Imbalanced Node Classification via Buffer Nodes","abstract":"Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs). To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced. However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes. To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation. Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings. Code is available at https://anonymous.4open.science/r/BuffGraph-730A.","sentences":["Class imbalance in graph-structured data, where minor classes are significantly underrepresented, poses a critical challenge for Graph Neural Networks (GNNs).","To address this challenge, existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced.","However, they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes.","To address this, we introduce BuffGraph, which inserts buffer nodes into the graph, modulating the impact of majority classes to improve minor class representation.","Our extensive experiments across diverse real-world datasets empirically demonstrate that BuffGraph outperforms existing baseline methods in class-imbalanced node classification in both natural settings and imbalanced settings.","Code is available at https://anonymous.4open.science/r/BuffGraph-730A."],"url":"http://arxiv.org/abs/2402.13114v1","category":"cs.LG"}
{"created":"2024-02-20 16:06:10","title":"Go Green: Selected Configuration Interaction as a More Sustainable Alternative for High Accuracy","abstract":"Recently, a new distributed implementation of the full configuration interaction (FCI) method has been reported (Gao et al., J. Chem Theory Comput. (2024), 10.1021/acs.jctc.3c01190]. Thanks to a hybrid parallelization scheme, the authors were able to compute the exact energy of propane (\\ce{C3H8}) in the minimal basis STO-3G. This formidable task involves handling an active space of 26 electrons in 23 orbitals or a Hilbert space of \\SI{1.3d12} determinants. This is, by far, the largest FCI calculation reported to date. Here, we illustrate how, from a general point of view, selected configuration interaction (SCI) can achieve microhartree accuracy at a fraction of the computational and memory cost, via a sparse exploration of the FCI space. The present SCI calculations are performed with the \\textit{Configuration Interaction using a Perturbative Selection made Iteratively} (CIPSI) algorithm, as implemented in a determinant-driven way in the \\textsc{quantum package} software. The present study reinforces the common wisdom that among the exponentially large number of determinants in the FCI space, only a tiny fraction of them significantly contribute to the energy. More importantly, it demonstrates the feasibility of achieving comparable accuracy using more reasonable and sustainable computational resources, hence reducing the ever-growing carbon footprint of computational chemistry.","sentences":["Recently, a new distributed implementation of the full configuration interaction (FCI) method has been reported (Gao et al., J. Chem Theory Comput.","(2024), 10.1021/acs.jctc.3c01190].","Thanks to a hybrid parallelization scheme, the authors were able to compute the exact energy of propane (\\ce{C3H8}) in the minimal basis STO-3G.","This formidable task involves handling an active space of 26 electrons in 23 orbitals or a Hilbert space of \\SI{1.3d12} determinants.","This is, by far, the largest FCI calculation reported to date.","Here, we illustrate how, from a general point of view, selected configuration interaction (SCI) can achieve microhartree accuracy at a fraction of the computational and memory cost, via a sparse exploration of the FCI space.","The present SCI calculations are performed with the \\textit{Configuration Interaction using a Perturbative Selection made Iteratively} (CIPSI) algorithm, as implemented in a determinant-driven way in the \\textsc{quantum package} software.","The present study reinforces the common wisdom that among the exponentially large number of determinants in the FCI space, only a tiny fraction of them significantly contribute to the energy.","More importantly, it demonstrates the feasibility of achieving comparable accuracy using more reasonable and sustainable computational resources, hence reducing the ever-growing carbon footprint of computational chemistry."],"url":"http://arxiv.org/abs/2402.13111v1","category":"physics.chem-ph"}
{"created":"2024-02-20 16:02:12","title":"CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models","abstract":"The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following. Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories. In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language. CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories. To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances. Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts. This work aims to uncover the current limitations of LLMs in handling Chinese tasks, pushing towards the development of more culturally informed and linguistically diverse models with the released data and benchmark (https://yizhilll.github.io/CIF-Bench/).","sentences":["The advancement of large language models (LLMs) has enhanced the ability to generalize across a wide range of unseen natural language processing (NLP) tasks through instruction-following.","Yet, their effectiveness often diminishes in low-resource languages like Chinese, exacerbated by biased evaluations from data leakage, casting doubt on their true generalizability to new linguistic territories.","In response, we introduce the Chinese Instruction-Following Benchmark (CIF-Bench), designed to evaluate the zero-shot generalizability of LLMs to the Chinese language.","CIF-Bench comprises 150 tasks and 15,000 input-output pairs, developed by native speakers to test complex reasoning and Chinese cultural nuances across 20 categories.","To mitigate evaluation bias, we release only half of the dataset publicly, with the remainder kept private, and introduce diversified instructions to minimize score variance, totaling 45,000 data instances.","Our evaluation of 28 selected LLMs reveals a noticeable performance gap, with the best model scoring only 52.9%, highlighting the limitations of LLMs in less familiar language and task contexts.","This work aims to uncover the current limitations of LLMs in handling Chinese tasks, pushing towards the development of more culturally informed and linguistically diverse models with the released data and benchmark (https://yizhilll.github.io/CIF-Bench/)."],"url":"http://arxiv.org/abs/2402.13109v1","category":"cs.CL"}
{"created":"2024-02-20 16:01:39","title":"On Generalization Bounds for Deep Compound Gaussian Neural Networks","abstract":"Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm. Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks. An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs. These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data. In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior. These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems. The generalization error bound is formulated by bounding the Rademacher complexity of the class of compound Gaussian network estimates with Dudley's integral. Under realistic conditions, we show that, at worst, the generalization error scales $\\mathcal{O}(n\\sqrt{\\ln(n)})$ in the signal dimension and $\\mathcal{O}(($Network Size$)^{3/2})$ in network size.","sentences":["Algorithm unfolding or unrolling is the technique of constructing a deep neural network (DNN) from an iterative algorithm.","Unrolled DNNs often provide better interpretability and superior empirical performance over standard DNNs in signal estimation tasks.","An important theoretical question, which has only recently received attention, is the development of generalization error bounds for unrolled DNNs.","These bounds deliver theoretical and practical insights into the performance of a DNN on empirical datasets that are distinct from, but sampled from, the probability density generating the DNN training data.","In this paper, we develop novel generalization error bounds for a class of unrolled DNNs that are informed by a compound Gaussian prior.","These compound Gaussian networks have been shown to outperform comparative standard and unfolded deep neural networks in compressive sensing and tomographic imaging problems.","The generalization error bound is formulated by bounding the Rademacher complexity of the class of compound Gaussian network estimates with Dudley's integral.","Under realistic conditions, we show that, at worst, the generalization error scales $\\mathcal{O}(n\\sqrt{\\ln(n)})$ in the signal dimension and $\\mathcal{O}(($Network Size$)^{3/2})$ in network size."],"url":"http://arxiv.org/abs/2402.13106v1","category":"stat.ML"}
{"created":"2024-02-20 15:54:50","title":"Are boron-nitride nanobelts capable to capture greenhouse gases?","abstract":"Why is the question in the title pertinent? Toxic gases, which are detrimental to both human health and the environment, have been released in greater quantities as a result of industrial development. These gases necessitate capture, immobilization, and measurement. Consequently, the present study investigates the interactions between boron-nitride nanobelt and M\\\"obius-type boron-nitride nanobelt and nine greenhouse gases, namely ammonia, carbon dioxide, carbon monoxide, hydrogen sulfide, methane, methanol, nitric dioxide, nitric oxide, and phosgene. The adsorption energies calculated for the structures with optimized geometry are all negative, suggesting that all gases are adsorbed favorably in both nanobelts. Furthermore, we discovered that the recovery time of the sensors ranges from two hours to a few nanoseconds, and that the nanobelts exhibit distinct responses to each gas. According to electronic and topological investigations, covalent bonds were exclusively formed by nitric oxide; the remaining gases formed non-covalent bonds. Molecular dynamics ultimately demonstrate that the interaction between a single gas molecule and the nanobelt remains consistent across the vast majority of gases, whereas the interaction between 500 gas molecules and the nanobelts functions as an attraction, notwithstanding the impact of volumetric effects characteristic of high volume gases on the interaction. For the completion of each calculation, semiempirical tight-binding methods were implemented utilizing the xTB software. The outcomes of our study generated a favorable response to the inquiry posed in the title.","sentences":["Why is the question in the title pertinent?","Toxic gases, which are detrimental to both human health and the environment, have been released in greater quantities as a result of industrial development.","These gases necessitate capture, immobilization, and measurement.","Consequently, the present study investigates the interactions between boron-nitride nanobelt and M\\\"obius-type boron-nitride nanobelt and nine greenhouse gases, namely ammonia, carbon dioxide, carbon monoxide, hydrogen sulfide, methane, methanol, nitric dioxide, nitric oxide, and phosgene.","The adsorption energies calculated for the structures with optimized geometry are all negative, suggesting that all gases are adsorbed favorably in both nanobelts.","Furthermore, we discovered that the recovery time of the sensors ranges from two hours to a few nanoseconds, and that the nanobelts exhibit distinct responses to each gas.","According to electronic and topological investigations, covalent bonds were exclusively formed by nitric oxide; the remaining gases formed non-covalent bonds.","Molecular dynamics ultimately demonstrate that the interaction between a single gas molecule and the nanobelt remains consistent across the vast majority of gases, whereas the interaction between 500 gas molecules and the nanobelts functions as an attraction, notwithstanding the impact of volumetric effects characteristic of high volume gases on the interaction.","For the completion of each calculation, semiempirical tight-binding methods were implemented utilizing the xTB software.","The outcomes of our study generated a favorable response to the inquiry posed in the title."],"url":"http://arxiv.org/abs/2402.13102v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 15:54:24","title":"A Microstructure-based Graph Neural Network for Accelerating Multiscale Simulations","abstract":"Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale simulations. However, the computational costs stand in the way of the practical application of this approach. The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point. A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models. In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step. Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest. We achieve this for an elasto-plastic material by predicting full-field microscopic strains using a graph neural network (GNN) while retaining the microscopic constitutive material model to obtain the stresses. This hybrid data-physics graph-based approach avoids the high dimensionality originating from predicting full-field responses while allowing non-locality to arise. By training the GNN on a variety of meshes, it learns to generalize to unseen meshes, allowing a single model to be used for a range of microstructures. The embedded microscopic constitutive model in the GNN implicitly tracks history-dependent variables and leads to improved accuracy. We demonstrate for several challenging scenarios that the surrogate can predict complex macroscopic stress-strain paths. As the computation time of our method scales favorably with the number of elements in the microstructure compared to the FE method, our method can significantly accelerate FE2 simulations.","sentences":["Simulating the mechanical response of advanced materials can be done more accurately using concurrent multiscale models than with single-scale simulations.","However, the computational costs stand in the way of the practical application of this approach.","The costs originate from microscale Finite Element (FE) models that must be solved at every macroscopic integration point.","A plethora of surrogate modeling strategies attempt to alleviate this cost by learning to predict macroscopic stresses from macroscopic strains, completely replacing the microscale models.","In this work, we introduce an alternative surrogate modeling strategy that allows for keeping the multiscale nature of the problem, allowing it to be used interchangeably with an FE solver for any time step.","Our surrogate provides all microscopic quantities, which are then homogenized to obtain macroscopic quantities of interest.","We achieve this for an elasto-plastic material by predicting full-field microscopic strains using a graph neural network (GNN) while retaining the microscopic constitutive material model to obtain the stresses.","This hybrid data-physics graph-based approach avoids the high dimensionality originating from predicting full-field responses while allowing non-locality to arise.","By training the GNN on a variety of meshes, it learns to generalize to unseen meshes, allowing a single model to be used for a range of microstructures.","The embedded microscopic constitutive model in the GNN implicitly tracks history-dependent variables and leads to improved accuracy.","We demonstrate for several challenging scenarios that the surrogate can predict complex macroscopic stress-strain paths.","As the computation time of our method scales favorably with the number of elements in the microstructure compared to the FE method, our method can significantly accelerate FE2 simulations."],"url":"http://arxiv.org/abs/2402.13101v1","category":"cs.LG"}
{"created":"2024-02-20 15:47:59","title":"ELAD: Explanation-Guided Large Language Models Active Distillation","abstract":"The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences. Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation. In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance. To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps. Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model's reasoning. Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLM knowledge distillation.","sentences":["The deployment and application of Large Language Models (LLMs) is hindered by their memory inefficiency, computational demands, and the high costs of API inferences.","Traditional distillation methods, which transfer the capabilities of LLMs to smaller models, often fail to determine whether the knowledge has been sufficiently transferred, potentially resulting in high costs or incomplete distillation.","In this paper, we propose an Explanation-Guided LLMs Active Distillation (ELAD) framework that employs an active learning strategy to optimize the balance between annotation costs and model performance.","To improve efficient sample selection, we introduce an explanation-guided sample selection method that identifies samples challenging its reasoning by exploiting uncertainties in explanation steps.","Additionally, we present a customized LLM-annotated explanation revision technique where the teacher model detects and corrects flaws in the student model's reasoning.","Our experiments across various reasoning datasets demonstrate that our framework significantly enhances the efficiency of LLM knowledge distillation."],"url":"http://arxiv.org/abs/2402.13098v1","category":"cs.CL"}
{"created":"2024-02-20 15:36:41","title":"Event-level Knowledge Editing","abstract":"Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated. Existing work edits LLMs at the level of factual knowledge triplets. However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets. In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency. A single event edit leads to updates in multiple entailed knowledge triplets. (2) Completeness. Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends. We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies. We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark. We find that ELKEN poses significant challenges to existing knowledge editing approaches. Our codes and dataset are publicly released to facilitate further research.","sentences":["Knowledge editing aims at updating knowledge of large language models (LLMs) to prevent them from becoming outdated.","Existing work edits LLMs at the level of factual knowledge triplets.","However, natural knowledge updates in the real world come from the occurrences of new events rather than direct changes in factual triplets.","In this paper, we propose a new task setting: event-level knowledge editing, which directly edits new events into LLMs and improves over conventional triplet-level editing on (1) Efficiency.","A single event edit leads to updates in multiple entailed knowledge triplets.","(2) Completeness.","Beyond updating factual knowledge, event-level editing also requires considering the event influences and updating LLMs' knowledge about future trends.","We construct a high-quality event-level editing benchmark ELKEN, consisting of 1,515 event edits, 6,449 questions about factual knowledge, and 10,150 questions about future tendencies.","We systematically evaluate the performance of various knowledge editing methods and LLMs on this benchmark.","We find that ELKEN poses significant challenges to existing knowledge editing approaches.","Our codes and dataset are publicly released to facilitate further research."],"url":"http://arxiv.org/abs/2402.13093v1","category":"cs.CL"}
{"created":"2024-02-20 15:35:40","title":"Contractivity of neural ODEs: an eigenvalue optimization problem","abstract":"We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs. When looking at contractivity properties of a one layer weight-tied neural ODE $\\dot{u}(t)=\\sigma(Au(t)+b)$ (with $u,b \\in {\\mathbb R}^n$, $A$ is a given $n \\times n$ matrix, $\\sigma : {\\mathbb R} \\to {\\mathbb R}^+$ denotes an activation function and for a vector $z \\in {\\mathbb R}^n$, $\\sigma(z) \\in {\\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\\mathrm{diag}}(D) \\in \\sigma'({\\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\\chi\\subseteq \\mathbb [0,\\infty)$ such that the logarithmic norm $\\mu(DA) \\le c$ for all diagonal matrices $D$ with $D_{ii}\\in \\chi$. We propose a two-level nested methodology: an inner level where, for a given $\\chi$, we compute an optimizer $D^\\star(\\chi)$ by a gradient system approach, and an outer level where we tune $\\chi$ so that the value $c$ is reached by $\\mu(D^\\star(\\chi)A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\\dot{u}(t) = \\sigma( A_k(t) \\ldots \\sigma ( A_{1}(t) u(t) + b_{1}(t) ) \\ldots + b_{k}(t) )$ and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the MNIST handwritten digits dataset.","sentences":["We propose a novel methodology to solve a key eigenvalue optimization problem which arises in the contractivity analysis of neural ODEs.","When looking at contractivity properties of a one layer weight-tied neural ODE $\\dot{u}(t)=\\sigma(Au(t)+b)$ (with $u,b \\in {\\mathbb R}^n$, $A$ is a given $n \\times n$ matrix, $\\sigma : {\\mathbb R} \\to {\\mathbb R}^+$ denotes an activation function and for a vector $z \\in {\\mathbb R}^n$, $\\sigma(z) \\in {\\mathbb R}^n$ has to be interpreted entry-wise), we are led to study the logarithmic norm of a set of products of type $D A$, where $D$ is a diagonal matrix such that ${\\mathrm{diag}}(D) \\in \\sigma'({\\mathbb R}^n)$. Specifically, given a real number $c$ (usually $c=0$), the problem consists in finding the largest positive interval $\\chi\\subseteq \\mathbb [0,\\infty)$ such that the logarithmic norm $\\mu(DA) \\le c$ for all diagonal matrices $D$ with $D_{ii}\\in \\chi$. We propose a two-level nested methodology: an inner level where, for a given $\\chi$, we compute an optimizer $D^\\star(\\chi)$ by a gradient system approach, and an outer level where we tune $\\chi$ so that the value $c$ is reached by $\\mu(D^\\star(\\chi)A)$. We extend the proposed two-level approach to the general multilayer, and possibly time-dependent, case $\\dot{u}(t) = \\sigma( A_k(t)","\\ldots \\sigma ( A_{1}(t) u(t) + b_{1}(t) )","\\ldots + b_{k}(t) )","$","and we propose several numerical examples to illustrate its behaviour, including its stabilizing performance on a one-layer neural ODE applied to the classification of the MNIST handwritten digits dataset."],"url":"http://arxiv.org/abs/2402.13092v1","category":"math.NA"}
{"created":"2024-02-20 15:35:02","title":"Gravitational Wave Signal Extraction Against Non-Stationary Instrumental Noises with Deep Neural Network","abstract":"Sapce-borne gravitational wave antennas, such as LISA and LISA-like mission (Taiji and Tianqin), will offer novel perspectives for exploring our Universe while introduce new challenges, especially in data analysis. Aside from the known challenges like high parameter space dimension, superposition of large number of signals and etc., gravitational wave detections in space would be more seriously affected by anomalies or non-stationarities in the science measurements. Considering the three types of foreseeable non-stationarities including data gaps, transients (glitches), and time-varying noise auto-correlations, which may come from routine maintenance or unexpected disturbances during science operations, we developed a deep learning model for accurate signal extractions confronted with such anomalous scenarios. Our model exhibits the same performance as the current state-of-the-art models do for the ideal and anomaly free scenario, while shows remarkable adaptability in extractions of coalescing massive black hole binary signal against all three types of non-stationarities and even their mixtures. This also provide new explorations into the robustness studies of deep learning models for data processing in space-borne gravitational wave missions.","sentences":["Sapce-borne gravitational wave antennas, such as LISA and LISA-like mission (Taiji and Tianqin), will offer novel perspectives for exploring our Universe while introduce new challenges, especially in data analysis.","Aside from the known challenges like high parameter space dimension, superposition of large number of signals and etc., gravitational wave detections in space would be more seriously affected by anomalies or non-stationarities in the science measurements.","Considering the three types of foreseeable non-stationarities including data gaps, transients (glitches), and time-varying noise auto-correlations, which may come from routine maintenance or unexpected disturbances during science operations, we developed a deep learning model for accurate signal extractions confronted with such anomalous scenarios.","Our model exhibits the same performance as the current state-of-the-art models do for the ideal and anomaly free scenario, while shows remarkable adaptability in extractions of coalescing massive black hole binary signal against all three types of non-stationarities and even their mixtures.","This also provide new explorations into the robustness studies of deep learning models for data processing in space-borne gravitational wave missions."],"url":"http://arxiv.org/abs/2402.13091v1","category":"gr-qc"}
{"created":"2024-02-20 15:31:44","title":"Towards an empirical understanding of MoE design choices","abstract":"In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels. We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential. Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing.","sentences":["In this study, we systematically evaluate the impact of common design choices in Mixture of Experts (MoEs) on validation performance, uncovering distinct influences at token and sequence levels.","We also present empirical evidence showing comparable performance between a learned router and a frozen, randomly initialized router, suggesting that learned routing may not be essential.","Our study further reveals that Sequence-level routing can result in topic-specific weak expert specialization, in contrast to syntax specialization observed with Token-level routing."],"url":"http://arxiv.org/abs/2402.13089v1","category":"cs.LG"}
{"created":"2024-02-20 15:30:09","title":"Slot-VLM: SlowFast Slots for Video-Language Modeling","abstract":"Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding. A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs. In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference. Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots. In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure. The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information. Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features. These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering. Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering.","sentences":["Video-Language Models (VLMs), powered by the advancements in Large Language Models (LLMs), are charting new frontiers in video understanding.","A pivotal challenge is the development of an efficient method to encapsulate video content into a set of representative tokens to align with LLMs.","In this work, we introduce Slot-VLM, a novel framework designed to generate semantically decomposed video tokens, in terms of object-wise and event-wise visual representations, to facilitate LLM inference.","Particularly, we design a SlowFast Slots module, i.e., SF-Slots, that adaptively aggregates the dense video tokens from the CLIP vision encoder to a set of representative slots.","In order to take into account both the spatial object details and the varied temporal dynamics, SF-Slots is built with a dual-branch structure.","The Slow-Slots branch focuses on extracting object-centric slots from features at high spatial resolution but low (slow) frame sample rate, emphasizing detailed object information.","Conversely, Fast-Slots branch is engineered to learn event-centric slots from high temporal sample rate but low spatial resolution features.","These complementary slots are combined to form the vision context, serving as the input to the LLM for efficient question answering.","Our experimental results demonstrate the effectiveness of our Slot-VLM, which achieves the state-of-the-art performance on video question-answering."],"url":"http://arxiv.org/abs/2402.13088v1","category":"cs.CV"}
{"created":"2024-02-20 15:29:49","title":"How Does Selection Leak Privacy: Revisiting Private Selection and Improved Results for Hyper-parameter Tuning","abstract":"We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several. Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood. Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   This paper contributes both positive and negative answers to this question. Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense. However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds. This is first demonstrated by applying privacy audit on the tuning process. Our findings underscore a substantial gap between the current theoretical privacy bound and the empirical bound derived even under the strongest audit setup.   The gap found is not a fluke. Our subsequent study provides an improved privacy result for private hyper-parameter tuning due to its distinct properties. Our privacy results are also more generalizable compared to prior analyses that are only easily applicable in specific setups.","sentences":["We study the problem of guaranteeing Differential Privacy (DP) in hyper-parameter tuning, a crucial process in machine learning involving the selection of the best run from several.","Unlike many private algorithms, including the prevalent DP-SGD, the privacy implications of tuning remain insufficiently understood.","Recent works propose a generic private solution for the tuning process, yet a fundamental question still persists: is the current privacy bound for this solution tight?   ","This paper contributes both positive and negative answers to this question.","Initially, we provide studies affirming the current privacy analysis is indeed tight in a general sense.","However, when we specifically study the hyper-parameter tuning problem, such tightness no longer holds.","This is first demonstrated by applying privacy audit on the tuning process.","Our findings underscore a substantial gap between the current theoretical privacy bound and the empirical bound derived even under the strongest audit setup.   ","The gap found is not a fluke.","Our subsequent study provides an improved privacy result for private hyper-parameter tuning due to its distinct properties.","Our privacy results are also more generalizable compared to prior analyses that are only easily applicable in specific setups."],"url":"http://arxiv.org/abs/2402.13087v1","category":"cs.LG"}
{"created":"2024-02-20 15:28:27","title":"Profinite trees, through monads and the lambda-calculus","abstract":"In its simplest form, the theory of regular languages is the study of sets of finite words recognized by finite monoids. The finiteness condition on monoids gives rise to a topological space whose points, called profinite words, encode the limiting behavior of words with respect to finite monoids. Yet, some aspects of the theory of regular languages are not particular to monoids and can be described in a general setting. On the one hand, Boja\\'{n}czyk has shown how to use monads to generalize the theory of regular languages and has given an abstract definition of the free profinite structure, defined by codensity, given a fixed monad and a notion of finite structure. On the other hand, Salvati has introduced the notion of language of $\\lambda$-terms, using denotational semantics, which generalizes the case of words and trees through the Church encoding. In recent work, the author and collaborators defined the notion of profinite $\\lambda$-term using semantics in finite sets and functions, which extend the Church encoding to profinite words.   In this article, we prove that these two generalizations, based on monads and denotational semantics, coincide in the case of trees. To do so, we consider the monad of abstract clones which, when applied to a ranked alphabet, gives the associated clone of ranked trees. This induces a notion of free profinite clone, and hence of profinite trees. The main contribution is a categorical proof that the free profinite clone on a ranked alphabet is isomorphic, as a Stone-enriched clone, to the clone of profinite $\\lambda$-terms of Church type. Moreover, we also prove a parametricity theorem on families of semantic elements which provides another equivalent formulation of profinite trees in terms of Reynolds parametricity.","sentences":["In its simplest form, the theory of regular languages is the study of sets of finite words recognized by finite monoids.","The finiteness condition on monoids gives rise to a topological space whose points, called profinite words, encode the limiting behavior of words with respect to finite monoids.","Yet, some aspects of the theory of regular languages are not particular to monoids and can be described in a general setting.","On the one hand, Boja\\'{n}czyk has shown how to use monads to generalize the theory of regular languages and has given an abstract definition of the free profinite structure, defined by codensity, given a fixed monad and a notion of finite structure.","On the other hand, Salvati has introduced the notion of language of $\\lambda$-terms, using denotational semantics, which generalizes the case of words and trees through the Church encoding.","In recent work, the author and collaborators defined the notion of profinite $\\lambda$-term using semantics in finite sets and functions, which extend the Church encoding to profinite words.   ","In this article, we prove that these two generalizations, based on monads and denotational semantics, coincide in the case of trees.","To do so, we consider the monad of abstract clones which, when applied to a ranked alphabet, gives the associated clone of ranked trees.","This induces a notion of free profinite clone, and hence of profinite trees.","The main contribution is a categorical proof that the free profinite clone on a ranked alphabet is isomorphic, as a Stone-enriched clone, to the clone of profinite $\\lambda$-terms of Church type.","Moreover, we also prove a parametricity theorem on families of semantic elements which provides another equivalent formulation of profinite trees in terms of Reynolds parametricity."],"url":"http://arxiv.org/abs/2402.13086v1","category":"cs.LO"}
{"created":"2024-02-20 15:25:56","title":"IT Intrusion Detection Using Statistical Learning and Testbed Measurements","abstract":"We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure. We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions. In contrast to most related research, we have abundant data to train the models and evaluate their predictive power. The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure. Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols. Investigating intrusions in offline as well as online scenarios, we find that both HMM and LSTM can be effective in predicting attack start time, attack type, and attack actions. If sufficient training data is available, LSTM achieves higher prediction accuracy than HMM. HMM, on the other hand, requires less computational resources and less training data for effective prediction. Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT.","sentences":["We study automated intrusion detection in an IT infrastructure, specifically the problem of identifying the start of an attack, the type of attack, and the sequence of actions an attacker takes, based on continuous measurements from the infrastructure.","We apply statistical learning methods, including Hidden Markov Model (HMM), Long Short-Term Memory (LSTM), and Random Forest Classifier (RFC) to map sequences of observations to sequences of predicted attack actions.","In contrast to most related research, we have abundant data to train the models and evaluate their predictive power.","The data comes from traces we generate on an in-house testbed where we run attacks against an emulated IT infrastructure.","Central to our work is a machine-learning pipeline that maps measurements from a high-dimensional observation space to a space of low dimensionality or to a small set of observation symbols.","Investigating intrusions in offline as well as online scenarios, we find that both HMM and LSTM can be effective in predicting attack start time, attack type, and attack actions.","If sufficient training data is available, LSTM achieves higher prediction accuracy than HMM.","HMM, on the other hand, requires less computational resources and less training data for effective prediction.","Also, we find that the methods we study benefit from data produced by traditional intrusion detection systems like SNORT."],"url":"http://arxiv.org/abs/2402.13081v1","category":"cs.LG"}
{"created":"2024-02-20 15:24:29","title":"A thermodynamic approach to quantifying incompatible instruments","abstract":"We consider a thermodynamic framework to quantify instrument incompatibility through a resource theory subject to thermodynamic constraints. In this resource theory, we use the minimal thermalisation time needed to erase incompatibility's signature to measure how incompatible an instrument is. We show that this measure has a clear operational meaning in some work extraction tasks, thereby uncovering the thermodynamic advantages of incompatible instruments. We further analyse the possibility and impossibility of extending the time for incompatible signature to survive under general evolution. Finally, we discuss the physical implications of our findings to measurement incompatibility and steering distillation.","sentences":["We consider a thermodynamic framework to quantify instrument incompatibility through a resource theory subject to thermodynamic constraints.","In this resource theory, we use the minimal thermalisation time needed to erase incompatibility's signature to measure how incompatible an instrument is.","We show that this measure has a clear operational meaning in some work extraction tasks, thereby uncovering the thermodynamic advantages of incompatible instruments.","We further analyse the possibility and impossibility of extending the time for incompatible signature to survive under general evolution.","Finally, we discuss the physical implications of our findings to measurement incompatibility and steering distillation."],"url":"http://arxiv.org/abs/2402.13080v1","category":"quant-ph"}
{"created":"2024-02-20 15:23:29","title":"On the properties of the set where a generalized function of bounded variation takes infinite value","abstract":"We study the properties of the set where a generalized function of bounded variation has infinite approximate limit, highlighting in this way the main geometric difference with functions of bounded variation. To this aim we prove a new result on strict approximation of sets of finite perimeter from the outside with open sets.","sentences":["We study the properties of the set where a generalized function of bounded variation has infinite approximate limit, highlighting in this way the main geometric difference with functions of bounded variation.","To this aim we prove a new result on strict approximation of sets of finite perimeter from the outside with open sets."],"url":"http://arxiv.org/abs/2402.13078v1","category":"math.AP"}
{"created":"2024-02-20 15:23:24","title":"Mechanistic Neural Networks for Scientific Machine Learning","abstract":"This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences. It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling. Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs. This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing. Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling. We prove their comprehensive capabilities in analyzing and interpreting complex scientific data across various applications, showing significant performance against specialized state-of-the-art methods.","sentences":["This paper presents Mechanistic Neural Networks, a neural network design for machine learning applications in the sciences.","It incorporates a new Mechanistic Block in standard architectures to explicitly learn governing differential equations as representations, revealing the underlying dynamics of data and enhancing interpretability and efficiency in data modeling.","Central to our approach is a novel Relaxed Linear Programming Solver (NeuRLP) inspired by a technique that reduces solving linear ODEs to solving linear programs.","This integrates well with neural networks and surpasses the limitations of traditional ODE solvers enabling scalable GPU parallel processing.","Overall, Mechanistic Neural Networks demonstrate their versatility for scientific machine learning applications, adeptly managing tasks from equation discovery to dynamic systems modeling.","We prove their comprehensive capabilities in analyzing and interpreting complex scientific data across various applications, showing significant performance against specialized state-of-the-art methods."],"url":"http://arxiv.org/abs/2402.13077v1","category":"cs.LG"}
{"created":"2024-02-20 15:19:52","title":"Towards Intelligent Communications: Large Model Empowered Semantic Communications","abstract":"Deep learning enabled semantic communications have shown great potential to significantly improve transmission efficiency and alleviate spectrum scarcity, by effectively exchanging the semantics behind the data. Recently, the emergence of large models, boasting billions of parameters, has unveiled remarkable human-like intelligence, offering a promising avenue for advancing semantic communication by enhancing semantic understanding and contextual understanding. This article systematically investigates the large model-empowered semantic communication systems from potential applications to system design. First, we propose a new semantic communication architecture that seamlessly integrates large models into semantic communication through the introduction of a memory module. Then, the typical applications are illustrated to show the benefits of the new architecture. Besides, we discuss the key designs in implementing the new semantic communication systems from module design to system training. Finally, the potential research directions are identified to boost the large model-empowered semantic communications.","sentences":["Deep learning enabled semantic communications have shown great potential to significantly improve transmission efficiency and alleviate spectrum scarcity, by effectively exchanging the semantics behind the data.","Recently, the emergence of large models, boasting billions of parameters, has unveiled remarkable human-like intelligence, offering a promising avenue for advancing semantic communication by enhancing semantic understanding and contextual understanding.","This article systematically investigates the large model-empowered semantic communication systems from potential applications to system design.","First, we propose a new semantic communication architecture that seamlessly integrates large models into semantic communication through the introduction of a memory module.","Then, the typical applications are illustrated to show the benefits of the new architecture.","Besides, we discuss the key designs in implementing the new semantic communication systems from module design to system training.","Finally, the potential research directions are identified to boost the large model-empowered semantic communications."],"url":"http://arxiv.org/abs/2402.13073v1","category":"eess.SP"}
{"created":"2024-02-20 15:03:14","title":"Framed DDF operators and the general solution to Virasoro constraints","abstract":"We define the framed DDF operators by introducing the concept of local frames in the usual formulation of DDF operators. In doing so it is possible to completely decouple the DDF operators from the associated tachyon and show that they are good zero-dimensional conformal operators. This allows for an explicit formulation of the general solution of the Virasoro constraints both on-shell and off-shell. We then make precise the realization of the intuitive idea that DDF operators can be used to embed light-cone states in the covariant formulation. This embedding is not unique, but depends on a coset. This coset is the little group of the embedding of the light-cone and is associated with a frame. The frame allows us to embed the $SO(D-2)$ light-cone physical polarizations into the $SO(1,D-1)$ covariant ones in the most general way. The solution to the Virasoro constraints is not in the gauge that is usually used. This happens since the states obtained from DDF operators are generically the sum of terms which are partially transverse due to the presence of a projector but not traceless and terms which are partially traceless but not transverse. To check the identification, we verify the matching of the expectation value of the second Casimir of the Poincar'e group for some light-cone states with the corresponding covariant states built using the framed DDFs.","sentences":["We define the framed DDF operators by introducing the concept of local frames in the usual formulation of DDF operators.","In doing so it is possible to completely decouple the DDF operators from the associated tachyon and show that they are good zero-dimensional conformal operators.","This allows for an explicit formulation of the general solution of the Virasoro constraints both on-shell and off-shell.","We then make precise the realization of the intuitive idea that DDF operators can be used to embed light-cone states in the covariant formulation.","This embedding is not unique, but depends on a coset.","This coset is the little group of the embedding of the light-cone and is associated with a frame.","The frame allows us to embed the $SO(D-2)$ light-cone physical polarizations into the $SO(1,D-1)$ covariant ones in the most general way.","The solution to the Virasoro constraints is not in the gauge that is usually used.","This happens since the states obtained from DDF operators are generically the sum of terms which are partially transverse due to the presence of a projector but not traceless and terms which are partially traceless but not transverse.","To check the identification, we verify the matching of the expectation value of the second Casimir of the Poincar'e group for some light-cone states with the corresponding covariant states built using the framed DDFs."],"url":"http://arxiv.org/abs/2402.13066v1","category":"hep-th"}
{"created":"2024-02-20 15:00:35","title":"Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models","abstract":"We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines. Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs. With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.","sentences":["We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs).","Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale synthetic instruction data across all disciplines.","Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs.","Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs.","With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills.","Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks.","In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy."],"url":"http://arxiv.org/abs/2402.13064v1","category":"cs.CL"}
{"created":"2024-02-20 14:56:58","title":"3D high-resolution imaging algorithm using 1D MIMO array for autonomous driving application","abstract":"The problem of 3D high-resolution imaging in automotive multiple-input multiple-output (MIMO) side-looking radar using a 1D array is considered. The concept of motion-enhanced snapshots is introduced for generating larger apertures in the azimuth dimension. For the first time, 3D imaging capabilities can be achieved with high angular resolution using a 1D MIMO antenna array, which can alleviate the requirement for large radar systems in autonomous vehicles. The robustness to variations in the vehicle's movement trajectory is also considered and addressed with relevant compensations in the steering vector. The available degrees of freedom as well as the Signal to Noise Ratio (SNR) are shown to increase with the proposed method compared to conventional imaging approaches. The performance of the algorithm has been studied in simulations, and validated with experimental data collected in a realistic driving scenario.","sentences":["The problem of 3D high-resolution imaging in automotive multiple-input multiple-output (MIMO) side-looking radar using a 1D array is considered.","The concept of motion-enhanced snapshots is introduced for generating larger apertures in the azimuth dimension.","For the first time, 3D imaging capabilities can be achieved with high angular resolution using a 1D MIMO antenna array, which can alleviate the requirement for large radar systems in autonomous vehicles.","The robustness to variations in the vehicle's movement trajectory is also considered and addressed with relevant compensations in the steering vector.","The available degrees of freedom as well as the Signal to Noise Ratio (SNR) are shown to increase with the proposed method compared to conventional imaging approaches.","The performance of the algorithm has been studied in simulations, and validated with experimental data collected in a realistic driving scenario."],"url":"http://arxiv.org/abs/2402.13062v1","category":"eess.SP"}
{"created":"2024-02-20 14:52:52","title":"Random Graph Set and Evidence Pattern Reasoning Model","abstract":"Evidence theory is widely used in decision-making and reasoning systems. In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model. In order to better fit the decision making goals, the Evidence Pattern Reasoning Model (EPRM) is proposed. By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks. Random Permutation Set (RPS) expands order information for evidence theory. It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships. Therefore, Random Graph Set (RGS) were proposed to model complex relationships and represent more event types. In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated. The implementation of EPRM called Conflict Resolution Decision optimized 18.17\\% of the cases compared to Mean Velocity Decision, effectively improving the aircraft velocity ranking. EPRM provides a unified solution for evidence-based decision making.","sentences":["Evidence theory is widely used in decision-making and reasoning systems.","In previous research, Transferable Belief Model (TBM) is a commonly used evidential decision making model, but TBM is a non-preference model.","In order to better fit the decision making goals, the Evidence Pattern Reasoning Model (EPRM) is proposed.","By defining pattern operators and decision making operators, corresponding preferences can be set for different tasks.","Random Permutation Set (RPS) expands order information for evidence theory.","It is hard for RPS to characterize the complex relationship between samples such as cycling, paralleling relationships.","Therefore, Random Graph Set (RGS) were proposed to model complex relationships and represent more event types.","In order to illustrate the significance of RGS and EPRM, an experiment of aircraft velocity ranking was designed and 10,000 cases were simulated.","The implementation of EPRM called Conflict Resolution Decision optimized 18.17\\% of the cases compared to Mean Velocity Decision, effectively improving the aircraft velocity ranking.","EPRM provides a unified solution for evidence-based decision making."],"url":"http://arxiv.org/abs/2402.13058v1","category":"cs.AI"}
{"created":"2024-02-20 14:51:02","title":"Edge Computing for IoT","abstract":"Over the past few years, The idea of edge computing has seen substantial expansion in both academic and industrial circles. This computing approach has garnered attention due to its integrating role in advancing various state-of-the-art technologies such as Internet of Things (IoT) , 5G, artificial intelligence, and augmented reality. In this chapter, we introduce computing paradigms for IoT, offering an overview of the current cutting-edge computing approaches that can be used with IoT. Furthermore, we go deeper into edge computing paradigms, specifically focusing on cloudlet and mobile edge computing. After that, we investigate the architecture of edge computing-based IoT, its advantages, and the technologies that make Edge computing-based IoT possible, including artificial intelligence and lightweight virtualization. Additionally, we review real-life case studies of how edge computing is applied in IoT-based Intelligent Systems, including areas like healthcare, manufacturing, agriculture, and transportation. Finally, we discuss current research obstacles and outline potential future directions for further investigation in this domain.","sentences":["Over the past few years, The idea of edge computing has seen substantial expansion in both academic and industrial circles.","This computing approach has garnered attention due to its integrating role in advancing various state-of-the-art technologies such as Internet of Things (IoT) , 5G, artificial intelligence, and augmented reality.","In this chapter, we introduce computing paradigms for IoT, offering an overview of the current cutting-edge computing approaches that can be used with IoT.","Furthermore, we go deeper into edge computing paradigms, specifically focusing on cloudlet and mobile edge computing.","After that, we investigate the architecture of edge computing-based IoT, its advantages, and the technologies that make Edge computing-based IoT possible, including artificial intelligence and lightweight virtualization.","Additionally, we review real-life case studies of how edge computing is applied in IoT-based Intelligent Systems, including areas like healthcare, manufacturing, agriculture, and transportation.","Finally, we discuss current research obstacles and outline potential future directions for further investigation in this domain."],"url":"http://arxiv.org/abs/2402.13056v1","category":"cs.NI"}
{"created":"2024-02-20 14:43:39","title":"Identifying Semantic Induction Heads to Understand In-Context Learning","abstract":"Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness. To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs. Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs. We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens. More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models. The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs.","sentences":["Although large language models (LLMs) have demonstrated remarkable performance, the lack of transparency in their inference logic raises concerns about their trustworthiness.","To gain a better understanding of LLMs, we conduct a detailed analysis of the operations of attention heads and aim to better understand the in-context learning of LLMs.","Specifically, we investigate whether attention heads encode two types of relationships between tokens present in natural languages: the syntactic dependency parsed from sentences and the relation within knowledge graphs.","We find that certain attention heads exhibit a pattern where, when attending to head tokens, they recall tail tokens and increase the output logits of those tail tokens.","More crucially, the formulation of such semantic induction heads has a close correlation with the emergence of the in-context learning ability of language models.","The study of semantic attention heads advances our understanding of the intricate operations of attention heads in transformers, and further provides new insights into the in-context learning of LLMs."],"url":"http://arxiv.org/abs/2402.13055v1","category":"cs.CL"}
{"created":"2024-02-20 14:41:42","title":"Inverse design of spinodoid structures using Bayesian optimization","abstract":"Tailoring materials to achieve a desired behavior in specific applications is of significant scientific and industrial interest as design of materials is a key driver to innovation. Overcoming the rather slow and expertise-bound traditional forward approaches of trial and error, inverse design is attracting substantial attention. Targeting a property, the design model proposes a candidate structure with the desired property. This concept can be particularly well applied to the field of architected materials as their structures can be directly tuned. The bone-like spinodoid materials are a specific class of architected materials. They are of considerable interest thanks to their non-periodicity, smoothness, and low-dimensional statistical description. Previous work successfully employed machine learning (ML) models for inverse design. The amount of data necessary for most ML approaches poses a severe obstacle for broader application, especially in the context of inelasticity. That is why we propose an inverse-design approach based on Bayesian optimization to operate in the small-data regime. Necessitating substantially less data, a small initial data set is iteratively augmented by in silico generated data until a structure with the targeted properties is found. The application to the inverse design of spinodoid structures of desired elastic properties demonstrates the framework's potential for paving the way for advance in inverse design.","sentences":["Tailoring materials to achieve a desired behavior in specific applications is of significant scientific and industrial interest as design of materials is a key driver to innovation.","Overcoming the rather slow and expertise-bound traditional forward approaches of trial and error, inverse design is attracting substantial attention.","Targeting a property, the design model proposes a candidate structure with the desired property.","This concept can be particularly well applied to the field of architected materials as their structures can be directly tuned.","The bone-like spinodoid materials are a specific class of architected materials.","They are of considerable interest thanks to their non-periodicity, smoothness, and low-dimensional statistical description.","Previous work successfully employed machine learning (ML) models for inverse design.","The amount of data necessary for most ML approaches poses a severe obstacle for broader application, especially in the context of inelasticity.","That is why we propose an inverse-design approach based on Bayesian optimization to operate in the small-data regime.","Necessitating substantially less data, a small initial data set is iteratively augmented by in silico generated data until a structure with the targeted properties is found.","The application to the inverse design of spinodoid structures of desired elastic properties demonstrates the framework's potential for paving the way for advance in inverse design."],"url":"http://arxiv.org/abs/2402.13054v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 14:41:41","title":"Sharpening the dark matter signature in gravitational waveforms I: Accretion and eccentricity evolution","abstract":"Dark matter overdensities around black holes can alter the dynamical evolution of a companion object orbiting around it, and cause a dephasing of the gravitational waveform. Here, we present a refined calculation of the co-evolution of the binary and the dark matter distribution, taking into account the accretion of dark matter particles on the companion black hole, and generalizing previous quasi-circular calculations to the general case of eccentric orbits. These calculations are validated by dedicated N-body simulations. We show that accretion can lead to a large dephasing, and therefore cannot be neglected in general. We also demonstrate that dark matter spikes tend to circularize eccentric orbits faster than previously thought.","sentences":["Dark matter overdensities around black holes can alter the dynamical evolution of a companion object orbiting around it, and cause a dephasing of the gravitational waveform.","Here, we present a refined calculation of the co-evolution of the binary and the dark matter distribution, taking into account the accretion of dark matter particles on the companion black hole, and generalizing previous quasi-circular calculations to the general case of eccentric orbits.","These calculations are validated by dedicated N-body simulations.","We show that accretion can lead to a large dephasing, and therefore cannot be neglected in general.","We also demonstrate that dark matter spikes tend to circularize eccentric orbits faster than previously thought."],"url":"http://arxiv.org/abs/2402.13053v1","category":"gr-qc"}
{"created":"2024-02-20 14:36:23","title":"Stable Knowledge Editing in Large Language Models","abstract":"Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale. However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge. The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities. It introduces instability to the performance of the knowledge editing method. To transcend these assumptions, we introduce StableKE, a method adopts a novel perspective based on knowledge augmentation rather than knowledge localization. To overcome the expense of human labeling, StableKE integrates two automated knowledge augmentation strategies: Semantic Paraphrase Enhancement strategy, which diversifies knowledge descriptions to facilitate the teaching of new information to the model, and Contextual Description Enrichment strategy, expanding the surrounding knowledge to prevent the forgetting of related information. StableKE surpasses other knowledge editing methods, demonstrating stability both edited knowledge and multi-hop knowledge, while also preserving unrelated knowledge and general abilities. Moreover, StableKE can edit knowledge on ChatGPT.","sentences":["Efficient knowledge editing of large language models is crucial for replacing obsolete information or incorporating specialized knowledge on a large scale.","However, previous methods implicitly assume that knowledge is localized and isolated within the model, an assumption that oversimplifies the interconnected nature of model knowledge.","The premise of localization results in an incomplete knowledge editing, whereas an isolated assumption may impair both other knowledge and general abilities.","It introduces instability to the performance of the knowledge editing method.","To transcend these assumptions, we introduce StableKE, a method adopts a novel perspective based on knowledge augmentation rather than knowledge localization.","To overcome the expense of human labeling, StableKE integrates two automated knowledge augmentation strategies: Semantic Paraphrase Enhancement strategy, which diversifies knowledge descriptions to facilitate the teaching of new information to the model, and Contextual Description Enrichment strategy, expanding the surrounding knowledge to prevent the forgetting of related information.","StableKE surpasses other knowledge editing methods, demonstrating stability both edited knowledge and multi-hop knowledge, while also preserving unrelated knowledge and general abilities.","Moreover, StableKE can edit knowledge on ChatGPT."],"url":"http://arxiv.org/abs/2402.13048v1","category":"cs.CL"}
{"created":"2024-02-20 14:35:54","title":"Optical appearance of black holes surrounded by a dark matter halo","abstract":"Black holes in General Relativity are described by space-time metrics that are simpler in comparison to non-vacuum compact objects. However, given the universality of the gravitational pull, it is expected that dark matter accumulates around astrophysical black holes, which can have an impact in the overall gravitational field, especially at galactic centers, and induce non-negligible effects in their observational imprints. In this work we study the optical appearance of a spherically symmetric black hole both when orbited by isotropically emitting light sources and when surrounded by a (geometrically and optically thin) accretion disk, while immersed in a dark matter halo. The black hole geometry plus the dark matter halo come as a solution of Einstein's field equations coupled to an anisotropic fluid whose density component follows a Hermquist-type distribution. Even in situations in which the geodesic description differs profoundly from the isolated black hole case, we find minor modifications to the primary and secondary tracks of the isotropic orbiting sources, and to the width, location, and relative luminosity of the corresponding photon rings as compared to the Schwarzschild black hole at equal black hole mass and emission models. This fact troubles distinguishing between both geometries using present observations of very-long baseline interferometry.","sentences":["Black holes in General Relativity are described by space-time metrics that are simpler in comparison to non-vacuum compact objects.","However, given the universality of the gravitational pull, it is expected that dark matter accumulates around astrophysical black holes, which can have an impact in the overall gravitational field, especially at galactic centers, and induce non-negligible effects in their observational imprints.","In this work we study the optical appearance of a spherically symmetric black hole both when orbited by isotropically emitting light sources and when surrounded by a (geometrically and optically thin) accretion disk, while immersed in a dark matter halo.","The black hole geometry plus the dark matter halo come as a solution of Einstein's field equations coupled to an anisotropic fluid whose density component follows a Hermquist-type distribution.","Even in situations in which the geodesic description differs profoundly from the isolated black hole case, we find minor modifications to the primary and secondary tracks of the isotropic orbiting sources, and to the width, location, and relative luminosity of the corresponding photon rings as compared to the Schwarzschild black hole at equal black hole mass and emission models.","This fact troubles distinguishing between both geometries using present observations of very-long baseline interferometry."],"url":"http://arxiv.org/abs/2402.13047v1","category":"gr-qc"}
{"created":"2024-02-20 14:31:47","title":"Conversion of Emitted Axionic Dark Matter to Photons for Non-Rotating Magnetized Neutron Stars","abstract":"We attempt to find the impact of a modified Tolman Oppenheimer Volkoff (TOV) system of equations on the luminosities of direct photons, neutrinos and axions for a particular axion mass in the presence of a magnetic field. We employ two different equation of states (EoSs) namely APR and FPS to generate the profiles of mass and pressure for spherically symmetric and non-rotating Neutron stars (NSs). We then compute the axions and neutrino emission rates by employing the Cooper-pair-breaking and formation process (PBF) in the core using the NSCool code. We also examine the possibility of axion to photon conversion in the magnetosphere of NSs. Furthermore, we investigate the impact of the magnetic field on the actual observables, such as the energy spectrum of axions and axion-converted photon flux for three different NSs. Our comparative study indicates that axions energy spectrum and axion-converted photon flux changes significantly due to an intense magnetic field.","sentences":["We attempt to find the impact of a modified Tolman Oppenheimer Volkoff (TOV) system of equations on the luminosities of direct photons, neutrinos and axions for a particular axion mass in the presence of a magnetic field.","We employ two different equation of states (EoSs) namely APR and FPS to generate the profiles of mass and pressure for spherically symmetric and non-rotating Neutron stars (NSs).","We then compute the axions and neutrino emission rates by employing the Cooper-pair-breaking and formation process (PBF) in the core using the NSCool code.","We also examine the possibility of axion to photon conversion in the magnetosphere of NSs.","Furthermore, we investigate the impact of the magnetic field on the actual observables, such as the energy spectrum of axions and axion-converted photon flux for three different NSs.","Our comparative study indicates that axions energy spectrum and axion-converted photon flux changes significantly due to an intense magnetic field."],"url":"http://arxiv.org/abs/2402.13044v1","category":"hep-ph"}
{"created":"2024-02-20 14:31:17","title":"Effective and Efficient Conversation Retrieval for Dialogue State Tracking with Implicit Text Summaries","abstract":"Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning. Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance. However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable. To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations. A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search. To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations. We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B. The experimental results show a significant improvement over relevant baselines in real few-shot DST settings.","sentences":["Few-shot dialogue state tracking (DST) with Large Language Models (LLM) relies on an effective and efficient conversation retriever to find similar in-context examples for prompt learning.","Previous works use raw dialogue context as search keys and queries, and a retriever is fine-tuned with annotated dialogues to achieve superior performance.","However, the approach is less suited for scaling to new domains or new annotation languages, where fine-tuning data is unavailable.","To address this problem, we handle the task of conversation retrieval based on text summaries of the conversations.","A LLM-based conversation summarizer is adopted for query and key generation, which enables effective maximum inner product search.","To avoid the extra inference cost brought by LLM-based conversation summarization, we further distill a light-weight conversation encoder which produces query embeddings without decoding summaries for test conversations.","We validate our retrieval approach on MultiWOZ datasets with GPT-Neo-2.7B and LLaMA-7B/30B.","The experimental results show a significant improvement over relevant baselines in real few-shot DST settings."],"url":"http://arxiv.org/abs/2402.13043v1","category":"cs.CL"}
{"created":"2024-02-20 14:29:37","title":"Data Repository of Finite Element Models of Normal and Deformed Thoracolumbar Spine","abstract":"Adult spine deformity (ASD) is prevalent and leads to a sagittal misalignment in the vertebral column. Computational methods, including Finite Element (FE) Models, have emerged as valuable tools for investigating the causes and treatment of ASD through biomechanical simulations. However, the process of generating personalized FE models is often complex and time-consuming. To address this challenge, we present a repository of FE models with diverse spine morphologies that statistically represent real geometries from a cohort of patients. These models are generated using EOS images, which are utilized to reconstruct 3D surface spine models. Subsequently, a Statistical Shape Model (SSM) is constructed, enabling the adaptation of a FE hexahedral mesh template for both the bone and soft tissues of the spine through mesh morphing. The SSM deformation fields facilitate the personalization of the mean hexahedral FE model based on sagittal balance measurements. Ultimately, this new hexahedral SSM tool offers a means to generate a virtual cohort of 16807 thoracolumbar FE spine models, which are openly shared in a public repository.","sentences":["Adult spine deformity (ASD) is prevalent and leads to a sagittal misalignment in the vertebral column.","Computational methods, including Finite Element (FE) Models, have emerged as valuable tools for investigating the causes and treatment of ASD through biomechanical simulations.","However, the process of generating personalized FE models is often complex and time-consuming.","To address this challenge, we present a repository of FE models with diverse spine morphologies that statistically represent real geometries from a cohort of patients.","These models are generated using EOS images, which are utilized to reconstruct 3D surface spine models.","Subsequently, a Statistical Shape Model (SSM) is constructed, enabling the adaptation of a FE hexahedral mesh template for both the bone and soft tissues of the spine through mesh morphing.","The SSM deformation fields facilitate the personalization of the mean hexahedral FE model based on sagittal balance measurements.","Ultimately, this new hexahedral SSM tool offers a means to generate a virtual cohort of 16807 thoracolumbar FE spine models, which are openly shared in a public repository."],"url":"http://arxiv.org/abs/2402.13041v1","category":"physics.bio-ph"}
{"created":"2024-02-20 14:29:02","title":"Text-Guided Molecule Generation with Diffusion Language Model","abstract":"Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions. Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture. In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods. TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process. The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations. We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources. Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains. Code will be released at: https://github.com/Deno-V/tgm-dlm.","sentences":["Text-guided molecule generation is a task where molecules are generated to match specific textual descriptions.","Recently, most existing SMILES-based molecule generation methods rely on an autoregressive architecture.","In this work, we propose the Text-Guided Molecule Generation with Diffusion Language Model (TGM-DLM), a novel approach that leverages diffusion models to address the limitations of autoregressive methods.","TGM-DLM updates token embeddings within the SMILES string collectively and iteratively, using a two-phase diffusion generation process.","The first phase optimizes embeddings from random noise, guided by the text description, while the second phase corrects invalid SMILES strings to form valid molecular representations.","We demonstrate that TGM-DLM outperforms MolT5-Base, an autoregressive model, without the need for additional data resources.","Our findings underscore the remarkable effectiveness of TGM-DLM in generating coherent and precise molecules with specific properties, opening new avenues in drug discovery and related scientific domains.","Code will be released at: https://github.com/Deno-V/tgm-dlm."],"url":"http://arxiv.org/abs/2402.13040v1","category":"cs.LG"}
{"created":"2024-02-20 14:24:00","title":"Align Your Intents: Offline Imitation Learning via Optimal Transport","abstract":"Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment. As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively. Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels. In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data. Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories. We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and improves the performance of other offline RL algorithms in the sparse-reward tasks.","sentences":["Offline reinforcement learning (RL) addresses the problem of sequential decision-making by learning optimal policy through pre-collected data, without interacting with the environment.","As yet, it has remained somewhat impractical, because one rarely knows the reward explicitly and it is hard to distill it retrospectively.","Here, we show that an imitating agent can still learn the desired behavior merely from observing the expert, despite the absence of explicit rewards or action labels.","In our method, AILOT (Aligned Imitation Learning via Optimal Transport), we involve special representation of states in a form of intents that incorporate pairwise spatial distances within the data.","Given such representations, we define intrinsic reward function via optimal transport distance between the expert's and the agent's trajectories.","We report that AILOT outperforms state-of-the art offline imitation learning algorithms on D4RL benchmarks and improves the performance of other offline RL algorithms in the sparse-reward tasks."],"url":"http://arxiv.org/abs/2402.13037v1","category":"cs.LG"}
{"created":"2024-02-20 14:23:34","title":"SiLLM: Large Language Models for Simultaneous Machine Translation","abstract":"Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words. Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations. However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model. Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks. We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT. The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy. The translation agent, leveraging the capabilities of LLM, generates translation using the partial source sentence. The two agents collaborate to accomplish SiMT. To facilitate the application of token-level policies determined by conventional SiMT models to LLM, we propose a word-level policy adapted for LLM. Experiments on two datasets demonstrate that, with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance.","sentences":["Simultaneous Machine Translation (SiMT) generates translations while reading the source sentence, necessitating a policy to determine the optimal timing for reading and generating words.","Despite the remarkable performance achieved by Large Language Models (LLM) across various NLP tasks, existing SiMT methods predominantly focus on conventional transformers, employing a single model to concurrently determine the policy and generate the translations.","However, given the complexity of SiMT, it is challenging to effectively address both tasks with a single model.","Therefore, there is a need to decouple the SiMT task into policy-decision and translation sub-tasks.","We propose SiLLM, which delegates the two sub-tasks to separate agents, thereby incorporating LLM into SiMT.","The policy-decision agent is managed by a conventional SiMT model, responsible for determining the translation policy.","The translation agent, leveraging the capabilities of LLM, generates translation using the partial source sentence.","The two agents collaborate to accomplish SiMT.","To facilitate the application of token-level policies determined by conventional SiMT models to LLM, we propose a word-level policy adapted for LLM.","Experiments on two datasets demonstrate that, with a small amount of data for fine-tuning LLM, SiLLM attains state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.13036v1","category":"cs.CL"}
{"created":"2024-02-20 14:23:23","title":"Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models","abstract":"Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction. However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction. In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction. We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''. Then we construct a checking-correction dataset for training models. After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction. We compare the performance of models fine-tuned with the ``Step CoT Check'' prompt against those refined using other promps within the context of checking-correction data. The ``Step CoT Check'' outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness. For reproducibility, all the datasets and codes are provided in \\url{https://github.com/bammt/Learn-to-check}.","sentences":["Large language models (LLMs) have made significant strides in reasoning capabilities, with ongoing efforts to refine their reasoning through self-correction.","However, recent studies suggest that self-correction can be limited or even counterproductive without external accurate knowledge, raising questions about the limits and effectiveness of self-correction.","In this paper, we aim to enhance LLM's self-checking capabilities by meticulously designing training data, thereby improving the accuracy of self-correction.","We conduct a detailed analysis of error types in mathematical reasoning and develop a tailored prompt, termed ``Step CoT Check''.","Then we construct a checking-correction dataset for training models.","After integrating the original CoT data and checking-correction data for training, we observe that models could improve their self-checking capabilities, thereby enhancing their self-correction capacity and eliminating the need for external feedback or ground truth labels to ascertain the endpoint of correction.","We compare the performance of models fine-tuned with the ``Step CoT Check'' prompt against those refined using other promps within the context of checking-correction data.","The ``Step CoT Check'' outperforms the other two check formats in model with lager parameters, providing more precise feedback thus achieving a higher rate of correctness.","For reproducibility, all the datasets and codes are provided in \\url{https://github.com/bammt/Learn-to-check}."],"url":"http://arxiv.org/abs/2402.13035v1","category":"cs.CL"}
{"created":"2024-02-20 14:23:06","title":"Ray Tracing Algorithm for Reconfigurable Intelligent Surfaces","abstract":"Ray tracing accelerated with graphics processing units (GPUs) is an accurate and efficient simulation technique of wireless communication channels. In this paper, we extend a GPU-accelerated ray tracer (RT) to support the effects of reconfigurable intelligent surfaces (RISs). To evaluate the electric field, we derived a RIS path loss model that can be integrated into a RT and enables further extensions for the implementation of additional features and incorporation into complex reflective scenarios. We verify the derivation and implementation of our model by comparison with empirical measurements in a lab environment. We demonstrate the capabilities of our model to support higher-order reflections from the RIS to the receiver. We find that such components have a significant effect on the received signal strength, concluding that the extensions of advanced functionality enabled by our model play an important role in the accurate modeling of radio wave propagation in an environment including RISs.","sentences":["Ray tracing accelerated with graphics processing units (GPUs) is an accurate and efficient simulation technique of wireless communication channels.","In this paper, we extend a GPU-accelerated ray tracer (RT) to support the effects of reconfigurable intelligent surfaces (RISs).","To evaluate the electric field, we derived a RIS path loss model that can be integrated into a RT and enables further extensions for the implementation of additional features and incorporation into complex reflective scenarios.","We verify the derivation and implementation of our model by comparison with empirical measurements in a lab environment.","We demonstrate the capabilities of our model to support higher-order reflections from the RIS to the receiver.","We find that such components have a significant effect on the received signal strength, concluding that the extensions of advanced functionality enabled by our model play an important role in the accurate modeling of radio wave propagation in an environment including RISs."],"url":"http://arxiv.org/abs/2402.13034v1","category":"eess.SP"}
{"created":"2024-02-20 14:18:07","title":"Observed epochal variations in X-ray lines from the O Supergiant $\u03b6$ Puppis do not require substantial changes in the wind mass flux","abstract":"We fit the high resolution \\textit{Chandra} X-ray spectra of the O supergiant $\\zeta$ Puppis using the variable boundary condition (VBC) line model to test the stability of its mass-loss rate between two epochs of observation: 2000 March and 2018 July -- 2019 August. At issue is whether the observed variations are induced by global changes in the cool (unshocked) wind itself or are isolated to the local pockets of hot gas (i.e., changes in the frequency and location of the shocks). Evidence in the literature favored the possibility of a 40 per cent increase in the mass flux of the entire stellar wind, based on X-ray reabsorption from a line-deshadowing-instability-inspired parameterization, whereas our fit parameters are consistent with a constant mass flux with a change in the velocity variations that determine the locations where shocks form. Our results suggest the shocks in the more recent data are formed at somewhat larger radii, mimicking the enhanced blueshifts and increased line fluxes interpreted in the previous analysis as being due to increases in both the X-ray generation and reabsorption from an overall stronger wind.","sentences":["We fit the high resolution \\textit{Chandra} X-ray spectra of the O supergiant $\\zeta$ Puppis using the variable boundary condition (VBC) line model to test the stability of its mass-loss rate between two epochs of observation: 2000 March and 2018 July -- 2019 August.","At issue is whether the observed variations are induced by global changes in the cool (unshocked) wind itself or are isolated to the local pockets of hot gas (i.e., changes in the frequency and location of the shocks).","Evidence in the literature favored the possibility of a 40 per cent increase in the mass flux of the entire stellar wind, based on X-ray reabsorption from a line-deshadowing-instability-inspired parameterization, whereas our fit parameters are consistent with a constant mass flux with a change in the velocity variations that determine the locations where shocks form.","Our results suggest the shocks in the more recent data are formed at somewhat larger radii, mimicking the enhanced blueshifts and increased line fluxes interpreted in the previous analysis as being due to increases in both the X-ray generation and reabsorption from an overall stronger wind."],"url":"http://arxiv.org/abs/2402.13032v1","category":"astro-ph.HE"}
{"created":"2024-02-20 14:17:53","title":"Inflationary resolution of the initial singularity","abstract":"We construct geodesically complete inflationary models, addressing and overcoming existing no-go theorems related to past eternal inflation. We show that a period of accelerated expansion is indispensable for achieving nontrivial, geodesically complete (Generalized) Friedmann-Robertson-Walker spacetimes. We show within the framework of General Relativity, the persistence of eternal inflation comes at the expense of violating the null energy condition.","sentences":["We construct geodesically complete inflationary models, addressing and overcoming existing no-go theorems related to past eternal inflation.","We show that a period of accelerated expansion is indispensable for achieving nontrivial, geodesically complete (Generalized) Friedmann-Robertson-Walker spacetimes.","We show within the framework of General Relativity, the persistence of eternal inflation comes at the expense of violating the null energy condition."],"url":"http://arxiv.org/abs/2402.13031v1","category":"hep-th"}
{"created":"2024-02-20 14:17:45","title":"Compressing the two-particle Green's function using wavelets: Theory and application to the Hubbard atom","abstract":"Precise algorithms capable of providing controlled solutions in the presence of strong interactions are transforming the landscape of quantum many-body physics. Particularly exciting breakthroughs are enabling the computation of non-zero temperature correlation functions. However, computational challenges arise due to constraints in resources and memory limitations, especially in scenarios involving complex Green's functions and lattice effects. Leveraging the principles of signal processing and data compression, this paper explores the wavelet decomposition as a versatile and efficient method for obtaining compact and resource-efficient representations of the many-body theory of interacting systems. The effectiveness of the wavelet decomposition is illustrated through its application to the representation of generalized susceptibilities and self-energies in a prototypical interacting fermionic system, namely the Hubbard model at half-filling in its atomic limit. These results are the first proof-of-principle application of the wavelet compression within the realm of many-body physics and demonstrate the potential of this wavelet-based compression scheme for understanding the physics of correlated electron systems.","sentences":["Precise algorithms capable of providing controlled solutions in the presence of strong interactions are transforming the landscape of quantum many-body physics.","Particularly exciting breakthroughs are enabling the computation of non-zero temperature correlation functions.","However, computational challenges arise due to constraints in resources and memory limitations, especially in scenarios involving complex Green's functions and lattice effects.","Leveraging the principles of signal processing and data compression, this paper explores the wavelet decomposition as a versatile and efficient method for obtaining compact and resource-efficient representations of the many-body theory of interacting systems.","The effectiveness of the wavelet decomposition is illustrated through its application to the representation of generalized susceptibilities and self-energies in a prototypical interacting fermionic system, namely the Hubbard model at half-filling in its atomic limit.","These results are the first proof-of-principle application of the wavelet compression within the realm of many-body physics and demonstrate the potential of this wavelet-based compression scheme for understanding the physics of correlated electron systems."],"url":"http://arxiv.org/abs/2402.13030v1","category":"cond-mat.str-el"}
{"created":"2024-02-20 14:14:26","title":"Federated Learning for Iot/Edge/Fog Computing Systems","abstract":"With the help of a new architecture called Edge/Fog (E/F) computing, cloud computing services can now be extended nearer to data generator devices. E/F computing in combination with Deep Learning (DL) is a promisedtechnique that is vastly applied in numerous fields. To train their models, data producers in conventional DL architectures with E/F computing enable them to repeatedly transmit and communicate data with third-party servers, like Edge/Fog or cloud servers. Due to the extensive bandwidth needs, legal issues, and privacy risks, this architecture is frequently impractical. Through a centralized server, the models can be co-trained by FL through distributed clients, including cars, hospitals, and mobile phones, while preserving data localization. As it facilitates group learning and model optimization, FL can therefore be seen as a motivating element in the E/F computing paradigm. Although FL applications in E/F computing environments have been considered in previous studies, FL execution and hurdles in the E/F computing framework have not been thoroughly covered. In order to identify advanced solutions, this chapter will provide a review of the application of FL in E/F computing systems. We think that by doing this chapter, researchers will learn more about how E/F computing and FL enable related concepts and technologies. Some case studies about the implementation of federated learning in E/F computing are being investigated. The open issues and future research directions are introduced.","sentences":["With the help of a new architecture called Edge/Fog (E/F) computing, cloud computing services can now be extended nearer to data generator devices.","E/F computing in combination with Deep Learning (DL) is a promisedtechnique that is vastly applied in numerous fields.","To train their models, data producers in conventional DL architectures with E/F computing enable them to repeatedly transmit and communicate data with third-party servers, like Edge/Fog or cloud servers.","Due to the extensive bandwidth needs, legal issues, and privacy risks, this architecture is frequently impractical.","Through a centralized server, the models can be co-trained by FL through distributed clients, including cars, hospitals, and mobile phones, while preserving data localization.","As it facilitates group learning and model optimization, FL can therefore be seen as a motivating element in the E/F computing paradigm.","Although FL applications in E/F computing environments have been considered in previous studies, FL execution and hurdles in the E/F computing framework have not been thoroughly covered.","In order to identify advanced solutions, this chapter will provide a review of the application of FL in E/F computing systems.","We think that by doing this chapter, researchers will learn more about how E/F computing and FL enable related concepts and technologies.","Some case studies about the implementation of federated learning in E/F computing are being investigated.","The open issues and future research directions are introduced."],"url":"http://arxiv.org/abs/2402.13029v1","category":"cs.NI"}
{"created":"2024-02-20 14:10:40","title":"Heterogeneous Graph Reasoning for Fact Checking over Texts and Tables","abstract":"Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces. It usually involves evidence retrieval and veracity reasoning. In this paper, we focus on the latter, reasoning over unstructured text and structured table information. Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models. Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures. To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC. Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties. We perform information propagation via a relational graph neural network, facilitating interactions between claims and evidence. An attention-based method is utilized to integrate information, combined with a language model for generating predictions. We introduce a multitask loss function to account for potential inaccuracies in evidence retrieval. Comprehensive experiments on the large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC. Code will be released at: https://github.com/Deno-V/HeterFC.","sentences":["Fact checking aims to predict claim veracity by reasoning over multiple evidence pieces.","It usually involves evidence retrieval and veracity reasoning.","In this paper, we focus on the latter, reasoning over unstructured text and structured table information.","Previous works have primarily relied on fine-tuning pretrained language models or training homogeneous-graph-based models.","Despite their effectiveness, we argue that they fail to explore the rich semantic information underlying the evidence with different structures.","To address this, we propose a novel word-level Heterogeneous-graph-based model for Fact Checking over unstructured and structured information, namely HeterFC.","Our approach leverages a heterogeneous evidence graph, with words as nodes and thoughtfully designed edges representing different evidence properties.","We perform information propagation via a relational graph neural network, facilitating interactions between claims and evidence.","An attention-based method is utilized to integrate information, combined with a language model for generating predictions.","We introduce a multitask loss function to account for potential inaccuracies in evidence retrieval.","Comprehensive experiments on the large fact checking dataset FEVEROUS demonstrate the effectiveness of HeterFC.","Code will be released at: https://github.com/Deno-V/HeterFC."],"url":"http://arxiv.org/abs/2402.13028v1","category":"cs.CL"}
{"created":"2024-02-20 14:08:36","title":"Dispersed Dyck paths revisited","abstract":"Dispersed Dyck paths are Dyck paths, with possible flat steps on level 0. We revisit and augment questions about them from the Encyclopedia of Integer Sequences, in a systematic way that uses generating functions and the kernel method.","sentences":["Dispersed Dyck paths are Dyck paths, with possible flat steps on level 0.","We revisit and augment questions about them from the Encyclopedia of Integer Sequences, in a systematic way that uses generating functions and the kernel method."],"url":"http://arxiv.org/abs/2402.13026v1","category":"math.CO"}
{"created":"2024-02-20 14:08:24","title":"CFEVER: A Chinese Fact Extraction and VERification Dataset","abstract":"We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification. CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia. Each claim in CFEVER is labeled as \"Supports\", \"Refutes\", or \"Not Enough Info\" to depict its degree of factualness. Similar to the FEVER dataset, claims in the \"Supports\" and \"Refutes\" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia. Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement. In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts. CFEVER is available at https://ikmlab.github.io/CFEVER.","sentences":["We present CFEVER, a Chinese dataset designed for Fact Extraction and VERification.","CFEVER comprises 30,012 manually created claims based on content in Chinese Wikipedia.","Each claim in CFEVER is labeled as \"Supports\", \"Refutes\", or \"Not Enough Info\" to depict its degree of factualness.","Similar to the FEVER dataset, claims in the \"Supports\" and \"Refutes\" categories are also annotated with corresponding evidence sentences sourced from single or multiple pages in Chinese Wikipedia.","Our labeled dataset holds a Fleiss' kappa value of 0.7934 for five-way inter-annotator agreement.","In addition, through the experiments with the state-of-the-art approaches developed on the FEVER dataset and a simple baseline for CFEVER, we demonstrate that our dataset is a new rigorous benchmark for factual extraction and verification, which can be further used for developing automated systems to alleviate human fact-checking efforts.","CFEVER is available at https://ikmlab.github.io/CFEVER."],"url":"http://arxiv.org/abs/2402.13025v1","category":"cs.CL"}
{"created":"2024-02-20 14:07:18","title":"SmartEx: A Framework for Generating User-Centric Explanations in Smart Environments","abstract":"Explainability is crucial for complex systems like pervasive smart environments, as they collect and analyze data from various sensors, follow multiple rules, and control different devices resulting in behavior that is not trivial and, thus, should be explained to the users. The current approaches, however, offer flat, static, and algorithm-focused explanations. User-centric explanations, on the other hand, consider the recipient and context, providing personalized and context-aware explanations. To address this gap, we propose an approach to incorporate user-centric explanations into smart environments. We introduce a conceptual model and a reference architecture for characterizing and generating such explanations. Our work is the first technical solution for generating context-aware and granular explanations in smart environments. Our architecture implementation demonstrates the feasibility of our approach through various scenarios.","sentences":["Explainability is crucial for complex systems like pervasive smart environments, as they collect and analyze data from various sensors, follow multiple rules, and control different devices resulting in behavior that is not trivial and, thus, should be explained to the users.","The current approaches, however, offer flat, static, and algorithm-focused explanations.","User-centric explanations, on the other hand, consider the recipient and context, providing personalized and context-aware explanations.","To address this gap, we propose an approach to incorporate user-centric explanations into smart environments.","We introduce a conceptual model and a reference architecture for characterizing and generating such explanations.","Our work is the first technical solution for generating context-aware and granular explanations in smart environments.","Our architecture implementation demonstrates the feasibility of our approach through various scenarios."],"url":"http://arxiv.org/abs/2402.13024v1","category":"cs.HC"}
{"created":"2024-02-20 14:02:45","title":"SoMeLVLM: A Large Vision Language Model for Social Media Processing","abstract":"The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks. The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks. In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge & comprehension, application, analysis, evaluation, and creation. SoMeLVLM is designed to understand and generate realistic social media behavior. We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model. Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks. Further analysis shows its significant advantages over baselines in terms of cognitive abilities.","sentences":["The growth of social media, characterized by its multimodal nature, has led to the emergence of diverse phenomena and challenges, which calls for an effective approach to uniformly solve automated tasks.","The powerful Large Vision Language Models make it possible to handle a variety of tasks simultaneously, but even with carefully designed prompting methods, the general domain models often fall short in aligning with the unique speaking style and context of social media tasks.","In this paper, we introduce a Large Vision Language Model for Social Media Processing (SoMeLVLM), which is a cognitive framework equipped with five key capabilities including knowledge & comprehension, application, analysis, evaluation, and creation.","SoMeLVLM is designed to understand and generate realistic social media behavior.","We have developed a 654k multimodal social media instruction-tuning dataset to support our cognitive framework and fine-tune our model.","Our experiments demonstrate that SoMeLVLM achieves state-of-the-art performance in multiple social media tasks.","Further analysis shows its significant advantages over baselines in terms of cognitive abilities."],"url":"http://arxiv.org/abs/2402.13022v1","category":"cs.CL"}
{"created":"2024-02-20 14:01:26","title":"Improving Neural-based Classification with Logical Background Knowledge","abstract":"Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems. This hybridization can take many shapes. In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge. We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected. We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization. We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network. We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets. Our results demonstrate that semantic conditioning at inference can be used to build more accurate neural-based systems with fewer resources while guaranteeing the semantic consistency of outputs.","sentences":["Neurosymbolic AI is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems.","This hybridization can take many shapes.","In this paper, we propose a new formalism for supervised multi-label classification with propositional background knowledge.","We introduce a new neurosymbolic technique called semantic conditioning at inference, which only constrains the system during inference while leaving the training unaffected.","We discuss its theoritical and practical advantages over two other popular neurosymbolic techniques: semantic conditioning and semantic regularization.","We develop a new multi-scale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network.","We then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets.","Our results demonstrate that semantic conditioning at inference can be used to build more accurate neural-based systems with fewer resources while guaranteeing the semantic consistency of outputs."],"url":"http://arxiv.org/abs/2402.13019v1","category":"cs.AI"}
{"created":"2024-02-20 14:00:53","title":"EMO-SUPERB: An In-depth Look at Speech Emotion Recognition","abstract":"Speech emotion recognition (SER) is a pivotal technology for human-computer interaction systems. However, 80.77% of SER papers yield results that cannot be reproduced. We develop EMO-SUPERB, short for EMOtion Speech Universal PERformance Benchmark, which aims to enhance open-source initiatives for SER. EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art speech self-supervised learning models (SSLMs) for exhaustive evaluation across six open-source SER datasets. EMO-SUPERB streamlines result sharing via an online leaderboard, fostering collaboration within a community-driven benchmark and thereby enhancing the development of SER. On average, 2.58% of annotations are annotated using natural language. SER relies on classification models and is unable to process natural languages, leading to the discarding of these valuable annotations. We prompt ChatGPT to mimic annotators, comprehend natural language annotations, and subsequently re-label the data. By utilizing labels generated by ChatGPT, we consistently achieve an average relative gain of 3.08% across all settings.","sentences":["Speech emotion recognition (SER) is a pivotal technology for human-computer interaction systems.","However, 80.77% of SER papers yield results that cannot be reproduced.","We develop EMO-SUPERB, short for EMOtion Speech Universal PERformance Benchmark, which aims to enhance open-source initiatives for SER.","EMO-SUPERB includes a user-friendly codebase to leverage 15 state-of-the-art speech self-supervised learning models (SSLMs) for exhaustive evaluation across six open-source SER datasets.","EMO-SUPERB streamlines result sharing via an online leaderboard, fostering collaboration within a community-driven benchmark and thereby enhancing the development of SER.","On average, 2.58% of annotations are annotated using natural language.","SER relies on classification models and is unable to process natural languages, leading to the discarding of these valuable annotations.","We prompt ChatGPT to mimic annotators, comprehend natural language annotations, and subsequently re-label the data.","By utilizing labels generated by ChatGPT, we consistently achieve an average relative gain of 3.08% across all settings."],"url":"http://arxiv.org/abs/2402.13018v1","category":"eess.AS"}
{"created":"2024-02-20 13:57:23","title":"An exact stationary axisymmetric vacuum solution within a metric--affine bumblebee gravity","abstract":"Within the framework of the spontaneous Lorentz symmetry breaking, we consider a metric--affine generalization of the gravitational sector of the Standard--Model Extension (SME), including the Lorentz--violating (LV) coefficients $u$ and $s^{\\mu\\nu}$. In this model, we derive the modified Einstein field equations in order to obtain a new axisymmetric vacuum spinning solution for a particular bumblebee's profile. Such a solution has the remarkable property of incorporating the effects of Lorentz symmetry breaking (LSB) through the LV dimensionless parameter $X=\\xi b^2$, as the LSB is turned off, $X=0$, we recover the well--established result, the Kerr solution, as expected. Afterwards, we calculate the geodesics, the radial acceleration and thermodynamic quantities for this new metric. We also estimate an upper bound for $X$ by using astrophysical data of the advance Mercury's perihelion.","sentences":["Within the framework of the spontaneous Lorentz symmetry breaking, we consider a metric--affine generalization of the gravitational sector of the Standard--Model Extension (SME), including the Lorentz--violating (LV) coefficients $u$ and","$s^{\\mu\\nu}$. In this model, we derive the modified Einstein field equations in order to obtain a new axisymmetric vacuum spinning solution for a particular bumblebee's profile.","Such a solution has the remarkable property of incorporating the effects of Lorentz symmetry breaking (LSB) through the LV dimensionless parameter $X=\\xi b^2$, as the LSB is turned off, $X=0$, we recover the well--established result, the Kerr solution, as expected.","Afterwards, we calculate the geodesics, the radial acceleration and thermodynamic quantities for this new metric.","We also estimate an upper bound for $X$ by using astrophysical data of the advance Mercury's perihelion."],"url":"http://arxiv.org/abs/2402.13014v1","category":"gr-qc"}
{"created":"2024-02-20 13:56:38","title":"Code Needs Comments: Enhancing Code LLMs with Comment Augmentation","abstract":"The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs). We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment. Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language. We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks. Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation.","sentences":["The programming skill is one crucial ability for Large Language Models (LLMs), necessitating a deep understanding of programming languages (PLs) and their correlation with natural languages (NLs).","We examine the impact of pre-training data on code-focused LLMs' performance by assessing the comment density as a measure of PL-NL alignment.","Given the scarcity of code-comment aligned data in pre-training corpora, we introduce a novel data augmentation method that generates comments for existing code, coupled with a data filtering strategy that filters out code data poorly correlated with natural language.","We conducted experiments on three code-focused LLMs and observed consistent improvements in performance on two widely-used programming skill benchmarks.","Notably, the model trained on the augmented data outperformed both the model used for generating comments and the model further trained on the data without augmentation."],"url":"http://arxiv.org/abs/2402.13013v1","category":"cs.CL"}
{"created":"2024-02-20 13:51:27","title":"An evolutionary game with reputation-based imitation-mutation dynamics","abstract":"Reputation plays a crucial role in social interactions by affecting the fitness of individuals during an evolutionary process. Previous works have extensively studied the result of imitation dynamics without focusing on potential irrational choices in strategy updates. We now fill this gap and explore the consequence of such kind of randomness, or one may interpret it as an autonomous thinking. In particular, we study how this extended dynamics alters the evolution of cooperation when individual reputation is directly linked to collected payoff, hence providing a general fitness function. For a broadly valid conclusion, our spatial populations cover different types of interaction topologies, including lattices, small-world and scale-free graphs. By means of intensive simulations we can detect substantial increase in cooperation level that shows a reasonable stability in the presence of a notable strategy mutation.","sentences":["Reputation plays a crucial role in social interactions by affecting the fitness of individuals during an evolutionary process.","Previous works have extensively studied the result of imitation dynamics without focusing on potential irrational choices in strategy updates.","We now fill this gap and explore the consequence of such kind of randomness, or one may interpret it as an autonomous thinking.","In particular, we study how this extended dynamics alters the evolution of cooperation when individual reputation is directly linked to collected payoff, hence providing a general fitness function.","For a broadly valid conclusion, our spatial populations cover different types of interaction topologies, including lattices, small-world and scale-free graphs.","By means of intensive simulations we can detect substantial increase in cooperation level that shows a reasonable stability in the presence of a notable strategy mutation."],"url":"http://arxiv.org/abs/2402.13011v1","category":"physics.soc-ph"}
{"created":"2024-02-20 13:42:36","title":"Improve Cross-Architecture Generalization on Dataset Distillation","abstract":"Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset. However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models. In response to this constraint, we propose a novel methodology termed \"model pool\". This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process. Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset. Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies.","sentences":["Dataset distillation, a pragmatic approach in machine learning, aims to create a smaller synthetic dataset from a larger existing dataset.","However, existing distillation methods primarily adopt a model-based paradigm, where the synthetic dataset inherits model-specific biases, limiting its generalizability to alternative models.","In response to this constraint, we propose a novel methodology termed \"model pool\".","This approach involves selecting models from a diverse model pool based on a specific probability distribution during the data distillation process.","Additionally, we integrate our model pool with the established knowledge distillation approach and apply knowledge distillation to the test process of the distilled dataset.","Our experimental results validate the effectiveness of the model pool approach across a range of existing models while testing, demonstrating superior performance compared to existing methodologies."],"url":"http://arxiv.org/abs/2402.13007v1","category":"cs.LG"}
{"created":"2024-02-20 13:41:21","title":"Investigating the Impact of Model Instability on Explanations and Uncertainty","abstract":"Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations. As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy. Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality. We artificially simulate epistemic uncertainty in text input by introducing noise at inference time. In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics. Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect. We find that high uncertainty doesn't necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during the training process. This suggests that noise-augmented models may be better at identifying salient tokens when uncertain. Furthermore, when predictive and epistemic uncertainty measures are over-confident, the robustness of a saliency map to perturbation can indicate model stability issues. Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance; however, this phenomenon is limited to smaller Transformer-based language models.","sentences":["Explainable AI methods facilitate the understanding of model behaviour, yet, small, imperceptible perturbations to inputs can vastly distort explanations.","As these explanations are typically evaluated holistically, before model deployment, it is difficult to assess when a particular explanation is trustworthy.","Some studies have tried to create confidence estimators for explanations, but none have investigated an existing link between uncertainty and explanation quality.","We artificially simulate epistemic uncertainty in text input by introducing noise at inference time.","In this large-scale empirical study, we insert different levels of noise perturbations and measure the effect on the output of pre-trained language models and different uncertainty metrics.","Realistic perturbations have minimal effect on performance and explanations, yet masking has a drastic effect.","We find that high uncertainty doesn't necessarily imply low explanation plausibility; the correlation between the two metrics can be moderately positive when noise is exposed during the training process.","This suggests that noise-augmented models may be better at identifying salient tokens when uncertain.","Furthermore, when predictive and epistemic uncertainty measures are over-confident, the robustness of a saliency map to perturbation can indicate model stability issues.","Integrated Gradients shows the overall greatest robustness to perturbation, while still showing model-specific patterns in performance; however, this phenomenon is limited to smaller Transformer-based language models."],"url":"http://arxiv.org/abs/2402.13006v1","category":"cs.LG"}
{"created":"2024-02-20 13:33:33","title":"Comparison of Conventional Hybrid and CTC/Attention Decoders for Continuous Visual Speech Recognition","abstract":"Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR). Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures. While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm. However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders. Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation. We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language. Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters.","sentences":["Thanks to the rise of deep learning and the availability of large-scale audio-visual databases, recent advances have been achieved in Visual Speech Recognition (VSR).","Similar to other speech processing tasks, these end-to-end VSR systems are usually based on encoder-decoder architectures.","While encoders are somewhat general, multiple decoding approaches have been explored, such as the conventional hybrid model based on Deep Neural Networks combined with Hidden Markov Models (DNN-HMM) or the Connectionist Temporal Classification (CTC) paradigm.","However, there are languages and tasks in which data is scarce, and in this situation, there is not a clear comparison between different types of decoders.","Therefore, we focused our study on how the conventional DNN-HMM decoder and its state-of-the-art CTC/Attention counterpart behave depending on the amount of data used for their estimation.","We also analyzed to what extent our visual speech features were able to adapt to scenarios for which they were not explicitly trained, either considering a similar dataset or another collected for a different language.","Results showed that the conventional paradigm reached recognition rates that improve the CTC/Attention model in data-scarcity scenarios along with a reduced training time and fewer parameters."],"url":"http://arxiv.org/abs/2402.13004v1","category":"cs.CV"}
{"created":"2024-02-20 13:31:12","title":"Generating Context-Aware Contrastive Explanations in Rule-based Systems","abstract":"Human explanations are often contrastive, meaning that they do not answer the indeterminate \"Why?\" question, but instead \"Why P, rather than Q?\". Automatically generating contrastive explanations is challenging because the contrastive event (Q) represents the expectation of a user in contrast to what happened. We present an approach that predicts a potential contrastive event in situations where a user asks for an explanation in the context of rule-based systems. Our approach analyzes a situation that needs to be explained and then selects the most likely rule a user may have expected instead of what the user has observed. This contrastive event is then used to create a contrastive explanation that is presented to the user. We have implemented the approach as a plugin for a home automation system and demonstrate its feasibility in four test scenarios.","sentences":["Human explanations are often contrastive, meaning that they do not answer the indeterminate \"Why?\" question, but instead \"Why P, rather than Q?\".","Automatically generating contrastive explanations is challenging because the contrastive event (Q) represents the expectation of a user in contrast to what happened.","We present an approach that predicts a potential contrastive event in situations where a user asks for an explanation in the context of rule-based systems.","Our approach analyzes a situation that needs to be explained and then selects the most likely rule a user may have expected instead of what the user has observed.","This contrastive event is then used to create a contrastive explanation that is presented to the user.","We have implemented the approach as a plugin for a home automation system and demonstrate its feasibility in four test scenarios."],"url":"http://arxiv.org/abs/2402.13000v1","category":"cs.SE"}
{"created":"2024-02-20 13:27:25","title":"Robust single divacancy defects near stacking faults in 4H-SiC under resonant excitation","abstract":"Color centers in silicon carbide (SiC) have demonstrated significant promise for quantum information processing. However, the undesirable ionization process that occurs during optical manipulation frequently causes fluctuations in the charge state and performance of these defects, thereby restricting the effectiveness of spin-photon interfaces. Recent predictions indicate that divacancy defects near stacking faults possess the capability to stabilize their neutral charge states, thereby providing robustness against photoionization effects. In this work, we present a comprehensive protocol for the scalable and targeted fabrication of single divacancy arrays in 4H-SiC using a high-resolution focused helium ion beam. Through photoluminescence emission (PLE) experiments, we demonstrate long-term emission stability with minimal linewidth shift ($\\sim$ 50 MHz over 3 hours) for the single c-axis divacancies within stacking faults. By measuring the ionization rate for different polytypes of divacancies, we found that the divacancies within stacking faults are more robust against resonant excitation. Additionally, angle-resolved PLE spectra reveal their two resonant-transition lines with mutually orthogonal polarizations. Notably, the PLE linewidths are approximately 7 times narrower and the spin-coherent times are 6 times longer compared to divacancies generated via carbon-ion implantation. These findings highlight the immense potential of SiC divacancies for on-chip quantum photonics and the construction of efficient spin-to-photon interfaces, indicating a significant step forward in the development of quantum technologies.","sentences":["Color centers in silicon carbide (SiC) have demonstrated significant promise for quantum information processing.","However, the undesirable ionization process that occurs during optical manipulation frequently causes fluctuations in the charge state and performance of these defects, thereby restricting the effectiveness of spin-photon interfaces.","Recent predictions indicate that divacancy defects near stacking faults possess the capability to stabilize their neutral charge states, thereby providing robustness against photoionization effects.","In this work, we present a comprehensive protocol for the scalable and targeted fabrication of single divacancy arrays in 4H-SiC using a high-resolution focused helium ion beam.","Through photoluminescence emission (PLE) experiments, we demonstrate long-term emission stability with minimal linewidth shift ($\\sim$ 50 MHz over 3 hours) for the single c-axis divacancies within stacking faults.","By measuring the ionization rate for different polytypes of divacancies, we found that the divacancies within stacking faults are more robust against resonant excitation.","Additionally, angle-resolved PLE spectra reveal their two resonant-transition lines with mutually orthogonal polarizations.","Notably, the PLE linewidths are approximately 7 times narrower and the spin-coherent times are 6 times longer compared to divacancies generated via carbon-ion implantation.","These findings highlight the immense potential of SiC divacancies for on-chip quantum photonics and the construction of efficient spin-to-photon interfaces, indicating a significant step forward in the development of quantum technologies."],"url":"http://arxiv.org/abs/2402.12999v1","category":"quant-ph"}
{"created":"2024-02-20 13:25:39","title":"Phonotactic Complexity across Dialects","abstract":"Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed.","sentences":["Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012).","We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties.","Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level.","A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity.","We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed."],"url":"http://arxiv.org/abs/2402.12998v1","category":"cs.CL"}
{"created":"2024-02-20 13:21:46","title":"An Autonomous Large Language Model Agent for Chemical Literature Data Mining","abstract":"Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency. The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry.","sentences":["Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare.","The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes.","Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields.","However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature.","To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature.","This AI agent employs large language models (LLMs) for prompt generation and iterative optimization.","It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance.","Our framework's efficacy is evaluated using accuracy, recall, and F1 score of reaction condition data, and we compared our method with human experts in terms of content correctness and time efficiency.","The proposed approach marks a significant advancement in automating chemical literature extraction and demonstrates the potential for AI to revolutionize data management and utilization in chemistry."],"url":"http://arxiv.org/abs/2402.12993v1","category":"cs.IR"}
{"created":"2024-02-20 13:21:27","title":"On gravito-inertial surface waves","abstract":"In geophysical environments, wave motions that are shaped by the action of gravity and global rotation bear the name of gravito-inertial waves. We present a geometrical description of gravito-inertial surface waves, which are low-frequency waves existing in the presence of a solid boundary. We consider an idealized fluid model for an incompressible fluid enclosed in a smooth compact three-dimensional domain, subject to a constant rotation vector. The fluid is also stratified in density under a constant Brunt-V{\\\"a}is{\\\"a}l{\\\"a} frequency. The spectral problem is formulated in terms of the pressure, which satisfies a Poincar\\'e equation within the domain, and a Kelvin equation on the boundary. The Poincar\\'e equation is elliptic when the wave frequency is small enough, such that we can use the Dirichlet-to-Neumann operator to reduce the Kelvin equation to a pseudo-differential equation on the boundary. We find that the wave energy is concentrated on the boundary for large covectors, and can exhibit surface wave attractors for generic domains. In an ellipsoid, we show that these waves are square-integrable and reduce to spherical harmonics on the boundary.","sentences":["In geophysical environments, wave motions that are shaped by the action of gravity and global rotation bear the name of gravito-inertial waves.","We present a geometrical description of gravito-inertial surface waves, which are low-frequency waves existing in the presence of a solid boundary.","We consider an idealized fluid model for an incompressible fluid enclosed in a smooth compact three-dimensional domain, subject to a constant rotation vector.","The fluid is also stratified in density under a constant Brunt-V{\\\"a}is{\\\"a}l{\\\"a} frequency.","The spectral problem is formulated in terms of the pressure, which satisfies a Poincar\\'e equation within the domain, and a Kelvin equation on the boundary.","The Poincar\\'e equation is elliptic when the wave frequency is small enough, such that we can use the Dirichlet-to-Neumann operator to reduce the Kelvin equation to a pseudo-differential equation on the boundary.","We find that the wave energy is concentrated on the boundary for large covectors, and can exhibit surface wave attractors for generic domains.","In an ellipsoid, we show that these waves are square-integrable and reduce to spherical harmonics on the boundary."],"url":"http://arxiv.org/abs/2402.12992v1","category":"math.AP"}
{"created":"2024-02-20 13:20:39","title":"TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification","abstract":"Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.","sentences":["Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them.","Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse.","In this context, we describe the novel problem of Black-box Identity Verification (BBIV).","The goal is to determine whether a third-party application uses a certain LLM through its chat function.","We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use.","We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers.","TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction.","TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function."],"url":"http://arxiv.org/abs/2402.12991v1","category":"cs.LG"}
{"created":"2024-02-20 13:17:37","title":"Towards Robust Graph Incremental Learning on Evolving Graphs","abstract":"Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once. This ability to learn incrementally from a stream of tasks is crucial for many real-world applications. However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL). This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added. In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks. We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the structural shift on catastrophic forgetting of the inductive NGIL problem. We show that the structural shift can lead to a shift in the input distribution for the existing tasks, and further lead to an increased risk of catastrophic forgetting. Through comprehensive empirical studies with several benchmark datasets, we demonstrate that our proposed method, Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to improve the performance of state-of-the-art GNN incremental learning frameworks in the inductive setting.","sentences":["Incremental learning is a machine learning approach that involves training a model on a sequence of tasks, rather than all tasks at once.","This ability to learn incrementally from a stream of tasks is crucial for many real-world applications.","However, incremental learning is a challenging problem on graph-structured data, as many graph-related problems involve prediction tasks for each individual node, known as Node-wise Graph Incremental Learning (NGIL).","This introduces non-independent and non-identically distributed characteristics in the sample data generation process, making it difficult to maintain the performance of the model as new tasks are added.","In this paper, we focus on the inductive NGIL problem, which accounts for the evolution of graph structure (structural shift) induced by emerging tasks.","We provide a formal formulation and analysis of the problem, and propose a novel regularization-based technique called Structural-Shift-Risk-Mitigation (SSRM) to mitigate the impact of the structural shift on catastrophic forgetting of the inductive NGIL problem.","We show that the structural shift can lead to a shift in the input distribution for the existing tasks, and further lead to an increased risk of catastrophic forgetting.","Through comprehensive empirical studies with several benchmark datasets, we demonstrate that our proposed method, Structural-Shift-Risk-Mitigation (SSRM), is flexible and easy to adapt to improve the performance of state-of-the-art GNN incremental learning frameworks in the inductive setting."],"url":"http://arxiv.org/abs/2402.12987v1","category":"cs.LG"}
{"created":"2024-02-20 13:13:13","title":"Can GNN be Good Adapter for LLMs?","abstract":"Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains. In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text. These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc. Thus, this paper explores how to utilize LLMs to model TAGs. Previous methods for TAG modeling are based on million-scale LMs. When scaled up to billion-scale LLMs, they face huge challenges in computational costs. Additionally, they also ignore the zero-shot inference capabilities of LLMs. Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs. In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs. The entire framework is trained using auto-regression on node text (next token prediction). Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks. Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5\\% in terms of node classification. Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2. The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling.","sentences":["Recently, large language models (LLMs) have demonstrated superior capabilities in understanding and zero-shot learning on textual data, promising significant advances for many text-related domains.","In the graph domain, various real-world scenarios also involve textual data, where tasks and node features can be described by text.","These text-attributed graphs (TAGs) have broad applications in social media, recommendation systems, etc.","Thus, this paper explores how to utilize LLMs to model TAGs.","Previous methods for TAG modeling are based on million-scale LMs.","When scaled up to billion-scale LLMs, they face huge challenges in computational costs.","Additionally, they also ignore the zero-shot inference capabilities of LLMs.","Therefore, we propose GraphAdapter, which uses a graph neural network (GNN) as an efficient adapter in collaboration with LLMs to tackle TAGs.","In terms of efficiency, the GNN adapter introduces only a few trainable parameters and can be trained with low computation costs.","The entire framework is trained using auto-regression on node text (next token prediction).","Once trained, GraphAdapter can be seamlessly fine-tuned with task-specific prompts for various downstream tasks.","Through extensive experiments across multiple real-world TAGs, GraphAdapter based on Llama 2 gains an average improvement of approximately 5\\% in terms of node classification.","Furthermore, GraphAdapter can also adapt to other language models, including RoBERTa, GPT-2.","The promising results demonstrate that GNNs can serve as effective adapters for LLMs in TAG modeling."],"url":"http://arxiv.org/abs/2402.12984v1","category":"cs.CL"}
{"created":"2024-02-20 13:03:47","title":"Fractional Boundary Value Problems and Elastic Sticky Brownian Motions, I: The half line","abstract":"We extend the results obtained in \\cite{Dov22} by introducing a new class of boundary value problems involving non-local dynamic boundary conditions. We focus on the problem to find a solution to a local problem on a domain $\\Omega$ with non-local dynamic conditions on the boundary $\\partial \\Omega$. Due to the pioneering nature of the present research, we propose here the apparently simple case of $\\Omega=(0, \\infty)$ with boundary $\\{0\\}$ of zero Lebesgue measure. Our results turn out to be instructive for the general case of boundary with positive (finite) Borel measures. Moreover, in our view, we bring new light to dynamic boundary value problems and the probabilistic description of the associated models.","sentences":["We extend the results obtained in \\cite{Dov22} by introducing a new class of boundary value problems involving non-local dynamic boundary conditions.","We focus on the problem to find a solution to a local problem on a domain $\\Omega$ with non-local dynamic conditions on the boundary $\\partial \\Omega$. Due to the pioneering nature of the present research, we propose here the apparently simple case of $\\Omega=(0, \\infty)$ with boundary $\\{0\\}$ of zero Lebesgue measure.","Our results turn out to be instructive for the general case of boundary with positive (finite) Borel measures.","Moreover, in our view, we bring new light to dynamic boundary value problems and the probabilistic description of the associated models."],"url":"http://arxiv.org/abs/2402.12982v1","category":"math.PR"}
{"created":"2024-02-20 13:02:51","title":"Efficient adjustment for complex covariates: Gaining efficiency with DOPE","abstract":"Covariate adjustment is a ubiquitous method used to estimate the average treatment effect (ATE) from observational data. Assuming a known graphical structure of the data generating model, recent results give graphical criteria for optimal adjustment, which enables efficient estimation of the ATE. However, graphical approaches are challenging for high-dimensional and complex data, and it is not straightforward to specify a meaningful graphical model of non-Euclidean data such as texts. We propose an general framework that accommodates adjustment for any subset of information expressed by the covariates. We generalize prior works and leverage these results to identify the optimal covariate information for efficient adjustment. This information is minimally sufficient for prediction of the outcome conditionally on treatment.   Based on our theoretical results, we propose the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of the ATE, and we provide asymptotic results for the DOPE under general conditions. Compared to the augmented inverse propensity weighted (AIPW) estimator, the DOPE can retain its efficiency even when the covariates are highly predictive of treatment. We illustrate this with a single-index model, and with an implementation of the DOPE based on neural networks, we demonstrate its performance on simulated and real data. Our results show that the DOPE provides an efficient and robust methodology for ATE estimation in various observational settings.","sentences":["Covariate adjustment is a ubiquitous method used to estimate the average treatment effect (ATE) from observational data.","Assuming a known graphical structure of the data generating model, recent results give graphical criteria for optimal adjustment, which enables efficient estimation of the ATE.","However, graphical approaches are challenging for high-dimensional and complex data, and it is not straightforward to specify a meaningful graphical model of non-Euclidean data such as texts.","We propose an general framework that accommodates adjustment for any subset of information expressed by the covariates.","We generalize prior works and leverage these results to identify the optimal covariate information for efficient adjustment.","This information is minimally sufficient for prediction of the outcome conditionally on treatment.   ","Based on our theoretical results, we propose the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of the ATE, and we provide asymptotic results for the DOPE under general conditions.","Compared to the augmented inverse propensity weighted (AIPW) estimator, the DOPE can retain its efficiency even when the covariates are highly predictive of treatment.","We illustrate this with a single-index model, and with an implementation of the DOPE based on neural networks, we demonstrate its performance on simulated and real data.","Our results show that the DOPE provides an efficient and robust methodology for ATE estimation in various observational settings."],"url":"http://arxiv.org/abs/2402.12980v1","category":"math.ST"}
{"created":"2024-02-20 13:00:40","title":"Adiabatic gauge potential and integrability breaking with free fermions","abstract":"We revisit the problem of integrability breaking in free fermionic quantum spin chains. We investigate the so-called adiabatic gauge potential (AGP), which was recently proposed as an accurate probe of quantum chaos. We also study the so-called weak integrability breaking, which occurs if the dynamical effects of the perturbation do not appear at leading order in the perturbing parameter. A recent statement in the literature claimed that integrability breaking should generally lead to an exponential growth of the AGP norm with respect to the volume. However, afterwards it was found that weak integrability breaking is a counter-example, leading to a cross-over between polynomial and exponential growth. Here we show that in free fermionic systems the AGP norm always grows polynomially, if the perturbation is local with respect to the fermions, even if the perturbation strongly breaks integrability. As a by-product of our computations we also find, that in free fermionic spin chains there are operators which weakly break integrability, but which are not associated with known long range deformations.","sentences":["We revisit the problem of integrability breaking in free fermionic quantum spin chains.","We investigate the so-called adiabatic gauge potential (AGP), which was recently proposed as an accurate probe of quantum chaos.","We also study the so-called weak integrability breaking, which occurs if the dynamical effects of the perturbation do not appear at leading order in the perturbing parameter.","A recent statement in the literature claimed that integrability breaking should generally lead to an exponential growth of the AGP norm with respect to the volume.","However, afterwards it was found that weak integrability breaking is a counter-example, leading to a cross-over between polynomial and exponential growth.","Here we show that in free fermionic systems the AGP norm always grows polynomially, if the perturbation is local with respect to the fermions, even if the perturbation strongly breaks integrability.","As a by-product of our computations we also find, that in free fermionic spin chains there are operators which weakly break integrability, but which are not associated with known long range deformations."],"url":"http://arxiv.org/abs/2402.12979v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-20 12:53:31","title":"The Impact of Demonstrations on Multilingual In-Context Learning: A Multidimensional Analysis","abstract":"In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates. Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context. To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages. Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages. We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations. Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether. These findings show that the importance of demonstrations might be overestimated. Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning.","sentences":["In-context learning is a popular inference strategy where large language models solve a task using only a few labelled demonstrations without needing any parameter updates.","Compared to work on monolingual (English) in-context learning, multilingual in-context learning is under-explored, and we lack an in-depth understanding of the role of demonstrations in this context.","To address this gap, we conduct a multidimensional analysis of multilingual in-context learning, experimenting with 5 models from different model families, 9 datasets covering classification and generation tasks, and 56 typologically diverse languages.","Our results reveal that the effectiveness of demonstrations varies significantly across models, tasks, and languages.","We also find that Llama 2-Chat, GPT-3.5, and GPT-4 are largely insensitive to the quality of demonstrations.","Instead, a carefully crafted template often eliminates the benefits of demonstrations for some tasks and languages altogether.","These findings show that the importance of demonstrations might be overestimated.","Our work highlights the need for granular evaluation across multiple axes towards a better understanding of in-context learning."],"url":"http://arxiv.org/abs/2402.12976v1","category":"cs.CL"}
{"created":"2024-02-20 12:51:38","title":"Besov Regularity of Weak solutions to a Class of Nonlinear Elliptic Equations","abstract":"In this article, we study a Besov regularity estimate of weak solutions to a class of nonlinear elliptic equations in divergence form. The main purpose is to establish Calderon-Zygmund type estimate in Besov spaces with more general assumptions on coefficients, non-homogeneous term and integrable index. By involving the Sharp maximal function, we establish an oscillation estimate of weak solutions in Orlicz-Sobolev spaces. By deriving a higher integrability estimate of weak solutions, we obtain the desired regularity estimate which expands the Calderon-Zygmund theory for nonlinear elliptic equations.","sentences":["In this article, we study a Besov regularity estimate of weak solutions to a class of nonlinear elliptic equations in divergence form.","The main purpose is to establish Calderon-Zygmund type estimate in Besov spaces with more general assumptions on coefficients, non-homogeneous term and integrable index.","By involving the Sharp maximal function, we establish an oscillation estimate of weak solutions in Orlicz-Sobolev spaces.","By deriving a higher integrability estimate of weak solutions, we obtain the desired regularity estimate which expands the Calderon-Zygmund theory for nonlinear elliptic equations."],"url":"http://arxiv.org/abs/2402.12975v1","category":"math.AP"}
{"created":"2024-02-20 12:51:17","title":"Visual Style Prompting with Swapping Self-Attention","abstract":"In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation. Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage. To address these challenges, we propose a novel approach, \\ours, to produce a diverse range of images while maintaining specific style elements and nuances. During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers. This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style. Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately. Our project page is available \\href{https://curryjung.github.io/VisualStylePrompt/}{here}.","sentences":["In the evolving domain of text-to-image generation, diffusion models have emerged as powerful tools in content creation.","Despite their remarkable capability, existing models still face challenges in achieving controlled generation with a consistent style, requiring costly fine-tuning or often inadequately transferring the visual elements due to content leakage.","To address these challenges, we propose a novel approach, \\ours, to produce a diverse range of images while maintaining specific style elements and nuances.","During the denoising process, we keep the query from original features while swapping the key and value with those from reference features in the late self-attention layers.","This approach allows for the visual style prompting without any fine-tuning, ensuring that generated images maintain a faithful style.","Through extensive evaluation across various styles and text prompts, our method demonstrates superiority over existing approaches, best reflecting the style of the references and ensuring that resulting images match the text prompts most accurately.","Our project page is available \\href{https://curryjung.github.io/VisualStylePrompt/}{here}."],"url":"http://arxiv.org/abs/2402.12974v1","category":"cs.CV"}
{"created":"2024-02-20 12:38:40","title":"See Further Than CFAR: a Data-Driven Radar Detector Trained by Lidar","abstract":"In this paper, we address the limitations of traditional constant false alarm rate (CFAR) target detectors in automotive radars, particularly in complex urban environments with multiple objects that appear as extended targets. We propose a data-driven radar target detector exploiting a highly efficient 2D CNN backbone inspired by the computer vision domain. Our approach is distinguished by a unique cross sensor supervision pipeline, enabling it to learn exclusively from unlabeled synchronized radar and lidar data, thus eliminating the need for costly manual object annotations. Using a novel large-scale, real-life multi-sensor dataset recorded in various driving scenarios, we demonstrate that the proposed detector generates dense, lidar-like point clouds, achieving a lower Chamfer distance to the reference lidar point clouds than CFAR detectors. Overall, it significantly outperforms CFAR baselines detection accuracy.","sentences":["In this paper, we address the limitations of traditional constant false alarm rate (CFAR) target detectors in automotive radars, particularly in complex urban environments with multiple objects that appear as extended targets.","We propose a data-driven radar target detector exploiting a highly efficient 2D CNN backbone inspired by the computer vision domain.","Our approach is distinguished by a unique cross sensor supervision pipeline, enabling it to learn exclusively from unlabeled synchronized radar and lidar data, thus eliminating the need for costly manual object annotations.","Using a novel large-scale, real-life multi-sensor dataset recorded in various driving scenarios, we demonstrate that the proposed detector generates dense, lidar-like point clouds, achieving a lower Chamfer distance to the reference lidar point clouds than CFAR detectors.","Overall, it significantly outperforms CFAR baselines detection accuracy."],"url":"http://arxiv.org/abs/2402.12970v1","category":"eess.SP"}
{"created":"2024-02-20 12:36:40","title":"Gl\u00f3rIA -- A Generative and Open Large Language Model for Portuguese","abstract":"Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs). These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language. Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese. We introduce Gl\\'orIA, a robust European Portuguese decoder LLM. To pre-train Gl\\'orIA, we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources. We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks. Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark. Evaluation shows that Gl\\'orIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text. The model also exhibits strong potential for various downstream tasks.","sentences":["Significant strides have been made in natural language tasks, largely attributed to the emergence of powerful large language models (LLMs).","These models, pre-trained on extensive and diverse corpora, have become increasingly capable of comprehending the intricacies of language.","Despite the abundance of LLMs for many high-resource languages, the availability of such models remains limited for European Portuguese.","We introduce Gl\\'orIA, a robust European Portuguese decoder LLM.","To pre-train Gl\\'orIA",", we assembled a comprehensive PT-PT text corpus comprising 35 billion tokens from various sources.","We present our pre-training methodology, followed by an assessment of the model's effectiveness on multiple downstream tasks.","Additionally, to evaluate our models' language modeling capabilities, we introduce CALAME-PT (Context-Aware LAnguage Modeling Evaluation for Portuguese), the first Portuguese zero-shot language-modeling benchmark.","Evaluation shows that Gl\\'orIA significantly outperforms existing open PT decoder models in language modeling and that it can generate sound, knowledge-rich, and coherent PT-PT text.","The model also exhibits strong potential for various downstream tasks."],"url":"http://arxiv.org/abs/2402.12969v1","category":"cs.CL"}
{"created":"2024-02-20 12:33:57","title":"High Schmidt number concentration in quantum bound entangled states","abstract":"A deep understanding of quantum entanglement is vital for advancing quantum technologies. The strength of entanglement can be quantified by counting the degrees of freedom that are entangled, which results in a quantity called Schmidt number. A particular challenge is to identify the strength of entanglement in quantum states which remain positive under partial transpose (PPT), otherwise recognized as undistillable states. Finding PPT states with high Schmidt number has become a mathematical and computational challenge. In this work, we introduce efficient analytical tools for calculating the Schmidt number for a class of bipartite states, called generalized grid states. Our methods improve the best known bounds for PPT states with high Schmidt number. Most notably, we construct a Schmidt number three PPT state in five dimensional systems and a family of states with a Schmidt number of $(d+1)/2$ for odd $d$-dimensional systems, representing the best-known scaling of the Schmidt number in a local dimension. Additionally, these states possess intriguing geometrical properties, which we utilize to construct indecomposable entanglement witnesses.","sentences":["A deep understanding of quantum entanglement is vital for advancing quantum technologies.","The strength of entanglement can be quantified by counting the degrees of freedom that are entangled, which results in a quantity called Schmidt number.","A particular challenge is to identify the strength of entanglement in quantum states which remain positive under partial transpose (PPT), otherwise recognized as undistillable states.","Finding PPT states with high Schmidt number has become a mathematical and computational challenge.","In this work, we introduce efficient analytical tools for calculating the Schmidt number for a class of bipartite states, called generalized grid states.","Our methods improve the best known bounds for PPT states with high Schmidt number.","Most notably, we construct a Schmidt number three PPT state in five dimensional systems and a family of states with a Schmidt number of $(d+1)/2$ for odd $d$-dimensional systems, representing the best-known scaling of the Schmidt number in a local dimension.","Additionally, these states possess intriguing geometrical properties, which we utilize to construct indecomposable entanglement witnesses."],"url":"http://arxiv.org/abs/2402.12966v1","category":"quant-ph"}
{"created":"2024-02-20 12:31:13","title":"Non-facial exposedness of copositive cones over symmetric cones","abstract":"In this paper, we consider copositive cones over symmetric cones and show that they are never facially exposed when the underlying cone has dimension at least 2. We do so by explicitly exhibiting a non-exposed extreme ray. Our result extends the known fact that the cone of copositive matrices over the nonnegative orthant is not facially exposed in general.","sentences":["In this paper, we consider copositive cones over symmetric cones and show that they are never facially exposed when the underlying cone has dimension at least 2.","We do so by explicitly exhibiting a non-exposed extreme ray.","Our result extends the known fact that the cone of copositive matrices over the nonnegative orthant is not facially exposed in general."],"url":"http://arxiv.org/abs/2402.12964v1","category":"math.OC"}
{"created":"2024-02-20 12:31:01","title":"Bivariant operadic categories","abstract":"We develop a self-dual, bivariant extension of the concept of an operadic category, its associated operads and their algebras. Our new theory covers, besides all classical subjects, also generalized traces and bivariant versions of Kapranov's charades. It is, moreover, combinatorially rich and aesthetically pleasing.","sentences":["We develop a self-dual, bivariant extension of the concept of an operadic category, its associated operads and their algebras.","Our new theory covers, besides all classical subjects, also generalized traces and bivariant versions of Kapranov's charades.","It is, moreover, combinatorially rich and aesthetically pleasing."],"url":"http://arxiv.org/abs/2402.12963v1","category":"math.CT"}
{"created":"2024-02-20 12:25:26","title":"Prompt Stealing Attacks Against Large Language Models","abstract":"The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs. With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge. In this paper, we propose a novel attack against LLMs, named prompt stealing attacks. Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers. The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction. The goal of the parameter extractor is to figure out the properties of the original prompts. We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt. Our parameter extractor first tries to distinguish the type of prompts based on the generated answers. Then, it can further predict which role or how many contexts are used based on the types of prompts. Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features. The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts. Our experimental results show the remarkable performance of our proposed attacks. Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs.","sentences":["The increasing reliance on large language models (LLMs) such as ChatGPT in various fields emphasizes the importance of ``prompt engineering,'' a technology to improve the quality of model outputs.","With companies investing significantly in expert prompt engineers and educational resources rising to meet market demand, designing high-quality prompts has become an intriguing challenge.","In this paper, we propose a novel attack against LLMs, named prompt stealing attacks.","Our proposed prompt stealing attack aims to steal these well-designed prompts based on the generated answers.","The prompt stealing attack contains two primary modules: the parameter extractor and the prompt reconstruction.","The goal of the parameter extractor is to figure out the properties of the original prompts.","We first observe that most prompts fall into one of three categories: direct prompt, role-based prompt, and in-context prompt.","Our parameter extractor first tries to distinguish the type of prompts based on the generated answers.","Then, it can further predict which role or how many contexts are used based on the types of prompts.","Following the parameter extractor, the prompt reconstructor can be used to reconstruct the original prompts based on the generated answers and the extracted features.","The final goal of the prompt reconstructor is to generate the reversed prompts, which are similar to the original prompts.","Our experimental results show the remarkable performance of our proposed attacks.","Our proposed attacks add a new dimension to the study of prompt engineering and call for more attention to the security issues on LLMs."],"url":"http://arxiv.org/abs/2402.12959v1","category":"cs.CR"}
{"created":"2024-02-20 12:22:59","title":"Go Static: Contextualized Logging Statement Generation","abstract":"Logging practices have been extensively investigated to assist developers in writing appropriate logging statements for documenting software behaviors. Although numerous automatic logging approaches have been proposed, their performance remains unsatisfactory due to the constraint of the single-method input, without informative programming context outside the method. Specifically, we identify three inherent limitations with single-method context: limited static scope of logging statements, inconsistent logging styles, and missing type information of logging variables. To tackle these limitations, we propose SCLogger, the first contextualized logging statement generation approach with inter-method static contexts. First, SCLogger extracts inter-method contexts with static analysis to construct the contextualized prompt for language models to generate a tentative logging statement. The contextualized prompt consists of an extended static scope and sampled similar methods, ordered by the chain-of-thought (COT) strategy. Second, SCLogger refines the access of logging variables by formulating a new refinement prompt for language models, which incorporates detailed type information of variables in the tentative logging statement. The evaluation results show that SCLogger surpasses the state-of-the-art approach by 8.7% in logging position accuracy, 32.1% in level accuracy, 19.6% in variable precision, and 138.4% in text BLEU-4 score. Furthermore, SCLogger consistently boosts the performance of logging statement generation across a range of large language models, thereby showcasing the generalizability of this approach.","sentences":["Logging practices have been extensively investigated to assist developers in writing appropriate logging statements for documenting software behaviors.","Although numerous automatic logging approaches have been proposed, their performance remains unsatisfactory due to the constraint of the single-method input, without informative programming context outside the method.","Specifically, we identify three inherent limitations with single-method context: limited static scope of logging statements, inconsistent logging styles, and missing type information of logging variables.","To tackle these limitations, we propose SCLogger, the first contextualized logging statement generation approach with inter-method static contexts.","First, SCLogger extracts inter-method contexts with static analysis to construct the contextualized prompt for language models to generate a tentative logging statement.","The contextualized prompt consists of an extended static scope and sampled similar methods, ordered by the chain-of-thought (COT) strategy.","Second, SCLogger refines the access of logging variables by formulating a new refinement prompt for language models, which incorporates detailed type information of variables in the tentative logging statement.","The evaluation results show that SCLogger surpasses the state-of-the-art approach by 8.7% in logging position accuracy, 32.1% in level accuracy, 19.6% in variable precision, and 138.4% in text BLEU-4 score.","Furthermore, SCLogger consistently boosts the performance of logging statement generation across a range of large language models, thereby showcasing the generalizability of this approach."],"url":"http://arxiv.org/abs/2402.12958v1","category":"cs.SE"}
{"created":"2024-02-20 12:22:10","title":"Hamilton-Jacobi-Bellman equations for Rydberg-blockade processes","abstract":"We discuss time-optimal control problems for two setups involving globally driven Rydberg atoms in the blockade limit by deriving the associated Hamilton-Jacobi-Bellman equations. From these equations, we extract the globally optimal trajectories and the corresponding controls for several target processes of the atomic system, using a generalized method of characteristics. We apply this method to retrieve known results for CZ and C-phase gates, and to find new optimal pulses for all elementary processes involved in the universal quantum computation scheme introduced in [Physical Review Letters 131, 170601 (2023)].","sentences":["We discuss time-optimal control problems for two setups involving globally driven Rydberg atoms in the blockade limit by deriving the associated Hamilton-Jacobi-Bellman equations.","From these equations, we extract the globally optimal trajectories and the corresponding controls for several target processes of the atomic system, using a generalized method of characteristics.","We apply this method to retrieve known results for CZ and C-phase gates, and to find new optimal pulses for all elementary processes involved in the universal quantum computation scheme introduced in [Physical Review Letters 131, 170601 (2023)]."],"url":"http://arxiv.org/abs/2402.12956v1","category":"quant-ph"}
{"created":"2024-02-20 12:17:01","title":"Conditional Logical Message Passing Transformer for Complex Query Answering","abstract":"Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task. Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning. However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously. Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors. While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph. In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear. In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type. We empirically verified that this approach can reduce computational costs without affecting performance. Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding. Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements. Experimental results show that CLMPT is a new state-of-the-art neural CQA model.","sentences":["Complex Query Answering (CQA) over Knowledge Graphs (KGs) is a challenging task.","Given that KGs are usually incomplete, neural models are proposed to solve CQA by performing multi-hop logical reasoning.","However, most of them cannot perform well on both one-hop and multi-hop queries simultaneously.","Recent work proposes a logical message passing mechanism based on the pre-trained neural link predictors.","While effective on both one-hop and multi-hop queries, it ignores the difference between the constant and variable nodes in a query graph.","In addition, during the node embedding update stage, this mechanism cannot dynamically measure the importance of different messages, and whether it can capture the implicit logical dependencies related to a node and received messages remains unclear.","In this paper, we propose Conditional Logical Message Passing Transformer (CLMPT), which considers the difference between constants and variables in the case of using pre-trained neural link predictors and performs message passing conditionally on the node type.","We empirically verified that this approach can reduce computational costs without affecting performance.","Furthermore, CLMPT uses the transformer to aggregate received messages and update the corresponding node embedding.","Through the self-attention mechanism, CLMPT can assign adaptive weights to elements in an input set consisting of received messages and the corresponding node and explicitly model logical dependencies between various elements.","Experimental results show that CLMPT is a new state-of-the-art neural CQA model."],"url":"http://arxiv.org/abs/2402.12954v1","category":"cs.LG"}
{"created":"2024-02-20 12:11:28","title":"QuanTest: Entanglement-Guided Testing of Quantum Neural Network Systems","abstract":"Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration. Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems. There is an urgent need for ways to test their correctness and security. However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing. These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons. In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems. We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs. Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement sufficiency and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples. Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48%-96.05% more test samples than the random noise under the same perturbation size constraints). The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%).","sentences":["Quantum Neural Network (QNN) combines the Deep Learning (DL) principle with the fundamental theory of quantum mechanics to achieve machine learning tasks with quantum acceleration.","Recently, QNN systems have been found to manifest robustness issues similar to classical DL systems.","There is an urgent need for ways to test their correctness and security.","However, QNN systems differ significantly from traditional quantum software and classical DL systems, posing critical challenges for QNN testing.","These challenges include the inapplicability of traditional quantum software testing methods, the dependence of quantum test sample generation on perturbation operators, and the absence of effective information in quantum neurons.","In this paper, we propose QuanTest, a quantum entanglement-guided adversarial testing framework to uncover potential erroneous behaviors in QNN systems.","We design a quantum entanglement adequacy criterion to quantify the entanglement acquired by the input quantum states from the QNN system, along with two similarity metrics to measure the proximity of generated quantum adversarial examples to the original inputs.","Subsequently, QuanTest formulates the problem of generating test inputs that maximize the quantum entanglement sufficiency and capture incorrect behaviors of the QNN system as a joint optimization problem and solves it in a gradient-based manner to generate quantum adversarial examples.","Experimental results demonstrate that QuanTest possesses the capability to capture erroneous behaviors in QNN systems (generating 67.48%-96.05% more test samples than the random noise under the same perturbation size constraints).","The entanglement-guided approach proves effective in adversarial testing, generating more adversarial examples (maximum increase reached 21.32%)."],"url":"http://arxiv.org/abs/2402.12950v1","category":"cs.SE"}
{"created":"2024-02-20 12:11:09","title":"Quantum degeneracy in mesoscopic matter: Casimir effect and Bose-Einstein condensation","abstract":"The ground-state phonon pressure is an analogue to the famous Casimir pressure of vacuum produced by zero-point photons. The acoustic Casimir forces are, however, many orders of magnitude weaker than the electromagnetic Casimir forces, as the typical speed of sound is 100 000 times smaller than the speed of light. Because of its weakness, zero-point acoustic Casimir pressure was never observed, although the pressure of artificially introduced sound noise on a narrow aperture has been reported. However, the magnitude of Casimir pressure increases as $1/L^3$ with the decrease of the sample size $L$, and reaches picoNewtons in the sub-micron scales. We demonstrate and measure the acoustic Casimir pressure induced by zero-point phonons in solid helium adsorbed on a carbon nanotube. We have also observed Casimir-like \"pushing out\" thermal phonons with the decreasing temperature or the length. We also show that all thermodynamic quantities are size-dependent, and therefore in the mesoscopic range $L\\lesssim\\hbar{c}/(k_BT)$ quadruple points are possible on the phase diagram where four different phases coexist. Due to the smallness of solid helium sample, temperature of Bose-Einstein condensation (BEC) of vacancies is relatively high, $10-100$ mK. This allowed us to experimentally discover the BEC in a system of zero-point vacancies, predicted more than 50 years ago.","sentences":["The ground-state phonon pressure is an analogue to the famous Casimir pressure of vacuum produced by zero-point photons.","The acoustic Casimir forces are, however, many orders of magnitude weaker than the electromagnetic Casimir forces, as the typical speed of sound is 100 000 times smaller than the speed of light.","Because of its weakness, zero-point acoustic Casimir pressure was never observed, although the pressure of artificially introduced sound noise on a narrow aperture has been reported.","However, the magnitude of Casimir pressure increases as $1/L^3$ with the decrease of the sample size $L$, and reaches picoNewtons in the sub-micron scales.","We demonstrate and measure the acoustic Casimir pressure induced by zero-point phonons in solid helium adsorbed on a carbon nanotube.","We have also observed Casimir-like \"pushing out\" thermal phonons with the decreasing temperature or the length.","We also show that all thermodynamic quantities are size-dependent, and therefore in the mesoscopic range $L\\lesssim\\hbar{c}/(k_BT)$ quadruple points are possible on the phase diagram where four different phases coexist.","Due to the smallness of solid helium sample, temperature of Bose-Einstein condensation (BEC) of vacancies is relatively high, $10-100$ mK. This allowed us to experimentally discover the BEC in a system of zero-point vacancies, predicted more than 50 years ago."],"url":"http://arxiv.org/abs/2402.12949v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 12:05:47","title":"GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick","abstract":"Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing other decoding-based watermarking methods by a minimum of 0.1.","sentences":["Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty.","Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability.","However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience.","To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity.","Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing other decoding-based watermarking methods by a minimum of 0.1."],"url":"http://arxiv.org/abs/2402.12948v1","category":"cs.CL"}
{"created":"2024-02-20 11:50:50","title":"Discovering Behavioral Modes in Deep Reinforcement Learning Policies Using Trajectory Clustering in Latent Space","abstract":"Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability. However, the complexity of their policies often makes them challenging to understand. In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks. Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task. Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements. We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space.","sentences":["Understanding the behavior of deep reinforcement learning (DRL) agents is crucial for improving their performance and reliability.","However, the complexity of their policies often makes them challenging to understand.","In this paper, we introduce a new approach for investigating the behavior modes of DRL policies, which involves utilizing dimensionality reduction and trajectory clustering in the latent space of neural networks.","Specifically, we use Pairwise Controlled Manifold Approximation Projection (PaCMAP) for dimensionality reduction and TRACLUS for trajectory clustering to analyze the latent space of a DRL policy trained on the Mountain Car control task.","Our methodology helps identify diverse behavior patterns and suboptimal choices by the policy, thus allowing for targeted improvements.","We demonstrate how our approach, combined with domain knowledge, can enhance a policy's performance in specific regions of the state space."],"url":"http://arxiv.org/abs/2402.12939v1","category":"cs.LG"}
{"created":"2024-02-20 11:30:57","title":"A Direct Real-Time Observation of Anion Intercalation in Graphite Process and Its Fully Reversibility by SAXS/WAXS Techniques","abstract":"The process of anion intercalation in graphite and its reversibility plays a crucial role in the next generation energy-storage devices. Herein the reaction mechanism of the aluminum graphite dual ion cell by operando X-ray scattering from small angles to wide angles is investigated. The staging behavior of the graphite intercalation compound (GIC) formation, its phase transitions, and its reversible process are observed for the first time by directly measuring the repeated intercalation distance, along with the microporosity of the cathode graphite. The investigation demonstrates complete reversibility of the electrochemical intercalation process, alongside nano- and micro-structural reorganization of natural graphite induced by intercalation. This work represents a new insight into thermodynamic aspects taking place during intermediate phase transitions in the GIC formation.","sentences":["The process of anion intercalation in graphite and its reversibility plays a crucial role in the next generation energy-storage devices.","Herein the reaction mechanism of the aluminum graphite dual ion cell by operando X-ray scattering from small angles to wide angles is investigated.","The staging behavior of the graphite intercalation compound (GIC) formation, its phase transitions, and its reversible process are observed for the first time by directly measuring the repeated intercalation distance, along with the microporosity of the cathode graphite.","The investigation demonstrates complete reversibility of the electrochemical intercalation process, alongside nano- and micro-structural reorganization of natural graphite induced by intercalation.","This work represents a new insight into thermodynamic aspects taking place during intermediate phase transitions in the GIC formation."],"url":"http://arxiv.org/abs/2402.12932v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 11:28:50","title":"A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence","abstract":"By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development.","sentences":["By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic.","However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers.","In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives.","First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically.","To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews.","Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords.","Second, based on these indicators, the study presents comparative analyses of different reviews, unveiling the characteristics of publications across various fields, periods, and journals.","The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in several aspects.","Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews.","This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews.","Finally, this Analysis offers insights into the current challenges of literature reviews and envisions future directions for their development."],"url":"http://arxiv.org/abs/2402.12928v1","category":"cs.DL"}
{"created":"2024-02-20 11:26:42","title":"CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection","abstract":"The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.","sentences":["The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content.","As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes.","In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection.","Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection.","However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial.","Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k).","To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios.","This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools."],"url":"http://arxiv.org/abs/2402.12927v1","category":"cs.CV"}
{"created":"2024-02-20 11:21:40","title":"Generalized tableaux over arbitrary digraphs and their associated differential equations","abstract":"We revisit the concepts of acyclic orderings and number of acyclic orderings of acyclic digraphs in terms of dispositions and counters for arbitrary multidigraphs. We prove that when we add a sequence of nested directed paths to a directed graph there is a unique polynomial such that the generatrix function of the family of counters is the product of the polynomial and the exponential function. We give an application, by considering a kind of digraphs arranged in rows introduced by the authors in a previous paper, called dispositional digraphs, in the particular case in which the digraph has two rows, to obtain new families of linear differential equations of small order whose coefficients are polynomials of small degree which admit polynomial solutions. In particular, we obtain a new differential equation associated to Catalan numbers, and the corresponding associated polynomials, which are solution of this differential equation; we term them Catalan differencial equation and Catalan polynomials, respectively. We prove that the Catalan polynomials obtained when we connect the directed path to the second vertex of the lower row of the digraph are orthogonal polynomials for an appropriate weight function. We characterize the digraphs that maximize the counter of connected dispositional digraphs and we find a new differential equation associated to these digraphs. We introduce also dispositions and counters in any multidigraph with non-strict inequalities in the dispositions, and we find new differential equations associated to some of them.","sentences":["We revisit the concepts of acyclic orderings and number of acyclic orderings of acyclic digraphs in terms of dispositions and counters for arbitrary multidigraphs.","We prove that when we add a sequence of nested directed paths to a directed graph there is a unique polynomial such that the generatrix function of the family of counters is the product of the polynomial and the exponential function.","We give an application, by considering a kind of digraphs arranged in rows introduced by the authors in a previous paper, called dispositional digraphs, in the particular case in which the digraph has two rows, to obtain new families of linear differential equations of small order whose coefficients are polynomials of small degree which admit polynomial solutions.","In particular, we obtain a new differential equation associated to Catalan numbers, and the corresponding associated polynomials, which are solution of this differential equation; we term them Catalan differencial equation and Catalan polynomials, respectively.","We prove that the Catalan polynomials obtained when we connect the directed path to the second vertex of the lower row of the digraph are orthogonal polynomials for an appropriate weight function.","We characterize the digraphs that maximize the counter of connected dispositional digraphs and we find a new differential equation associated to these digraphs.","We introduce also dispositions and counters in any multidigraph with non-strict inequalities in the dispositions, and we find new differential equations associated to some of them."],"url":"http://arxiv.org/abs/2402.12926v1","category":"math.CO"}
{"created":"2024-02-20 11:20:50","title":"Quantum graphs and microwave networks as narrow band filters for quantum and microwave devices","abstract":"We investigate properties of the transmission amplitude of quantum graphs and microwave networks composed of regular polygons such as triangles and squares. We show that for the graphs composed of regular polygons with the edges of the length $l$ the transmission amplitude displays a band of transmission suppression with some narrow peaks of full transmission. The peaks are distributed symmetrically with respect to the symmetry axis $kl=\\pi$, where $k$ is the wave vector. For microwave networks the transmission peak amplitudes are reduced and their symmetry is broken due to the influence of internal absorption. We demonstrate that for the graphs composed of the same polygons but separated by the edges of length $l' < l$ the transmission spectrum is generally not symmetric according to the axis $kl'=\\pi$. We also show that graphs composed of regular polygons of different size with the edges being irrational numbers are not fully chaotic and their level spacing distribution and the spectral rigidity are well described by the Berry-Robnik distributions. Moreover, the transmission spectrum of such a graph displays peaks which are very close to $1$. Furthermore, the microwave networks are investigated in the time-domain using short Gaussian pulses. In this case the delay-time distributions, though very sensitive to the internal structure of the networks, show the sequences of transmitted peaks with the amplitudes much smaller than the input one. The analyzed properties of the graphs and networks suggest that they can be effectively used to manipulate quantum and wave transport.","sentences":["We investigate properties of the transmission amplitude of quantum graphs and microwave networks composed of regular polygons such as triangles and squares.","We show that for the graphs composed of regular polygons with the edges of the length $l$ the transmission amplitude displays a band of transmission suppression with some narrow peaks of full transmission.","The peaks are distributed symmetrically with respect to the symmetry axis $kl=\\pi$, where $k$ is the wave vector.","For microwave networks the transmission peak amplitudes are reduced and their symmetry is broken due to the influence of internal absorption.","We demonstrate that for the graphs composed of the same polygons but separated by the edges of length $l' < l$ the transmission spectrum is generally not symmetric according to the axis $kl'=\\pi$. We also show that graphs composed of regular polygons of different size with the edges being irrational numbers are not fully chaotic and their level spacing distribution and the spectral rigidity are well described by the Berry-Robnik distributions.","Moreover, the transmission spectrum of such a graph displays peaks which are very close to $1$.","Furthermore, the microwave networks are investigated in the time-domain using short Gaussian pulses.","In this case the delay-time distributions, though very sensitive to the internal structure of the networks, show the sequences of transmitted peaks with the amplitudes much smaller than the input one.","The analyzed properties of the graphs and networks suggest that they can be effectively used to manipulate quantum and wave transport."],"url":"http://arxiv.org/abs/2402.12925v1","category":"quant-ph"}
{"created":"2024-02-20 11:13:15","title":"Neural-Network-Based Optimal Guidance for Lunar Vertical Landing","abstract":"This paper addresses an optimal guidance problem concerning the vertical landing of a lunar lander with the objective of minimizing fuel consumption. The vertical landing imposes a final attitude constraint, which is treated as a final control constraint. To handle this constraint, we propose a nonnegative small regularization term to augment the original cost functional. This ensures the satisfaction of the final control constraint in accordance with Pontryagin's Minimum Principle. By leveraging the necessary conditions for optimality, we establish a parameterized system that facilitates the generation of numerous optimal trajectories, which contain the nonlinear mapping from the flight state to the optimal guidance command. Subsequently, a neural network is trained to approximate such mapping. Finally, numerical examples are presented to validate the proposed method.","sentences":["This paper addresses an optimal guidance problem concerning the vertical landing of a lunar lander with the objective of minimizing fuel consumption.","The vertical landing imposes a final attitude constraint, which is treated as a final control constraint.","To handle this constraint, we propose a nonnegative small regularization term to augment the original cost functional.","This ensures the satisfaction of the final control constraint in accordance with Pontryagin's Minimum Principle.","By leveraging the necessary conditions for optimality, we establish a parameterized system that facilitates the generation of numerous optimal trajectories, which contain the nonlinear mapping from the flight state to the optimal guidance command.","Subsequently, a neural network is trained to approximate such mapping.","Finally, numerical examples are presented to validate the proposed method."],"url":"http://arxiv.org/abs/2402.12920v1","category":"math.OC"}
{"created":"2024-02-20 11:11:03","title":"Compact sum-of-products form of the molecular electronic Hamiltonian based on canonical polyadic decomposition","abstract":"We propose an approach to represent the second-quantized electronic Hamiltonian in a compact sum-of-products (SOP) form. The approach is based on the canonical polyadic decomposition (CPD) of the original Hamiltonian projected onto the sub-Fock spaces formed by groups of spin orbitals. The algorithm for obtaining the canonical polyadic form starts from an exact sum-of-products, which is then optimally compactified using an alternating least-squares procedure. We discuss the relation of this specific SOP with related forms, namely the Tucker format and the matrix product operator often used in conjunction with matrix product states. We benchmark the method on the electronic dynamics of an excited water molecule, trans-polyenes, and the charge migration in glycine upon inner-valence ionization. The quantum dynamics are performed with the multilayer multi-configuration time-dependent Hartree method in second quantization representation (MCTDH-SQR). Other methods based on tree-tensor Ans\\\"atze may profit from this general approach.","sentences":["We propose an approach to represent the second-quantized electronic Hamiltonian in a compact sum-of-products (SOP) form.","The approach is based on the canonical polyadic decomposition (CPD) of the original Hamiltonian projected onto the sub-Fock spaces formed by groups of spin orbitals.","The algorithm for obtaining the canonical polyadic form starts from an exact sum-of-products, which is then optimally compactified using an alternating least-squares procedure.","We discuss the relation of this specific SOP with related forms, namely the Tucker format and the matrix product operator often used in conjunction with matrix product states.","We benchmark the method on the electronic dynamics of an excited water molecule, trans-polyenes, and the charge migration in glycine upon inner-valence ionization.","The quantum dynamics are performed with the multilayer multi-configuration time-dependent Hartree method in second quantization representation (MCTDH-SQR).","Other methods based on tree-tensor Ans\\\"atze may profit from this general approach."],"url":"http://arxiv.org/abs/2402.12919v1","category":"physics.chem-ph"}
{"created":"2024-02-20 11:06:42","title":"Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of Machine Learning Models","abstract":"Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products. With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems. This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline. We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks. By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape. This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more significant outcomes in increasingly intricate data domains. Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning","sentences":["Data Pipeline plays an indispensable role in tasks such as modeling machine learning and developing data products.","With the increasing diversification and complexity of Data sources, as well as the rapid growth of data volumes, building an efficient Data Pipeline has become crucial for improving work efficiency and solving complex problems.","This paper focuses on exploring how to optimize data flow through automated machine learning methods by integrating AutoML with Data Pipeline.","We will discuss how to leverage AutoML technology to enhance the intelligence of Data Pipeline, thereby achieving better results in machine learning tasks.","By delving into the automation and optimization of Data flows, we uncover key strategies for constructing efficient data pipelines that can adapt to the ever-changing data landscape.","This not only accelerates the modeling process but also provides innovative solutions to complex problems, enabling more significant outcomes in increasingly intricate data domains.","Keywords- Data Pipeline Training;AutoML; Data environment; Machine learning"],"url":"http://arxiv.org/abs/2402.12916v1","category":"cs.LG"}
{"created":"2024-02-20 11:01:39","title":"OPDAI at SemEval-2024 Task 6: Small LLMs can Accelerate Hallucination Detection with Weakly Supervised Data","abstract":"This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track. This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data. We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data. Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters. Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4.","sentences":["This paper mainly describes a unified system for hallucination detection of LLMs, which wins the second prize in the model-agnostic track of the SemEval-2024 Task 6, and also achieves considerable results in the model-aware track.","This task aims to detect hallucination with LLMs for three different text-generation tasks without labeled training data.","We utilize prompt engineering and few-shot learning to verify the performance of different LLMs on the validation data.","Then we select the LLMs with better performance to generate high-quality weakly supervised training data, which not only satisfies the consistency of different LLMs, but also satisfies the consistency of the optimal LLM with different sampling parameters.","Furthermore, we finetune different LLMs by using the constructed training data, and finding that a relatively small LLM can achieve a competitive level of performance in hallucination detection, when compared to the large LLMs and the prompt-based approaches using GPT-4."],"url":"http://arxiv.org/abs/2402.12913v1","category":"cs.CL"}
{"created":"2024-02-20 10:56:58","title":"Bloch-Ros principle and its application to surface theory","abstract":"There exists the duality between normal family theory and value distribution theory of meromorphic functions, which is called the Bloch principle. Zalcman formulated a more precise statement on it. In this paper, based on the Zalcman and Ros work, we comprehend the phenomenon of the trinity among normal family theory, value distribution theory and minimal surface theory and give a systematic description to the relationship among the Montel theorem, the Liuoville theorem and the Bernstein theorem as well as the Carath\\'{e}odory-Montel theorem, the Picard little theorem and the Fujimoto theorem. We call this phenomenon Bloch-Ros principle. We also generalize the Bloch-Ros principle to various classes of surfaces, for instance, maxfaces in the Lorentz-Minkowski $3$-space, improper affine fronts in the affine $3$-space and flat fronts in the hyperbolic $3$-space. In particular, we give an effective criterion to determine which properties for meromorphic functions that play a role of the Gauss maps of these classes of surfaces satisfy the Gaussian curvature estimate.","sentences":["There exists the duality between normal family theory and value distribution theory of meromorphic functions, which is called the Bloch principle.","Zalcman formulated a more precise statement on it.","In this paper, based on the Zalcman and Ros work, we comprehend the phenomenon of the trinity among normal family theory, value distribution theory and minimal surface theory and give a systematic description to the relationship among the Montel theorem, the Liuoville theorem and the Bernstein theorem as well as the Carath\\'{e}odory-Montel theorem, the Picard little theorem and the Fujimoto theorem.","We call this phenomenon Bloch-Ros principle.","We also generalize the Bloch-Ros principle to various classes of surfaces, for instance, maxfaces in the Lorentz-Minkowski $3$-space, improper affine fronts in the affine $3$-space and flat fronts in the hyperbolic $3$-space.","In particular, we give an effective criterion to determine which properties for meromorphic functions that play a role of the Gauss maps of these classes of surfaces satisfy the Gaussian curvature estimate."],"url":"http://arxiv.org/abs/2402.12909v1","category":"math.DG"}
{"created":"2024-02-20 10:56:52","title":"RealCompo: Dynamic Equilibrium between Realism and Compositionality Improves Text-to-Image Diffusion Models","abstract":"Diffusion models have achieved remarkable advancements in text-to-image generation. However, existing models still have many difficulties when faced with multiple-object compositional generation. In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images. An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training. Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images. Code is available at https://github.com/YangLing0818/RealCompo","sentences":["Diffusion models have achieved remarkable advancements in text-to-image generation.","However, existing models still have many difficulties when faced with multiple-object compositional generation.","In this paper, we propose a new training-free and transferred-friendly text-to-image generation framework, namely RealCompo, which aims to leverage the advantages of text-to-image and layout-to-image models to enhance both realism and compositionality of the generated images.","An intuitive and novel balancer is proposed to dynamically balance the strengths of the two models in denoising process, allowing plug-and-play use of any model without extra training.","Extensive experiments show that our RealCompo consistently outperforms state-of-the-art text-to-image models and layout-to-image models in multiple-object compositional generation while keeping satisfactory realism and compositionality of the generated images.","Code is available at https://github.com/YangLing0818/RealCompo"],"url":"http://arxiv.org/abs/2402.12908v1","category":"cs.CV"}
{"created":"2024-02-20 10:52:57","title":"Incentive Compatibility for AI Alignment in Sociotechnical Systems: Positions and Prospects","abstract":"The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety. While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts. To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP). We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts. We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions.","sentences":["The burgeoning integration of artificial intelligence (AI) into human society brings forth significant implications for societal governance and safety.","While considerable strides have been made in addressing AI alignment challenges, existing methodologies primarily focus on technical facets, often neglecting the intricate sociotechnical nature of AI systems, which can lead to a misalignment between the development and deployment contexts.","To this end, we posit a new problem worth exploring: Incentive Compatibility Sociotechnical Alignment Problem (ICSAP).","We hope this can call for more researchers to explore how to leverage the principles of Incentive Compatibility (IC) from game theory to bridge the gap between technical and societal components to maintain AI consensus with human societies in different contexts.","We further discuss three classical game problems for achieving IC: mechanism design, contract theory, and Bayesian persuasion, in addressing the perspectives, potentials, and challenges of solving ICSAP, and provide preliminary implementation conceptions."],"url":"http://arxiv.org/abs/2402.12907v1","category":"cs.AI"}
{"created":"2024-02-20 10:50:01","title":"Locally Rainbow Paths","abstract":"We introduce the algorithmic problem of finding a locally rainbow path of length $\\ell$ connecting two distinguished vertices $s$ and $t$ in a vertex-colored directed graph. Herein, a path is locally rainbow if between any two visits of equally colored vertices, the path traverses consecutively at least $r$ differently colored vertices. This problem generalizes the well-known problem of finding a rainbow path. It finds natural applications whenever there are different types of resources that must be protected from overuse, such as crop sequence optimization or production process scheduling. We show that the problem is computationally intractable even if $r=2$ or if one looks for a locally rainbow among the shortest paths. On the positive side, if one looks for a path that takes only a short detour (i.e., it is slightly longer than the shortest path) and if $r$ is small, the problem can be solved efficiently. Indeed, the running time of the respective algorithm is near-optimal unless the ETH fails.","sentences":["We introduce the algorithmic problem of finding a locally rainbow path of length $\\ell$ connecting two distinguished vertices $s$ and $t$ in a vertex-colored directed graph.","Herein, a path is locally rainbow if between any two visits of equally colored vertices, the path traverses consecutively at least $r$ differently colored vertices.","This problem generalizes the well-known problem of finding a rainbow path.","It finds natural applications whenever there are different types of resources that must be protected from overuse, such as crop sequence optimization or production process scheduling.","We show that the problem is computationally intractable even if $r=2$ or if one looks for a locally rainbow among the shortest paths.","On the positive side, if one looks for a path that takes only a short detour (i.e., it is slightly longer than the shortest path) and if $r$ is small, the problem can be solved efficiently.","Indeed, the running time of the respective algorithm is near-optimal unless the ETH fails."],"url":"http://arxiv.org/abs/2402.12905v1","category":"cs.DS"}
{"created":"2024-02-20 10:49:33","title":"Inverse problems for semilinear Schr\u00f6dinger equations at large frequency via polynomial resolvent estimates on manifolds","abstract":"We study inverse boundary problems for semilinear Schr\\\"odinger equations on smooth compact Riemannian manifolds of dimensions $\\ge 2$ with smooth boundary, at a large fixed frequency. We show that certain classes of cubic nonlinearities are determined uniquely from the knowledge of the nonlinear Dirichlet--to--Neumann map at a large fixed frequency on quite general Riemannian manifolds. In particular, in contrast to the previous results available, here the manifolds need not satisfy any product structure, may have trapped geodesics, and the geodesic ray transform need not be injective. Only a mild assumption about the geometry of intersecting geodesics is required. We also establish a polynomial resolvent estimate for the Laplacian on an arbitrary smooth compact Riemannian manifold without boundary, valid for most frequencies. This estimate, along with the invariant construction of Gaussian beam quasimodes with uniform bounds for underlying constants and a stationary phase lemma with explicit control over all involved constants, constitutes the key elements in proving the uniqueness results for the considered inverse problems.","sentences":["We study inverse boundary problems for semilinear Schr\\\"odinger equations on smooth compact Riemannian manifolds of dimensions $\\ge 2$ with smooth boundary, at a large fixed frequency.","We show that certain classes of cubic nonlinearities are determined uniquely from the knowledge of the nonlinear Dirichlet--to--Neumann map at a large fixed frequency on quite general Riemannian manifolds.","In particular, in contrast to the previous results available, here the manifolds need not satisfy any product structure, may have trapped geodesics, and the geodesic ray transform need not be injective.","Only a mild assumption about the geometry of intersecting geodesics is required.","We also establish a polynomial resolvent estimate for the Laplacian on an arbitrary smooth compact Riemannian manifold without boundary, valid for most frequencies.","This estimate, along with the invariant construction of Gaussian beam quasimodes with uniform bounds for underlying constants and a stationary phase lemma with explicit control over all involved constants, constitutes the key elements in proving the uniqueness results for the considered inverse problems."],"url":"http://arxiv.org/abs/2402.12903v1","category":"math.AP"}
{"created":"2024-02-20 10:48:21","title":"Bi-invariant Dissimilarity Measures for Sample Distributions in Lie Groups","abstract":"Data sets sampled in Lie groups are widespread, and as with multivariate data, it is important for many applications to assess the differences between the sets in terms of their distributions. Indices for this task are usually derived by considering the Lie group as a Riemannian manifold. Then, however, compatibility with the group operation is guaranteed only if a bi-invariant metric exists, which is not the case for most non-compact and non-commutative groups. We show here that if one considers an affine connection structure instead, one obtains bi-invariant generalizations of well-known dissimilarity measures: a Hotelling $T^2$ statistic, Bhattacharyya distance and Hellinger distance. Each of the dissimilarity measures matches its multivariate counterpart for Euclidean data and is translation-invariant, so that biases, e.g., through an arbitrary choice of reference, are avoided. We further derive non-parametric two-sample tests that are bi-invariant and consistent. We demonstrate the potential of these dissimilarity measures by performing group tests on data of knee configurations and epidemiological shape data. Significant differences are revealed in both cases.","sentences":["Data sets sampled in Lie groups are widespread, and as with multivariate data, it is important for many applications to assess the differences between the sets in terms of their distributions.","Indices for this task are usually derived by considering the Lie group as a Riemannian manifold.","Then, however, compatibility with the group operation is guaranteed only if a bi-invariant metric exists, which is not the case for most non-compact and non-commutative groups.","We show here that if one considers an affine connection structure instead, one obtains bi-invariant generalizations of well-known dissimilarity measures: a Hotelling $T^2$ statistic, Bhattacharyya distance and Hellinger distance.","Each of the dissimilarity measures matches its multivariate counterpart for Euclidean data and is translation-invariant, so that biases, e.g., through an arbitrary choice of reference, are avoided.","We further derive non-parametric two-sample tests that are bi-invariant and consistent.","We demonstrate the potential of these dissimilarity measures by performing group tests on data of knee configurations and epidemiological shape data.","Significant differences are revealed in both cases."],"url":"http://arxiv.org/abs/2402.12901v1","category":"stat.ME"}
{"created":"2024-02-20 10:46:04","title":"High-harmonic generation in semi-Dirac and Weyl semimetals with broken time-reversal symmetry: Exploring merging of Weyl nodes","abstract":"We explore anomalous high-harmonic generation in a model that realizes a transition from a broken time-reversal symmetry Weyl-semimetal to a semi-Dirac regime, i.e. a gapless semimetal with dispersion that is parabolic in one direction and conical in the other two. We point out the intensity of the induced anomalous high harmonics is high in the semi-Dirac regime. For Weyl semimetals, we reveal anomalous high harmonics are due to excitations at momenta where the dispersion is not strictly linear and that in the linearized low-energy theory the anomalous response is harmonic only. Our findings aid experimental characterization of Weyl, Dirac, and semi-Dirac semimetals.","sentences":["We explore anomalous high-harmonic generation in a model that realizes a transition from a broken time-reversal symmetry Weyl-semimetal to a semi-Dirac regime, i.e. a gapless semimetal with dispersion that is parabolic in one direction and conical in the other two.","We point out the intensity of the induced anomalous high harmonics is high in the semi-Dirac regime.","For Weyl semimetals, we reveal anomalous high harmonics are due to excitations at momenta where the dispersion is not strictly linear and that in the linearized low-energy theory the anomalous response is harmonic only.","Our findings aid experimental characterization of Weyl, Dirac, and semi-Dirac semimetals."],"url":"http://arxiv.org/abs/2402.12899v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 10:40:33","title":"A prop structure on partitions","abstract":"Motivated by its link with functor homology, we study the prop freely generated by the operadic suspension of the operad Com. We exhibit a particular family of generators, for which the composition and the symmetric group actions admit simple descriptions. We highlight associated subcategories of its Karoubi envelope which allows us to compute extensions groups between simple functors from free groups. We construct a particular prop structure on partitions whose composition corresponds to the Yoneda product of extensions between exterior power functors.","sentences":["Motivated by its link with functor homology, we study the prop freely generated by the operadic suspension of the operad Com.","We exhibit a particular family of generators, for which the composition and the symmetric group actions admit simple descriptions.","We highlight associated subcategories of its Karoubi envelope which allows us to compute extensions groups between simple functors from free groups.","We construct a particular prop structure on partitions whose composition corresponds to the Yoneda product of extensions between exterior power functors."],"url":"http://arxiv.org/abs/2402.12895v1","category":"math.AT"}
{"created":"2024-02-20 10:38:49","title":"Enhanced photon-pair generation under coherent control","abstract":"The generation of the narrowband strong-correlated biphotons via spontaneous four-wave mixing can be effectively controlled and enhanced by an additional driving field which drives a transition with its upper level being a Rydberg state. We study the properties of the noise of the generated biphotons and show that in the region of weak pumping and low atomic density, a high degree of the photon correlation is maintained with the photon-pair generation rate siginificantly enhanced.","sentences":["The generation of the narrowband strong-correlated biphotons via spontaneous four-wave mixing can be effectively controlled and enhanced by an additional driving field which drives a transition with its upper level being a Rydberg state.","We study the properties of the noise of the generated biphotons and show that in the region of weak pumping and low atomic density, a high degree of the photon correlation is maintained with the photon-pair generation rate siginificantly enhanced."],"url":"http://arxiv.org/abs/2402.12894v1","category":"quant-ph"}
{"created":"2024-02-20 10:33:35","title":"Transformer-based Learned Image Compression for Joint Decoding and Denoising","abstract":"This work introduces a Transformer-based image compression system. It has the flexibility to switch between the standard image reconstruction and the denoising reconstruction from a single compressed bitstream. Instead of training separate decoders for these tasks, we incorporate two add-on modules to adapt a pre-trained image decoder from performing the standard image reconstruction to joint decoding and denoising. Our scheme adopts a two-pronged approach. It features a latent refinement module to refine the latent representation of a noisy input image for reconstructing a noise-free image. Additionally, it incorporates an instance-specific prompt generator that adapts the decoding process to improve on the latent refinement. Experimental results show that our method achieves a similar level of denoising quality to training a separate decoder for joint decoding and denoising at the expense of only a modest increase in the decoder's model size and computational complexity.","sentences":["This work introduces a Transformer-based image compression system.","It has the flexibility to switch between the standard image reconstruction and the denoising reconstruction from a single compressed bitstream.","Instead of training separate decoders for these tasks, we incorporate two add-on modules to adapt a pre-trained image decoder from performing the standard image reconstruction to joint decoding and denoising.","Our scheme adopts a two-pronged approach.","It features a latent refinement module to refine the latent representation of a noisy input image for reconstructing a noise-free image.","Additionally, it incorporates an instance-specific prompt generator that adapts the decoding process to improve on the latent refinement.","Experimental results show that our method achieves a similar level of denoising quality to training a separate decoder for joint decoding and denoising at the expense of only a modest increase in the decoder's model size and computational complexity."],"url":"http://arxiv.org/abs/2402.12888v1","category":"eess.IV"}
{"created":"2024-02-20 10:30:36","title":"The practice of qualitative parameterisation in the development of Bayesian networks","abstract":"The typical phases of Bayesian network (BN) structured development include specification of purpose and scope, structure development, parameterisation and validation. Structure development is typically focused on qualitative issues and parameterisation quantitative issues, however there are qualitative and quantitative issues that arise in both phases. A common step that occurs after the initial structure has been developed is to perform a rough parameterisation that only captures and illustrates the intended qualitative behaviour of the model. This is done prior to a more rigorous parameterisation, ensuring that the structure is fit for purpose, as well as supporting later development and validation. In our collective experience and in discussions with other modellers, this step is an important part of the development process, but is under-reported in the literature. Since the practice focuses on qualitative issues, despite being quantitative in nature, we call this step qualitative parameterisation and provide an outline of its role in the BN development process.","sentences":["The typical phases of Bayesian network (BN) structured development include specification of purpose and scope, structure development, parameterisation and validation.","Structure development is typically focused on qualitative issues and parameterisation quantitative issues, however there are qualitative and quantitative issues that arise in both phases.","A common step that occurs after the initial structure has been developed is to perform a rough parameterisation that only captures and illustrates the intended qualitative behaviour of the model.","This is done prior to a more rigorous parameterisation, ensuring that the structure is fit for purpose, as well as supporting later development and validation.","In our collective experience and in discussions with other modellers, this step is an important part of the development process, but is under-reported in the literature.","Since the practice focuses on qualitative issues, despite being quantitative in nature, we call this step qualitative parameterisation and provide an outline of its role in the BN development process."],"url":"http://arxiv.org/abs/2402.12887v1","category":"cs.AI"}
{"created":"2024-02-20 10:25:04","title":"Lower bounds for the Randi\u0107 index in terms of matching number","abstract":"We investigate how small the Randi\\'c index of a graph can be in terms of its matching number, and prove several results. We give best-possible linear bounds for graphs of small excess and for subcubic graphs; in the former case the size of excess we permit is qualitatively the best possible. We show that a linear bound holds for any sparse hereditary graph class (such as planar graphs). In general, however, we show that it can be much smaller than linear. We determine the asymptotic growth rate of the minimum Randi\\'c index for graphs with a near perfect matching, and conjecture that the same bounds hold for all graphs.","sentences":["We investigate how small the Randi\\'c index of a graph can be in terms of its matching number, and prove several results.","We give best-possible linear bounds for graphs of small excess and for subcubic graphs; in the former case the size of excess we permit is qualitatively the best possible.","We show that a linear bound holds for any sparse hereditary graph class (such as planar graphs).","In general, however, we show that it can be much smaller than linear.","We determine the asymptotic growth rate of the minimum Randi\\'c index for graphs with a near perfect matching, and conjecture that the same bounds hold for all graphs."],"url":"http://arxiv.org/abs/2402.12884v1","category":"math.CO"}
{"created":"2024-02-20 10:23:33","title":"An 8-flow theorem for signed graphs","abstract":"We prove that a signed graph admits a nowhere-zero $8$-flow provided that it is flow-admissible and the underlying graph admits a nowhere-zero $4$-flow. When combined with the 4-color theorem, this implies that every flow-admissible bridgeless planar signed graph admits a nowhere-zero $8$-flow. Our result improves and generalizes previous results of Li et al. (European J. Combin. 108 (2023), 103627), which state that every flow-admissible signed $3$-edge-colorable cubic graph admits a nowhere-zero $10$-flow, and that every flow-admissible signed hamiltonian graph admits a nowhere-zero $8$-flow.","sentences":["We prove that a signed graph admits a nowhere-zero $8$-flow provided that it is flow-admissible and the underlying graph admits a nowhere-zero $4$-flow.","When combined with the 4-color theorem, this implies that every flow-admissible bridgeless planar signed graph admits a nowhere-zero $8$-flow.","Our result improves and generalizes previous results of Li et al.","(European J. Combin. 108 (2023), 103627), which state that every flow-admissible signed $3$-edge-colorable cubic graph admits a nowhere-zero $10$-flow, and that every flow-admissible signed hamiltonian graph admits a nowhere-zero $8$-flow."],"url":"http://arxiv.org/abs/2402.12883v1","category":"math.CO"}
{"created":"2024-02-20 10:23:26","title":"Clifford Theory: A Geometrical Interpretation of Multivectorial Apparent Power","abstract":"In this paper, a generalization of the concept of electrical power for periodic current and voltage waveforms based on a new generalized complex geometric algebra (GCGA) is proposed. This powerful tool permits, in n-sinusoidal/nonlinear situations, representing and calculating the voltage, current, and apparent power in a single-port electrical network in terms of multivectors. The new expressions result in a novel representation of the apparent power, similar to the Steinmetz's phasor model, based on complex numbers, but limited to the purely sinusoidal case. The multivectorial approach presented is based on the frequency-domain decomposition of the apparent power into three components: the real part and the imaginary part of the complex-scalar associated to active and reactive power respectively, and distortion power, associated to the complex-bivector. A geometrical interpretation of the multivectorial components of apparent power is discussed. Numerical examples illustrate the clear advantages of the suggested approach.","sentences":["In this paper, a generalization of the concept of electrical power for periodic current and voltage waveforms based on a new generalized complex geometric algebra (GCGA) is proposed.","This powerful tool permits, in n-sinusoidal/nonlinear situations, representing and calculating the voltage, current, and apparent power in a single-port electrical network in terms of multivectors.","The new expressions result in a novel representation of the apparent power, similar to the Steinmetz's phasor model, based on complex numbers, but limited to the purely sinusoidal case.","The multivectorial approach presented is based on the frequency-domain decomposition of the apparent power into three components: the real part and the imaginary part of the complex-scalar associated to active and reactive power respectively, and distortion power, associated to the complex-bivector.","A geometrical interpretation of the multivectorial components of apparent power is discussed.","Numerical examples illustrate the clear advantages of the suggested approach."],"url":"http://arxiv.org/abs/2402.12882v1","category":"eess.SY"}
{"created":"2024-02-20 10:11:03","title":"Chain of Thought Empowers Transformers to Solve Inherently Serial Problems","abstract":"Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks. However, the mechanism behind CoT remains unclear. This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness. Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low. Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers.","sentences":["Instructing the model to generate a sequence of intermediate steps, a.k.a., a chain of thought (CoT), is a highly effective method to improve the accuracy of large language models (LLMs) on arithmetics and symbolic reasoning tasks.","However, the mechanism behind CoT remains unclear.","This work provides a theoretical understanding of the power of CoT for decoder-only transformers through the lens of expressiveness.","Conceptually, CoT empowers the model with the ability to perform inherently serial computation, which is otherwise lacking in transformers, especially when depth is low.","Given input length $n$, previous works have shown that constant-depth transformers with finite precision $\\mathsf{poly}(n)$ embedding size can only solve problems in $\\mathsf{TC}^0$ without CoT. We first show an even tighter expressiveness upper bound for constant-depth transformers with constant-bit precision, which can only solve problems in $\\mathsf{AC}^0$, a proper subset of $ \\mathsf{TC}^0$. However, with $T$ steps of CoT, constant-depth transformers using constant-bit precision and $O(\\log n)$ embedding size can solve any problem solvable by boolean circuits of size $T$. Empirically, enabling CoT dramatically improves the accuracy for tasks that are hard for parallel computation, including the composition of permutation groups, iterated squaring, and circuit value problems, especially for low-depth transformers."],"url":"http://arxiv.org/abs/2402.12875v1","category":"cs.LG"}
{"created":"2024-02-20 10:00:58","title":"Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data","abstract":"Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention. However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information. Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus. Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems. In this paper, we address this research gap in two steps. First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data. Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (DSFT and RAG frameworks) with four representative methods: Markdown format, Template serialization, TPLM-based method, and LLM-based method. Based on the experimental results, we draw some empirical findings and explore the underlying reasons behind the success of some methods. We hope the findings of this work will provide a valuable reference for the academic and industrial communities in developing robust QA systems.","sentences":["Augmenting Large Language Models (LLMs) for Question Answering (QA) with domain specific data has attracted wide attention.","However, domain data often exists in a hybrid format, including text and semi-structured tables, posing challenges for the seamless integration of information.","Table-to-Text Generation is a promising solution by facilitating the transformation of hybrid data into a uniformly text-formatted corpus.","Although this technique has been widely studied by the NLP community, there is currently no comparative analysis on how corpora generated by different table-to-text methods affect the performance of QA systems.","In this paper, we address this research gap in two steps.","First, we innovatively integrate table-to-text generation into the framework of enhancing LLM-based QA systems with domain hybrid data.","Then, we utilize this framework in real-world industrial data to conduct extensive experiments on two types of QA systems (DSFT and RAG frameworks) with four representative methods: Markdown format, Template serialization, TPLM-based method, and LLM-based method.","Based on the experimental results, we draw some empirical findings and explore the underlying reasons behind the success of some methods.","We hope the findings of this work will provide a valuable reference for the academic and industrial communities in developing robust QA systems."],"url":"http://arxiv.org/abs/2402.12869v1","category":"cs.CL"}
{"created":"2024-02-20 09:57:49","title":"Towards MLOps: A DevOps Tools Recommender System for Machine Learning System","abstract":"Applying DevOps practices to machine learning system is termed as MLOps and machine learning systems evolve on new data unlike traditional systems on requirements. The objective of MLOps is to establish a connection between different open-source tools to construct a pipeline that can automatically perform steps to construct a dataset, train the machine learning model and deploy the model to the production as well as store different versions of model and dataset. Benefits of MLOps is to make sure the fast delivery of the new trained models to the production to have accurate results. Furthermore, MLOps practice impacts the overall quality of the software products and is completely dependent on open-source tools and selection of relevant open-source tools is considered as challenged while a generalized method to select an appropriate open-source tools is desirable. In this paper, we present a framework for recommendation system that processes the contextual information (e.g., nature of data, type of the data) of the machine learning project and recommends a relevant toolchain (tech-stack) for the operationalization of machine learning systems. To check the applicability of the proposed framework, four different approaches i.e., rule-based, random forest, decision trees and k-nearest neighbors were investigated where precision, recall and f-score is measured, the random forest out classed other approaches with highest f-score value of 0.66.","sentences":["Applying DevOps practices to machine learning system is termed as MLOps and machine learning systems evolve on new data unlike traditional systems on requirements.","The objective of MLOps is to establish a connection between different open-source tools to construct a pipeline that can automatically perform steps to construct a dataset, train the machine learning model and deploy the model to the production as well as store different versions of model and dataset.","Benefits of MLOps is to make sure the fast delivery of the new trained models to the production to have accurate results.","Furthermore, MLOps practice impacts the overall quality of the software products and is completely dependent on open-source tools and selection of relevant open-source tools is considered as challenged while a generalized method to select an appropriate open-source tools is desirable.","In this paper, we present a framework for recommendation system that processes the contextual information (e.g., nature of data, type of the data) of the machine learning project and recommends a relevant toolchain (tech-stack) for the operationalization of machine learning systems.","To check the applicability of the proposed framework, four different approaches i.e., rule-based, random forest, decision trees and k-nearest neighbors were investigated where precision, recall and f-score is measured, the random forest out classed other approaches with highest f-score value of 0.66."],"url":"http://arxiv.org/abs/2402.12867v1","category":"cs.SE"}
{"created":"2024-02-20 09:57:08","title":"Backward Lens: Projecting Language Model Gradients into the Vocabulary Space","abstract":"Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.","sentences":["Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community.","Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs.","In this work, we extend this methodology to LMs' backward pass and gradients.","We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs.","We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons."],"url":"http://arxiv.org/abs/2402.12865v1","category":"cs.CL"}
{"created":"2024-02-20 09:54:52","title":"Finding Cross-rule Optimization Bugs in Datalog Engines","abstract":"Datalog is a popular and widely-used declarative logic programming language. Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results. To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem. The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine. Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules. Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated-we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts. We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely Souffl\\'e, CozoDB, $\\mu$Z, and DDlog, and discovered a total of 30 bugs. Of these, 13 were logic bugs, while the remaining were crash and error bugs. Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach. Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5. Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17$\\times$ (for DDlog) to 31.02$\\times$ (for Souffl\\'e) as many valid test cases with non-empty results as the naive random method. We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice.","sentences":["Datalog is a popular and widely-used declarative logic programming language.","Datalog engines apply many cross-rule optimizations; bugs in them can cause incorrect results.","To detect such optimization bugs, we propose an automated testing approach called Incremental Rule Evaluation (IRE), which synergistically tackles the test oracle and test case generation problem.","The core idea behind the test oracle is to compare the results of an optimized program and a program without cross-rule optimization; any difference indicates a bug in the Datalog engine.","Our core insight is that, for an optimized, incrementally-generated Datalog program, we can evaluate all rules individually by constructing a reference program to disable the optimizations that are performed among multiple rules.","Incrementally generating test cases not only allows us to apply the test oracle for every new rule generated-we also can ensure that every newly added rule generates a non-empty result with a given probability and eschew recomputing already-known facts.","We implemented IRE as a tool named Deopt, and evaluated Deopt on four mature Datalog engines, namely Souffl\\'e, CozoDB, $\\mu$Z, and DDlog, and discovered a total of 30 bugs.","Of these, 13 were logic bugs, while the remaining were crash and error bugs.","Deopt can detect all bugs found by queryFuzz, a state-of-the-art approach.","Out of the bugs identified by Deopt, queryFuzz might be unable to detect 5.","Our incremental test case generation approach is efficient; for example, for test cases containing 60 rules, our incremental approach can produce 1.17$\\times$ (for DDlog) to 31.02$\\times$ (for Souffl\\'e) as many valid test cases with non-empty results as the naive random method.","We believe that the simplicity and the generality of the approach will lead to its wide adoption in practice."],"url":"http://arxiv.org/abs/2402.12863v1","category":"cs.SE"}
{"created":"2024-02-20 09:51:26","title":"General geometry realized by four-scalar model and application to $f(Q)$ gravity","abstract":"In this paper, we propose a model including four scalar fields coupled with general gravity theories, which is a generalization of the two-scalar model proposed in Phys. Rev. D \\textbf{103} (2021) no.4, 044055, where it has been shown that any given spherically symmetric static/time-dependent spacetime can be realized by using the two-scalar model. We show that by using the four-scalar model, we can construct a model that realizes any given spacetime as a solution even if the spacetime does not have a spherical symmetry or any other symmetry. We also show that by imposing constraints on the scalar fields by using the Lagrange multiplier fields, the scalar fields become non-dynamical and they do not propagate. This tells that there does not appear any sound which is usually generated by the density fluctuation of the fluid. In this sense, the model with the constraints is a most general extension of the mimetic theory in JHEP \\textbf{11} (2013), 135, where there appears an effective dark matter. The dark matter is non-dynamical and it does not collapse even under gravitational force. Our model can be regarded as a general extension of any kind of fluid besides dark matter. We may consider the case that the potential of the scalar fields vanishes and the model becomes a non-linear $\\sigma$ model. Then our formulation gives a mapping from the geometry of the spacetime to the geometry of the target space of the non-linear $\\sigma$ model via gravity theory although the physical meaning has not been clear. We also consider the application of the model to $f(Q)$ gravity theory, which is based on a non-metricity tensor and $Q$ is a scalar quantity constructed from the non-metricity tensor. ...","sentences":["In this paper, we propose a model including four scalar fields coupled with general gravity theories, which is a generalization of the two-scalar model proposed in Phys.","Rev. D \\textbf{103} (2021) no.4, 044055, where it has been shown that any given spherically symmetric static/time-dependent spacetime can be realized by using the two-scalar model.","We show that by using the four-scalar model, we can construct a model that realizes any given spacetime as a solution even if the spacetime does not have a spherical symmetry or any other symmetry.","We also show that by imposing constraints on the scalar fields by using the Lagrange multiplier fields, the scalar fields become non-dynamical and they do not propagate.","This tells that there does not appear any sound which is usually generated by the density fluctuation of the fluid.","In this sense, the model with the constraints is a most general extension of the mimetic theory in JHEP \\textbf{11} (2013), 135, where there appears an effective dark matter.","The dark matter is non-dynamical and it does not collapse even under gravitational force.","Our model can be regarded as a general extension of any kind of fluid besides dark matter.","We may consider the case that the potential of the scalar fields vanishes and the model becomes a non-linear $\\sigma$ model.","Then our formulation gives a mapping from the geometry of the spacetime to the geometry of the target space of the non-linear $\\sigma$ model via gravity theory although the physical meaning has not been clear.","We also consider the application of the model to $f(Q)$ gravity theory, which is based on a non-metricity tensor and $Q$ is a scalar quantity constructed from the non-metricity tensor. ..."],"url":"http://arxiv.org/abs/2402.12860v1","category":"gr-qc"}
{"created":"2024-02-20 09:43:00","title":"ATLAS: A Model of Short-term European Electricity Market Processes under Uncertainty -- Balancing Modules","abstract":"The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes. ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail. Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints -- both regarding different production units and the different market constraints. This working paper describes the simulated balancing processes in detail and is the second part of the ATLAS documentation.","sentences":["The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes.","ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail.","Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints -- both regarding different production units and the different market constraints.","This working paper describes the simulated balancing processes in detail and is the second part of the ATLAS documentation."],"url":"http://arxiv.org/abs/2402.12859v1","category":"econ.GN"}
{"created":"2024-02-20 09:39:31","title":"A proof of the only mode of a unimodal sequence","abstract":"In study the generalized Jacobsthal and Jaco-Lucas polynomials, Sun introduced the interesting numerical triangle Jaco-Lucas sequence $\\{JL_{n,k}\\}_{n\\geq k\\geq0}$. In this paper, we proved this sequence is log-concave with the only mode by computer algebra.","sentences":["In study the generalized Jacobsthal and Jaco-Lucas polynomials, Sun introduced the interesting numerical triangle Jaco-Lucas sequence $\\{JL_{n,k}\\}_{n\\geq k\\geq0}$.","In this paper, we proved this sequence is log-concave with the only mode by computer algebra."],"url":"http://arxiv.org/abs/2402.12858v1","category":"math.CO"}
{"created":"2024-02-20 09:39:04","title":"Radially symmetric solutions of the ultra-relativistic Euler equations in several space dimensions","abstract":"The ultra-relativistic Euler equations for an ideal gas are described in terms of the pressure, the spatial part of the dimensionless four-velocity and the particle density. Radially symmetric solutions of these equations are studied in two and three space dimensions. Of particular interest in the solutions are the formation of shock waves and a pressure blow up. For the investigation of these phenomena we develop a one-dimensional scheme using radial symmetry and integral conservation laws. We compare the numerical results with solutions of multi-dimensional high-order numerical schemes for general initial data in two space dimensions. The presented test cases and results may serve as interesting benchmark tests for multi-dimensional solvers.","sentences":["The ultra-relativistic Euler equations for an ideal gas are described in terms of the pressure, the spatial part of the dimensionless four-velocity and the particle density.","Radially symmetric solutions of these equations are studied in two and three space dimensions.","Of particular interest in the solutions are the formation of shock waves and a pressure blow up.","For the investigation of these phenomena we develop a one-dimensional scheme using radial symmetry and integral conservation laws.","We compare the numerical results with solutions of multi-dimensional high-order numerical schemes for general initial data in two space dimensions.","The presented test cases and results may serve as interesting benchmark tests for multi-dimensional solvers."],"url":"http://arxiv.org/abs/2402.12857v1","category":"math-ph"}
{"created":"2024-02-20 09:36:18","title":"A new simplified MOPSO based on Swarm Elitism and Swarm Memory: MO-ETPSO","abstract":"This paper presents an algorithm based on Particle Swarm Optimization (PSO), adapted for multi-objective optimization problems: the Elitist PSO (MO-ETPSO). The proposed algorithm integrates core strategies from the well-established NSGA-II approach, such as the Crowding Distance Algorithm, while leveraging the advantages of Swarm Intelligence in terms of individual and social cognition. A novel aspect of the algorithm is the introduction of a swarm memory and swarm elitism, which may turn the adoption of NSGA-II strategies in PSO. These features enhance the algorithm's ability to retain and utilize high-quality solutions throughout optimization. Furthermore, all operators within the algorithm are intentionally designed for simplicity, ensuring ease of replication and implementation in various settings. Preliminary comparisons with the NSGA-II algorithm for the Green Vehicle Routing Problem, both in terms of solutions found and convergence, have yielded promising results in favor of MO-ETPSO.","sentences":["This paper presents an algorithm based on Particle Swarm Optimization (PSO), adapted for multi-objective optimization problems: the Elitist PSO (MO-ETPSO).","The proposed algorithm integrates core strategies from the well-established NSGA-II approach, such as the Crowding Distance Algorithm, while leveraging the advantages of Swarm Intelligence in terms of individual and social cognition.","A novel aspect of the algorithm is the introduction of a swarm memory and swarm elitism, which may turn the adoption of NSGA-II strategies in PSO.","These features enhance the algorithm's ability to retain and utilize high-quality solutions throughout optimization.","Furthermore, all operators within the algorithm are intentionally designed for simplicity, ensuring ease of replication and implementation in various settings.","Preliminary comparisons with the NSGA-II algorithm for the Green Vehicle Routing Problem, both in terms of solutions found and convergence, have yielded promising results in favor of MO-ETPSO."],"url":"http://arxiv.org/abs/2402.12856v1","category":"cs.NE"}
{"created":"2024-02-20 09:33:22","title":"Differentiable Mapper For Topological Optimization Of Data Representation","abstract":"Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science. Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself. While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures. However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itself. In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper graphs. In order to achieve this, we propose a relaxed and more general version of the Mapper graph, whose convergence properties are investigated. Finally, we demonstrate the usefulness of our approach by optimizing Mapper graph representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones.","sentences":["Unsupervised data representation and visualization using tools from topology is an active and growing field of Topological Data Analysis (TDA) and data science.","Its most prominent line of work is based on the so-called Mapper graph, which is a combinatorial graph whose topological structures (connected components, branches, loops) are in correspondence with those of the data itself.","While highly generic and applicable, its use has been hampered so far by the manual tuning of its many parameters-among these, a crucial one is the so-called filter: it is a continuous function whose variations on the data set are the main ingredient for both building the Mapper representation and assessing the presence and sizes of its topological structures.","However, while a few parameter tuning methods have already been investigated for the other Mapper parameters (i.e., resolution, gain, clustering), there is currently no method for tuning the filter itself.","In this work, we build on a recently proposed optimization framework incorporating topology to provide the first filter optimization scheme for Mapper graphs.","In order to achieve this, we propose a relaxed and more general version of the Mapper graph, whose convergence properties are investigated.","Finally, we demonstrate the usefulness of our approach by optimizing Mapper graph representations on several datasets, and showcasing the superiority of the optimized representation over arbitrary ones."],"url":"http://arxiv.org/abs/2402.12854v1","category":"cs.LG"}
{"created":"2024-02-20 09:27:09","title":"ATLAS: A Model of Short-term European Electricity Market Processes under Uncertainty","abstract":"The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes. ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail. Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints--both regarding different production units and the different market constraints.","sentences":["The ATLAS model simulates the various stages of the electricity market chain in Europe, including the formulation of offers by different market actors, the coupling of European markets, strategic optimization of production portfolios and, finally, real-time system balancing processes.","ATLAS was designed to simulate the various electricity markets and processes that occur from the day ahead timeframe to real-time with a high level of detail.","Its main aim is to capture impacts from imperfect actor coordination, evolving forecast errors and a high-level of technical constraints--both regarding different production units and the different market constraints."],"url":"http://arxiv.org/abs/2402.12848v1","category":"econ.GN"}
{"created":"2024-02-20 09:20:32","title":"Instruction-tuned Language Models are Better Knowledge Learners","abstract":"In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.","sentences":["In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data.","The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs.","However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized.","We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner.","Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions.","Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents.","This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents.","Extensive experiments and ablation studies demonstrate that PIT significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%."],"url":"http://arxiv.org/abs/2402.12847v1","category":"cs.CL"}
{"created":"2024-02-20 09:20:30","title":"ConVQG: Contrastive Visual Question Generation with Multimodal Guidance","abstract":"Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems. Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions. These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only. However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of grounding. In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one. Experiments on both knowledge-aware and standard VQG benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions. Our human evaluation results also show preference for ConVQG questions compared to non-contrastive baselines.","sentences":["Asking questions about visual environments is a crucial way for intelligent agents to understand rich multi-faceted scenes, raising the importance of Visual Question Generation (VQG) systems.","Apart from being grounded to the image, existing VQG systems can use textual constraints, such as expected answers or knowledge triplets, to generate focused questions.","These constraints allow VQG systems to specify the question content or leverage external commonsense knowledge that can not be obtained from the image content only.","However, generating focused questions using textual constraints while enforcing a high relevance to the image content remains a challenge, as VQG systems often ignore one or both forms of grounding.","In this work, we propose Contrastive Visual Question Generation (ConVQG), a method using a dual contrastive objective to discriminate questions generated using both modalities from those based on a single one.","Experiments on both knowledge-aware and standard VQG benchmarks demonstrate that ConVQG outperforms the state-of-the-art methods and generates image-grounded, text-guided, and knowledge-rich questions.","Our human evaluation results also show preference for ConVQG questions compared to non-contrastive baselines."],"url":"http://arxiv.org/abs/2402.12846v1","category":"cs.CV"}
{"created":"2024-02-20 09:15:50","title":"MORE-3S:Multimodal-based Offline Reinforcement Learning with Shared Semantic Spaces","abstract":"Drawing upon the intuition that aligning different modalities to the same semantic embedding space would allow models to understand states and actions more easily, we propose a new perspective to the offline reinforcement learning (RL) challenge. More concretely, we transform it into a supervised learning task by integrating multimodal and pre-trained language models. Our approach incorporates state information derived from images and action-related data obtained from text, thereby bolstering RL training performance and promoting long-term strategic thinking. We emphasize the contextual understanding of language and demonstrate how decision-making in RL can benefit from aligning states' and actions' representation with languages' representation. Our method significantly outperforms current baselines as evidenced by evaluations conducted on Atari and OpenAI Gym environments. This contributes to advancing offline RL performance and efficiency while providing a novel perspective on offline RL.Our code and data are available at https://github.com/Zheng0428/MORE_.","sentences":["Drawing upon the intuition that aligning different modalities to the same semantic embedding space would allow models to understand states and actions more easily, we propose a new perspective to the offline reinforcement learning (RL) challenge.","More concretely, we transform it into a supervised learning task by integrating multimodal and pre-trained language models.","Our approach incorporates state information derived from images and action-related data obtained from text, thereby bolstering RL training performance and promoting long-term strategic thinking.","We emphasize the contextual understanding of language and demonstrate how decision-making in RL can benefit from aligning states' and actions' representation with languages' representation.","Our method significantly outperforms current baselines as evidenced by evaluations conducted on Atari and OpenAI Gym environments.","This contributes to advancing offline RL performance and efficiency while providing a novel perspective on offline RL.Our code and data are available at https://github.com/Zheng0428/MORE_."],"url":"http://arxiv.org/abs/2402.12845v1","category":"cs.AI"}
{"created":"2024-02-20 09:13:15","title":"ICON: Improving Inter-Report Consistency of Radiology Report Generation via Lesion-aware Mix-up Augmentation","abstract":"Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports. In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs. This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust. Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants. To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation. Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesions, our approach involves first extracting lesions from input images and examining their characteristics. Then, we introduce a lesion-aware mix-up augmentation technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, by linearly interpolating them during the training phase. Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports.","sentences":["Previous research on radiology report generation has made significant progress in terms of increasing the clinical accuracy of generated reports.","In this paper, we emphasize another crucial quality that it should possess, i.e., inter-report consistency, which refers to the capability of generating consistent reports for semantically equivalent radiographs.","This quality is even of greater significance than the overall report accuracy in terms of ensuring the system's credibility, as a system prone to providing conflicting results would severely erode users' trust.","Regrettably, existing approaches struggle to maintain inter-report consistency, exhibiting biases towards common patterns and susceptibility to lesion variants.","To address this issue, we propose ICON, which improves the inter-report consistency of radiology report generation.","Aiming at enhancing the system's ability to capture the similarities in semantically equivalent lesions, our approach involves first extracting lesions from input images and examining their characteristics.","Then, we introduce a lesion-aware mix-up augmentation technique to ensure that the representations of the semantically equivalent lesions align with the same attributes, by linearly interpolating them during the training phase.","Extensive experiments on three publicly available chest X-ray datasets verify the effectiveness of our approach, both in terms of improving the consistency and accuracy of the generated reports."],"url":"http://arxiv.org/abs/2402.12844v1","category":"cs.CV"}
{"created":"2024-02-20 09:13:11","title":"SolarPanel Segmentation :Self-Supervised Learning for Imperfect Datasets","abstract":"The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations. A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency. This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning. We explore and apply Self-Supervised Learning (SSL) to solve these challenges. We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions.","sentences":["The increasing adoption of solar energy necessitates advanced methodologies for monitoring and maintenance to ensure optimal performance of solar panel installations.","A critical component in this context is the accurate segmentation of solar panels from aerial or satellite imagery, which is essential for identifying operational issues and assessing efficiency.","This paper addresses the significant challenges in panel segmentation, particularly the scarcity of annotated data and the labour-intensive nature of manual annotation for supervised learning.","We explore and apply Self-Supervised Learning (SSL) to solve these challenges.","We demonstrate that SSL significantly enhances model generalization under various conditions and reduces dependency on manually annotated data, paving the way for robust and adaptable solar panel segmentation solutions."],"url":"http://arxiv.org/abs/2402.12843v1","category":"cs.CV"}
{"created":"2024-02-20 09:10:08","title":"PromptKD: Distilling Student-Friendly Knowledge for Generative Language Models via Prompt Tuning","abstract":"Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression. While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models. To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge. Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance. Extensive experiments on instruction-following datasets using the GPT-2 model family show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts. Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements.","sentences":["Recent advancements in large language models (LLMs) have raised concerns about inference costs, increasing the need for research into model compression.","While knowledge distillation (KD) is a prominent method for this, research on KD for generative language models like LLMs is relatively sparse, and the approach of distilling student-friendly knowledge, which has shown promising performance in KD for classification models, remains unexplored in generative language models.","To explore this approach, we propose PromptKD, a simple yet effective method that utilizes prompt tuning - for the first time in KD - to enable generative language models to transfer student-friendly knowledge.","Unlike previous works in classification that require fine-tuning the entire teacher model for extracting student-friendly knowledge, PromptKD achieves similar effects by adding a small number of prompt tokens and tuning only the prompt with student guidance.","Extensive experiments on instruction-following datasets using the GPT-2 model family show that PromptKD achieves state-of-the-art performance while adding only 0.0007% of the teacher's parameters as prompts.","Further analysis suggests that distilling student-friendly knowledge alleviates exposure bias effectively throughout the entire training process, leading to performance enhancements."],"url":"http://arxiv.org/abs/2402.12842v1","category":"cs.CL"}
{"created":"2024-02-20 09:02:55","title":"PANDA: Preference Adaptation for Enhancing Domain-Specific Abilities of LLMs","abstract":"While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models. One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets. However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs. In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning. Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks. Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld. This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization.","sentences":["While Large language models (LLMs) have demonstrated considerable capabilities across various natural language tasks, they often fall short of the performance achieved by domain-specific state-of-the-art models.","One potential approach to enhance domain-specific capabilities of LLMs involves fine-tuning them using corresponding datasets.","However, this method can be both resource and time-intensive, and not applicable to closed-source commercial LLMs.","In this paper, we propose Preference Adaptation for Enhancing Domain-specific Abilities of LLMs (PANDA), a method designed to augment the domain-specific capabilities of LLMs by leveraging insights from the response preference of expert models without requiring fine-tuning.","Our experimental results reveal that PANDA significantly enhances the domain-specific ability of LLMs on text classification and interactive decision tasks.","Moreover, LLM with PANDA even outperforms the expert model that being learned on 4 tasks of ScienceWorld.","This finding highlights the potential of exploring tuning-free approaches to achieve weak-to-strong generalization."],"url":"http://arxiv.org/abs/2402.12835v1","category":"cs.CL"}
{"created":"2024-02-20 09:01:28","title":"SAT-based Exact Modulo Scheduling Mapping for Resource-Constrained CGRAs","abstract":"Coarse-Grain Reconfigurable Arrays (CGRAs) represent emerging low-power architectures designed to accelerate Compute-Intensive Loops (CILs). The effectiveness of CGRAs in providing acceleration relies on the quality of mapping: how efficiently the CIL is compiled onto the platform. State of the Art (SoA) compilation techniques utilize modulo scheduling to minimize the Iteration Interval (II) and use graph algorithms like Max-Clique Enumeration to address mapping challenges. Our work approaches the mapping problem through a satisfiability (SAT) formulation. We introduce the Kernel Mobility Schedule (KMS), an ad-hoc schedule used with the Data Flow Graph and CGRA architectural information to generate Boolean statements that, when satisfied, yield a valid mapping. Experimental results demonstrate SAT-MapIt outperforming SoA alternatives in almost 50\\% of explored benchmarks. Additionally, we evaluated the mapping results in a synthesizable CGRA design and emphasized the run-time metrics trends, i.e. energy efficiency and latency, across different CILs and CGRA sizes. We show that a hardware-agnostic analysis performed on compiler-level metrics can optimally prune the architectural design space, while still retaining Pareto-optimal configurations. Moreover, by exploring how implementation details impact cost and performance on real hardware, we highlight the importance of holistic software-to-hardware mapping flows, as the one presented herein.","sentences":["Coarse-Grain Reconfigurable Arrays (CGRAs) represent emerging low-power architectures designed to accelerate Compute-Intensive Loops (CILs).","The effectiveness of CGRAs in providing acceleration relies on the quality of mapping: how efficiently the CIL is compiled onto the platform.","State of the Art (SoA) compilation techniques utilize modulo scheduling to minimize the Iteration Interval (II) and use graph algorithms like Max-Clique Enumeration to address mapping challenges.","Our work approaches the mapping problem through a satisfiability (SAT) formulation.","We introduce the Kernel Mobility Schedule (KMS), an ad-hoc schedule used with the Data Flow Graph and CGRA architectural information to generate Boolean statements that, when satisfied, yield a valid mapping.","Experimental results demonstrate SAT-MapIt outperforming SoA alternatives in almost 50\\% of explored benchmarks.","Additionally, we evaluated the mapping results in a synthesizable CGRA design and emphasized the run-time metrics trends, i.e. energy efficiency and latency, across different CILs and CGRA sizes.","We show that a hardware-agnostic analysis performed on compiler-level metrics can optimally prune the architectural design space, while still retaining Pareto-optimal configurations.","Moreover, by exploring how implementation details impact cost and performance on real hardware, we highlight the importance of holistic software-to-hardware mapping flows, as the one presented herein."],"url":"http://arxiv.org/abs/2402.12834v1","category":"cs.AR"}
{"created":"2024-02-20 08:55:38","title":"The Fundamental Parameters of Astrophysical Plasma Turbulence and its Dissipation: Nonrelativistic Limit","abstract":"A specific set of dimensionless plasma and turbulence parameters is introduced to characterize the nature of turbulence and its dissipation in weakly collisional space and astrophysical plasmas. Key considerations are discussed for the development of predictive models of the turbulent plasma heating that characterize the partitioning of dissipated turbulent energy between the ion and electron species and between the perpendicular and parallel degrees of freedom for each species. Identifying the kinetic physical mechanisms that govern the damping of the turbulent fluctuations is a critical first step in constructing such turbulent heating models. A set of ten general plasma and turbulence parameters are defined, and reasonable approximations along with the exploitation of existing scaling theories for magnetohydrodynamic turbulence are used to reduce this general set of ten parameters to just three parameters in the isotropic temperature case. A critical step forward in this study is to identify the dependence of all of the proposed kinetic mechanisms for turbulent damping in terms of the same set of fundamental plasma and turbulence parameters. Analytical estimations of the scaling of each damping mechanism on these fundamental parameters are presented, and this information is synthesized to produce the first phase diagram for the turbulent damping mechanisms as a function of driving scale and ion plasma beta.","sentences":["A specific set of dimensionless plasma and turbulence parameters is introduced to characterize the nature of turbulence and its dissipation in weakly collisional space and astrophysical plasmas.","Key considerations are discussed for the development of predictive models of the turbulent plasma heating that characterize the partitioning of dissipated turbulent energy between the ion and electron species and between the perpendicular and parallel degrees of freedom for each species.","Identifying the kinetic physical mechanisms that govern the damping of the turbulent fluctuations is a critical first step in constructing such turbulent heating models.","A set of ten general plasma and turbulence parameters are defined, and reasonable approximations along with the exploitation of existing scaling theories for magnetohydrodynamic turbulence are used to reduce this general set of ten parameters to just three parameters in the isotropic temperature case.","A critical step forward in this study is to identify the dependence of all of the proposed kinetic mechanisms for turbulent damping in terms of the same set of fundamental plasma and turbulence parameters.","Analytical estimations of the scaling of each damping mechanism on these fundamental parameters are presented, and this information is synthesized to produce the first phase diagram for the turbulent damping mechanisms as a function of driving scale and ion plasma beta."],"url":"http://arxiv.org/abs/2402.12829v1","category":"astro-ph.SR"}
{"created":"2024-02-20 08:54:07","title":"SGD with Clipping is Secretly Estimating the Median Gradient","abstract":"There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient. For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself. Here we study SGD with robust gradient estimators based on estimating the median. We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise. We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof. Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework.","sentences":["There are several applications of stochastic optimization where one can benefit from a robust estimate of the gradient.","For example, domains such as distributed learning with corrupted nodes, the presence of large outliers in the training data, learning under privacy constraints, or even heavy-tailed noise due to the dynamics of the algorithm itself.","Here we study SGD with robust gradient estimators based on estimating the median.","We first consider computing the median gradient across samples, and show that the resulting method can converge even under heavy-tailed, state-dependent noise.","We then derive iterative methods based on the stochastic proximal point method for computing the geometric median and generalizations thereof.","Finally we propose an algorithm estimating the median gradient across iterations, and find that several well known methods - in particular different forms of clipping - are particular cases of this framework."],"url":"http://arxiv.org/abs/2402.12828v1","category":"stat.ML"}
{"created":"2024-02-20 08:52:45","title":"Quantum tricriticality and universal scaling in a tricritical quantum Rabi system","abstract":"Quantum tricriticality, a unique form of high-order criticality, is expected to exhibit fascinating features including unconventional critical exponents and universal scaling laws. However, a quantum tricritical point (QTCP) is much harder to access, and the corresponding phenomena at tricriticality have rarely been investigated. In this study, we explore a tricritical quantum Rabi model, which incorporates a nontrivial parameter for adjusting the coupling ratio between a cavity and a three-level atom. The QTCP emerges at the intersection of a first- and second-order superradiant phase transitions according to Landau theory. By using finite-frequency scaling analyses for quantum fluctuations and the mean photon number, universal critical exponents differentiate the QTCP from the second-order critical point. We find that the phase transition at the tricritical point goes beyond the conventional second-order phase transition. Our work explores an interesting direction in the generalization of the well-known Rabi model for the study of higher-order critical points due to its high control and tunability.","sentences":["Quantum tricriticality, a unique form of high-order criticality, is expected to exhibit fascinating features including unconventional critical exponents and universal scaling laws.","However, a quantum tricritical point (QTCP) is much harder to access, and the corresponding phenomena at tricriticality have rarely been investigated.","In this study, we explore a tricritical quantum Rabi model, which incorporates a nontrivial parameter for adjusting the coupling ratio between a cavity and a three-level atom.","The QTCP emerges at the intersection of a first- and second-order superradiant phase transitions according to Landau theory.","By using finite-frequency scaling analyses for quantum fluctuations and the mean photon number, universal critical exponents differentiate the QTCP from the second-order critical point.","We find that the phase transition at the tricritical point goes beyond the conventional second-order phase transition.","Our work explores an interesting direction in the generalization of the well-known Rabi model for the study of higher-order critical points due to its high control and tunability."],"url":"http://arxiv.org/abs/2402.12827v1","category":"quant-ph"}
{"created":"2024-02-20 08:41:23","title":"Identifying Factual Inconsistency in Summaries: Towards Effective Utilization of Large Language Model","abstract":"Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers. Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy? Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering. Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average. To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero-shot approaches by much larger LLM, serving as an effective and efficient ready-to-use scorer.","sentences":["Factual inconsistency poses a significant hurdle for the commercial deployment of abstractive summarizers.","Under this Large Language Model (LLM) era, this work focuses around two important questions: what is the best way to leverage LLM for factual inconsistency detection, and how could we distill a smaller LLM with both high efficiency and efficacy?","Three zero-shot paradigms are firstly proposed and evaluated across five diverse datasets: direct inference on the entire summary or each summary window; entity verification through question generation and answering.","Experiments suggest that LLM itself is capable to resolve this task train-free under the proper paradigm design, surpassing strong trained baselines by 2.8% on average.","To further promote practical utility, we then propose training strategies aimed at distilling smaller open-source LLM that learns to score the entire summary at once with high accuracy, which outperforms the zero-shot approaches by much larger LLM, serving as an effective and efficient ready-to-use scorer."],"url":"http://arxiv.org/abs/2402.12821v1","category":"cs.CL"}
{"created":"2024-02-20 08:38:24","title":"Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?","abstract":"When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results variance.","sentences":["When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model.","When enough labels are available, the specialised models outperform the general ones on many NLP tasks.","In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration.","Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones.","At the same time, the amount of required labelled data strongly depends on the task complexity and results variance."],"url":"http://arxiv.org/abs/2402.12819v1","category":"cs.CL"}
{"created":"2024-02-20 08:38:21","title":"ESA Science Programme Missions: Contributions and Exploitation -- ESA Mission Publications","abstract":"We examine over 68,000 refereed publications based on data from 25 missions in the ESA Science Programme and 11 additional missions in which ESA is involved as a junior partner. The publications cover the fields of astronomy, planetary science, and heliophysics and are spread over almost 50 years, spanning the period between the year a mission was launched and the end of 2021. We study the number of papers as a function of time and the evolution of several metrics, including citations and other indices. We also investigate the geographical distribution of the authors, and for ESA Member States we correlate the various indices with the level of financial contribution of the individual countries to the ESA Science Programme. We find that in general the involvement of the scientific communities in the various Member States follows the distribution expected from the countries' gross domestic products, with communities in some field and countries, both large and small, being particularly effective at turning data into scientific discoveries. We also analyse the differences between papers written by investigators directly involved in the provision of the payloads or in the definition of the scientific projects and those written by other scientists not directly involved in the process. We find that the latter, the so-called \"archival papers\", represent more than 50\\,\\% of the literature based on data from ESA Space Science missions, and have a similar impact on the literature in the respective fields, as judged by the number of citations. This highlights the importance of sharing and preserving the scientific data produced by the missions.","sentences":["We examine over 68,000 refereed publications based on data from 25 missions in the ESA Science Programme and 11 additional missions in which ESA is involved as a junior partner.","The publications cover the fields of astronomy, planetary science, and heliophysics and are spread over almost 50 years, spanning the period between the year a mission was launched and the end of 2021.","We study the number of papers as a function of time and the evolution of several metrics, including citations and other indices.","We also investigate the geographical distribution of the authors, and for ESA Member States we correlate the various indices with the level of financial contribution of the individual countries to the ESA Science Programme.","We find that in general the involvement of the scientific communities in the various Member States follows the distribution expected from the countries' gross domestic products, with communities in some field and countries, both large and small, being particularly effective at turning data into scientific discoveries.","We also analyse the differences between papers written by investigators directly involved in the provision of the payloads or in the definition of the scientific projects and those written by other scientists not directly involved in the process.","We find that the latter, the so-called \"archival papers\", represent more than 50\\,\\% of the literature based on data from ESA Space Science missions, and have a similar impact on the literature in the respective fields, as judged by the number of citations.","This highlights the importance of sharing and preserving the scientific data produced by the missions."],"url":"http://arxiv.org/abs/2402.12818v1","category":"astro-ph.IM"}
{"created":"2024-02-20 08:38:19","title":"On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness: Impact of Interactions and Systematic Choices","abstract":"While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data). We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration. To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs. Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format.","sentences":["While learning with limited labelled data can improve performance when the labels are lacking, it is also sensitive to the effects of uncontrolled randomness introduced by so-called randomness factors (e.g., varying order of data).","We propose a method to systematically investigate the effects of randomness factors while taking the interactions between them into consideration.","To measure the true effects of an individual randomness factor, our method mitigates the effects of other factors and observes how the performance varies across multiple runs.","Applying our method to multiple randomness factors across in-context learning and fine-tuning approaches on 7 representative text classification tasks and meta-learning on 3 tasks, we show that: 1) disregarding interactions between randomness factors in existing works caused inconsistent findings due to incorrect attribution of the effects of randomness factors, such as disproving the consistent sensitivity of in-context learning to sample order even with random sample selection; and 2) besides mutual interactions, the effects of randomness factors, especially sample order, are also dependent on more systematic choices unexplored in existing works, such as number of classes, samples per class or choice of prompt format."],"url":"http://arxiv.org/abs/2402.12817v1","category":"cs.CL"}
{"created":"2024-02-20 08:33:37","title":"Quantum fluctuations and unusual critical exponents in a quantum Rabi Triangle","abstract":"Quantum fluctuations of a quantum Rabi triangle are studied using an analytical approach beyond the mean-field theory. By applying an artificial magnetic field among three cavities, time-reversal symmetry breaking is manifested through a directional transfer dynamics of photons. In contrast to previous studies, we focus on the scaling exponents of the fluctuations of the local photon number and the position variance near the critical point. By accurate calculation using Bogoliubov transformation we show that two scaling laws emerge respectively for the frustrated cavity and the remaining cavities, which are associated with the geometric frustrations. Especially, for the frustrated cavity, the scaling exponent in the chiral superradiant phase is different from that in the frustrated antiferromagnetic superradiant phase without an artificial magnetic field. The unusual scaling exponents predict distinct universality classes from the single-cavity Rabi universality. We suggest that the accurate critical exponents in few-body system is useful for identifying exotic quantum phase transition in light-matter coupling system.","sentences":["Quantum fluctuations of a quantum Rabi triangle are studied using an analytical approach beyond the mean-field theory.","By applying an artificial magnetic field among three cavities, time-reversal symmetry breaking is manifested through a directional transfer dynamics of photons.","In contrast to previous studies, we focus on the scaling exponents of the fluctuations of the local photon number and the position variance near the critical point.","By accurate calculation using Bogoliubov transformation we show that two scaling laws emerge respectively for the frustrated cavity and the remaining cavities, which are associated with the geometric frustrations.","Especially, for the frustrated cavity, the scaling exponent in the chiral superradiant phase is different from that in the frustrated antiferromagnetic superradiant phase without an artificial magnetic field.","The unusual scaling exponents predict distinct universality classes from the single-cavity Rabi universality.","We suggest that the accurate critical exponents in few-body system is useful for identifying exotic quantum phase transition in light-matter coupling system."],"url":"http://arxiv.org/abs/2402.12815v1","category":"quant-ph"}
{"created":"2024-02-20 08:33:03","title":"Exploring the Interaction of Creative Writers with AI-Powered Writing Tools","abstract":"AI-based virtual assistants are increasingly used to support daily ideation tasks. The values or bias present in these agents can influence output in hidden ways. They may also affect how people perceive the ideas produced with these AI agents and lead to implications for the design of AI-based tools. We explored the effects of AI agents with different values on the ideation process and user perception of idea quality, ownership, agent competence, and values present in the output. Our study tasked 180 participants with brainstorming practical solutions to a set of problems with AI agents of different values. Results show no significant difference in self-evaluation of idea quality and perception of the agent based on value alignment; however, ideas generated reflected the AI's values and feeling of ownership is affected. This highlights an intricate interplay between AI values and human ideation, suggesting careful design considerations for future AI-supported brainstorming tools.","sentences":["AI-based virtual assistants are increasingly used to support daily ideation tasks.","The values or bias present in these agents can influence output in hidden ways.","They may also affect how people perceive the ideas produced with these AI agents and lead to implications for the design of AI-based tools.","We explored the effects of AI agents with different values on the ideation process and user perception of idea quality, ownership, agent competence, and values present in the output.","Our study tasked 180 participants with brainstorming practical solutions to a set of problems with AI agents of different values.","Results show no significant difference in self-evaluation of idea quality and perception of the agent based on value alignment; however, ideas generated reflected the AI's values and feeling of ownership is affected.","This highlights an intricate interplay between AI values and human ideation, suggesting careful design considerations for future AI-supported brainstorming tools."],"url":"http://arxiv.org/abs/2402.12814v1","category":"cs.HC"}
{"created":"2024-02-20 08:28:45","title":"PIP-Net: Pedestrian Intention Prediction in the Wild","abstract":"Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field. In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios. We offer two variants of PIP-Net designed for different camera mounts and setups. Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance. To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics. Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the model's contextual perception. Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance which is a breakthrough in current research studies in pedestrian intention prediction. Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios.","sentences":["Accurate pedestrian intention prediction (PIP) by Autonomous Vehicles (AVs) is one of the current research challenges in this field.","In this article, we introduce PIP-Net, a novel framework designed to predict pedestrian crossing intentions by AVs in real-world urban scenarios.","We offer two variants of PIP-Net designed for different camera mounts and setups.","Leveraging both kinematic data and spatial features from the driving scene, the proposed model employs a recurrent and temporal attention-based solution, outperforming state-of-the-art performance.","To enhance the visual representation of road users and their proximity to the ego vehicle, we introduce a categorical depth feature map, combined with a local motion flow feature, providing rich insights into the scene dynamics.","Additionally, we explore the impact of expanding the camera's field of view, from one to three cameras surrounding the ego vehicle, leading to enhancement in the model's contextual perception.","Depending on the traffic scenario and road environment, the model excels in predicting pedestrian crossing intentions up to 4 seconds in advance which is a breakthrough in current research studies in pedestrian intention prediction.","Finally, for the first time, we present the Urban-PIP dataset, a customised pedestrian intention prediction dataset, with multi-camera annotations in real-world automated driving scenarios."],"url":"http://arxiv.org/abs/2402.12810v1","category":"cs.CV"}
{"created":"2024-02-20 08:27:55","title":"Stability of pairwise social dilemma games: destructive agents, constructive agents, and their joint effects","abstract":"Destructive agents, who opt out of the game and indiscriminately harm others, paradoxically foster cooperation, representing an intriguing variant of the voluntary participation strategy. Yet, their impact on cooperation remains inadequately understood, particularly in the context of pairwise social dilemma games and in comparison to their counterparts, constructive agents, who opt out of the game but indiscriminately benefit others. Furthermore, little is known about the combined effects of both agent types on cooperation dynamics. Using replicator dynamics in infinite and well-mixed populations, we find that, contrary to their role in facilitating cooperation in multiplayer games, destructive agents fail to encourage cooperation in pairwise social dilemmas. Instead, they destabilize and may even replace defection in the prisoners' dilemma and stag-hunt games. Similarly, in the chicken game, they can destabilize or replace the mixed equilibrium of cooperation and defection, and they undermine cooperation in the harmony game. Conversely, constructive agents, when their payoffs exceed their contributions to opponents, can exhibit effects similar to destructive agents. However, if their payoffs are lower, while they destabilize defection in prisoners' dilemma and stag-hunt games, they do not disrupt the cooperation equilibrium in harmony games and have a negligible impact on the coexistence of cooperation in chicken games. The combination of destructive and constructive agents does not facilitate cooperation but instead generates complex evolutionary dynamics, including bi-stable, tri-stable, and quad-stable states, with outcomes contingent on their relative payoffs and game types. These results, taken together, enhance our understanding of the impact of the voluntary participation mechanism on cooperation, contributing to a more comprehensive understanding of its influence.","sentences":["Destructive agents, who opt out of the game and indiscriminately harm others, paradoxically foster cooperation, representing an intriguing variant of the voluntary participation strategy.","Yet, their impact on cooperation remains inadequately understood, particularly in the context of pairwise social dilemma games and in comparison to their counterparts, constructive agents, who opt out of the game but indiscriminately benefit others.","Furthermore, little is known about the combined effects of both agent types on cooperation dynamics.","Using replicator dynamics in infinite and well-mixed populations, we find that, contrary to their role in facilitating cooperation in multiplayer games, destructive agents fail to encourage cooperation in pairwise social dilemmas.","Instead, they destabilize and may even replace defection in the prisoners' dilemma and stag-hunt games.","Similarly, in the chicken game, they can destabilize or replace the mixed equilibrium of cooperation and defection, and they undermine cooperation in the harmony game.","Conversely, constructive agents, when their payoffs exceed their contributions to opponents, can exhibit effects similar to destructive agents.","However, if their payoffs are lower, while they destabilize defection in prisoners' dilemma and stag-hunt games, they do not disrupt the cooperation equilibrium in harmony games and have a negligible impact on the coexistence of cooperation in chicken games.","The combination of destructive and constructive agents does not facilitate cooperation but instead generates complex evolutionary dynamics, including bi-stable, tri-stable, and quad-stable states, with outcomes contingent on their relative payoffs and game types.","These results, taken together, enhance our understanding of the impact of the voluntary participation mechanism on cooperation, contributing to a more comprehensive understanding of its influence."],"url":"http://arxiv.org/abs/2402.12809v1","category":"physics.soc-ph"}
{"created":"2024-02-20 18:51:35","title":"Interaction-enhanced nesting in Spin-Fermion and Fermi-Hubbard models","abstract":"The spin-fermion (SF) model postulates that the dominant coupling between low-energy fermions in near critical metals is mediated by collective spin fluctuations (paramagnons) peaked at the N\\'{e}el wave vector, ${\\bf Q}_N$, connecting hot spots on opposite sides of the Fermi surface. It has been argued that strong correlations at hot spots lead to a Fermi surface deformation (FSD) featuring flat regions and increased nesting. This conjecture was confirmed in the perturbative self-consistent calculations when the paramagnon propagator dependence on momentum deviation from ${\\bf Q}_N$ is given by $\\chi^{-1} \\propto |\\Delta q|$. Using diagrammatic Monte Carlo (diagMC) technique we show that such a dependence holds only at temperatures orders of magnitude smaller than any other energy scale in the problem, indicating that a different mechanism may be at play. Instead, we find that a $\\chi^{-1} \\propto |\\Delta q|^{2}$ dependence yields a robust finite-$T$ scenario for achieving FSD. To link phenomenological and microscopic descriptions, we applied the connected determinant diagMC method to the $(t-t')$ Hubbard model and found that in this case: (i) the FSD is not very pronounced, and, instead, it is the lines of zeros of the renormalized dispersion relation that deform towards nesting; (ii) this phenomenon appears at large $U/t>5.5$ before the formation of electron and hole pockets; (iii) the static spin susceptibility is well described by $\\chi^{-1} \\propto |\\Delta q|^{2}$. Flat FS regions yield a non-trivial scenario for realizing a non-Fermi liquid state.","sentences":["The spin-fermion (SF) model postulates that the dominant coupling between low-energy fermions in near critical metals is mediated by collective spin fluctuations (paramagnons) peaked at the N\\'{e}el wave vector, ${\\bf Q}_N$, connecting hot spots on opposite sides of the Fermi surface.","It has been argued that strong correlations at hot spots lead to a Fermi surface deformation (FSD) featuring flat regions and increased nesting.","This conjecture was confirmed in the perturbative self-consistent calculations when the paramagnon propagator dependence on momentum deviation from ${\\bf Q}_N$ is given by $\\chi^{-1} \\propto |\\Delta q|$. Using diagrammatic Monte Carlo (diagMC) technique we show that such a dependence holds only at temperatures orders of magnitude smaller than any other energy scale in the problem, indicating that a different mechanism may be at play.","Instead, we find that a $\\chi^{-1} \\propto |\\Delta q|^{2}$ dependence yields a robust finite-$T$ scenario for achieving FSD.","To link phenomenological and microscopic descriptions, we applied the connected determinant diagMC method to the $(t-t')$ Hubbard model and found that in this case: (i) the FSD is not very pronounced, and, instead, it is the lines of zeros of the renormalized dispersion relation that deform towards nesting; (ii) this phenomenon appears at large $U/t>5.5$ before the formation of electron and hole pockets; (iii) the static spin susceptibility is well described by $\\chi^{-1} \\propto |\\Delta q|^{2}$. Flat FS regions yield a non-trivial scenario for realizing a non-Fermi liquid state."],"url":"http://arxiv.org/abs/2402.13238v1","category":"cond-mat.str-el"}
{"created":"2024-02-20 18:48:49","title":"SMORE: Similarity-based Hyperdimensional Domain Adaptation for Multi-Sensor Time Series Classification","abstract":"Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors. However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance. Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices. In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing. SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate the negative impacts of domain shifts. Our evaluation on a variety of multi-sensor time series classification tasks shows that SMORE achieves on average 1.98% higher accuracy than state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and 4.63x faster inference.","sentences":["Many real-world applications of the Internet of Things (IoT) employ machine learning (ML) algorithms to analyze time series information collected by interconnected sensors.","However, distribution shift, a fundamental challenge in data-driven ML, arises when a model is deployed on a data distribution different from the training data and can substantially degrade model performance.","Additionally, increasingly sophisticated deep neural networks (DNNs) are required to capture intricate spatial and temporal dependencies in multi-sensor time series data, often exceeding the capabilities of today's edge devices.","In this paper, we propose SMORE, a novel resource-efficient domain adaptation (DA) algorithm for multi-sensor time series classification, leveraging the efficient and parallel operations of hyperdimensional computing.","SMORE dynamically customizes test-time models with explicit consideration of the domain context of each sample to mitigate the negative impacts of domain shifts.","Our evaluation on a variety of multi-sensor time series classification tasks shows that SMORE achieves on average 1.98% higher accuracy than state-of-the-art (SOTA) DNN-based DA algorithms with 18.81x faster training and 4.63x faster inference."],"url":"http://arxiv.org/abs/2402.13233v1","category":"cs.LG"}
{"created":"2024-02-20 18:47:28","title":"Investigating Cultural Alignment of Large Language Models","abstract":"The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology. Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures? Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture. We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references. Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions. Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values. Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment. Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer.","sentences":["The intricate relationship between language and culture has long been a subject of exploration within the realm of linguistic anthropology.","Large Language Models (LLMs), promoted as repositories of collective human knowledge, raise a pivotal question: do these models genuinely encapsulate the diverse knowledge adopted by different cultures?","Our study reveals that these models demonstrate greater cultural alignment along two dimensions -- firstly, when prompted with the dominant language of a specific culture, and secondly, when pretrained with a refined mixture of languages employed by that culture.","We quantify cultural alignment by simulating sociological surveys, comparing model responses to those of actual survey participants as references.","Specifically, we replicate a survey conducted in various regions of Egypt and the United States through prompting LLMs with different pretraining data mixtures in both Arabic and English with the personas of the real respondents and the survey questions.","Further analysis reveals that misalignment becomes more pronounced for underrepresented personas and for culturally sensitive topics, such as those probing social values.","Finally, we introduce Anthropological Prompting, a novel method leveraging anthropological reasoning to enhance cultural alignment.","Our study emphasizes the necessity for a more balanced multilingual pretraining dataset to better represent the diversity of human experience and the plurality of different cultures with many implications on the topic of cross-lingual transfer."],"url":"http://arxiv.org/abs/2402.13231v1","category":"cs.CL"}
{"created":"2024-02-20 18:06:00","title":"Design and Flight Demonstration of a Quadrotor for Urban Mapping and Target Tracking Research","abstract":"This paper describes the hardware design and flight demonstration of a small quadrotor with imaging sensors for urban mapping, hazard avoidance, and target tracking research. The vehicle is equipped with five cameras, including two pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a two-axis gimbaled camera. An onboard NVIDIA Jetson Orin Nano computer running the Robot Operating System software is used for data collection. An autonomous tracking behavior was implemented to coordinate the motion of the quadrotor and gimbaled camera to track a moving GPS coordinate. The data collection system was demonstrated through a flight test that tracked a moving GPS-tagged vehicle through a series of roads and parking lots. A map of the environment was reconstructed from the collected images using the Direct Sparse Odometry (DSO) algorithm. The performance of the quadrotor was also characterized by acoustic noise, communication range, battery voltage in hover, and maximum speed tests.","sentences":["This paper describes the hardware design and flight demonstration of a small quadrotor with imaging sensors for urban mapping, hazard avoidance, and target tracking research.","The vehicle is equipped with five cameras, including two pairs of fisheye stereo cameras that enable a nearly omnidirectional view and a two-axis gimbaled camera.","An onboard NVIDIA Jetson Orin Nano computer running the Robot Operating System software is used for data collection.","An autonomous tracking behavior was implemented to coordinate the motion of the quadrotor and gimbaled camera to track a moving GPS coordinate.","The data collection system was demonstrated through a flight test that tracked a moving GPS-tagged vehicle through a series of roads and parking lots.","A map of the environment was reconstructed from the collected images using the Direct Sparse Odometry (DSO) algorithm.","The performance of the quadrotor was also characterized by acoustic noise, communication range, battery voltage in hover, and maximum speed tests."],"url":"http://arxiv.org/abs/2402.13195v1","category":"cs.RO"}
{"created":"2024-02-20 18:00:13","title":"Integrating Blockchain technology within an Information Ecosystem","abstract":"Context: Blockchain-based Information Ecosystems (BBIEs) are a type of information ecosystem in which blockchain technology is used to provide a trust mechanism among parties and to manage shared business logic, breaking the traditional scheme of Information Ecosystems dominated by a leading company and leveraging the decentralization of data management, information flow, and business logic. Objective: In this paper, we propose architecture and technical aspects concerning the creation of a BBIE, underlining the advantages supplied and the logic decomposition among the business and storage components. Method: The requirements are derived from the current needs of the collaborative business and the data collected by surveying practitioners. To get these needs we followed the Grounded Theory research approach. We validate our architectural schema against a case study dealing with the management of a wine supply chain, also involving different companies and supervision authorities. Results: The proposed solution integrates blockchain-based applications with the existing information system as a module of the ecosystem, leveraging on the low costs, scalability, and high-level security because of the restricted access to the network. Conclusion: We must go a long way in deepening and refining the possibilities offered by technology in supporting innovative multi-organizational business models. BBIEs can contribute substantially to paving the way in such a direction.","sentences":["Context: Blockchain-based Information Ecosystems (BBIEs) are a type of information ecosystem in which blockchain technology is used to provide a trust mechanism among parties and to manage shared business logic, breaking the traditional scheme of Information Ecosystems dominated by a leading company and leveraging the decentralization of data management, information flow, and business logic.","Objective:","In this paper, we propose architecture and technical aspects concerning the creation of a BBIE, underlining the advantages supplied and the logic decomposition among the business and storage components.","Method: The requirements are derived from the current needs of the collaborative business and the data collected by surveying practitioners.","To get these needs we followed the Grounded Theory research approach.","We validate our architectural schema against a case study dealing with the management of a wine supply chain, also involving different companies and supervision authorities.","Results:","The proposed solution integrates blockchain-based applications with the existing information system as a module of the ecosystem, leveraging on the low costs, scalability, and high-level security because of the restricted access to the network.","Conclusion: We must go a long way in deepening and refining the possibilities offered by technology in supporting innovative multi-organizational business models.","BBIEs can contribute substantially to paving the way in such a direction."],"url":"http://arxiv.org/abs/2402.13191v1","category":"cs.DC"}
{"created":"2024-02-20 17:07:08","title":"AnnoTheia: A Semi-Automatic Annotation Toolkit for Audio-Visual Speech Technologies","abstract":"More than 7,000 known languages are spoken around the world. However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies. Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English. This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed. In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription. In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task. The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub.","sentences":["More than 7,000 known languages are spoken around the world.","However, due to the lack of annotated resources, only a small fraction of them are currently covered by speech technologies.","Albeit self-supervised speech representations, recent massive speech corpora collections, as well as the organization of challenges, have alleviated this inequality, most studies are mainly benchmarked on English.","This situation is aggravated when tasks involving both acoustic and visual speech modalities are addressed.","In order to promote research on low-resource languages for audio-visual speech technologies, we present AnnoTheia, a semi-automatic annotation toolkit that detects when a person speaks on the scene and the corresponding transcription.","In addition, to show the complete process of preparing AnnoTheia for a language of interest, we also describe the adaptation of a pre-trained model for active speaker detection to Spanish, using a database not initially conceived for this type of task.","The AnnoTheia toolkit, tutorials, and pre-trained models are available on GitHub."],"url":"http://arxiv.org/abs/2402.13152v1","category":"cs.CV"}
{"created":"2024-02-20 15:20:32","title":"Josephson Signatures of the Superconducting Higgs/Amplitude Mode","abstract":"The Higgs/amplitude collective mode in superconductors corresponds to oscillations of the amplitude of the order parameter. While its detection typically relies on optical techniques with an external electromagnetic field resonant with the Higgs mode, we present a purely transport-based setup wherein it is excited in a voltage biased Josephson junction. Demonstrating the importance of the order parameter dynamics, we find that in highly transparent junctions featuring single-band s-wave superconductors, the interplay of the Higgs resonance and the Josephson physics enhances the second harmonic Josephson current oscillating at twice the Josephson frequency. If the leads have unequal equilibrium superconducting gaps, this second harmonic component may even eclipse its usual first harmonic counterpart, thus furnishing a unique hallmark of the Higgs oscillations.","sentences":["The Higgs/amplitude collective mode in superconductors corresponds to oscillations of the amplitude of the order parameter.","While its detection typically relies on optical techniques with an external electromagnetic field resonant with the Higgs mode, we present a purely transport-based setup wherein it is excited in a voltage biased Josephson junction.","Demonstrating the importance of the order parameter dynamics, we find that in highly transparent junctions featuring single-band s-wave superconductors, the interplay of the Higgs resonance and the Josephson physics enhances the second harmonic Josephson current oscillating at twice the Josephson frequency.","If the leads have unequal equilibrium superconducting gaps, this second harmonic component may even eclipse its usual first harmonic counterpart, thus furnishing a unique hallmark of the Higgs oscillations."],"url":"http://arxiv.org/abs/2402.13074v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 14:39:35","title":"Measurements of Lund subjet multiplicities in 13 TeV proton-proton collisions with the ATLAS detector","abstract":"This Letter presents a differential cross-section measurement of Lund subjet multiplicities, suitable for testing current and future parton shower Monte Carlo algorithms. This measurement is made in dijet events in 140 fb$^{-1}$ of $\\sqrt{s}=13$ TeV proton-proton collision data collected with the ATLAS detector at CERN's Large Hadron Collider. The data are unfolded to account for acceptance and detector-related effects, and are then compared with several Monte Carlo models and to recent resummed analytical calculations. The experimental precision achieved in the measurement allows tests of higher-order effects in QCD predictions. Most predictions fail to accurately describe the measured data, particularly at large values of jet transverse momentum accessible at the Large Hadron Collider, indicating the measurement's utility as an input to future parton shower developments and other studies probing fundamental properties of QCD and the production of hadronic final states up to the TeV-scale.","sentences":["This Letter presents a differential cross-section measurement of Lund subjet multiplicities, suitable for testing current and future parton shower Monte Carlo algorithms.","This measurement is made in dijet events in 140 fb$^{-1}$ of $\\sqrt{s}=13$ TeV proton-proton collision data collected with the ATLAS detector at CERN's Large Hadron Collider.","The data are unfolded to account for acceptance and detector-related effects, and are then compared with several Monte Carlo models and to recent resummed analytical calculations.","The experimental precision achieved in the measurement allows tests of higher-order effects in QCD predictions.","Most predictions fail to accurately describe the measured data, particularly at large values of jet transverse momentum accessible at the Large Hadron Collider, indicating the measurement's utility as an input to future parton shower developments and other studies probing fundamental properties of QCD and the production of hadronic final states up to the TeV-scale."],"url":"http://arxiv.org/abs/2402.13052v1","category":"hep-ex"}
{"created":"2024-02-20 14:09:28","title":"Solving the decision-making analysis differential equation using eye fixation data in Unity software with Hermite Long-Short-Term Memory","abstract":"Decision-making is a fundamental component of our personal and professional lives. To analyze decision-making accuracy, this study proposes a virtual environment designed as an industrial town to investigate the relationship between eye movements and decision-making. Eye tracking provides a tool to examine eye movements, which contain information related to eye position, head position, and gaze direction. The game is designed using Unity software, with the collected data being analyzed using a differential equation and the Hermite neural network method. The game is used to identify the behaviors exhibited by bad and good individuals and differentiate between them before taking action. This paper investigates the accuracy of an individual's decision-making process by analyzing their eye movements and the correctness of the decisions made.","sentences":["Decision-making is a fundamental component of our personal and professional lives.","To analyze decision-making accuracy, this study proposes a virtual environment designed as an industrial town to investigate the relationship between eye movements and decision-making.","Eye tracking provides a tool to examine eye movements, which contain information related to eye position, head position, and gaze direction.","The game is designed using Unity software, with the collected data being analyzed using a differential equation and the Hermite neural network method.","The game is used to identify the behaviors exhibited by bad and good individuals and differentiate between them before taking action.","This paper investigates the accuracy of an individual's decision-making process by analyzing their eye movements and the correctness of the decisions made."],"url":"http://arxiv.org/abs/2402.13027v1","category":"cs.HC"}
{"created":"2024-02-20 13:38:04","title":"SzCORE: A Seizure Community Open-source Research Evaluation framework for the validation of EEG-based automated seizure detection algorithms","abstract":"The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring. Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging. This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics. In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms. Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics. We also propose the 10-20 seizure detection benchmark, a machine-learning benchmark based on public datasets converted to a standardized format. This benchmark defines the machine-learning task as well as reporting metrics. We illustrate the use of the benchmark by evaluating a set of existing seizure detection algorithms. The SzCORE (Seizure Community Open-source Research Evaluation) framework and benchmark are made publicly available along with an open-source software library to facilitate research use, while enabling rigorous evaluation of the clinical significance of the algorithms, fostering a collective effort to more optimally detect seizures to improve the lives of people with epilepsy.","sentences":["The need for high-quality automated seizure detection algorithms based on electroencephalography (EEG) becomes ever more pressing with the increasing use of ambulatory and long-term EEG monitoring.","Heterogeneity in validation methods of these algorithms influences the reported results and makes comprehensive evaluation and comparison challenging.","This heterogeneity concerns in particular the choice of datasets, evaluation methodologies, and performance metrics.","In this paper, we propose a unified framework designed to establish standardization in the validation of EEG-based seizure detection algorithms.","Based on existing guidelines and recommendations, the framework introduces a set of recommendations and standards related to datasets, file formats, EEG data input content, seizure annotation input and output, cross-validation strategies, and performance metrics.","We also propose the 10-20 seizure detection benchmark, a machine-learning benchmark based on public datasets converted to a standardized format.","This benchmark defines the machine-learning task as well as reporting metrics.","We illustrate the use of the benchmark by evaluating a set of existing seizure detection algorithms.","The SzCORE (Seizure Community Open-source Research Evaluation) framework and benchmark are made publicly available along with an open-source software library to facilitate research use, while enabling rigorous evaluation of the clinical significance of the algorithms, fostering a collective effort to more optimally detect seizures to improve the lives of people with epilepsy."],"url":"http://arxiv.org/abs/2402.13005v1","category":"eess.SP"}
{"created":"2024-02-20 13:21:57","title":"Distributionally Robust Graph-based Recommendation System","abstract":"With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS). However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts. Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS. Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse. To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation. DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support. Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently. Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts. The code is available at https://github.com/WANGBohaO-jpg/DR-GNN .","sentences":["With the capacity to capture high-order collaborative signals, Graph Neural Networks (GNNs) have emerged as powerful methods in Recommender Systems (RS).","However, their efficacy often hinges on the assumption that training and testing data share the same distribution (a.k.a. IID assumption), and exhibits significant declines under distribution shifts.","Distribution shifts commonly arises in RS, often attributed to the dynamic nature of user preferences or ubiquitous biases during data collection in RS.","Despite its significance, researches on GNN-based recommendation against distribution shift are still sparse.","To bridge this gap, we propose Distributionally Robust GNN (DR-GNN) that incorporates Distributional Robust Optimization (DRO) into the GNN-based recommendation.","DR-GNN addresses two core challenges: 1) To enable DRO to cater to graph data intertwined with GNN, we reinterpret GNN as a graph smoothing regularizer, thereby facilitating the nuanced application of DRO; 2) Given the typically sparse nature of recommendation data, which might impede robust optimization, we introduce slight perturbations in the training distribution to expand its support.","Notably, while DR-GNN involves complex optimization, it can be implemented easily and efficiently.","Our extensive experiments validate the effectiveness of DR-GNN against three typical distribution shifts.","The code is available at https://github.com/WANGBohaO-jpg/DR-GNN ."],"url":"http://arxiv.org/abs/2402.12994v1","category":"cs.IR"}
{"created":"2024-02-20 10:43:01","title":"Evidence of apsidal motion and a possible co-moving companion star detected in the WASP-19 system","abstract":"Love numbers measure the reaction of a celestial body to perturbing forces, such as the centrifugal force caused by rotation, or tidal forces resulting from the interaction with a companion body. These parameters are related to the interior density profile. The non-point mass nature of the host star and a planet orbiting around each other contributes to the periastron precession. The rate of this precession is characterized mainly by the second-order Love number, which offers an opportunity to determine its value. We collected all available radial velocity (RV) data, along with the transit and occultation times from the previous investigations of the system. We supplemented the data set with 19 new RV data points of the host star WASP-19A obtained by HARPS. Here, we summarize the technique for modeling the RV observations and the photometric transit timing variations (TTVs) to determine the rate of periastron precession in this system for the first time. We excluded the presence of a second possible planet up to a period of ~4200 d and with a radial velocity amplitude bigger than ~1 m/s. We show that a constant period is not able to reproduce the observed radial velocities. We also investigated and excluded the possibility of tidal decay and long-term acceleration in the system. However, the inclusion of a small periastron precession term did indeed improve the quality of the fit. We measured the periastron precession rate to be 233 $^{+25}_{-35}$''/. By assuming synchronous rotation for the planet, it indicates a k2 Love number of 0.20 $^{+0.02}_{-0.03}$ for WASP-19Ab. The derived k2 value of the planet has the same order of magnitude as the estimated fluid Love number of other Jupiter-sized exoplanets (WASP-18Ab, WASP-103b, and WASP-121b). A low value of k2 indicates a higher concentration of mass toward the planetary nucleus.","sentences":["Love numbers measure the reaction of a celestial body to perturbing forces, such as the centrifugal force caused by rotation, or tidal forces resulting from the interaction with a companion body.","These parameters are related to the interior density profile.","The non-point mass nature of the host star and a planet orbiting around each other contributes to the periastron precession.","The rate of this precession is characterized mainly by the second-order Love number, which offers an opportunity to determine its value.","We collected all available radial velocity (RV) data, along with the transit and occultation times from the previous investigations of the system.","We supplemented the data set with 19 new RV data points of the host star WASP-19A obtained by HARPS.","Here, we summarize the technique for modeling the RV observations and the photometric transit timing variations (TTVs) to determine the rate of periastron precession in this system for the first time.","We excluded the presence of a second possible planet up to a period of ~4200 d and with a radial velocity amplitude bigger than ~1 m/s. We show that a constant period is not able to reproduce the observed radial velocities.","We also investigated and excluded the possibility of tidal decay and long-term acceleration in the system.","However, the inclusion of a small periastron precession term did indeed improve the quality of the fit.","We measured the periastron precession rate to be 233 $^{+25}_{-35}$''/. By assuming synchronous rotation for the planet, it indicates a k2 Love number of 0.20 $^{+0.02}_{-0.03}$ for WASP-19Ab.","The derived k2 value of the planet has the same order of magnitude as the estimated fluid Love number of other Jupiter-sized exoplanets (WASP-18Ab, WASP-103b, and WASP-121b).","A low value of k2 indicates a higher concentration of mass toward the planetary nucleus."],"url":"http://arxiv.org/abs/2402.12896v1","category":"astro-ph.EP"}
{"created":"2024-02-20 10:23:00","title":"GRAFFORD: A Benchmark Dataset for Testing the Knowledge of Object Affordances of Language and Vision Models","abstract":"We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs). Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks. In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding. To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes. Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances. Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances. We also observe that pre-trained VLMs do not necessarily capture object affordances effectively. Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and VLMs. Our research contributes a novel dataset for language grounding tasks, and presents insights into LM capabilities, advancing the understanding of object affordances. Codes and data are available at https://github.com/sayantan11995/Affordance","sentences":["We investigate the knowledge of object affordances in pre-trained language models (LMs) and pre-trained Vision-Language models (VLMs).","Transformers-based large pre-trained language models (PTLM) learn contextual representation from massive amounts of unlabeled text and are shown to perform impressively in downstream NLU tasks.","In parallel, a growing body of literature shows that PTLMs fail inconsistently and non-intuitively, showing a lack of reasoning and grounding.","To take a first step toward quantifying the effect of grounding (or lack thereof), we curate a novel and comprehensive dataset of object affordances -- GrAFFORD, characterized by 15 affordance classes.","Unlike affordance datasets collected in vision and language domains, we annotate in-the-wild sentences with objects and affordances.","Experimental results reveal that PTLMs exhibit limited reasoning abilities when it comes to uncommon object affordances.","We also observe that pre-trained VLMs do not necessarily capture object affordances effectively.","Through few-shot fine-tuning, we demonstrate improvement in affordance knowledge in PTLMs and VLMs.","Our research contributes a novel dataset for language grounding tasks, and presents insights into LM capabilities, advancing the understanding of object affordances.","Codes and data are available at https://github.com/sayantan11995/Affordance"],"url":"http://arxiv.org/abs/2402.12881v1","category":"cs.CL"}
{"created":"2024-02-20 09:27:49","title":"Almost fifty years of Mets\u00e4hovi solar observations on 37 GHz with recovered digitised historical maps","abstract":"Context. Aalto University Mets\\\"ahovi Radio Observatory has collected solar intensity maps for over 45 years. Most data coverage is on the 37 GHz frequency band, tracking emissions primarily at the chromosphere and coronal transition region. The data spans four sunspot cycles or two solar magnetic cycles. Aims. We present solar maps, including recently restored data prior to 1989, spanning 1978 to 2020 after correcting for observational and temporal bias. Methods. The solar maps consist of radio intensity sampled along scanlines of the antenna sweep. We fit a circular disk to the set of intensity samples, neglecting any exceptional features in the fitting process to improve accuracy. Applying a simple astronomical model of Sun and Earth, we assign each radio specimen its heliographic coordinates at the time of observation. We bin the sample data by time and heliographic latitude to construct a diagram analoguous to the classic butterfly diagram of sunspot activity. Results. Radio butterfly diagram at 37 GHz, spanning solar cycles 21 to 24 and extending near to the poles. Conclusions. We have developed a method for compensating for seasonal and atmospheric bias in the radio data, as well as correcting for the effects of limb brightening and beamwidth convolution to isolate physical features. Our observations are consistent with observations in nearby bandwidths and indicate the possibility of polar cyclic behaviour with a period exceeding the solar 11 year cycle. Key words. Solar physics, radioastronomy, butterfly diagram, solar cycle","sentences":["Context.","Aalto University Mets\\\"ahovi Radio Observatory has collected solar intensity maps for over 45 years.","Most data coverage is on the 37 GHz frequency band, tracking emissions primarily at the chromosphere and coronal transition region.","The data spans four sunspot cycles or two solar magnetic cycles.","Aims.","We present solar maps, including recently restored data prior to 1989, spanning 1978 to 2020 after correcting for observational and temporal bias.","Methods.","The solar maps consist of radio intensity sampled along scanlines of the antenna sweep.","We fit a circular disk to the set of intensity samples, neglecting any exceptional features in the fitting process to improve accuracy.","Applying a simple astronomical model of Sun and Earth, we assign each radio specimen its heliographic coordinates at the time of observation.","We bin the sample data by time and heliographic latitude to construct a diagram analoguous to the classic butterfly diagram of sunspot activity.","Results.","Radio butterfly diagram at 37 GHz, spanning solar cycles 21 to 24 and extending near to the poles.","Conclusions.","We have developed a method for compensating for seasonal and atmospheric bias in the radio data, as well as correcting for the effects of limb brightening and beamwidth convolution to isolate physical features.","Our observations are consistent with observations in nearby bandwidths and indicate the possibility of polar cyclic behaviour with a period exceeding the solar 11 year cycle.","Key words.","Solar physics, radioastronomy, butterfly diagram, solar cycle"],"url":"http://arxiv.org/abs/2402.12849v1","category":"astro-ph.SR"}
{"created":"2024-02-20 09:05:40","title":"ESA Science Programme Missions: Contributions and Exploitation -- Payload Provision","abstract":"We have collected data pertaining to the Principal Investigators (PIs), and co-PIs (where appropriate) for all ESA-led Science Directorate missions since the first such launch, namely of COS-B in 1975. For a total of 28 missions (including 4 in preparation awaiting launch), 437 individuals have been recorded along with their institution, location, academic age and gender. We have correlated the number of PIs by country with the financial contribution of those countries to the ESA Science programme. We have also investigated issues associated with age and gender of the PIs. As a result of these analyses, we make suggestions for actions which ESA and its Member States may wish to consider with the aim of encouraging equity and diversity while still placing scientific excellence as the overarching goal.","sentences":["We have collected data pertaining to the Principal Investigators (PIs), and co-PIs (where appropriate) for all ESA-led Science Directorate missions since the first such launch, namely of COS-B in 1975.","For a total of 28 missions (including 4 in preparation awaiting launch), 437 individuals have been recorded along with their institution, location, academic age and gender.","We have correlated the number of PIs by country with the financial contribution of those countries to the ESA Science programme.","We have also investigated issues associated with age and gender of the PIs.","As a result of these analyses, we make suggestions for actions which ESA and its Member States may wish to consider with the aim of encouraging equity and diversity while still placing scientific excellence as the overarching goal."],"url":"http://arxiv.org/abs/2402.12837v1","category":"astro-ph.IM"}
{"created":"2024-02-20 08:30:46","title":"Scalable Decentralized Algorithms for Online Personalized Mean Estimation","abstract":"In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based approach, with complexity of O( r |A| log |A|) and O(r |A|), respectively. We establish conditions under which both algorithms yield asymptotically optimal estimates and offer a theoretical characterization of their performance.","sentences":["In numerous settings, agents lack sufficient data to directly learn a model.","Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ.","A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved.","This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean.","Existing algorithms face impractical space and time complexities (quadratic in the number of agents A).","To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based approach, with complexity of O( r |A| log |A|) and O(r |A|), respectively.","We establish conditions under which both algorithms yield asymptotically optimal estimates and offer a theoretical characterization of their performance."],"url":"http://arxiv.org/abs/2402.12812v1","category":"cs.LG"}
{"created":"2024-02-20 08:20:49","title":"Few shot clinical entity recognition in three languages: Masked language models outperform LLM prompting","abstract":"Large Language Models are becoming the go-to solution for many natural language processing tasks, including in specialized domains where their few-shot capacities are expected to yield high performance in low-resource settings. Herein, we aim to assess the performance of Large Language Models for few shot clinical entity recognition in multiple languages. We evaluate named entity recognition in English, French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard corpora. We assess the performance of 10 auto-regressive language models using prompting and 16 masked language models used for text encoding in a biLSTM-CRF supervised tagger. We create a few-shot set-up by limiting the amount of annotated data available to 100 sentences. Our experiments show that although larger prompt-based models tend to achieve competitive F-measure for named entity recognition outside the clinical domain, this level of performance does not carry over to the clinical domain where lighter supervised taggers relying on masked language models perform better, even with the performance drop incurred from the few-shot set-up. In all experiments, the CO2 impact of masked language models is inferior to that of auto-regressive models. Results are consistent over the three languages and suggest that few-shot learning using Large language models is not production ready for named entity recognition in the clinical domain. Instead, models could be used for speeding-up the production of gold standard annotated data.","sentences":["Large Language Models are becoming the go-to solution for many natural language processing tasks, including in specialized domains where their few-shot capacities are expected to yield high performance in low-resource settings.","Herein, we aim to assess the performance of Large Language Models for few shot clinical entity recognition in multiple languages.","We evaluate named entity recognition in English, French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard corpora.","We assess the performance of 10 auto-regressive language models using prompting and 16 masked language models used for text encoding in a biLSTM-CRF supervised tagger.","We create a few-shot set-up by limiting the amount of annotated data available to 100 sentences.","Our experiments show that although larger prompt-based models tend to achieve competitive F-measure for named entity recognition outside the clinical domain, this level of performance does not carry over to the clinical domain where lighter supervised taggers relying on masked language models perform better, even with the performance drop incurred from the few-shot set-up.","In all experiments, the CO2 impact of masked language models is inferior to that of auto-regressive models.","Results are consistent over the three languages and suggest that few-shot learning using Large language models is not production ready for named entity recognition in the clinical domain.","Instead, models could be used for speeding-up the production of gold standard annotated data."],"url":"http://arxiv.org/abs/2402.12801v1","category":"cs.CL"}
{"created":"2024-02-20 08:10:16","title":"The dynamics of neural codes in biological and artificial neural networks","abstract":"Advancing our knowledge of how the brain processes information remains a key challenge in neuroscience. This thesis combines three different approaches to the study of the dynamics of neural networks and their encoding representations: a computational approach, that builds upon basic biological features of neurons and their networks to construct effective models that can simulate their structure and dynamics; a machine-learning approach, which draws a parallel with the functional capabilities of brain networks, allowing us to infer the dynamical and encoding properties required to solve certain input-processing tasks; and a final, theoretical treatment, which will take us into the fascinating hypothesis of the \"critical\" brain as the mathematical foundation that can explain the emergent collective properties arising from the interactions of millions of neurons. Hand in hand with physics, we venture into the realm of neuroscience to explain the existence of quasi-universal scaling properties across brain regions, setting out to quantify the distance of their dynamics from a critical point. Next, we move into the grounds of artificial intelligence, where the very same theory of critical phenomena will prove very useful for explaining the effects of biologically-inspired plasticity rules in the forecasting ability of Reservoir Computers. Halfway into our journey, we explore the concept of neural representations of external stimuli, unveiling a surprising link between the dynamical regime of neural networks and the optimal topological properties of such representation manifolds. The thesis ends with the singular problem of representational drift in the process of odor encoding carried out by the olfactory cortex, uncovering the potential synaptic plasticity mechanisms that could explain this recently observed phenomenon.","sentences":["Advancing our knowledge of how the brain processes information remains a key challenge in neuroscience.","This thesis combines three different approaches to the study of the dynamics of neural networks and their encoding representations: a computational approach, that builds upon basic biological features of neurons and their networks to construct effective models that can simulate their structure and dynamics; a machine-learning approach, which draws a parallel with the functional capabilities of brain networks, allowing us to infer the dynamical and encoding properties required to solve certain input-processing tasks; and a final, theoretical treatment, which will take us into the fascinating hypothesis of the \"critical\" brain as the mathematical foundation that can explain the emergent collective properties arising from the interactions of millions of neurons.","Hand in hand with physics, we venture into the realm of neuroscience to explain the existence of quasi-universal scaling properties across brain regions, setting out to quantify the distance of their dynamics from a critical point.","Next, we move into the grounds of artificial intelligence, where the very same theory of critical phenomena will prove very useful for explaining the effects of biologically-inspired plasticity rules in the forecasting ability of Reservoir Computers.","Halfway into our journey, we explore the concept of neural representations of external stimuli, unveiling a surprising link between the dynamical regime of neural networks and the optimal topological properties of such representation manifolds.","The thesis ends with the singular problem of representational drift in the process of odor encoding carried out by the olfactory cortex, uncovering the potential synaptic plasticity mechanisms that could explain this recently observed phenomenon."],"url":"http://arxiv.org/abs/2402.12796v1","category":"q-bio.NC"}
{"created":"2024-02-20 08:08:07","title":"Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles","abstract":"Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH). In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site. To minimize human intervention, this paper introduces a novel methodology for autonomous 3D Reality Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors. These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach. The outcomes of this automated process may find applications in digital twin platforms, facilitating secure monitoring and management of cultural heritage sites and spaces, in both indoor and outdoor environments.","sentences":["Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH).","In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site.","To minimize human intervention, this paper introduces a novel methodology for autonomous 3D Reality","Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors.","These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach.","The outcomes of this automated process may find applications in digital twin platforms, facilitating secure monitoring and management of cultural heritage sites and spaces, in both indoor and outdoor environments."],"url":"http://arxiv.org/abs/2402.12794v1","category":"cs.RO"}
{"created":"2024-02-20 07:57:38","title":"Fair Classifiers Without Fair Training: An Influence-Guided Data Sampling Approach","abstract":"A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training. Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important. In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information. Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training. We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in sampling or used in training. Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm.","sentences":["A fair classifier should ensure the benefit of people from different groups, while the group information is often sensitive and unsuitable for model training.","Therefore, learning a fair classifier but excluding sensitive attributes in the training dataset is important.","In this paper, we study learning fair classifiers without implementing fair training algorithms to avoid possible leakage of sensitive information.","Our theoretical analyses validate the possibility of this approach, that traditional training on a dataset with an appropriate distribution shift can reduce both the upper bound for fairness disparity and model generalization error, indicating that fairness and accuracy can be improved simultaneously with simply traditional training.","We then propose a tractable solution to progressively shift the original training data during training by sampling influential data, where the sensitive attribute of new data is not accessed in sampling or used in training.","Extensive experiments on real-world data demonstrate the effectiveness of our proposed algorithm."],"url":"http://arxiv.org/abs/2402.12789v1","category":"cs.LG"}
{"created":"2024-02-20 07:51:43","title":"Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations","abstract":"In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that \"even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different\". Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods.","sentences":["In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles.","The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality.","When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn.","In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly.","Our goal is to teach the LLM that \"even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different\".","Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different.","To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles.","We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles.","Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods."],"url":"http://arxiv.org/abs/2402.12786v1","category":"cs.CL"}
{"created":"2024-02-20 07:47:39","title":"Advancing GenAI Assisted Programming--A Comparative Study on Prompt Efficiency and Code Quality Between GPT-4 and GLM-4","abstract":"This study aims to explore the best practices for utilizing GenAI as a programming tool, through a comparative analysis between GPT-4 and GLM-4. By evaluating prompting strategies at different levels of complexity, we identify that simplest and straightforward prompting strategy yields best code generation results. Additionally, adding a CoT-like preliminary confirmation step would further increase the success rate. Our results reveal that while GPT-4 marginally outperforms GLM-4, the difference is minimal for average users. In our simplified evaluation model, we see a remarkable 30 to 100-fold increase in code generation efficiency over traditional coding norms. Our GenAI Coding Workshop highlights the effectiveness and accessibility of the prompting methodology developed in this study. We observe that GenAI-assisted coding would trigger a paradigm shift in programming landscape, which necessitates developers to take on new roles revolving around supervising and guiding GenAI, and to focus more on setting high-level objectives and engaging more towards innovation.","sentences":["This study aims to explore the best practices for utilizing GenAI as a programming tool, through a comparative analysis between GPT-4 and GLM-4.","By evaluating prompting strategies at different levels of complexity, we identify that simplest and straightforward prompting strategy yields best code generation results.","Additionally, adding a CoT-like preliminary confirmation step would further increase the success rate.","Our results reveal that while GPT-4 marginally outperforms GLM-4, the difference is minimal for average users.","In our simplified evaluation model, we see a remarkable 30 to 100-fold increase in code generation efficiency over traditional coding norms.","Our GenAI Coding Workshop highlights the effectiveness and accessibility of the prompting methodology developed in this study.","We observe that GenAI-assisted coding would trigger a paradigm shift in programming landscape, which necessitates developers to take on new roles revolving around supervising and guiding GenAI, and to focus more on setting high-level objectives and engaging more towards innovation."],"url":"http://arxiv.org/abs/2402.12782v1","category":"cs.SE"}
{"created":"2024-02-20 07:17:00","title":"Tuning the bandstructure of electrons in a two-dimensional artificial electrostatic crystal in GaAs quantum wells","abstract":"The electronic properties of solids are determined by the crystal structure and interactions between electrons, giving rise to a variety of collective phenomena including superconductivity, strange metals and correlated insulators. The mechanisms underpinning many of these collective phenomena remain unknown, driving interest in creating artificial crystals which replicate the system of interest while allowing precise control of key parameters. Cold atoms trapped in optical lattices provide great flexibility and tunability [1, 2], but cannot replicate the long range Coulomb interactions and long range hopping that drive collective phenomena in real crystals. Solid state approaches support long range hopping and interactions, but previous attempts with laterally patterned semiconductor systems were not able to create tunable low disorder artificial crystals, while approaches based on Moire superlattices in twisted two-dimensional (2D) materials [3, 4] have limited tunability and control of lattice geometry. Here we demonstrate the formation of highly tunable artificial crystals by superimposing a periodic electrostatic potential on the 2D electron gas in an ultrashallow (25 nm deep) GaAs quantum well. The 100 nm period artificial crystal is identified by the formation of a new bandstructure, different from the original cubic crystal and unique to the artificial triangular lattice: transport measurements show the Hall coefficient changing sign as the chemical potential sweeps through the artificial bands. Uniquely, the artificial bandstructure can be continuously tuned from parabolic free-electron bands into linear graphene-like and flat kagome-like bands in a single device. This approach allows the formation arbitrary geometry 2D artificial crystals, opening a new route to studying collective quantum states.","sentences":["The electronic properties of solids are determined by the crystal structure and interactions between electrons, giving rise to a variety of collective phenomena including superconductivity, strange metals and correlated insulators.","The mechanisms underpinning many of these collective phenomena remain unknown, driving interest in creating artificial crystals which replicate the system of interest while allowing precise control of key parameters.","Cold atoms trapped in optical lattices provide great flexibility and tunability","[1, 2], but cannot replicate the long range Coulomb interactions and long range hopping that drive collective phenomena in real crystals.","Solid state approaches support long range hopping and interactions, but previous attempts with laterally patterned semiconductor systems were not able to create tunable low disorder artificial crystals, while approaches based on Moire superlattices in twisted two-dimensional (2D) materials [3, 4] have limited tunability and control of lattice geometry.","Here we demonstrate the formation of highly tunable artificial crystals by superimposing a periodic electrostatic potential on the 2D electron gas in an ultrashallow (25 nm deep) GaAs quantum well.","The 100 nm period artificial crystal is identified by the formation of a new bandstructure, different from the original cubic crystal and unique to the artificial triangular lattice: transport measurements show the Hall coefficient changing sign as the chemical potential sweeps through the artificial bands.","Uniquely, the artificial bandstructure can be continuously tuned from parabolic free-electron bands into linear graphene-like and flat kagome-like bands in a single device.","This approach allows the formation arbitrary geometry 2D artificial crystals, opening a new route to studying collective quantum states."],"url":"http://arxiv.org/abs/2402.12769v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 07:03:59","title":"FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework","abstract":"Graph anomaly detection (GAD) aims to identify anomalous graphs that significantly deviate from other ones, which has raised growing attention due to the broad existence and complexity of graph-structured data in many real-world scenarios. However, existing GAD methods usually execute with centralized training, which may lead to privacy leakage risk in some sensitive cases, thereby impeding collaboration among organizations seeking to collectively develop robust GAD models. Although federated learning offers a promising solution, the prevalent non-IID problems and high communication costs present significant challenges, particularly pronounced in collaborations with graph data distributed among different participants. To tackle these challenges, we propose an effective federated graph anomaly detection framework (FGAD). We first introduce an anomaly generator to perturb the normal graphs to be anomalous, and train a powerful anomaly detector by distinguishing generated anomalous graphs from normal ones. Then, we leverage a student model to distill knowledge from the trained anomaly detector (teacher model), which aims to maintain the personality of local models and alleviate the adverse impact of non-IID problems. Moreover, we design an effective collaborative learning mechanism that facilitates the personalization preservation of local models and significantly reduces communication costs among clients. Empirical results of the GAD tasks on non-IID graphs compared with state-of-the-art baselines demonstrate the superiority and efficiency of the proposed FGAD method.","sentences":["Graph anomaly detection (GAD) aims to identify anomalous graphs that significantly deviate from other ones, which has raised growing attention due to the broad existence and complexity of graph-structured data in many real-world scenarios.","However, existing GAD methods usually execute with centralized training, which may lead to privacy leakage risk in some sensitive cases, thereby impeding collaboration among organizations seeking to collectively develop robust GAD models.","Although federated learning offers a promising solution, the prevalent non-IID problems and high communication costs present significant challenges, particularly pronounced in collaborations with graph data distributed among different participants.","To tackle these challenges, we propose an effective federated graph anomaly detection framework (FGAD).","We first introduce an anomaly generator to perturb the normal graphs to be anomalous, and train a powerful anomaly detector by distinguishing generated anomalous graphs from normal ones.","Then, we leverage a student model to distill knowledge from the trained anomaly detector (teacher model), which aims to maintain the personality of local models and alleviate the adverse impact of non-IID problems.","Moreover, we design an effective collaborative learning mechanism that facilitates the personalization preservation of local models and significantly reduces communication costs among clients.","Empirical results of the GAD tasks on non-IID graphs compared with state-of-the-art baselines demonstrate the superiority and efficiency of the proposed FGAD method."],"url":"http://arxiv.org/abs/2402.12761v1","category":"cs.LG"}
{"created":"2024-02-20 06:58:49","title":"A User-Friendly Framework for Generating Model-Preferred Prompts in Text-to-Image Synthesis","abstract":"Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images. Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts. To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization. For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods. For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts. Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs. Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts. Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results. Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics.","sentences":["Well-designed prompts have demonstrated the potential to guide text-to-image models in generating amazing images.","Although existing prompt engineering methods can provide high-level guidance, it is challenging for novice users to achieve the desired results by manually entering prompts due to a discrepancy between novice-user-input prompts and the model-preferred prompts.","To bridge the distribution gap between user input behavior and model training datasets, we first construct a novel Coarse-Fine Granularity Prompts dataset (CFP) and propose a novel User-Friendly Fine-Grained Text Generation framework (UF-FGTG) for automated prompt optimization.","For CFP, we construct a novel dataset for text-to-image tasks that combines coarse and fine-grained prompts to facilitate the development of automated prompt generation methods.","For UF-FGTG, we propose a novel framework that automatically translates user-input prompts into model-preferred prompts.","Specifically, we propose a prompt refiner that continually rewrites prompts to empower users to select results that align with their unique needs.","Meanwhile, we integrate image-related loss functions from the text-to-image model into the training process of text generation to generate model-preferred prompts.","Additionally, we propose an adaptive feature extraction module to ensure diversity in the generated results.","Experiments demonstrate that our approach is capable of generating more visually appealing and diverse images than previous state-of-the-art methods, achieving an average improvement of 5% across six quality and aesthetic metrics."],"url":"http://arxiv.org/abs/2402.12760v1","category":"cs.MM"}
{"created":"2024-02-20 06:38:10","title":"Model Composition for Multimodal Large Language Models","abstract":"Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities. However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities. In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model. Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters. Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance. To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities. Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities.","sentences":["Recent developments in Multimodal Large Language Models (MLLMs) have shown rapid progress, moving towards the goal of creating versatile MLLMs that understand inputs from various modalities.","However, existing methods typically rely on joint training with paired multimodal instruction data, which is resource-intensive and challenging to extend to new modalities.","In this paper, we propose a new paradigm through the model composition of existing MLLMs to create a new model that retains the modal understanding capabilities of each original model.","Our basic implementation, NaiveMC, demonstrates the effectiveness of this paradigm by reusing modality encoders and merging LLM parameters.","Furthermore, we introduce DAMC to address parameter interference and mismatch issues during the merging process, thereby enhancing the model performance.","To facilitate research in this area, we propose MCUB, a benchmark for assessing ability of MLLMs to understand inputs from diverse modalities.","Experiments on this benchmark and four other multimodal understanding tasks show significant improvements over baselines, proving that model composition can create a versatile model capable of processing inputs from multiple modalities."],"url":"http://arxiv.org/abs/2402.12750v1","category":"cs.CV"}
{"created":"2024-02-20 06:37:31","title":"Me LLaMA: Foundation Large Language Models for Medical Applications","abstract":"Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications. However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets. This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data. Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets. Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 6 out of 8 datasets and GPT-4 in 3 out of 8 datasets. In addition, we empirically investigated the catastrophic forgetting problem, and our results show that Me LLaMA models outperform other medical LLMs. Me LLaMA is one of the first and largest open-source foundational LLMs designed for the medical domain, using both biomedical and clinical data. It exhibits superior performance across both general and medical tasks compared to other medical LLMs, rendering it an attractive choice for medical AI applications. All resources are available at: https://github.com/BIDS-Xu-Lab/Me-LLaMA.","sentences":["Recent large language models (LLMs) like ChatGPT and LLaMA have shown great promise in many AI applications.","However, their performance on medical tasks is suboptimal and can be further improved by training on large domain-specific datasets.","This study introduces Me LLaMA, a medical LLM family including foundation models - Me LLaMA 13/70B and their chat-enhanced versions - Me LLaMA 13/70B-chat, developed through the continual pre-training and instruction tuning of LLaMA2 using large medical data.","Our domain-specific data suite for training and evaluation, includes a large-scale continual pre-training dataset with 129B tokens, an instruction tuning dataset with 214k samples, and a medical evaluation benchmark (MIBE) across six tasks with 14 datasets.","Our extensive evaluation using MIBE shows that Me LLaMA models surpass existing open-source medical LLMs in zero-shot and few-shot learning and outperform commercial giants like ChatGPT on 6 out of 8 datasets and GPT-4 in 3 out of 8 datasets.","In addition, we empirically investigated the catastrophic forgetting problem, and our results show that Me LLaMA models outperform other medical LLMs.","Me LLaMA is one of the first and largest open-source foundational LLMs designed for the medical domain, using both biomedical and clinical data.","It exhibits superior performance across both general and medical tasks compared to other medical LLMs, rendering it an attractive choice for medical AI applications.","All resources are available at: https://github.com/BIDS-Xu-Lab/Me-LLaMA."],"url":"http://arxiv.org/abs/2402.12749v1","category":"cs.CL"}
{"created":"2024-02-20 06:19:55","title":"APT-MMF: An advanced persistent threat actor attribution method based on multimodal and multilevel feature fusion","abstract":"Threat actor attribution is a crucial defense strategy for combating advanced persistent threats (APTs). Cyber threat intelligence (CTI), which involves analyzing multisource heterogeneous data from APTs, plays an important role in APT actor attribution. The current attribution methods extract features from different CTI perspectives and employ machine learning models to classify CTI reports according to their threat actors. However, these methods usually extract only one kind of feature and ignore heterogeneous information, especially the attributes and relations of indicators of compromise (IOCs), which form the core of CTI. To address these problems, we propose an APT actor attribution method based on multimodal and multilevel feature fusion (APT-MMF). First, we leverage a heterogeneous attributed graph to characterize APT reports and their IOC information. Then, we extract and fuse multimodal features, including attribute type features, natural language text features and topological relationship features, to construct comprehensive node representations. Furthermore, we design multilevel heterogeneous graph attention networks to learn the deep hidden features of APT report nodes; these networks integrate IOC type-level, metapath-based neighbor node-level, and metapath semantic-level attention. Utilizing multisource threat intelligence, we construct a heterogeneous attributed graph dataset for verification purposes. The experimental results show that our method not only outperforms the existing methods but also demonstrates its good interpretability for attribution analysis tasks.","sentences":["Threat actor attribution is a crucial defense strategy for combating advanced persistent threats (APTs).","Cyber threat intelligence (CTI), which involves analyzing multisource heterogeneous data from APTs, plays an important role in APT actor attribution.","The current attribution methods extract features from different CTI perspectives and employ machine learning models to classify CTI reports according to their threat actors.","However, these methods usually extract only one kind of feature and ignore heterogeneous information, especially the attributes and relations of indicators of compromise (IOCs), which form the core of CTI.","To address these problems, we propose an APT actor attribution method based on multimodal and multilevel feature fusion (APT-MMF).","First, we leverage a heterogeneous attributed graph to characterize APT reports and their IOC information.","Then, we extract and fuse multimodal features, including attribute type features, natural language text features and topological relationship features, to construct comprehensive node representations.","Furthermore, we design multilevel heterogeneous graph attention networks to learn the deep hidden features of APT report nodes; these networks integrate IOC type-level, metapath-based neighbor node-level, and metapath semantic-level attention.","Utilizing multisource threat intelligence, we construct a heterogeneous attributed graph dataset for verification purposes.","The experimental results show that our method not only outperforms the existing methods but also demonstrates its good interpretability for attribution analysis tasks."],"url":"http://arxiv.org/abs/2402.12743v1","category":"cs.CR"}
{"created":"2024-02-20 06:14:30","title":"MuLan: Multimodal-LLM Agent for Progressive Multi-Object Diffusion","abstract":"Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings. In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter. MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion. Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task. Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt. Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for. We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan. The results demonstrate the superiority of MuLan in generating multiple objects over baselines. The code is available on https://github.com/measure-infinity/mulan-code.","sentences":["Existing text-to-image models still struggle to generate images of multiple objects, especially in handling their spatial positions, relative sizes, overlapping, and attribute bindings.","In this paper, we develop a training-free Multimodal-LLM agent (MuLan) to address these challenges by progressive multi-object generation with planning and feedback control, like a human painter.","MuLan harnesses a large language model (LLM) to decompose a prompt to a sequence of sub-tasks, each generating only one object conditioned on previously generated objects by stable diffusion.","Unlike existing LLM-grounded methods, MuLan only produces a high-level plan at the beginning while the exact size and location of each object are determined by an LLM and attention guidance upon each sub-task.","Moreover, MuLan adopts a vision-language model (VLM) to provide feedback to the image generated in each sub-task and control the diffusion model to re-generate the image if it violates the original prompt.","Hence, each model in every step of MuLan only needs to address an easy sub-task it is specialized for.","We collect 200 prompts containing multi-objects with spatial relationships and attribute bindings from different benchmarks to evaluate MuLan.","The results demonstrate the superiority of MuLan in generating multiple objects over baselines.","The code is available on https://github.com/measure-infinity/mulan-code."],"url":"http://arxiv.org/abs/2402.12741v1","category":"cs.CV"}
{"created":"2024-02-20 06:05:36","title":"Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues","abstract":"Mental health care poses an increasingly serious challenge to modern societies. In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems. However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models. For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors. To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data. Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human counselors.","sentences":["Mental health care poses an increasingly serious challenge to modern societies.","In this context, there has been a surge in research that utilizes information technologies to address mental health problems, including those aiming to develop counseling dialogue systems.","However, there is a need for more evaluations of the performance of counseling dialogue systems that use large language models.","For this study, we collected counseling dialogue data via role-playing scenarios involving expert counselors, and the utterances were annotated with the intentions of the counselors.","To determine the feasibility of a dialogue system in real-world counseling scenarios, third-party counselors evaluated the appropriateness of responses from human counselors and those generated by GPT-4 in identical contexts in role-play dialogue data.","Analysis of the evaluation results showed that the responses generated by GPT-4 were competitive with those of human counselors."],"url":"http://arxiv.org/abs/2402.12738v1","category":"cs.CL"}
{"created":"2024-02-20 06:01:31","title":"CST: Calibration Side-Tuning for Parameter and Memory Efficient Transfer Learning","abstract":"Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects. However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference. This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions. This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet. The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process. Furthermore, this paper has conducted an analysis on multiple fine-tuning strategies and have implemented their application within ResNet, thereby expanding the research on fine-tuning strategies for object detection networks. Besides, this paper carried out extensive experiments using five benchmark datasets. The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the finetune schemes is achieved.","sentences":["Achieving a universally high accuracy in object detection is quite challenging, and the mainstream focus in the industry currently lies on detecting specific classes of objects.","However, deploying one or multiple object detection networks requires a certain amount of GPU memory for training and storage capacity for inference.","This presents challenges in terms of how to effectively coordinate multiple object detection tasks under resource-constrained conditions.","This paper introduces a lightweight fine-tuning strategy called Calibration side tuning, which integrates aspects of adapter tuning and side tuning to adapt the successful techniques employed in transformers for use with ResNet.","The Calibration side tuning architecture that incorporates maximal transition calibration, utilizing a small number of additional parameters to enhance network performance while maintaining a smooth training process.","Furthermore, this paper has conducted an analysis on multiple fine-tuning strategies and have implemented their application within ResNet, thereby expanding the research on fine-tuning strategies for object detection networks.","Besides, this paper carried out extensive experiments using five benchmark datasets.","The experimental results demonstrated that this method outperforms other compared state-of-the-art techniques, and a better balance between the complexity and performance of the finetune schemes is achieved."],"url":"http://arxiv.org/abs/2402.12736v1","category":"cs.CV"}
{"created":"2024-02-20 05:57:01","title":"BMLP: Behavior-aware MLP for Heterogeneous Sequential Recommendation","abstract":"In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying. Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors. However, most multi-behavior approaches have limitations in learning the relationship between different behaviors. In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP). Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users' purchase intent. Compared with mainstream sequence models, MLP is competitive in terms of accuracy and has unique advantages in simplicity and efficiency. Extensive experiments show that BMLP achieves significant improvement over state-of-the-art algorithms on four public datasets. In addition, its pure MLP architecture leads to a linear time complexity.","sentences":["In real recommendation scenarios, users often have different types of behaviors, such as clicking and buying.","Existing research methods show that it is possible to capture the heterogeneous interests of users through different types of behaviors.","However, most multi-behavior approaches have limitations in learning the relationship between different behaviors.","In this paper, we propose a novel multilayer perceptron (MLP)-based heterogeneous sequential recommendation method, namely behavior-aware multilayer perceptron (BMLP).","Specifically, it has two main modules, including a heterogeneous interest perception (HIP) module, which models behaviors at multiple granularities through behavior types and transition relationships, and a purchase intent perception (PIP) module, which adaptively fuses subsequences of auxiliary behaviors to capture users' purchase intent.","Compared with mainstream sequence models, MLP is competitive in terms of accuracy and has unique advantages in simplicity and efficiency.","Extensive experiments show that BMLP achieves significant improvement over state-of-the-art algorithms on four public datasets.","In addition, its pure MLP architecture leads to a linear time complexity."],"url":"http://arxiv.org/abs/2402.12733v1","category":"cs.IR"}
{"created":"2024-02-20 05:46:29","title":"UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation","abstract":"This paper describes the system we developed for SemEval-2024 Task 1, \"Semantic Textual Relatedness for African and Asian Languages.\" The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our model results for the 12 languages in subtask C resulted in 1st place for Africaans, 2nd place for Indonesian, and 3rd place for English with low performance for the remaining 9 languages.","sentences":["This paper describes the system we developed for SemEval-2024 Task 1, \"Semantic Textual Relatedness for African and Asian Languages.\"","The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages.","We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs).","Pre-trained large language models have been extensively used for machine translation and semantic similarity.","Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C.","Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages.","Our model results for the 12 languages in subtask C resulted in 1st place for Africaans, 2nd place for Indonesian, and 3rd place for English with low performance for the remaining 9 languages."],"url":"http://arxiv.org/abs/2402.12730v1","category":"cs.CL"}
{"created":"2024-02-20 05:39:32","title":"Scalable and reliable deep transfer learning for intelligent fault detection via multi-scale neural processes embedded with knowledge","abstract":"Deep transfer learning (DTL) is a fundamental method in the field of Intelligent Fault Detection (IFD). It aims to mitigate the degradation of method performance that arises from the discrepancies in data distribution between training set (source domain) and testing set (target domain). Considering the fact that fault data collection is challenging and certain faults are scarce, DTL-based methods face the limitation of available observable data, which reduces the detection performance of the methods in the target domain. Furthermore, DTL-based methods lack comprehensive uncertainty analysis that is essential for building reliable IFD systems. To address the aforementioned problems, this paper proposes a novel DTL-based method known as Neural Processes-based deep transfer learning with graph convolution network (GTNP). Feature-based transfer strategy of GTNP bridges the data distribution discrepancies of source domain and target domain in high-dimensional space. Both the joint modeling based on global and local latent variables and sparse sampling strategy reduce the demand of observable data in the target domain. The multi-scale uncertainty analysis is obtained by using the distribution characteristics of global and local latent variables. Global analysis of uncertainty enables GTNP to provide quantitative values that reflect the complexity of methods and the difficulty of tasks. Local analysis of uncertainty allows GTNP to model uncertainty (confidence of the fault detection result) at each sample affected by noise and bias. The validation of the proposed method is conducted across 3 IFD tasks, consistently showing the superior detection performance of GTNP compared to the other DTL-based methods.","sentences":["Deep transfer learning (DTL) is a fundamental method in the field of Intelligent Fault Detection (IFD).","It aims to mitigate the degradation of method performance that arises from the discrepancies in data distribution between training set (source domain) and testing set (target domain).","Considering the fact that fault data collection is challenging and certain faults are scarce, DTL-based methods face the limitation of available observable data, which reduces the detection performance of the methods in the target domain.","Furthermore, DTL-based methods lack comprehensive uncertainty analysis that is essential for building reliable IFD systems.","To address the aforementioned problems, this paper proposes a novel DTL-based method known as Neural Processes-based deep transfer learning with graph convolution network (GTNP).","Feature-based transfer strategy of GTNP bridges the data distribution discrepancies of source domain and target domain in high-dimensional space.","Both the joint modeling based on global and local latent variables and sparse sampling strategy reduce the demand of observable data in the target domain.","The multi-scale uncertainty analysis is obtained by using the distribution characteristics of global and local latent variables.","Global analysis of uncertainty enables GTNP to provide quantitative values that reflect the complexity of methods and the difficulty of tasks.","Local analysis of uncertainty allows GTNP to model uncertainty (confidence of the fault detection result) at each sample affected by noise and bias.","The validation of the proposed method is conducted across 3 IFD tasks, consistently showing the superior detection performance of GTNP compared to the other DTL-based methods."],"url":"http://arxiv.org/abs/2402.12729v1","category":"cs.LG"}
{"created":"2024-02-20 05:32:24","title":"Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering","abstract":"Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two benchmark datasets show the superiority of MAIL with 24x less resources.","sentences":["Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs).","While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations.","Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios.","To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL).","It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning.","Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts.","(iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion.","We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums.","Extensive experiments on two benchmark datasets show the superiority of MAIL with 24x less resources."],"url":"http://arxiv.org/abs/2402.12728v1","category":"cs.CV"}
{"created":"2024-02-20 05:28:13","title":"Diffusion Posterior Sampling is Computationally Intractable","abstract":"Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$. In posterior sampling, one is also given a measurement model $p(y \\mid x)$ and a measurement $y$, and would like to sample from $p(x \\mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.   In this paper we show that posterior sampling is \\emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \\emph{every} algorithm takes superpolynomial time, even though \\emph{unconditional} sampling is provably fast. We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert.","sentences":["Diffusion models are a remarkably effective way of learning and sampling from a distribution $p(x)$.","In posterior sampling, one is also given a measurement model $p(y \\mid x)$ and a measurement $y$, and would like to sample from $p(x \\mid y)$. Posterior sampling is useful for tasks such as inpainting, super-resolution, and MRI reconstruction, so a number of recent works have given algorithms to heuristically approximate it; but none are known to converge to the correct distribution in polynomial time.   ","In this paper we show that posterior sampling is \\emph{computationally intractable}: under the most basic assumption in cryptography -- that one-way functions exist -- there are instances for which \\emph{every} algorithm takes superpolynomial time, even though \\emph{unconditional} sampling is provably fast.","We also show that the exponential-time rejection sampling algorithm is essentially optimal under the stronger plausible assumption that there are one-way functions that take exponential time to invert."],"url":"http://arxiv.org/abs/2402.12727v1","category":"cs.LG"}
{"created":"2024-02-20 05:06:20","title":"PAC-FNO: Parallel-Structured All-Component Fourier Neural Operators for Recognizing Low-Quality Images","abstract":"A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it. However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts. While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice. To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem. Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model. We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the original, downstream model. Moreover, the proposed PAC-FNO is ready to work with existing image recognition models. Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference.","sentences":["A standard practice in developing image recognition models is to train a model on a specific image resolution and then deploy it.","However, in real-world inference, models often encounter images different from the training sets in resolution and/or subject to natural variations such as weather changes, noise types and compression artifacts.","While traditional solutions involve training multiple models for different resolutions or input variations, these methods are computationally expensive and thus do not scale in practice.","To this end, we propose a novel neural network model, parallel-structured and all-component Fourier neural operator (PAC-FNO), that addresses the problem.","Unlike conventional feed-forward neural networks, PAC-FNO operates in the frequency domain, allowing it to handle images of varying resolutions within a single model.","We also propose a two-stage algorithm for training PAC-FNO with a minimal modification to the original, downstream model.","Moreover, the proposed PAC-FNO is ready to work with existing image recognition models.","Extensively evaluating methods with seven image recognition benchmarks, we show that the proposed PAC-FNO improves the performance of existing baseline models on images with various resolutions by up to 77.1% and various types of natural variations in the images at inference."],"url":"http://arxiv.org/abs/2402.12721v1","category":"cs.CV"}
{"created":"2024-02-20 05:05:28","title":"Revisiting the Information Capacity of Neural Network Watermarks: Upper Bound Estimation and Beyond","abstract":"To trace the copyright of deep neural networks, an owner can embed its identity information into its model as a watermark. The capacity of the watermark quantify the maximal volume of information that can be verified from the watermarked model. Current studies on capacity focus on the ownership verification accuracy under ordinary removal attacks and fail to capture the relationship between robustness and fidelity. This paper studies the capacity of deep neural network watermarks from an information theoretical perspective. We propose a new definition of deep neural network watermark capacity analogous to channel capacity, analyze its properties, and design an algorithm that yields a tight estimation of its upper bound under adversarial overwriting. We also propose a universal non-invasive method to secure the transmission of the identity message beyond capacity by multiple rounds of ownership verification. Our observations provide evidence for neural network owners and defenders that are curious about the tradeoff between the integrity of their ownership and the performance degradation of their products.","sentences":["To trace the copyright of deep neural networks, an owner can embed its identity information into its model as a watermark.","The capacity of the watermark quantify the maximal volume of information that can be verified from the watermarked model.","Current studies on capacity focus on the ownership verification accuracy under ordinary removal attacks and fail to capture the relationship between robustness and fidelity.","This paper studies the capacity of deep neural network watermarks from an information theoretical perspective.","We propose a new definition of deep neural network watermark capacity analogous to channel capacity, analyze its properties, and design an algorithm that yields a tight estimation of its upper bound under adversarial overwriting.","We also propose a universal non-invasive method to secure the transmission of the identity message beyond capacity by multiple rounds of ownership verification.","Our observations provide evidence for neural network owners and defenders that are curious about the tradeoff between the integrity of their ownership and the performance degradation of their products."],"url":"http://arxiv.org/abs/2402.12720v1","category":"cs.CR"}
{"created":"2024-02-20 03:59:27","title":"From Cloud to Edge: Rethinking Generative AI for Low-Resource Design Challenges","abstract":"Generative Artificial Intelligence (AI) has shown tremendous prospects in all aspects of technology, including design. However, due to its heavy demand on resources, it is usually trained on large computing infrastructure and often made available as a cloud-based service. In this position paper, we consider the potential, challenges, and promising approaches for generative AI for design on the edge, i.e., in resource-constrained settings where memory, compute, energy (battery) and network connectivity may be limited. Adapting generative AI for such settings involves overcoming significant hurdles, primarily in how to streamline complex models to function efficiently in low-resource environments. This necessitates innovative approaches in model compression, efficient algorithmic design, and perhaps even leveraging edge computing. The objective is to harness the power of generative AI in creating bespoke solutions for design problems, such as medical interventions, farm equipment maintenance, and educational material design, tailored to the unique constraints and needs of remote areas. These efforts could democratize access to advanced technology and foster sustainable development, ensuring universal accessibility and environmental consideration of AI-driven design benefits.","sentences":["Generative Artificial Intelligence (AI) has shown tremendous prospects in all aspects of technology, including design.","However, due to its heavy demand on resources, it is usually trained on large computing infrastructure and often made available as a cloud-based service.","In this position paper, we consider the potential, challenges, and promising approaches for generative AI for design on the edge, i.e., in resource-constrained settings where memory, compute, energy (battery) and network connectivity may be limited.","Adapting generative AI for such settings involves overcoming significant hurdles, primarily in how to streamline complex models to function efficiently in low-resource environments.","This necessitates innovative approaches in model compression, efficient algorithmic design, and perhaps even leveraging edge computing.","The objective is to harness the power of generative AI in creating bespoke solutions for design problems, such as medical interventions, farm equipment maintenance, and educational material design, tailored to the unique constraints and needs of remote areas.","These efforts could democratize access to advanced technology and foster sustainable development, ensuring universal accessibility and environmental consideration of AI-driven design benefits."],"url":"http://arxiv.org/abs/2402.12702v1","category":"cs.AI"}
{"created":"2024-02-20 03:48:42","title":"A fully-integrated lattice Boltzmann method for fluid-structure interaction","abstract":"We present a fully-integrated lattice Boltzmann (LB) method for fluid--structure interaction (FSI) simulations that efficiently models deformable solids in complex suspensions and active systems. Our Eulerian method (LBRMT) couples finite-strain solids to the LB fluid on the same fixed computational grid with the reference map technique (RMT). An integral part of the LBRMT is a new LB boundary condition for moving deformable interfaces across different densities. With this fully Eulerian solid--fluid coupling, the LBRMT is well-suited for parallelization and simulating multi-body contact without remeshing or extra meshes. We validate its accuracy via a benchmark of a deformable solid in a lid-driven cavity, then showcase its versatility through examples of soft solids rotating and settling. With simulations of complex suspensions mixing, we highlight potentials of the LBRMT for studying collective behavior in soft matter and biofluid dynamics.","sentences":["We present a fully-integrated lattice Boltzmann (LB) method for fluid--structure interaction (FSI) simulations that efficiently models deformable solids in complex suspensions and active systems.","Our Eulerian method (LBRMT) couples finite-strain solids to the LB fluid on the same fixed computational grid with the reference map technique (RMT).","An integral part of the LBRMT is a new LB boundary condition for moving deformable interfaces across different densities.","With this fully Eulerian solid--fluid coupling, the LBRMT is well-suited for parallelization and simulating multi-body contact without remeshing or extra meshes.","We validate its accuracy via a benchmark of a deformable solid in a lid-driven cavity, then showcase its versatility through examples of soft solids rotating and settling.","With simulations of complex suspensions mixing, we highlight potentials of the LBRMT for studying collective behavior in soft matter and biofluid dynamics."],"url":"http://arxiv.org/abs/2402.12696v1","category":"physics.flu-dyn"}
{"created":"2024-02-20 03:22:56","title":"Platform-Driven Collaboration Patterns: Structural Evolution Over Time and Scale","abstract":"Within an increasingly digitalized organizational landscape, this research delves into the dynamics of decentralized collaboration, contrasting it with traditional collaboration models. An effective capturing of high-level collaborations (beyond direct massages) is introduced as the network construction methodology including both temporal and content dimensions of user collaborations - an Alternating Timed Interaction (ATI) metric as the first aspect, and a quantitative strategy of thematic similarity as the second aspect. This study validates three hypotheses that collectively underscore the complexities of digital team dynamics within sociotechnical systems: Firstly, it establishes the significant influence of problem context on team structures in work environments, emphasizing the need to consider the specific nature of tasks in analyzing collaborative dynamics. Secondly, the study reveals specific evolving patterns of team structures on digital platforms concerning team size and artifact maturity. Lastly, it identifies substantial differences in team structure patterns between digital platforms and traditional organizational settings, underscoring the unexplored nature of digital collaboration dynamics. The findings of this study are instrumental for organizations navigating the digital era, offering insights into effective knowledge sharing in the decentralized leadership of digital teams. By mapping out network structures and collaborative patterns, this study, with a focus on Wikipedia as a representative digital platform, paves the way for strategic interventions to optimize digital team dynamics and align them with broader organizational goals.","sentences":["Within an increasingly digitalized organizational landscape, this research delves into the dynamics of decentralized collaboration, contrasting it with traditional collaboration models.","An effective capturing of high-level collaborations (beyond direct massages) is introduced as the network construction methodology including both temporal and content dimensions of user collaborations - an Alternating Timed Interaction (ATI) metric as the first aspect, and a quantitative strategy of thematic similarity as the second aspect.","This study validates three hypotheses that collectively underscore the complexities of digital team dynamics within sociotechnical systems: Firstly, it establishes the significant influence of problem context on team structures in work environments, emphasizing the need to consider the specific nature of tasks in analyzing collaborative dynamics.","Secondly, the study reveals specific evolving patterns of team structures on digital platforms concerning team size and artifact maturity.","Lastly, it identifies substantial differences in team structure patterns between digital platforms and traditional organizational settings, underscoring the unexplored nature of digital collaboration dynamics.","The findings of this study are instrumental for organizations navigating the digital era, offering insights into effective knowledge sharing in the decentralized leadership of digital teams.","By mapping out network structures and collaborative patterns, this study, with a focus on Wikipedia as a representative digital platform, paves the way for strategic interventions to optimize digital team dynamics and align them with broader organizational goals."],"url":"http://arxiv.org/abs/2402.12686v1","category":"cs.SI"}
{"created":"2024-02-20 03:20:37","title":"XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques","abstract":"Reinforcement Learning (RL) has demonstrated substantial potential across diverse fields, yet understanding its decision-making process, especially in real-world scenarios where rationality and safety are paramount, is an ongoing challenge. This paper delves in to Explainable RL (XRL), a subfield of Explainable AI (XAI) aimed at unravelling the complexities of RL models. Our focus rests on state-explaining techniques, a crucial subset within XRL methods, as they reveal the underlying factors influencing an agent's actions at any given time. Despite their significant role, the lack of a unified evaluation framework hinders assessment of their accuracy and effectiveness. To address this, we introduce XRL-Bench, a unified standardized benchmark tailored for the evaluation and comparison of XRL methods, encompassing three main modules: standard RL environments, explainers based on state importance, and standard evaluators. XRL-Bench supports both tabular and image data for state explanation. We also propose TabularSHAP, an innovative and competitive XRL method. We demonstrate the practical utility of TabularSHAP in real-world online gaming services and offer an open-source benchmark platform for the straightforward implementation and evaluation of XRL methods. Our contributions facilitate the continued progression of XRL technology.","sentences":["Reinforcement Learning (RL) has demonstrated substantial potential across diverse fields, yet understanding its decision-making process, especially in real-world scenarios where rationality and safety are paramount, is an ongoing challenge.","This paper delves in to Explainable RL (XRL), a subfield of Explainable AI (XAI) aimed at unravelling the complexities of RL models.","Our focus rests on state-explaining techniques, a crucial subset within XRL methods, as they reveal the underlying factors influencing an agent's actions at any given time.","Despite their significant role, the lack of a unified evaluation framework hinders assessment of their accuracy and effectiveness.","To address this, we introduce XRL-Bench, a unified standardized benchmark tailored for the evaluation and comparison of XRL methods, encompassing three main modules: standard RL environments, explainers based on state importance, and standard evaluators.","XRL-Bench supports both tabular and image data for state explanation.","We also propose TabularSHAP, an innovative and competitive XRL method.","We demonstrate the practical utility of TabularSHAP in real-world online gaming services and offer an open-source benchmark platform for the straightforward implementation and evaluation of XRL methods.","Our contributions facilitate the continued progression of XRL technology."],"url":"http://arxiv.org/abs/2402.12685v1","category":"cs.AI"}
{"created":"2024-02-20 02:54:03","title":"Object-level Geometric Structure Preserving for Natural Image Stitching","abstract":"The topic of stitching images with globally natural structures holds paramount significance. Current methodologies exhibit the ability to preserve local geometric structures, yet fall short in maintaining relationships between these geometric structures. In this paper, we endeavor to safeguard the overall, OBJect-level structures within images based on Global Similarity Prior, while concurrently mitigating distortion and ghosting artifacts with OBJ-GSP. Our approach leverages the Segment Anything Model to extract geometric structures with semantic information, enhancing the algorithm's ability to preserve objects in a manner that aligns more intuitively with human perception. We seek to identify spatial constraints that govern the relationships between various geometric boundaries. Recognizing that multiple geometric boundaries collectively define complete objects, we employ triangular meshes to safeguard not only individual geometric structures but also the overall shapes of objects within the images. Empirical evaluations across multiple image stitching datasets demonstrate that our method establishes a new state-of-the-art benchmark in image stitching. Our implementation and dataset is publicly available at https://github.com/RussRobin/OBJ-GSP .","sentences":["The topic of stitching images with globally natural structures holds paramount significance.","Current methodologies exhibit the ability to preserve local geometric structures, yet fall short in maintaining relationships between these geometric structures.","In this paper, we endeavor to safeguard the overall, OBJect-level structures within images based on Global Similarity Prior, while concurrently mitigating distortion and ghosting artifacts with OBJ-GSP.","Our approach leverages the Segment Anything Model to extract geometric structures with semantic information, enhancing the algorithm's ability to preserve objects in a manner that aligns more intuitively with human perception.","We seek to identify spatial constraints that govern the relationships between various geometric boundaries.","Recognizing that multiple geometric boundaries collectively define complete objects, we employ triangular meshes to safeguard not only individual geometric structures but also the overall shapes of objects within the images.","Empirical evaluations across multiple image stitching datasets demonstrate that our method establishes a new state-of-the-art benchmark in image stitching.","Our implementation and dataset is publicly available at https://github.com/RussRobin/OBJ-GSP ."],"url":"http://arxiv.org/abs/2402.12677v1","category":"cs.CV"}
{"created":"2024-02-20 02:48:14","title":"Visual Reasoning in Object-Centric Deep Neural Networks: A Comparative Cognition Approach","abstract":"Achieving visual reasoning is a long-term goal of artificial intelligence. In the last decade, several studies have applied deep neural networks (DNNs) to the task of learning visual relations from images, with modest results in terms of generalization of the relations learned. However, in recent years, object-centric representation learning has been put forward as a way to achieve visual reasoning within the deep learning framework. Object-centric models attempt to model input scenes as compositions of objects and relations between them. To this end, these models use several kinds of attention mechanisms to segregate the individual objects in a scene from the background and from other objects. In this work we tested relation learning and generalization in several object-centric models, as well as a ResNet-50 baseline. In contrast to previous research, which has focused heavily in the same-different task in order to asses relational reasoning in DNNs, we use a set of tasks -- with varying degrees of difficulty -- derived from the comparative cognition literature. Our results show that object-centric models are able to segregate the different objects in a scene, even in many out-of-distribution cases. In our simpler tasks, this improves their capacity to learn and generalize visual relations in comparison to the ResNet-50 baseline. However, object-centric models still struggle in our more difficult tasks and conditions. We conclude that abstract visual reasoning remains an open challenge for DNNs, including object-centric models.","sentences":["Achieving visual reasoning is a long-term goal of artificial intelligence.","In the last decade, several studies have applied deep neural networks (DNNs) to the task of learning visual relations from images, with modest results in terms of generalization of the relations learned.","However, in recent years, object-centric representation learning has been put forward as a way to achieve visual reasoning within the deep learning framework.","Object-centric models attempt to model input scenes as compositions of objects and relations between them.","To this end, these models use several kinds of attention mechanisms to segregate the individual objects in a scene from the background and from other objects.","In this work we tested relation learning and generalization in several object-centric models, as well as a ResNet-50 baseline.","In contrast to previous research, which has focused heavily in the same-different task in order to asses relational reasoning in DNNs, we use a set of tasks -- with varying degrees of difficulty -- derived from the comparative cognition literature.","Our results show that object-centric models are able to segregate the different objects in a scene, even in many out-of-distribution cases.","In our simpler tasks, this improves their capacity to learn and generalize visual relations in comparison to the ResNet-50 baseline.","However, object-centric models still struggle in our more difficult tasks and conditions.","We conclude that abstract visual reasoning remains an open challenge for DNNs, including object-centric models."],"url":"http://arxiv.org/abs/2402.12675v1","category":"cs.CV"}
{"created":"2024-02-20 02:32:27","title":"Pre-trained Transformer-Enabled Strategies with Human-Guided Fine-Tuning for End-to-end Navigation of Autonomous Vehicles","abstract":"Autonomous driving (AD) technology, leveraging artificial intelligence, strives for vehicle automation. End-toend strategies, emerging to simplify traditional driving systems by integrating perception, decision-making, and control, offer new avenues for advanced driving functionalities. Despite their potential, current challenges include data efficiency, training complexities, and poor generalization. This study addresses these issues with a novel end-to-end AD training model, enhancing system adaptability and intelligence. The model incorporates a Transformer module into the policy network, undergoing initial behavior cloning (BC) pre-training for update gradients. Subsequently, fine-tuning through reinforcement learning with human guidance (RLHG) adapts the model to specific driving environments, aiming to surpass the performance limits of imitation learning (IL). The fine-tuning process involves human interactions, guiding the model to acquire more efficient and safer driving behaviors through supervision, intervention, demonstration, and reward feedback. Simulation results demonstrate that this framework accelerates learning, achieving precise control and significantly enhancing safety and reliability. Compared to other advanced baseline methods, the proposed approach excels in challenging AD tasks. The introduction of the Transformer module and human-guided fine-tuning provides valuable insights and methods for research and applications in the AD field.","sentences":["Autonomous driving (AD) technology, leveraging artificial intelligence, strives for vehicle automation.","End-toend strategies, emerging to simplify traditional driving systems by integrating perception, decision-making, and control, offer new avenues for advanced driving functionalities.","Despite their potential, current challenges include data efficiency, training complexities, and poor generalization.","This study addresses these issues with a novel end-to-end AD training model, enhancing system adaptability and intelligence.","The model incorporates a Transformer module into the policy network, undergoing initial behavior cloning (BC) pre-training for update gradients.","Subsequently, fine-tuning through reinforcement learning with human guidance (RLHG) adapts the model to specific driving environments, aiming to surpass the performance limits of imitation learning (IL).","The fine-tuning process involves human interactions, guiding the model to acquire more efficient and safer driving behaviors through supervision, intervention, demonstration, and reward feedback.","Simulation results demonstrate that this framework accelerates learning, achieving precise control and significantly enhancing safety and reliability.","Compared to other advanced baseline methods, the proposed approach excels in challenging AD tasks.","The introduction of the Transformer module and human-guided fine-tuning provides valuable insights and methods for research and applications in the AD field."],"url":"http://arxiv.org/abs/2402.12666v1","category":"cs.RO"}
{"created":"2024-02-20 02:16:16","title":"The FinBen: An Holistic Financial Benchmark for Large Language Models","abstract":"LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks. This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain. FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more. Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations within the financial domain. The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements. Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities. FinBen seeks to continuously evaluate LLMs in finance, fostering AI development with regular updates of tasks and models.","sentences":["LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of thorough evaluations and the complexity of financial tasks.","This along with the rapid development of LLMs, highlights the urgent need for a systematic financial evaluation benchmark for LLMs.","In this paper, we introduce FinBen, the first comprehensive open-sourced evaluation benchmark, specifically designed to thoroughly assess the capabilities of LLMs in the financial domain.","FinBen encompasses 35 datasets across 23 financial tasks, organized into three spectrums of difficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs' cognitive abilities in inductive reasoning, associative memory, quantitative reasoning, crystallized intelligence, and more.","Our evaluation of 15 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals insights into their strengths and limitations within the financial domain.","The findings indicate that GPT-4 leads in quantification, extraction, numerical reasoning, and stock trading, while Gemini shines in generation and forecasting; however, both struggle with complex extraction and forecasting, showing a clear need for targeted enhancements.","Instruction tuning boosts simple task performance but falls short in improving complex reasoning and forecasting abilities.","FinBen seeks to continuously evaluate LLMs in finance, fostering AI development with regular updates of tasks and models."],"url":"http://arxiv.org/abs/2402.12659v1","category":"cs.CL"}
{"created":"2024-02-20 02:09:55","title":"HyperMoE: Towards Better Mixture of Experts via Transferring Among Experts","abstract":"The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing. Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection. To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks. This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning. Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity. Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts.","sentences":["The Mixture of Experts (MoE) for language models has been proven effective in augmenting the capacity of models by dynamically routing each input token to a specific subset of experts for processing.","Despite the success, most existing methods face a challenge for balance between sparsity and the availability of expert knowledge: enhancing performance through increased use of expert knowledge often results in diminishing sparsity during expert selection.","To mitigate this contradiction, we propose HyperMoE, a novel MoE framework built upon Hypernetworks.","This framework integrates the computational processes of MoE with the concept of knowledge transferring in multi-task learning.","Specific modules generated based on the information of unselected experts serve as supplementary information, which allows the knowledge of experts not selected to be used while maintaining selection sparsity.","Our comprehensive empirical evaluations across multiple datasets and backbones establish that HyperMoE significantly outperforms existing MoE methods under identical conditions concerning the number of experts."],"url":"http://arxiv.org/abs/2402.12656v1","category":"cs.LG"}
{"created":"2024-02-20 01:47:25","title":"Training Artificial Neural Networks by Coordinate Search Algorithm","abstract":"Training Artificial Neural Networks poses a challenging and critical problem in machine learning. Despite the effectiveness of gradient-based learning methods, such as Stochastic Gradient Descent (SGD), in training neural networks, they do have several limitations. For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized. Furthermore, the training in any DNN can be possible with a small size of the training dataset. To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks. The proposed algorithm can be used with non-differentiable activation functions and tailored to multi-objective/multi-loss problems. Finding the optimal values for weights of ANNs is a large-scale optimization problem. Therefore instead of finding the optimal value for each variable, which is the common technique in classical CS, we accelerate optimization and convergence by bundling the weights. In fact, this strategy is a form of dimension reduction for optimization problems. Based on the experimental results, the proposed method, in some cases, outperforms the gradient-based approach, particularly, in situations with insufficient labeled training data. The performance plots demonstrate a high convergence rate, highlighting the capability of our suggested method to find a reasonable solution with fewer function calls. As of now, the only practical and efficient way of training ANNs with hundreds of thousands of weights is gradient-based algorithms such as SGD or Adam. In this paper we introduce an alternative method for training ANN.","sentences":["Training Artificial Neural Networks poses a challenging and critical problem in machine learning.","Despite the effectiveness of gradient-based learning methods, such as Stochastic Gradient Descent (SGD), in training neural networks, they do have several limitations.","For instance, they require differentiable activation functions, and cannot optimize a model based on several independent non-differentiable loss functions simultaneously; for example, the F1-score, which is used during testing, can be used during training when a gradient-free optimization algorithm is utilized.","Furthermore, the training in any DNN can be possible with a small size of the training dataset.","To address these concerns, we propose an efficient version of the gradient-free Coordinate Search (CS) algorithm, an instance of General Pattern Search methods, for training neural networks.","The proposed algorithm can be used with non-differentiable activation functions and tailored to multi-objective/multi-loss problems.","Finding the optimal values for weights of ANNs is a large-scale optimization problem.","Therefore instead of finding the optimal value for each variable, which is the common technique in classical CS, we accelerate optimization and convergence by bundling the weights.","In fact, this strategy is a form of dimension reduction for optimization problems.","Based on the experimental results, the proposed method, in some cases, outperforms the gradient-based approach, particularly, in situations with insufficient labeled training data.","The performance plots demonstrate a high convergence rate, highlighting the capability of our suggested method to find a reasonable solution with fewer function calls.","As of now, the only practical and efficient way of training ANNs with hundreds of thousands of weights is gradient-based algorithms such as SGD or Adam.","In this paper we introduce an alternative method for training ANN."],"url":"http://arxiv.org/abs/2402.12646v1","category":"cs.LG"}
{"created":"2024-02-20 01:43:51","title":"Neuromorphic Synergy for Video Binarization","abstract":"Bimodal objects, such as the checkerboard pattern used in camera calibration, markers for object tracking, and text on road signs, to name a few, are prevalent in our daily lives and serve as a visual form to embed information that can be easily recognized by vision systems. While binarization from intensity images is crucial for extracting the embedded information in the bimodal objects, few previous works consider the task of binarization of blurry images due to the relative motion between the vision sensor and the environment. The blurry images can result in a loss in the binarization quality and thus degrade the downstream applications where the vision system is in motion. Recently, neuromorphic cameras offer new capabilities for alleviating motion blur, but it is non-trivial to first deblur and then binarize the images in a real-time manner. In this work, we propose an event-based binary reconstruction method that leverages the prior knowledge of the bimodal target's properties to perform inference independently in both event space and image space and merge the results from both domains to generate a sharp binary image. We also develop an efficient integration method to propagate this binary image to high frame rate binary video. Finally, we develop a novel method to naturally fuse events and images for unsupervised threshold identification. The proposed method is evaluated in publicly available and our collected data sequence, and shows the proposed method can outperform the SOTA methods to generate high frame rate binary video in real-time on CPU-only devices.","sentences":["Bimodal objects, such as the checkerboard pattern used in camera calibration, markers for object tracking, and text on road signs, to name a few, are prevalent in our daily lives and serve as a visual form to embed information that can be easily recognized by vision systems.","While binarization from intensity images is crucial for extracting the embedded information in the bimodal objects, few previous works consider the task of binarization of blurry images due to the relative motion between the vision sensor and the environment.","The blurry images can result in a loss in the binarization quality and thus degrade the downstream applications where the vision system is in motion.","Recently, neuromorphic cameras offer new capabilities for alleviating motion blur, but it is non-trivial to first deblur and then binarize the images in a real-time manner.","In this work, we propose an event-based binary reconstruction method that leverages the prior knowledge of the bimodal target's properties to perform inference independently in both event space and image space and merge the results from both domains to generate a sharp binary image.","We also develop an efficient integration method to propagate this binary image to high frame rate binary video.","Finally, we develop a novel method to naturally fuse events and images for unsupervised threshold identification.","The proposed method is evaluated in publicly available and our collected data sequence, and shows the proposed method can outperform the SOTA methods to generate high frame rate binary video in real-time on CPU-only devices."],"url":"http://arxiv.org/abs/2402.12644v1","category":"cs.CV"}
{"created":"2024-02-20 01:20:31","title":"Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale","abstract":"In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India. Previous methods, which often relied solely on text, fall short in capturing the multimedia essence of these debates. To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis. Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of prime-time television debates in India. These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization. Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames. Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility. This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate. We will make our data analysis pipeline and collected data publicly available to catalyze further research in this domain.","sentences":["In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India.","Previous methods, which often relied solely on text, fall short in capturing the multimedia essence of these debates.","To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis.","Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of prime-time television debates in India.","These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization.","Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames.","Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility.","This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate.","We will make our data analysis pipeline and collected data publicly available to catalyze further research in this domain."],"url":"http://arxiv.org/abs/2402.12629v1","category":"cs.MM"}
{"created":"2024-02-20 01:16:01","title":"A Comprehensive Review of Machine Learning Advances on Data Change: A Cross-Field Perspective","abstract":"Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries. However, in the real world, dynamic data lead to principal challenges for deploying AI models. An unexpected data change brings about severe performance degradation in AI models. We identify two major related research fields, domain shift and concept drift according to the setting of the data change. Although these two popular research fields aim to solve distribution shift and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches. In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields. We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields. We thus provide a novel scope for researchers to explore contemporary technical strategies, learn industrial applications, and identify future directions for addressing data change challenges.","sentences":["Recent artificial intelligence (AI) technologies show remarkable evolution in various academic fields and industries.","However, in the real world, dynamic data lead to principal challenges for deploying AI models.","An unexpected data change brings about severe performance degradation in AI models.","We identify two major related research fields, domain shift and concept drift according to the setting of the data change.","Although these two popular research fields aim to solve distribution shift and non-stationary data stream problems, the underlying properties remain similar which also encourages similar technical approaches.","In this review, we regroup domain shift and concept drift into a single research problem, namely the data change problem, with a systematic overview of state-of-the-art methods in the two research fields.","We propose a three-phase problem categorization scheme to link the key ideas in the two technical fields.","We thus provide a novel scope for researchers to explore contemporary technical strategies, learn industrial applications, and identify future directions for addressing data change challenges."],"url":"http://arxiv.org/abs/2402.12627v1","category":"cs.LG"}
{"created":"2024-02-20 01:07:32","title":"Efficient Parameter Mining and Freezing for Continual Object Detection","abstract":"Continual Object Detection is essential for enabling intelligent agents to interact proactively with humans in real-world settings. While parameter-isolation strategies have been extensively explored in the context of continual learning for classification, they have yet to be fully harnessed for incremental object detection scenarios. Drawing inspiration from prior research that focused on mining individual neuron responses and integrating insights from recent developments in neural pruning, we proposed efficient ways to identify which layers are the most important for a network to maintain the performance of a detector across sequential updates. The presented findings highlight the substantial advantages of layer-level parameter isolation in facilitating incremental learning within object detection models, offering promising avenues for future research and application in real-world scenarios.","sentences":["Continual Object Detection is essential for enabling intelligent agents to interact proactively with humans in real-world settings.","While parameter-isolation strategies have been extensively explored in the context of continual learning for classification, they have yet to be fully harnessed for incremental object detection scenarios.","Drawing inspiration from prior research that focused on mining individual neuron responses and integrating insights from recent developments in neural pruning, we proposed efficient ways to identify which layers are the most important for a network to maintain the performance of a detector across sequential updates.","The presented findings highlight the substantial advantages of layer-level parameter isolation in facilitating incremental learning within object detection models, offering promising avenues for future research and application in real-world scenarios."],"url":"http://arxiv.org/abs/2402.12624v1","category":"cs.CV"}
{"created":"2024-02-20 01:06:07","title":"Studies on $R_K$ with Large Dilepton Invariant-Mass, Scalable Pythonic Fitting, and Online Event Interpretation with GNNs at LHCb","abstract":"The Standard Model of particle physics is well established, yet recently showed tensions with experimental observations. A large part of this thesis is dedicated to the first measurement of the ratio of branching fractions of the decays $B^+ \\rightarrow K^+ \\mu^+ \\mu^-$ and $B^+ \\rightarrow K^+ e^+ e^-$ , referred to as $R_K$ , in the high dilepton invariant-mass region. The presented analysis uses the full dataset of proton-proton collisions collected by the LHCb experiment in the years 2011-2018, corresponding to an integrated luminosity of 9 $fb^{-1}$. The final result for $R_K$ is still blinded. The sensitivity of the developed analysis is estimated to be $\\sigma_{R_K}^{\\mathrm{stat}} = 0.073$ and $\\sigma_{R_K}^{\\mathrm{stat}} = 0.031$. In addition to the precision measurement of $R_K$ at a high dilepton invariant mass, this thesis contains two more technical topics. First, an algorithm that selects particles in an event in the LHCb detector by performing a full event interpretation, referred to as DFEI. This tool is based on multiple Graph Neural Networks and aims to cope with the increase in luminosity in current and future upgrades of the LHCb detector. Comparisons with the current approach show at least similar, sometimes better, performance with respect to decay reconstruction and selection using charged particles. The efficiency is mostly independent of the luminosity, which is crucial for future upgrades. Second, a Python package for likelihood model fitting called zfit. The increasing popularity of the Python programming language in High Energy Physics creates a need for a flexible, modular, and performant fitting library. The zfit package is well integrated into the Python ecosystem, highly customizable and fast thanks to its computational backend TensorFlow.","sentences":["The Standard Model of particle physics is well established, yet recently showed tensions with experimental observations.","A large part of this thesis is dedicated to the first measurement of the ratio of branching fractions of the decays $B^+ \\rightarrow K^+ \\mu^+ \\mu^-$ and $B^+ \\rightarrow K^+ e^+ e^-$ , referred to as $R_K$ , in the high dilepton invariant-mass region.","The presented analysis uses the full dataset of proton-proton collisions collected by the LHCb experiment in the years 2011-2018, corresponding to an integrated luminosity of 9 $fb^{-1}$. The final result for $R_K$ is still blinded.","The sensitivity of the developed analysis is estimated to be $\\sigma_{R_K}^{\\mathrm{stat}} = 0.073$ and $\\sigma_{R_K}^{\\mathrm{stat}} = 0.031$.","In addition to the precision measurement of $R_K$ at a high dilepton invariant mass, this thesis contains two more technical topics.","First, an algorithm that selects particles in an event in the LHCb detector by performing a full event interpretation, referred to as DFEI.","This tool is based on multiple Graph Neural Networks and aims to cope with the increase in luminosity in current and future upgrades of the LHCb detector.","Comparisons with the current approach show at least similar, sometimes better, performance with respect to decay reconstruction and selection using charged particles.","The efficiency is mostly independent of the luminosity, which is crucial for future upgrades.","Second, a Python package for likelihood model fitting called zfit.","The increasing popularity of the Python programming language in High Energy Physics creates a need for a flexible, modular, and performant fitting library.","The zfit package is well integrated into the Python ecosystem, highly customizable and fast thanks to its computational backend TensorFlow."],"url":"http://arxiv.org/abs/2402.12622v1","category":"hep-ex"}
{"created":"2024-02-20 00:51:05","title":"Generative AI Security: Challenges and Countermeasures","abstract":"Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny. This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks.","sentences":["Generative AI's expanding footprint across numerous industries has led to both excitement and increased scrutiny.","This paper delves into the unique security challenges posed by Generative AI, and outlines potential research directions for managing these risks."],"url":"http://arxiv.org/abs/2402.12617v1","category":"cs.CR"}
{"created":"2024-02-20 00:34:58","title":"Analysis of Using Sigmoid Loss for Contrastive Learning","abstract":"Contrastive learning has emerged as a prominent branch of self-supervised learning for several years. Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention. Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss. SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view. However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored. In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings. First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable. Interestingly, the proposed CCEM is proven to contain the optimal embedding with respect to the sigmoid loss. Second, we mathematically analyze the optimal embedding minimizing the sigmoid loss for contrastive learning. The optimal embedding ranges from simplex equiangular-tight-frame to antipodal structure, depending on the temperature parameter used in the sigmoid loss. Third, our experimental results on synthetic datasets coincide with the theoretical results on the optimal embedding structures.","sentences":["Contrastive learning has emerged as a prominent branch of self-supervised learning for several years.","Especially, CLIP, which applies contrastive learning to large sets of captioned images, has garnered significant attention.","Recently, SigLIP, a variant of CLIP, has been proposed, which uses the sigmoid loss instead of the standard InfoNCE loss.","SigLIP achieves the performance comparable to CLIP in a more efficient manner by eliminating the need for a global view.","However, theoretical understanding of using the sigmoid loss in contrastive learning is underexplored.","In this paper, we provide a theoretical analysis of using the sigmoid loss in contrastive learning, in the perspective of the geometric structure of learned embeddings.","First, we propose the double-Constant Embedding Model (CCEM), a framework for parameterizing various well-known embedding structures by a single variable.","Interestingly, the proposed CCEM is proven to contain the optimal embedding with respect to the sigmoid loss.","Second, we mathematically analyze the optimal embedding minimizing the sigmoid loss for contrastive learning.","The optimal embedding ranges from simplex equiangular-tight-frame to antipodal structure, depending on the temperature parameter used in the sigmoid loss.","Third, our experimental results on synthetic datasets coincide with the theoretical results on the optimal embedding structures."],"url":"http://arxiv.org/abs/2402.12613v1","category":"cs.LG"}
{"created":"2024-02-20 00:07:55","title":"Patient-Centric Knowledge Graphs: A Survey of Current Methods, Challenges, and Applications","abstract":"Patient-Centric Knowledge Graphs (PCKGs) represent an important shift in healthcare that focuses on individualized patient care by mapping the patient's health information in a holistic and multi-dimensional way. PCKGs integrate various types of health data to provide healthcare professionals with a comprehensive understanding of a patient's health, enabling more personalized and effective care. This literature review explores the methodologies, challenges, and opportunities associated with PCKGs, focusing on their role in integrating disparate healthcare data and enhancing patient care through a unified health perspective. In addition, this review also discusses the complexities of PCKG development, including ontology design, data integration techniques, knowledge extraction, and structured representation of knowledge. It highlights advanced techniques such as reasoning, semantic search, and inference mechanisms essential in constructing and evaluating PCKGs for actionable healthcare insights. We further explore the practical applications of PCKGs in personalized medicine, emphasizing their significance in improving disease prediction and formulating effective treatment plans. Overall, this review provides a foundational perspective on the current state-of-the-art and best practices of PCKGs, guiding future research and applications in this dynamic field.","sentences":["Patient-Centric Knowledge Graphs (PCKGs) represent an important shift in healthcare that focuses on individualized patient care by mapping the patient's health information in a holistic and multi-dimensional way.","PCKGs integrate various types of health data to provide healthcare professionals with a comprehensive understanding of a patient's health, enabling more personalized and effective care.","This literature review explores the methodologies, challenges, and opportunities associated with PCKGs, focusing on their role in integrating disparate healthcare data and enhancing patient care through a unified health perspective.","In addition, this review also discusses the complexities of PCKG development, including ontology design, data integration techniques, knowledge extraction, and structured representation of knowledge.","It highlights advanced techniques such as reasoning, semantic search, and inference mechanisms essential in constructing and evaluating PCKGs for actionable healthcare insights.","We further explore the practical applications of PCKGs in personalized medicine, emphasizing their significance in improving disease prediction and formulating effective treatment plans.","Overall, this review provides a foundational perspective on the current state-of-the-art and best practices of PCKGs, guiding future research and applications in this dynamic field."],"url":"http://arxiv.org/abs/2402.12608v1","category":"cs.AI"}
{"created":"2024-02-19 23:51:35","title":"Physically Consistent Modeling of Stacked Intelligent Metasurfaces Implemented with Beyond Diagonal RIS","abstract":"Stacked intelligent metasurface (SIM) has emerged as a technology enabling wave domain beamforming through multiple stacked reconfigurable intelligent surfaces (RISs). SIM has been implemented so far with diagonal RIS (D-RIS), while SIM implemented with beyond diagonal RIS (BD-RIS) remains unexplored. Furthermore, a model of SIM accounting for mutual coupling is not yet available. To fill these gaps, we derive a physically consistent channel model for SIM-aided systems and clarify the assumptions needed to obtain the simplified model used in related works. Using this model, we show that 1-layer SIM implemented with BD-RIS achieves the performance upper bound with limited complexity.","sentences":["Stacked intelligent metasurface (SIM) has emerged as a technology enabling wave domain beamforming through multiple stacked reconfigurable intelligent surfaces (RISs).","SIM has been implemented so far with diagonal RIS (D-RIS), while SIM implemented with beyond diagonal RIS (BD-RIS) remains unexplored.","Furthermore, a model of SIM accounting for mutual coupling is not yet available.","To fill these gaps, we derive a physically consistent channel model for SIM-aided systems and clarify the assumptions needed to obtain the simplified model used in related works.","Using this model, we show that 1-layer SIM implemented with BD-RIS achieves the performance upper bound with limited complexity."],"url":"http://arxiv.org/abs/2402.12602v1","category":"cs.IT"}
{"created":"2024-02-19 23:22:30","title":"Graph-based Virtual Sensing from Sparse and Partial Multivariate Observations","abstract":"Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations. However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation. In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest. From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate. We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework. The proposed approach relies on propagating information over a nested graph structure that is used to learn dependencies between variables as well as locations. GgNet is extensively evaluated under different virtual sensing scenarios, demonstrating higher reconstruction accuracy compared to the state-of-the-art.","sentences":["Virtual sensing techniques allow for inferring signals at new unmonitored locations by exploiting spatio-temporal measurements coming from physical sensors at different locations.","However, as the sensor coverage becomes sparse due to costs or other constraints, physical proximity cannot be used to support interpolation.","In this paper, we overcome this challenge by leveraging dependencies between the target variable and a set of correlated variables (covariates) that can frequently be associated with each location of interest.","From this viewpoint, covariates provide partial observability, and the problem consists of inferring values for unobserved channels by exploiting observations at other locations to learn how such variables can correlate.","We introduce a novel graph-based methodology to exploit such relationships and design a graph deep learning architecture, named GgNet, implementing the framework.","The proposed approach relies on propagating information over a nested graph structure that is used to learn dependencies between variables as well as locations.","GgNet is extensively evaluated under different virtual sensing scenarios, demonstrating higher reconstruction accuracy compared to the state-of-the-art."],"url":"http://arxiv.org/abs/2402.12598v1","category":"cs.LG"}
{"created":"2024-02-19 22:59:43","title":"Evolving AI Collectives to Enhance Human Diversity and Enable Self-Regulation","abstract":"Large language models steer their behaviors based on texts generated by others. This capacity and their increasing prevalence in online settings portend that they will intentionally or unintentionally \"program\" one another and form emergent AI subjectivities, relationships, and collectives. Here, we call upon the research community to investigate these \"society-like\" properties of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments. We use a simple model and its outputs to illustrate how such emergent, decentralized AI collectives can expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online. Finally, we discuss opportunities for AI self-moderation and address ethical issues and design challenges associated with creating and maintaining decentralized AI collectives.","sentences":["Large language models steer their behaviors based on texts generated by others.","This capacity and their increasing prevalence in online settings portend that they will intentionally or unintentionally \"program\" one another and form emergent AI subjectivities, relationships, and collectives.","Here, we call upon the research community to investigate these \"society-like\" properties of interacting artificial intelligences to increase their rewards and reduce their risks for human society and the health of online environments.","We use a simple model and its outputs to illustrate how such emergent, decentralized AI collectives can expand the bounds of human diversity and reduce the risk of toxic, anti-social behavior online.","Finally, we discuss opportunities for AI self-moderation and address ethical issues and design challenges associated with creating and maintaining decentralized AI collectives."],"url":"http://arxiv.org/abs/2402.12590v1","category":"cs.CL"}
{"created":"2024-02-19 21:53:43","title":"FairProof : Confidential and Certifiable Fairness for Neural Networks","abstract":"Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential. Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions. To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality. We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system. We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible.","sentences":["Machine learning models are increasingly used in societal applications, yet legal and privacy concerns demand that they very often be kept confidential.","Consequently, there is a growing distrust about the fairness properties of these models in the minds of consumers, who are often at the receiving end of model predictions.","To this end, we propose FairProof - a system that uses Zero-Knowledge Proofs (a cryptographic primitive) to publicly verify the fairness of a model, while maintaining confidentiality.","We also propose a fairness certification algorithm for fully-connected neural networks which is befitting to ZKPs and is used in this system.","We implement FairProof in Gnark and demonstrate empirically that our system is practically feasible."],"url":"http://arxiv.org/abs/2402.12572v1","category":"cs.LG"}
{"created":"2024-02-19 21:52:44","title":"Offline Multi-task Transfer RL with Representational Penalization","abstract":"We study the problem of representation transfer in offline Reinforcement Learning (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task. Unlike in online RL where the agent interacts with the environment while learning a policy, in the offline setting there cannot be such interactions in either the source tasks or the target task; thus multi-task offline RL can suffer from incomplete coverage.   We propose an algorithm to compute pointwise uncertainty measures for the learnt representation, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task. Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meaningful transfer by existing offline algorithms. We complement our theoretical results with empirical evaluation on a rich-observation MDP which requires many samples for complete coverage. Our findings illustrate the benefits of penalizing and quantifying the uncertainty in the learnt representation.","sentences":["We study the problem of representation transfer in offline Reinforcement Learning (RL), where a learner has access to episodic data from a number of source tasks collected a priori, and aims to learn a shared representation to be used in finding a good policy for a target task.","Unlike in online RL where the agent interacts with the environment while learning a policy, in the offline setting there cannot be such interactions in either the source tasks or the target task; thus multi-task offline RL can suffer from incomplete coverage.   ","We propose an algorithm to compute pointwise uncertainty measures for the learnt representation, and establish a data-dependent upper bound for the suboptimality of the learnt policy for the target task.","Our algorithm leverages the collective exploration done by source tasks to mitigate poor coverage at some points by a few tasks, thus overcoming the limitation of needing uniformly good coverage for a meaningful transfer by existing offline algorithms.","We complement our theoretical results with empirical evaluation on a rich-observation MDP which requires many samples for complete coverage.","Our findings illustrate the benefits of penalizing and quantifying the uncertainty in the learnt representation."],"url":"http://arxiv.org/abs/2402.12570v1","category":"cs.LG"}
{"created":"2024-02-19 21:45:54","title":"A Simple Detection and Identification Scheme For Reconfigurable Intelligent Surfaces","abstract":"Reconfigurable intelligent surface (RIS)-empowered communication is one of the promising physical layer enabling technologies for the sixth generation (6G) wireless networks due to their unprecedented capabilities in shaping the wireless communication environment. RISs are modeled as passive objects that can not transmit or receive wireless signals. While the passiveness of these surfaces is a key advantage in terms of power consumption and implementation complexity, it limits their capability to interact with the other active components in the network. Specifically, unlike conventional base stations (BSs), which actively identify themselves to user equipment (UEs) by periodically sending pilot signals, RISs need to be detected from the UE side. This paper proposes a novel RIS identification (RIS- ID) scheme, enabling UEs to detect and uniquely identify RISs in their surrounding environment. Furthermore, to assess the proposed RIS-ID scheme, we propose two performance metrics: the false and miss detection probabilities. These probabilities are analytically derived and verified through computer simulations, revealing the effectiveness of the proposed RIS-ID scheme under different operating scenarios.","sentences":["Reconfigurable intelligent surface (RIS)-empowered communication is one of the promising physical layer enabling technologies for the sixth generation (6G) wireless networks due to their unprecedented capabilities in shaping the wireless communication environment.","RISs are modeled as passive objects that can not transmit or receive wireless signals.","While the passiveness of these surfaces is a key advantage in terms of power consumption and implementation complexity, it limits their capability to interact with the other active components in the network.","Specifically, unlike conventional base stations (BSs), which actively identify themselves to user equipment (UEs) by periodically sending pilot signals, RISs need to be detected from the UE side.","This paper proposes a novel RIS identification (RIS- ID) scheme, enabling UEs to detect and uniquely identify RISs in their surrounding environment.","Furthermore, to assess the proposed RIS-ID scheme, we propose two performance metrics: the false and miss detection probabilities.","These probabilities are analytically derived and verified through computer simulations, revealing the effectiveness of the proposed RIS-ID scheme under different operating scenarios."],"url":"http://arxiv.org/abs/2402.12565v1","category":"eess.SP"}
{"created":"2024-02-19 21:38:02","title":"Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models","abstract":"The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.","sentences":["The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities.","This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility.","Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process.","Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction.","We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses.","It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections.","We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers.","Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''.","The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}."],"url":"http://arxiv.org/abs/2402.12563v1","category":"cs.CL"}
{"created":"2024-02-19 21:35:56","title":"CausalGym: Benchmarking causal interpretability methods on linguistic tasks","abstract":"Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.","sentences":["Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons).","At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior.","To help bring these strands of research closer together, we introduce CausalGym.","We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour.","To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS).","We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap dependencies.","Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually."],"url":"http://arxiv.org/abs/2402.12560v1","category":"cs.CL"}
{"created":"2024-02-19 21:20:56","title":"Landmark-based Localization using Stereo Vision and Deep Learning in GPS-Denied Battlefield Environment","abstract":"Localization in a battlefield environment is increasingly challenging as GPS connectivity is often denied or unreliable, and physical deployment of anchor nodes across wireless networks for localization can be difficult in hostile battlefield terrain. Existing range-free localization methods rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in dynamic and sparse wireless network topology. Vision-based methods like SLAM and Visual Odometry use expensive sensor fusion techniques for map generation and pose estimation. This paper proposes a novel framework for localization in non-GPS battlefield environments using only the passive camera sensors and considering naturally existing or artificial landmarks as anchors. The proposed method utilizes a customcalibrated stereo vision camera for distance estimation and the YOLOv8s model, which is trained and fine-tuned with our real-world dataset for landmark recognition. The depth images are generated using an efficient stereomatching algorithm, and distances to landmarks are determined by extracting the landmark depth feature utilizing a bounding box predicted by the landmark recognition model. The position of the unknown node is then obtained using the efficient least square algorithm and then optimized using the L-BFGS-B (limited-memory quasi-Newton code for bound-constrained optimization) method. Experimental results demonstrate that our proposed framework performs better than existing anchorbased DV-Hop algorithms and competes with the most efficient vision-based algorithms in terms of localization error (RMSE).","sentences":["Localization in a battlefield environment is increasingly challenging as GPS connectivity is often denied or unreliable, and physical deployment of anchor nodes across wireless networks for localization can be difficult in hostile battlefield terrain.","Existing range-free localization methods rely on radio-based anchors and their average hop distance which suffers from accuracy and stability in dynamic and sparse wireless network topology.","Vision-based methods like SLAM and Visual Odometry use expensive sensor fusion techniques for map generation and pose estimation.","This paper proposes a novel framework for localization in non-GPS battlefield environments using only the passive camera sensors and considering naturally existing or artificial landmarks as anchors.","The proposed method utilizes a customcalibrated stereo vision camera for distance estimation and the YOLOv8s model, which is trained and fine-tuned with our real-world dataset for landmark recognition.","The depth images are generated using an efficient stereomatching algorithm, and distances to landmarks are determined by extracting the landmark depth feature utilizing a bounding box predicted by the landmark recognition model.","The position of the unknown node is then obtained using the efficient least square algorithm and then optimized using the L-BFGS-B (limited-memory quasi-Newton code for bound-constrained optimization) method.","Experimental results demonstrate that our proposed framework performs better than existing anchorbased DV-Hop algorithms and competes with the most efficient vision-based algorithms in terms of localization error (RMSE)."],"url":"http://arxiv.org/abs/2402.12551v1","category":"cs.CV"}
{"created":"2024-02-19 21:01:11","title":"Impact of data usage for forecasting on performance of model predictive control in buildings with smart energy storage","abstract":"Data is required to develop forecasting models for use in Model Predictive Control (MPC) schemes in building energy systems. However, data usage incurs costs from both its collection and exploitation. Determining cost optimal data usage requires understanding of the forecast accuracy and resulting MPC operational performance it enables. This study investigates the performance of both simple and state-of-the-art machine learning prediction models for MPC in a multi-building energy system simulation using historic building energy data. The impact of data usage on forecast accuracy is quantified for the following data efficiency measures: reuse of prediction models, reduction of training data volumes, reduction of model data features, and online model training. A simple linear multi-layer perceptron model is shown to provide equivalent forecast accuracy to state-of-the-art models, with greater data efficiency and generalisability. The use of more than 2 years of training data for load prediction models provided no significant improvement in forecast accuracy. Forecast accuracy and data efficiency were improved simultaneously by using change-point analysis to screen training data. Reused models and those trained with 3 months of data had on average 10% higher error than baseline, indicating that deploying MPC systems without prior data collection may be economic.","sentences":["Data is required to develop forecasting models for use in Model Predictive Control (MPC) schemes in building energy systems.","However, data usage incurs costs from both its collection and exploitation.","Determining cost optimal data usage requires understanding of the forecast accuracy and resulting MPC operational performance it enables.","This study investigates the performance of both simple and state-of-the-art machine learning prediction models for MPC in a multi-building energy system simulation using historic building energy data.","The impact of data usage on forecast accuracy is quantified for the following data efficiency measures: reuse of prediction models, reduction of training data volumes, reduction of model data features, and online model training.","A simple linear multi-layer perceptron model is shown to provide equivalent forecast accuracy to state-of-the-art models, with greater data efficiency and generalisability.","The use of more than 2 years of training data for load prediction models provided no significant improvement in forecast accuracy.","Forecast accuracy and data efficiency were improved simultaneously by using change-point analysis to screen training data.","Reused models and those trained with 3 months of data had on average 10% higher error than baseline, indicating that deploying MPC systems without prior data collection may be economic."],"url":"http://arxiv.org/abs/2402.12539v1","category":"eess.SY"}
{"created":"2024-02-19 20:55:12","title":"A Machine Learning Ensemble Model for the Detection of Cyberbullying","abstract":"The pervasive use of social media platforms, such as Facebook, Instagram, and X, has significantly amplified our electronic interconnectedness. Moreover, these platforms are now easily accessible from any location at any given time. However, the increased popularity of social media has also led to cyberbullying.It is imperative to address the need for finding, monitoring, and mitigating cyberbullying posts on social media platforms. Motivated by this necessity, we present this paper to contribute to developing an automated system for detecting binary labels of aggressive tweets.Our study has demonstrated remarkable performance compared to previous experiments on the same dataset. We employed the stacking ensemble machine learning method, utilizing four various feature extraction techniques to optimize performance within the stacking ensemble learning framework. Combining five machine learning algorithms,Decision Trees, Random Forest, Linear Support Vector Classification, Logistic Regression, and K-Nearest Neighbors into an ensemble method, we achieved superior results compared to traditional machine learning classifier models. The stacking classifier achieved a high accuracy rate of 94.00%, outperforming traditional machine learning models and surpassing the results of prior experiments that utilized the same dataset. The outcomes of our experiments showcased an accuracy rate of 0.94% in detection tweets as aggressive or non-aggressive.","sentences":["The pervasive use of social media platforms, such as Facebook, Instagram, and X, has significantly amplified our electronic interconnectedness.","Moreover, these platforms are now easily accessible from any location at any given time.","However, the increased popularity of social media has also led to cyberbullying.","It is imperative to address the need for finding, monitoring, and mitigating cyberbullying posts on social media platforms.","Motivated by this necessity, we present this paper to contribute to developing an automated system for detecting binary labels of aggressive tweets.","Our study has demonstrated remarkable performance compared to previous experiments on the same dataset.","We employed the stacking ensemble machine learning method, utilizing four various feature extraction techniques to optimize performance within the stacking ensemble learning framework.","Combining five machine learning algorithms,Decision Trees, Random Forest, Linear Support Vector Classification, Logistic Regression, and K-Nearest Neighbors into an ensemble method, we achieved superior results compared to traditional machine learning classifier models.","The stacking classifier achieved a high accuracy rate of 94.00%, outperforming traditional machine learning models and surpassing the results of prior experiments that utilized the same dataset.","The outcomes of our experiments showcased an accuracy rate of 0.94% in detection tweets as aggressive or non-aggressive."],"url":"http://arxiv.org/abs/2402.12538v1","category":"cs.SI"}
{"created":"2024-02-19 20:40:48","title":"Parallel Structures in Pre-training Data Yield In-Context Learning","abstract":"Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs' ICL ability depends on $\\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data.","sentences":["Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update.","However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts.","In this work, we study what patterns of the pre-training data contribute to ICL.","We find that LMs' ICL ability depends on $\\textit{parallel structures}$ in the pre-training data -- pairs of phrases following similar templates in the same context window.","Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL.","We show that removing parallel structures in the pre-training data reduces LMs' ICL accuracy by 51% (vs 2% from random ablation).","This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures.","A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data."],"url":"http://arxiv.org/abs/2402.12530v1","category":"cs.CL"}
{"created":"2024-02-19 20:38:00","title":"The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning","abstract":"Offline reinforcement learning aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset. Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model. The prevailing theoretical understanding is that this can then be viewed as online reinforcement learning in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model. Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a major misconception. Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overestimation and collapse in Bellman-based algorithms. We term this the edge-of-reach problem. Based on this, we fill some gaps in existing theory and also explain how prior model-based methods are inadvertently addressing the true underlying edge-of-reach problem. Finally, we propose Reach-Aware Value Learning (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and achieves strong performance across both proprioceptive and pixel-based benchmarks. Code open-sourced at: https://github.com/anyasims/edge-of-reach.","sentences":["Offline reinforcement learning aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset.","Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model.","The prevailing theoretical understanding is that this can then be viewed as online reinforcement learning in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model.","Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail.","This reveals a major misconception.","Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overestimation and collapse in Bellman-based algorithms.","We term this the edge-of-reach problem.","Based on this, we fill some gaps in existing theory and also explain how prior model-based methods are inadvertently addressing the true underlying edge-of-reach problem.","Finally, we propose Reach-Aware Value Learning (RAVL), a simple and robust method that directly addresses the edge-of-reach problem and achieves strong performance across both proprioceptive and pixel-based benchmarks.","Code open-sourced at: https://github.com/anyasims/edge-of-reach."],"url":"http://arxiv.org/abs/2402.12527v1","category":"cs.LG"}
{"created":"2024-02-19 20:36:37","title":"Optimize Energy Consumption of Wireless Sensor Networks by using modified Ant Colony Optimization ACO","abstract":"Routing represents a pivotal concern in the context of Wireless Sensor Networks (WSN) owing to its divergence from traditional network routing paradigms. The inherent dynamism of the WSN environment, coupled with the scarcity of available resources, engenders considerable challenges for industry and academia alike in devising efficient routing strategies. Addressing these challenges, a viable recourse lies in applying heuristic search methodologies to ascertain the most optimal path in WSNs. Ant Colony Optimization (ACO) is a well-established heuristic algorithm that has demonstrated notable advancements in routing contexts. This paper introduces a modify routing protocols based on Ant colony optimization. In these protocols, we incorporate the inverse of the distance between nodes and their neighbours in the probability equations of ACO along with considering pheromone levels and residual energy. These formulation modifications facilitate the selection of the most suitable candidate for the subsequent hop, effectively minimizing the average energy consumption across all nodes in each iteration. Furthermore, in this protocol, we iteratively fine-tune ACO's parameter values based on the outcomes of several experimental trials. The experimental analysis is conducted through a diverse set of network topologies, and the results are subjected to comparison against well-established ACO algorithm and routing protocols. The efficacy of the proposed protocol is assessed based on various performance metrics, encompassing throughput, energy consumption, network lifetime, energy consumption, the extent of data transferred over the network, and the length of paths traversed by packets. These metrics collectively provide a comprehensive evaluation of the performance attainments of the routing protocols.","sentences":["Routing represents a pivotal concern in the context of Wireless Sensor Networks (WSN) owing to its divergence from traditional network routing paradigms.","The inherent dynamism of the WSN environment, coupled with the scarcity of available resources, engenders considerable challenges for industry and academia alike in devising efficient routing strategies.","Addressing these challenges, a viable recourse lies in applying heuristic search methodologies to ascertain the most optimal path in WSNs.","Ant Colony Optimization (ACO) is a well-established heuristic algorithm that has demonstrated notable advancements in routing contexts.","This paper introduces a modify routing protocols based on Ant colony optimization.","In these protocols, we incorporate the inverse of the distance between nodes and their neighbours in the probability equations of ACO along with considering pheromone levels and residual energy.","These formulation modifications facilitate the selection of the most suitable candidate for the subsequent hop, effectively minimizing the average energy consumption across all nodes in each iteration.","Furthermore, in this protocol, we iteratively fine-tune ACO's parameter values based on the outcomes of several experimental trials.","The experimental analysis is conducted through a diverse set of network topologies, and the results are subjected to comparison against well-established ACO algorithm and routing protocols.","The efficacy of the proposed protocol is assessed based on various performance metrics, encompassing throughput, energy consumption, network lifetime, energy consumption, the extent of data transferred over the network, and the length of paths traversed by packets.","These metrics collectively provide a comprehensive evaluation of the performance attainments of the routing protocols."],"url":"http://arxiv.org/abs/2402.12526v1","category":"cs.NI"}
{"created":"2024-02-19 20:36:32","title":"LangXAI: Integrating Large Vision Models for Generating Textual Explanations to Enhance Explainability in Visual Perception Tasks","abstract":"LangXAI is a framework that integrates Explainable Artificial Intelligence (XAI) with advanced vision models to generate textual explanations for visual recognition tasks. Despite XAI advancements, an understanding gap persists for end-users with limited domain knowledge in artificial intelligence and computer vision. LangXAI addresses this by furnishing text-based explanations for classification, object detection, and semantic segmentation model outputs to end-users. Preliminary results demonstrate LangXAI's enhanced plausibility, with high BERTScore across tasks, fostering a more transparent and reliable AI framework on vision tasks for end-users.","sentences":["LangXAI is a framework that integrates Explainable Artificial Intelligence (XAI) with advanced vision models to generate textual explanations for visual recognition tasks.","Despite XAI advancements, an understanding gap persists for end-users with limited domain knowledge in artificial intelligence and computer vision.","LangXAI addresses this by furnishing text-based explanations for classification, object detection, and semantic segmentation model outputs to end-users.","Preliminary results demonstrate LangXAI's enhanced plausibility, with high BERTScore across tasks, fostering a more transparent and reliable AI framework on vision tasks for end-users."],"url":"http://arxiv.org/abs/2402.12525v1","category":"cs.CV"}
{"created":"2024-02-19 20:29:34","title":"Gaussian Process Neural Additive Models","abstract":"Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required. The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets. In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM). GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality. It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions. We demonstrate the performance of GP-NAM on several tabular datasets, showing that it achieves comparable or better performance in both classification and regression tasks with a large reduction in the number of parameters.","sentences":["Deep neural networks have revolutionized many fields, but their black-box nature also occasionally prevents their wider adoption in fields such as healthcare and finance, where interpretable and explainable models are required.","The recent development of Neural Additive Models (NAMs) is a significant step in the direction of interpretable deep learning for tabular datasets.","In this paper, we propose a new subclass of NAMs that use a single-layer neural network construction of the Gaussian process via random Fourier features, which we call Gaussian Process Neural Additive Models (GP-NAM).","GP-NAMs have the advantage of a convex objective function and number of trainable parameters that grows linearly with feature dimensionality.","It suffers no loss in performance compared to deeper NAM approaches because GPs are well-suited for learning complex non-parametric univariate functions.","We demonstrate the performance of GP-NAM on several tabular datasets, showing that it achieves comparable or better performance in both classification and regression tasks with a large reduction in the number of parameters."],"url":"http://arxiv.org/abs/2402.12518v1","category":"cs.LG"}
{"created":"2024-02-19 20:21:09","title":"Induced Model Matching: How Restricted Models Can Help Larger Ones","abstract":"We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model. This restricted model may be thought of as \"side-information\", derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction. How can the restricted model be useful to the full model? We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model's context-restricted performance with that of the restricted model's. We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using logistic regression as a toy example. We then explore IMM's use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising. We demonstrate the methodology on both LSTM and transformer full models, using $N$-grams as restricted models. To further illustrate the potential of the principle whenever it is much cheaper to collect restricted rather than full information, we conclude with a simple RL example where POMDP policies can improve learned MDP policies via IMM.","sentences":["We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model.","This restricted model may be thought of as \"side-information\", derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction.","How can the restricted model be useful to the full model?","We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model's context-restricted performance with that of the restricted model's.","We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using logistic regression as a toy example.","We then explore IMM's use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising.","We demonstrate the methodology on both LSTM and transformer full models, using $N$-grams as restricted models.","To further illustrate the potential of the principle whenever it is much cheaper to collect restricted rather than full information, we conclude with a simple RL example where POMDP policies can improve learned MDP policies via IMM."],"url":"http://arxiv.org/abs/2402.12513v1","category":"cs.LG"}
{"created":"2024-02-19 20:19:51","title":"Talk Through It: End User Directed Manipulation Learning","abstract":"Training generalist robot agents is an immensely difficult feat due to the requirement to perform a huge range of tasks in many different environments. We propose selectively training robots based on end-user preferences instead. Given a factory model that lets an end user instruct a robot to perform lower-level actions (e.g. 'Move left'), we show that end users can collect demonstrations using language to train their home model for higher-level tasks specific to their needs (e.g. 'Open the top drawer and put the block inside'). We demonstrate this hierarchical robot learning framework on robot manipulation tasks using RLBench environments. Our method results in a 16% improvement in skill success rates compared to a baseline method. In further experiments, we explore the use of the large vision-language model (VLM), Bard, to automatically break down tasks into sequences of lower-level instructions, aiming to bypass end-user involvement. The VLM is unable to break tasks down to our lowest level, but does achieve good results breaking high-level tasks into mid-level skills. We have a supplemental video and additional results at talk-through-it.github.io.","sentences":["Training generalist robot agents is an immensely difficult feat due to the requirement to perform a huge range of tasks in many different environments.","We propose selectively training robots based on end-user preferences instead.","Given a factory model that lets an end user instruct a robot to perform lower-level actions (e.g. 'Move left'), we show that end users can collect demonstrations using language to train their home model for higher-level tasks specific to their needs (e.g. 'Open the top drawer and put the block inside').","We demonstrate this hierarchical robot learning framework on robot manipulation tasks using RLBench environments.","Our method results in a 16% improvement in skill success rates compared to a baseline method.","In further experiments, we explore the use of the large vision-language model (VLM), Bard, to automatically break down tasks into sequences of lower-level instructions, aiming to bypass end-user involvement.","The VLM is unable to break tasks down to our lowest level, but does achieve good results breaking high-level tasks into mid-level skills.","We have a supplemental video and additional results at talk-through-it.github.io."],"url":"http://arxiv.org/abs/2402.12509v1","category":"cs.RO"}
{"created":"2024-02-19 20:06:15","title":"Automated Security Response through Online Learning with Adaptive Conjectures","abstract":"We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We present our method through an advanced persistent threat use case. Simulation studies based on testbed measurements show that our method produces effective security strategies that adapt to a changing environment. We also find that our method enables faster convergence than current reinforcement learning techniques.","sentences":["We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game.","We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0.","This formulation allows us to capture uncertainty about the infrastructure and the intents of the players.","To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout.","We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model.","To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium.","We present our method through an advanced persistent threat use case.","Simulation studies based on testbed measurements show that our method produces effective security strategies that adapt to a changing environment.","We also find that our method enables faster convergence than current reinforcement learning techniques."],"url":"http://arxiv.org/abs/2402.12499v1","category":"cs.GT"}
{"created":"2024-02-19 19:54:03","title":"Towards Cross-Domain Continual Learning","abstract":"Continual learning is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data. The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting. Existing methods primarily focus on single domains, restricting their applicability to specific problems.   In this work, we introduce a novel approach called Cross-Domain Continual Learning (CDCL) that addresses the limitations of being limited to single supervised domains. Our method combines inter- and intra-task cross-attention mechanisms within a compact convolutional network. This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing unsupervised cross-domain (UDA) between related domains. By leveraging an intra-task-specific pseudo-labeling method, we ensure accurate input pairs for both labeled and unlabeled samples, enhancing the learning process. To validate our approach, we conduct extensive experiments on public UDA datasets, showcasing its positive performance on cross-domain continual learning challenges. Additionally, our work introduces incremental ideas that contribute to the advancement of this field.   We make our code and models available to encourage further exploration and reproduction of our results: \\url{https://github.com/Ivsucram/CDCL}","sentences":["Continual learning is a process that involves training learning agents to sequentially master a stream of tasks or classes without revisiting past data.","The challenge lies in leveraging previously acquired knowledge to learn new tasks efficiently, while avoiding catastrophic forgetting.","Existing methods primarily focus on single domains, restricting their applicability to specific problems.   ","In this work, we introduce a novel approach called Cross-Domain Continual Learning (CDCL) that addresses the limitations of being limited to single supervised domains.","Our method combines inter- and intra-task cross-attention mechanisms within a compact convolutional network.","This integration enables the model to maintain alignment with features from previous tasks, thereby delaying the data drift that may occur between tasks, while performing unsupervised cross-domain (UDA) between related domains.","By leveraging an intra-task-specific pseudo-labeling method, we ensure accurate input pairs for both labeled and unlabeled samples, enhancing the learning process.","To validate our approach, we conduct extensive experiments on public UDA datasets, showcasing its positive performance on cross-domain continual learning challenges.","Additionally, our work introduces incremental ideas that contribute to the advancement of this field.   ","We make our code and models available to encourage further exploration and reproduction of our results: \\url{https://github.com/Ivsucram/CDCL}"],"url":"http://arxiv.org/abs/2402.12490v1","category":"cs.LG"}
{"created":"2024-02-19 19:38:10","title":"Exploring the Interplay of Excitatory and Inhibitory Interactions in the Kuramoto Model on Circle Topologies","abstract":"In the field of collective dynamics, the Kuramoto model serves as a benchmark for the investigation of synchronization phenomena. While mean-field approaches and complex networks have been widely studied, the simple topology of a circle is still relatively unexplored, especially in the context of excitatory and inhibitory interactions. In this work, we focus on the dynamics of the Kuramoto model on a circle with positive and negative connections paying attention to the existence of new attractors different from the synchronized state. Using analytical and computational methods, we find that even for identical oscillators, the introduction of inhibitory interactions significantly modifies the structure of the attractors of the system. Our results extend the current understanding of synchronization in simple topologies and open new avenues for the study of collective dynamics in physical systems.","sentences":["In the field of collective dynamics, the Kuramoto model serves as a benchmark for the investigation of synchronization phenomena.","While mean-field approaches and complex networks have been widely studied, the simple topology of a circle is still relatively unexplored, especially in the context of excitatory and inhibitory interactions.","In this work, we focus on the dynamics of the Kuramoto model on a circle with positive and negative connections paying attention to the existence of new attractors different from the synchronized state.","Using analytical and computational methods, we find that even for identical oscillators, the introduction of inhibitory interactions significantly modifies the structure of the attractors of the system.","Our results extend the current understanding of synchronization in simple topologies and open new avenues for the study of collective dynamics in physical systems."],"url":"http://arxiv.org/abs/2402.12481v1","category":"nlin.AO"}
{"created":"2024-02-19 19:34:07","title":"In deep reinforcement learning, a pruned network is a good network","abstract":"Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of \"scaling law\", using only a small fraction of the full network parameters.","sentences":["Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters.","We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables agents to maximize parameter effectiveness.","This results in networks that yield dramatic performance improvements over traditional networks and exhibit a type of \"scaling law\", using only a small fraction of the full network parameters."],"url":"http://arxiv.org/abs/2402.12479v1","category":"cs.LG"}
{"created":"2024-02-19 19:11:22","title":"Neuro-mimetic Task-free Unsupervised Online Learning with Continual Self-Organizing Maps","abstract":"An intelligent system capable of continual learning is one that can process and extract knowledge from potentially infinitely long streams of pattern vectors. The major challenge that makes crafting such a system difficult is known as catastrophic forgetting - an agent, such as one based on artificial neural networks (ANNs), struggles to retain previously acquired knowledge when learning from new samples. Furthermore, ensuring that knowledge is preserved for previous tasks becomes more challenging when input is not supplemented with task boundary information. Although forgetting in the context of ANNs has been studied extensively, there still exists far less work investigating it in terms of unsupervised architectures such as the venerable self-organizing map (SOM), a neural model often used in clustering and dimensionality reduction. While the internal mechanisms of SOMs could, in principle, yield sparse representations that improve memory retention, we observe that, when a fixed-size SOM processes continuous data streams, it experiences concept drift. In light of this, we propose a generalization of the SOM, the continual SOM (CSOM), which is capable of online unsupervised learning under a low memory budget. Our results, on benchmarks including MNIST, Kuzushiji-MNIST, and Fashion-MNIST, show almost a two times increase in accuracy, and CIFAR-10 demonstrates a state-of-the-art result when tested on (online) unsupervised class incremental learning setting.","sentences":["An intelligent system capable of continual learning is one that can process and extract knowledge from potentially infinitely long streams of pattern vectors.","The major challenge that makes crafting such a system difficult is known as catastrophic forgetting - an agent, such as one based on artificial neural networks (ANNs), struggles to retain previously acquired knowledge when learning from new samples.","Furthermore, ensuring that knowledge is preserved for previous tasks becomes more challenging when input is not supplemented with task boundary information.","Although forgetting in the context of ANNs has been studied extensively, there still exists far less work investigating it in terms of unsupervised architectures such as the venerable self-organizing map (SOM), a neural model often used in clustering and dimensionality reduction.","While the internal mechanisms of SOMs could, in principle, yield sparse representations that improve memory retention, we observe that, when a fixed-size SOM processes continuous data streams, it experiences concept drift.","In light of this, we propose a generalization of the SOM, the continual SOM (CSOM), which is capable of online unsupervised learning under a low memory budget.","Our results, on benchmarks including MNIST, Kuzushiji-MNIST, and Fashion-MNIST, show almost a two times increase in accuracy, and CIFAR-10 demonstrates a state-of-the-art result when tested on (online) unsupervised class incremental learning setting."],"url":"http://arxiv.org/abs/2402.12465v1","category":"cs.LG"}
{"created":"2024-02-19 19:01:01","title":"The (R)Evolution of Multimodal Large Language Models: A Survey","abstract":"Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.","sentences":["Connecting text and visual modalities plays an essential role in generative intelligence.","For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs).","These models can seamlessly integrate visual and textual modalities, both as input and output, while providing a dialogue-based interface and instruction-following capabilities.","In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques.","We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications.","Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements.","Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs."],"url":"http://arxiv.org/abs/2402.12451v1","category":"cs.CV"}
{"created":"2024-02-19 19:00:01","title":"Aharonov-Bohm interference and the evolution of phase jumps in fractional quantum Hall Fabry-Perot interferometers based on bi-layer graphene","abstract":"Quasi-particles in fractional quantum Hall states are collective excitations that carry fractional charge and anyonic statistics. While the fractional charge affects semi-classical characteristics such as shot noise and charging energies, the anyonic statistics is most notable in quantum interference phenomena. In this study, we utilize a versatile bilayer graphene-based Fabry-P\\'erot Interferometer (FPI) that facilitates the study of a broad spectrum of operating regimes, from Coulomb-dominated to Aharonov-Bohm, for both integer and fractional quantum Hall states. Focusing on the $\\nu$=$1 \\over 3$ fractional quantum Hall state, we study the Aharonov-Bohm interference of quasi-particles when the magnetic flux through an interference loop and the charge density within the loop are independently varied. When their combined variation is such that the Landau filling remains $1 \\over 3$ we observe pristine Aharonov-Bohm oscillations with a period of three flux quanta, as is expected from the interference of quasi-particles of one-third of the electron charge. When the combined variation is such that it leads to quasi-particles addition or removal from the loop, phase jumps emerge, and alter the phase evolution. Notably, across all cases, the average phase consistently increases by 2$\\pi$ with each addition of one electron to the loop.","sentences":["Quasi-particles in fractional quantum Hall states are collective excitations that carry fractional charge and anyonic statistics.","While the fractional charge affects semi-classical characteristics such as shot noise and charging energies, the anyonic statistics is most notable in quantum interference phenomena.","In this study, we utilize a versatile bilayer graphene-based Fabry-P\\'erot Interferometer (FPI) that facilitates the study of a broad spectrum of operating regimes, from Coulomb-dominated to Aharonov-Bohm, for both integer and fractional quantum Hall states.","Focusing on the $\\nu$=$1 \\over 3$ fractional quantum Hall state, we study the Aharonov-Bohm interference of quasi-particles when the magnetic flux through an interference loop and the charge density within the loop are independently varied.","When their combined variation is such that the Landau filling remains $1 \\over 3$ we observe pristine Aharonov-Bohm oscillations with a period of three flux quanta, as is expected from the interference of quasi-particles of one-third of the electron charge.","When the combined variation is such that it leads to quasi-particles addition or removal from the loop, phase jumps emerge, and alter the phase evolution.","Notably, across all cases, the average phase consistently increases by 2$\\pi$ with each addition of one electron to the loop."],"url":"http://arxiv.org/abs/2402.12432v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-19 19:00:00","title":"GalaPy, the highly optimised C++/Python spectral modelling tool for galaxies -- I. Library presentation and photometric fitting","abstract":"Fostered by upcoming data from new generation observational campaigns, we are about to enter a new era for the study of how galaxies form and evolve. The unprecedented quantity of data that will be collected, from distances only marginally grasped up to now, will require analysis tools designed to target the specific physical peculiarities of the observed sources and handle extremely large datasets. One powerful method to investigate the complex astrophysical processes that govern the properties of galaxies is to model their observed spectral energy distribution (SED) at different stages of evolution and times throughout the history of the Universe. To address these challenges, we have developed GalaPy, a new library for modelling and fitting SEDs of galaxies from the X-ray to the radio band, as well as the evolution of their components and dust attenuation/reradiation. GalaPy incorporates both empirical and physically-motivated star formation histories, state-of-the-art single stellar population synthesis libraries, a two-component dust model for attenuation, an age-dependent energy conservation algorithm to compute dust reradiation, and additional sources of stellar continuum such as synchrotron, nebular/free-free emission and X-ray radiation from low and high mass binary stars. GalaPy has a hybrid implementation that combines the high performance of compiled C++ with the flexibility of Python, and exploits an object-oriented design. It generates models on the fly without relying on templates, and exploits fully Bayesian parameter space sampling. In this first work, we introduce the project and showcase the photometric SED fitting tools already available to users. The library is available on the Python Package Index (PyPI) and comes with extensive online documentation and tutorials.","sentences":["Fostered by upcoming data from new generation observational campaigns, we are about to enter a new era for the study of how galaxies form and evolve.","The unprecedented quantity of data that will be collected, from distances only marginally grasped up to now, will require analysis tools designed to target the specific physical peculiarities of the observed sources and handle extremely large datasets.","One powerful method to investigate the complex astrophysical processes that govern the properties of galaxies is to model their observed spectral energy distribution (SED) at different stages of evolution and times throughout the history of the Universe.","To address these challenges, we have developed GalaPy, a new library for modelling and fitting SEDs of galaxies from the X-ray to the radio band, as well as the evolution of their components and dust attenuation/reradiation.","GalaPy incorporates both empirical and physically-motivated star formation histories, state-of-the-art single stellar population synthesis libraries, a two-component dust model for attenuation, an age-dependent energy conservation algorithm to compute dust reradiation, and additional sources of stellar continuum such as synchrotron, nebular/free-free emission and X-ray radiation from low and high mass binary stars.","GalaPy has a hybrid implementation that combines the high performance of compiled C++ with the flexibility of Python, and exploits an object-oriented design.","It generates models on the fly without relying on templates, and exploits fully Bayesian parameter space sampling.","In this first work, we introduce the project and showcase the photometric SED fitting tools already available to users.","The library is available on the Python Package Index (PyPI) and comes with extensive online documentation and tutorials."],"url":"http://arxiv.org/abs/2402.12427v1","category":"astro-ph.GA"}
{"created":"2024-02-19 18:58:18","title":"HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools","abstract":"With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications. Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents. However, tools for these two steps are rarely applied in the same context in which they were developed. Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type. This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications. Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied systematically to corpora not used during their training. Based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types. Comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting. HunFlair2 showed the best performance on average, being closely followed by PubTator. Our results indicate that users of BTM tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make BTM tools more robust.","sentences":["With the exponential growth of the life science literature, biomedical text mining (BTM) has become an essential technology for accelerating the extraction of insights from publications.","Identifying named entities (e.g., diseases, drugs, or genes) in texts and their linkage to reference knowledge bases are crucial steps in BTM pipelines to enable information aggregation from different documents.","However, tools for these two steps are rarely applied in the same context in which they were developed.","Instead, they are applied in the wild, i.e., on application-dependent text collections different from those used for the tools' training, varying, e.g., in focus, genre, style, and text type.","This raises the question of whether the reported performance of BTM tools can be trusted for downstream applications.","Here, we report on the results of a carefully designed cross-corpus benchmark for named entity extraction, where tools were applied systematically to corpora not used during their training.","Based on a survey of 28 published systems, we selected five for an in-depth analysis on three publicly available corpora encompassing four different entity types.","Comparison between tools results in a mixed picture and shows that, in a cross-corpus setting, the performance is significantly lower than the one reported in an in-corpus setting.","HunFlair2 showed the best performance on average, being closely followed by PubTator.","Our results indicate that users of BTM tools should expect diminishing performances when applying them in the wild compared to original publications and show that further research is necessary to make BTM tools more robust."],"url":"http://arxiv.org/abs/2402.12372v2","category":"cs.CL"}
{"created":"2024-02-19 17:52:29","title":"Attacks on Node Attributes in Graph Neural Networks","abstract":"Graphs are commonly used to model complex networks prevalent in modern social media and literacy applications. Our research investigates the vulnerability of these graphs through the application of feature based adversarial attacks, focusing on both decision-time attacks and poisoning attacks. In contrast to state-of-the-art models like Net Attack and Meta Attack, which target node attributes and graph structure, our study specifically targets node attributes. For our analysis, we utilized the text dataset Hellaswag and graph datasets Cora and CiteSeer, providing a diverse basis for evaluation. Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean Node Embeddings and Graph Contrastive Learning strategies. This provides insights for graph data security, pinpointing where graph-based models are most vulnerable and thereby informing the development of stronger defense mechanisms against such attacks.","sentences":["Graphs are commonly used to model complex networks prevalent in modern social media and literacy applications.","Our research investigates the vulnerability of these graphs through the application of feature based adversarial attacks, focusing on both decision-time attacks and poisoning attacks.","In contrast to state-of-the-art models like Net Attack and Meta Attack, which target node attributes and graph structure, our study specifically targets node attributes.","For our analysis, we utilized the text dataset Hellaswag and graph datasets Cora and CiteSeer, providing a diverse basis for evaluation.","Our findings indicate that decision-time attacks using Projected Gradient Descent (PGD) are more potent compared to poisoning attacks that employ Mean Node Embeddings and Graph Contrastive Learning strategies.","This provides insights for graph data security, pinpointing where graph-based models are most vulnerable and thereby informing the development of stronger defense mechanisms against such attacks."],"url":"http://arxiv.org/abs/2402.12426v1","category":"cs.SI"}
{"created":"2024-02-19 16:34:50","title":"Tables as Images? Exploring the Strengths and Limitations of LLMs on Multimodal Representations of Tabular Data","abstract":"In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking. We introduce for the first time the assessment of LLMs' performance on image-based table representations. Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance. Our study provides insights into the effective use of LLMs on table-related tasks.","sentences":["In this paper, we investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats.","Our analysis extends across six benchmarks for table-related tasks such as question-answering and fact-checking.","We introduce for the first time the assessment of LLMs' performance on image-based table representations.","Specifically, we compare five text-based and three image-based table representations, demonstrating the influence of representation and prompting on LLM performance.","Our study provides insights into the effective use of LLMs on table-related tasks."],"url":"http://arxiv.org/abs/2402.12424v1","category":"cs.LG"}
{"created":"2024-02-20 18:57:59","title":"Explicit expressions for the gamma vector leading to connections to upper/lower bounds and structural properties","abstract":"We find an explicit formula for the gamma vector in terms of the input polynomial in a way that extends it to arbitrary polynomials. More specifically, we find explicit linear combination in terms of coefficients of the input polynomial (using Catalan numbers and binomial coefficients) and an expression involving the derivative of the input polynomial. In the case where the input is the $h$-polynomial of a simplicial complex, this gives an interpretation of the gamma vector as a measure of local-global contributions. We also apply them to connect signs/inequalities of (shifts of) the gamma vector to upper/lower bound conditions on coefficients of the input polynomial. Finally, we make use of the shape of the sums used to make these estimates and connections with intersection numbers to relate these properties of the gamma vector to algebraic structures.","sentences":["We find an explicit formula for the gamma vector in terms of the input polynomial in a way that extends it to arbitrary polynomials.","More specifically, we find explicit linear combination in terms of coefficients of the input polynomial (using Catalan numbers and binomial coefficients) and an expression involving the derivative of the input polynomial.","In the case where the input is the $h$-polynomial of a simplicial complex, this gives an interpretation of the gamma vector as a measure of local-global contributions.","We also apply them to connect signs/inequalities of (shifts of) the gamma vector to upper/lower bound conditions on coefficients of the input polynomial.","Finally, we make use of the shape of the sums used to make these estimates and connections with intersection numbers to relate these properties of the gamma vector to algebraic structures."],"url":"http://arxiv.org/abs/2402.13248v1","category":"math.CO"}
{"created":"2024-02-20 18:51:20","title":"Continuous Pushdown VASS in One Dimension are Easy","abstract":"A pushdown vector addition system with states (PVASS) extends the model of vector addition systems with a pushdown stack. The algorithmic analysis of PVASS has applications such as static analysis of recursive programs manipulating integer variables. Unfortunately, reachability analysis, even for one-dimensional PVASS is not known to be decidable. We relax the model of one-dimensional PVASS to make the counter updates continuous and show that in this case reachability, coverability, and boundedness are decidable in polynomial time. In addition, for the extension of the model with lower-bound guards on the states, we show that coverability and reachability are in NP, and boundedness is in coNP.","sentences":["A pushdown vector addition system with states (PVASS) extends the model of vector addition systems with a pushdown stack.","The algorithmic analysis of PVASS has applications such as static analysis of recursive programs manipulating integer variables.","Unfortunately, reachability analysis, even for one-dimensional PVASS is not known to be decidable.","We relax the model of one-dimensional PVASS to make the counter updates continuous and show that in this case reachability, coverability, and boundedness are decidable in polynomial time.","In addition, for the extension of the model with lower-bound guards on the states, we show that coverability and reachability are in NP, and boundedness is in coNP."],"url":"http://arxiv.org/abs/2402.13237v1","category":"cs.LO"}
{"created":"2024-02-20 18:47:02","title":"Chemical abundances and ionizing mechanisms in the star-forming double-ring of AM 0644-741 using MUSE data","abstract":"We present the analysis of archival Very Large Telescope (VLT) Multi-Unit Spectroscopic Explorer (MUSE) observations of 179 HII regions in the star-forming double-ring collisional galaxy AM 0644-741 at 98.6 Mpc. We determined ionic abundances of He, N, O and Fe using the direct method for the brightest H II region (ID 39); we report $\\log\\rm{(\\frac{N}{O})}=-1.3\\pm0.2$ and $12+\\log\\rm{(\\frac{O}{H})}=8.9\\pm0.2$. We also find the so-called `blue-bump', broad He II $\\lambda4686$, in the spectrum of this knot of massive star-formation; its luminosity being consistent with the presence of $\\sim430$ Wolf-Rayet (WR) stars of the Nitrogen late-type. We determined the O abundances for 137 HII regions using the strong-line method; we report a median value of $12+\\log\\rm{(\\frac{O}{H})}=8.5\\pm0.8$. The location of three objects, including the WR complex, coincide with that of an Ultra Luminous X-ray source. Nebular He II is not detected in any H II region. We investigate the physical mechanisms responsible for the observed spectral lines using appropriate diagnostic diagrams and ionization models. We find that the H II regions are being photoionized by star clusters with ages $\\sim2.5-20$ Myr and ionization potential $-3.5<$$\\log\\langle U\\rangle$$<-3.0$. In these diagrams, a binary population is needed to reproduce the observables considered in this work.","sentences":["We present the analysis of archival Very Large Telescope (VLT) Multi-Unit Spectroscopic Explorer (MUSE) observations of 179 HII regions in the star-forming double-ring collisional galaxy AM 0644-741 at 98.6 Mpc.","We determined ionic abundances of He, N, O and Fe using the direct method for the brightest H II region (ID 39); we report $\\log\\rm{(\\frac{N}{O})}=-1.3\\pm0.2$ and $12+\\log\\rm{(\\frac{O}{H})}=8.9\\pm0.2$. We also find the so-called `blue-bump', broad He II $\\lambda4686$, in the spectrum of this knot of massive star-formation; its luminosity being consistent with the presence of $\\sim430$ Wolf-Rayet (WR) stars of the Nitrogen late-type.","We determined the O abundances for 137 HII regions using the strong-line method; we report a median value of $12+\\log\\rm{(\\frac{O}{H})}=8.5\\pm0.8$. The location of three objects, including the WR complex, coincide with that of an Ultra Luminous X-ray source.","Nebular He II is not detected in any H II region.","We investigate the physical mechanisms responsible for the observed spectral lines using appropriate diagnostic diagrams and ionization models.","We find that the H II regions are being photoionized by star clusters with ages $\\sim2.5-20$ Myr and ionization potential $-3.5<$$\\log\\langle U\\rangle$$<-3.0$. In these diagrams, a binary population is needed to reproduce the observables considered in this work."],"url":"http://arxiv.org/abs/2402.13230v1","category":"astro-ph.GA"}
{"created":"2024-02-20 18:16:33","title":"European VLBI Network observations of the peculiar radio source 4C 35.06 overlapping with a compact group of nine galaxies","abstract":"Context. According to the hierarchical structure formation model, brightest cluster galaxies (BCGs) evolve into the most luminous and massive galaxies in the Universe through multiple merger events. The peculiar radio source 4C 35.06 is located at the core of the galaxy cluster Abell 407, overlapping with a compact group of nine galaxies. Low-frequency radio observations have revealed a helical, steep-spectrum, kiloparsec-scale jet structure and inner lobes with less steep spectra, compatible with a recurring active galactic nucleus (AGN) activity scenario. However, the host galaxy of the AGN responsible for the detected radio emission remained unclear.   Aims. We aim to identify the host of 4C 35.06 by studying the object at high angular resolution and thereby confirm the recurrent AGN activity scenario.   Methods. To reveal the host of the radio source, we carried out very long baseline interferometry (VLBI) observations with the European VLBI Network of the nine galaxies in the group at 1.7 and 4.9 GHz.   Results. We detected compact radio emission from an AGN located between the two inner lobes at both observing frequencies. In addition, we detected another galaxy at 1.7 GHz, whose position appears more consistent with the principal jet axis and is located closer to the low-frequency radio peak of 4C 35.06. The presence of another radio-loud AGN in the nonet sheds new light on the BCG formation and provides an alternative scenario in which not just one but two AGNs are responsible for the complex large-scale radio structure","sentences":["Context.","According to the hierarchical structure formation model, brightest cluster galaxies (BCGs) evolve into the most luminous and massive galaxies in the Universe through multiple merger events.","The peculiar radio source 4C 35.06 is located at the core of the galaxy cluster Abell 407, overlapping with a compact group of nine galaxies.","Low-frequency radio observations have revealed a helical, steep-spectrum, kiloparsec-scale jet structure and inner lobes with less steep spectra, compatible with a recurring active galactic nucleus (AGN) activity scenario.","However, the host galaxy of the AGN responsible for the detected radio emission remained unclear.   ","Aims.","We aim to identify the host of 4C 35.06 by studying the object at high angular resolution and thereby confirm the recurrent AGN activity scenario.   ","Methods.","To reveal the host of the radio source, we carried out very long baseline interferometry (VLBI) observations with the European VLBI Network of the nine galaxies in the group at 1.7 and 4.9 GHz.   Results.","We detected compact radio emission from an AGN located between the two inner lobes at both observing frequencies.","In addition, we detected another galaxy at 1.7 GHz, whose position appears more consistent with the principal jet axis and is located closer to the low-frequency radio peak of 4C 35.06.","The presence of another radio-loud AGN in the nonet sheds new light on the BCG formation and provides an alternative scenario in which not just one but two AGNs are responsible for the complex large-scale radio structure"],"url":"http://arxiv.org/abs/2402.13207v1","category":"astro-ph.GA"}
{"created":"2024-02-20 18:01:59","title":"Spatial Queues with Nearest Neighbour Shifts","abstract":"In this work we study multi-server queues on a Euclidean space. Consider $N$ servers that are distributed uniformly in $[0,1]^d$. Customers (users) arrive at the servers according to independent Poisson processes of intensity $\\lambda$. However, they probabilistically decide whether to join the queue they arrived at, or move to one of the nearest neighbours. The strategy followed by the customers affects the load on the servers in the long run. In this paper, we are interested in characterizing the fraction of servers that bear a larger load as compared to when the users do not follow any strategy, i.e., they join the queue they arrive at. These are called overloaded servers. In the one-dimensional case ($d=1$), we evaluate the expected fraction of overloaded servers for any finite $N$ when the users follow probabilistic nearest neighbour shift strategies. Additionally, for servers distributed in a $d$-dimensional space we provide expressions for the fraction of overloaded servers in the system as the total number of servers $N\\rightarrow \\infty$. Numerical experiments are provided to support our claims. Typical applications of our results include electric vehicles queueing at charging stations, and queues in airports or supermarkets.","sentences":["In this work we study multi-server queues on a Euclidean space.","Consider $N$ servers that are distributed uniformly in $[0,1]^d$. Customers (users) arrive at the servers according to independent Poisson processes of intensity $\\lambda$. However, they probabilistically decide whether to join the queue they arrived at, or move to one of the nearest neighbours.","The strategy followed by the customers affects the load on the servers in the long run.","In this paper, we are interested in characterizing the fraction of servers that bear a larger load as compared to when the users do not follow any strategy, i.e., they join the queue they arrive at.","These are called overloaded servers.","In the one-dimensional case ($d=1$), we evaluate the expected fraction of overloaded servers for any finite $N$ when the users follow probabilistic nearest neighbour shift strategies.","Additionally, for servers distributed in a $d$-dimensional space we provide expressions for the fraction of overloaded servers in the system as the total number of servers $N\\rightarrow \\infty$. Numerical experiments are provided to support our claims.","Typical applications of our results include electric vehicles queueing at charging stations, and queues in airports or supermarkets."],"url":"http://arxiv.org/abs/2402.13192v1","category":"math.PR"}
{"created":"2024-02-20 17:59:48","title":"A novel image correction method for cloud-affected observations with Imaging Atmospheric Cherenkov Telescopes","abstract":"Context. The presence of clouds during observations with Imaging Atmospheric Cherenkov Telescopes can strongly affect the performance of the instrument due to additional absorption of light and scattering of light beyond the field of view of the instrument. If not corrected for, the presence of clouds leads to increased systematic errors in the results. Aims. One approach to correct for the effects of clouds is to include clouds in Monte Carlo simulations to produce models for primary particle classification, energy and direction estimation. However, this method is challenging due to the dynamic nature of cloudy conditions and requires extensive computational resources. The second approach focuses on correcting the data itself for cloud effects, which allows the use of standard simulations. However, existing corrections often prioritise limiting systematic errors without optimising overall performance. By correcting the data already at the image level, it is possible to improve event reconstruction without the need for specialised simulations. Methods. We introduce a novel analysis method, based on a geometrical model that can correct the data already at the image level given a vertical transmission profile of a cloud. Using Monte Carlo simulations of an array of four Large-Sized Telescopes of the Cherenkov Telescope Array, we investigate the effect of the correction on the image parameters and the performance of the system. We compare the data correction at the camera level with the use of dedicated simulations for clouds with different transmissions and heights. Results. The proposed method efficiently corrects the extinction of light in clouds, eliminating the need for dedicated simulations. Evaluation using Monte Carlo simulations demonstrates improved gamma-ray event reconstruction and overall system performance.","sentences":["Context.","The presence of clouds during observations with Imaging Atmospheric Cherenkov Telescopes can strongly affect the performance of the instrument due to additional absorption of light and scattering of light beyond the field of view of the instrument.","If not corrected for, the presence of clouds leads to increased systematic errors in the results.","Aims.","One approach to correct for the effects of clouds is to include clouds in Monte Carlo simulations to produce models for primary particle classification, energy and direction estimation.","However, this method is challenging due to the dynamic nature of cloudy conditions and requires extensive computational resources.","The second approach focuses on correcting the data itself for cloud effects, which allows the use of standard simulations.","However, existing corrections often prioritise limiting systematic errors without optimising overall performance.","By correcting the data already at the image level, it is possible to improve event reconstruction without the need for specialised simulations.","Methods.","We introduce a novel analysis method, based on a geometrical model that can correct the data already at the image level given a vertical transmission profile of a cloud.","Using Monte Carlo simulations of an array of four Large-Sized Telescopes of the Cherenkov Telescope Array, we investigate the effect of the correction on the image parameters and the performance of the system.","We compare the data correction at the camera level with the use of dedicated simulations for clouds with different transmissions and heights.","Results.","The proposed method efficiently corrects the extinction of light in clouds, eliminating the need for dedicated simulations.","Evaluation using Monte Carlo simulations demonstrates improved gamma-ray event reconstruction and overall system performance."],"url":"http://arxiv.org/abs/2402.13190v1","category":"astro-ph.IM"}
{"created":"2024-02-20 17:53:24","title":"Testing Calibration in Subquadratic Time","abstract":"In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$ is $\\varepsilon$-far from calibration.   We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factors) in time $O(n^{1.5} \\log(n))$. This improves upon state-of-the-art black-box linear program solvers requiring $\\Omega(n^\\omega)$ time, where $\\omega > 2$ is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem, and give sample complexity lower bounds for alternative calibration distances to the one considered in this work. Finally, we present preliminary experiments showing that the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale to accommodate moderate sample sizes.","sentences":["In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models.","However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored.","Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing.","We define the problem of calibration testing from samples where given $n$ draws from a distribution $\\mathcal{D}$ on (predictions, binary outcomes), our goal is to distinguish between the case where $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$ is $\\varepsilon$-far from calibration.   ","We design an algorithm based on approximate linear programming, which solves calibration testing information-theoretically optimally (up to constant factors) in time $O(n^{1.5} \\log(n))$. This improves upon state-of-the-art black-box linear program solvers requiring $\\Omega(n^\\omega)$ time, where $\\omega > 2$ is the exponent of matrix multiplication.","We also develop algorithms for tolerant variants of our testing problem, and give sample complexity lower bounds for alternative calibration distances to the one considered in this work.","Finally, we present preliminary experiments showing that the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale to accommodate moderate sample sizes."],"url":"http://arxiv.org/abs/2402.13187v1","category":"cs.LG"}
{"created":"2024-02-20 17:35:50","title":"Long-term evolution of solar activity and prediction of the following solar cycles","abstract":"Solar activities have a great impact on modern high-tech systems, such as human aerospace, satellite communication and navigation, deep space exploration, and related scientific research. Therefore, studying the long - term evolution trend of solar activity and accurately predicting the future solar cycles is highly anticipated. Based on wavelet transform and empirical function fitting of the longest recorded data of the annual average relative sunspot number (ASN) series of 323 years to date, this work decisively verified the existence of the solar century cycles and confirmed that its length is about 104.0 years, and the magnitude has a slightly increasing trend on the time scale of several hundreds of years. Based on this long-term evolutionary trend, we predicted solar cycle 25 and 26 by using phase similar prediction methods. As for the solar cycle 25, its maximum ASN will be about $146.7\\pm 33.40$, obviously stronger than solar cycle 24. The peak year will occur approximately in 2024, and the period is about $11\\pm 1$ years. As for the solar cycle 26, it will start around 2030, reach the maximum between 2035 and 2036, with maximum ASN of about $133.0\\pm 3.200$, and the period is about 10 years.","sentences":["Solar activities have a great impact on modern high-tech systems, such as human aerospace, satellite communication and navigation, deep space exploration, and related scientific research.","Therefore, studying the long - term evolution trend of solar activity and accurately predicting the future solar cycles is highly anticipated.","Based on wavelet transform and empirical function fitting of the longest recorded data of the annual average relative sunspot number (ASN) series of 323 years to date, this work decisively verified the existence of the solar century cycles and confirmed that its length is about 104.0 years, and the magnitude has a slightly increasing trend on the time scale of several hundreds of years.","Based on this long-term evolutionary trend, we predicted solar cycle 25 and 26 by using phase similar prediction methods.","As for the solar cycle 25, its maximum ASN will be about $146.7\\pm 33.40$, obviously stronger than solar cycle 24.","The peak year will occur approximately in 2024, and the period is about $11\\pm 1$ years.","As for the solar cycle 26, it will start around 2030, reach the maximum between 2035 and 2036, with maximum ASN of about $133.0\\pm 3.200$, and the period is about 10 years."],"url":"http://arxiv.org/abs/2402.13173v1","category":"astro-ph.SR"}
{"created":"2024-02-20 17:30:45","title":"Improved Space Bounds for Subset Sum","abstract":"More than 40 years ago, Schroeppel and Shamir presented an algorithm that solves the Subset Sum problem for $n$ integers in time $O^*(2^{0.5n})$ and space $O^*(2^{0.25n})$. The time upper bound remains unbeaten, but the space upper bound has been improved to $O^*(2^{0.249999n})$ in a recent breakthrough paper by Nederlof and W\\k{e}grzycki (STOC 2021). Their algorithm is a clever combination of a number of previously known techniques with a new reduction and a new algorithm for the Orthogonal Vectors problem.   In this paper, we give two new algorithms for Subset Sum. We start by presenting an Arthur--Merlin algorithm: upon receiving the verifier's randomness, the prover sends an $n/4$-bit long proof to the verifier who checks it in (deterministic) time and space $O^*(2^{n/4})$. The simplicity of this algorithm has a number of interesting consequences: it can be parallelized easily; also, by enumerating all possible proofs, one recovers upper bounds on time and space for Subset Sum proved by Schroeppel and Shamir in 1979. As it is the case with the previously known algorithms for Subset Sum, our algorithm follows from an algorithm for $4$-SUM: we prove that, using verifier's coin tosses, the prover can prepare a $\\log_2 n$-bit long proof verifiable in time $\\tilde{O}(n)$. Another interesting consequence of this result is the following fine-grained lower bound: assuming that $4$-SUM cannot be solved in time $O(n^{2-\\varepsilon})$ for all $\\varepsilon>0$, Circuit SAT cannot be solved in time $O(g2^{(1-\\varepsilon)n})$, for all $\\varepsilon>0$.   Then, we improve the space bound by Nederlof and W\\k{e}grzycki to $O^*(2^{0.246n})$ and also simplify their algorithm and its analysis. We achieve this space bound by further filtering sets of subsets using a random prime number. This allows us to reduce an instance of Subset Sum to a larger number of instances of smaller size.","sentences":["More than 40 years ago, Schroeppel and Shamir presented an algorithm that solves the Subset Sum problem for $n$ integers in time $O^*(2^{0.5n})$ and space $O^*(2^{0.25n})$.","The time upper bound remains unbeaten, but the space upper bound has been improved to $O^*(2^{0.249999n})$ in a recent breakthrough paper by Nederlof and W\\k{e}grzycki (STOC 2021).","Their algorithm is a clever combination of a number of previously known techniques with a new reduction and a new algorithm for the Orthogonal Vectors problem.   ","In this paper, we give two new algorithms for Subset Sum.","We start by presenting an Arthur--Merlin algorithm: upon receiving the verifier's randomness, the prover sends an $n/4$-bit long proof to the verifier who checks it in (deterministic) time and space $O^*(2^{n/4})$. The simplicity of this algorithm has a number of interesting consequences: it can be parallelized easily; also, by enumerating all possible proofs, one recovers upper bounds on time and space for Subset Sum proved by Schroeppel and Shamir in 1979.","As it is the case with the previously known algorithms for Subset Sum, our algorithm follows from an algorithm for $4$-SUM: we prove that, using verifier's coin tosses, the prover can prepare a $\\log_2 n$-bit long proof verifiable in time $\\tilde{O}(n)$. Another interesting consequence of this result is the following fine-grained lower bound: assuming that $4$-SUM cannot be solved in time $O(n^{2-\\varepsilon})$ for all $\\varepsilon>0$, Circuit SAT cannot be solved in time $O(g2^{(1-\\varepsilon)n})$, for all $\\varepsilon>0$.   Then, we improve the space bound by Nederlof and W\\k{e}grzycki to $O^*(2^{0.246n})$ and also simplify their algorithm and its analysis.","We achieve this space bound by further filtering sets of subsets using a random prime number.","This allows us to reduce an instance of Subset Sum to a larger number of instances of smaller size."],"url":"http://arxiv.org/abs/2402.13170v1","category":"cs.CC"}
{"created":"2024-02-20 17:29:19","title":"Nonequilibrium fluctuations of chemical reaction networks at criticality: The Schl\u00f6gl model as paradigmatic case","abstract":"Chemical reaction networks can undergo nonequilibrium phase transitions upon variation of external control parameters like the chemical potential of a species. We investigate the flux in the associated chemostats that is proportional to the entropy production and its critical fluctuations within the Schl\\\"ogl model. Numerical simulations show that the corresponding diffusion coefficient diverges at the critical point as a function of system size. In the vicinity of the critical point, the diffusion coefficient follows a scaling form. We develop an analytical approach based on the chemical Langevin equation and van Kampen's system size expansion that yields the corresponding exponents in the monostable regime. In the bistable regime, we rely on a two-state approximation in order to analytically describe the critical behavior.","sentences":["Chemical reaction networks can undergo nonequilibrium phase transitions upon variation of external control parameters like the chemical potential of a species.","We investigate the flux in the associated chemostats that is proportional to the entropy production and its critical fluctuations within the Schl\\\"ogl model.","Numerical simulations show that the corresponding diffusion coefficient diverges at the critical point as a function of system size.","In the vicinity of the critical point, the diffusion coefficient follows a scaling form.","We develop an analytical approach based on the chemical Langevin equation and van Kampen's system size expansion that yields the corresponding exponents in the monostable regime.","In the bistable regime, we rely on a two-state approximation in order to analytically describe the critical behavior."],"url":"http://arxiv.org/abs/2402.13168v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-20 17:28:57","title":"Convergence Rate for Moderate Interaction particles and Application to Mean Field Games","abstract":"We study two interacting particle systems, both modeled as a system of $N$ stochastic differential equations driven by Brownian motions with singular kernels and moderate interaction. We show a quantitative result where the convergence rate depends on the moderate scaling parameter, the regularity of the solution of the limit equation and the dimension. Our approach is based on the techniques of stochastic calculus, some properties of Besov and Triebel-Lizorkin space, and the semigroup approach introduced in [9].","sentences":["We study two interacting particle systems, both modeled as a system of $N$ stochastic differential equations driven by Brownian motions with singular kernels and moderate interaction.","We show a quantitative result where the convergence rate depends on the moderate scaling parameter, the regularity of the solution of the limit equation and the dimension.","Our approach is based on the techniques of stochastic calculus, some properties of Besov and Triebel-Lizorkin space, and the semigroup approach introduced in [9]."],"url":"http://arxiv.org/abs/2402.13167v1","category":"math.PR"}
{"created":"2024-02-20 17:22:36","title":"Measurement-induced phase transitions by matrix product states scaling","abstract":"We study the time evolution of long quantum spin chains subjected to continuous monitoring via matrix product states (MPS) at fixed bond dimension, with the Time-Dependent Variational Principle (TDVP) algorithm. The latter gives an effective classical non-linear evolution with a conserved charge, which approximates the real quantum evolution up to an error. We show that the error rate displays a phase transition in the monitoring strength, which can be well detected by scaling analysis with relatively low values of bond dimensions. The method allows for an efficient numerical determination of the critical measurement-induced phase transition parameters in many-body quantum systems. Moreover, in the presence of U(1) global spin charge, we show the existence of a charge-sharpening transition well separated from the entanglement transition which we detect by studying the charge fluctuations of a local sub-part of the system at very large times. Our work substantiates the TDVP time evolution as a method to identify measured-induced phase transitions in systems of arbitrary dimensions and sizes.","sentences":["We study the time evolution of long quantum spin chains subjected to continuous monitoring via matrix product states (MPS) at fixed bond dimension, with the Time-Dependent Variational Principle (TDVP) algorithm.","The latter gives an effective classical non-linear evolution with a conserved charge, which approximates the real quantum evolution up to an error.","We show that the error rate displays a phase transition in the monitoring strength, which can be well detected by scaling analysis with relatively low values of bond dimensions.","The method allows for an efficient numerical determination of the critical measurement-induced phase transition parameters in many-body quantum systems.","Moreover, in the presence of U(1) global spin charge, we show the existence of a charge-sharpening transition well separated from the entanglement transition which we detect by studying the charge fluctuations of a local sub-part of the system at very large times.","Our work substantiates the TDVP time evolution as a method to identify measured-induced phase transitions in systems of arbitrary dimensions and sizes."],"url":"http://arxiv.org/abs/2402.13160v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-20 17:12:16","title":"Disorder-Free Sachdev-Ye-Kitaev models: Integrability and Quantum Chaos","abstract":"We introduce two disorder-free variants of the Sachdev-Ye-Kitaev (SYK) model, demonstrate their integrability, and study their static and dynamical properties. Unlike diagrammatic techniques, the integrability of these models allows us to obtain dynamical correlation functions even when the number of Majorana fermions is finite. From the solutions, we find that out-of-time-order correlators (OTOCs) in these models exhibit exponential growth at early times, resembling that of quantum chaotic systems such as those with disorder or external kick terms. Conversely, our analysis shows no evidence of random-matrix behavior in level statistics or the spectral form factor. Our findings illustrate that the clean versions of the SYK models represent simple but nontrivial examples of disorder-free quantum many-body systems displaying chaos-like behavior of OTOCs.","sentences":["We introduce two disorder-free variants of the Sachdev-Ye-Kitaev (SYK) model, demonstrate their integrability, and study their static and dynamical properties.","Unlike diagrammatic techniques, the integrability of these models allows us to obtain dynamical correlation functions even when the number of Majorana fermions is finite.","From the solutions, we find that out-of-time-order correlators (OTOCs) in these models exhibit exponential growth at early times, resembling that of quantum chaotic systems such as those with disorder or external kick terms.","Conversely, our analysis shows no evidence of random-matrix behavior in level statistics or the spectral form factor.","Our findings illustrate that the clean versions of the SYK models represent simple but nontrivial examples of disorder-free quantum many-body systems displaying chaos-like behavior of OTOCs."],"url":"http://arxiv.org/abs/2402.13154v1","category":"cond-mat.str-el"}
{"created":"2024-02-20 17:05:16","title":"Choosing a Suitable Requirement Prioritization Method: A Survey","abstract":"Software requirements prioritization plays a crucial role in software development. It can be viewed as the process of ordering requirements by determining which requirements must be done first and which can be done later. Powerful requirements prioritization techniques are of paramount importance to finish the implementation on time and within budget. Many factors affect requirement prioritization such as stakeholder expectations, complexity, dependency, scalability, risk, and cost. Therefore, finding the proper order of requirements is a challenging process. Hence, different types of requirements prioritization techniques have been developed to support this task. In this survey, we propose a novel classification that can classify the prioritization techniques under two major classes: relative and exact prioritization techniques class, where each class is divided into two subclasses. We depend in our classification on the way the value of ranking is given to the requirement, either explicitly as a specific value in the case of the exact prioritization techniques class, or implicitly in the case of the Relative prioritization technique class. An overview of fifteen different requirements prioritization techniques are presented and organized according to the proposed classification criteria's. Moreover, we make a comparison between methods that are related to the same subclass to analyze their strengths and weaknesses. Based on the comparison results, the properties for each proposed subclass of techniques are identified. Depending on these properties, we present some recommendations to help project managers in the process of selecting the most suitable technique to prioritize requirements based on their project characteristics (number of requirements, time, cost, and accuracy).","sentences":["Software requirements prioritization plays a crucial role in software development.","It can be viewed as the process of ordering requirements by determining which requirements must be done first and which can be done later.","Powerful requirements prioritization techniques are of paramount importance to finish the implementation on time and within budget.","Many factors affect requirement prioritization such as stakeholder expectations, complexity, dependency, scalability, risk, and cost.","Therefore, finding the proper order of requirements is a challenging process.","Hence, different types of requirements prioritization techniques have been developed to support this task.","In this survey, we propose a novel classification that can classify the prioritization techniques under two major classes: relative and exact prioritization techniques class, where each class is divided into two subclasses.","We depend in our classification on the way the value of ranking is given to the requirement, either explicitly as a specific value in the case of the exact prioritization techniques class, or implicitly in the case of the Relative prioritization technique class.","An overview of fifteen different requirements prioritization techniques are presented and organized according to the proposed classification criteria's.","Moreover, we make a comparison between methods that are related to the same subclass to analyze their strengths and weaknesses.","Based on the comparison results, the properties for each proposed subclass of techniques are identified.","Depending on these properties, we present some recommendations to help project managers in the process of selecting the most suitable technique to prioritize requirements based on their project characteristics (number of requirements, time, cost, and accuracy)."],"url":"http://arxiv.org/abs/2402.13149v1","category":"cs.SE"}
{"created":"2024-02-20 16:51:30","title":"Relaxing Trust Assumptions on Quantum Key Distribution Networks","abstract":"Quantum security over long distances with untrusted relays is largely unfounded and is still an open question for active research. Nevertheless, quantum networks based on trusted relays are being built across the globe. However, standard QKD network architecture implores a complete trust requirement on QKD relays, which is too demanding and limits the use cases for QKD networks. In this work, we explore the possibility to securely relay a secret in a QKD network by relaxing the trust assumptions (if not completely) on the relay. We characterize QKD relays with different trust levels, namely, Full Access Trust (FAT), Partial Access Trust (PAT), and No Access Trust (NAT). As the name suggests, each level defines the degree with which a relay is required to be trusted with the secret provided by the key management system for end-to-end communication. We then review and propose multiple constructions of the QKD key management system based on the different trust levels. Main contribution of the paper is realized by evaluating key management systems with no access trust level. In principle, we review key management with centralized topology and propose a new decentralized key management system. These different topologies provide various advantages based on the QKD network requirements, allowing an operational flexibility in the architecture. We believe this work presents a new perspective to the open problem of providing a confiding and a practical solution for future long range secure communications","sentences":["Quantum security over long distances with untrusted relays is largely unfounded and is still an open question for active research.","Nevertheless, quantum networks based on trusted relays are being built across the globe.","However, standard QKD network architecture implores a complete trust requirement on QKD relays, which is too demanding and limits the use cases for QKD networks.","In this work, we explore the possibility to securely relay a secret in a QKD network by relaxing the trust assumptions (if not completely) on the relay.","We characterize QKD relays with different trust levels, namely, Full Access Trust (FAT), Partial Access Trust (PAT), and No Access Trust (NAT).","As the name suggests, each level defines the degree with which a relay is required to be trusted with the secret provided by the key management system for end-to-end communication.","We then review and propose multiple constructions of the QKD key management system based on the different trust levels.","Main contribution of the paper is realized by evaluating key management systems with no access trust level.","In principle, we review key management with centralized topology and propose a new decentralized key management system.","These different topologies provide various advantages based on the QKD network requirements, allowing an operational flexibility in the architecture.","We believe this work presents a new perspective to the open problem of providing a confiding and a practical solution for future long range secure communications"],"url":"http://arxiv.org/abs/2402.13136v1","category":"quant-ph"}
{"created":"2024-02-20 16:50:47","title":"A Systematic Literature Review on Task Allocation and Performance Management Techniques in Cloud Data Center","abstract":"As cloud computing usage grows, cloud data centers play an increasingly important role. To maximize resource utilization, ensure service quality, and enhance system performance, it is crucial to allocate tasks and manage performance effectively. The purpose of this study is to provide an extensive analysis of task allocation and performance management techniques employed in cloud data centers. The aim is to systematically categorize and organize previous research by identifying the cloud computing methodologies, categories, and gaps. A literature review was conducted, which included the analysis of 463 task allocations and 480 performance management papers. The review revealed three task allocation research topics and seven performance management methods. Task allocation research areas are resource allocation, load-Balancing, and scheduling. Performance management includes monitoring and control, power and energy management, resource utilization optimization, quality of service management, fault management, virtual machine management, and network management. The study proposes new techniques to enhance cloud computing work allocation and performance management. Short-comings in each approach can guide future research. The research's findings on cloud data center task allocation and performance management can assist academics, practitioners, and cloud service providers in optimizing their systems for dependability, cost-effectiveness, and scalability. Innovative methodologies can steer future research to fill gaps in the literature.","sentences":["As cloud computing usage grows, cloud data centers play an increasingly important role.","To maximize resource utilization, ensure service quality, and enhance system performance, it is crucial to allocate tasks and manage performance effectively.","The purpose of this study is to provide an extensive analysis of task allocation and performance management techniques employed in cloud data centers.","The aim is to systematically categorize and organize previous research by identifying the cloud computing methodologies, categories, and gaps.","A literature review was conducted, which included the analysis of 463 task allocations and 480 performance management papers.","The review revealed three task allocation research topics and seven performance management methods.","Task allocation research areas are resource allocation, load-Balancing, and scheduling.","Performance management includes monitoring and control, power and energy management, resource utilization optimization, quality of service management, fault management, virtual machine management, and network management.","The study proposes new techniques to enhance cloud computing work allocation and performance management.","Short-comings in each approach can guide future research.","The research's findings on cloud data center task allocation and performance management can assist academics, practitioners, and cloud service providers in optimizing their systems for dependability, cost-effectiveness, and scalability.","Innovative methodologies can steer future research to fill gaps in the literature."],"url":"http://arxiv.org/abs/2402.13135v1","category":"cs.DC"}
{"created":"2024-02-20 16:27:07","title":"Tactile Weight Rendering: A Review for Researchers and Developers","abstract":"Haptic rendering of weight plays an essential role in naturalistic object interaction in virtual environments. While kinesthetic devices have traditionally been used for this aim by applying forces on the limbs, tactile interfaces acting on the skin have recently offered potential solutions to enhance or substitute kinesthetic ones. Here, we aim to provide an in-depth overview and comparison of existing tactile weight rendering approaches. We categorized these approaches based on their type of stimulation into asymmetric vibration and skin stretch, further divided according to the working mechanism of the devices. Then, we compared these approaches using various criteria, including physical, mechanical, and perceptual characteristics of the reported devices and their potential applications. We found that asymmetric vibration devices have the smallest form factor, while skin stretch devices relying on the motion of flat surfaces, belts, or tactors present numerous mechanical and perceptual advantages for scenarios requiring more accurate weight rendering. Finally, we discussed the selection of the proposed categorization of devices and their application scopes, together with the limitations and opportunities for future research. We hope this study guides the development and use of tactile interfaces to achieve a more naturalistic object interaction and manipulation in virtual environments.","sentences":["Haptic rendering of weight plays an essential role in naturalistic object interaction in virtual environments.","While kinesthetic devices have traditionally been used for this aim by applying forces on the limbs, tactile interfaces acting on the skin have recently offered potential solutions to enhance or substitute kinesthetic ones.","Here, we aim to provide an in-depth overview and comparison of existing tactile weight rendering approaches.","We categorized these approaches based on their type of stimulation into asymmetric vibration and skin stretch, further divided according to the working mechanism of the devices.","Then, we compared these approaches using various criteria, including physical, mechanical, and perceptual characteristics of the reported devices and their potential applications.","We found that asymmetric vibration devices have the smallest form factor, while skin stretch devices relying on the motion of flat surfaces, belts, or tactors present numerous mechanical and perceptual advantages for scenarios requiring more accurate weight rendering.","Finally, we discussed the selection of the proposed categorization of devices and their application scopes, together with the limitations and opportunities for future research.","We hope this study guides the development and use of tactile interfaces to achieve a more naturalistic object interaction and manipulation in virtual environments."],"url":"http://arxiv.org/abs/2402.13120v1","category":"cs.HC"}
{"created":"2024-02-20 16:22:23","title":"Multistatic OFDM Radar Fusion of MUSIC-based Angle Estimation","abstract":"This study investigates the problem of angle-based localization of multiple targets using a multistatic OFDM radar. Although the maximum likelihood (ML) approach can be employed to merge data from different radar pairs, this method requires a high complexity multi-dimensional search process. The multiple signal classification (MUSIC) algorithm simplifies the complexity to a two-dimensional search, but no framework is derived for combining MUSIC pseudo-spectrums in a multistatic configuration. This paper exploits the relationship between MUSIC and ML estimators to approximate the multidimensional ML parameter estimation with a weighted combination of MUSIC pseudo-spectrum. This enables the computation of a likelihood map on which a peak selection is applied for target detection. In addition to reducing the computational complexity, the proposed method relies only on transmitting the estimated channel covariance matrices of each radar pair to the central processor. A numerical analysis is conducted to assess the benefits of the proposed fusion.","sentences":["This study investigates the problem of angle-based localization of multiple targets using a multistatic OFDM radar.","Although the maximum likelihood (ML) approach can be employed to merge data from different radar pairs, this method requires a high complexity multi-dimensional search process.","The multiple signal classification (MUSIC) algorithm simplifies the complexity to a two-dimensional search, but no framework is derived for combining MUSIC pseudo-spectrums in a multistatic configuration.","This paper exploits the relationship between MUSIC and ML estimators to approximate the multidimensional ML parameter estimation with a weighted combination of MUSIC pseudo-spectrum.","This enables the computation of a likelihood map on which a peak selection is applied for target detection.","In addition to reducing the computational complexity, the proposed method relies only on transmitting the estimated channel covariance matrices of each radar pair to the central processor.","A numerical analysis is conducted to assess the benefits of the proposed fusion."],"url":"http://arxiv.org/abs/2402.13118v1","category":"eess.SP"}
{"created":"2024-02-20 16:19:12","title":"Faster and Deterministic Subtrajectory Clustering","abstract":"Given a trajectory $T$ and a distance $\\Delta$, we wish to find a set $C$ of curves of complexity at most $\\ell$, such that we can cover $T$ with subcurves that each are within Fr\\'echet distance $\\Delta$ to at least one curve in $C$. We call $C$ an $(\\ell,\\Delta)$-clustering and aim to find an $(\\ell,\\Delta)$-clustering of minimum cardinality. This problem was introduced by Akitaya $et$ $al.$ (2021) and shown to be NP-complete. The main focus has therefore been on bicriterial approximation algorithms, allowing for the clustering to be an $(\\ell, \\Theta(\\Delta))$-clustering of roughly optimal size. We present algorithms that construct $(\\ell,4\\Delta)$-clusterings of $\\mathcal{O}(k \\log n)$ size, where $k$ is the size of the optimal $(\\ell, \\Delta)$-clustering. For the discrete Fr\\'echet distance, we use $\\mathcal{O}(n \\ell \\log n)$ space and $\\mathcal{O}(k n^2 \\log^3 n)$ deterministic worst case time. For the continuous Fr\\'echet distance, we use $\\mathcal{O}(n^2 \\log n)$ space and $\\mathcal{O}(k n^3 \\log^3 n)$ time. Our algorithms significantly improve upon the clustering quality (improving the approximation factor in $\\Delta$) and size (whenever $\\ell \\in \\Omega(\\log n)$). We offer deterministic running times comparable to known expected bounds. Additionally, in the continuous setting, we give a near-linear improvement upon the space usage. When compared only to deterministic results, we offer a near-linear speedup and a near-quadratic improvement in the space usage. When we may restrict ourselves to only considering clusters where all subtrajectories are vertex-to-vertex subcurves, we obtain even better results under the continuous Fr\\'echet distance. Our algorithm becomes near quadratic and uses space that is near linear in $n \\ell$.","sentences":["Given a trajectory $T$ and a distance $\\Delta$, we wish to find a set $C$ of curves of complexity at most $\\ell$, such that we can cover $T$ with subcurves that each are within Fr\\'echet distance $\\Delta$ to at least one curve in $C$.","We call $C$ an $(\\ell,\\Delta)$-clustering and aim to find an $(\\ell,\\Delta)$-clustering of minimum cardinality.","This problem was introduced by Akitaya $et$ $al.$ (2021) and shown to be NP-complete.","The main focus has therefore been on bicriterial approximation algorithms, allowing for the clustering to be an $(\\ell, \\Theta(\\Delta))$-clustering of roughly optimal size.","We present algorithms that construct $(\\ell,4\\Delta)$-clusterings of $\\mathcal{O}(k \\log n)$ size, where $k$ is the size of the optimal $(\\ell, \\Delta)$-clustering.","For the discrete Fr\\'echet distance, we use $\\mathcal{O}(n \\ell \\log n)$ space and $\\mathcal{O}(k n^2 \\log^3 n)$ deterministic worst case time.","For the continuous Fr\\'echet distance, we use $\\mathcal{O}(n^2 \\log n)$ space and $\\mathcal{O}(k n^3 \\log^3 n)$ time.","Our algorithms significantly improve upon the clustering quality (improving the approximation factor in $\\Delta$) and size (whenever $\\ell \\in \\Omega(\\log n)$).","We offer deterministic running times comparable to known expected bounds.","Additionally, in the continuous setting, we give a near-linear improvement upon the space usage.","When compared only to deterministic results, we offer a near-linear speedup and a near-quadratic improvement in the space usage.","When we may restrict ourselves to only considering clusters where all subtrajectories are vertex-to-vertex subcurves, we obtain even better results under the continuous Fr\\'echet distance.","Our algorithm becomes near quadratic and uses space that is near linear in $n \\ell$."],"url":"http://arxiv.org/abs/2402.13117v1","category":"cs.CG"}
{"created":"2024-02-20 16:09:29","title":"Observation of time crystal comb in a driven-dissipative system","abstract":"We report the observation of a time crystal comb in the continuously driven-dissipative and strongly interacting Rydberg thermal gas, in which continuous time crystal and discrete time crystal as well as the higher-order harmonic oscillation phases are observed in the same system by tuning the Rydberg excitation Rabi frequency. Remarkably, our work establishes the fundamental relation of time crystalline and driven-dissipative system, and provides new ways to explore the nonequilibrium phases of matter in open systems. Such time crystals with persistent oscillation rooted in emergent quantum correlations, may emerge as a ubiquitous tool in quantum metrology, for instance, continuous sensing and parameter estimation surpassing the standard quantum limit.","sentences":["We report the observation of a time crystal comb in the continuously driven-dissipative and strongly interacting Rydberg thermal gas, in which continuous time crystal and discrete time crystal as well as the higher-order harmonic oscillation phases are observed in the same system by tuning the Rydberg excitation Rabi frequency.","Remarkably, our work establishes the fundamental relation of time crystalline and driven-dissipative system, and provides new ways to explore the nonequilibrium phases of matter in open systems.","Such time crystals with persistent oscillation rooted in emergent quantum correlations, may emerge as a ubiquitous tool in quantum metrology, for instance, continuous sensing and parameter estimation surpassing the standard quantum limit."],"url":"http://arxiv.org/abs/2402.13112v1","category":"physics.atom-ph"}
{"created":"2024-02-20 16:03:02","title":"HiRIS: an Airborne Sonar Sensor with a 1024 Channel Microphone Array for In-Air Acoustic Imaging","abstract":"Airborne 3D imaging using ultrasound is a promising sensing modality for robotic applications in harsh environments. Over the last decade, several high-performance systems have been proposed in the literature. Most of these sensors use a reduced aperture microphone array, leading to artifacts in the resulting acoustic images. This paper presents a novel in-air ultrasound sensor that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array, in combination with a distributed embedded hardware design to perform the data acquisition. Using a broadband Minimum Variance Distortionless Response (MVDR) beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able to create both 2D and 3D ultrasound images of the full-frontal hemisphere with high angular accuracy with up to 70dB main lobe to side lobe ratio. This paper describes both the hardware infrastructure needed to obtain such highly detailed acoustical images, as well as the signal processing chain needed to convert the raw acoustic data into said images. Utilizing this novel high-resolution ultrasound imaging sensor, we wish to investigate the limits of both passive and active airborne ultrasound sensing by utilizing this virtually artifact-free imaging modality.","sentences":["Airborne 3D imaging using ultrasound is a promising sensing modality for robotic applications in harsh environments.","Over the last decade, several high-performance systems have been proposed in the literature.","Most of these sensors use a reduced aperture microphone array, leading to artifacts in the resulting acoustic images.","This paper presents a novel in-air ultrasound sensor that incorporates 1024 microphones, in a 32-by- 32 uniform rectangular array, in combination with a distributed embedded hardware design to perform the data acquisition.","Using a broadband Minimum Variance Distortionless Response (MVDR) beamformer with Forward-Backward Spatial Smoothing (FB-SS), the sensor is able to create both 2D and 3D ultrasound images of the full-frontal hemisphere with high angular accuracy with up to 70dB main lobe to side lobe ratio.","This paper describes both the hardware infrastructure needed to obtain such highly detailed acoustical images, as well as the signal processing chain needed to convert the raw acoustic data into said images.","Utilizing this novel high-resolution ultrasound imaging sensor, we wish to investigate the limits of both passive and active airborne ultrasound sensing by utilizing this virtually artifact-free imaging modality."],"url":"http://arxiv.org/abs/2402.13110v1","category":"eess.SP"}
{"created":"2024-02-20 15:59:56","title":"Self-Perception Versus Objective Driving Behavior: Subject Study of Lateral Vehicle Guidance","abstract":"Advancements in technology are steering attention toward creating comfortable and acceptable driving characteristics in autonomous vehicles. Ensuring a safe and comfortable ride experience is vital for the widespread adoption of autonomous vehicles, as mismatches in driving styles between humans and autonomous systems can impact passenger confidence. Current driving functions have fixed parameters, and there is no universally agreed-upon driving style for autonomous vehicles. Integrating driving style preferences into automated vehicles may enhance acceptance and reduce uncertainty, expediting their adoption. A controlled vehicle study (N = 62) was conducted with a variety of German participants to identify the individual lateral driving behavior of human drivers, specifically emphasizing rural roads. We introduce novel indicators for assessing stationary and transient curve negotiation, directly applicable in developing personalized lateral driving functions. To assess the predictability of these indicators using self-reports, we introduce the MDSI-DE, the German version of the Multidimensional Driving Style Inventory. The correlation analysis between MDSI factor scores and proposed indicators showed modest but significant associations, primarily with acceleration and jerk statistics while the in-depth lateral driving behavior turned out to be highly driver-heterogeneous. The dataset including the anonymized socio-demographics and questionnaire responses, the raw vehicle measurements including labels, and the derived driving behavior indicators are publicly available at https://www.kaggle.com/datasets/jhaselberger/spodb-subject-study-oflateral-vehicle-guidance.","sentences":["Advancements in technology are steering attention toward creating comfortable and acceptable driving characteristics in autonomous vehicles.","Ensuring a safe and comfortable ride experience is vital for the widespread adoption of autonomous vehicles, as mismatches in driving styles between humans and autonomous systems can impact passenger confidence.","Current driving functions have fixed parameters, and there is no universally agreed-upon driving style for autonomous vehicles.","Integrating driving style preferences into automated vehicles may enhance acceptance and reduce uncertainty, expediting their adoption.","A controlled vehicle study (N = 62) was conducted with a variety of German participants to identify the individual lateral driving behavior of human drivers, specifically emphasizing rural roads.","We introduce novel indicators for assessing stationary and transient curve negotiation, directly applicable in developing personalized lateral driving functions.","To assess the predictability of these indicators using self-reports, we introduce the MDSI-DE, the German version of the Multidimensional Driving Style Inventory.","The correlation analysis between MDSI factor scores and proposed indicators showed modest but significant associations, primarily with acceleration and jerk statistics while the in-depth lateral driving behavior turned out to be highly driver-heterogeneous.","The dataset including the anonymized socio-demographics and questionnaire responses, the raw vehicle measurements including labels, and the derived driving behavior indicators are publicly available at https://www.kaggle.com/datasets/jhaselberger/spodb-subject-study-oflateral-vehicle-guidance."],"url":"http://arxiv.org/abs/2402.13104v1","category":"eess.SY"}
{"created":"2024-02-20 15:51:50","title":"An Introduction to Causal Inference Methods with Multi-omics Data","abstract":"Omics biomarkers play a pivotal role in personalized medicine by providing molecular-level insights into the etiology of diseases, guiding precise diagnostics, and facilitating targeted therapeutic interventions. Recent advancements in omics technologies have resulted in an increasing abundance of multimodal omics data, providing unprecedented opportunities for identifying novel omics biomarkers for human diseases. Mendelian randomization (MR) is a practically useful causal inference method that uses genetic variants as instrumental variables (IVs) to infer causal relationships between omics biomarkers and complex traits/diseases by removing hidden confounding bias. In this article, we first present current challenges in performing MR analysis with omics data, and then describe four MR methods for analyzing multi-omics data including epigenomics, transcriptomics, proteomics, and metabolomics data, all executable within the R software environment.","sentences":["Omics biomarkers play a pivotal role in personalized medicine by providing molecular-level insights into the etiology of diseases, guiding precise diagnostics, and facilitating targeted therapeutic interventions.","Recent advancements in omics technologies have resulted in an increasing abundance of multimodal omics data, providing unprecedented opportunities for identifying novel omics biomarkers for human diseases.","Mendelian randomization (MR) is a practically useful causal inference method that uses genetic variants as instrumental variables (IVs) to infer causal relationships between omics biomarkers and complex traits/diseases by removing hidden confounding bias.","In this article, we first present current challenges in performing MR analysis with omics data, and then describe four MR methods for analyzing multi-omics data including epigenomics, transcriptomics, proteomics, and metabolomics data, all executable within the R software environment."],"url":"http://arxiv.org/abs/2402.13100v1","category":"stat.AP"}
{"created":"2024-02-20 15:39:50","title":"Coherent Detection of Discrete Variable Quantum Key Distribution using Homodyne Technique","abstract":"In Discrete Variable Quantum Key Distribution (DV-QKD), homodyne detection method is frequently employed for its simplicity in use, effectiveness in terms of error correction, and suitability with contemporary optical communication systems. Being a coherent detection method, it relies on a local oscillator whose frequency is matched to that of the transmitted carrier's signal. In this paper we evaluate a Free Space Optical (FSO) DV-QKD system based on the KMB09 protocol using Homodyne detection under random phase fluctuation and depolarizing noise error. We present simulation results for System Efficiency and Quantum Bit Error Rate (QBER) for the proposed model. An obtained efficiency (approximately 25%) for our proposed DV-QKD system model shows that under atmospheric turbulence and noise effect, it is inline with the available analytical results. However, the inclusion of random phase fluctuation and noise led to higher-than-normal QBER which is anticipated in a real-world scenario","sentences":["In Discrete Variable Quantum Key Distribution (DV-QKD), homodyne detection method is frequently employed for its simplicity in use, effectiveness in terms of error correction, and suitability with contemporary optical communication systems.","Being a coherent detection method, it relies on a local oscillator whose frequency is matched to that of the transmitted carrier's signal.","In this paper we evaluate a Free Space Optical (FSO) DV-QKD system based on the KMB09 protocol using Homodyne detection under random phase fluctuation and depolarizing noise error.","We present simulation results for System Efficiency and Quantum Bit Error Rate (QBER) for the proposed model.","An obtained efficiency (approximately 25%) for our proposed DV-QKD system model shows that under atmospheric turbulence and noise effect, it is inline with the available analytical results.","However, the inclusion of random phase fluctuation and noise led to higher-than-normal QBER which is anticipated in a real-world scenario"],"url":"http://arxiv.org/abs/2402.13095v1","category":"quant-ph"}
{"created":"2024-02-20 15:37:08","title":"Digital Comprehensibility Assessment of Simplified Texts among Persons with Intellectual Disabilities","abstract":"Text simplification refers to the process of increasing the comprehensibility of texts. Automatic text simplification models are most commonly evaluated by experts or crowdworkers instead of the primary target groups of simplified texts, such as persons with intellectual disabilities. We conducted an evaluation study of text comprehensibility including participants with and without intellectual disabilities reading unsimplified, automatically and manually simplified German texts on a tablet computer. We explored four different approaches to measuring comprehensibility: multiple-choice comprehension questions, perceived difficulty ratings, response time, and reading speed. The results revealed significant variations in these measurements, depending on the reader group and whether the text had undergone automatic or manual simplification. For the target group of persons with intellectual disabilities, comprehension questions emerged as the most reliable measure, while analyzing reading speed provided valuable insights into participants' reading behavior.","sentences":["Text simplification refers to the process of increasing the comprehensibility of texts.","Automatic text simplification models are most commonly evaluated by experts or crowdworkers instead of the primary target groups of simplified texts, such as persons with intellectual disabilities.","We conducted an evaluation study of text comprehensibility including participants with and without intellectual disabilities reading unsimplified, automatically and manually simplified German texts on a tablet computer.","We explored four different approaches to measuring comprehensibility: multiple-choice comprehension questions, perceived difficulty ratings, response time, and reading speed.","The results revealed significant variations in these measurements, depending on the reader group and whether the text had undergone automatic or manual simplification.","For the target group of persons with intellectual disabilities, comprehension questions emerged as the most reliable measure, while analyzing reading speed provided valuable insights into participants' reading behavior."],"url":"http://arxiv.org/abs/2402.13094v1","category":"cs.CL"}
{"created":"2024-02-20 15:33:48","title":"Fast and memory-efficient optimization for large-scale data-driven predictive control","abstract":"Recently, data-enabled predictive control (DeePC) schemes based on Willems' fundamental lemma have attracted considerable attention. At the core are computations using Hankel-like matrices and their connection to the concept of persistency of excitation. We propose an iterative solver for the underlying data-driven optimal control problems resulting from linear discrete-time systems. To this end, we apply factorizations based on the discrete Fourier transform of the Hankel-like matrices, which enable fast and memory-efficient computations. To take advantage of this factorization in an optimal control solver and to reduce the effect of inherent bad conditioning of the Hankel-like matrices, we propose an augmented Lagrangian lBFGS-method. We illustrate the performance of our method by means of a numerical study.","sentences":["Recently, data-enabled predictive control (DeePC) schemes based on Willems' fundamental lemma have attracted considerable attention.","At the core are computations using Hankel-like matrices and their connection to the concept of persistency of excitation.","We propose an iterative solver for the underlying data-driven optimal control problems resulting from linear discrete-time systems.","To this end, we apply factorizations based on the discrete Fourier transform of the Hankel-like matrices, which enable fast and memory-efficient computations.","To take advantage of this factorization in an optimal control solver and to reduce the effect of inherent bad conditioning of the Hankel-like matrices, we propose an augmented Lagrangian lBFGS-method.","We illustrate the performance of our method by means of a numerical study."],"url":"http://arxiv.org/abs/2402.13090v1","category":"math.OC"}
{"created":"2024-02-20 15:21:30","title":"Formal Synthesis of Controllers for Safety-Critical Autonomous Systems: Developments and Challenges","abstract":"In recent years, formal methods have been extensively used in the design of autonomous systems. By employing mathematically rigorous techniques, formal methods can provide fully automated reasoning processes with provable safety guarantees for complex dynamic systems with intricate interactions between continuous dynamics and discrete logics. This paper provides a comprehensive review of formal controller synthesis techniques for safety-critical autonomous systems. Specifically, we categorize the formal control synthesis problem based on diverse system models, encompassing deterministic, non-deterministic, and stochastic, and various formal safety-critical specifications involving logic, real-time, and real-valued domains. The review covers fundamental formal control synthesis techniques, including abstraction-based approaches and abstraction-free methods. We explore the integration of data-driven synthesis approaches in formal control synthesis. Furthermore, we review formal techniques tailored for multi-agent systems (MAS), with a specific focus on various approaches to address the scalability challenges in large-scale systems. Finally, we discuss some recent trends and highlight research challenges in this area.","sentences":["In recent years, formal methods have been extensively used in the design of autonomous systems.","By employing mathematically rigorous techniques, formal methods can provide fully automated reasoning processes with provable safety guarantees for complex dynamic systems with intricate interactions between continuous dynamics and discrete logics.","This paper provides a comprehensive review of formal controller synthesis techniques for safety-critical autonomous systems.","Specifically, we categorize the formal control synthesis problem based on diverse system models, encompassing deterministic, non-deterministic, and stochastic, and various formal safety-critical specifications involving logic, real-time, and real-valued domains.","The review covers fundamental formal control synthesis techniques, including abstraction-based approaches and abstraction-free methods.","We explore the integration of data-driven synthesis approaches in formal control synthesis.","Furthermore, we review formal techniques tailored for multi-agent systems (MAS), with a specific focus on various approaches to address the scalability challenges in large-scale systems.","Finally, we discuss some recent trends and highlight research challenges in this area."],"url":"http://arxiv.org/abs/2402.13075v1","category":"eess.SY"}
{"created":"2024-02-20 15:10:02","title":"Radiation back-reaction during dark-matter freeze-out via metastable bound states","abstract":"The formation and decay of metastable bound states can deplete significantly the density of multi-TeV thermal-relic dark matter. The effect depends on the interplay of bound-state formation, ionisation, transition and decay processes. Existing calculations take into account bound-state ionisation and excitations due to the radiation of the thermal bath. However, the dynamics of Hydrogen recombination suggests that the resonant radiation produced in bound-state formation or de-excitations may backreact, ionising or exciting the bound states thus impeding recombination. In this paper we examine this effect in the context of dark-matter freeze-out. To this end, we employ the generalised Saha equilibrium equation for metastable bound states, and discuss its salient features. We show that, in sharp contrast to Hydrogen recombination, the radiation produced during dark matter freeze-out is more likely to thermalise or redshift, rather than ionise or excite the metastable bound states. This holds not only for the low-energy (resonant) radiation produced in bound-state formation and transition processes, but also for the high-energy radiation produced in dark-matter annihilations and bound-state decays. While our computations are carried out in a minimal dark $U(1)$ model, our conclusions only strengthen in more complex models.","sentences":["The formation and decay of metastable bound states can deplete significantly the density of multi-TeV thermal-relic dark matter.","The effect depends on the interplay of bound-state formation, ionisation, transition and decay processes.","Existing calculations take into account bound-state ionisation and excitations due to the radiation of the thermal bath.","However, the dynamics of Hydrogen recombination suggests that the resonant radiation produced in bound-state formation or de-excitations may backreact, ionising or exciting the bound states thus impeding recombination.","In this paper we examine this effect in the context of dark-matter freeze-out.","To this end, we employ the generalised Saha equilibrium equation for metastable bound states, and discuss its salient features.","We show that, in sharp contrast to Hydrogen recombination, the radiation produced during dark matter freeze-out is more likely to thermalise or redshift, rather than ionise or excite the metastable bound states.","This holds not only for the low-energy (resonant) radiation produced in bound-state formation and transition processes, but also for the high-energy radiation produced in dark-matter annihilations and bound-state decays.","While our computations are carried out in a minimal dark $U(1)$ model, our conclusions only strengthen in more complex models."],"url":"http://arxiv.org/abs/2402.13069v1","category":"hep-ph"}
{"created":"2024-02-20 15:08:19","title":"Turbulent boundary layer response to uniform changes of the pressure force contribution","abstract":"We investigate a turbulent boundary layer (TBL) with uniform pressure force variations, focusing on understanding its response to local pressure force, local pressure force variation (local disequilibrating effect), and upstream history. The studied flow starts as a zero-pressure-gradient (ZPG) TBL, followed by a uniform increase in the ratio of pressure force to turbulent force in the outer region and concludes with a uniform decrease of the same magnitude. The second zone includes a subzone with a diminishing adverse-pressure-gradient (APG), followed by an increasing favorable-pressure-gradient (FPG). In both subzones, the impact remains the same, mean momentum gain in the boundary layer and reduced turbulence. In the outer region, the mean flow responds to force balance changes with a considerable delay. The accumulated flow history effects lead to a FPG TBL at the domain's end with a momentum defect comparable to APG TBLs. Below $y^+=10$, the mean flow responds almost instantaneously to pressure force changes. In the overlap layer, the profiles deviate from the conventional logarithmic law of the ZPG TBL. Regarding the outer-layer turbulence, its subsequent decay is slower than its initial increase, the latter persisting even after the pressure force begins to decrease. As a result of the slow turbulence decay, the FPG TBL at the domain's end exhibits unusually high outer turbulence levels. Near the wall, turbulence responds with a delay to changes in the pressure force partly due to large-scale turbulence influence. This study underscores the complexity of the triple action of the pressure force.","sentences":["We investigate a turbulent boundary layer (TBL) with uniform pressure force variations, focusing on understanding its response to local pressure force, local pressure force variation (local disequilibrating effect), and upstream history.","The studied flow starts as a zero-pressure-gradient (ZPG) TBL, followed by a uniform increase in the ratio of pressure force to turbulent force in the outer region and concludes with a uniform decrease of the same magnitude.","The second zone includes a subzone with a diminishing adverse-pressure-gradient (APG), followed by an increasing favorable-pressure-gradient (FPG).","In both subzones, the impact remains the same, mean momentum gain in the boundary layer and reduced turbulence.","In the outer region, the mean flow responds to force balance changes with a considerable delay.","The accumulated flow history effects lead to a FPG TBL at the domain's end with a momentum defect comparable to APG TBLs.","Below $y^+=10$, the mean flow responds almost instantaneously to pressure force changes.","In the overlap layer, the profiles deviate from the conventional logarithmic law of the ZPG TBL.","Regarding the outer-layer turbulence, its subsequent decay is slower than its initial increase, the latter persisting even after the pressure force begins to decrease.","As a result of the slow turbulence decay, the FPG TBL at the domain's end exhibits unusually high outer turbulence levels.","Near the wall, turbulence responds with a delay to changes in the pressure force partly due to large-scale turbulence influence.","This study underscores the complexity of the triple action of the pressure force."],"url":"http://arxiv.org/abs/2402.13067v1","category":"physics.flu-dyn"}
{"created":"2024-02-20 15:02:24","title":"Scalable Pattern Matching in Computation Graphs","abstract":"Graph rewriting is a popular tool for the optimisation and modification of graph expressions in domains such as compilers, machine learning and quantum computing. The underlying data structures are often port graphs - graphs with labels at edge endpoints. These port labels greatly simplify pattern matching.   A pre-requisite for graph rewriting is the ability to find subgraphs of the input that match known graph identities: the pattern matching problem. We propose a new solution to pattern matching in port graphs. Its novelty lies in the use of a pre-computed data structure that makes the pattern matching runtime complexity independent of the number of patterns. The runtime is bound by the maximum width $w$ and depth $d$ of the patterns, as well as the input graph size $|G|$ as $O(|G| \\cdot c^w / w^{1/2} \\cdot d)$ with $c = 6.75$. This offers a significant advantage over existing solutions for use cases where patterns have low width and the set of patterns is large and fixed ahead of time.   In the context of quantum circuits, pattern width can be limited to qubit number. Quantum superoptimisers may use thousands of rewrite rules on circuits with less than 5 qubits, making them an ideal use case. We provide benchmarks showing that our algorithm offers a 20x speedup over current implementations on a dataset of 10'000 real world patterns describing quantum circuits.","sentences":["Graph rewriting is a popular tool for the optimisation and modification of graph expressions in domains such as compilers, machine learning and quantum computing.","The underlying data structures are often port graphs - graphs with labels at edge endpoints.","These port labels greatly simplify pattern matching.   ","A pre-requisite for graph rewriting is the ability to find subgraphs of the input that match known graph identities: the pattern matching problem.","We propose a new solution to pattern matching in port graphs.","Its novelty lies in the use of a pre-computed data structure that makes the pattern matching runtime complexity independent of the number of patterns.","The runtime is bound by the maximum width $w$ and depth $d$ of the patterns, as well as the input graph size $|G|$ as $O(|G| \\cdot c^w / w^{1/2} \\cdot d)$ with $c =","6.75$. This offers a significant advantage over existing solutions for use cases where patterns have low width and the set of patterns is large and fixed ahead of time.   ","In the context of quantum circuits, pattern width can be limited to qubit number.","Quantum superoptimisers may use thousands of rewrite rules on circuits with less than 5 qubits, making them an ideal use case.","We provide benchmarks showing that our algorithm offers a 20x speedup over current implementations on a dataset of 10'000 real world patterns describing quantum circuits."],"url":"http://arxiv.org/abs/2402.13065v1","category":"cs.DS"}
{"created":"2024-02-20 14:58:15","title":"Energy exchanges between coherent modes in the near wake of a rotor model at different tip speed ratios","abstract":"In this work we investigate the spatio-temporal nature of various coherent modes present in a rotor wake using a combination of new PIV experiments and data from Biswas and Buxton (2024). A multi-scale triple decomposition of the acquired velocity field is sought to extract the coherent modes and thereafter, the energy exchanges to and from them are studied using the multi-scale triple-decomposed coherent kinetic energy budgets developed by Baj and Buxton (2017). Different frequencies forming the tip vortex system (such as the blade passing frequency, turbine's rotational frequency and their harmonics) are found to be energised by different sources such as production from the mean flow or non-linear triadic interaction or both, similar to the primary, secondary or the mixed modes discussed in Biswas et al. (2022). In fact, the tip vortex system forms a complex network of nonlinear triadic energy transfers, the nature and the magnitudes of which depend on \\lambda. On the other hand, the modes associated with the sheddings from the nacelle or tower and wake meandering are found to be primarily energised by the mean flow. We show that the tip vortex system exchanges energy with the mean flow primarily through the turbine's rotational frequency. In fact, the system transfers energy back to the mean flow through the turbine's rotational frequency at some distance downstream marking the onset location of wake recovery (x_{wr}). x_{wr} is shown to reduce with \\lambda due to stronger interaction and earlier merging of the tip vortices at a higher \\lambda.","sentences":["In this work we investigate the spatio-temporal nature of various coherent modes present in a rotor wake using a combination of new PIV experiments and data from Biswas and Buxton (2024).","A multi-scale triple decomposition of the acquired velocity field is sought to extract the coherent modes and thereafter, the energy exchanges to and from them are studied using the multi-scale triple-decomposed coherent kinetic energy budgets developed by Baj and Buxton (2017).","Different frequencies forming the tip vortex system (such as the blade passing frequency, turbine's rotational frequency and their harmonics) are found to be energised by different sources such as production from the mean flow or non-linear triadic interaction or both, similar to the primary, secondary or the mixed modes discussed in Biswas et al. (2022).","In fact, the tip vortex system forms a complex network of nonlinear triadic energy transfers, the nature and the magnitudes of which depend on \\lambda.","On the other hand, the modes associated with the sheddings from the nacelle or tower and wake meandering are found to be primarily energised by the mean flow.","We show that the tip vortex system exchanges energy with the mean flow primarily through the turbine's rotational frequency.","In fact, the system transfers energy back to the mean flow through the turbine's rotational frequency at some distance downstream marking the onset location of wake recovery (x_{wr}).","x_{wr} is shown to reduce with \\lambda due to stronger interaction and earlier merging of the tip vortices at a higher \\lambda."],"url":"http://arxiv.org/abs/2402.13063v1","category":"physics.flu-dyn"}
{"created":"2024-02-20 14:54:49","title":"How accurate are simulations and experiments for the lattice energies of molecular crystals?","abstract":"Molecular crystals play a central role in a wide range of scientific fields, including pharmaceuticals and organic semiconductor devices. However, they are challenging systems to model accurately with computational approaches because of a delicate interplay of intermolecular interactions such as hydrogen bonding and van der Waals dispersion forces. Here, by exploiting recent algorithmic developments, we report the first set of diffusion Monte Carlo lattice energies for all 23 molecular crystals in the popular and widely used X23 dataset. Comparisons with previous state-of-the-art lattice energy predictions (on a subset of the dataset) and a careful analysis of experimental sublimation enthalpies reveals that high-accuracy computational methods are now at least as reliable as (computationally derived) experiments for the lattice energies of molecular crystals. Overall, this work demonstrates the feasibility of high-level explicitly correlated electronic structure methods for broad benchmarking studies in complex condensed phase systems, and signposts a route towards closer agreement between experiment and simulation.","sentences":["Molecular crystals play a central role in a wide range of scientific fields, including pharmaceuticals and organic semiconductor devices.","However, they are challenging systems to model accurately with computational approaches because of a delicate interplay of intermolecular interactions such as hydrogen bonding and van der Waals dispersion forces.","Here, by exploiting recent algorithmic developments, we report the first set of diffusion Monte Carlo lattice energies for all 23 molecular crystals in the popular and widely used X23 dataset.","Comparisons with previous state-of-the-art lattice energy predictions (on a subset of the dataset) and a careful analysis of experimental sublimation enthalpies reveals that high-accuracy computational methods are now at least as reliable as (computationally derived) experiments for the lattice energies of molecular crystals.","Overall, this work demonstrates the feasibility of high-level explicitly correlated electronic structure methods for broad benchmarking studies in complex condensed phase systems, and signposts a route towards closer agreement between experiment and simulation."],"url":"http://arxiv.org/abs/2402.13059v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 14:52:00","title":"Non-interferometric rotational test of the Continuous Spontaneous Localisation model: enhancement of the collapse noise through shape optimisation","abstract":"The Continuous Spontaneous Localisation (CSL) model is the most studied among collapse models, which describes the breakdown of the superposition principle for macroscopic systems. Here, we derive an upper bound on the parameters of the model by applying it to the rotational noise measured in a recent short-distance gravity experiment [Lee et al., Phys. Rev. Lett. 124, 101101 (2020)]. Specifically, considering the noise affecting the rotational motion, we found that despite being a table-top experiment the bound is only one order of magnitude weaker than that from LIGO for the relevant values of the collapse parameter. Further, we analyse possible ways to optimise the shape of the test mass to enhance the collapse noise by several orders of magnitude and eventually derive stronger bounds that can address the unexplored region of the CSL parameters space.","sentences":["The Continuous Spontaneous Localisation (CSL) model is the most studied among collapse models, which describes the breakdown of the superposition principle for macroscopic systems.","Here, we derive an upper bound on the parameters of the model by applying it to the rotational noise measured in a recent short-distance gravity experiment [Lee et al., Phys.","Rev. Lett.","124, 101101 (2020)].","Specifically, considering the noise affecting the rotational motion, we found that despite being a table-top experiment the bound is only one order of magnitude weaker than that from LIGO for the relevant values of the collapse parameter.","Further, we analyse possible ways to optimise the shape of the test mass to enhance the collapse noise by several orders of magnitude and eventually derive stronger bounds that can address the unexplored region of the CSL parameters space."],"url":"http://arxiv.org/abs/2402.13057v1","category":"quant-ph"}
{"created":"2024-02-20 14:37:10","title":"Symmetric Power L-functions of the hyper-Kloosterman Family","abstract":"The symmetric power L-function of the hyper-Kloosterman family is a rational function over the integers. Its degree and complex absolute values of its zeros and poles are now known through the work of Fu and Wan. The purpose of this paper is to study the p-adic absolute value of these zeros and poles. In particular, we give a uniform lower bound, independent of the symmetric power, of the q-adic Newton polygon of this $L$-function under suitable conditions. We also give similar results for any other linear algebra operation of the hyper-Kloosterman family, such as tensor, exterior, symmetric powers, or combinations thereof.","sentences":["The symmetric power L-function of the hyper-Kloosterman family is a rational function over the integers.","Its degree and complex absolute values of its zeros and poles are now known through the work of Fu and Wan.","The purpose of this paper is to study the p-adic absolute value of these zeros and poles.","In particular, we give a uniform lower bound, independent of the symmetric power, of the q-adic Newton polygon of this $L$-function under suitable conditions.","We also give similar results for any other linear algebra operation of the hyper-Kloosterman family, such as tensor, exterior, symmetric powers, or combinations thereof."],"url":"http://arxiv.org/abs/2402.13051v1","category":"math.NT"}
{"created":"2024-02-20 14:36:59","title":"Two Quantum Paradigms, but Still No Signal","abstract":"An overwhelming majority of quantum (pure and mixed) states, when undertaking a POVM measurement, will result in a classical probability with no algorithmic information. Thus most quantum states produce white noise when measured. Furthermore most non-pointer states, when undergoing the decoherence process, will produce white noise. These results can be seen as consequences of the vastness of Hilbert spaces.","sentences":["An overwhelming majority of quantum (pure and mixed) states, when undertaking a POVM measurement, will result in a classical probability with no algorithmic information.","Thus most quantum states produce white noise when measured.","Furthermore most non-pointer states, when undergoing the decoherence process, will produce white noise.","These results can be seen as consequences of the vastness of Hilbert spaces."],"url":"http://arxiv.org/abs/2402.13049v1","category":"cs.CC"}
{"created":"2024-02-20 14:35:07","title":"Excited state-specific CASSCF theory for the torsion of ethylene","abstract":"State-specific complete active space self-consistent field (SS-CASSCF) theory has emerged as a promising route to accurately predict electronically excited energy surfaces away from molecular equilibria. However, its accuracy and practicality for chemical systems of photochemical interest has yet to be fully determined. We investigate the performance of SS-CASSCF theory for the low-lying ground and excited states in the double bond rotation of ethylene. We show that state-specific approximations with a minimal (2e, 2o) active space provide comparable accuracy to state-averaged calculations with much larger active spaces, while optimising the orbitals for each excited state significantly improves the spatial diffusivity of the wave function. However, the unbalanced post-CASSCF dynamic correlation in valence and Rydberg excitations, or the use of a non-diffuse basis set, causes excited state solutions to coalesce and disappear, creating unphysical discontinuities in the potential energy surface. Our findings highlight the theoretical challenges that must be overcome to realise practical applications of state-specific electronic structure theory for computational photochemistry.","sentences":["State-specific complete active space self-consistent field (SS-CASSCF) theory has emerged as a promising route to accurately predict electronically excited energy surfaces away from molecular equilibria.","However, its accuracy and practicality for chemical systems of photochemical interest has yet to be fully determined.","We investigate the performance of SS-CASSCF theory for the low-lying ground and excited states in the double bond rotation of ethylene.","We show that state-specific approximations with a minimal (2e, 2o) active space provide comparable accuracy to state-averaged calculations with much larger active spaces, while optimising the orbitals for each excited state significantly improves the spatial diffusivity of the wave function.","However, the unbalanced post-CASSCF dynamic correlation in valence and Rydberg excitations, or the use of a non-diffuse basis set, causes excited state solutions to coalesce and disappear, creating unphysical discontinuities in the potential energy surface.","Our findings highlight the theoretical challenges that must be overcome to realise practical applications of state-specific electronic structure theory for computational photochemistry."],"url":"http://arxiv.org/abs/2402.13046v1","category":"physics.chem-ph"}
{"created":"2024-02-20 14:27:20","title":"The Santa Barabara Binary-Disk Code Comparison","abstract":"We have performed numerical calculations of a binary interacting with a gas disk, using eleven different numerical methods and a standard binary-disk setup. The goal of this study is to determine whether all codes agree on a numerically converged solution, and to determine the necessary resolution for convergence and the number of binary orbits that must be computed to reach an agreed-upon relaxed state of the binary-disk system. We find that all codes can agree on a converged solution (depending on the diagnostic being measured). The zone spacing required for most codes to reach a converged measurement of the torques applied to the binary by the disk is roughly 1% of the binary separation in the vicinity of the binary components. For our disk model to reach a relaxed state, codes must be run for at least 200 binary orbits, corresponding to about a viscous time for our parameters, $0.2 (a^2 \\Omega_B /\\nu)$ binary orbits, where $\\nu$ is the kinematic viscosity. We did not investigate dependence on binary mass ratio, eccentricity, disk temperature, or disk viscosity; therefore, these benchmarks may act as guides towards expanding converged solutions to the wider parameter space but might need to be updated in a future study that investigates dependence on system parameters. We find the most major discrepancies between codes resulted from the dimensionality of the setup (3D vs 2D disks). Beyond this, we find good agreement in the total torque on the binary between codes, although the partition of this torque between the gravitational torque, orbital accretion torque, and spin accretion torque depends sensitively on the sink prescriptions employed. In agreement with previous studies, we find a modest difference in torques and accretion variability between 2D and 3D disk models. We find cavity precession rates to be appreciably faster in 3D than in 2D.","sentences":["We have performed numerical calculations of a binary interacting with a gas disk, using eleven different numerical methods and a standard binary-disk setup.","The goal of this study is to determine whether all codes agree on a numerically converged solution, and to determine the necessary resolution for convergence and the number of binary orbits that must be computed to reach an agreed-upon relaxed state of the binary-disk system.","We find that all codes can agree on a converged solution (depending on the diagnostic being measured).","The zone spacing required for most codes to reach a converged measurement of the torques applied to the binary by the disk is roughly 1% of the binary separation in the vicinity of the binary components.","For our disk model to reach a relaxed state, codes must be run for at least 200 binary orbits, corresponding to about a viscous time for our parameters, $0.2 (a^2 \\Omega_B /\\nu)$ binary orbits, where $\\nu$ is the kinematic viscosity.","We did not investigate dependence on binary mass ratio, eccentricity, disk temperature, or disk viscosity; therefore, these benchmarks may act as guides towards expanding converged solutions to the wider parameter space but might need to be updated in a future study that investigates dependence on system parameters.","We find the most major discrepancies between codes resulted from the dimensionality of the setup (3D vs 2D disks).","Beyond this, we find good agreement in the total torque on the binary between codes, although the partition of this torque between the gravitational torque, orbital accretion torque, and spin accretion torque depends sensitively on the sink prescriptions employed.","In agreement with previous studies, we find a modest difference in torques and accretion variability between 2D and 3D disk models.","We find cavity precession rates to be appreciably faster in 3D than in 2D."],"url":"http://arxiv.org/abs/2402.13039v1","category":"astro-ph.SR"}
{"created":"2024-02-20 14:18:43","title":"Enhancing Real-World Complex Network Representations with Hyperedge Augmentation","abstract":"Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs). Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations. These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise. Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges. Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues. In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary node features by extracting from the virtual hyperedge information, which are used for enhancing GNN performances on downstream tasks. We design three diverse virtual hyperedge construction strategies to accompany the augmentation scheme: (1) via graph statistics, (2) from multiple data perspectives, and (3) utilising multi-modality. Furthermore, to facilitate HyperAug evaluation, we provide 23 novel real-world graph datasets across various domains including social media, biology, and e-commerce. Our empirical study shows that HyperAug consistently and significantly outperforms GNN baselines and other graph augmentation methods, across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into graph augmentation methods for real-world complex networks.","sentences":["Graph augmentation methods play a crucial role in improving the performance and enhancing generalisation capabilities in Graph Neural Networks (GNNs).","Existing graph augmentation methods mainly perturb the graph structures and are usually limited to pairwise node relations.","These methods cannot fully address the complexities of real-world large-scale networks that often involve higher-order node relations beyond only being pairwise.","Meanwhile, real-world graph datasets are predominantly modelled as simple graphs, due to the scarcity of data that can be used to form higher-order edges.","Therefore, reconfiguring the higher-order edges as an integration into graph augmentation strategies lights up a promising research path to address the aforementioned issues.","In this paper, we present Hyperedge Augmentation (HyperAug), a novel graph augmentation method that constructs virtual hyperedges directly form the raw data, and produces auxiliary node features by extracting from the virtual hyperedge information, which are used for enhancing GNN performances on downstream tasks.","We design three diverse virtual hyperedge construction strategies to accompany the augmentation scheme: (1) via graph statistics, (2) from multiple data perspectives, and (3) utilising multi-modality.","Furthermore, to facilitate HyperAug evaluation, we provide 23 novel real-world graph datasets across various domains including social media, biology, and e-commerce.","Our empirical study shows that HyperAug consistently and significantly outperforms GNN baselines and other graph augmentation methods, across a variety of application contexts, which clearly indicates that it can effectively incorporate higher-order node relations into graph augmentation methods for real-world complex networks."],"url":"http://arxiv.org/abs/2402.13033v1","category":"cs.LG"}
{"created":"2024-02-20 14:02:17","title":"Excitons in epitaxially grown WS2 on Graphene: a nanometer-resolved EELS and DFT study","abstract":"In this study, we investigate excitonic properties of epitaxially grown WS2, which is of particular interest for various applications due to its potential for upscaling to wafer sized structures. Understanding the effect of the dielectric environment due to changing layer numbers and multi-material heterostructures on the optical properties is crucial for tailoring device properties. Monochromated electron energy loss spectroscopy in a scanning transmission electron microscope is employed to characterize the excitonic spectrum of WS2 on graphene grown by metal organic chemical vapor deposition. This technique provides the required spatial resolution at the nanometer scale in combination with high quality spectra. To complement the experimental results, theoretical investigations using density functional theory and applying the Bethe-Salpeter equations are conducted. We find that by transitioning from mono- to bi- to multilayers of WS2 the spectra show redshifts for both, the K-valley excitons at about 2.0 and 2.4 eV as well as excitonic features of higher energies. The latter features originate from so called band nesting of transitions between the Gamma- and K-point. In summary, this study provides valuable insights into the excitonic properties of WS2 in different layer configurations and environments, which are realistically needed for future device fabrication and property tuning. Finally, we can show that nanometer scale electron spectroscopy supported by careful theoretical modelling can successfully link atomic structure and optical properties, such as exciton shifts, in non-idealized complex material systems like multilayer 2D heterostructures.","sentences":["In this study, we investigate excitonic properties of epitaxially grown WS2, which is of particular interest for various applications due to its potential for upscaling to wafer sized structures.","Understanding the effect of the dielectric environment due to changing layer numbers and multi-material heterostructures on the optical properties is crucial for tailoring device properties.","Monochromated electron energy loss spectroscopy in a scanning transmission electron microscope is employed to characterize the excitonic spectrum of WS2 on graphene grown by metal organic chemical vapor deposition.","This technique provides the required spatial resolution at the nanometer scale in combination with high quality spectra.","To complement the experimental results, theoretical investigations using density functional theory and applying the Bethe-Salpeter equations are conducted.","We find that by transitioning from mono- to bi- to multilayers of WS2 the spectra show redshifts for both, the K-valley excitons at about 2.0 and 2.4 eV as well as excitonic features of higher energies.","The latter features originate from so called band nesting of transitions between the Gamma- and K-point.","In summary, this study provides valuable insights into the excitonic properties of WS2 in different layer configurations and environments, which are realistically needed for future device fabrication and property tuning.","Finally, we can show that nanometer scale electron spectroscopy supported by careful theoretical modelling can successfully link atomic structure and optical properties, such as exciton shifts, in non-idealized complex material systems like multilayer 2D heterostructures."],"url":"http://arxiv.org/abs/2402.13020v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 13:59:40","title":"Sedimentation dynamics of triply-twisted M\u00f6bius bands: Geometry versus topology","abstract":"Chiral objects have intrigued scientists across several disciplines, including mathematics, crystallography, chemistry, and biology. A M\\\"obius band, an emblematic chiral structure, can be made by connecting the ends of a strip after applying an odd number of twists. Traditionally, the direction of the twist governs its rotational behaviour during sedimentation in a fluid. Here, we present experimental and computational investigations of triply-twisted M\\\"obius bands boasting threefold rotational symmetry that challenge this prevailing understanding. We explore three types of bands with different curvatures, each defined by its construction method. Experimental observations reveal that all three types of bands align axially and exhibit rotational motion during sedimentation. Surprisingly, for only one type of band the spinning direction (chiral hydrodynamic response) departs from expectations; it is not solely determined by the twist direction but changes with the aspect ratio of the band. Numerical simulations corroborate this observation, and an in-depth analysis of the resistance tensors of each type of band sheds light on the possible causes of this transition. We propose that modifications in fluid-induced drag, combined with inertial effects, underpin this phenomenon. Our study challenges existing knowledge of chiral object hydrodynamics, enriching our understanding of complex fluid dynamics. Moreover, it offers transformative potential across diverse fields, promising advancements in mixing, separation processes, and innovative passive swimmers.","sentences":["Chiral objects have intrigued scientists across several disciplines, including mathematics, crystallography, chemistry, and biology.","A M\\\"obius band, an emblematic chiral structure, can be made by connecting the ends of a strip after applying an odd number of twists.","Traditionally, the direction of the twist governs its rotational behaviour during sedimentation in a fluid.","Here, we present experimental and computational investigations of triply-twisted M\\\"obius bands boasting threefold rotational symmetry that challenge this prevailing understanding.","We explore three types of bands with different curvatures, each defined by its construction method.","Experimental observations reveal that all three types of bands align axially and exhibit rotational motion during sedimentation.","Surprisingly, for only one type of band the spinning direction (chiral hydrodynamic response) departs from expectations; it is not solely determined by the twist direction but changes with the aspect ratio of the band.","Numerical simulations corroborate this observation, and an in-depth analysis of the resistance tensors of each type of band sheds light on the possible causes of this transition.","We propose that modifications in fluid-induced drag, combined with inertial effects, underpin this phenomenon.","Our study challenges existing knowledge of chiral object hydrodynamics, enriching our understanding of complex fluid dynamics.","Moreover, it offers transformative potential across diverse fields, promising advancements in mixing, separation processes, and innovative passive swimmers."],"url":"http://arxiv.org/abs/2402.13017v1","category":"physics.flu-dyn"}
{"created":"2024-02-20 13:43:16","title":"Efficient Enumeration of Large Maximal k-Plexes","abstract":"Finding cohesive subgraphs in a large graph has many important applications, such as community detection and biological network analysis. Clique is often a too strict cohesive structure since communities or biological modules rarely form as cliques for various reasons such as data noise. Therefore, $k$-plex is introduced as a popular clique relaxation, which is a graph where every vertex is adjacent to all but at most $k$ vertices. In this paper, we propose an efficient branch-and-bound algorithm as well as its task-based parallel version to enumerate all maximal $k$-plexes with at least $q$ vertices. Our algorithm adopts an effective search space partitioning approach that provides a good time complexity, a new pivot vertex selection method that reduces candidate vertex size, an effective upper-bounding technique to prune useless branches, and three novel pruning techniques by vertex pairs. Our parallel algorithm uses a timeout mechanism to eliminate straggler tasks, and maximizes cache locality while ensuring load balancing. Extensive experiments show that compared with the state-of-the-art algorithms, our sequential and parallel algorithms enumerate large maximal $k$-plexes with up to $5 \\times$ and $18.9 \\times$ speedup, respectively. Ablation results also demonstrate that our pruning techniques bring up to $7 \\times$ speedup compared with our basic algorithm.","sentences":["Finding cohesive subgraphs in a large graph has many important applications, such as community detection and biological network analysis.","Clique is often a too strict cohesive structure since communities or biological modules rarely form as cliques for various reasons such as data noise.","Therefore, $k$-plex is introduced as a popular clique relaxation, which is a graph where every vertex is adjacent to all but at most $k$ vertices.","In this paper, we propose an efficient branch-and-bound algorithm as well as its task-based parallel version to enumerate all maximal $k$-plexes with at least $q$ vertices.","Our algorithm adopts an effective search space partitioning approach that provides a good time complexity, a new pivot vertex selection method that reduces candidate vertex size, an effective upper-bounding technique to prune useless branches, and three novel pruning techniques by vertex pairs.","Our parallel algorithm uses a timeout mechanism to eliminate straggler tasks, and maximizes cache locality while ensuring load balancing.","Extensive experiments show that compared with the state-of-the-art algorithms, our sequential and parallel algorithms enumerate large maximal $k$-plexes with up to $5 \\times$ and $18.9 \\times$ speedup, respectively.","Ablation results also demonstrate that our pruning techniques bring up to $7 \\times$ speedup compared with our basic algorithm."],"url":"http://arxiv.org/abs/2402.13008v1","category":"cs.DS"}
{"created":"2024-02-20 13:32:31","title":"The influence of silicon on the formation and transformation of corrosion products","abstract":"Accurate model predictions of corrosion-driven damage in reinforced concrete structures necessitate a comprehensive understanding of the rate of corrosion product formation. Here, we investigate the influence of dissolved Si characteristic of cementitious systems on the rate of corrosion product transformation at alkaline pH. Compared to systems aged in the absence of Si, small amounts of Si retard the formation rate of the thermodynamically stable corrosion product goethite by a factor of 10. The estimated first order rate constant of transformation k decreases exponentially as a function of the dissolved Si concentration and follows the progression log10k = log10k_0 - 14.65[Si]^0.28. Findings further suggest that the observed retardation is primarily due to the formation of a mobile aqueous Fe-Si complex. The concentration of Si in cementitious systems has a crucial influence, and additional research is required to fully incorporate this factor into reactive transport models, ultimately essential for accurate service life predictions.","sentences":["Accurate model predictions of corrosion-driven damage in reinforced concrete structures necessitate a comprehensive understanding of the rate of corrosion product formation.","Here, we investigate the influence of dissolved Si characteristic of cementitious systems on the rate of corrosion product transformation at alkaline pH. Compared to systems aged in the absence of Si, small amounts of Si retard the formation rate of the thermodynamically stable corrosion product goethite by a factor of 10.","The estimated first order rate constant of transformation k decreases exponentially as a function of the dissolved Si concentration and follows the progression log10k = log10k_0 - 14.65[Si]^0.28.","Findings further suggest that the observed retardation is primarily due to the formation of a mobile aqueous Fe-Si complex.","The concentration of Si in cementitious systems has a crucial influence, and additional research is required to fully incorporate this factor into reactive transport models, ultimately essential for accurate service life predictions."],"url":"http://arxiv.org/abs/2402.13003v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 13:25:16","title":"Towards Trustworthy Reranking: A Simple yet Effective Abstention Mechanism","abstract":"Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems. Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query. We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase. We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism. We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts.","sentences":["Neural Information Retrieval (NIR) has significantly improved upon heuristic-based IR systems.","Yet, failures remain frequent, the models used often being unable to retrieve documents relevant to the user's query.","We address this challenge by proposing a lightweight abstention mechanism tailored for real-world constraints, with particular emphasis placed on the reranking phase.","We introduce a protocol for evaluating abstention strategies in a black-box scenario, demonstrating their efficacy, and propose a simple yet effective data-driven mechanism.","We provide open-source code for experiment replication and abstention implementation, fostering wider adoption and application in diverse contexts."],"url":"http://arxiv.org/abs/2402.12997v1","category":"cs.IR"}
{"created":"2024-02-20 13:19:57","title":"Data Unfolding with Mean Integrated Square Error Optimization","abstract":"Experimental data in Particle and Nuclear physics, Particle Astrophysics and Radiation Protection Dosimetry are obtained from experimental facilities comprising a complex array of sensors, electronics and software. Computer simulation is used to study the measurement process. Probability Density Functions (PDFs) of measured physical parameters deviate from true PDFs due to resolution, bias, and efficiency effects. Good estimates of the true PDF are necessary for testing theoretical models, comparing results from different experiments, and combining results from various research endeavors. In the article, the histogram method is employed to estimate both the measured and true PDFs. The binning of histograms is determined using the K-means clustering algorithm. The true PDF is estimated through the maximization of the likelihood function with entropy regularization, utilizing a non-linear optimization algorithm specially designed for this purpose. The accuracy of the results is assessed using the Mean Integrated Square Error. To determine the optimal value for the regularization parameter, a bootstrap method is applied. Additionally, a mathematical model of the measurement system is formulated using system identification methods. This approach enhances the robustness and precision of the estimation process, providing a more reliable analysis of the system's characteristics.","sentences":["Experimental data in Particle and Nuclear physics, Particle Astrophysics and Radiation Protection Dosimetry are obtained from experimental facilities comprising a complex array of sensors, electronics and software.","Computer simulation is used to study the measurement process.","Probability Density Functions (PDFs) of measured physical parameters deviate from true PDFs due to resolution, bias, and efficiency effects.","Good estimates of the true PDF are necessary for testing theoretical models, comparing results from different experiments, and combining results from various research endeavors.","In the article, the histogram method is employed to estimate both the measured and true PDFs.","The binning of histograms is determined using the K-means clustering algorithm.","The true PDF is estimated through the maximization of the likelihood function with entropy regularization, utilizing a non-linear optimization algorithm specially designed for this purpose.","The accuracy of the results is assessed using the Mean Integrated Square Error.","To determine the optimal value for the regularization parameter, a bootstrap method is applied.","Additionally, a mathematical model of the measurement system is formulated using system identification methods.","This approach enhances the robustness and precision of the estimation process, providing a more reliable analysis of the system's characteristics."],"url":"http://arxiv.org/abs/2402.12990v1","category":"physics.data-an"}
{"created":"2024-02-20 13:18:19","title":"Spectral Properties of Dual Complex Unit Gain Graphs","abstract":"In this paper, we study dual complex unit gain graphs and their spectral properties. We establish the interlacing theorem for dual complex unit gain graphs, and show that the spectral radius of a dual complex unit gain graph is always not greater than the spectral radius of the underlying graph, and these two radii are equal if and only if the dual complex unit gain graph is balanced. Similar results hold for the spectral radius of the Laplacian matrix of the dual complex unit gain graph too.","sentences":["In this paper, we study dual complex unit gain graphs and their spectral properties.","We establish the interlacing theorem for dual complex unit gain graphs, and show that the spectral radius of a dual complex unit gain graph is always not greater than the spectral radius of the underlying graph, and these two radii are equal if and only if the dual complex unit gain graph is balanced.","Similar results hold for the spectral radius of the Laplacian matrix of the dual complex unit gain graph too."],"url":"http://arxiv.org/abs/2402.12988v1","category":"math.CO"}
{"created":"2024-02-20 13:16:32","title":"Enabling Efficient Hybrid Systolic Computation in Shared L1-Memory Manycore Clusters","abstract":"Systolic arrays and shared L1-memory manycore clusters are commonly used architectural paradigms that offer different trade-offs to accelerate parallel workloads. While the first excel with regular dataflow at the cost of rigid architectures and complex programming models, the second are versatile and easy to program but require explicit data flow management and synchronization. This work aims at enabling efficient systolic execution on shared L1-memory manycore clusters. We devise a flexible architecture where small and energy-efficient RISC-V cores act as the systolic array's processing elements (PEs) and can form diverse, reconfigurable systolic topologies through queues mapped in the cluster's shared memory. We introduce two low-overhead RISC-V ISA extensions for efficient systolic execution, namely Xqueue and Queue-linked registers (QLRs), which support queue management in hardware. The Xqueue extension enables single-instruction access to shared-memory-mapped queues, while QLRs allow implicit and autonomous access to them, relieving the cores of explicit communication instructions. We demonstrate Xqueue and QLRs in MemPool, an open-source manycore cluster with 256 PEs, and analyze the hybrid systolic-shared-memory architecture's trade-offs on matrix multiplication, convolution, and FFT kernels. For an area increase of just 6%, our hybrid architecture almost doubles MemPool's compute unit utilization to up to 95% and significantly improves energy efficiency, achieving up to 63% of power spent in the PEs. In typical conditions (TT/0.80V/25{\\deg}C) in a 22nm FDX technology, our hybrid architecture runs at 600MHz with no frequency degradation and is up to 64% more energy efficient than the shared-memory baseline, achieving up to 208GOPS/W.","sentences":["Systolic arrays and shared L1-memory manycore clusters are commonly used architectural paradigms that offer different trade-offs to accelerate parallel workloads.","While the first excel with regular dataflow at the cost of rigid architectures and complex programming models, the second are versatile and easy to program but require explicit data flow management and synchronization.","This work aims at enabling efficient systolic execution on shared L1-memory manycore clusters.","We devise a flexible architecture where small and energy-efficient RISC-V cores act as the systolic array's processing elements (PEs) and can form diverse, reconfigurable systolic topologies through queues mapped in the cluster's shared memory.","We introduce two low-overhead RISC-V ISA extensions for efficient systolic execution, namely Xqueue and Queue-linked registers (QLRs), which support queue management in hardware.","The Xqueue extension enables single-instruction access to shared-memory-mapped queues, while QLRs allow implicit and autonomous access to them, relieving the cores of explicit communication instructions.","We demonstrate Xqueue and QLRs in MemPool, an open-source manycore cluster with 256 PEs, and analyze the hybrid systolic-shared-memory architecture's trade-offs on matrix multiplication, convolution, and FFT kernels.","For an area increase of just 6%, our hybrid architecture almost doubles MemPool's compute unit utilization to up to 95% and significantly improves energy efficiency, achieving up to 63% of power spent in the PEs.","In typical conditions (TT/0.80V/25{\\deg}C) in a 22nm FDX technology, our hybrid architecture runs at 600MHz with no frequency degradation and is up to 64% more energy efficient than the shared-memory baseline, achieving up to 208GOPS/W."],"url":"http://arxiv.org/abs/2402.12986v1","category":"cs.AR"}
{"created":"2024-02-20 13:03:26","title":"Funktionalanalysis Teil I","abstract":"Roughly spoken, Functionalanalysis means the study of the category of infinite-dimensional vectorspaces over the field of real or complex numbers, together with their linear maps. In most cases, one further needs a topological structure on such a vectorspace, because then, you can consider the continuous linear maps between such spaces. The name Functionalanalysis is due to the fact, that in the beginning of the theory, the authors wanted to expand Calculus onto functionals of spaces of functions. Functionalanalytical results give the possibility to solve problems in the Theory of (Partial) Differential Equations, in Complex Analysis or in Quantum Mechanics. But the aim of this lines is not to explain the applications. We will discuss the mathematical theory of metric spaces, normed vector spaces and algebras, spaces of continuous resp. integrable functions as well as reflexive and uniformly convex spaces.","sentences":["Roughly spoken, Functionalanalysis means the study of the category of infinite-dimensional vectorspaces over the field of real or complex numbers, together with their linear maps.","In most cases, one further needs a topological structure on such a vectorspace, because then, you can consider the continuous linear maps between such spaces.","The name Functionalanalysis is due to the fact, that in the beginning of the theory, the authors wanted to expand Calculus onto functionals of spaces of functions.","Functionalanalytical results give the possibility to solve problems in the Theory of (Partial) Differential Equations, in Complex Analysis or in Quantum Mechanics.","But the aim of this lines is not to explain the applications.","We will discuss the mathematical theory of metric spaces, normed vector spaces and algebras, spaces of continuous resp.","integrable functions as well as reflexive and uniformly convex spaces."],"url":"http://arxiv.org/abs/2402.12981v1","category":"math.FA"}
{"created":"2024-02-20 12:59:28","title":"A sequence of Type Ib, IIb, II-L, and II-P supernovae from binary-star progenitors of varying initial separation","abstract":"Over the last decade, evidence has accumulated that massive stars do not typically evolve in isolation but instead follow a tumultuous journey with a companion star on their way to core collapse. While Roche-lobe overflow appears instrumental for the production of a large fraction of supernovae (SNe) of Type Ib and Ic, variations in the initial orbital period Pinit of massive interacting binaries may also produce a wide diversity of case B, BC, or C systems, with preSN stars endowed from minute to massive H-rich envelopes. Focusing here on the explosion of the primary, donor star, originally of 12.6Msun, we use radiation-hydrodynamics and NLTE time-dependent radiative transfer to document the gas and radiation properties of such SNe, covering from Type Ib, IIb, II-L to II-P. Variations in Pinit are the root cause behind the wide diversity of our SN light curves, with single-peak, double-peak, fast-declining or plateau-like morphologies in the V band. The different ejecta structures, expansion rates, and relative abundances (e.g., H, He, 56Ni) are conducive to much diversity in spectral line shapes (absorption vs emission strength, width) and evolution. We emphasize that Halpha is a key tracer of these modulations, and that HeI7065 is an enduring optical diagnostic for the presence of He. Our grid of simulations fare well against representative SNe Ib, IIb, and IIP SNe, but interaction with circumstellar material, which is ignored in this work, is likely at the origin of the tension between our Type IIL SN models and observations (e.g., SN2006Y). Remaining discrepancies in our model rise time to bolometric maximum call for a proper account of both small-scale and large-scale structures in core-collapse SN ejecta. Discrepant Type IIP SN models, with a large plateau brightness but small line widths, may be cured by adopting more compact red-supergiant star progenitors.","sentences":["Over the last decade, evidence has accumulated that massive stars do not typically evolve in isolation but instead follow a tumultuous journey with a companion star on their way to core collapse.","While Roche-lobe overflow appears instrumental for the production of a large fraction of supernovae (SNe) of Type Ib and Ic, variations in the initial orbital period Pinit of massive interacting binaries may also produce a wide diversity of case B, BC, or C systems, with preSN stars endowed from minute to massive H-rich envelopes.","Focusing here on the explosion of the primary, donor star, originally of 12.6Msun, we use radiation-hydrodynamics and NLTE time-dependent radiative transfer to document the gas and radiation properties of such SNe, covering from Type Ib, IIb, II-L to II-P. Variations in Pinit are the root cause behind the wide diversity of our SN light curves, with single-peak, double-peak, fast-declining or plateau-like morphologies in the V band.","The different ejecta structures, expansion rates, and relative abundances (e.g., H, He, 56Ni) are conducive to much diversity in spectral line shapes (absorption vs emission strength, width) and evolution.","We emphasize that Halpha is a key tracer of these modulations, and that HeI7065 is an enduring optical diagnostic for the presence of He.","Our grid of simulations fare well against representative SNe Ib, IIb, and IIP SNe, but interaction with circumstellar material, which is ignored in this work, is likely at the origin of the tension between our Type IIL SN models and observations (e.g., SN2006Y).","Remaining discrepancies in our model rise time to bolometric maximum call for a proper account of both small-scale and large-scale structures in core-collapse SN ejecta.","Discrepant Type IIP SN models, with a large plateau brightness but small line widths, may be cured by adopting more compact red-supergiant star progenitors."],"url":"http://arxiv.org/abs/2402.12977v1","category":"astro-ph.SR"}
{"created":"2024-02-20 12:48:16","title":"Between Green Hills and Green Bills: Unveiling the Green Shades of Sustainability and Burden Shifting through Multi-Objective Optimization in Swiss Energy System Planning","abstract":"The Paris agreement is the first-ever universally accepted and legally binding agreement on global climate change. It is a bridge between today's and climate-neutrality policies and strategies before the end of the century. Critical to this endeavor is energy system modeling, which, while adept at devising cost-effective carbon-neutral strategies, often overlooks the broader environmental and social implications. This study introduces an innovative methodology that integrates life-cycle impact assessment indicators into energy system modeling, enabling a comprehensive assessment of both economic and environmental outcomes.   Focusing on Switzerland's energy system as a case study, our model reveals that optimizing key environomic indicators can lead to significant economic advantages, with system costs potentially decreasing by 15% to 47% by minimizing potential impacts from operating fossil technologies to the indirect impact related to the construction of the renewable infrastructure. However, a system optimized solely for economic efficiency, despite achieving 63% reduction in carbon footprint compared to 2020, our results show a potential risk of burden shift to other environmental issues.   The adoption of multi-objective optimization in our approach nuances the exploration of the complex interplay between environomic objectives and technological choices. Our results illuminate pathways towards more holistically optimized energy systems, effectively addressing trade-offs across environmental problems and enhancing societal acceptance of the solutions to this century's defining challenge.","sentences":["The Paris agreement is the first-ever universally accepted and legally binding agreement on global climate change.","It is a bridge between today's and climate-neutrality policies and strategies before the end of the century.","Critical to this endeavor is energy system modeling, which, while adept at devising cost-effective carbon-neutral strategies, often overlooks the broader environmental and social implications.","This study introduces an innovative methodology that integrates life-cycle impact assessment indicators into energy system modeling, enabling a comprehensive assessment of both economic and environmental outcomes.   ","Focusing on Switzerland's energy system as a case study, our model reveals that optimizing key environomic indicators can lead to significant economic advantages, with system costs potentially decreasing by 15% to 47% by minimizing potential impacts from operating fossil technologies to the indirect impact related to the construction of the renewable infrastructure.","However, a system optimized solely for economic efficiency, despite achieving 63% reduction in carbon footprint compared to 2020, our results show a potential risk of burden shift to other environmental issues.   ","The adoption of multi-objective optimization in our approach nuances the exploration of the complex interplay between environomic objectives and technological choices.","Our results illuminate pathways towards more holistically optimized energy systems, effectively addressing trade-offs across environmental problems and enhancing societal acceptance of the solutions to this century's defining challenge."],"url":"http://arxiv.org/abs/2402.12973v1","category":"cs.CE"}
{"created":"2024-02-20 12:42:53","title":"The kinematics of multiple Compton scattering of two-photon systems","abstract":"We present a Stokes-Mueller method to calculate the cross sections associated with multiple Compton scattering of an arbitrary two-photon system. This method is used to calculate the cross section in the scenario in which one of the maximally entangled annihilation photons undergoes intermediate Compton scattering followed by the detection of both photons using a pair of Compton polarimeters. The method accounts for potential quantum-decoherence effects caused by Compton scattering. Despite being grounded in quantum field theory, the method does not require an in-depth understanding of its typically intricate prerequisites. As a result, this method is anticipated to be accessible to a broader community of physicists and is readily applicable in Monte Carlo simulations.","sentences":["We present a Stokes-Mueller method to calculate the cross sections associated with multiple Compton scattering of an arbitrary two-photon system.","This method is used to calculate the cross section in the scenario in which one of the maximally entangled annihilation photons undergoes intermediate Compton scattering followed by the detection of both photons using a pair of Compton polarimeters.","The method accounts for potential quantum-decoherence effects caused by Compton scattering.","Despite being grounded in quantum field theory, the method does not require an in-depth understanding of its typically intricate prerequisites.","As a result, this method is anticipated to be accessible to a broader community of physicists and is readily applicable in Monte Carlo simulations."],"url":"http://arxiv.org/abs/2402.12972v1","category":"hep-th"}
{"created":"2024-02-20 12:40:31","title":"How Temporal Unrolling Supports Neural Physics Simulators","abstract":"Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators. We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories. In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients. Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients. We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios. It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does not utilize this solver. We also quantify a difference in the accuracy of models trained in a fully differentiable setup compared to their non-differentiable counterparts. While differentiable setups perform best, the accuracy of unrolling without temporal gradients comes comparatively close. Furthermore, we empirically show that these behaviors are invariant to changes in the underlying physical system, the network architecture and size, and the numerical scheme. These results motivate integrating non-differentiable numerical simulators into training setups even if full differentiability is unavailable. We also observe that the convergence rate of common neural architectures is low compared to numerical algorithms. This encourages the use of hybrid approaches combining neural and numerical algorithms to utilize the benefits of both.","sentences":["Unrolling training trajectories over time strongly influences the inference accuracy of neural network-augmented physics simulators.","We analyze these effects by studying three variants of training neural networks on discrete ground truth trajectories.","In addition to commonly used one-step setups and fully differentiable unrolling, we include a third, less widely used variant: unrolling without temporal gradients.","Comparing networks trained with these three modalities makes it possible to disentangle the two dominant effects of unrolling, training distribution shift and long-term gradients.","We present a detailed study across physical systems, network sizes, network architectures, training setups, and test scenarios.","It provides an empirical basis for our main findings: A non-differentiable but unrolled training setup supported by a numerical solver can yield 4.5-fold improvements over a fully differentiable prediction setup that does not utilize this solver.","We also quantify a difference in the accuracy of models trained in a fully differentiable setup compared to their non-differentiable counterparts.","While differentiable setups perform best, the accuracy of unrolling without temporal gradients comes comparatively close.","Furthermore, we empirically show that these behaviors are invariant to changes in the underlying physical system, the network architecture and size, and the numerical scheme.","These results motivate integrating non-differentiable numerical simulators into training setups even if full differentiability is unavailable.","We also observe that the convergence rate of common neural architectures is low compared to numerical algorithms.","This encourages the use of hybrid approaches combining neural and numerical algorithms to utilize the benefits of both."],"url":"http://arxiv.org/abs/2402.12971v1","category":"physics.comp-ph"}
{"created":"2024-02-20 12:28:25","title":"Multi-Level ML Based Burst-Aware Autoscaling for SLO Assurance and Cost Efficiency","abstract":"Autoscaling is a technology to automatically scale the resources provided to their applications without human intervention to guarantee runtime Quality of Service (QoS) while saving costs. However, user-facing cloud applications serve dynamic workloads that often exhibit variable and contain bursts, posing challenges to autoscaling for maintaining QoS within Service-Level Objectives (SLOs). Conservative strategies risk over-provisioning, while aggressive ones may cause SLO violations, making it more challenging to design effective autoscaling. This paper introduces BAScaler, a Burst-Aware Autoscaling framework for containerized cloud services or applications under complex workloads, combining multi-level machine learning (ML) techniques to mitigate SLO violations while saving costs. BAScaler incorporates a novel prediction-based burst detection mechanism that distinguishes between predictable periodic workload spikes and actual bursts. When bursts are detected, BAScaler appropriately overestimates them and allocates resources accordingly to address the rapid growth in resource demand. On the other hand, BAScaler employs reinforcement learning to rectify potential inaccuracies in resource estimation, enabling more precise resource allocation during non-bursts. Experiments across ten real-world workloads demonstrate BAScaler's effectiveness, achieving a 57% average reduction in SLO violations and cutting resource costs by 10% compared to other prominent methods.","sentences":["Autoscaling is a technology to automatically scale the resources provided to their applications without human intervention to guarantee runtime Quality of Service (QoS) while saving costs.","However, user-facing cloud applications serve dynamic workloads that often exhibit variable and contain bursts, posing challenges to autoscaling for maintaining QoS within Service-Level Objectives (SLOs).","Conservative strategies risk over-provisioning, while aggressive ones may cause SLO violations, making it more challenging to design effective autoscaling.","This paper introduces BAScaler, a Burst-Aware Autoscaling framework for containerized cloud services or applications under complex workloads, combining multi-level machine learning (ML) techniques to mitigate SLO violations while saving costs.","BAScaler incorporates a novel prediction-based burst detection mechanism that distinguishes between predictable periodic workload spikes and actual bursts.","When bursts are detected, BAScaler appropriately overestimates them and allocates resources accordingly to address the rapid growth in resource demand.","On the other hand, BAScaler employs reinforcement learning to rectify potential inaccuracies in resource estimation, enabling more precise resource allocation during non-bursts.","Experiments across ten real-world workloads demonstrate BAScaler's effectiveness, achieving a 57% average reduction in SLO violations and cutting resource costs by 10% compared to other prominent methods."],"url":"http://arxiv.org/abs/2402.12962v1","category":"cs.SE"}
{"created":"2024-02-20 12:12:07","title":"On pole-skipping with gauge-invariant variables in holographic axion theories","abstract":"We study the pole-skipping phenomenon within holographic axion theories, a common framework for studying strongly coupled systems with chemical potential ($\\mu$) and momentum relaxation ($\\beta$). Considering the backreaction characterized by $\\mu$ and $\\beta$, we encounter coupled equations of motion for the metric, gauge, and axion field, which are classified into spin-0, spin-1, and spin-2 channels. Employing gauge-invariant variables, we systematically address these equations and explore pole-skipping points within each sector using the near-horizon method. Our analysis reveals two classes of pole-skipping points: regular and singular pole-skipping points in which the latter is identified when standard linear differential equations exhibit singularity. Notably, pole-skipping points in the lower-half plane are regular, while those elsewhere are singular. This suggests that the pole-skipping point in the spin-0 channel, associated with quantum chaos, corresponds to a singular pole-skipping point. Additionally, we observe that the pole-skipping momentum, if purely real or imaginary for $\\mu=\\beta=0$, retains this characteristic for $\\mu \\neq0$ and $\\beta \\neq 0$.","sentences":["We study the pole-skipping phenomenon within holographic axion theories, a common framework for studying strongly coupled systems with chemical potential ($\\mu$) and momentum relaxation ($\\beta$).","Considering the backreaction characterized by $\\mu$ and $\\beta$, we encounter coupled equations of motion for the metric, gauge, and axion field, which are classified into spin-0, spin-1, and spin-2 channels.","Employing gauge-invariant variables, we systematically address these equations and explore pole-skipping points within each sector using the near-horizon method.","Our analysis reveals two classes of pole-skipping points: regular and singular pole-skipping points in which the latter is identified when standard linear differential equations exhibit singularity.","Notably, pole-skipping points in the lower-half plane are regular, while those elsewhere are singular.","This suggests that the pole-skipping point in the spin-0 channel, associated with quantum chaos, corresponds to a singular pole-skipping point.","Additionally, we observe that the pole-skipping momentum, if purely real or imaginary for $\\mu=\\beta=0$, retains this characteristic for $\\mu \\neq0$ and $\\beta \\neq 0$."],"url":"http://arxiv.org/abs/2402.12951v1","category":"hep-th"}
{"created":"2024-02-20 11:53:08","title":"Apparent color and Raman vibrational modes of the unconventional superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+\u03b4}$ exfoliated flakes","abstract":"Studying and controlling the properties of individual exfoliated materials is one of the first steps towards the fabrication of complex van der Waals systems. However, prolonged exposure to ambient conditions can affect the properties of very thin exfoliated materials altering their physical properties. For this reason, it is imperative to employ versatile characterization strategies compatible with reduced ambient exposure times. In this work, we demonstrate that optical microscopy and Raman spectroscopy are quick and non-invasive techniques to study flakes of the high-temperature superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (BSCCO-2212). The apparent color of BSCCO-2212 exfoliated flakes on SiO$_2$/Si has been studied allowing a rough and fast identification of the number of layers. Moreover, we find that thin flakes have a refractive index of around 1.7 in the visible range and 0.5 for the absorption coefficient near the maximum at 550 nm. We determine the optimal combination of illumination wavelength and substrate properties for the identification of different numbers of unit cells of BSCCO-2212. In addition, we report the hardening of the characteristic Raman modes at 116 cm$^{-1}$ and 460 cm$^{-1}$ as flake thickness decreases, possibly due to strain in the BiO and CuO$_2$ planes, respectively. Moreover, the evolution of the Raman modes establishes a second approach to determine the thickness of BSCCO-2212 thin flakes. As BSCCO-2212 is a challenging material to be due to its sensitivity to ambient conditions deriving in an insulating state, the present work provides a guide for the fabrication and characterization of complex van der Waals systems paving the way for studying heterostructures based on unconventional superconductors in the 2D limit.","sentences":["Studying and controlling the properties of individual exfoliated materials is one of the first steps towards the fabrication of complex van der Waals systems.","However, prolonged exposure to ambient conditions can affect the properties of very thin exfoliated materials altering their physical properties.","For this reason, it is imperative to employ versatile characterization strategies compatible with reduced ambient exposure times.","In this work, we demonstrate that optical microscopy and Raman spectroscopy are quick and non-invasive techniques to study flakes of the high-temperature superconductor Bi$_2$Sr$_2$CaCu$_2$O$_{8+\\delta}$ (BSCCO-2212).","The apparent color of BSCCO-2212 exfoliated flakes on SiO$_2$/Si has been studied allowing a rough and fast identification of the number of layers.","Moreover, we find that thin flakes have a refractive index of around 1.7 in the visible range and 0.5 for the absorption coefficient near the maximum at 550 nm.","We determine the optimal combination of illumination wavelength and substrate properties for the identification of different numbers of unit cells of BSCCO-2212.","In addition, we report the hardening of the characteristic Raman modes at 116 cm$^{-1}$ and 460 cm$^{-1}$ as flake thickness decreases, possibly due to strain in the BiO and CuO$_2$ planes, respectively.","Moreover, the evolution of the Raman modes establishes a second approach to determine the thickness of BSCCO-2212 thin flakes.","As BSCCO-2212 is a challenging material to be due to its sensitivity to ambient conditions deriving in an insulating state, the present work provides a guide for the fabrication and characterization of complex van der Waals systems paving the way for studying heterostructures based on unconventional superconductors in the 2D limit."],"url":"http://arxiv.org/abs/2402.12941v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 11:38:52","title":"GRAPHGINI: Fostering Individual and Group Fairness in Graph Neural Networks","abstract":"We address the growing apprehension that GNNs, in the absence of fairness constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals. Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of fairness to be used within the GNN framework. Our proposal, GRAPHGINI, works with the two different goals of individual and group fairness in a single system, while maintaining high prediction accuracy. GRAPHGINI enforces individual fairness through learnable attention scores that help in aggregating more information through similar nodes. A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group fairness. Both the individual fairness constraint and the group fairness constraint are stated in terms of a differentiable approximation of the Gini coefficient. This approximation is a contribution that is likely to be of interest even beyond the scope of the problem studied in this paper. Unlike other state-of-the-art, GRAPHGINI automatically balances all three optimization objectives (utility, individual, and group fairness) of the GNN and is free from any manual tuning of weight parameters. Extensive experimentation on real-world datasets showcases the efficacy of GRAPHGINI in making significant improvements in individual fairness compared to all currently available state-of-the-art methods while maintaining utility and group equality.","sentences":["We address the growing apprehension that GNNs, in the absence of fairness constraints, might produce biased decisions that disproportionately affect underprivileged groups or individuals.","Departing from previous work, we introduce for the first time a method for incorporating the Gini coefficient as a measure of fairness to be used within the GNN framework.","Our proposal, GRAPHGINI, works with the two different goals of individual and group fairness in a single system, while maintaining high prediction accuracy.","GRAPHGINI enforces individual fairness through learnable attention scores that help in aggregating more information through similar nodes.","A heuristic-based maximum Nash social welfare constraint ensures the maximum possible group fairness.","Both the individual fairness constraint and the group fairness constraint are stated in terms of a differentiable approximation of the Gini coefficient.","This approximation is a contribution that is likely to be of interest even beyond the scope of the problem studied in this paper.","Unlike other state-of-the-art, GRAPHGINI automatically balances all three optimization objectives (utility, individual, and group fairness) of the GNN and is free from any manual tuning of weight parameters.","Extensive experimentation on real-world datasets showcases the efficacy of GRAPHGINI in making significant improvements in individual fairness compared to all currently available state-of-the-art methods while maintaining utility and group equality."],"url":"http://arxiv.org/abs/2402.12937v1","category":"cs.LG"}
{"created":"2024-02-20 11:38:02","title":"Characterizing the detailed balance property by means of measurements in chemical networks","abstract":"In this paper we study how to determine if a linear biochemical network satisfies the detailed balance condition, without knowing the details of all the reactions taking place in the network. To this end, we use the formalism of response functions $R_{ij} (t) $ that measure how the system reacts to the injection of the substance $j$ at time $t=0$, by measuring the concentration of the substance $i \\neq j$ for $t >0$. In particular, we obtain a condition involving two reciprocal measurements (i.e.~$R_{ij}(t), \\, R_{ji}(t)$) that is necessary, but not sufficient for the detailed balance condition to hold in the network. Moreover, we prove that this necessary condition is also sufficient if a topological condition is satisfied, as well as a stability property that guarantees that the chemical rates are not fine-tuned.","sentences":["In this paper we study how to determine if a linear biochemical network satisfies the detailed balance condition, without knowing the details of all the reactions taking place in the network.","To this end, we use the formalism of response functions $R_{ij} (t) $ that measure how the system reacts to the injection of the substance $j$ at time $t=0$, by measuring the concentration of the substance $i \\neq j$ for $t >0$.","In particular, we obtain a condition involving two reciprocal measurements (i.e.~$R_{ij}(t), \\, R_{ji}(t)$) that is necessary, but not sufficient for the detailed balance condition to hold in the network.","Moreover, we prove that this necessary condition is also sufficient if a topological condition is satisfied, as well as a stability property that guarantees that the chemical rates are not fine-tuned."],"url":"http://arxiv.org/abs/2402.12935v1","category":"math-ph"}
{"created":"2024-02-20 11:18:40","title":"Advancements in Point Cloud-Based 3D Defect Detection and Classification for Industrial Systems: A Comprehensive Survey","abstract":"In recent years, 3D point clouds (PCs) have gained significant attention due to their diverse applications across various fields such as computer vision (CV), condition monitoring, virtual reality, robotics, autonomous driving etc. Deep learning (DL) has proven effective in leveraging 3D PCs to address various challenges previously encountered in 2D vision. However, the application of deep neural networks (DNN) to process 3D PCs presents its own set of challenges. To address these challenges, numerous methods have been proposed. This paper provides an in-depth review of recent advancements in DL-based condition monitoring (CM) using 3D PCs, with a specific focus on defect shape classification and segmentation within industrial applications for operational and maintenance purposes. Recognizing the crucial role of these aspects in industrial maintenance, the paper provides insightful observations that offer perspectives on the strengths and limitations of the reviewed DL-based PC processing methods. This synthesis of knowledge aims to contribute to the understanding and enhancement of CM processes, particularly within the framework of remaining useful life (RUL), in industrial systems.","sentences":["In recent years, 3D point clouds (PCs) have gained significant attention due to their diverse applications across various fields such as computer vision (CV), condition monitoring, virtual reality, robotics, autonomous driving etc.","Deep learning (DL) has proven effective in leveraging 3D PCs to address various challenges previously encountered in 2D vision.","However, the application of deep neural networks (DNN) to process 3D PCs presents its own set of challenges.","To address these challenges, numerous methods have been proposed.","This paper provides an in-depth review of recent advancements in DL-based condition monitoring (CM) using 3D PCs, with a specific focus on defect shape classification and segmentation within industrial applications for operational and maintenance purposes.","Recognizing the crucial role of these aspects in industrial maintenance, the paper provides insightful observations that offer perspectives on the strengths and limitations of the reviewed DL-based PC processing methods.","This synthesis of knowledge aims to contribute to the understanding and enhancement of CM processes, particularly within the framework of remaining useful life (RUL), in industrial systems."],"url":"http://arxiv.org/abs/2402.12923v1","category":"cs.CV"}
{"created":"2024-02-20 11:17:53","title":"Gimbal Actuator Modeling for a Spin-Stabilized Spacecraft Equipped with a 1DoF Gimbaled-Thruster and two Reaction Wheels","abstract":"Attitude control of spacecraft during an impulsive orbital maneuver is a vital task. Many spacecraft and launchers use the gimbaled thrust vector control (TVC) in their attitude control system during an orbital maneuver. Mathematical modeling of the gimbal actuator is an important task because we should show the applicability of the gimbaled-TVC in a spacecraft. In this paper, a spin-stabilized spacecraft equipped with one degree of freedom (DoF) gimbaled-thruster and two reaction wheels (RWs) is considered. The control goals are disturbance rejection and thrust vector (spin-axis) stabilization based on one DoF gimbal actuator and two RWs. The gimbal is assumed to be equipped with a gearbox and a DC electric motor. This actuator must supply the gimbal torque to rotate the spacecraft nozzle. The mathematical model of the mentioned spacecraft is extended with respect to the DC motor equations. In order to investigate the applicability of the proposed method, an industrial DC electric motor is considered for the gimbal actuator. The simulation results prove that an industrial DC electric motor is able to be used for attitude control of the mentioned spacecraft. The simulation results indicate the applicability of the proposed control method in an impulsive orbital maneuver.","sentences":["Attitude control of spacecraft during an impulsive orbital maneuver is a vital task.","Many spacecraft and launchers use the gimbaled thrust vector control (TVC) in their attitude control system during an orbital maneuver.","Mathematical modeling of the gimbal actuator is an important task because we should show the applicability of the gimbaled-TVC in a spacecraft.","In this paper, a spin-stabilized spacecraft equipped with one degree of freedom (DoF) gimbaled-thruster and two reaction wheels (RWs) is considered.","The control goals are disturbance rejection and thrust vector (spin-axis) stabilization based on one DoF gimbal actuator and two RWs.","The gimbal is assumed to be equipped with a gearbox and a DC electric motor.","This actuator must supply the gimbal torque to rotate the spacecraft nozzle.","The mathematical model of the mentioned spacecraft is extended with respect to the DC motor equations.","In order to investigate the applicability of the proposed method, an industrial DC electric motor is considered for the gimbal actuator.","The simulation results prove that an industrial DC electric motor is able to be used for attitude control of the mentioned spacecraft.","The simulation results indicate the applicability of the proposed control method in an impulsive orbital maneuver."],"url":"http://arxiv.org/abs/2402.12922v1","category":"eess.SY"}
{"created":"2024-02-20 11:03:36","title":"Large Language Model-based Human-Agent Collaboration for Complex Task Solving","abstract":"In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest. Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs. In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential. In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC. This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process. We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment. Our validation tests confirm the model's effectiveness. The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention. Datasets and code are available at: https://github.com/XueyangFeng/ReHAC.","sentences":["In recent developments within the research community, the integration of Large Language Models (LLMs) in creating fully autonomous agents has garnered significant interest.","Despite this, LLM-based agents frequently demonstrate notable shortcomings in adjusting to dynamic environments and fully grasping human needs.","In this work, we introduce the problem of LLM-based human-agent collaboration for complex task-solving, exploring their synergistic potential.","In addition, we propose a Reinforcement Learning-based Human-Agent Collaboration method, ReHAC.","This approach includes a policy model designed to determine the most opportune stages for human intervention within the task-solving process.","We construct a human-agent collaboration dataset to train this policy model in an offline reinforcement learning environment.","Our validation tests confirm the model's effectiveness.","The results demonstrate that the synergistic efforts of humans and LLM-based agents significantly improve performance in complex tasks, primarily through well-planned, limited human intervention.","Datasets and code are available at: https://github.com/XueyangFeng/ReHAC."],"url":"http://arxiv.org/abs/2402.12914v1","category":"cs.CL"}
{"created":"2024-02-20 10:58:08","title":"Calculation of Poincar\u00e9 recurrence time and irreversibility in Statistical Mechanics","abstract":"One of the important questions in statistical mechanics is how irreversibility (time's arrow) occurs when Newton equations of motion are time reversal invariant. Boltzmann's argument is that the observed irreversibility is due to the very large number of particles. One objection to Boltzmann was based on Poincar\\'e's recursion theorem: a classical hamiltonian confined system returns after some time, called today Poincar\\'e recurrence time (PRT), close to its initial configuration. Boltzmann's reply was that for $N = 10^{23} $ particles, PRT is very large and exceeds the age of the universe. In this paper we compute for the first time, a typical recurrence time $ T(N)$ for a gas of $N$ particles. We find that $T(N) \\sim N^z \\exp (y N) $ and determine the exponents $y$ and $z$ for different values of the particle density and kinetic energy (temperature).   We find that $ T(N) $ exceeds the age of the Universe for a relatively small number of particles, much smaller than $ 10^{23} $.   Our results support Boltzmann's argument on the origin of irreversibility in statistical mechanics.","sentences":["One of the important questions in statistical mechanics is how irreversibility (time's arrow) occurs when Newton equations of motion are time reversal invariant.","Boltzmann's argument is that the observed irreversibility is due to the very large number of particles.","One objection to Boltzmann was based on Poincar\\'e's recursion theorem: a classical hamiltonian confined system returns after some time, called today Poincar\\'e recurrence time (PRT), close to its initial configuration.","Boltzmann's reply was that for $N = 10^{23} $ particles, PRT is very large and exceeds the age of the universe.","In this paper we compute for the first time, a typical recurrence time $ T(N)$ for a gas of $N$ particles.","We find that $T(N) \\sim N^z \\exp (y N) $ and determine the exponents $y$ and $z$ for different values of the particle density and kinetic energy (temperature).   ","We find that $ T(N) $ exceeds the age of the Universe for a relatively small number of particles, much smaller than $ 10^{23} $.   ","Our results support Boltzmann's argument on the origin of irreversibility in statistical mechanics."],"url":"http://arxiv.org/abs/2402.12910v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-20 10:49:53","title":"On the Gluing of germs of complex analytic spaces, Betti numbers and their structure","abstract":"In this paper we introduce new classes of gluing of complex analytic spaces germs, called weakly large, large and strongly large. We give a description of their Poincar\\'e series and, as applications, we give numerical criteria to determine when these classes of gluing of germs of complex analytic spaces are smooth, singular, complete intersections and Gorenstein in terms of their Betti numbers. In particular, we show that the gluing of the same germ of complex analytic space along of any subspace is always a singular germ.","sentences":["In this paper we introduce new classes of gluing of complex analytic spaces germs, called weakly large, large and strongly large.","We give a description of their Poincar\\'e series and, as applications, we give numerical criteria to determine when these classes of gluing of germs of complex analytic spaces are smooth, singular, complete intersections and Gorenstein in terms of their Betti numbers.","In particular, we show that the gluing of the same germ of complex analytic space along of any subspace is always a singular germ."],"url":"http://arxiv.org/abs/2402.12904v1","category":"math.AG"}
{"created":"2024-02-20 10:49:17","title":"Lipschitz stability for an inverse source problem of the wave equation with kinetic boundary conditions","abstract":"In this paper, we present a refined approach to establish a global Lipschitz stability for an inverse source problem concerning the determination of forcing terms in the wave equation with mixed boundary conditions. It consists of boundary conditions incorporating a dynamic boundary condition and Dirichlet boundary condition on disjoint subsets of the boundary. The primary contribution of this article is the rigorous derivation of a sharp Carleman estimate for the wave system with a dynamic boundary condition. In particular, our findings complete and drastically improve the earlier results established by Gal and Tebou [SIAM J. Control Optim., 55 (2017), 324-364]. This is achieved by using a different weight function to overcome some relevant difficulties. As for the stability proof, we extend to dynamic boundary conditions a recent argument avoiding cut-off functions. Finally, we also show that our developed Carleman estimate yields a sharp boundary controllability result.","sentences":["In this paper, we present a refined approach to establish a global Lipschitz stability for an inverse source problem concerning the determination of forcing terms in the wave equation with mixed boundary conditions.","It consists of boundary conditions incorporating a dynamic boundary condition and Dirichlet boundary condition on disjoint subsets of the boundary.","The primary contribution of this article is the rigorous derivation of a sharp Carleman estimate for the wave system with a dynamic boundary condition.","In particular, our findings complete and drastically improve the earlier results established by Gal and Tebou","[SIAM J. Control Optim., 55 (2017), 324-364].","This is achieved by using a different weight function to overcome some relevant difficulties.","As for the stability proof, we extend to dynamic boundary conditions a recent argument avoiding cut-off functions.","Finally, we also show that our developed Carleman estimate yields a sharp boundary controllability result."],"url":"http://arxiv.org/abs/2402.12902v1","category":"math.AP"}
{"created":"2024-02-20 10:45:33","title":"Density matrix renormalization group study of the interacting Kitaev chain with quasi-periodic disorder","abstract":"We document the ground state phase diagram of the one-dimensional Kitaev chain with quasi-periodic disorder in the presence of two-body interactions. Our data was obtained for systems of $L=1000$ sites using large-scale density matrix renormalization group numerics and is benchmarked against known results for the clean system. We demonstrate that moderate quasi-periodic disorder stabilizes the topological phase both for repulsive and attractive interactions. For larger disorder strengths, the system features re-entrance behavior and multiple phase transitions.","sentences":["We document the ground state phase diagram of the one-dimensional Kitaev chain with quasi-periodic disorder in the presence of two-body interactions.","Our data was obtained for systems of $L=1000$ sites using large-scale density matrix renormalization group numerics and is benchmarked against known results for the clean system.","We demonstrate that moderate quasi-periodic disorder stabilizes the topological phase both for repulsive and attractive interactions.","For larger disorder strengths, the system features re-entrance behavior and multiple phase transitions."],"url":"http://arxiv.org/abs/2402.12897v1","category":"cond-mat.str-el"}
{"created":"2024-02-20 10:36:09","title":"Extensive search for axion dark matter over 1\\,GHz with CAPP's Main Axion eXperiment","abstract":"We report an extensive high-sensitivity search for axion dark matter above 1\\,GHz at the Center for Axion and Precision Physics Research (CAPP). The cavity resonant search, exploiting the coupling between axions and photons, explored the frequency (mass) range of 1.025\\,GHz (4.24\\,$\\mu$eV) to 1.185\\,GHz (4.91\\,$\\mu$eV). We have introduced a number of innovations in this field, demonstrating the practical approach of optimizing all the relevant parameters of axion haloscopes, extending presently available technology. The CAPP 12\\,T magnet with an aperture of 320\\,mm made of Nb$_3$Sn and NbTi superconductors surrounding a 37-liter ultralight-weight copper cavity is expected to convert DFSZ axions into approximately $10^2$ microwave photons per second. A powerful dilution refrigerator, capable of keeping the core system below 40\\,mK, combined with quantum-noise limited readout electronics, achieved a total system noise of about 200\\,mK or below, which corresponds to a background of roughly $4\\times 10^3$ photons per second within the axion bandwidth. The combination of all those improvements provides unprecedented search performance, imposing the most stringent exclusion limits on axion--photon coupling in this frequency range to date. These results also suggest an experimental capability suitable for highly-sensitive searches for axion dark matter above 1\\,GHz.","sentences":["We report an extensive high-sensitivity search for axion dark matter above 1\\,GHz at the Center for Axion and Precision Physics Research (CAPP).","The cavity resonant search, exploiting the coupling between axions and photons, explored the frequency (mass) range of 1.025\\,GHz (4.24\\,$\\mu$eV) to 1.185\\,GHz (4.91\\,$\\mu$eV).","We have introduced a number of innovations in this field, demonstrating the practical approach of optimizing all the relevant parameters of axion haloscopes, extending presently available technology.","The CAPP 12\\,T magnet with an aperture of 320\\,mm made of Nb$_3$Sn and NbTi superconductors surrounding a 37-liter ultralight-weight copper cavity is expected to convert DFSZ axions into approximately $10^2$ microwave photons per second.","A powerful dilution refrigerator, capable of keeping the core system below 40\\,mK, combined with quantum-noise limited readout electronics, achieved a total system noise of about 200\\,mK or below, which corresponds to a background of roughly $4\\times 10^3$ photons per second within the axion bandwidth.","The combination of all those improvements provides unprecedented search performance, imposing the most stringent exclusion limits on axion--photon coupling in this frequency range to date.","These results also suggest an experimental capability suitable for highly-sensitive searches for axion dark matter above 1\\,GHz."],"url":"http://arxiv.org/abs/2402.12892v1","category":"hep-ex"}
{"created":"2024-02-20 10:27:44","title":"Real-time High-resolution View Synthesis of Complex Scenes with Explicit 3D Visibility Reasoning","abstract":"Rendering photo-realistic novel-view images of complex scenes has been a long-standing challenge in computer graphics. In recent years, great research progress has been made on enhancing rendering quality and accelerating rendering speed in the realm of view synthesis. However, when rendering complex dynamic scenes with sparse views, the rendering quality remains limited due to occlusion problems. Besides, for rendering high-resolution images on dynamic scenes, the rendering speed is still far from real-time. In this work, we propose a generalizable view synthesis method that can render high-resolution novel-view images of complex static and dynamic scenes in real-time from sparse views. To address the occlusion problems arising from the sparsity of input views and the complexity of captured scenes, we introduce an explicit 3D visibility reasoning approach that can efficiently estimate the visibility of sampled 3D points to the input views. The proposed visibility reasoning approach is fully differentiable and can gracefully fit inside the volume rendering pipeline, allowing us to train our networks with only multi-view images as supervision while refining geometry and texture simultaneously. Besides, each module in our pipeline is carefully designed to bypass the time-consuming MLP querying process and enhance the rendering quality of high-resolution images, enabling us to render high-resolution novel-view images in real-time.Experimental results show that our method outperforms previous view synthesis methods in both rendering quality and speed, particularly when dealing with complex dynamic scenes with sparse views.","sentences":["Rendering photo-realistic novel-view images of complex scenes has been a long-standing challenge in computer graphics.","In recent years, great research progress has been made on enhancing rendering quality and accelerating rendering speed in the realm of view synthesis.","However, when rendering complex dynamic scenes with sparse views, the rendering quality remains limited due to occlusion problems.","Besides, for rendering high-resolution images on dynamic scenes, the rendering speed is still far from real-time.","In this work, we propose a generalizable view synthesis method that can render high-resolution novel-view images of complex static and dynamic scenes in real-time from sparse views.","To address the occlusion problems arising from the sparsity of input views and the complexity of captured scenes, we introduce an explicit 3D visibility reasoning approach that can efficiently estimate the visibility of sampled 3D points to the input views.","The proposed visibility reasoning approach is fully differentiable and can gracefully fit inside the volume rendering pipeline, allowing us to train our networks with only multi-view images as supervision while refining geometry and texture simultaneously.","Besides, each module in our pipeline is carefully designed to bypass the time-consuming MLP querying process and enhance the rendering quality of high-resolution images, enabling us to render high-resolution novel-view images in real-time.","Experimental results show that our method outperforms previous view synthesis methods in both rendering quality and speed, particularly when dealing with complex dynamic scenes with sparse views."],"url":"http://arxiv.org/abs/2402.12886v1","category":"cs.GR"}
{"created":"2024-02-20 09:55:20","title":"A Novel Protocol Using Captive Portals for FIDO2 Network Authentication","abstract":"FIDO2 authentication is starting to be applied in numerous web authentication services, aiming to replace passwords and their known vulnerabilities. However, this new authentication method has not been integrated yet with network authentication systems. In this paper, we introduce FIDO2CAP: FIDO2 Captive-portal Authentication Protocol. Our proposal describes a novel protocol for captive-portal network authentication using FIDO2 authenticators, as security keys and passkeys. For validating our proposal, we have developed a prototype of FIDO2CAP authentication in a mock scenario. Using this prototype, we performed an usability experiment with 15 real users. This work makes the first systematic approach for adapting network authentication to the new authentication paradigm relying on FIDO2 authentication.","sentences":["FIDO2 authentication is starting to be applied in numerous web authentication services, aiming to replace passwords and their known vulnerabilities.","However, this new authentication method has not been integrated yet with network authentication systems.","In this paper, we introduce FIDO2CAP:","FIDO2 Captive-portal Authentication Protocol.","Our proposal describes a novel protocol for captive-portal network authentication using FIDO2 authenticators, as security keys and passkeys.","For validating our proposal, we have developed a prototype of FIDO2CAP authentication in a mock scenario.","Using this prototype, we performed an usability experiment with 15 real users.","This work makes the first systematic approach for adapting network authentication to the new authentication paradigm relying on FIDO2 authentication."],"url":"http://arxiv.org/abs/2402.12864v1","category":"cs.CR"}
{"created":"2024-02-20 09:35:11","title":"Partial null-controllabiliy of evolution equations by one-dimensional control","abstract":"The problem of partial null controllability for linear autonomous evolution equations, which are controlled by a one-dimensional control, is under consideration. The partial null-controllability conditions for coupled abstract evolution systems have been obtained using the moment problem approach.","sentences":["The problem of partial null controllability for linear autonomous evolution equations, which are controlled by a one-dimensional control, is under consideration.","The partial null-controllability conditions for coupled abstract evolution systems have been obtained using the moment problem approach."],"url":"http://arxiv.org/abs/2402.12855v1","category":"math.OC"}
{"created":"2024-02-20 09:33:09","title":"A mechanical analogue of Faraday's law for waves in chiral elastic media","abstract":"Faraday's law on electromagnetic induction, one of the most fundamental laws of nature, indicates that a change of magnetic field through a coil wire induces a current in the wire. Electromagnetic induction has many paramount technological applications today, and provides the link between electric and magnetic fields, which is crucial to explain the existence of electromagnetic waves. In our quest to replicate Faraday's law for mechanical systems, we design an infinite mass-spring \"helix-like structure\", which consists of a helix and a central line, and implement Bloch-Floquet conditions to obtain travelling wave solutions to the proposed problem. The structure's geometrical chirality is considered in conjunction with a dynamic chirality, introduced by placing gyroscopes along its central line. It is shown that the interplay between these two chiralities acts as a mechanical analogue of Faraday's law, breaking the symmetry of the associated dispersion diagram.","sentences":["Faraday's law on electromagnetic induction, one of the most fundamental laws of nature, indicates that a change of magnetic field through a coil wire induces a current in the wire.","Electromagnetic induction has many paramount technological applications today, and provides the link between electric and magnetic fields, which is crucial to explain the existence of electromagnetic waves.","In our quest to replicate Faraday's law for mechanical systems, we design an infinite mass-spring \"helix-like structure\", which consists of a helix and a central line, and implement Bloch-Floquet conditions to obtain travelling wave solutions to the proposed problem.","The structure's geometrical chirality is considered in conjunction with a dynamic chirality, introduced by placing gyroscopes along its central line.","It is shown that the interplay between these two chiralities acts as a mechanical analogue of Faraday's law, breaking the symmetry of the associated dispersion diagram."],"url":"http://arxiv.org/abs/2402.12853v1","category":"physics.class-ph"}
{"created":"2024-02-20 09:28:44","title":"Estimation methods for estimands using the treatment policy strategy; a simulation study based on the PIONEER 1 Trial","abstract":"Estimands using the treatment policy strategy for addressing intercurrent events are common in Phase III clinical trials. One estimation approach for this strategy is retrieved dropout whereby observed data following an intercurrent event are used to multiply impute missing data. However, such methods have had issues with variance inflation and model fitting due to data sparsity. This paper introduces likelihood-based versions of these approaches, investigating and comparing their statistical properties to the existing retrieved dropout approaches, simpler analysis models and reference-based multiple imputation. We use a simulation based upon the data from the PIONEER 1 Phase III clinical trial in Type II diabetics to present complex and relevant estimation challenges. The likelihood-based methods display similar statistical properties to their multiple imputation equivalents, but all retrieved dropout approaches suffer from high variance. Retrieved dropout approaches appear less biased than reference-based approaches, resulting in a bias-variance trade-off, but we conclude that the large degree of variance inflation is often more problematic than the bias. Therefore, only the simpler retrieved dropout models appear appropriate as a primary analysis in a clinical trial, and only where it is believed most data following intercurrent events will be observed. The jump-to-reference approach may represent a more promising estimation approach for symptomatic treatments due to its relatively high power and ability to fit in the presence of much missing data, despite its strong assumptions and tendency towards conservative bias. More research is needed to further develop how to estimate the treatment effect for a treatment policy strategy.","sentences":["Estimands using the treatment policy strategy for addressing intercurrent events are common in Phase III clinical trials.","One estimation approach for this strategy is retrieved dropout whereby observed data following an intercurrent event are used to multiply impute missing data.","However, such methods have had issues with variance inflation and model fitting due to data sparsity.","This paper introduces likelihood-based versions of these approaches, investigating and comparing their statistical properties to the existing retrieved dropout approaches, simpler analysis models and reference-based multiple imputation.","We use a simulation based upon the data from the PIONEER 1 Phase III clinical trial in Type II diabetics to present complex and relevant estimation challenges.","The likelihood-based methods display similar statistical properties to their multiple imputation equivalents, but all retrieved dropout approaches suffer from high variance.","Retrieved dropout approaches appear less biased than reference-based approaches, resulting in a bias-variance trade-off, but we conclude that the large degree of variance inflation is often more problematic than the bias.","Therefore, only the simpler retrieved dropout models appear appropriate as a primary analysis in a clinical trial, and only where it is believed most data following intercurrent events will be observed.","The jump-to-reference approach may represent a more promising estimation approach for symptomatic treatments due to its relatively high power and ability to fit in the presence of much missing data, despite its strong assumptions and tendency towards conservative bias.","More research is needed to further develop how to estimate the treatment effect for a treatment policy strategy."],"url":"http://arxiv.org/abs/2402.12850v1","category":"stat.AP"}
{"created":"2024-02-20 09:07:18","title":"Critical thresholds in pressureless Euler--Poisson equations with background states","abstract":"We investigate the critical threshold phenomena in a large class of one dimensional pressureless Euler--Poisson (EP) equations, with non-vanishing background states. First, we establish local-in-time well-posedness in proper regularity spaces, which are adapted for a certain \\textit{neutrality condition} to hold. The neutrality condition is shown to be necessary: we construct smooth solutions that exhibit instantaneous failure of the neutrality condition, which in turn yields non-existence of solutions, even locally in time, in the classical Sobolev spaces $H^s({\\mathbb R})$, $s \\geq 2$. Next, we study the critical threshold phenomena in the neutrality-condition-satisfying pressureless EP systems, where we distinguish between two cases. We prove that in the case of attractive forcing, the neutrality condition can further restrict the sub-critical region into its borderline, namely -- the sub-critical region is reduced to a single line in the phase plane. We then turn to provide a rather definitive answer for the critical thresholds in the case of repulsive EP systems with variable backgrounds. As an application, we analyze the critical thresholds for the damped EP system for cold plasma ion dynamics, where the density of electrons is given by the \\textit{Maxwell--Boltzmann relation}.","sentences":["We investigate the critical threshold phenomena in a large class of one dimensional pressureless Euler--Poisson (EP) equations, with non-vanishing background states.","First, we establish local-in-time well-posedness in proper regularity spaces, which are adapted for a certain \\textit{neutrality condition} to hold.","The neutrality condition is shown to be necessary: we construct smooth solutions that exhibit instantaneous failure of the neutrality condition, which in turn yields non-existence of solutions, even locally in time, in the classical Sobolev spaces $H^s({\\mathbb R})$, $s \\geq 2$.","Next, we study the critical threshold phenomena in the neutrality-condition-satisfying pressureless EP systems, where we distinguish between two cases.","We prove that in the case of attractive forcing, the neutrality condition can further restrict the sub-critical region into its borderline, namely -- the sub-critical region is reduced to a single line in the phase plane.","We then turn to provide a rather definitive answer for the critical thresholds in the case of repulsive EP systems with variable backgrounds.","As an application, we analyze the critical thresholds for the damped EP system for cold plasma ion dynamics, where the density of electrons is given by the \\textit{Maxwell--Boltzmann relation}."],"url":"http://arxiv.org/abs/2402.12839v1","category":"math.AP"}
{"created":"2024-02-20 08:58:39","title":"Integrating Multi-Preconditioned Conjugate Gradient with Additive Multigrid Strategy","abstract":"Due to its optimal complexity, the multigrid (MG) method is one of the most popular approaches for solving large-scale linear systems arising from the discretization of partial differential equations. However, the parallel implementation of standard MG methods, which are inherently multiplicative, suffers from increasing communication complexity. In such cases, the additive variants of MG methods provide a good alternative due to their inherently parallel nature, although they exhibit slower convergence. This work combines the additive multigrid method with the multipreconditioned conjugate gradient (MPCG) method. In the proposed approach, the MPCG method employs the corrections from the different levels of the MG hierarchy as separate preconditioned search directions. In this approach, the MPCG method updates the current iterate by using the linear combination of the preconditioned search directions, where the optimal coefficients for the linear combination are computed by exploiting the energy norm minimization of the CG method. The idea behind our approach is to combine the $A$-conjugacy of the search directions of the MPCG method and the quasi $H_1$-orthogonality of the corrections from the MG hierarchy. In the numerical section, we study the performance of the proposed method compared to the standard additive and multiplicative MG methods used as preconditioners for the CG method.","sentences":["Due to its optimal complexity, the multigrid (MG) method is one of the most popular approaches for solving large-scale linear systems arising from the discretization of partial differential equations.","However, the parallel implementation of standard MG methods, which are inherently multiplicative, suffers from increasing communication complexity.","In such cases, the additive variants of MG methods provide a good alternative due to their inherently parallel nature, although they exhibit slower convergence.","This work combines the additive multigrid method with the multipreconditioned conjugate gradient (MPCG) method.","In the proposed approach, the MPCG method employs the corrections from the different levels of the MG hierarchy as separate preconditioned search directions.","In this approach, the MPCG method updates the current iterate by using the linear combination of the preconditioned search directions, where the optimal coefficients for the linear combination are computed by exploiting the energy norm minimization of the CG method.","The idea behind our approach is to combine the $A$-conjugacy of the search directions of the MPCG method and the quasi $H_1$-orthogonality of the corrections from the MG hierarchy.","In the numerical section, we study the performance of the proposed method compared to the standard additive and multiplicative MG methods used as preconditioners for the CG method."],"url":"http://arxiv.org/abs/2402.12833v1","category":"math.NA"}
{"created":"2024-02-20 08:58:37","title":"Nearly Optimal Fault Tolerant Distance Oracle","abstract":"We present an $f$-fault tolerant distance oracle for an undirected weighted graph where each edge has an integral weight from $[1 \\dots W]$. Given a set $F$ of $f$ edges, as well as a source node $s$ and a destination node $t$, our oracle returns the \\emph{shortest path} from $s$ to $t$ avoiding $F$ in $O((cf \\log (nW))^{O(f^2)})$ time, where $c > 1$ is a constant. The space complexity of our oracle is $O(f^4n^2\\log^2 (nW))$. For a constant $f$, our oracle is nearly optimal both in terms of space and time (barring some logarithmic factor).","sentences":["We present an $f$-fault tolerant distance oracle for an undirected weighted graph where each edge has an integral weight from $[1 \\dots W]$. Given a set $F$ of $f$ edges, as well as a source node $s$ and a destination node $t$, our oracle returns the \\emph{shortest path} from $s$ to $t$ avoiding $F$ in $O((cf \\log (nW))^{O(f^2)})$ time, where $c > 1$ is a constant.","The space complexity of our oracle is $O(f^4n^2\\log^2 (nW))$. For a constant $f$, our oracle is nearly optimal both in terms of space and time (barring some logarithmic factor)."],"url":"http://arxiv.org/abs/2402.12832v1","category":"cs.DS"}
{"created":"2024-02-20 08:58:10","title":"A sparse hierarchical $hp$-finite element method on disks and annuli","abstract":"We develop a sparse hierarchical $hp$-finite element method ($hp$-FEM) for the Helmholtz equation with rotationally invariant variable coefficients posed on a two-dimensional disk or annulus. The mesh is an inner disk cell (omitted if on an annulus domain) and concentric annuli cells. The discretization preserves the Fourier mode decoupling of rotationally invariant operators, such as the Laplacian, which manifests as block diagonal mass and stiffness matrices. Moreover, the matrices have a sparsity pattern independent of the order of the discretization and admit an optimal complexity factorization. The sparse $hp$-FEM can handle radial discontinuities in the right-hand side and in rotationally invariant Helmholtz coefficients. We consider examples such as a high-frequency Helmholtz equation with radial discontinuities, the time-dependent Schr\\\"odinger equation, and an extension to a three-dimensional cylinder domain, with a quasi-optimal solve, via the Alternating Direction Implicit (ADI) algorithm.","sentences":["We develop a sparse hierarchical $hp$-finite element method ($hp$-FEM) for the Helmholtz equation with rotationally invariant variable coefficients posed on a two-dimensional disk or annulus.","The mesh is an inner disk cell (omitted if on an annulus domain) and concentric annuli cells.","The discretization preserves the Fourier mode decoupling of rotationally invariant operators, such as the Laplacian, which manifests as block diagonal mass and stiffness matrices.","Moreover, the matrices have a sparsity pattern independent of the order of the discretization and admit an optimal complexity factorization.","The sparse $hp$-FEM can handle radial discontinuities in the right-hand side and in rotationally invariant Helmholtz coefficients.","We consider examples such as a high-frequency Helmholtz equation with radial discontinuities, the time-dependent Schr\\\"odinger equation, and an extension to a three-dimensional cylinder domain, with a quasi-optimal solve, via the Alternating Direction Implicit (ADI) algorithm."],"url":"http://arxiv.org/abs/2402.12831v1","category":"math.NA"}
{"created":"2024-02-20 08:41:03","title":"ASCEND: Accurate yet Efficient End-to-End Stochastic Computing Acceleration of Vision Transformer","abstract":"Stochastic computing (SC) has emerged as a promising computing paradigm for neural acceleration. However, how to accelerate the state-of-the-art Vision Transformer (ViT) with SC remains unclear. Unlike convolutional neural networks, ViTs introduce notable compatibility and efficiency challenges because of their nonlinear functions, e.g., softmax and Gaussian Error Linear Units (GELU). In this paper, for the first time, a ViT accelerator based on end-to-end SC, dubbed ASCEND, is proposed. ASCEND co-designs the SC circuits and ViT networks to enable accurate yet efficient acceleration. To overcome the compatibility challenges, ASCEND proposes a novel deterministic SC block for GELU and leverages an SC-friendly iterative approximate algorithm to design an accurate and efficient softmax circuit. To improve inference efficiency, ASCEND develops a two-stage training pipeline to produce accurate low-precision ViTs. With extensive experiments, we show the proposed GELU and softmax blocks achieve 56.3% and 22.6% error reduction compared to existing SC designs, respectively and reduce the area-delay product (ADP) by 5.29x and 12.6x, respectively. Moreover, compared to the baseline low-precision ViTs, ASCEND also achieves significant accuracy improvements on CIFAR10 and CIFAR100.","sentences":["Stochastic computing (SC) has emerged as a promising computing paradigm for neural acceleration.","However, how to accelerate the state-of-the-art Vision Transformer (ViT) with SC remains unclear.","Unlike convolutional neural networks, ViTs introduce notable compatibility and efficiency challenges because of their nonlinear functions, e.g., softmax and Gaussian Error Linear Units (GELU).","In this paper, for the first time, a ViT accelerator based on end-to-end SC, dubbed ASCEND, is proposed.","ASCEND co-designs the SC circuits and ViT networks to enable accurate yet efficient acceleration.","To overcome the compatibility challenges, ASCEND proposes a novel deterministic SC block for GELU and leverages an SC-friendly iterative approximate algorithm to design an accurate and efficient softmax circuit.","To improve inference efficiency, ASCEND develops a two-stage training pipeline to produce accurate low-precision ViTs.","With extensive experiments, we show the proposed GELU and softmax blocks achieve 56.3% and 22.6% error reduction compared to existing SC designs, respectively and reduce the area-delay product (ADP) by 5.29x and 12.6x, respectively.","Moreover, compared to the baseline low-precision ViTs, ASCEND also achieves significant accuracy improvements on CIFAR10 and CIFAR100."],"url":"http://arxiv.org/abs/2402.12820v1","category":"eess.SY"}
{"created":"2024-02-20 08:27:30","title":"Principle of least action for quasi-adiabatic state transfers with dissipation","abstract":"We discuss a general formalism to optimize quasi-adiabatic state-transfer protocols, where high fidelity is achieved by maintaining the system in a dark subspace protected from the dominant dissipative channels. We cast the residual fidelity loss, induced by a combination of dissipation and non-adiabatic transitions, in the form of a classical action where the time-dependent control parameters act as coordinates. This allows us to apply the least action principle, yielding the fidelity upper-bound and the corresponding optimal transfer time. As an application, we analyze a system of two qubits subject to weak relaxation and dephasing, interacting through a strongly dissipative quantum bus. In this case, our formalism, we obtain a full characterization of the optimal state-transfer fidelity.","sentences":["We discuss a general formalism to optimize quasi-adiabatic state-transfer protocols, where high fidelity is achieved by maintaining the system in a dark subspace protected from the dominant dissipative channels.","We cast the residual fidelity loss, induced by a combination of dissipation and non-adiabatic transitions, in the form of a classical action where the time-dependent control parameters act as coordinates.","This allows us to apply the least action principle, yielding the fidelity upper-bound and the corresponding optimal transfer time.","As an application, we analyze a system of two qubits subject to weak relaxation and dephasing, interacting through a strongly dissipative quantum bus.","In this case, our formalism, we obtain a full characterization of the optimal state-transfer fidelity."],"url":"http://arxiv.org/abs/2402.12807v1","category":"quant-ph"}
{"created":"2024-02-20 08:23:06","title":"Modular Assurance of Complex Systems Using Contract-Based Design Principles","abstract":"A growing number of safety-critical industries agree that building confidence in complex systems can be achieved through evidence and structured argumentation framed in assurance cases. Nevertheless, assurance cases can easily become too rigorous and difficult to develop and maintain when applied to complex systems. Therefore, we propose to use contract-based development (CBD), a method to manage complexity originally developed in computer science, to simplify assurance cases by modularizing them. This paper will not only summarize relevant previous work such as constructing consistent modular assurance cases using CBD, but more importantly also propose a novel approach to integrate CBD with the argumentation in assurance case modules. This approach will allow interdisciplinary subject-matter and domain experts to build assurance cases together without even knowing about CBD. This helps subject matter experts outside of computer science to reap benefits from CBD and helps with interdisciplinary co-development of assurance cases that cover all the required fields. This paper motivates four rules of thumb aimed to help practitioners developing high-quality modular assurance cases. It also explains how modularization of assurance is an enabler for multi-concern assurance that accounts for the inter-dependency of different concerns such as safety, security and performance.","sentences":["A growing number of safety-critical industries agree that building confidence in complex systems can be achieved through evidence and structured argumentation framed in assurance cases.","Nevertheless, assurance cases can easily become too rigorous and difficult to develop and maintain when applied to complex systems.","Therefore, we propose to use contract-based development (CBD), a method to manage complexity originally developed in computer science, to simplify assurance cases by modularizing them.","This paper will not only summarize relevant previous work such as constructing consistent modular assurance cases using CBD, but more importantly also propose a novel approach to integrate CBD with the argumentation in assurance case modules.","This approach will allow interdisciplinary subject-matter and domain experts to build assurance cases together without even knowing about CBD.","This helps subject matter experts outside of computer science to reap benefits from CBD and helps with interdisciplinary co-development of assurance cases that cover all the required fields.","This paper motivates four rules of thumb aimed to help practitioners developing high-quality modular assurance cases.","It also explains how modularization of assurance is an enabler for multi-concern assurance that accounts for the inter-dependency of different concerns such as safety, security and performance."],"url":"http://arxiv.org/abs/2402.12804v1","category":"cs.LO"}
{"created":"2024-02-20 08:08:19","title":"Symmetry-breaking-induced giant Stark effect in 2D Janus materials","abstract":"Symmetry breaking generally induce exotic physical properties, particularly for low-dimensional materials. Herein we demonstrate that symmetry breaking induces a giant Stark effect in 2D Janus materials using group IV-V monolayers with a four-atom-layer structure as a model system, which are constructed by Ge and As element substitution of symmetrical SnSb monolayer. A linear giant Stark effect is found in Janus semiconductor monolayers, as verified by the band gap variation up to 134 meV of Sn2SbAs monolayer, which is 30 times larger than that of SnSb monolayer (4 meV) when the applied electric field is increased from -0.30 to 0.30 V/{\\AA}. By considering the induced electronic field, we propose a generalized and effective formula that efficiently determines the band gap variation owing to Stark effect. The calculated results from proposed formula are well agreement with those from DFT-HSE06 functional. The giant Stark effect is originated from the large spatial separation of centers of the conduction band minimum and valence band maximum states of Janus structure due to its intrinsic potential gradient. The wide-range tuning of band gap under electronic field shows potential applications of 2D Janus materials in optoelectronic devices.","sentences":["Symmetry breaking generally induce exotic physical properties, particularly for low-dimensional materials.","Herein we demonstrate that symmetry breaking induces a giant Stark effect in 2D Janus materials using group IV-V monolayers with a four-atom-layer structure as a model system, which are constructed by Ge and As element substitution of symmetrical SnSb monolayer.","A linear giant Stark effect is found in Janus semiconductor monolayers, as verified by the band gap variation up to 134 meV of Sn2SbAs monolayer, which is 30 times larger than that of SnSb monolayer (4 meV) when the applied electric field is increased from -0.30 to 0.30 V/{\\AA}.","By considering the induced electronic field, we propose a generalized and effective formula that efficiently determines the band gap variation owing to Stark effect.","The calculated results from proposed formula are well agreement with those from DFT-HSE06 functional.","The giant Stark effect is originated from the large spatial separation of centers of the conduction band minimum and valence band maximum states of Janus structure due to its intrinsic potential gradient.","The wide-range tuning of band gap under electronic field shows potential applications of 2D Janus materials in optoelectronic devices."],"url":"http://arxiv.org/abs/2402.12795v1","category":"physics.app-ph"}
{"created":"2024-02-20 08:07:59","title":"The Harish-Chandra Theorem for Two-parameter Quantum Groups with Even Rank","abstract":"The centre of two-parameter quantum groups $U_{r,s}(\\mathfrak{g})$ is determined through the Harish-Chandra homomorphism. Based on the Rosso form and the representation theory of weight modules, we prove that when rank $\\mathfrak{g}$ is even, the Harish-Chandra homomorphism is an isomorphism, and the centre of the quantum group $\\breve{U}_{r,s}(\\mathfrak{g})$ of the weight lattice type is isomorphic to a polynomial algebra. A system of the canonical generators of the centre can be obtained by the formulas derived in the process.","sentences":["The centre of two-parameter quantum groups $U_{r,s}(\\mathfrak{g})$ is determined through the Harish-Chandra homomorphism.","Based on the Rosso form and the representation theory of weight modules, we prove that when rank $\\mathfrak{g}$ is even, the Harish-Chandra homomorphism is an isomorphism, and the centre of the quantum group $\\breve{U}_{r,s}(\\mathfrak{g})$ of the weight lattice type is isomorphic to a polynomial algebra.","A system of the canonical generators of the centre can be obtained by the formulas derived in the process."],"url":"http://arxiv.org/abs/2402.12793v1","category":"math.QA"}
{"created":"2024-02-20 07:56:02","title":"RhythmFormer: Extracting rPPG Signals Based on Hierarchical Temporal Periodic Transformer","abstract":"Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications such as healthcare, affective computing, anti-spoofing, etc. Due to the periodicity nature of rPPG, the long-range dependency capturing capacity of the Transformer was assumed to be advantageous for such signals. However, existing approaches have not conclusively demonstrated the superior performance of Transformer over traditional convolutional neural network methods, this gap may stem from a lack of thorough exploration of rPPG periodicity. In this paper, we propose RhythmFormer, a fully end-to-end transformer-based method for extracting rPPG signals by explicitly leveraging the quasi-periodic nature of rPPG. The core module, Hierarchical Temporal Periodic Transformer, hierarchically extracts periodic features from multiple temporal scales. It utilizes dynamic sparse attention based on periodicity in the temporal domain, allowing for fine-grained modeling of rPPG features. Furthermore, a fusion stem is proposed to guide self-attention to rPPG features effectively, and it can be easily transferred to existing methods to enhance their performance significantly. RhythmFormer achieves state-of-the-art performance with fewer parameters and reduced computational complexity in comprehensive experiments compared to previous approaches. The codes are available at https://github.com/zizheng-guo/RhythmFormer.","sentences":["Remote photoplethysmography (rPPG) is a non-contact method for detecting physiological signals based on facial videos, holding high potential in various applications such as healthcare, affective computing, anti-spoofing, etc.","Due to the periodicity nature of rPPG, the long-range dependency capturing capacity of the Transformer was assumed to be advantageous for such signals.","However, existing approaches have not conclusively demonstrated the superior performance of Transformer over traditional convolutional neural network methods, this gap may stem from a lack of thorough exploration of rPPG periodicity.","In this paper, we propose RhythmFormer, a fully end-to-end transformer-based method for extracting rPPG signals by explicitly leveraging the quasi-periodic nature of rPPG.","The core module, Hierarchical Temporal Periodic Transformer, hierarchically extracts periodic features from multiple temporal scales.","It utilizes dynamic sparse attention based on periodicity in the temporal domain, allowing for fine-grained modeling of rPPG features.","Furthermore, a fusion stem is proposed to guide self-attention to rPPG features effectively, and it can be easily transferred to existing methods to enhance their performance significantly.","RhythmFormer achieves state-of-the-art performance with fewer parameters and reduced computational complexity in comprehensive experiments compared to previous approaches.","The codes are available at https://github.com/zizheng-guo/RhythmFormer."],"url":"http://arxiv.org/abs/2402.12788v1","category":"cs.CV"}
{"created":"2024-02-20 07:49:30","title":"Understanding and Mitigating the Threat of Vec2Text to Dense Retrieval Systems","abstract":"The introduction of Vec2Text, a technique for inverting text embeddings, has raised serious privacy concerns within dense retrieval systems utilizing text embeddings, including those provided by OpenAI and Cohere. This threat comes from the ability for a malicious attacker with access to text embeddings to reconstruct the original text.   In this paper, we investigate various aspects of embedding models that could influence the recoverability of text using Vec2Text. Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions -- aspects not previously addressed in the original Vec2Text paper. Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between text recoverability and retrieval effectiveness in dense retrieval systems. This analysis provides valuable insights for practitioners involved in designing privacy-aware dense retrieval systems. Additionally, we propose a straightforward fix for embedding transformation that ensures equal ranking effectiveness while mitigating the risk of text recoverability.   Furthermore, we extend the application of Vec2Text to the separate task of corpus poisoning, where, theoretically, Vec2Text presents a more potent threat compared to previous attack methods. Notably, Vec2Text does not require access to the dense retriever's model parameters and can efficiently generate numerous adversarial passages.   In summary, this study highlights the potential threat posed by Vec2Text to existing dense retrieval systems, while also presenting effective methods to patch and strengthen such systems against such risks.","sentences":["The introduction of Vec2Text, a technique for inverting text embeddings, has raised serious privacy concerns within dense retrieval systems utilizing text embeddings, including those provided by OpenAI and Cohere.","This threat comes from the ability for a malicious attacker with access to text embeddings to reconstruct the original text.   ","In this paper, we investigate various aspects of embedding models that could influence the recoverability of text using Vec2Text.","Our exploration involves factors such as distance metrics, pooling functions, bottleneck pre-training, training with noise addition, embedding quantization, and embedding dimensions -- aspects not previously addressed in the original Vec2Text paper.","Through a thorough analysis of these factors, our aim is to gain a deeper understanding of the critical elements impacting the trade-offs between text recoverability and retrieval effectiveness in dense retrieval systems.","This analysis provides valuable insights for practitioners involved in designing privacy-aware dense retrieval systems.","Additionally, we propose a straightforward fix for embedding transformation that ensures equal ranking effectiveness while mitigating the risk of text recoverability.   ","Furthermore, we extend the application of Vec2Text to the separate task of corpus poisoning, where, theoretically, Vec2Text presents a more potent threat compared to previous attack methods.","Notably, Vec2Text does not require access to the dense retriever's model parameters and can efficiently generate numerous adversarial passages.   ","In summary, this study highlights the potential threat posed by Vec2Text to existing dense retrieval systems, while also presenting effective methods to patch and strengthen such systems against such risks."],"url":"http://arxiv.org/abs/2402.12784v1","category":"cs.IR"}
{"created":"2024-02-20 07:33:22","title":"First-Principle Characterization of Structural, Electronic, and Optical Properties of Tin-Halide Monomers","abstract":"The growing interest in tin-halide semiconductors for photovoltaic applications demands an in-depth knowledge of the fundamental properties of its constituents, starting from the smallest monomers entering the initial stages of formation. In this first-principles work based on time-dependent density-functional theory, we investigate the structural, electronic, and optical properties of tin-halide molecules SnX$_n^{2-n}$, with $n=1,2,3,4$ and X = Cl, Br, I, simulating these compounds in vacuo as well as in an implicit solvent. We find that structural properties are very sensitive to the halogen species while the charge distribution is also affected by stoichiometry. The ionicity of the Sn-X bond is confirmed by the Bader charge analysis albeit charge displacement plots point to more complex metal-halide coordination. Particular focus is posed on the neutral molecules SnX$_2$, for which electronic and optical properties are discussed in detail. Band gaps and absorption onset decrease with increasing size of the halogen species and despite general common features, each molecule displays peculiar optical signatures. Our results are elaborated in the context of experimental and theoretical literature, including the more widely studied lead-halide analogs, aiming to contribute with microscopic insight to a better understanding of tin-halide perovskites.","sentences":["The growing interest in tin-halide semiconductors for photovoltaic applications demands an in-depth knowledge of the fundamental properties of its constituents, starting from the smallest monomers entering the initial stages of formation.","In this first-principles work based on time-dependent density-functional theory, we investigate the structural, electronic, and optical properties of tin-halide molecules SnX$_n^{2-n}$, with $n=1,2,3,4$ and X = Cl, Br, I, simulating these compounds in vacuo as well as in an implicit solvent.","We find that structural properties are very sensitive to the halogen species while the charge distribution is also affected by stoichiometry.","The ionicity of the Sn-X bond is confirmed by the Bader charge analysis albeit charge displacement plots point to more complex metal-halide coordination.","Particular focus is posed on the neutral molecules SnX$_2$, for which electronic and optical properties are discussed in detail.","Band gaps and absorption onset decrease with increasing size of the halogen species and despite general common features, each molecule displays peculiar optical signatures.","Our results are elaborated in the context of experimental and theoretical literature, including the more widely studied lead-halide analogs, aiming to contribute with microscopic insight to a better understanding of tin-halide perovskites."],"url":"http://arxiv.org/abs/2402.12778v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 07:20:03","title":"Acknowledgment of Emotional States: Generating Validating Responses for Empathetic Dialogue","abstract":"In the realm of human-AI dialogue, the facilitation of empathetic responses is important. Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions. This study introduces the first framework designed to engender empathetic dialogue with validating responses. Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation. Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules. Further validation of our model's efficacy is confirmed in its application to the TUT Emotional Storytelling Corpus (TESC), a speech-based dialogue dataset, by surpassing both random baseline and the ChatGPT. This consistent performance across both textual and speech-based dialogues underscores the effectiveness of our framework in fostering empathetic human-AI communication.","sentences":["In the realm of human-AI dialogue, the facilitation of empathetic responses is important.","Validation is one of the key communication techniques in psychology, which entails recognizing, understanding, and acknowledging others' emotional states, thoughts, and actions.","This study introduces the first framework designed to engender empathetic dialogue with validating responses.","Our approach incorporates a tripartite module system: 1) validation timing detection, 2) users' emotional state identification, and 3) validating response generation.","Utilizing Japanese EmpatheticDialogues dataset - a textual-based dialogue dataset consisting of 8 emotional categories from Plutchik's wheel of emotions - the Task Adaptive Pre-Training (TAPT) BERT-based model outperforms both random baseline and the ChatGPT performance, in term of F1-score, in all modules.","Further validation of our model's efficacy is confirmed in its application to the TUT Emotional Storytelling Corpus (TESC), a speech-based dialogue dataset, by surpassing both random baseline and the ChatGPT.","This consistent performance across both textual and speech-based dialogues underscores the effectiveness of our framework in fostering empathetic human-AI communication."],"url":"http://arxiv.org/abs/2402.12770v1","category":"cs.CL"}
{"created":"2024-02-20 07:12:09","title":"Attosecond Delays in X-ray Molecular Ionization","abstract":"The photoelectric effect is not truly instantaneous, but exhibits attosecond delays that can reveal complex molecular dynamics. Sub-femtosecond duration light pulses provide the requisite tools to resolve the dynamics of photoionization. Accordingly, the past decade has produced a large volume of work on photoionization delays following single photon absorption of an extreme ultraviolet (XUV) photon. However, the measurement of time-resolved core-level photoionization remained out of reach. The required x-ray photon energies needed for core-level photoionization were not available with attosecond tabletop sources. We have now measured the x-ray photoemission delay of core-level electrons, and here report unexpectedly large delays, ranging up to 700 attoseconds in NO near the oxygen K-shell threshold. These measurements exploit attosecond soft x-ray pulses from a free-electron laser (XFEL) to scan across the entire region near the K-shell threshold. Furthermore, we find the delay spectrum is richly modulated, suggesting several contributions including transient trapping of the photoelectron due to shape resonances, collisions with the Auger-Meitner electron that is emitted in the rapid non-radiative relaxation of the molecule, and multi-electron scattering effects. The results demonstrate how x-ray attosecond experiments, supported by comprehensive theoretical modelling, can unravel the complex correlated dynamics of core-level photoionization.","sentences":["The photoelectric effect is not truly instantaneous, but exhibits attosecond delays that can reveal complex molecular dynamics.","Sub-femtosecond duration light pulses provide the requisite tools to resolve the dynamics of photoionization.","Accordingly, the past decade has produced a large volume of work on photoionization delays following single photon absorption of an extreme ultraviolet (XUV) photon.","However, the measurement of time-resolved core-level photoionization remained out of reach.","The required x-ray photon energies needed for core-level photoionization were not available with attosecond tabletop sources.","We have now measured the x-ray photoemission delay of core-level electrons, and here report unexpectedly large delays, ranging up to 700 attoseconds in NO near the oxygen K-shell threshold.","These measurements exploit attosecond soft x-ray pulses from a free-electron laser (XFEL) to scan across the entire region near the K-shell threshold.","Furthermore, we find the delay spectrum is richly modulated, suggesting several contributions including transient trapping of the photoelectron due to shape resonances, collisions with the Auger-Meitner electron that is emitted in the rapid non-radiative relaxation of the molecule, and multi-electron scattering effects.","The results demonstrate how x-ray attosecond experiments, supported by comprehensive theoretical modelling, can unravel the complex correlated dynamics of core-level photoionization."],"url":"http://arxiv.org/abs/2402.12764v1","category":"physics.atom-ph"}
{"created":"2024-02-20 07:11:27","title":"BronchoTrack: Airway Lumen Tracking for Branch-Level Bronchoscopic Localization","abstract":"Localizing the bronchoscope in real time is essential for ensuring intervention quality. However, most existing methods struggle to balance between speed and generalization. To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.To achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection. We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures. To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \\%, while accessing up to the 4th generation of airways.Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications.","sentences":["Localizing the bronchoscope in real time is essential for ensuring intervention quality.","However, most existing methods struggle to balance between speed and generalization.","To address these challenges, we present BronchoTrack, an innovative real-time framework for accurate branch-level localization, encompassing lumen detection, tracking, and airway association.","To achieve real-time performance, we employ a benchmark lightweight detector for efficient lumen detection.","We are the first to introduce multi-object tracking to bronchoscopic localization, mitigating temporal confusion in lumen identification caused by rapid bronchoscope movement and complex airway structures.","To ensure generalization across patient cases, we propose a training-free detection-airway association method based on a semantic airway graph that encodes the hierarchy of bronchial tree structures.","Experiments on nine patient datasets demonstrate BronchoTrack's localization accuracy of 85.64 \\%, while accessing up to the 4th generation of airways.","Furthermore, we tested BronchoTrack in an in-vivo animal study using a porcine model, where it successfully localized the bronchoscope into the 8th generation airway.","Experimental evaluation underscores BronchoTrack's real-time performance in both satisfying accuracy and generalization, demonstrating its potential for clinical applications."],"url":"http://arxiv.org/abs/2402.12763v1","category":"cs.CV"}
{"created":"2024-02-20 06:49:43","title":"Static vs. Dynamic Databases for Indoor Localization based on Wi-Fi Fingerprinting: A Discussion from a Data Perspective","abstract":"Wi-Fi fingerprinting has emerged as the most popular approach to indoor localization. The use of ML algorithms has greatly improved the localization performance of Wi-Fi fingerprinting, but its success depends on the availability of fingerprint databases composed of a large number of RSSIs, the MAC addresses of access points, and the other measurement information. However, most fingerprint databases do not reflect well the time varying nature of electromagnetic interferences in complicated modern indoor environment. This could result in significant changes in statistical characteristics of training/validation and testing datasets, which are often constructed at different times, and even the characteristics of the testing datasets could be different from those of the data submitted by users during the operation of localization systems after their deployment. In this paper, we consider the implications of time-varying Wi-Fi fingerprints on indoor localization from a data-centric point of view and discuss the differences between static and dynamic databases. As a case study, we have constructed a dynamic database covering three floors of the IR building of XJTLU based on RSSI measurements, over 44 days, and investigated the differences between static and dynamic databases in terms of statistical characteristics and localization performance. The analyses based on variance calculations and Isolation Forest show the temporal shifts in RSSIs, which result in a noticeable trend of the increase in the localization error of a Gaussian process regression model with the maximum error of 6.65 m after 14 days of training without model adjustments. The results of the case study with the XJTLU dynamic database clearly demonstrate the limitations of static databases and the importance of the creation and adoption of dynamic databases for future indoor localization research and real-world deployment.","sentences":["Wi-Fi fingerprinting has emerged as the most popular approach to indoor localization.","The use of ML algorithms has greatly improved the localization performance of Wi-Fi fingerprinting, but its success depends on the availability of fingerprint databases composed of a large number of RSSIs, the MAC addresses of access points, and the other measurement information.","However, most fingerprint databases do not reflect well the time varying nature of electromagnetic interferences in complicated modern indoor environment.","This could result in significant changes in statistical characteristics of training/validation and testing datasets, which are often constructed at different times, and even the characteristics of the testing datasets could be different from those of the data submitted by users during the operation of localization systems after their deployment.","In this paper, we consider the implications of time-varying Wi-Fi fingerprints on indoor localization from a data-centric point of view and discuss the differences between static and dynamic databases.","As a case study, we have constructed a dynamic database covering three floors of the IR building of XJTLU based on RSSI measurements, over 44 days, and investigated the differences between static and dynamic databases in terms of statistical characteristics and localization performance.","The analyses based on variance calculations and Isolation Forest show the temporal shifts in RSSIs, which result in a noticeable trend of the increase in the localization error of a Gaussian process regression model with the maximum error of 6.65 m after 14 days of training without model adjustments.","The results of the case study with the XJTLU dynamic database clearly demonstrate the limitations of static databases and the importance of the creation and adoption of dynamic databases for future indoor localization research and real-world deployment."],"url":"http://arxiv.org/abs/2402.12756v1","category":"cs.LG"}
{"created":"2024-02-20 06:49:31","title":"Observation of Berry curvature in non-Hermitian system from far-field radiation","abstract":"Berry curvature that describes local geometrical properties of energy bands can elucidate many fascinating phenomena in solid-state, photonic, and phononic systems, given its connection to global topological invariants such as the Chern number. Despite its significance, the observation of Berry curvature poses a substantial challenging since wavefunctions are deeply embedded within the system. Here, we theoretically propose a correspondence between the geometry of far-field radiation and the underneath band topology of non-Hermitian systems, thus providing a general method to fully capture the Berry curvature without strongly disturbing the eigenstates. We further experimentally observe the Berry curvature in a honeycomb photonic crystal slab from polarimetry measurements and quantitatively obtain the non-trivial valley Chern number. Our work reveals the feasibility of retrieving the bulk band topology from escaping photons and paves the way to exploring intriguing topological landscapes in non-Hermitian systems.","sentences":["Berry curvature that describes local geometrical properties of energy bands can elucidate many fascinating phenomena in solid-state, photonic, and phononic systems, given its connection to global topological invariants such as the Chern number.","Despite its significance, the observation of Berry curvature poses a substantial challenging since wavefunctions are deeply embedded within the system.","Here, we theoretically propose a correspondence between the geometry of far-field radiation and the underneath band topology of non-Hermitian systems, thus providing a general method to fully capture the Berry curvature without strongly disturbing the eigenstates.","We further experimentally observe the Berry curvature in a honeycomb photonic crystal slab from polarimetry measurements and quantitatively obtain the non-trivial valley Chern number.","Our work reveals the feasibility of retrieving the bulk band topology from escaping photons and paves the way to exploring intriguing topological landscapes in non-Hermitian systems."],"url":"http://arxiv.org/abs/2402.12755v1","category":"physics.optics"}
{"created":"2024-02-20 06:47:12","title":"Fingerprint Presentation Attack Detector Using Global-Local Model","abstract":"The vulnerability of automated fingerprint recognition systems (AFRSs) to presentation attacks (PAs) promotes the vigorous development of PA detection (PAD) technology. However, PAD methods have been limited by information loss and poor generalization ability, resulting in new PA materials and fingerprint sensors. This paper thus proposes a global-local model-based PAD (RTK-PAD) method to overcome those limitations to some extent. The proposed method consists of three modules, called: 1) the global module; 2) the local module; and 3) the rethinking module. By adopting the cut-out-based global module, a global spoofness score predicted from nonlocal features of the entire fingerprint images can be achieved. While by using the texture in-painting-based local module, a local spoofness score predicted from fingerprint patches is obtained. The two modules are not independent but connected through our proposed rethinking module by localizing two discriminative patches for the local module based on the global spoofness score. Finally, the fusion spoofness score by averaging the global and local spoofness scores is used for PAD. Our experimental results evaluated on LivDet 2017 show that the proposed RTK-PAD can achieve an average classification error (ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false detection rate (FDR) equals 1.0%, which significantly outperformed the state-of-the-art methods by $\\sim$10% in terms of TDR (91.19% versus 80.74%).","sentences":["The vulnerability of automated fingerprint recognition systems (AFRSs) to presentation attacks (PAs) promotes the vigorous development of PA detection (PAD) technology.","However, PAD methods have been limited by information loss and poor generalization ability, resulting in new PA materials and fingerprint sensors.","This paper thus proposes a global-local model-based PAD (RTK-PAD) method to overcome those limitations to some extent.","The proposed method consists of three modules, called: 1) the global module; 2) the local module; and 3) the rethinking module.","By adopting the cut-out-based global module, a global spoofness score predicted from nonlocal features of the entire fingerprint images can be achieved.","While by using the texture in-painting-based local module, a local spoofness score predicted from fingerprint patches is obtained.","The two modules are not independent but connected through our proposed rethinking module by localizing two discriminative patches for the local module based on the global spoofness score.","Finally, the fusion spoofness score by averaging the global and local spoofness scores is used for PAD.","Our experimental results evaluated on LivDet 2017 show that the proposed RTK-PAD can achieve an average classification error (ACE) of 2.28% and a true detection rate (TDR) of 91.19% when the false detection rate (FDR) equals 1.0%, which significantly outperformed the state-of-the-art methods by $\\sim$10% in terms of TDR (91.19% versus 80.74%)."],"url":"http://arxiv.org/abs/2402.12754v1","category":"cs.CV"}
{"created":"2024-02-20 06:43:43","title":"Giant enhancement of higher-order harmonics of an optical-tweezer phonon laser","abstract":"Phonon lasers, as mechanical analogues of optical lasers, are unique tools for not only fundamental studies of phononics but also diverse applications such as acoustic imaging and force sensing. Very recently, by levitating a micro-size sphere in an optical tweezer, higher-order mechanical harmonics were observed in the phonon-lasing regime, as the first step towards nonlinear levitated optomechanics [Nat. Phys. 19, 414 (2023)]. However, both the lasing strengths and the quality factors of the observed harmonics are typically very low, thus severely hindering their applications. Here we show that, by applying a simple but powerful electronic control to such a levitated micro-sphere, three orders of magnitude enhancement are achievable in the brightness of the phonon lasers, including both the fundamental mode and all its higher-order harmonics. Also, giant improvements of their linewidth and frequency stability are realized in such an electro-optomechanical system, together with further improved higher-order phonon coherence. These results, as a significant step forward for enhancing and controlling micro-object phonon lasers, can be readily used for a wide range of applications involving nonlinear phonon lasers, such as acoustic frequency comb, ultra-sound sensing, atmospherical monitoring, and even bio-medical diagnosis of levitated micro-size objects.","sentences":["Phonon lasers, as mechanical analogues of optical lasers, are unique tools for not only fundamental studies of phononics but also diverse applications such as acoustic imaging and force sensing.","Very recently, by levitating a micro-size sphere in an optical tweezer, higher-order mechanical harmonics were observed in the phonon-lasing regime, as the first step towards nonlinear levitated optomechanics [Nat. Phys. 19, 414 (2023)].","However, both the lasing strengths and the quality factors of the observed harmonics are typically very low, thus severely hindering their applications.","Here we show that, by applying a simple but powerful electronic control to such a levitated micro-sphere, three orders of magnitude enhancement are achievable in the brightness of the phonon lasers, including both the fundamental mode and all its higher-order harmonics.","Also, giant improvements of their linewidth and frequency stability are realized in such an electro-optomechanical system, together with further improved higher-order phonon coherence.","These results, as a significant step forward for enhancing and controlling micro-object phonon lasers, can be readily used for a wide range of applications involving nonlinear phonon lasers, such as acoustic frequency comb, ultra-sound sensing, atmospherical monitoring, and even bio-medical diagnosis of levitated micro-size objects."],"url":"http://arxiv.org/abs/2402.12753v1","category":"physics.optics"}
{"created":"2024-02-20 06:29:51","title":"Enhanced Physical Layer Security for Full-duplex Symbiotic Radio with AN Generation and Forward Noise Suppression","abstract":"Due to the constraints on power supply and limited encryption capability, data security based on physical layer security (PLS) techniques in backscatter communications has attracted a lot of attention. In this work, we propose to enhance PLS in a full-duplex symbiotic radio (FDSR) system with a proactive eavesdropper, which may overhear the information and interfere legitimate communications simultaneously by emitting attack signals. To deal with the eavesdroppers, we propose a security strategy based on pseudo-decoding and artificial noise (AN) injection to ensure the performance of legitimate communications through forward noise suppression. A novel AN signal generation scheme is proposed using a pseudo-decoding method, where AN signal is superimposed on data signal to safeguard the legitimate channel. The phase control in the forward noise suppression scheme and the power allocation between AN and data signals are optimized to maximize security throughput. The formulated problem can be solved via problem decomposition and alternate optimization algorithms. Simulation results demonstrate the superiority of the proposed scheme in terms of security throughput and attack mitigation performance.","sentences":["Due to the constraints on power supply and limited encryption capability, data security based on physical layer security (PLS) techniques in backscatter communications has attracted a lot of attention.","In this work, we propose to enhance PLS in a full-duplex symbiotic radio (FDSR) system with a proactive eavesdropper, which may overhear the information and interfere legitimate communications simultaneously by emitting attack signals.","To deal with the eavesdroppers, we propose a security strategy based on pseudo-decoding and artificial noise (AN) injection to ensure the performance of legitimate communications through forward noise suppression.","A novel AN signal generation scheme is proposed using a pseudo-decoding method, where AN signal is superimposed on data signal to safeguard the legitimate channel.","The phase control in the forward noise suppression scheme and the power allocation between AN and data signals are optimized to maximize security throughput.","The formulated problem can be solved via problem decomposition and alternate optimization algorithms.","Simulation results demonstrate the superiority of the proposed scheme in terms of security throughput and attack mitigation performance."],"url":"http://arxiv.org/abs/2402.12747v1","category":"cs.NI"}
{"created":"2024-02-20 06:23:36","title":"Near-Optimal Quantum Algorithm for Minimizing the Maximal Loss","abstract":"The problem of minimizing the maximum of $N$ convex, Lipschitz functions plays significant roles in optimization and machine learning. It has a series of results, with the most recent one requiring $O(N\\epsilon^{-2/3} + \\epsilon^{-8/3})$ queries to a first-order oracle to compute an $\\epsilon$-suboptimal point. On the other hand, quantum algorithms for optimization are rapidly advancing with speedups shown on many important optimization problems. In this paper, we conduct a systematic study for quantum algorithms and lower bounds for minimizing the maximum of $N$ convex, Lipschitz functions. On one hand, we develop quantum algorithms with an improved complexity bound of $\\tilde{O}(\\sqrt{N}\\epsilon^{-5/3} + \\epsilon^{-8/3})$. On the other hand, we prove that quantum algorithms must take $\\tilde{\\Omega}(\\sqrt{N}\\epsilon^{-2/3})$ queries to a first order quantum oracle, showing that our dependence on $N$ is optimal up to poly-logarithmic factors.","sentences":["The problem of minimizing the maximum of $N$ convex, Lipschitz functions plays significant roles in optimization and machine learning.","It has a series of results, with the most recent one requiring $O(N\\epsilon^{-2/3} + \\epsilon^{-8/3})$ queries to a first-order oracle to compute an $\\epsilon$-suboptimal point.","On the other hand, quantum algorithms for optimization are rapidly advancing with speedups shown on many important optimization problems.","In this paper, we conduct a systematic study for quantum algorithms and lower bounds for minimizing the maximum of $N$ convex, Lipschitz functions.","On one hand, we develop quantum algorithms with an improved complexity bound of $\\tilde{O}(\\sqrt{N}\\epsilon^{-5/3} + \\epsilon^{-8/3})$. On the other hand, we prove that quantum algorithms must take $\\tilde{\\Omega}(\\sqrt{N}\\epsilon^{-2/3})$ queries to a first order quantum oracle, showing that our dependence on $N$ is optimal up to poly-logarithmic factors."],"url":"http://arxiv.org/abs/2402.12745v1","category":"quant-ph"}
{"created":"2024-02-20 06:00:42","title":"Denoising OCT Images Using Steered Mixture of Experts with Multi-Model Inference","abstract":"In Optical Coherence Tomography (OCT), speckle noise significantly hampers image quality, affecting diagnostic accuracy. Current methods, including traditional filtering and deep learning techniques, have limitations in noise reduction and detail preservation. Addressing these challenges, this study introduces a novel denoising algorithm, Block-Matching Steered-Mixture of Experts with Multi-Model Inference and Autoencoder (BM-SMoE-AE). This method combines block-matched implementation of the SMoE algorithm with an enhanced autoencoder architecture, offering efficient speckle noise reduction while retaining critical image details. Our method stands out by providing improved edge definition and reduced processing time. Comparative analysis with existing denoising techniques demonstrates the superior performance of BM-SMoE-AE in maintaining image integrity and enhancing OCT image usability for medical diagnostics.","sentences":["In Optical Coherence Tomography (OCT), speckle noise significantly hampers image quality, affecting diagnostic accuracy.","Current methods, including traditional filtering and deep learning techniques, have limitations in noise reduction and detail preservation.","Addressing these challenges, this study introduces a novel denoising algorithm, Block-Matching Steered-Mixture of Experts with Multi-Model Inference and Autoencoder (BM-SMoE-AE).","This method combines block-matched implementation of the SMoE algorithm with an enhanced autoencoder architecture, offering efficient speckle noise reduction while retaining critical image details.","Our method stands out by providing improved edge definition and reduced processing time.","Comparative analysis with existing denoising techniques demonstrates the superior performance of BM-SMoE-AE in maintaining image integrity and enhancing OCT image usability for medical diagnostics."],"url":"http://arxiv.org/abs/2402.12735v1","category":"eess.IV"}
{"created":"2024-02-20 05:50:02","title":"Floquet Nonadiabatic Mixed Quantum-Classical Dynamics in Laser-Dressed Solid Systems","abstract":"In this paper, we introduce the Floquet Ehrenfest and Floquet surface hopping approaches to study the nonadiabatic dynamics in the laser-dressed solid systems. We demonstrate that these two approaches can be formulated in both real and reciprocal spaces. Using these approaches, we are able to simulate the interaction between electronic carriers and phonons under periodic drivings, such as strong light-matter interactions. Employing the Holstein and Peierls models, we show that the strong light-matter interactions can effectively modulate the dynamics of electronic population and mobility. Notably, our study demonstrates the feasibility and effectiveness of modeling low-momentum carriers' interactions with phonons using a truncated reciprocal-space basis, an approach impractical in real-space frameworks. Moreover, we reveal that even with significant truncation, carrier populations derived from surface hopping maintain greater accuracy compared to those obtained via meanfield dynamics. These results underscore the potential of our proposed methods in advancing the understanding of carrier-phonon interactions in various laser-dressed materials.","sentences":["In this paper, we introduce the Floquet Ehrenfest and Floquet surface hopping approaches to study the nonadiabatic dynamics in the laser-dressed solid systems.","We demonstrate that these two approaches can be formulated in both real and reciprocal spaces.","Using these approaches, we are able to simulate the interaction between electronic carriers and phonons under periodic drivings, such as strong light-matter interactions.","Employing the Holstein and Peierls models, we show that the strong light-matter interactions can effectively modulate the dynamics of electronic population and mobility.","Notably, our study demonstrates the feasibility and effectiveness of modeling low-momentum carriers' interactions with phonons using a truncated reciprocal-space basis, an approach impractical in real-space frameworks.","Moreover, we reveal that even with significant truncation, carrier populations derived from surface hopping maintain greater accuracy compared to those obtained via meanfield dynamics.","These results underscore the potential of our proposed methods in advancing the understanding of carrier-phonon interactions in various laser-dressed materials."],"url":"http://arxiv.org/abs/2402.12732v1","category":"physics.chem-ph"}
{"created":"2024-02-20 04:49:34","title":"Spurious Correlations in Machine Learning: A Survey","abstract":"Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as \"spurious\" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.","sentences":["Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels.","These features and their correlations with the labels are known as \"spurious\" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness.","In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models.","Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research.","The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains."],"url":"http://arxiv.org/abs/2402.12715v1","category":"cs.LG"}
{"created":"2024-02-20 04:26:08","title":"Are Large Language Models Rational Investors?","abstract":"Large Language Models (LLMs) are progressively being adopted in financial analysis to harness their extensive knowledge base for interpreting complex market data and trends. However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight. This study introduces a novel framework, Financial Bias Indicators (FBI), to critically evaluate the financial rationality of LLMs, focusing on their ability to discern and navigate the subtleties of financial information and to identify any irrational biases that might skew market analysis.   Our research adopts an innovative methodology to measure financial rationality, integrating principles of behavioral finance to scrutinize the biases and decision-making patterns of LLMs. We conduct a comprehensive evaluation of 19 leading LLMs, considering factors such as model scale, training datasets, input strategies, etc. The findings reveal varying degrees of financial irrationality among the models, influenced by their design and training. Models trained specifically on financial datasets might exhibit greater irrationality, and it's possible that even larger financial language models (FinLLMs) could display more biases than smaller, more generalized models. This outcomes provide profound insights into how these elements affect the financial rationality of LLMs, indicating that targeted training and structured input methods could improve model performance. This work enriches our understanding of LLMs' strengths and weaknesses in financial applications, laying the groundwork for the development of more dependable and rational financial analysis tools.","sentences":["Large Language Models (LLMs) are progressively being adopted in financial analysis to harness their extensive knowledge base for interpreting complex market data and trends.","However, their application in the financial domain is challenged by intrinsic biases (i.e., risk-preference bias) and a superficial grasp of market intricacies, underscoring the need for a thorough assessment of their financial insight.","This study introduces a novel framework, Financial Bias Indicators (FBI), to critically evaluate the financial rationality of LLMs, focusing on their ability to discern and navigate the subtleties of financial information and to identify any irrational biases that might skew market analysis.   ","Our research adopts an innovative methodology to measure financial rationality, integrating principles of behavioral finance to scrutinize the biases and decision-making patterns of LLMs.","We conduct a comprehensive evaluation of 19 leading LLMs, considering factors such as model scale, training datasets, input strategies, etc.","The findings reveal varying degrees of financial irrationality among the models, influenced by their design and training.","Models trained specifically on financial datasets might exhibit greater irrationality, and it's possible that even larger financial language models (FinLLMs) could display more biases than smaller, more generalized models.","This outcomes provide profound insights into how these elements affect the financial rationality of LLMs, indicating that targeted training and structured input methods could improve model performance.","This work enriches our understanding of LLMs' strengths and weaknesses in financial applications, laying the groundwork for the development of more dependable and rational financial analysis tools."],"url":"http://arxiv.org/abs/2402.12713v1","category":"cs.CL"}
{"created":"2024-02-20 04:13:59","title":"Integrating Active Learning in Causal Inference with Interference: A Novel Approach in Online Experiments","abstract":"In the domain of causal inference research, the prevalent potential outcomes framework, notably the Rubin Causal Model (RCM), often overlooks individual interference and assumes independent treatment effects. This assumption, however, is frequently misaligned with the intricate realities of real-world scenarios, where interference is not merely a possibility but a common occurrence. Our research endeavors to address this discrepancy by focusing on the estimation of direct and spillover treatment effects under two assumptions: (1) network-based interference, where treatments on neighbors within connected networks affect one's outcomes, and (2) non-random treatment assignments influenced by confounders. To improve the efficiency of estimating potentially complex effects functions, we introduce an novel active learning approach: Active Learning in Causal Inference with Interference (ACI). This approach uses Gaussian process to flexibly model the direct and spillover treatment effects as a function of a continuous measure of neighbors' treatment assignment. The ACI framework sequentially identifies the experimental settings that demand further data. It further optimizes the treatment assignments under the network interference structure using genetic algorithms to achieve efficient learning outcome. By applying our method to simulation data and a Tencent game dataset, we demonstrate its feasibility in achieving accurate effects estimations with reduced data requirements. This ACI approach marks a significant advancement in the realm of data efficiency for causal inference, offering a robust and efficient alternative to traditional methodologies, particularly in scenarios characterized by complex interference patterns.","sentences":["In the domain of causal inference research, the prevalent potential outcomes framework, notably the Rubin Causal Model (RCM), often overlooks individual interference and assumes independent treatment effects.","This assumption, however, is frequently misaligned with the intricate realities of real-world scenarios, where interference is not merely a possibility but a common occurrence.","Our research endeavors to address this discrepancy by focusing on the estimation of direct and spillover treatment effects under two assumptions: (1) network-based interference, where treatments on neighbors within connected networks affect one's outcomes, and (2) non-random treatment assignments influenced by confounders.","To improve the efficiency of estimating potentially complex effects functions, we introduce an novel active learning approach: Active Learning in Causal Inference with Interference (ACI).","This approach uses Gaussian process to flexibly model the direct and spillover treatment effects as a function of a continuous measure of neighbors' treatment assignment.","The ACI framework sequentially identifies the experimental settings that demand further data.","It further optimizes the treatment assignments under the network interference structure using genetic algorithms to achieve efficient learning outcome.","By applying our method to simulation data and a Tencent game dataset, we demonstrate its feasibility in achieving accurate effects estimations with reduced data requirements.","This ACI approach marks a significant advancement in the realm of data efficiency for causal inference, offering a robust and efficient alternative to traditional methodologies, particularly in scenarios characterized by complex interference patterns."],"url":"http://arxiv.org/abs/2402.12710v1","category":"stat.ME"}
{"created":"2024-02-20 04:13:27","title":"On quasiconformal non-equivalence of gasket Julia sets and limit sets","abstract":"This paper studies quasiconformal non-equivalence of Julia sets and limit sets. We proved that any Julia set is quasiconformally different from the Apollonian gasket. We also proved that any Julia set of a quadratic rational map is quasiconformally different from the gasket limit set of a geometrically finite Kleinian group.","sentences":["This paper studies quasiconformal non-equivalence of Julia sets and limit sets.","We proved that any Julia set is quasiconformally different from the Apollonian gasket.","We also proved that any Julia set of a quadratic rational map is quasiconformally different from the gasket limit set of a geometrically finite Kleinian group."],"url":"http://arxiv.org/abs/2402.12709v1","category":"math.DS"}
{"created":"2024-02-20 04:12:40","title":"Quantum computation of conical intersections on a programmable superconducting quantum processor","abstract":"Conical intersections play a vital role in photochemical processes. The standard quantum chemistry approach to study conical intersections between ground and excited states are the state-average multi-configurational methods, which at least require solving an active space problem whose computational cost on classical computers scales exponentially in the worst case. Quantum computing offers an alternative tool to solve this problem, however, its applicability to study conical intersections, in particular, on real quantum hardware remains to be explored. In this work, we realize a hybrid quantum-classical state-average complete active space self-consistent field method based on the variational quantum eigensolver (VQE-SA-CASSCF) for the first time on a programmable superconducting quantum processor, and applied it to study conical intersections of two prototypical systems - ethylene (C2H4) and triatomic hydrogen (H3). We show that a combination of different strategies can lead to a qualitatively correct reproduction of conical intersections using VQE-SA-CASSCF, including improving the stability of quantum hardware, reducing the depth of variational circuits, grouping Pauli terms to minimize measurements, and applying appropriate error mitigation. These results allow us to identify the challenges to be overcome in the future and pave the way for using quantum computers to study conical intersections of more complex systems.","sentences":["Conical intersections play a vital role in photochemical processes.","The standard quantum chemistry approach to study conical intersections between ground and excited states are the state-average multi-configurational methods, which at least require solving an active space problem whose computational cost on classical computers scales exponentially in the worst case.","Quantum computing offers an alternative tool to solve this problem, however, its applicability to study conical intersections, in particular, on real quantum hardware remains to be explored.","In this work, we realize a hybrid quantum-classical state-average complete active space self-consistent field method based on the variational quantum eigensolver (VQE-SA-CASSCF) for the first time on a programmable superconducting quantum processor, and applied it to study conical intersections of two prototypical systems - ethylene (C2H4) and triatomic hydrogen (H3).","We show that a combination of different strategies can lead to a qualitatively correct reproduction of conical intersections using VQE-SA-CASSCF, including improving the stability of quantum hardware, reducing the depth of variational circuits, grouping Pauli terms to minimize measurements, and applying appropriate error mitigation.","These results allow us to identify the challenges to be overcome in the future and pave the way for using quantum computers to study conical intersections of more complex systems."],"url":"http://arxiv.org/abs/2402.12708v1","category":"quant-ph"}
{"created":"2024-02-20 04:09:58","title":"Learning Domain-Invariant Temporal Dynamics for Few-Shot Action Recognition","abstract":"Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples. Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model. Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance). We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for knowledge transfer. To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training. Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders. Then we pre-train the model with the self-supervised signals to learn the representation. After that, we fix the whole representation model and tune the classifier. During adaptation, we fix the transferable temporal dynamics and update the image encoder. The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard few-shot action recognition datasets. Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities.","sentences":["Few-shot action recognition aims at quickly adapting a pre-trained model to the novel data with a distribution shift using only a limited number of samples.","Key challenges include how to identify and leverage the transferable knowledge learned by the pre-trained model.","Our central hypothesis is that temporal invariance in the dynamic system between latent variables lends itself to transferability (domain-invariance).","We therefore propose DITeD, or Domain-Invariant Temporal Dynamics for knowledge transfer.","To detect the temporal invariance part, we propose a generative framework with a two-stage training strategy during pre-training.","Specifically, we explicitly model invariant dynamics including temporal dynamic generation and transitions, and the variant visual and domain encoders.","Then we pre-train the model with the self-supervised signals to learn the representation.","After that, we fix the whole representation model and tune the classifier.","During adaptation, we fix the transferable temporal dynamics and update the image encoder.","The efficacy of our approach is revealed by the superior accuracy of DITeD over leading alternatives across standard few-shot action recognition datasets.","Moreover, we validate that the learned temporal dynamic transition and temporal dynamic generation modules possess transferable qualities."],"url":"http://arxiv.org/abs/2402.12706v1","category":"cs.CV"}
{"created":"2024-02-20 03:55:48","title":"Positive temperature-dependent thermal conductivity induced by wavelike phonons in complex Ag-based argyrodites","abstract":"The phonon transport mechanisms and the anomalous temperature-dependent lattice thermal conductivities (kL) in Ag-based argyrodites have not been fully understood. Herein, we systematically study the phonon thermal transport of five Ag-based crystalline argyrodites Ag7PS6, Ag7AsS6, Ag8SnS6, Ag8GeS6 and Ag9GaS6 utilizing perturbation theory and the unified theory thermal transport model. Our results show that, as the complexity of the unit cell increases, the proportion of the population terms falls while the coherence contributions become more significant, leading to the relatively weak temperature-dependent kL of Ag7PS6 and Ag7AsS6, while the more complex crystalline argyrodites, Ag8SnS6, Ag8GeS6 and Ag9GaS6, exhibiting a glass-like behavior in their temperature dependence of kL. We attribute the positive temperature-dependent and ultralow kL of Ag8SnS6, Ag8GeS6 and Ag9GaS6 to the dominance of wavelike phonons and the strong phonon broadening. Furthermore, using laser flash measurements and the homogeneous non-equilibrium molecular dynamics simulations based on accurate machine learning neuroevolution potentials, we provide further evidence for the glass-like temperature-dependent kL of Ag8SnS6 and Ag8GeS6.","sentences":["The phonon transport mechanisms and the anomalous temperature-dependent lattice thermal conductivities (kL) in Ag-based argyrodites have not been fully understood.","Herein, we systematically study the phonon thermal transport of five Ag-based crystalline argyrodites Ag7PS6, Ag7AsS6, Ag8SnS6, Ag8GeS6 and Ag9GaS6 utilizing perturbation theory and the unified theory thermal transport model.","Our results show that, as the complexity of the unit cell increases, the proportion of the population terms falls while the coherence contributions become more significant, leading to the relatively weak temperature-dependent kL of Ag7PS6 and Ag7AsS6, while the more complex crystalline argyrodites, Ag8SnS6, Ag8GeS6 and Ag9GaS6, exhibiting a glass-like behavior in their temperature dependence of kL. We attribute the positive temperature-dependent and ultralow kL of Ag8SnS6, Ag8GeS6 and Ag9GaS6 to the dominance of wavelike phonons and the strong phonon broadening.","Furthermore, using laser flash measurements and the homogeneous non-equilibrium molecular dynamics simulations based on accurate machine learning neuroevolution potentials, we provide further evidence for the glass-like temperature-dependent kL of Ag8SnS6 and Ag8GeS6."],"url":"http://arxiv.org/abs/2402.12699v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-20 03:54:50","title":"Dynamics of quantum coherence in many-body localized systems","abstract":"We demonstrate that the dynamics of quantum coherence serves as an effective probe for identifying dephasing, which is a distinctive signature of many-body localization (MBL). Quantum coherence can be utilized to measure both the local coherence of specific subsystems and the total coherence of the whole system in a consistent manner. Our results reveal that the local coherence of small subsystems decays over time following a power law in the MBL phase, while it reaches a stable value within the same time window in the Anderson localized (AL) phase. In contrast, the total coherence of the whole system exhibits logarithmic growth during the MBL phase and reaches a stable value in the AL phase. Notably, this dynamic characteristic of quantum coherence remains robust even with weak interactions and displays unbounded behavior in infinite systems. Our results provide insights into understanding many-body dephasing phenomena in MBL systems and propose a novel feasible method for identifying and characterizing MBL phases in experiments.","sentences":["We demonstrate that the dynamics of quantum coherence serves as an effective probe for identifying dephasing, which is a distinctive signature of many-body localization (MBL).","Quantum coherence can be utilized to measure both the local coherence of specific subsystems and the total coherence of the whole system in a consistent manner.","Our results reveal that the local coherence of small subsystems decays over time following a power law in the MBL phase, while it reaches a stable value within the same time window in the Anderson localized (AL) phase.","In contrast, the total coherence of the whole system exhibits logarithmic growth during the MBL phase and reaches a stable value in the AL phase.","Notably, this dynamic characteristic of quantum coherence remains robust even with weak interactions and displays unbounded behavior in infinite systems.","Our results provide insights into understanding many-body dephasing phenomena in MBL systems and propose a novel feasible method for identifying and characterizing MBL phases in experiments."],"url":"http://arxiv.org/abs/2402.12698v1","category":"quant-ph"}
{"created":"2024-02-20 18:56:17","title":"Quantized shift response in multi-gap topological phases","abstract":"We show that certain 3D multi-gap topological insulators can host quantized shift photoconductivities due to bulk invariants that are defined under reality conditions imposed by additional symmetries. We recast the quantization in terms of the integrated torsion tensor and the non-Abelian Berry connection constituting Chern-Simons forms. Physically, we recognize that the topological quantization emerges purely from virtual transitions contributing to the optical response. Our findings provide another quantized electromagnetic DC response due to the non-trivial band topology, beyond the quantum anomalous Hall effect of Chern insulators and quantized circular photogalvanic effect found in Weyl semimetals.","sentences":["We show that certain 3D multi-gap topological insulators can host quantized shift photoconductivities due to bulk invariants that are defined under reality conditions imposed by additional symmetries.","We recast the quantization in terms of the integrated torsion tensor and the non-Abelian Berry connection constituting Chern-Simons forms.","Physically, we recognize that the topological quantization emerges purely from virtual transitions contributing to the optical response.","Our findings provide another quantized electromagnetic DC response due to the non-trivial band topology, beyond the quantum anomalous Hall effect of Chern insulators and quantized circular photogalvanic effect found in Weyl semimetals."],"url":"http://arxiv.org/abs/2402.13245v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 18:55:09","title":"VADv2: End-to-End Vectorized Autonomous Driving via Probabilistic Planning","abstract":"Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging. In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning. VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle. Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods. It runs stably in a fully end-to-end manner, even without the rule-based wrapper. Closed-loop demos are presented at https://hgao-cv.github.io/VADv2.","sentences":["Learning a human-like driving policy from large-scale driving demonstrations is promising, but the uncertainty and non-deterministic nature of planning make it challenging.","In this work, to cope with the uncertainty problem, we propose VADv2, an end-to-end driving model based on probabilistic planning.","VADv2 takes multi-view image sequences as input in a streaming manner, transforms sensor data into environmental token embeddings, outputs the probabilistic distribution of action, and samples one action to control the vehicle.","Only with camera sensors, VADv2 achieves state-of-the-art closed-loop performance on the CARLA Town05 benchmark, significantly outperforming all existing methods.","It runs stably in a fully end-to-end manner, even without the rule-based wrapper.","Closed-loop demos are presented at https://hgao-cv.github.io/VADv2."],"url":"http://arxiv.org/abs/2402.13243v1","category":"cs.CV"}
{"created":"2024-02-20 18:41:11","title":"Online Matching on $3$-Uniform Hypergraphs","abstract":"The online matching problem was introduced by Karp, Vazirani and Vazirani (STOC 1990) on bipartite graphs with vertex arrivals. It is well-known that the optimal competitive ratio is $1-1/e$ for both integral and fractional versions of the problem. Since then, there has been considerable effort to find optimal competitive ratios for other related settings. In this work, we go beyond the graph case and study the online matching problem on $k$-uniform hypergraphs. For $k=3$, we provide an optimal primal-dual fractional algorithm, which achieves a competitive ratio of $(e-1)/(e+1)\\approx 0.4621$. As our main technical contribution, we present a carefully constructed adversarial instance, which shows that this ratio is in fact optimal. It combines ideas from known hard instances for bipartite graphs under the edge-arrival and vertex-arrival models. For $k\\geq 3$, we give a simple integral algorithm which performs better than greedy when the online nodes have bounded degree. As a corollary, it achieves the optimal competitive ratio of 1/2 on 3-uniform hypergraphs when every online node has degree at most 2. This is because the special case where every online node has degree 1 is equivalent to the edge-arrival model on graphs, for which an upper bound of 1/2 is known.","sentences":["The online matching problem was introduced by Karp, Vazirani and Vazirani (STOC 1990) on bipartite graphs with vertex arrivals.","It is well-known that the optimal competitive ratio is $1-1/e$ for both integral and fractional versions of the problem.","Since then, there has been considerable effort to find optimal competitive ratios for other related settings.","In this work, we go beyond the graph case and study the online matching problem on $k$-uniform hypergraphs.","For $k=3$, we provide an optimal primal-dual fractional algorithm, which achieves a competitive ratio of $(e-1)/(e+1)\\approx 0.4621$. As our main technical contribution, we present a carefully constructed adversarial instance, which shows that this ratio is in fact optimal.","It combines ideas from known hard instances for bipartite graphs under the edge-arrival and vertex-arrival models.","For $k\\geq 3$, we give a simple integral algorithm which performs better than greedy when the online nodes have bounded degree.","As a corollary, it achieves the optimal competitive ratio of 1/2 on 3-uniform hypergraphs when every online node has degree at most 2.","This is because the special case where every online node has degree 1 is equivalent to the edge-arrival model on graphs, for which an upper bound of 1/2 is known."],"url":"http://arxiv.org/abs/2402.13227v1","category":"cs.DS"}
{"created":"2024-02-20 18:31:27","title":"How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts","abstract":"The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions. To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion. We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%. We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question. Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts.","sentences":["The remarkable advancements in Multimodal Large Language Models (MLLMs) have not rendered them immune to challenges, particularly in the context of handling deceptive information in prompts, thus producing hallucinated responses under such conditions.","To quantitatively assess this vulnerability, we present MAD-Bench, a carefully curated benchmark that contains 850 test samples divided into 6 categories, such as non-existent objects, count of objects, spatial relationship, and visual confusion.","We provide a comprehensive analysis of popular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as LLaVA-1.5 and CogVLM.","Empirically, we observe significant performance gaps between GPT-4V and other models; and previous robust instruction-tuned models, such as LRV-Instruction and LLaVA-RLHF, are not effective on this new benchmark.","While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of any other model in our experiments ranges from 5% to 35%.","We further propose a remedy that adds an additional paragraph to the deceptive prompts to encourage models to think twice before answering the question.","Surprisingly, this simple method can even double the accuracy; however, the absolute numbers are still too low to be satisfactory.","We hope MAD-Bench can serve as a valuable benchmark to stimulate further research to enhance models' resilience against deceptive prompts."],"url":"http://arxiv.org/abs/2402.13220v1","category":"cs.CV"}
{"created":"2024-02-20 18:20:59","title":"Bayesian Reward Models for LLM Alignment","abstract":"To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data. We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback). However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference. This is especially problematic as the prompt or response diverges from the training data. It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution. Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimization in best-of-n sampling.","sentences":["To ensure that large language model (LLM) responses are helpful and non-toxic, we usually fine-tune a reward model on human preference data.","We then select policy responses with high rewards (best-of-n sampling) or further optimize the policy to produce responses with high rewards (reinforcement learning from human feedback).","However, this process is vulnerable to reward overoptimization or hacking, in which the responses selected have high rewards due to errors in the reward model rather than a genuine preference.","This is especially problematic as the prompt or response diverges from the training data.","It should be possible to mitigate these issues by training a Bayesian reward model, which signals higher uncertainty further from the training data distribution.","Therefore, we trained Bayesian reward models using Laplace-LoRA (Yang et al., 2024) and found that the resulting uncertainty estimates can successfully mitigate reward overoptimization in best-of-n sampling."],"url":"http://arxiv.org/abs/2402.13210v1","category":"cs.LG"}
{"created":"2024-02-20 18:15:20","title":"Momentum-space Observation of Optically Excited Non-Thermal Electrons in Graphene with Persistent Pseudospin Polarization","abstract":"The unique optical properties of graphene, with broadband absorption and ultrafast response, make it a critical component of optoelectronic and spintronic devices. Using time-resolved momentum microscopy with high data rate and high dynamic range, we report momentum-space measurements of electrons promoted to the graphene conduction band with visible light, and their subsequent relaxation. We observe a pronounced non-thermal distribution of nascent photoexcited electrons with lattice pseudospin polarization in remarkable agreement with results of simple tight-binding theory. By varying the excitation fluence, we vary the relative importance of electron-electron vs. electron-phonon scattering in the relaxation of the initial distribution. Increasing the excitation fluence results in increased noncollinear electron-electron scattering and reduced pseudospin polarization, although up-scattered electrons retain a degree of polarization. These detailed momentum-resolved electron dynamics in graphene demonstrate the capabilities of high-performance time-resolved momentum microscopy in the study of 2D materials and can inform the design of graphene devices.","sentences":["The unique optical properties of graphene, with broadband absorption and ultrafast response, make it a critical component of optoelectronic and spintronic devices.","Using time-resolved momentum microscopy with high data rate and high dynamic range, we report momentum-space measurements of electrons promoted to the graphene conduction band with visible light, and their subsequent relaxation.","We observe a pronounced non-thermal distribution of nascent photoexcited electrons with lattice pseudospin polarization in remarkable agreement with results of simple tight-binding theory.","By varying the excitation fluence, we vary the relative importance of electron-electron vs. electron-phonon scattering in the relaxation of the initial distribution.","Increasing the excitation fluence results in increased noncollinear electron-electron scattering and reduced pseudospin polarization, although up-scattered electrons retain a degree of polarization.","These detailed momentum-resolved electron dynamics in graphene demonstrate the capabilities of high-performance time-resolved momentum microscopy in the study of 2D materials and can inform the design of graphene devices."],"url":"http://arxiv.org/abs/2402.13205v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 18:09:54","title":"Un crible minorant effectif pour les entiers friables","abstract":"Let $\\mathcal{A}$ be a finite set of positive integers and $y\\geq 1$. We give an effective lower bound of the cardinality of the set $\\{n\\in\\mathcal{A};\\,p|n\\Longrightarrow p\\leq y\\}$ under the condition of a good knowledge of the level of distribution of the set $\\mathcal{A}$. Some consequences are studied: an application to the smooth values of irreducible polynomials or binary forms with integer coefficients, and an application to shifted smooth numbers.","sentences":["Let $\\mathcal{A}$ be a finite set of positive integers and $y\\geq 1$.","We give an effective lower bound of the cardinality of the set $\\{n\\in\\mathcal{A};\\,p|n\\Longrightarrow p\\leq y\\}$ under the condition of a good knowledge of the level of distribution of the set $\\mathcal{A}$. Some consequences are studied: an application to the smooth values of irreducible polynomials or binary forms with integer coefficients, and an application to shifted smooth numbers."],"url":"http://arxiv.org/abs/2402.13198v1","category":"math.NT"}
{"created":"2024-02-20 17:49:10","title":"Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness","abstract":"We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space. Each agent sequentially queries the function to obtain noisy observations at the query points. Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents. We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server. Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication.","sentences":["We consider distributed kernel bandits where $N$ agents aim to collaboratively maximize an unknown reward function that lies in a reproducing kernel Hilbert space.","Each agent sequentially queries the function to obtain noisy observations at the query points.","Agents can share information through a central server, with the objective of minimizing regret that is accumulating over time $T$ and aggregating over agents.","We develop the first algorithm that achieves the optimal regret order (as defined by centralized learning) with a communication cost that is sublinear in both $N$ and $T$. The key features of the proposed algorithm are the uniform exploration at the local agents and shared randomness with the central server.","Working together with the sparse approximation of the GP model, these two key components make it possible to preserve the learning rate of the centralized setting at a diminishing rate of communication."],"url":"http://arxiv.org/abs/2402.13182v1","category":"cs.LG"}
{"created":"2024-02-20 17:16:22","title":"Regret-Minimizing Contracts: Agency Under Uncertainty","abstract":"We study the fundamental problem of designing contracts in principal-agent problems under uncertainty. Previous works mostly addressed Bayesian settings in which principal's uncertainty is modeled as a probability distribution over agent's types. In this paper, we study a setting in which the principal has no distributional information about agent's type. In particular, in our setting, the principal only knows some uncertainty set defining possible agent's action costs. Thus, the principal takes a robust (adversarial) approach by trying to design contracts which minimize the (additive) regret: the maximum difference between what the principal could have obtained had them known agent's costs and what they actually get under the selected contract.","sentences":["We study the fundamental problem of designing contracts in principal-agent problems under uncertainty.","Previous works mostly addressed Bayesian settings in which principal's uncertainty is modeled as a probability distribution over agent's types.","In this paper, we study a setting in which the principal has no distributional information about agent's type.","In particular, in our setting, the principal only knows some uncertainty set defining possible agent's action costs.","Thus, the principal takes a robust (adversarial) approach by trying to design contracts which minimize the (additive) regret: the maximum difference between what the principal could have obtained had them known agent's costs and what they actually get under the selected contract."],"url":"http://arxiv.org/abs/2402.13156v1","category":"cs.GT"}
{"created":"2024-02-20 17:10:42","title":"Clustered Planarity Variants for Level Graphs","abstract":"We consider variants of the clustered planarity problem for level-planar drawings. So far, only convex clusters have been studied in this setting. We introduce two new variants that both insist on a level-planar drawing of the input graph but relax the requirements on the shape of the clusters. In unrestricted Clustered Level Planarity (uCLP) we only require that they are bounded by simple closed curves that enclose exactly the vertices of the cluster and cross each edge of the graph at most once. The problem y-monotone Clustered Level Planarity (y-CLP) requires that additionally it must be possible to augment each cluster with edges that do not cross the cluster boundaries so that it becomes connected while the graph remains level-planar, thereby mimicking a classic characterization of clustered planarity in the level-planar setting.   We give a polynomial-time algorithm for uCLP if the input graph is biconnected and has a single source. By contrast, we show that y-CLP is hard under the same restrictions and it remains NP-hard even if the number of levels is bounded by a constant and there is only a single non-trivial cluster.","sentences":["We consider variants of the clustered planarity problem for level-planar drawings.","So far, only convex clusters have been studied in this setting.","We introduce two new variants that both insist on a level-planar drawing of the input graph but relax the requirements on the shape of the clusters.","In unrestricted Clustered Level Planarity (uCLP) we only require that they are bounded by simple closed curves that enclose exactly the vertices of the cluster and cross each edge of the graph at most once.","The problem y-monotone Clustered Level Planarity (y-CLP) requires that additionally it must be possible to augment each cluster with edges that do not cross the cluster boundaries so that it becomes connected while the graph remains level-planar, thereby mimicking a classic characterization of clustered planarity in the level-planar setting.   ","We give a polynomial-time algorithm for uCLP if the input graph is biconnected and has a single source.","By contrast, we show that y-CLP is hard under the same restrictions and it remains NP-hard even if the number of levels is bounded by a constant and there is only a single non-trivial cluster."],"url":"http://arxiv.org/abs/2402.13153v1","category":"cs.CG"}
{"created":"2024-02-20 16:55:33","title":"Some new isoclasses of one-parameter exotic small quantum groups arising from the two-parameter setting","abstract":"The classification of one-parameter small quantum groups is an interesting open question. The current paper reveals a new phenomenon that there exist abundant exotic (about 5 times the standard) small quantum groups beyond the Lusztig small quantum groups (with double grouplikes) , which arise from the two-parameter setting. In particular, when the order $\\ell$ of one-parameter $q$ satisfies $(\\ell, 210)\\not=1$, the isoclasses of explicit representatives (most of them have Drinfeld doubles) are new finite dimensional pointed Hopf algebras, which complement to the work under the assumption $(\\ell, 210)=1$","sentences":["The classification of one-parameter small quantum groups is an interesting open question.","The current paper reveals a new phenomenon that there exist abundant exotic (about 5 times the standard) small quantum groups beyond the Lusztig small quantum groups (with double grouplikes) , which arise from the two-parameter setting.","In particular, when the order $\\ell$ of one-parameter $q$ satisfies $(\\ell, 210)\\not=1$, the isoclasses of explicit representatives (most of them have Drinfeld doubles) are new finite dimensional pointed Hopf algebras, which complement to the work under the assumption $(\\ell, 210)=1$"],"url":"http://arxiv.org/abs/2402.13141v1","category":"math.QA"}
{"created":"2024-02-20 16:43:05","title":"Magnetic transitions of biphenylene network layers induced by external perturbations","abstract":"We present a comprehensive investigation of the magnetic ordering in biphenylene network (BPN) layers, employing density functional theory (DFT) calculations under external perturbations, including uniaxial strains and hole doping. We compute fully relaxed structures, energy bands, and magnetic states by performing DFT calculations augmented with extended Hubbard interactions, encompassing both on-site and inter-site interactions, to accurately capture electron correlations. We emphasize the importance of the extended Hubbard forces by contrasting BPN layers with and without the forces. Our results reveal that in their fully relaxed structures, both BPN monolayer and bilayer are non-magnetic. We exploit external perturbations to induce magnetic ordering. The application of uniaxial strains induces magnetic phase transitions, leading to ferrimagnetic and antiferromagnetic states in BPN monolayer and bilayer, respectively. Additionally, we investigate hole doping as an alternative mechanism for inducing magnetic transitions. Our findings shed light on the tunability of magnetic properties in BPN layers through external perturbations, demonstrating the promise of low-dimensional materials in future spintronics and nanoelectronic applications.","sentences":["We present a comprehensive investigation of the magnetic ordering in biphenylene network (BPN) layers, employing density functional theory (DFT) calculations under external perturbations, including uniaxial strains and hole doping.","We compute fully relaxed structures, energy bands, and magnetic states by performing DFT calculations augmented with extended Hubbard interactions, encompassing both on-site and inter-site interactions, to accurately capture electron correlations.","We emphasize the importance of the extended Hubbard forces by contrasting BPN layers with and without the forces.","Our results reveal that in their fully relaxed structures, both BPN monolayer and bilayer are non-magnetic.","We exploit external perturbations to induce magnetic ordering.","The application of uniaxial strains induces magnetic phase transitions, leading to ferrimagnetic and antiferromagnetic states in BPN monolayer and bilayer, respectively.","Additionally, we investigate hole doping as an alternative mechanism for inducing magnetic transitions.","Our findings shed light on the tunability of magnetic properties in BPN layers through external perturbations, demonstrating the promise of low-dimensional materials in future spintronics and nanoelectronic applications."],"url":"http://arxiv.org/abs/2402.13129v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 16:40:33","title":"Explicit upper bounds on the average of Euler-Kronecker constants of narrow ray class fields","abstract":"For a number field $K$, the Euler-Kronecker constant $\\gamma_K$ associated to $K$ is an arithmetic invariant the size and nature of which is linked to some of the deepest questions in number theory. This theme was given impetus by Ihara who obtained bounds, both unconditional as well as under GRH for Dedekind zeta functions. In this note, we study the analogous constants associated to the narrow ray class fields of an imaginary quadratic field. Our goal for studying such families is twofold. First to show that for such families, the conditional bounds obtained by Ihara can be improved on the average, again under GRH for Dedekind zeta functions. Further, our family of number fields are non-abelian while such average bounds have earlier been studied for cyclotomic fields. The technical part of our work is to make the dependence of these upper bounds on the ambient number field explicit. Such explicit dependence is essential to further our objective.","sentences":["For a number field $K$, the Euler-Kronecker constant $\\gamma_K$ associated to $K$ is an arithmetic invariant the size and nature of which is linked to some of the deepest questions in number theory.","This theme was given impetus by Ihara who obtained bounds, both unconditional as well as under GRH for Dedekind zeta functions.","In this note, we study the analogous constants associated to the narrow ray class fields of an imaginary quadratic field.","Our goal for studying such families is twofold.","First to show that for such families, the conditional bounds obtained by Ihara can be improved on the average, again under GRH for Dedekind zeta functions.","Further, our family of number fields are non-abelian while such average bounds have earlier been studied for cyclotomic fields.","The technical part of our work is to make the dependence of these upper bounds on the ambient number field explicit.","Such explicit dependence is essential to further our objective."],"url":"http://arxiv.org/abs/2402.13127v1","category":"math.NT"}
{"created":"2024-02-20 16:35:14","title":"Cross-Domain Transfer Learning with CoRTe: Consistent and Reliable Transfer from Black-Box to Lightweight Segmentation Model","abstract":"Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware. Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data. Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons. In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to black-box source model predictions. Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the black-box source model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels. We benchmark CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using black-box models to transfer knowledge on lightweight models for a target data distribution.","sentences":["Many practical applications require training of semantic segmentation models on unlabelled datasets and their execution on low-resource hardware.","Distillation from a trained source model may represent a solution for the first but does not account for the different distribution of the training data.","Unsupervised domain adaptation (UDA) techniques claim to solve the domain shift, but in most cases assume the availability of the source data or an accessible white-box source model, which in practical applications are often unavailable for commercial and/or safety reasons.","In this paper, we investigate a more challenging setting in which a lightweight model has to be trained on a target unlabelled dataset for semantic segmentation, under the assumption that we have access only to black-box source model predictions.","Our method, named CoRTe, consists of (i) a pseudo-labelling function that extracts reliable knowledge from the black-box source model using its relative confidence, (ii) a pseudo label refinement method to retain and enhance the novel information learned by the student model on the target data, and (iii) a consistent training of the model using the extracted pseudo labels.","We benchmark CoRTe on two synthetic-to-real settings, demonstrating remarkable results when using black-box models to transfer knowledge on lightweight models for a target data distribution."],"url":"http://arxiv.org/abs/2402.13122v1","category":"cs.CV"}
{"created":"2024-02-20 16:01:42","title":"On the Stability of Gradient Descent for Large Learning Rate","abstract":"There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset in parameter space. Additionally, we prove that if the step-size is too big, then the set of initializations from which gradient descent converges to a critical point has measure zero.","sentences":["There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate).","Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets.","In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption.","More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset in parameter space.","Additionally, we prove that if the step-size is too big, then the set of initializations from which gradient descent converges to a critical point has measure zero."],"url":"http://arxiv.org/abs/2402.13108v1","category":"cs.LG"}
{"created":"2024-02-20 15:11:31","title":"Design of narrowband infrared emitters by hybridizing guided-mode resonance structures with van der Waals materials","abstract":"In this paper, narrowband emitters have been designed using particle swarm optimization (PSO) in the 10-20 {\\mu}m infrared range. The device structure consists of an anisotropic {\\alpha}-MoO3 layer combined with the one- and two-dimensional guided-mode resonance structures. Well-defined absorption lines are present in the reflection spectrum for both TE and TM polarizations, thereby yielding narrowband emissivity at desired wavelengths. The band structure of the designed emitters under TM polarization demonstrates distinct features unlike its TE counterpart. These features are attributed to the interaction between guided-mode resonances and phonon polaritons. The results are relevant for applications in active and passive photonic elements in mid- and long-wave IR bands.","sentences":["In this paper, narrowband emitters have been designed using particle swarm optimization (PSO) in the 10-20 {\\mu}m infrared range.","The device structure consists of an anisotropic {\\alpha}-MoO3 layer combined with the one- and two-dimensional guided-mode resonance structures.","Well-defined absorption lines are present in the reflection spectrum for both TE and TM polarizations, thereby yielding narrowband emissivity at desired wavelengths.","The band structure of the designed emitters under TM polarization demonstrates distinct features unlike its TE counterpart.","These features are attributed to the interaction between guided-mode resonances and phonon polaritons.","The results are relevant for applications in active and passive photonic elements in mid- and long-wave IR bands."],"url":"http://arxiv.org/abs/2402.13070v1","category":"physics.optics"}
{"created":"2024-02-20 14:56:20","title":"Hardware Density Reduction To Avoid Proximal Junction Failure In Adult Spine Surgery: In Silico Case Studies and Virtual Cohort","abstract":"Background: Proximal Junctional Failure (PJF) is a post-operative complication in adult spine surgery, often requiring reoperation. Osteotomy is often used in revision surgeries, leading to 34.8% complications. Hence, suboptimal decisions might be extending hardware without osteotomy, which yields to severe Global Alignment and Proportion (GAP) scores. High GAPs increase PJF risk, but Hardware Density Reduction (HDR) might limit it.   Methods: Two clinical cases were evaluated: 1) Initially operated with hardware extended to T10, GAP 10; 2) PJF at T11 and hardware extended to T3, GAP 11. Two patient-personalized spine FE models were constructed through Statistical Shape Modelling (SSM) and mesh morphing. Intervertebral Disk (IVD) fiber strain, screw pull-out force, and rod stress were evaluated for the cases 1) and 2), also for 91 virtual HDR scenarios with different GAP scores, using Finite Element (FE) simulations. Different rod and bone material properties were also assessed.   Results: HDR could decrease IVD fiber strain (-70% at most) and increase screw pull-out forces (+142% at most) for cases with Ti rod and normal bone. Cr-Co rod and osteopenia, and osteoporotic bones had high PJF risk. Trade-off analyses could determine the best configurations avoiding PJF. Virtual cohort study showed that GAP 12 and 13 could not avoid PJF in any HDR scenarios either with Ti or Cr-Co rods. HDR in a UIV T10 virtual patient with GAP 11 could not de-risk in case of Cr-Co rods. UIV T3 with GAP 13 could not benefit any HDR strategy, independently of rod properties. In contrast, Ti rods might allow HDR to de-risk GAP 12 patients with UIV T3.   Conclusions: HDR could avoid PJF in the patients with medium high GAP scores, depending on the screw reduction pattern, and bone and rod material properties. Remarkably, HDR technique might avoid excessive spine surgeries and minimize the surgery cost.","sentences":["Background: Proximal Junctional Failure (PJF) is a post-operative complication in adult spine surgery, often requiring reoperation.","Osteotomy is often used in revision surgeries, leading to 34.8% complications.","Hence, suboptimal decisions might be extending hardware without osteotomy, which yields to severe Global Alignment and Proportion (GAP) scores.","High GAPs increase PJF risk, but Hardware Density Reduction (HDR) might limit it.   ","Methods: Two clinical cases were evaluated: 1) Initially operated with hardware extended to T10, GAP 10; 2) PJF at T11 and hardware extended to T3, GAP 11.","Two patient-personalized spine FE models were constructed through Statistical Shape Modelling (SSM) and mesh morphing.","Intervertebral Disk (IVD) fiber strain, screw pull-out force, and rod stress were evaluated for the cases 1) and 2), also for 91 virtual HDR scenarios with different GAP scores, using Finite Element (FE) simulations.","Different rod and bone material properties were also assessed.   ","Results: HDR could decrease IVD fiber strain (-70% at most) and increase screw pull-out forces (+142% at most) for cases with Ti rod and normal bone.","Cr-Co rod and osteopenia, and osteoporotic bones had high PJF risk.","Trade-off analyses could determine the best configurations avoiding PJF.","Virtual cohort study showed that GAP 12 and 13 could not avoid PJF in any HDR scenarios either with Ti or Cr-Co rods.","HDR in a UIV T10 virtual patient with GAP 11 could not de-risk in case of Cr-Co rods.","UIV T3 with GAP 13 could not benefit any HDR strategy, independently of rod properties.","In contrast, Ti rods might allow HDR to de-risk GAP 12 patients with UIV T3.   ","Conclusions: HDR could avoid PJF in the patients with medium high GAP scores, depending on the screw reduction pattern, and bone and rod material properties.","Remarkably, HDR technique might avoid excessive spine surgeries and minimize the surgery cost."],"url":"http://arxiv.org/abs/2402.13060v1","category":"physics.med-ph"}
{"created":"2024-02-20 14:32:48","title":"A Recurrent Neural Network Enhanced Unscented Kalman Filter for Human Motion Prediction","abstract":"This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing. Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions. Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces. Then a Recurrent Neural Network (RNN) is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model. Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another RNN to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF. A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework. These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time. This adaption, driven by the uncertainties of the RNN models, addresses inaccuracies stemming from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions. Compared to the traditional RNN-based prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions.","sentences":["This paper presents a deep learning enhanced adaptive unscented Kalman filter (UKF) for predicting human arm motion in the context of manufacturing.","Unlike previous network-based methods that solely rely on captured human motion data, which is represented as bone vectors in this paper, we incorporate a human arm dynamic model into the motion prediction algorithm and use the UKF to iteratively forecast human arm motions.","Specifically, a Lagrangian-mechanics-based physical model is employed to correlate arm motions with associated muscle forces.","Then a Recurrent Neural Network (RNN) is integrated into the framework to predict future muscle forces, which are transferred back to future arm motions based on the dynamic model.","Given the absence of measurement data for future human motions that can be input into the UKF to update the state, we integrate another RNN to directly predict human future motions and treat the prediction as surrogate measurement data fed into the UKF.","A noteworthy aspect of this study involves the quantification of uncertainties associated with both the data-driven and physical models in one unified framework.","These quantified uncertainties are used to dynamically adapt the measurement and process noises of the UKF over time.","This adaption, driven by the uncertainties of the RNN models, addresses inaccuracies stemming from the data-driven model and mitigates discrepancies between the assumed and true physical models, ultimately enhancing the accuracy and robustness of our predictions.","Compared to the traditional RNN-based prediction, our method demonstrates improved accuracy and robustness in extensive experimental validations of various types of human motions."],"url":"http://arxiv.org/abs/2402.13045v1","category":"cs.RO"}
{"created":"2024-02-20 14:31:16","title":"Not all distributional shifts are equal: Fine-grained robust conformal inference","abstract":"We introduce a fine-grained framework for uncertainty quantification of predictive models under distributional shifts. This framework distinguishes the shift in covariate distributions from that in the conditional relationship between the outcome (Y) and the covariates (X). We propose to reweight the training samples to adjust for an identifiable covariate shift while protecting against worst-case conditional distribution shift bounded in an $f$-divergence ball. Based on ideas from conformal inference and distributionally robust learning, we present an algorithm that outputs (approximately) valid and efficient prediction intervals in the presence of distributional shifts. As a use case, we apply the framework to sensitivity analysis of individual treatment effects with hidden confounding. The proposed methods are evaluated in simulation studies and three real data applications, demonstrating superior robustness and efficiency compared with existing benchmarks.","sentences":["We introduce a fine-grained framework for uncertainty quantification of predictive models under distributional shifts.","This framework distinguishes the shift in covariate distributions from that in the conditional relationship between the outcome (Y) and the covariates (X).","We propose to reweight the training samples to adjust for an identifiable covariate shift while protecting against worst-case conditional distribution shift bounded in an $f$-divergence ball.","Based on ideas from conformal inference and distributionally robust learning, we present an algorithm that outputs (approximately) valid and efficient prediction intervals in the presence of distributional shifts.","As a use case, we apply the framework to sensitivity analysis of individual treatment effects with hidden confounding.","The proposed methods are evaluated in simulation studies and three real data applications, demonstrating superior robustness and efficiency compared with existing benchmarks."],"url":"http://arxiv.org/abs/2402.13042v1","category":"stat.ME"}
{"created":"2024-02-20 14:06:39","title":"Bridging Methodologies: Angrist and Imbens' Contributions to Causal Identification","abstract":"In the 1990s, Joshua Angrist and Guido Imbens studied the causal interpretation of Instrumental Variable estimates (a widespread methodology in economics) through the lens of potential outcomes (a classical framework to formalize causality in statistics). Bridging a gap between those two strands of literature, they stress the importance of treatment effect heterogeneity and show that, under defendable assumptions in various applications, this method recovers an average causal effect for a specific subpopulation of individuals whose treatment is affected by the instrument. They were awarded the Nobel Prize primarily for this Local Average Treatment Effect (LATE). The first part of this article presents that methodological contribution in-depth: the origination in earlier applied articles, the different identification results and extensions, and related debates on the relevance of LATEs for public policy decisions. The second part reviews the main contributions of the authors beyond the LATE. J. Angrist has pursued the search for informative and varied empirical research designs in several fields, particularly in education. G. Imbens has complemented the toolbox for treatment effect estimation in many ways, notably through propensity score reweighting, matching, and, more recently, adapting machine learning procedures.","sentences":["In the 1990s, Joshua Angrist and Guido Imbens studied the causal interpretation of Instrumental Variable estimates (a widespread methodology in economics) through the lens of potential outcomes (a classical framework to formalize causality in statistics).","Bridging a gap between those two strands of literature, they stress the importance of treatment effect heterogeneity and show that, under defendable assumptions in various applications, this method recovers an average causal effect for a specific subpopulation of individuals whose treatment is affected by the instrument.","They were awarded the Nobel Prize primarily for this Local Average Treatment Effect (LATE).","The first part of this article presents that methodological contribution in-depth: the origination in earlier applied articles, the different identification results and extensions, and related debates on the relevance of LATEs for public policy decisions.","The second part reviews the main contributions of the authors beyond the LATE.","J. Angrist has pursued the search for informative and varied empirical research designs in several fields, particularly in education.","G. Imbens has complemented the toolbox for treatment effect estimation in many ways, notably through propensity score reweighting, matching, and, more recently, adapting machine learning procedures."],"url":"http://arxiv.org/abs/2402.13023v1","category":"econ.EM"}
{"created":"2024-02-20 13:32:16","title":"On a-prior bounding the growth of thermal instability waves","abstract":"We have previously shown that the nonlinear growth of a finite-amplitude perturbation to a basic state given by a baroclinic zonal flow on the \\b{eta}-plane in a thermal quasigeostrophic reduced-gravity model can be a-priori bounded. In this note we show that, unlike we stated earlier, Lyapunov stability can be proved even when buoyancy varies linearly with the meridional coordinate. In addition to rectifying our prior results we expand them by deriving an instability saturation bound by making use of the existence of such a class of Lyapunov-stable basic states. This bound can be smaller than that one we estimated before, reinforcing our previous conclusions. We also present a numerical test of the accuracy of the derived bound.","sentences":["We have previously shown that the nonlinear growth of a finite-amplitude perturbation to a basic state given by a baroclinic zonal flow on the \\b{eta}-plane in a thermal quasigeostrophic reduced-gravity model can be a-priori bounded.","In this note we show that, unlike we stated earlier, Lyapunov stability can be proved even when buoyancy varies linearly with the meridional coordinate.","In addition to rectifying our prior results we expand them by deriving an instability saturation bound by making use of the existence of such a class of Lyapunov-stable basic states.","This bound can be smaller than that one we estimated before, reinforcing our previous conclusions.","We also present a numerical test of the accuracy of the derived bound."],"url":"http://arxiv.org/abs/2402.13002v1","category":"physics.ao-ph"}
{"created":"2024-02-20 13:23:28","title":"Spectral and temporal metrology with bandlimited functions and finite-time measurements","abstract":"We perform an analysis supplementing the metrology toolbox in the time-frequency domain. While the relevant time-frequency-based metrological protocols can be borrowed from the spatial domain, where they have recently been well developed, their ultimate practical usefulness is shown to be restricted by limits put on the bandwidth of both the signal and measurements, as well as by the finite measurement time. As we demonstrate for the well-known problem of multiparameter estimation for two incoherent, point-like sources, the impact of these experimental limitations on the optimal protocol's efficiency can be detrimental. Nonetheless, we propose necessary operational criteria for attainability of the quantum Cram\\'{e}r-Rao bound under the discussed restrictions.","sentences":["We perform an analysis supplementing the metrology toolbox in the time-frequency domain.","While the relevant time-frequency-based metrological protocols can be borrowed from the spatial domain, where they have recently been well developed, their ultimate practical usefulness is shown to be restricted by limits put on the bandwidth of both the signal and measurements, as well as by the finite measurement time.","As we demonstrate for the well-known problem of multiparameter estimation for two incoherent, point-like sources, the impact of these experimental limitations on the optimal protocol's efficiency can be detrimental.","Nonetheless, we propose necessary operational criteria for attainability of the quantum Cram\\'{e}r-Rao bound under the discussed restrictions."],"url":"http://arxiv.org/abs/2402.12995v1","category":"quant-ph"}
{"created":"2024-02-20 13:11:24","title":"Proton and Neutron Induced SEU Cross Section Modeling and Simulation: A Unified Analytical Approach","abstract":"A new physics-based compact model, which makes it possible to simulate in a unified way the neutron and proton of cosmic ray induced SEU cross sections, including effects from nuclear reaction products and from direct ionization by low-energy protons, has been proposed and vali-dated. The proposed approach is analytical and based on explicit analytical relationships and approximations with physics-based fitting parameters. GEANT4 or SRIM numerical calculations can be used as an aid to adjust or refine the phenomenological parameters or functions included in the model taking into account real geometrical configurations and chemical compositions of the devices. In particular, explicit energy dependencies of the soft error cross sections for protons and neutrons over a wide range of nucleon energies were obtained and validated. Main application areas of developed model include space physics, accelerator studies high energy physics and nuclear experiments.","sentences":["A new physics-based compact model, which makes it possible to simulate in a unified way the neutron and proton of cosmic ray induced SEU cross sections, including effects from nuclear reaction products and from direct ionization by low-energy protons, has been proposed and vali-dated.","The proposed approach is analytical and based on explicit analytical relationships and approximations with physics-based fitting parameters.","GEANT4 or SRIM numerical calculations can be used as an aid to adjust or refine the phenomenological parameters or functions included in the model taking into account real geometrical configurations and chemical compositions of the devices.","In particular, explicit energy dependencies of the soft error cross sections for protons and neutrons over a wide range of nucleon energies were obtained and validated.","Main application areas of developed model include space physics, accelerator studies high energy physics and nuclear experiments."],"url":"http://arxiv.org/abs/2402.12983v1","category":"nucl-th"}
{"created":"2024-02-20 12:22:42","title":"Energy-Efficient Wireless Federated Learning via Doubly Adaptive Quantization","abstract":"Federated learning (FL) has been recognized as a viable distributed learning paradigm for training a machine learning model across distributed clients without uploading raw data. However, FL in wireless networks still faces two major challenges, i.e., large communication overhead and high energy consumption, which are exacerbated by client heterogeneity in dataset sizes and wireless channels. While model quantization is effective for energy reduction, existing works ignore adapting quantization to heterogeneous clients and FL convergence. To address these challenges, this paper develops an energy optimization problem of jointly designing quantization levels, scheduling clients, allocating channels, and controlling computation frequencies (QCCF) in wireless FL. Specifically, we derive an upper bound identifying the influence of client scheduling and quantization errors on FL convergence. Under the longterm convergence constraints and wireless constraints, the problem is established and transformed into an instantaneous problem with Lyapunov optimization. Solving Karush-Kuhn-Tucker conditions, our closed-form solution indicates that the doubly adaptive quantization level rises with the training process and correlates negatively with dataset sizes. Experiment results validate our theoretical results, showing that QCCF consumes less energy with faster convergence compared with state-of-the-art baselines.","sentences":["Federated learning (FL) has been recognized as a viable distributed learning paradigm for training a machine learning model across distributed clients without uploading raw data.","However, FL in wireless networks still faces two major challenges, i.e., large communication overhead and high energy consumption, which are exacerbated by client heterogeneity in dataset sizes and wireless channels.","While model quantization is effective for energy reduction, existing works ignore adapting quantization to heterogeneous clients and FL convergence.","To address these challenges, this paper develops an energy optimization problem of jointly designing quantization levels, scheduling clients, allocating channels, and controlling computation frequencies (QCCF) in wireless FL.","Specifically, we derive an upper bound identifying the influence of client scheduling and quantization errors on FL convergence.","Under the longterm convergence constraints and wireless constraints, the problem is established and transformed into an instantaneous problem with Lyapunov optimization.","Solving Karush-Kuhn-Tucker conditions, our closed-form solution indicates that the doubly adaptive quantization level rises with the training process and correlates negatively with dataset sizes.","Experiment results validate our theoretical results, showing that QCCF consumes less energy with faster convergence compared with state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.12957v1","category":"cs.DC"}
{"created":"2024-02-20 12:15:54","title":"Two-layered logics for probabilities and belief functions over Belnap--Dunn logic","abstract":"This paper is an extended version of an earlier submission to WoLLIC 2023. We discuss two-layered logics formalising reasoning with probabilities and belief functions that combine the Lukasiewicz $[0,1]$-valued logic with Baaz $\\triangle$ operator and the Belnap--Dunn logic. We consider two probabilistic logics that present two perspectives on the probabilities in the Belnap--Dunn logic: $\\pm$-probabilities and $\\mathbf{4}$-probabilities. In the first case, every event $\\phi$ has independent positive and negative measures that denote the likelihoods of $\\phi$ and $\\neg\\phi$, respectively. In the second case, the measures of the events are treated as partitions of the sample into four exhaustive and mutually exclusive parts corresponding to pure belief, pure disbelief, conflict and uncertainty of an agent in $\\phi$. In addition to that, we discuss two logics for the paraconsistent reasoning with belief and plausibility functions. They equip events with two measures (positive and negative) with their main difference being whether the negative measure of $\\phi$ is defined as the \\emph{belief in $\\neg\\phi$} or treated independently as \\emph{the plausibility of $\\neg\\phi$}. We provide a sound and complete Hilbert-style axiomatisation of the logic of $\\mathbf{4}$-probabilities and establish faithful translations between it and the logic of $\\pm$-probabilities. We also show that the satisfiability problem in all the logics is $\\mathsf{NP}$-complete.","sentences":["This paper is an extended version of an earlier submission to WoLLIC 2023.","We discuss two-layered logics formalising reasoning with probabilities and belief functions that combine the Lukasiewicz $[0,1]$-valued logic with Baaz $\\triangle$ operator and the Belnap--Dunn logic.","We consider two probabilistic logics that present two perspectives on the probabilities in the Belnap--Dunn logic: $\\pm$-probabilities and $\\mathbf{4}$-probabilities.","In the first case, every event $\\phi$ has independent positive and negative measures that denote the likelihoods of $\\phi$ and $\\neg\\phi$, respectively.","In the second case, the measures of the events are treated as partitions of the sample into four exhaustive and mutually exclusive parts corresponding to pure belief, pure disbelief, conflict and uncertainty of an agent in $\\phi$. In addition to that, we discuss two logics for the paraconsistent reasoning with belief and plausibility functions.","They equip events with two measures (positive and negative) with their main difference being whether the negative measure of $\\phi$ is defined as the \\emph{belief in $\\neg\\phi$} or treated independently as \\emph{the plausibility of $\\neg\\phi$}.","We provide a sound and complete Hilbert-style axiomatisation of the logic of $\\mathbf{4}$-probabilities and establish faithful translations between it and the logic of $\\pm$-probabilities.","We also show that the satisfiability problem in all the logics is $\\mathsf{NP}$-complete."],"url":"http://arxiv.org/abs/2402.12953v1","category":"math.LO"}
{"created":"2024-02-20 11:37:55","title":"Qualitative analysis to an eigenvalue problem of the Hartree type Br\u00e9zis-Nirenberg problem","abstract":"In this paper, we are concerned with the critical Hartree equation \\begin{equation*} \\begin{cases} -\\Delta u=\\left(\\displaystyle{\\displaystyle{\\int_{\\Omega}}}\\frac{u^{2^{*}_{\\mu}}(y)}{|x-y|^{\\mu}}dy\\right)u^{2^{*}_{\\mu}-1}+\\varepsilon u,\\quad u>0,\\quad &\\text{in $\\Omega$,}\\\\ u=0,\\quad &\\text{on $\\partial\\Omega$,} \\end{cases} \\end{equation*} where $\\Omega\\subset \\mathbb{R}^N$ ($N\\geq 5$) is a smooth bounded domain, $\\mu\\in (0,4)$ and $2^{*}_{\\mu}=\\frac{2N-\\mu}{N-2}$ is the upper critical exponent in the sense of the Hardy-Littlewood-Sobolev inequality. Under a non-degeneracy condition on the critical point $x_0\\in\\Omega$ of the Robin function $R(x)$, we perform that for $\\varepsilon>0$ sufficiently small, the Morse index of the blow-up solutions $u_\\varepsilon$ concentrating at $x_0$ can be computed in terms of the negative eigenvalues of the Hessian matrix $D^{2}R(x)$ at $x_0$. Compared with the usual local cases, our problem is non-local due to the nonlinearity with Hartree-type, and several difficulties arise and new estimates of the eigenpairs $\\{\\left(\\lambda_{i,\\varepsilon},v_{i,\\varepsilon}\\right)\\}$ to the associated linearized problem at $u_{\\varepsilon}$ should be introduced. To our knowledge, this seems to be the first paper to consider the qualitative analysis of a Hartree type Br\\'ezis-Nirenberg problem and our results extend the works established by M. Grossi et al in \\cite{GP} and F. Takahashi in \\cite{Ta3} to the non-local case.","sentences":["In this paper, we are concerned with the critical Hartree equation \\begin{equation*} \\begin{cases} -\\Delta u=\\left(\\displaystyle{\\displaystyle{\\int_{\\Omega}}}\\frac{u^{2^{*}_{\\mu}}(y)}{|x-y|^{\\mu}}dy\\right)u^{2^{*}_{\\mu}-1}+\\varepsilon u,\\quad u>0,\\quad &\\text{in $\\Omega$,}\\\\ u=0,\\quad &\\text{on $\\partial\\Omega$,} \\end{cases} \\end{equation*} where $\\Omega\\subset \\mathbb{R}^N$ ($N\\geq 5$) is a smooth bounded domain, $\\mu\\in (0,4)$ and $2^{*}_{\\mu}=\\frac{2N-\\mu}{N-2}$ is the upper critical exponent in the sense of the Hardy-Littlewood-Sobolev inequality.","Under a non-degeneracy condition on the critical point $x_0\\in\\Omega$ of the Robin function $R(x)$, we perform that for $\\varepsilon>0$ sufficiently small, the Morse index of the blow-up solutions $u_\\varepsilon$ concentrating at $x_0$ can be computed in terms of the negative eigenvalues of the Hessian matrix $D^{2}R(x)$ at $x_0$. Compared with the usual local cases, our problem is non-local due to the nonlinearity with Hartree-type, and several difficulties arise and new estimates of the eigenpairs $\\{\\left(\\lambda_{i,\\varepsilon},v_{i,\\varepsilon}\\right)\\}$ to the associated linearized problem at $u_{\\varepsilon}$ should be introduced.","To our knowledge, this seems to be the first paper to consider the qualitative analysis of a Hartree type Br\\'ezis-Nirenberg problem and our results extend the works established by M. Grossi et al in \\cite{GP} and F. Takahashi in \\cite{Ta3} to the non-local case."],"url":"http://arxiv.org/abs/2402.12934v1","category":"math.AP"}
{"created":"2024-02-20 11:01:06","title":"Effects of incompressibility $K_{0}$ on the symmetry energy observables in heavy-ion collisions at intermediate energies","abstract":"Within the possible least uncertainty on the nuclear incompressibility $K_{0}$, we examine effects of $K_{0}$ on the symmetry energy observables in heavy-ion collisions at intermediate energies. Based on simulations of Au + Au collision at 400 MeV/nucleon using an isospin- and momentum-dependent transport model, we find that the incompressibility $K_{0}$ indeed affects significantly the attainable density in central regions, and thus the possible symmetry energy observables, e.g., nucleon rapidity distributions and yields of charged pions. Nevertheless, through examining the combined observables, e.g., the free neutron over proton ratios $n/p$, the neutron-proton differential transverse and directed flows as well as the charged pion ratio $\\pi^{-}/\\pi^{+}$ and its kinetic energy distribution, we find that the combined observables are less affected by the uncertainty of $K_{0}$. We also compare and discuss our results with the corresponding data.","sentences":["Within the possible least uncertainty on the nuclear incompressibility $K_{0}$, we examine effects of $K_{0}$ on the symmetry energy observables in heavy-ion collisions at intermediate energies.","Based on simulations of Au + Au collision at 400 MeV/nucleon using an isospin- and momentum-dependent transport model, we find that the incompressibility $K_{0}$ indeed affects significantly the attainable density in central regions, and thus the possible symmetry energy observables, e.g., nucleon rapidity distributions and yields of charged pions.","Nevertheless, through examining the combined observables, e.g., the free neutron over proton ratios $n/p$, the neutron-proton differential transverse and directed flows as well as the charged pion ratio $\\pi^{-}/\\pi^{+}$ and its kinetic energy distribution, we find that the combined observables are less affected by the uncertainty of $K_{0}$. We also compare and discuss our results with the corresponding data."],"url":"http://arxiv.org/abs/2402.12912v1","category":"nucl-th"}
{"created":"2024-02-20 10:48:20","title":"On the validity of fMRI studies with subject-level data processed through different pipelines","abstract":"In recent years, the lack of reproducibility of research findings has become an important source of concerns in many scientific fields, including functional Magnetic Resonance Imaging (fMRI). The low statistical power often observed in fMRI studies was identified as one of the leading causes of irreproducibility. The development of data sharing opens up new opportunities to achieve larger sample sizes by reusing existing data. FMRI studies proceed by first preparing subject-level data using a given analysis pipeline and then combining those into a group analysis. Historically the subject-level analysis pipeline was identical for all subjects. As practices evolve towards more data reuse, researchers might want to directly combine subject-level data thata were processed using different pipelines. Here, we investigate the impact of combining subject-level data processed with different pipelines in between-group fMRI studies. We used the HCP Young-Adult dataset (N=1,080 participants) processed with 24 different pipelines. We then performed between-group analyses comparing subject data processed with different pipelines. We worked under the null hypothesis of no differences between subjects and compared the estimated false-positive rates obtained with the nominal rates. We showed that the analytical variability induced by the parameters explored in this dataset increases the false positive rates of studies combining data from different pipelines. We conclude that different processed subject data cannot be combined without taking into account the processing applied on these data.","sentences":["In recent years, the lack of reproducibility of research findings has become an important source of concerns in many scientific fields, including functional Magnetic Resonance Imaging (fMRI).","The low statistical power often observed in fMRI studies was identified as one of the leading causes of irreproducibility.","The development of data sharing opens up new opportunities to achieve larger sample sizes by reusing existing data.","FMRI studies proceed by first preparing subject-level data using a given analysis pipeline and then combining those into a group analysis.","Historically the subject-level analysis pipeline was identical for all subjects.","As practices evolve towards more data reuse, researchers might want to directly combine subject-level data thata were processed using different pipelines.","Here, we investigate the impact of combining subject-level data processed with different pipelines in between-group fMRI studies.","We used the HCP Young-Adult dataset (N=1,080 participants) processed with 24 different pipelines.","We then performed between-group analyses comparing subject data processed with different pipelines.","We worked under the null hypothesis of no differences between subjects and compared the estimated false-positive rates obtained with the nominal rates.","We showed that the analytical variability induced by the parameters explored in this dataset increases the false positive rates of studies combining data from different pipelines.","We conclude that different processed subject data cannot be combined without taking into account the processing applied on these data."],"url":"http://arxiv.org/abs/2402.12900v1","category":"q-bio.NC"}
{"created":"2024-02-20 10:33:45","title":"BFT-DSN: A Byzantine Fault Tolerant Decentralized Storage Network","abstract":"With the rapid development of blockchain and its applications, the amount of data stored on decentralized storage networks (DSNs) has grown exponentially. DSNs bring together affordable storage resources from around the world to provide robust, decentralized storage services for tens of thousands of decentralized applications (dApps). However, existing DSNs do not offer verifiability when implementing erasure coding for redundant storage, making them vulnerable to Byzantine encoders. Additionally, there is a lack of Byzantine fault-tolerant consensus for optimal resilience in DSNs. This paper introduces BFT-DSN, a Byzantine fault-tolerant decentralized storage network designed to address these challenges. BFT-DSN combines storage-weighted BFT consensus with erasure coding and incorporates homomorphic fingerprints and weighted threshold signatures for decentralized verification. The implementation of BFT-DSN demonstrates its comparable performance in terms of storage cost and latency as well as superior performance in Byzantine resilience when compared to existing industrial decentralized storage networks.","sentences":["With the rapid development of blockchain and its applications, the amount of data stored on decentralized storage networks (DSNs) has grown exponentially.","DSNs bring together affordable storage resources from around the world to provide robust, decentralized storage services for tens of thousands of decentralized applications (dApps).","However, existing DSNs do not offer verifiability when implementing erasure coding for redundant storage, making them vulnerable to Byzantine encoders.","Additionally, there is a lack of Byzantine fault-tolerant consensus for optimal resilience in DSNs.","This paper introduces BFT-DSN, a Byzantine fault-tolerant decentralized storage network designed to address these challenges.","BFT-DSN combines storage-weighted BFT consensus with erasure coding and incorporates homomorphic fingerprints and weighted threshold signatures for decentralized verification.","The implementation of BFT-DSN demonstrates its comparable performance in terms of storage cost and latency as well as superior performance in Byzantine resilience when compared to existing industrial decentralized storage networks."],"url":"http://arxiv.org/abs/2402.12889v1","category":"cs.CR"}
{"created":"2024-02-20 10:18:18","title":"Autism Detection in Speech -- A Survey","abstract":"There has been a range of studies of how autism is displayed in voice, speech, and language. We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism. Our survey looks at all three domains. We define autism and which comorbidities might influence the correct detection of the disorder. We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate. We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts. Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched. Also, most NLP research focuses on traditional machine learning methods instead of transformers which could be beneficial in this context. Additionally, we were unable to find research combining both features from audio and transcripts.","sentences":["There has been a range of studies of how autism is displayed in voice, speech, and language.","We analyse studies from the biomedical, as well as the psychological domain, but also from the NLP domain in order to find linguistic, prosodic and acoustic cues that could indicate autism.","Our survey looks at all three domains.","We define autism and which comorbidities might influence the correct detection of the disorder.","We especially look at observations such as verbal and semantic fluency, prosodic features, but also disfluencies and speaking rate.","We also show word-based approaches and describe machine learning and transformer-based approaches both on the audio data as well as the transcripts.","Lastly, we conclude, while there already is a lot of research, female patients seem to be severely under-researched.","Also, most NLP research focuses on traditional machine learning methods instead of transformers which could be beneficial in this context.","Additionally, we were unable to find research combining both features from audio and transcripts."],"url":"http://arxiv.org/abs/2402.12880v1","category":"cs.CL"}
{"created":"2024-02-20 10:04:51","title":"Interface Identification constrained by Local-to-Nonlocal Coupling","abstract":"Models of physical phenomena that use nonlocal operators are better suited for some applications than their classical counterparts that employ partial differential operators. However, the numerical solution of these nonlocal problems can be quite expensive. Therefore, Local-to-Nonlocal couplings have emerged that combine partial differential operators with nonlocal operators. In this work, we make use of an energy-based Local-to-Nonlocal coupling that serves as a constraint for an interface identification problem.","sentences":["Models of physical phenomena that use nonlocal operators are better suited for some applications than their classical counterparts that employ partial differential operators.","However, the numerical solution of these nonlocal problems can be quite expensive.","Therefore, Local-to-Nonlocal couplings have emerged that combine partial differential operators with nonlocal operators.","In this work, we make use of an energy-based Local-to-Nonlocal coupling that serves as a constraint for an interface identification problem."],"url":"http://arxiv.org/abs/2402.12871v1","category":"math.OC"}
{"created":"2024-02-20 09:59:33","title":"Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets","abstract":"In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments. Here, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets. Additionally, it achieves an $O(\\sqrt{T})$ regret even in adversarial environments where FTL suffers an $\\Omega(T)$ regret, and attains an $O(\\rho \\log T + \\sqrt{C \\rho \\log T})$ regret bound in corrupted stochastic environments with corruption level $C$. Furthermore, by extending our analysis, we establish a regret upper bound of $O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for $q$-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and $\\ell_p$-balls for $p \\in [1,\\infty)$. This bound bridges the gap between the $O(\\log T)$ regret bound for strongly convex sets ($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$).","sentences":["In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets.","In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret.","This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets.","We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\\rho \\log T)$ in stochastic environments.","Here, $\\rho > 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set.","Our approach, unlike existing ones, can work directly with convex loss functions, exploiting the curvature of loss functions simultaneously, and can achieve the logarithmic regret only with a local property of feasible sets.","Additionally, it achieves an $O(\\sqrt{T})$ regret even in adversarial environments where FTL suffers an $\\Omega(T)$ regret, and attains an $O(\\rho \\log T + \\sqrt{C \\rho \\log T})$ regret bound in corrupted stochastic environments with corruption level $C$.","Furthermore, by extending our analysis, we establish a regret upper bound of $O\\Big(T^{\\frac{q-2}{2(q-1)}} (\\log T)^{\\frac{q}{2(q-1)}}\\Big)$ for $q$-uniformly convex feasible sets, where uniformly convex sets include strongly convex sets and $\\ell_p$-balls for $p \\in","[1,\\infty)$. This bound bridges the gap between the $O(\\log T)$ regret bound for strongly convex sets ($q=2$) and the $O(\\sqrt{T})$ regret bound for non-curved sets ($q\\to\\infty$)."],"url":"http://arxiv.org/abs/2402.12868v1","category":"cs.LG"}
{"created":"2024-02-20 09:57:46","title":"On new tests for the Poisson distribution based on empirical weight functions","abstract":"We propose new goodness-of-fit tests for the Poisson distribution. The testing procedure entails fitting a weighted Poisson distribution, which has the Poisson as a special case, to observed data. Based on sample data, we calculate an empirical weight function which is compared to its theoretical counterpart under the Poisson assumption. Weighted Lp distances between these empirical and theoretical functions are proposed as test statistics and closed form expressions are derived for L1, L2 and L1 distances. A Monte Carlo study is included in which the newly proposed tests are shown to be powerful when compared to existing tests, especially in the case of overdispersed alternatives. We demonstrate the use of the tests with two practical examples.","sentences":["We propose new goodness-of-fit tests for the Poisson distribution.","The testing procedure entails fitting a weighted Poisson distribution, which has the Poisson as a special case, to observed data.","Based on sample data, we calculate an empirical weight function which is compared to its theoretical counterpart under the Poisson assumption.","Weighted Lp distances between these empirical and theoretical functions are proposed as test statistics and closed form expressions are derived for L1, L2 and L1 distances.","A Monte Carlo study is included in which the newly proposed tests are shown to be powerful when compared to existing tests, especially in the case of overdispersed alternatives.","We demonstrate the use of the tests with two practical examples."],"url":"http://arxiv.org/abs/2402.12866v1","category":"stat.ME"}
{"created":"2024-02-20 09:53:38","title":"Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation","abstract":"The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.","sentences":["The subjective perception of emotion leads to inconsistent labels from human annotators.","Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing.","This paper investigates three methods to handle ambiguous emotion.","First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes.","Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning.","This approach retains the classification accuracy while effectively detects ambiguous emotion expressions.","Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label.","The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion.","The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation.","Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation."],"url":"http://arxiv.org/abs/2402.12862v1","category":"cs.CL"}
{"created":"2024-02-20 09:52:30","title":"Bounding Reconstruction Attack Success of Adversaries Without Data Priors","abstract":"Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.","sentences":["Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data.","In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients.","When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided.","So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality.","In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results.","With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter."],"url":"http://arxiv.org/abs/2402.12861v1","category":"cs.LG"}
{"created":"2024-02-20 09:05:43","title":"Extending the Scope of Inference About Predictive Ability to Machine Learning Methods","abstract":"Though out-of-sample forecast evaluation is systematically employed with modern machine learning methods and there exists a well-established classic inference theory for predictive ability, see, e.g., West (1996, Asymptotic Inference About Predictive Ability, \\textit{Econometrica}, 64, 1067-1084), such theory is not directly applicable to modern machine learners such as the Lasso in the high dimensional setting. We investigate under which conditions such extensions are possible. Two key properties for standard out-of-sample asymptotic inference to be valid with machine learning are (i) a zero-mean condition for the score of the prediction loss function; and (ii) a fast rate of convergence for the machine learner. Monte Carlo simulations confirm our theoretical findings. For accurate finite sample inferences with machine learning, we recommend a small out-of-sample vs in-sample size ratio. We illustrate the wide applicability of our results with a new out-of-sample test for the Martingale Difference Hypothesis (MDH). We obtain the asymptotic null distribution of our test and use it to evaluate","sentences":["Though out-of-sample forecast evaluation is systematically employed with modern machine learning methods and there exists a well-established classic inference theory for predictive ability, see, e.g., West (1996, Asymptotic Inference About Predictive Ability, \\textit{Econometrica}, 64, 1067-1084), such theory is not directly applicable to modern machine learners such as the Lasso in the high dimensional setting.","We investigate under which conditions such extensions are possible.","Two key properties for standard out-of-sample asymptotic inference to be valid with machine learning are (i) a zero-mean condition for the score of the prediction loss function; and (ii) a fast rate of convergence for the machine learner.","Monte Carlo simulations confirm our theoretical findings.","For accurate finite sample inferences with machine learning, we recommend a small out-of-sample vs in-sample size ratio.","We illustrate the wide applicability of our results with a new out-of-sample test for the Martingale Difference Hypothesis (MDH).","We obtain the asymptotic null distribution of our test and use it to evaluate"],"url":"http://arxiv.org/abs/2402.12838v1","category":"econ.EM"}
{"created":"2024-02-20 09:05:28","title":"Using biocompatible materials as substrate coating for electric field enhancement in tip-enhanced Raman spectroscopy","abstract":"In this article, tip-enhanced Raman spectroscopy (TERS) is investigated as a precise method for analysis of biological samples. Using Finite Difference Time Domain (FDTD) simulation, it has been tried to design the required structures for analysis of these samples. At first, by comparing different TERS structures and considering the material, dimensions and other parameters in the simulation, the ideal structure is introduced from a physical point of view and considering the electric field enhancement. In the following, taking into possibility of the effect of environmental and chemical reactions during testing on biological samples, biocompatible materials are used as substrate coating in the simulation, and the effect of using these materials are investigated in comparison with the previous conditions. After performing the simulations, we concluded that Au, Cu and Ag have the highest electric field enhancement, respectively, and the presence of Au next to Cu, also Au and Cu next to Ag, leads to the enhancement of the electric field in them. In the following, we found that the material and thickness of the layer under the coating in the substrate and tip have a great effect on the enhancement. finally, using five biocompatible materials as a coating in the case of using the Au tip and substrate, which creates the most electric field enhancement, we saw that the use of biocompatible materials greatly reduces the enhancement and the use of these five materials does not differ much from each other. Anyway, the use of a 1 nm layer of biocompatible coating creates a much more favorable effect on enhancement than upper thicknesses.","sentences":["In this article, tip-enhanced Raman spectroscopy (TERS) is investigated as a precise method for analysis of biological samples.","Using Finite Difference Time Domain (FDTD) simulation, it has been tried to design the required structures for analysis of these samples.","At first, by comparing different TERS structures and considering the material, dimensions and other parameters in the simulation, the ideal structure is introduced from a physical point of view and considering the electric field enhancement.","In the following, taking into possibility of the effect of environmental and chemical reactions during testing on biological samples, biocompatible materials are used as substrate coating in the simulation, and the effect of using these materials are investigated in comparison with the previous conditions.","After performing the simulations, we concluded that Au, Cu and Ag have the highest electric field enhancement, respectively, and the presence of Au next to Cu, also Au and Cu next to Ag, leads to the enhancement of the electric field in them.","In the following, we found that the material and thickness of the layer under the coating in the substrate and tip have a great effect on the enhancement.","finally, using five biocompatible materials as a coating in the case of using the Au tip and substrate, which creates the most electric field enhancement, we saw that the use of biocompatible materials greatly reduces the enhancement and the use of these five materials does not differ much from each other.","Anyway, the use of a 1 nm layer of biocompatible coating creates a much more favorable effect on enhancement than upper thicknesses."],"url":"http://arxiv.org/abs/2402.12836v1","category":"physics.atom-ph"}
{"created":"2024-02-20 08:49:58","title":"On scalable ARMA models","abstract":"This paper considers both the least squares and quasi-maximum likelihood estimation for the recently proposed scalable ARMA model, a parametric infinite-order vector AR model, and their asymptotic normality is also established. It makes feasible the inference on this computationally efficient model, especially for financial time series. An efficient block coordinate descent algorithm is further introduced to search for estimates, and a Bayesian information criterion is suggested for model selection. Simulation experiments are conducted to illustrate their finite sample performance, and a real application on six macroeconomic indicators illustrates the usefulness of the proposed methodology.","sentences":["This paper considers both the least squares and quasi-maximum likelihood estimation for the recently proposed scalable ARMA model, a parametric infinite-order vector AR model, and their asymptotic normality is also established.","It makes feasible the inference on this computationally efficient model, especially for financial time series.","An efficient block coordinate descent algorithm is further introduced to search for estimates, and a Bayesian information criterion is suggested for model selection.","Simulation experiments are conducted to illustrate their finite sample performance, and a real application on six macroeconomic indicators illustrates the usefulness of the proposed methodology."],"url":"http://arxiv.org/abs/2402.12825v1","category":"stat.ME"}
{"created":"2024-02-20 08:21:46","title":"Joint Mean and Correlation Regression Models for Multivariate Data","abstract":"We propose a new joint mean and correlation regression model for correlated multivariate discrete responses, that simultaneously regresses the mean of each response against a set of covariates, and the correlations between responses against a set of similarity/distance measures. A set of joint estimating equations are formulated to construct an estimator of both the mean regression coefficients and the correlation regression parameters. Under a general setting where the number of responses can tend to infinity, the joint estimator is demonstrated to be consistent and asymptotically normally distributed, with differing rates of convergence due to the mean regression coefficients being heterogeneous across responses. An iterative estimation procedure is developed to obtain parameter estimates in the required, constrained parameter space. We apply the proposed model to a multivariate abundance dataset comprising overdispersed counts of 38 Carabidae ground beetle species sampled throughout Scotland, along with information about the environmental conditions of each site and the traits of each species. Results show in particular that the relationships between the mean abundances of various beetle species and environmental covariates are different and that beetle total length has statistically important effect in driving the correlations between the species. Simulations demonstrate the strong finite sample performance of the proposed estimator in terms of point estimation and inference.","sentences":["We propose a new joint mean and correlation regression model for correlated multivariate discrete responses, that simultaneously regresses the mean of each response against a set of covariates, and the correlations between responses against a set of similarity/distance measures.","A set of joint estimating equations are formulated to construct an estimator of both the mean regression coefficients and the correlation regression parameters.","Under a general setting where the number of responses can tend to infinity, the joint estimator is demonstrated to be consistent and asymptotically normally distributed, with differing rates of convergence due to the mean regression coefficients being heterogeneous across responses.","An iterative estimation procedure is developed to obtain parameter estimates in the required, constrained parameter space.","We apply the proposed model to a multivariate abundance dataset comprising overdispersed counts of 38 Carabidae ground beetle species sampled throughout Scotland, along with information about the environmental conditions of each site and the traits of each species.","Results show in particular that the relationships between the mean abundances of various beetle species and environmental covariates are different and that beetle total length has statistically important effect in driving the correlations between the species.","Simulations demonstrate the strong finite sample performance of the proposed estimator in terms of point estimation and inference."],"url":"http://arxiv.org/abs/2402.12803v1","category":"stat.ME"}
{"created":"2024-02-20 08:21:20","title":"The Minkowski problem for the non-compact convex set with an asymptotic boundary condition","abstract":"In this paper, combining the covolume, we study the Minkowski theory for the non-compact convex set with an asymptotic boundary condition. In particular, the mixed covolume of two non-compact convex sets is introduced and its geometric interpretation is obtained by the Hadamard variational formula. The Brunn-Minkowski and Minkowski inequalities for covolume are established, and the equivalence of these two inequalities are discussed as well. The Minkowski problem for non-compact convex set is proposed and solved under the asymptotic conditions. In the end, we give a solution to the Minkowski problem for $\\sigma$-finite measure on the conic domain $\\Omega_C$.","sentences":["In this paper, combining the covolume, we study the Minkowski theory for the non-compact convex set with an asymptotic boundary condition.","In particular, the mixed covolume of two non-compact convex sets is introduced and its geometric interpretation is obtained by the Hadamard variational formula.","The Brunn-Minkowski and Minkowski inequalities for covolume are established, and the equivalence of these two inequalities are discussed as well.","The Minkowski problem for non-compact convex set is proposed and solved under the asymptotic conditions.","In the end, we give a solution to the Minkowski problem for $\\sigma$-finite measure on the conic domain $\\Omega_C$."],"url":"http://arxiv.org/abs/2402.12802v1","category":"math.DG"}
{"created":"2024-02-20 07:48:11","title":"Pressure evolution of the normal- and superconducting-state properties of the line-nodal material CaSb$_2$ revealed by $^{123}$Sb nuclear quadrupole resonance","abstract":"CaSb$_2$ is the Dirac line-nodal material that exhibits a superconducting (SC) transition at 1.7 K. In spite of its conventional SC state at ambient pressure, the transition temperature $T_{\\mathrm{c}}$ shows a peak structure against hydrostatic pressure. We performed ac magnetic susceptibility and $^{123}$Sb nuclear quadrupole resonance (NQR) measurements on single-crystalline CaSb$_2$ under pressures up to 2.08 GPa. $T_{\\mathrm{c}}$ monotonically increased in this pressure region, which is consistent with a previous study. We observed continuous broadening of the NQR spectrum against pressure, which is a sign of unique compression behavior of the lattice. In the normal state, the nuclear spin-lattice relaxation rate 1/$T_1$ is proportional to temperature in all pressure values; typical of a metal. However, 1/$T_1T$ in the normal state is independent of pressure, indicating that the density of states at the Fermi energy $N(E_{\\mathrm{F}})$, which is one of the parameters governing $T_{\\mathrm{c}}$, is insensitive to pressure. From these results, we conclude that $N(E_{\\mathrm{F}})$ does not govern the origin of the enhancement in $T_{\\mathrm{c}}$. This is unusual for a weak electron-phonon coupling superconductor. In the SC state, we revealed that the SC gap becomes larger and more isotropic under pressure.","sentences":["CaSb$_2$ is the Dirac line-nodal material that exhibits a superconducting (SC) transition at 1.7 K. In spite of its conventional SC state at ambient pressure, the transition temperature $T_{\\mathrm{c}}$ shows a peak structure against hydrostatic pressure.","We performed ac magnetic susceptibility and $^{123}$Sb nuclear quadrupole resonance (NQR) measurements on single-crystalline CaSb$_2$ under pressures up to 2.08 GPa.","$T_{\\mathrm{c}}$ monotonically increased in this pressure region, which is consistent with a previous study.","We observed continuous broadening of the NQR spectrum against pressure, which is a sign of unique compression behavior of the lattice.","In the normal state, the nuclear spin-lattice relaxation rate 1/$T_1$ is proportional to temperature in all pressure values; typical of a metal.","However, 1/$T_1T$ in the normal state is independent of pressure, indicating that the density of states at the Fermi energy $N(E_{\\mathrm{F}})$, which is one of the parameters governing $T_{\\mathrm{c}}$, is insensitive to pressure.","From these results, we conclude that $N(E_{\\mathrm{F}})$ does not govern the origin of the enhancement in $T_{\\mathrm{c}}$. This is unusual for a weak electron-phonon coupling superconductor.","In the SC state, we revealed that the SC gap becomes larger and more isotropic under pressure."],"url":"http://arxiv.org/abs/2402.12783v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 07:40:11","title":"Tackling Byzantine Clients in Federated Learning","abstract":"The possibility of adversarial (a.k.a., {\\em Byzantine}) clients makes federated learning (FL) prone to arbitrary manipulation. The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\\mathsf{FedAvg}$ algorithm by a \\emph{robust averaging rule}. While a significant amount of work has been devoted to studying the convergence of federated {\\em robust averaging} (which we denote by $\\mathsf{FedRo}$), prior work has largely ignored the impact of {\\em client subsampling} and {\\em local steps}, two fundamental FL characteristics. While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients. Consequently, a careless deployment of $\\mathsf{FedRo}$ could yield poor performance. We validate this observation by presenting an in-depth analysis of $\\mathsf{FedRo}$ tightly analyzing the impact of client subsampling and local steps. Specifically, we present a sufficient condition on client subsampling for nearly-optimal convergence of $\\mathsf{FedRo}$ (for smooth non-convex loss). Also, we show that the rate of improvement in learning accuracy {\\em diminishes} with respect to the number of clients subsampled, as soon as the sample size exceeds a threshold value. Interestingly, we also observe that under a careful choice of step-sizes, the learning error due to Byzantine clients decreases with the number of local steps. We validate our theory by experiments on the FEMNIST and CIFAR-$10$ image classification tasks.","sentences":["The possibility of adversarial (a.k.a., {\\em Byzantine}) clients makes federated learning (FL) prone to arbitrary manipulation.","The natural approach to robustify FL against adversarial clients is to replace the simple averaging operation at the server in the standard $\\mathsf{FedAvg}$ algorithm by a \\emph{robust averaging rule}.","While a significant amount of work has been devoted to studying the convergence of federated {\\em robust averaging} (which we denote by $\\mathsf{FedRo}$), prior work has largely ignored the impact of {\\em client subsampling} and {\\em local steps}, two fundamental FL characteristics.","While client subsampling increases the effective fraction of Byzantine clients, local steps increase the drift between the local updates computed by honest (i.e., non-Byzantine) clients.","Consequently, a careless deployment of $\\mathsf{FedRo}$ could yield poor performance.","We validate this observation by presenting an in-depth analysis of $\\mathsf{FedRo}$ tightly analyzing the impact of client subsampling and local steps.","Specifically, we present a sufficient condition on client subsampling for nearly-optimal convergence of $\\mathsf{FedRo}$ (for smooth non-convex loss).","Also, we show that the rate of improvement in learning accuracy {\\em diminishes} with respect to the number of clients subsampled, as soon as the sample size exceeds a threshold value.","Interestingly, we also observe that under a careful choice of step-sizes, the learning error due to Byzantine clients decreases with the number of local steps.","We validate our theory by experiments on the FEMNIST and CIFAR-$10$ image classification tasks."],"url":"http://arxiv.org/abs/2402.12780v1","category":"cs.LG"}
{"created":"2024-02-20 07:37:32","title":"Two-stage Rainfall-Forecasting Diffusion Model","abstract":"Deep neural networks have made great achievements in rainfall prediction.However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions. To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling. TRDM is a two-stage method for rainfall prediction tasks. The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions. The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images. We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets. Our project is open source and available on GitHub at: \\href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}.","sentences":["Deep neural networks have made great achievements in rainfall prediction.","However, the current forecasting methods have certain limitations, such as with blurry generated images and incorrect spatial positions.","To overcome these challenges, we propose a Two-stage Rainfall-Forecasting Diffusion Model (TRDM) aimed at improving the accuracy of long-term rainfall forecasts and addressing the imbalance in performance between temporal and spatial modeling.","TRDM is a two-stage method for rainfall prediction tasks.","The task of the first stage is to capture robust temporal information while preserving spatial information under low-resolution conditions.","The task of the second stage is to reconstruct the low-resolution images generated in the first stage into high-resolution images.","We demonstrate state-of-the-art results on the MRMS and Swedish radar datasets.","Our project is open source and available on GitHub at: \\href{https://github.com/clearlyzerolxd/TRDM}{https://github.com/clearlyzerolxd/TRDM}."],"url":"http://arxiv.org/abs/2402.12779v1","category":"cs.CV"}
{"created":"2024-02-20 07:16:12","title":"When and How: Learning Identifiable Latent States for Nonstationary Time Series Forecasting","abstract":"Temporal distribution shifts are ubiquitous in time series data. One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies. But this assumption is difficult to meet, as we do not know when the distribution shifts occur. To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur. Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change. Specifically, we formalize the causal process with environment-irrelated stationary and environment-related nonstationary variables. Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable. Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov model to estimate latent environments and modular prior networks to identify latent states. The IDEA model outperforms several latest nonstationary forecasting methods on various benchmark datasets, highlighting its advantages in real-world scenarios.","sentences":["Temporal distribution shifts are ubiquitous in time series data.","One of the most popular methods assumes that the temporal distribution shift occurs uniformly to disentangle the stationary and nonstationary dependencies.","But this assumption is difficult to meet, as we do not know when the distribution shifts occur.","To solve this problem, we propose to learn IDentifiable latEnt stAtes (IDEA) to detect when the distribution shifts occur.","Beyond that, we further disentangle the stationary and nonstationary latent states via sufficient observation assumption to learn how the latent states change.","Specifically, we formalize the causal process with environment-irrelated stationary and environment-related nonstationary variables.","Under mild conditions, we show that latent environments and stationary/nonstationary variables are identifiable.","Based on these theories, we devise the IDEA model, which incorporates an autoregressive hidden Markov model to estimate latent environments and modular prior networks to identify latent states.","The IDEA model outperforms several latest nonstationary forecasting methods on various benchmark datasets, highlighting its advantages in real-world scenarios."],"url":"http://arxiv.org/abs/2402.12767v1","category":"cs.LG"}
{"created":"2024-02-20 07:12:22","title":"GOOD: Towards Domain Generalized Orientated Object Detection","abstract":"Oriented object detection has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality. In this paper, we propose the task of domain generalized oriented object detection, which intends to explore the generalization of oriented object detectors on arbitrary unseen target domains. Learning domain generalized oriented object detectors is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions. To address these challenges, we propose a generalized oriented object detector (GOOD). After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC). The proposed RAC allows the oriented object detector to learn stable orientation representation from style-diversified samples. The proposed SEC further stabilizes the generalization ability of content representation from different image styles. Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD. Source code will be publicly available.","sentences":["Oriented object detection has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality.","In this paper, we propose the task of domain generalized oriented object detection, which intends to explore the generalization of oriented object detectors on arbitrary unseen target domains.","Learning domain generalized oriented object detectors is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions.","To address these challenges, we propose a generalized oriented object detector (GOOD).","After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC).","The proposed RAC allows the oriented object detector to learn stable orientation representation from style-diversified samples.","The proposed SEC further stabilizes the generalization ability of content representation from different image styles.","Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD.","Source code will be publicly available."],"url":"http://arxiv.org/abs/2402.12765v1","category":"cs.CV"}
{"created":"2024-02-20 07:09:39","title":"Learning under Singularity: An Information Criterion improving WBIC and sBIC","abstract":"We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC). LS is effective without regularity constraints and demonstrates stability. Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite. In contrast, models not meeting these conditions are termed singular. Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC. WBIC is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients. Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood estimates. LS addresses these limitations by enhancing the utility of both WBIC and sBIC. It incorporates the empirical loss from the Widely Applicable Information Criterion (WAIC) to represent the goodness of fit to the statistical model, along with a penalty term similar to that of sBIC. This approach offers a flexible and robust method for model selection, free from regularity constraints.","sentences":["We introduce a novel Information Criterion (IC), termed Learning under Singularity (LS), designed to enhance the functionality of the Widely Applicable Bayes Information Criterion (WBIC) and the Singular Bayesian Information Criterion (sBIC).","LS is effective without regularity constraints and demonstrates stability.","Watanabe defined a statistical model or a learning machine as regular if the mapping from a parameter to a probability distribution is one-to-one and its Fisher information matrix is positive definite.","In contrast, models not meeting these conditions are termed singular.","Over the past decade, several information criteria for singular cases have been proposed, including WBIC and sBIC.","WBIC is applicable in non-regular scenarios but faces challenges with large sample sizes and redundant estimation of known learning coefficients.","Conversely, sBIC is limited in its broader application due to its dependence on maximum likelihood estimates.","LS addresses these limitations by enhancing the utility of both WBIC and sBIC.","It incorporates the empirical loss from the Widely Applicable Information Criterion (WAIC) to represent the goodness of fit to the statistical model, along with a penalty term similar to that of sBIC.","This approach offers a flexible and robust method for model selection, free from regularity constraints."],"url":"http://arxiv.org/abs/2402.12762v1","category":"stat.ML"}
{"created":"2024-02-20 06:58:00","title":"Towards Fair Allocation in Social Commerce Platforms","abstract":"Social commerce platforms are emerging businesses where producers sell products through re-sellers who advertise the products to other customers in their social network. Due to the increasing popularity of this business model, thousands of small producers and re-sellers are starting to depend on these platforms for their livelihood; thus, it is important to provide fair earning opportunities to them. The enormous product space in such platforms prohibits manual search, and motivates the need for recommendation algorithms to effectively allocate product exposure and, consequently, earning opportunities. In this work, we focus on the fairness of such allocations in social commerce platforms and formulate the problem of assigning products to re-sellers as a fair division problem with indivisible items under two-sided cardinality constraints, wherein each product must be given to at least a certain number of re-sellers and each re-seller must get a certain number of products.   Our work systematically explores various well-studied benchmarks of fairness -- including Nash social welfare, envy-freeness up to one item (EF1), and equitability up to one item (EQ1) -- from both theoretical and experimental perspectives. We find that the existential and computational guarantees of these concepts known from the unconstrained setting do not extend to our constrained model. To address this limitation, we develop a mixed-integer linear program and other scalable heuristics that provide near-optimal approximation of Nash social welfare in simulated and real social commerce datasets. Overall, our work takes the first step towards achieving provable fairness alongside reasonable revenue guarantees on social commerce platforms.","sentences":["Social commerce platforms are emerging businesses where producers sell products through re-sellers who advertise the products to other customers in their social network.","Due to the increasing popularity of this business model, thousands of small producers and re-sellers are starting to depend on these platforms for their livelihood; thus, it is important to provide fair earning opportunities to them.","The enormous product space in such platforms prohibits manual search, and motivates the need for recommendation algorithms to effectively allocate product exposure and, consequently, earning opportunities.","In this work, we focus on the fairness of such allocations in social commerce platforms and formulate the problem of assigning products to re-sellers as a fair division problem with indivisible items under two-sided cardinality constraints, wherein each product must be given to at least a certain number of re-sellers and each re-seller must get a certain number of products.   ","Our work systematically explores various well-studied benchmarks of fairness -- including Nash social welfare, envy-freeness up to one item (EF1), and equitability up to one item (EQ1) -- from both theoretical and experimental perspectives.","We find that the existential and computational guarantees of these concepts known from the unconstrained setting do not extend to our constrained model.","To address this limitation, we develop a mixed-integer linear program and other scalable heuristics that provide near-optimal approximation of Nash social welfare in simulated and real social commerce datasets.","Overall, our work takes the first step towards achieving provable fairness alongside reasonable revenue guarantees on social commerce platforms."],"url":"http://arxiv.org/abs/2402.12759v1","category":"cs.CY"}
{"created":"2024-02-20 06:56:31","title":"Time-dependent Ginzburg-Landau theory of the vortex spin Hall effect","abstract":"We develop a time-dependent Ginzburg-Landau theory of the vortex spin Hall effect, i.e., a novel spin Hall effect that is driven by the motion of superconducting vortices. For the direct vortex spin Hall effect in which an input charge current drives the transverse spin current accompanying the vortex motion, we start from the well-known Schmid-Caroli-Maki solution for the time-dependent Ginzburg-Landau equation under the applied electric field, and find out the expression of the induced spin current. For the inverse vortex spin Hall effect in which an input spin current drives the longitudinal vortex motion and produces the transverse charge current, we microscopically construct the time-dependent Ginzburg-Landau equation under the applied spin accumulation gradient, and calculate the induced transverse charge current as well as the open circuit voltage. The time-dependent Ginzburg-Landau equation and its analytical solution developed here can be a basis for more qualitative numerical simulations of the vortex spin Hall effect.","sentences":["We develop a time-dependent Ginzburg-Landau theory of the vortex spin Hall effect, i.e., a novel spin Hall effect that is driven by the motion of superconducting vortices.","For the direct vortex spin Hall effect in which an input charge current drives the transverse spin current accompanying the vortex motion, we start from the well-known Schmid-Caroli-Maki solution for the time-dependent Ginzburg-Landau equation under the applied electric field, and find out the expression of the induced spin current.","For the inverse vortex spin Hall effect in which an input spin current drives the longitudinal vortex motion and produces the transverse charge current, we microscopically construct the time-dependent Ginzburg-Landau equation under the applied spin accumulation gradient, and calculate the induced transverse charge current as well as the open circuit voltage.","The time-dependent Ginzburg-Landau equation and its analytical solution developed here can be a basis for more qualitative numerical simulations of the vortex spin Hall effect."],"url":"http://arxiv.org/abs/2402.12758v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 06:24:38","title":"Plugin Speech Enhancement: A Universal Speech Enhancement Framework Inspired by Dynamic Neural Network","abstract":"The expectation to deploy a universal neural network for speech enhancement, with the aim of improving noise robustness across diverse speech processing tasks, faces challenges due to the existing lack of awareness within static speech enhancement frameworks regarding the expected speech in downstream modules. These limitations impede the effectiveness of static speech enhancement approaches in achieving optimal performance for a range of speech processing tasks, thereby challenging the notion of universal applicability. The fundamental issue in achieving universal speech enhancement lies in effectively informing the speech enhancement module about the features of downstream modules. In this study, we present a novel weighting prediction approach, which explicitly learns the task relationships from downstream training information to address the core challenge of universal speech enhancement. We found the role of deciding whether to employ data augmentation techniques as crucial downstream training information. This decision significantly impacts the expected speech and the performance of the speech enhancement module. Moreover, we introduce a novel speech enhancement network, the Plugin Speech Enhancement (Plugin-SE). The Plugin-SE is a dynamic neural network that includes the speech enhancement module, gate module, and weight prediction module. Experimental results demonstrate that the proposed Plugin-SE approach is competitive or superior to other joint training methods across various downstream tasks.","sentences":["The expectation to deploy a universal neural network for speech enhancement, with the aim of improving noise robustness across diverse speech processing tasks, faces challenges due to the existing lack of awareness within static speech enhancement frameworks regarding the expected speech in downstream modules.","These limitations impede the effectiveness of static speech enhancement approaches in achieving optimal performance for a range of speech processing tasks, thereby challenging the notion of universal applicability.","The fundamental issue in achieving universal speech enhancement lies in effectively informing the speech enhancement module about the features of downstream modules.","In this study, we present a novel weighting prediction approach, which explicitly learns the task relationships from downstream training information to address the core challenge of universal speech enhancement.","We found the role of deciding whether to employ data augmentation techniques as crucial downstream training information.","This decision significantly impacts the expected speech and the performance of the speech enhancement module.","Moreover, we introduce a novel speech enhancement network, the Plugin Speech Enhancement (Plugin-SE).","The Plugin-SE is a dynamic neural network that includes the speech enhancement module, gate module, and weight prediction module.","Experimental results demonstrate that the proposed Plugin-SE approach is competitive or superior to other joint training methods across various downstream tasks."],"url":"http://arxiv.org/abs/2402.12746v1","category":"eess.AS"}
{"created":"2024-02-20 06:19:12","title":"DDC: A Vision for a Disaggregated Datacenter","abstract":"Datacenters of today have maintained the same architecture for decades using the server as the primary building block. However, this traditional approach suffers from under-utilization of its resources, often caused by over-allocating these resources when deploying applications to accommodate worst-case scenarios. Specifically, servers can quickly drain their over-allocated memory resources while their CPUs are not fully utilized.   This problem gives rise to a different school of thought, where resources are disaggregated instead of tightly bound to servers. This can address the utilization problem by allowing each type of resource to be allocated, utilized and freed separately as required. New high performance communication protocols, like CXL, could pave the way for practical implementations of resource disaggregation. In this article, we argue it is time to reconsider the datacenter architecture as a whole. We present our vision for a disaggregated datacenter aided by well-established computer architecture design methodologies.","sentences":["Datacenters of today have maintained the same architecture for decades using the server as the primary building block.","However, this traditional approach suffers from under-utilization of its resources, often caused by over-allocating these resources when deploying applications to accommodate worst-case scenarios.","Specifically, servers can quickly drain their over-allocated memory resources while their CPUs are not fully utilized.   ","This problem gives rise to a different school of thought, where resources are disaggregated instead of tightly bound to servers.","This can address the utilization problem by allowing each type of resource to be allocated, utilized and freed separately as required.","New high performance communication protocols, like CXL, could pave the way for practical implementations of resource disaggregation.","In this article, we argue it is time to reconsider the datacenter architecture as a whole.","We present our vision for a disaggregated datacenter aided by well-established computer architecture design methodologies."],"url":"http://arxiv.org/abs/2402.12742v1","category":"cs.AR"}
{"created":"2024-02-20 06:08:12","title":"Progress of photonuclear cross sections for medical radioisotope production at the SLEGS energy domain","abstract":"Photonuclear reactions using a laser Compton scattering (LCS) gamma source provide a new method for producing radioisotopes for medical applications. Compared with the conventional method, this method has the advantages of a high specific activity and less heat. Initiated by the Shanghai Laser Electron Gamma Source (SLEGS), we conducted a survey of potential photonuclear reactions, $(\\upgamma,n)$, $(\\upgamma,p)$, and $(\\upgamma,\\upgamma')$ whose cross-sections can be measured at SLEGS by summarizing the experimental progress. In general, the data are rare and occasionally inconsistent. Therefore, theoretical calculations are often used to evaluate the production of medical radioisotopes. Subsequently, we verified the model uncertainties of the widely used reaction code TALYS-1.96, using the experimental data of the \\ce{^100Mo}$(\\upgamma,n)$\\ce{^99Mo}, \\ce{^65Cu}$(\\upgamma,n)$\\ce{^64Cu}, and \\ce{^68Zn}$(\\upgamma,p)$\\ce{^67Cu} reactions.","sentences":["Photonuclear reactions using a laser Compton scattering (LCS) gamma source provide a new method for producing radioisotopes for medical applications.","Compared with the conventional method, this method has the advantages of a high specific activity and less heat.","Initiated by the Shanghai Laser Electron Gamma Source (SLEGS), we conducted a survey of potential photonuclear reactions, $(\\upgamma,n)$, $(\\upgamma,p)$, and $(\\upgamma,\\upgamma')$ whose cross-sections can be measured at SLEGS by summarizing the experimental progress.","In general, the data are rare and occasionally inconsistent.","Therefore, theoretical calculations are often used to evaluate the production of medical radioisotopes.","Subsequently, we verified the model uncertainties of the widely used reaction code TALYS-1.96, using the experimental data of the \\ce{^100Mo}$(\\upgamma,n)$\\ce{^99Mo}, \\ce{^65Cu}$(\\upgamma,n)$\\ce{^64Cu}, and \\ce{^68Zn}$(\\upgamma,p)$\\ce{^67Cu} reactions."],"url":"http://arxiv.org/abs/2402.12739v1","category":"nucl-ex"}
{"created":"2024-02-20 05:27:55","title":"Lowest-lying ${\\frac{1}{2}}^-$ and ${\\frac{3}{2}}^-$ $\u039b_{Q}$ resonances: from the strange to the bottom sectors","abstract":"We present a detailed study of the lowest-lying ${\\frac{1}{2}}^-$ and ${\\frac{3}{2}}^-$ $\\Lambda_{Q}$ resonances both in the heavy quark (bottom and charm) and the strange sectors. We have paid special attention to the interplay between the constituent quark-model and chiral baryon-meson degrees of freedom, which are coupled using a unitarized scheme consistent with leading-order heavy quark symmetries. We show that the $\\Lambda_b(5912)$ [$J^P=1/2^-$], $\\Lambda_b(5920)$ [$J^P=3/2^-$] and the $\\Lambda_c(2625)$ [$J^P=3/2^-$], and the $\\Lambda(1520)$ [$J^P=3/2^-$] admitting larger breaking corrections, are heavy-quark spin-flavor siblings. They can be seen as dressed quark-model states with $\\Sigma_{Q}^{(*)}\\pi$ molecular components of the order of 30\\%. The ${J^P=\\frac{1}{2}}^-$ $\\Lambda_c(2595)$ has, however, a higher molecular probability of at least $50$\\%, and even values greater than 70\\% can be easily accommodated. This is because it is located almost on top of the threshold of the $\\Sigma_c\\pi$ pair, which largely influences its properties. Although the light degrees of freedom in this resonance would be coupled to spin-parity $1^-$ as in the $\\Lambda_b(5912)$, $\\Lambda_b(5920)$ and $\\Lambda_c(2625)$, the $\\Lambda_c(2595)$ should not be considered as a heavy-quark spin-flavor partner of the former ones. We also show that the $\\Lambda(1405)$ chiral two-pole pattern does not have analogs in the $\\frac{1}{2}^-$ charmed and bottomed sectors, because the $ND^{(*)}$ and $N\\overline{B}{}^{(*)} $ channels do not play for heavy quarks the decisive role that the $N \\overline{K}$ does in the strange sector, and the notable influence of the bare quark-model states for the charm and bottom resonances. Finally, we predict the existence of two $\\Lambda_b(6070)$ and two $\\Lambda_c(2765)$ heavy-quark spin and flavor sibling odd parity states.","sentences":["We present a detailed study of the lowest-lying ${\\frac{1}{2}}^-$ and ${\\frac{3}{2}}^-$ $\\Lambda_{Q}$ resonances both in the heavy quark (bottom and charm) and the strange sectors.","We have paid special attention to the interplay between the constituent quark-model and chiral baryon-meson degrees of freedom, which are coupled using a unitarized scheme consistent with leading-order heavy quark symmetries.","We show that the $\\Lambda_b(5912)$ [$J^P=1/2^-$], $\\Lambda_b(5920)$ [$J^P=3/2^-$] and the $\\Lambda_c(2625)$ [$J^P=3/2^-$], and the $\\Lambda(1520)$ [$J^P=3/2^-$] admitting larger breaking corrections, are heavy-quark spin-flavor siblings.","They can be seen as dressed quark-model states with $\\Sigma_{Q}^{(*)}\\pi$ molecular components of the order of 30\\%.","The ${J^P=\\frac{1}{2}}^-$ $\\Lambda_c(2595)$ has, however, a higher molecular probability of at least $50$\\%, and even values greater than 70\\% can be easily accommodated.","This is because it is located almost on top of the threshold of the $\\Sigma_c\\pi$ pair, which largely influences its properties.","Although the light degrees of freedom in this resonance would be coupled to spin-parity $1^-$ as in the $\\Lambda_b(5912)$, $\\Lambda_b(5920)$ and $\\Lambda_c(2625)$, the $\\Lambda_c(2595)$ should not be considered as a heavy-quark spin-flavor partner of the former ones.","We also show that the $\\Lambda(1405)$ chiral two-pole pattern does not have analogs in the $\\frac{1}{2}^-$ charmed and bottomed sectors, because the $ND^{(*)}$ and $N\\overline{B}{}^{(*)} $ channels do not play for heavy quarks the decisive role that the $N \\overline{K}$ does in the strange sector, and the notable influence of the bare quark-model states for the charm and bottom resonances.","Finally, we predict the existence of two $\\Lambda_b(6070)$ and two $\\Lambda_c(2765)$ heavy-quark spin and flavor sibling odd parity states."],"url":"http://arxiv.org/abs/2402.12726v1","category":"hep-ph"}
{"created":"2024-02-20 05:11:20","title":"Structural Knowledge Informed Continual Multivariate Time Series Forecasting","abstract":"Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations. However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages). Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance. To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative MTS samples from each regime for memory replay. Specifically, we develop a forecasting model based on graph structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the MTS data. As such, MTS representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the continual learning context. Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of MTS data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime. Thorough empirical studies on synthetic and real-world benchmarks validate SKI-CL's efficacy and advantages over the state-of-the-art for continual MTS forecasting tasks.","sentences":["Recent studies in multivariate time series (MTS) forecasting reveal that explicitly modeling the hidden dependencies among different time series can yield promising forecasting performance and reliable explanations.","However, modeling variable dependencies remains underexplored when MTS is continuously accumulated under different regimes (stages).","Due to the potential distribution and dependency disparities, the underlying model may encounter the catastrophic forgetting problem, i.e., it is challenging to memorize and infer different types of variable dependencies across different regimes while maintaining forecasting performance.","To address this issue, we propose a novel Structural Knowledge Informed Continual Learning (SKI-CL) framework to perform MTS forecasting within a continual learning paradigm, which leverages structural knowledge to steer the forecasting model toward identifying and adapting to different regimes, and selects representative MTS samples from each regime for memory replay.","Specifically, we develop a forecasting model based on graph structure learning, where a consistency regularization scheme is imposed between the learned variable dependencies and the structural knowledge while optimizing the forecasting objective over the MTS data.","As such, MTS representations learned in each regime are associated with distinct structural knowledge, which helps the model memorize a variety of conceivable scenarios and results in accurate forecasts in the continual learning context.","Meanwhile, we develop a representation-matching memory replay scheme that maximizes the temporal coverage of MTS data to efficiently preserve the underlying temporal dynamics and dependency structures of each regime.","Thorough empirical studies on synthetic and real-world benchmarks validate SKI-CL's efficacy and advantages over the state-of-the-art for continual MTS forecasting tasks."],"url":"http://arxiv.org/abs/2402.12722v1","category":"cs.LG"}
{"created":"2024-02-20 05:02:14","title":"Fostering Joint Innovation: A Global Online Platform for Ideas Sharing and Collaboration","abstract":"In today's world, where moving forward hinges on innovation and working together, this article introduces a new global online platform that is all about sparking teamwork to come up with new ideas. This platform goes beyond borders and barriers between different fields, creating an exciting space where people from all over the world can swap ideas, get helpful feedback, and team up on exciting projects. What sets our platform apart is its ability to tap into the combined brainpower of a diverse bunch of users, giving people the power to come up with game-changing ideas that tackle big global problems. By making it easy for people to share ideas and promoting a culture of working together, our platform is like a buddy for innovation, boosting creativity and problem-solving on a global level. This article spills the details on what the platform aims to do, how it works, and what makes it special, emphasizing how it can kickstart creativity, ramp up problem-solving skills, and get different fields collaborating. It is not just a tool it is a whole new way of teaming up to make daily life better and build a global community of problem-solving pals.","sentences":["In today's world, where moving forward hinges on innovation and working together, this article introduces a new global online platform that is all about sparking teamwork to come up with new ideas.","This platform goes beyond borders and barriers between different fields, creating an exciting space where people from all over the world can swap ideas, get helpful feedback, and team up on exciting projects.","What sets our platform apart is its ability to tap into the combined brainpower of a diverse bunch of users, giving people the power to come up with game-changing ideas that tackle big global problems.","By making it easy for people to share ideas and promoting a culture of working together, our platform is like a buddy for innovation, boosting creativity and problem-solving on a global level.","This article spills the details on what the platform aims to do, how it works, and what makes it special, emphasizing how it can kickstart creativity, ramp up problem-solving skills, and get different fields collaborating.","It is not just a tool it is a whole new way of teaming up to make daily life better and build a global community of problem-solving pals."],"url":"http://arxiv.org/abs/2402.12718v1","category":"cs.HC"}
{"created":"2024-02-20 05:01:48","title":"Operahedron Lattices","abstract":"Laplante-Anfossi associated to each rooted plane tree a polytope called an operahedron. He also defined a partial order on the vertex set of an operahedron and asked if the resulting poset is a lattice. We answer this question in the affirmative, motivating us to name Laplante-Anfossi's posets operahedron lattices. The operahedron lattice of a chain with $n+1$ vertices is isomorphic to the $n$-th Tamari lattice, while the operahedron lattice of a claw with $n+1$ vertices is isomorphic to $\\mathrm{Weak}(\\mathfrak S_n)$, the weak order on the symmetric group $\\mathfrak S_n$. We characterize semidistributive operahedron lattices and trim operahedron lattices. Let $\\Delta_{\\mathrm{Weak}(\\mathfrak S_n)}(w_\\circ(k,n))$ be the principal order ideal of $\\mathrm{Weak}(\\mathfrak S_n)$ generated by the permutation ${w_\\circ(k,n)=k(k-1)\\cdots 1(k+1)(k+2)\\cdots n}$. Our final result states that the operahedron lattice of a broom with $n+1$ vertices and $k$ leaves is isomorphic to the subposet of $\\mathrm{Weak}(\\mathfrak S_n)$ consisting of the preimages of $\\Delta_{\\mathrm{Weak}(\\mathfrak S_n)}(w_\\circ(k,n))$ under West's stack-sorting map; as a consequence, we deduce that this subposet is a semidistributive lattice.","sentences":["Laplante-Anfossi associated to each rooted plane tree a polytope called an operahedron.","He also defined a partial order on the vertex set of an operahedron and asked if the resulting poset is a lattice.","We answer this question in the affirmative, motivating us to name Laplante-Anfossi's posets operahedron lattices.","The operahedron lattice of a chain with $n+1$ vertices is isomorphic to the $n$-th Tamari lattice, while the operahedron lattice of a claw with $n+1$ vertices is isomorphic to $\\mathrm{Weak}(\\mathfrak S_n)$, the weak order on the symmetric group $\\mathfrak S_n$. We characterize semidistributive operahedron lattices and trim operahedron lattices.","Let $\\Delta_{\\mathrm{Weak}(\\mathfrak S_n)}(w_\\circ(k,n))$ be the principal order ideal of $\\mathrm{Weak}(\\mathfrak S_n)$ generated by the permutation ${w_\\circ(k,n)=k(k-1)\\cdots 1(k+1)(k+2)\\cdots n}$. Our final result states that the operahedron lattice of a broom with $n+1$ vertices and $k$ leaves is isomorphic to the subposet of $\\mathrm{Weak}(\\mathfrak S_n)$ consisting of the preimages of $\\Delta_{\\mathrm{Weak}(\\mathfrak S_n)}(w_\\circ(k,n))$ under West's stack-sorting map; as a consequence, we deduce that this subposet is a semidistributive lattice."],"url":"http://arxiv.org/abs/2402.12717v1","category":"math.CO"}
{"created":"2024-02-20 03:33:54","title":"Robust-Wide: Robust Watermarking against Instruction-driven Image Editing","abstract":"Instruction-driven image editing allows users to quickly edit an image according to text instructions in a forward pass. Nevertheless, malicious users can easily exploit this technique to create fake images, which could cause a crisis of trust and harm the rights of the original image owners. Watermarking is a common solution to trace such malicious behavior. Unfortunately, instruction-driven image editing can significantly change the watermarked image at the semantic level, making it less robust and effective. We propose Robust-Wide, the first robust watermarking methodology against instruction-driven image editing. Specifically, we adopt the widely-used encoder-decoder framework for watermark embedding and extraction. To achieve robustness against semantic distortions, we introduce a novel Partial Instruction-driven Denoising Sampling Guidance (PIDSG) module, which consists of a large variety of instruction injections and substantial modifications of images at different semantic levels. With PIDSG, the encoder tends to embed the watermark into more robust and semantic-aware areas, which remains in existence even after severe image editing. Experiments demonstrate that Robust-Wide can effectively extract the watermark from the edited image with a low bit error rate of nearly 2.6% for 64-bit watermark messages. Meanwhile, it only induces a neglectable influence on the visual quality and editability of the original images. Moreover, Robust-Wide holds general robustness against different sampling configurations and other image editing methods such as ControlNet-InstructPix2Pix, MagicBrush, Inpainting and DDIM Inversion.","sentences":["Instruction-driven image editing allows users to quickly edit an image according to text instructions in a forward pass.","Nevertheless, malicious users can easily exploit this technique to create fake images, which could cause a crisis of trust and harm the rights of the original image owners.","Watermarking is a common solution to trace such malicious behavior.","Unfortunately, instruction-driven image editing can significantly change the watermarked image at the semantic level, making it less robust and effective.","We propose Robust-Wide, the first robust watermarking methodology against instruction-driven image editing.","Specifically, we adopt the widely-used encoder-decoder framework for watermark embedding and extraction.","To achieve robustness against semantic distortions, we introduce a novel Partial Instruction-driven Denoising Sampling Guidance (PIDSG) module, which consists of a large variety of instruction injections and substantial modifications of images at different semantic levels.","With PIDSG, the encoder tends to embed the watermark into more robust and semantic-aware areas, which remains in existence even after severe image editing.","Experiments demonstrate that Robust-Wide can effectively extract the watermark from the edited image with a low bit error rate of nearly 2.6% for 64-bit watermark messages.","Meanwhile, it only induces a neglectable influence on the visual quality and editability of the original images.","Moreover, Robust-Wide holds general robustness against different sampling configurations and other image editing methods such as ControlNet-InstructPix2Pix, MagicBrush, Inpainting and DDIM Inversion."],"url":"http://arxiv.org/abs/2402.12688v1","category":"cs.CR"}
{"created":"2024-02-20 03:27:53","title":"Learning on manifolds without manifold learning","abstract":"Function approximation based on data drawn randomly from an unknown distribution is an important problem in machine learning. In contrast to the prevalent paradigm of solving this problem by minimizing a loss functional, we have given a direct one-shot construction together with optimal error bounds under the manifold assumption; i.e., one assumes that the data is sampled from an unknown sub-manifold of a high dimensional Euclidean space. A great deal of research deals with obtaining information about this manifold, such as the eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and using this information for function approximation. This two step approach implies some extra errors in the approximation stemming from basic quantities of the data in addition to the errors inherent in function approximation. In Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to achieve function approximation without requiring the extraction of any information about the manifold other than its dimension. However, one cannot pin down the class of approximants used in that paper.   In this paper, we view the unknown manifold as a sub-manifold of an ambient hypersphere and study the question of constructing a one-shot approximation using the spherical polynomials based on the hypersphere. Our approach does not require preprocessing of the data to obtain information about the manifold other than its dimension. We give optimal rates of approximation for relatively \"rough\" functions.","sentences":["Function approximation based on data drawn randomly from an unknown distribution is an important problem in machine learning.","In contrast to the prevalent paradigm of solving this problem by minimizing a loss functional, we have given a direct one-shot construction together with optimal error bounds under the manifold assumption; i.e., one assumes that the data is sampled from an unknown sub-manifold of a high dimensional Euclidean space.","A great deal of research deals with obtaining information about this manifold, such as the eigendecomposition of the Laplace-Beltrami operator or coordinate charts, and using this information for function approximation.","This two step approach implies some extra errors in the approximation stemming from basic quantities of the data in addition to the errors inherent in function approximation.","In Neural Networks, 132:253268, 2020, we have proposed a one-shot direct method to achieve function approximation without requiring the extraction of any information about the manifold other than its dimension.","However, one cannot pin down the class of approximants used in that paper.   ","In this paper, we view the unknown manifold as a sub-manifold of an ambient hypersphere and study the question of constructing a one-shot approximation using the spherical polynomials based on the hypersphere.","Our approach does not require preprocessing of the data to obtain information about the manifold other than its dimension.","We give optimal rates of approximation for relatively \"rough\" functions."],"url":"http://arxiv.org/abs/2402.12687v1","category":"cs.LG"}
{"created":"2024-02-20 03:14:47","title":"TorchCP: A Library for Conformal Prediction based on PyTorch","abstract":"TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\\href{https://github.com/ml-stat-Sustech/TorchCP}{\\text{this https URL}}$.","sentences":["TorchCP is a Python toolbox for conformal prediction research on deep learning models.","It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output).","TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations.","The code is licensed under the LGPL license and is open-sourced at $\\href{https://github.com/ml-stat-Sustech/TorchCP}{\\text{this https URL}}$."],"url":"http://arxiv.org/abs/2402.12683v1","category":"cs.LG"}
{"created":"2024-02-20 02:45:20","title":"Beyond Worst-case Attacks: Robust RL with Adaptive Defense via Non-dominated Policies","abstract":"In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time. Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios. While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks. To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks. We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aiming for efficient adaptation within a finite policy class $\\Tilde{\\Pi}$, which can resort to an adversarial bandit subroutine. In light of the importance of a small, finite $\\Tilde{\\Pi}$, we propose a novel training-time algorithm to iteratively discover \\textit{non-dominated policies}, forming a near-optimal and minimal $\\Tilde{\\Pi}$, thereby ensuring both robustness and test-time efficiency. Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios.","sentences":["In light of the burgeoning success of reinforcement learning (RL) in diverse real-world applications, considerable focus has been directed towards ensuring RL policies are robust to adversarial attacks during test time.","Current approaches largely revolve around solving a minimax problem to prepare for potential worst-case scenarios.","While effective against strong attacks, these methods often compromise performance in the absence of attacks or the presence of only weak attacks.","To address this, we study policy robustness under the well-accepted state-adversarial attack model, extending our focus beyond only worst-case attacks.","We first formalize this task at test time as a regret minimization problem and establish its intrinsic hardness in achieving sublinear regret when the baseline policy is from a general continuous policy class, $\\Pi$. This finding prompts us to \\textit{refine} the baseline policy class $\\Pi$ prior to test time, aiming for efficient adaptation within a finite policy class $\\Tilde{\\Pi}$, which can resort to an adversarial bandit subroutine.","In light of the importance of a small, finite $\\Tilde{\\Pi}$, we propose a novel training-time algorithm to iteratively discover \\textit{non-dominated policies}, forming a near-optimal and minimal $\\Tilde{\\Pi}$, thereby ensuring both robustness and test-time efficiency.","Empirical validation on the Mujoco corroborates the superiority of our approach in terms of natural and robust performance, as well as adaptability to various attack scenarios."],"url":"http://arxiv.org/abs/2402.12673v1","category":"cs.LG"}
{"created":"2024-02-20 02:38:21","title":"Lax-Wendroff Flux Reconstruction for advection-diffusion equations with error-based time stepping","abstract":"This work introduces an extension of the high order, single stage Lax-Wendroff Flux Reconstruction (LWFR) of Babbar et al., JCP (2022) to solve second order time-dependent partial differential equations in conservative form on curvilinear meshes. The method uses BR1 scheme to reduce the system to first order so that the earlier LWFR scheme can be applied. The work makes use of the embedded error-based time stepping introduced in Babbar, Chandrashekar (2024) which becomes particularly relevant in the absence of CFL stability limit for parabolic equations. The scheme is verified to show optimal order convergence and validated with transonic flow over airfoil and unsteady flow over cylinder.","sentences":["This work introduces an extension of the high order, single stage Lax-Wendroff Flux Reconstruction (LWFR) of Babbar et al., JCP (2022) to solve second order time-dependent partial differential equations in conservative form on curvilinear meshes.","The method uses BR1 scheme to reduce the system to first order so that the earlier LWFR scheme can be applied.","The work makes use of the embedded error-based time stepping introduced in Babbar, Chandrashekar (2024) which becomes particularly relevant in the absence of CFL stability limit for parabolic equations.","The scheme is verified to show optimal order convergence and validated with transonic flow over airfoil and unsteady flow over cylinder."],"url":"http://arxiv.org/abs/2402.12669v1","category":"math.NA"}
{"created":"2024-02-20 02:28:57","title":"Antifragile Perimeter Control: Anticipating and Gaining from Disruptions with Reinforcement Learning","abstract":"The optimal operation of transportation networks is often susceptible to unexpected disruptions, such as traffic incidents and social events. Many established control strategies rely on mathematical models that struggle to cope with real-world uncertainties, leading to a significant decline in effectiveness when faced with substantial disruptions. While previous research works have dedicated efforts to improving the robustness or resilience of transportation systems against disruptions, this paper applies the cutting-edge concept of antifragility to better design a traffic control strategy for urban road networks. Antifragility sets itself apart from robustness and resilience as it represents a system's ability to not only withstand stressors, shocks, and volatility but also thrive and enhance performance in the presence of such adversarial events. Hence, modern transportation systems call for solutions that are antifragile. In this work, we propose a model-free deep Reinforcement Learning (RL) scheme to control a two-region urban traffic perimeter network. The system exploits the learning capability of RL under disruptions to achieve antifragility. By monitoring the change rate and curvature of the traffic state with the RL framework, the proposed algorithm anticipates imminent disruptions. An additional term is also integrated into the RL algorithm as redundancy to improve the performance under disruption scenarios. When compared to a state-of-the-art model predictive control approach and a state-of-the-art RL algorithm, our proposed method demonstrates two antifragility-related properties: (a) gradual performance improvement under disruptions of constant magnitude; and (b) increasingly superior performance under growing disruptions.","sentences":["The optimal operation of transportation networks is often susceptible to unexpected disruptions, such as traffic incidents and social events.","Many established control strategies rely on mathematical models that struggle to cope with real-world uncertainties, leading to a significant decline in effectiveness when faced with substantial disruptions.","While previous research works have dedicated efforts to improving the robustness or resilience of transportation systems against disruptions, this paper applies the cutting-edge concept of antifragility to better design a traffic control strategy for urban road networks.","Antifragility sets itself apart from robustness and resilience as it represents a system's ability to not only withstand stressors, shocks, and volatility but also thrive and enhance performance in the presence of such adversarial events.","Hence, modern transportation systems call for solutions that are antifragile.","In this work, we propose a model-free deep Reinforcement Learning (RL) scheme to control a two-region urban traffic perimeter network.","The system exploits the learning capability of RL under disruptions to achieve antifragility.","By monitoring the change rate and curvature of the traffic state with the RL framework, the proposed algorithm anticipates imminent disruptions.","An additional term is also integrated into the RL algorithm as redundancy to improve the performance under disruption scenarios.","When compared to a state-of-the-art model predictive control approach and a state-of-the-art RL algorithm, our proposed method demonstrates two antifragility-related properties: (a) gradual performance improvement under disruptions of constant magnitude; and (b) increasingly superior performance under growing disruptions."],"url":"http://arxiv.org/abs/2402.12665v1","category":"eess.SY"}
{"created":"2024-02-20 02:26:48","title":"Discriminant Distance-Aware Representation on Deterministic Uncertainty Quantification Methods","abstract":"Uncertainty estimation is a crucial aspect of deploying dependable deep learning models in safety-critical systems. In this study, we introduce a novel and efficient method for deterministic uncertainty estimation called Discriminant Distance-Awareness Representation (DDAR). Our approach involves constructing a DNN model that incorporates a set of prototypes in its latent representations, enabling us to analyze valuable feature information from the input data. By leveraging a distinction maximization layer over optimal trainable prototypes, DDAR can learn a discriminant distance-awareness representation. We demonstrate that DDAR overcomes feature collapse by relaxing the Lipschitz constraint that hinders the practicality of deterministic uncertainty methods (DUMs) architectures. Our experiments show that DDAR is a flexible and architecture-agnostic method that can be easily integrated as a pluggable layer with distance-sensitive metrics, outperforming state-of-the-art uncertainty estimation methods on multiple benchmark problems.","sentences":["Uncertainty estimation is a crucial aspect of deploying dependable deep learning models in safety-critical systems.","In this study, we introduce a novel and efficient method for deterministic uncertainty estimation called Discriminant Distance-Awareness Representation (DDAR).","Our approach involves constructing a DNN model that incorporates a set of prototypes in its latent representations, enabling us to analyze valuable feature information from the input data.","By leveraging a distinction maximization layer over optimal trainable prototypes, DDAR can learn a discriminant distance-awareness representation.","We demonstrate that DDAR overcomes feature collapse by relaxing the Lipschitz constraint that hinders the practicality of deterministic uncertainty methods (DUMs) architectures.","Our experiments show that DDAR is a flexible and architecture-agnostic method that can be easily integrated as a pluggable layer with distance-sensitive metrics, outperforming state-of-the-art uncertainty estimation methods on multiple benchmark problems."],"url":"http://arxiv.org/abs/2402.12664v1","category":"cs.LG"}
{"created":"2024-02-20 02:12:15","title":"Coded Backscattering Communication with LTE Pilots as Ambient Signal","abstract":"The 3GPP has recently conducted a study on the Ambient Internet of Things (AIoT), with a particular emphasis on examining backscatter communications as one of the primary techniques under consideration. Previous investigations into Ambient Backscatter Communications (AmBC) within the long term evolution (LTE) downlink have shown that it is feasible to utilize the user equipment channel estimator as a receiver for demodulating frequency shift keyed (FSK) messages transmitted by the backscatter devices. In practical deployment scenarios, the backscattered link often experiences a low signal-to-noise ratio, leading to subpar bit error rate (BER) performance in the case of uncoded transmissions. In this paper, we propose the adoption of the same convolutional coding methodology for backscatter links that is already employed for LTE downlink control signals. This approach facilitates the reuse of identical demodulation functions at the modem for both control signals and backscattered AIoT messages. To assess the performance of the proposed scheme, we conducted experiments utilizing real LTE downlink signals generated by a mobile operator within an office environment. When compared to uncoded FSK, convolutional channel coding delivers a notable gain of approximately 6 dB at a BER of $10^{-3}$. Consequently, the AmBC system demonstrates a high level of reliability, achieving a BER of $10^{-3}$ at a Signal-to-Noise Ratio (SNR) of 5 dB.","sentences":["The 3GPP has recently conducted a study on the Ambient Internet of Things (AIoT), with a particular emphasis on examining backscatter communications as one of the primary techniques under consideration.","Previous investigations into Ambient Backscatter Communications (AmBC) within the long term evolution (LTE) downlink have shown that it is feasible to utilize the user equipment channel estimator as a receiver for demodulating frequency shift keyed (FSK) messages transmitted by the backscatter devices.","In practical deployment scenarios, the backscattered link often experiences a low signal-to-noise ratio, leading to subpar bit error rate (BER) performance in the case of uncoded transmissions.","In this paper, we propose the adoption of the same convolutional coding methodology for backscatter links that is already employed for LTE downlink control signals.","This approach facilitates the reuse of identical demodulation functions at the modem for both control signals and backscattered AIoT messages.","To assess the performance of the proposed scheme, we conducted experiments utilizing real LTE downlink signals generated by a mobile operator within an office environment.","When compared to uncoded FSK, convolutional channel coding delivers a notable gain of approximately 6 dB at a BER of $10^{-3}$. Consequently, the AmBC system demonstrates a high level of reliability, achieving a BER of $10^{-3}$ at a Signal-to-Noise Ratio (SNR) of 5 dB."],"url":"http://arxiv.org/abs/2402.12657v1","category":"eess.SP"}
{"created":"2024-02-20 02:07:01","title":"Ego Group Partition: A Novel Framework for Improving Ego Experiments in Social Networks","abstract":"Estimating the average treatment effect in social networks is challenging due to individuals influencing each other. One approach to address interference is ego cluster experiments, where each cluster consists of a central individual (ego) and its peers (alters). Clusters are randomized, and only the effects on egos are measured. In this work, we propose an improved framework for ego cluster experiments called ego group partition (EGP), which directly generates two groups and an ego sub-population instead of ego clusters. Under specific model assumptions, we propose two ego group partition algorithms. Compared to the original ego clustering algorithm, our algorithms produce more egos, yield smaller biases, and support parallel computation. The performance of our algorithms is validated through simulation and real-world case studies.","sentences":["Estimating the average treatment effect in social networks is challenging due to individuals influencing each other.","One approach to address interference is ego cluster experiments, where each cluster consists of a central individual (ego) and its peers (alters).","Clusters are randomized, and only the effects on egos are measured.","In this work, we propose an improved framework for ego cluster experiments called ego group partition (EGP), which directly generates two groups and an ego sub-population instead of ego clusters.","Under specific model assumptions, we propose two ego group partition algorithms.","Compared to the original ego clustering algorithm, our algorithms produce more egos, yield smaller biases, and support parallel computation.","The performance of our algorithms is validated through simulation and real-world case studies."],"url":"http://arxiv.org/abs/2402.12655v1","category":"cs.SI"}
{"created":"2024-02-20 02:03:59","title":"Unbiased Estimation for Total Treatment Effect Under Interference Using Aggregated Dyadic Data","abstract":"In social media platforms, user behavior is often influenced by interactions with other users, complicating the accurate estimation of causal effects in traditional A/B experiments. This study investigates situations where an individual's outcome can be broken down into the sum of multiple pairwise outcomes, a reflection of user interactions. These outcomes, referred to as dyadic data, are prevalent in many social network contexts. Utilizing a Bernoulli randomized design, we introduce a novel unbiased estimator for the total treatment effect (TTE), which quantifies the difference in population mean when all individuals are assigned to treatment versus control groups. We further explore the bias of our estimator in scenarios where it is impractical to include all individuals in the experiment, a common constraint in online control experiments. Our numerical results reveal that our proposed estimator consistently outperforms some commonly used estimators, underscoring its potential for more precise causal effect estimation in social media environments.","sentences":["In social media platforms, user behavior is often influenced by interactions with other users, complicating the accurate estimation of causal effects in traditional A/B experiments.","This study investigates situations where an individual's outcome can be broken down into the sum of multiple pairwise outcomes, a reflection of user interactions.","These outcomes, referred to as dyadic data, are prevalent in many social network contexts.","Utilizing a Bernoulli randomized design, we introduce a novel unbiased estimator for the total treatment effect (TTE), which quantifies the difference in population mean when all individuals are assigned to treatment versus control groups.","We further explore the bias of our estimator in scenarios where it is impractical to include all individuals in the experiment, a common constraint in online control experiments.","Our numerical results reveal that our proposed estimator consistently outperforms some commonly used estimators, underscoring its potential for more precise causal effect estimation in social media environments."],"url":"http://arxiv.org/abs/2402.12653v1","category":"cs.SI"}
{"created":"2024-02-20 01:48:43","title":"On the group pseudo-algebra of finite groups","abstract":"Let $G$ be a finite group. The group pseudo-algebra of $G$ is defined as the multi-set $C(G)=\\{(d,m_G(d))\\mid d\\in{\\rm Cod}(G)\\},$ where $m_G(d)$ is the number of irreducible characters of with codegree $d\\in {\\rm Cod}(G)$. We show that there exist two finite $p$-groups with distinct orders that have the same group pseudo-algebra, providing an answer to Question 3.2 in \\cite{Moreto2023}. In addition, we also discuss under what hypothesis two $p$-groups with the same group pseudo-algebra will be isomorphic.","sentences":["Let $G$ be a finite group.","The group pseudo-algebra of $G$ is defined as the multi-set $C(G)=\\{(d,m_G(d))\\mid d\\in{\\rm Cod}(G)\\},$ where $m_G(d)$ is the number of irreducible characters of with codegree $d\\in {\\rm Cod}(G)$.","We show that there exist two finite $p$-groups with distinct orders that have the same group pseudo-algebra, providing an answer to Question 3.2 in \\cite{Moreto2023}.","In addition, we also discuss under what hypothesis two $p$-groups with the same group pseudo-algebra will be isomorphic."],"url":"http://arxiv.org/abs/2402.12648v1","category":"math.GR"}
{"created":"2024-02-20 01:48:33","title":"DiffusionNOCS: Managing Symmetry and Uncertainty in Sim2Real Multi-Modal Category-level Pose Estimation","abstract":"This paper addresses the challenging problem of category-level pose estimation. Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training. In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation. Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations. We demonstrate the effectiveness of our method by testing it on a range of real datasets. Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain.","sentences":["This paper addresses the challenging problem of category-level pose estimation.","Current state-of-the-art methods for this task face challenges when dealing with symmetric objects and when attempting to generalize to new environments solely through synthetic data training.","In this work, we address these challenges by proposing a probabilistic model that relies on diffusion to estimate dense canonical maps crucial for recovering partial object shapes as well as establishing correspondences essential for pose estimation.","Furthermore, we introduce critical components to enhance performance by leveraging the strength of the diffusion models with multi-modal input representations.","We demonstrate the effectiveness of our method by testing it on a range of real datasets.","Despite being trained solely on our generated synthetic data, our approach achieves state-of-the-art performance and unprecedented generalization qualities, outperforming baselines, even those specifically trained on the target domain."],"url":"http://arxiv.org/abs/2402.12647v1","category":"cs.CV"}
{"created":"2024-02-20 01:31:17","title":"Social Mechanics","abstract":"Social physics is the application of ideas, concepts and tools from physics to study social phenomena. In this article, we present a mechanical theory underlying a mathematical treatment of social physics. We explore the possibility of using fundamental concepts like position, motion, inertia, and interaction, to effectively regard social phenomena analogously to particles interacting with each other in physics. From these concepts, along with heuristics of social change, we investigate the notions of free motion, motion under the influence of a net deterministic, as well as stochastic force. To test these ideas we model partisan preferences in the United States according to the outcomes of presidential elections.","sentences":["Social physics is the application of ideas, concepts and tools from physics to study social phenomena.","In this article, we present a mechanical theory underlying a mathematical treatment of social physics.","We explore the possibility of using fundamental concepts like position, motion, inertia, and interaction, to effectively regard social phenomena analogously to particles interacting with each other in physics.","From these concepts, along with heuristics of social change, we investigate the notions of free motion, motion under the influence of a net deterministic, as well as stochastic force.","To test these ideas we model partisan preferences in the United States according to the outcomes of presidential elections."],"url":"http://arxiv.org/abs/2402.12638v1","category":"physics.soc-ph"}
{"created":"2024-02-20 01:12:59","title":"Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors","abstract":"Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible. Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data. However, such a process may also raise concerns regarding data poisoning attacks. For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning. In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors. Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks to directly craft poisoned data in the input space. However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space. Our experiments examine such attacks in popular downstream tasks of fine-tuning on the same dataset and transfer learning that considers domain adaptation. Empirical results reveal that transfer learning is more vulnerable to our attacks. Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks.","sentences":["Machine learning models have achieved great success in supervised learning tasks for end-to-end training, which requires a large amount of labeled data that is not always feasible.","Recently, many practitioners have shifted to self-supervised learning methods that utilize cheap unlabeled data to learn a general feature extractor via pre-training, which can be further applied to personalized downstream tasks by simply training an additional linear layer with limited labeled data.","However, such a process may also raise concerns regarding data poisoning attacks.","For instance, indiscriminate data poisoning attacks, which aim to decrease model utility by injecting a small number of poisoned data into the training set, pose a security risk to machine learning models, but have only been studied for end-to-end supervised learning.","In this paper, we extend the exploration of the threat of indiscriminate attacks on downstream tasks that apply pre-trained feature extractors.","Specifically, we propose two types of attacks: (1) the input space attacks, where we modify existing attacks to directly craft poisoned data in the input space.","However, due to the difficulty of optimization under constraints, we further propose (2) the feature targeted attacks, where we mitigate the challenge with three stages, firstly acquiring target parameters for the linear head; secondly finding poisoned features by treating the learned feature representations as a dataset; and thirdly inverting the poisoned features back to the input space.","Our experiments examine such attacks in popular downstream tasks of fine-tuning on the same dataset and transfer learning that considers domain adaptation.","Empirical results reveal that transfer learning is more vulnerable to our attacks.","Additionally, input space attacks are a strong threat if no countermeasures are posed, but are otherwise weaker than feature targeted attacks."],"url":"http://arxiv.org/abs/2402.12626v1","category":"cs.LG"}
{"created":"2024-02-20 01:04:21","title":"Reflect-RL: Two-Player Online RL Fine-Tuning for LMs","abstract":"As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model. To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more efficiently. Empirically, we verify that Reflect-RL outperforms SFT and online RL without reflection. Testing results indicate GPT-2-xl after Reflect-RL also outperforms those of untuned pre-trained LMs, such as Mistral 7B.","sentences":["As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular.","These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance.","However, only a few works attempted to directly train the LMs within interactive decision-making environments.","We aim to create an effective mechanism to fine-tune LMs with online reinforcement learning (RL) in these environments.","We propose Reflect-RL, a two-player system to fine-tune an LM using online RL, where a frozen reflection model assists the policy model.","To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model.","Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more efficiently.","Empirically, we verify that Reflect-RL outperforms SFT and online RL without reflection.","Testing results indicate GPT-2-xl after Reflect-RL also outperforms those of untuned pre-trained LMs, such as Mistral 7B."],"url":"http://arxiv.org/abs/2402.12621v1","category":"cs.LG"}
{"created":"2024-02-20 00:38:54","title":"Sharing Bell nonlocality of bipartite high-dimensional pure states using only projective measurements","abstract":"Bell nonlocality is the key quantum resource in some device-independent quantum information processing. It is of great importance to study the efficient sharing of this resource. Unsharp measurements are widely used in sharing the nonlocality of an entangled state shared among several sequential observers. Recently, the authors in [Phys. Rev. Lett.129, 230402(2022)] showed that the Bell nonlocality of two-qubit pure states can be shared even when one only uses projective measurements and local randomness. We demonstrate that projective measurements are also sufficient for sharing the Bell nonlocality of arbitrary high-dimensional pure bipartite states. Our results promote further understanding of the nonlocality sharing of high-dimensional quantum states under projective measurements.","sentences":["Bell nonlocality is the key quantum resource in some device-independent quantum information processing.","It is of great importance to study the efficient sharing of this resource.","Unsharp measurements are widely used in sharing the nonlocality of an entangled state shared among several sequential observers.","Recently, the authors in [Phys. Rev. Lett.129, 230402(2022)] showed that the Bell nonlocality of two-qubit pure states can be shared even when one only uses projective measurements and local randomness.","We demonstrate that projective measurements are also sufficient for sharing the Bell nonlocality of arbitrary high-dimensional pure bipartite states.","Our results promote further understanding of the nonlocality sharing of high-dimensional quantum states under projective measurements."],"url":"http://arxiv.org/abs/2402.12614v1","category":"quant-ph"}
{"created":"2024-02-20 00:14:01","title":"A System Development Kit for Big Data Applications on FPGA-based Clusters: The EVEREST Approach","abstract":"Modern big data workflows are characterized by computationally intensive kernels. The simulated results are often combined with knowledge extracted from AI models to ultimately support decision-making. These energy-hungry workflows are increasingly executed in data centers with energy-efficient hardware accelerators since FPGAs are well-suited for this task due to their inherent parallelism. We present the H2020 project EVEREST, which has developed a system development kit (SDK) to simplify the creation of FPGA-accelerated kernels and manage the execution at runtime through a virtualization environment. This paper describes the main components of the EVEREST SDK and the benefits that can be achieved in our use cases.","sentences":["Modern big data workflows are characterized by computationally intensive kernels.","The simulated results are often combined with knowledge extracted from AI models to ultimately support decision-making.","These energy-hungry workflows are increasingly executed in data centers with energy-efficient hardware accelerators since FPGAs are well-suited for this task due to their inherent parallelism.","We present the H2020 project EVEREST, which has developed a system development kit (SDK) to simplify the creation of FPGA-accelerated kernels and manage the execution at runtime through a virtualization environment.","This paper describes the main components of the EVEREST SDK and the benefits that can be achieved in our use cases."],"url":"http://arxiv.org/abs/2402.12612v1","category":"cs.AR"}
{"created":"2024-02-20 00:12:39","title":"On Jordan superderivations and Jordan super-biderivations of trivial extensions and triangular matrix rings","abstract":"Triangular matrix rings are example of trivial extensions. In this article we describe the Jordan superderivations of the trivial extensions and upper triangular matrix rings. We deduce then that any Jordan superderivation of an upper triangular matrix ring, under some conditions, is a derivation, and any Jordan super-biderivation of a trivial extension, and hence an upper triangular matrix ring, is a Jordan biderivation.","sentences":["Triangular matrix rings are example of trivial extensions.","In this article we describe the Jordan superderivations of the trivial extensions and upper triangular matrix rings.","We deduce then that any Jordan superderivation of an upper triangular matrix ring, under some conditions, is a derivation, and any Jordan super-biderivation of a trivial extension, and hence an upper triangular matrix ring, is a Jordan biderivation."],"url":"http://arxiv.org/abs/2402.12611v1","category":"math.RA"}
{"created":"2024-02-20 00:08:08","title":"Existence of Approximately Macroscopically Unique States","abstract":"Let $H$ be an infinite dimensional separable Hilbert space and $B(H)$ the C*-algebra of bounded operators on $H.$ Suppose that $T_1,T_2,..., T_n$ are self-adjoint operators in $B(H).$ We show that, if commutators $[T_i, T_j]$ are sufficiently small in norm, then ``Approximately Macroscopically Unique\" states always exist for any values in a synthetic spectrum of the $n$-tuple of self-adjoint operators. This is achieved under the circumstance for which the $n$-tuple may not be approximated by commuting ones. This answers a question proposed by David Mumford for measurements in quantum theory. If commutators are not small in norm but small modulo compact operators, then ``Approximate Macroscopic Uniqueness\" states also exist.","sentences":["Let $H$ be an infinite dimensional separable Hilbert space and $B(H)$ the C*-algebra of bounded operators on $H.$ Suppose that $T_1,T_2,..., T_n$ are self-adjoint operators in $B(H).$ We show that, if commutators $[T_i, T_j]$ are sufficiently small in norm, then ``Approximately Macroscopically Unique\" states always exist for any values in a synthetic spectrum of the $n$-tuple of self-adjoint operators.","This is achieved under the circumstance for which the $n$-tuple may not be approximated by commuting ones.","This answers a question proposed by David Mumford for measurements in quantum theory.","If commutators are not small in norm but small modulo compact operators, then ``Approximate Macroscopic Uniqueness\" states also exist."],"url":"http://arxiv.org/abs/2402.12609v1","category":"math.OA"}
{"created":"2024-02-20 00:04:40","title":"Inference on LATEs with covariates","abstract":"In theory, two-stage least squares (TSLS) identifies a weighted average of covariate-specific local average treatment effects (LATEs) from a saturated specification without making parametric assumptions on how available covariates enter the model. In practice, TSLS is severely biased when saturation leads to a number of control dummies that is of the same order of magnitude as the sample size, and the use of many, arguably weak, instruments. This paper derives asymptotically valid tests and confidence intervals for an estimand that identifies the weighted average of LATEs targeted by saturated TSLS, even when the number of control dummies and instrument interactions is large. The proposed inference procedure is robust against four key features of saturated economic data: treatment effect heterogeneity, covariates with rich support, weak identification strength, and conditional heteroskedasticity.","sentences":["In theory, two-stage least squares (TSLS) identifies a weighted average of covariate-specific local average treatment effects (LATEs) from a saturated specification without making parametric assumptions on how available covariates enter the model.","In practice, TSLS is severely biased when saturation leads to a number of control dummies that is of the same order of magnitude as the sample size, and the use of many, arguably weak, instruments.","This paper derives asymptotically valid tests and confidence intervals for an estimand that identifies the weighted average of LATEs targeted by saturated TSLS, even when the number of control dummies and instrument interactions is large.","The proposed inference procedure is robust against four key features of saturated economic data: treatment effect heterogeneity, covariates with rich support, weak identification strength, and conditional heteroskedasticity."],"url":"http://arxiv.org/abs/2402.12607v1","category":"econ.EM"}
{"created":"2024-02-19 23:18:57","title":"Long Term Space Data and Informatics Needs","abstract":"Policy Brief on \"Long Term Space Data and Informatics Needs\", distilled from the corresponding panel that was part of the discussions during S20 Policy Webinar on Astroinformatics for Sustainable Development held on 6-7 July 2023.   Persistent space data gathering, retention, transmission, and analysis play a pivotal role in deepening our grasp of the Universe and fostering the achievement of global sustainable development goals. Long-term data storage and curation is crucial not only to make the wide range of burgeoning data sets available to the global science community, but also to stabilize those data sets, enabling new science in the future to analyse long-term trends over unprecedented time spans. In addition to this, over the long-term, the imperative to store all data on the ground should be ameliorated by use of space-based data stores --maintained and seen to be as reliable as any other data archive. This concept is sometimes referred to as Memory of the Sky. Storing the data must be accompanied by the ability to analyse them. Several concepts covered below acknowledge roots and inspiration based in the Virtual Observatory effort. Within this policy document, we delve into the complexities surrounding the long-term utilization of space data and informatics, shedding light on the challenges and opportunities inherent in this endeavour. Further, we present a series of pragmatic recommendations designed to address these challenges proactively.   The policy webinar took place during the G20 presidency in India (2023). A summary based on the seven panels can be found here: arxiv:2401.04623.","sentences":["Policy Brief on \"Long Term Space Data and Informatics Needs\", distilled from the corresponding panel that was part of the discussions during S20 Policy Webinar on Astroinformatics for Sustainable Development held on 6-7 July 2023.   ","Persistent space data gathering, retention, transmission, and analysis play a pivotal role in deepening our grasp of the Universe and fostering the achievement of global sustainable development goals.","Long-term data storage and curation is crucial not only to make the wide range of burgeoning data sets available to the global science community, but also to stabilize those data sets, enabling new science in the future to analyse long-term trends over unprecedented time spans.","In addition to this, over the long-term, the imperative to store all data on the ground should be ameliorated by use of space-based data stores --maintained and seen to be as reliable as any other data archive.","This concept is sometimes referred to as Memory of the Sky.","Storing the data must be accompanied by the ability to analyse them.","Several concepts covered below acknowledge roots and inspiration based in the Virtual Observatory effort.","Within this policy document, we delve into the complexities surrounding the long-term utilization of space data and informatics, shedding light on the challenges and opportunities inherent in this endeavour.","Further, we present a series of pragmatic recommendations designed to address these challenges proactively.   ","The policy webinar took place during the G20 presidency in India (2023).","A summary based on the seven panels can be found here: arxiv:2401.04623."],"url":"http://arxiv.org/abs/2402.12594v1","category":"astro-ph.IM"}
{"created":"2024-02-19 23:16:28","title":"Global existence for non-homogeneous incompressible inviscid fluids in presence of Ekman pumping","abstract":"In this paper, we study the global solvability of the density-dependent incompressible Euler equations, supplemented with a damping term of the form $ \\mathfrak{D}_{\\alpha}^{\\gamma}(\\rho, u) = \\alpha \\rho^{\\gamma} u $, where $\\alpha>0$ and $ \\gamma \\in \\{0,1\\} $. To some extent, this system can be seen as a simplified model describing the mean dynamics in the ocean; from this perspective, the damping term can be interpreted as a term encoding the effects of the celebrated Ekman pumping in the system.   On the one hand, in the general case of space dimension $d\\geq 2$, we establish global well-posedness in the Besov spaces framework, under a non-linear smallness condition involving the size of the initial velocity field $u_0$, of the initial non-homogeneity $\\rho_0-1$ and of the damping coefficient $\\alpha$. On the other hand, in the specific situation of planar motions and damping term with $\\gamma=1$, we exhibit a second smallness condition implying global existence, which in particular yields global well-posedness for arbitrarily large initial velocity fields, provided the initial density variations $\\rho_0-1$ are small enough. The formulated smallness conditions rely only on the endpoint Besov norm $B^1_{\\infty,1}$ of the initial datum, whereas, as a byproduct of our analysis, we derive exponential decay of the velocity field and of the pressure gradient in the high regularity norms $B^s_{p,r}$.","sentences":["In this paper, we study the global solvability of the density-dependent incompressible Euler equations, supplemented with a damping term of the form $ \\mathfrak{D}_{\\alpha}^{\\gamma}(\\rho, u) = \\alpha \\rho^{\\gamma} u $, where $\\alpha>0$ and $ \\gamma \\in \\{0,1\\} $.","To some extent, this system can be seen as a simplified model describing the mean dynamics in the ocean; from this perspective, the damping term can be interpreted as a term encoding the effects of the celebrated Ekman pumping in the system.   ","On the one hand, in the general case of space dimension $d\\geq 2$, we establish global well-posedness in the Besov spaces framework, under a non-linear smallness condition involving the size of the initial velocity field $u_0$, of the initial non-homogeneity $\\rho_0-1$ and of the damping coefficient $\\alpha$. On the other hand, in the specific situation of planar motions and damping term with $\\gamma=1$, we exhibit a second smallness condition implying global existence, which in particular yields global well-posedness for arbitrarily large initial velocity fields, provided the initial density variations $\\rho_0-1$ are small enough.","The formulated smallness conditions rely only on the endpoint Besov norm $B^1_{\\infty,1}$ of the initial datum, whereas, as a byproduct of our analysis, we derive exponential decay of the velocity field and of the pressure gradient in the high regularity norms $B^s_{p,r}$."],"url":"http://arxiv.org/abs/2402.12592v1","category":"math.AP"}
{"created":"2024-02-19 22:52:40","title":"Local and local-to-global Principles for zero-cycles on geometrically Kummer $K3$ surfaces","abstract":"Let $X$ be a $K3$ surface over a $p$-adic field $k$ such that for some abelian surface $A$ isogenous to a product of two elliptic curves, there is an isomorphism over the algebraic closure of $k$ between $X$ and the Kummer surface associated to $A$. Under some assumptions on the reduction types of the elliptic curve factors of $A$, we prove that the Chow group $A_0(X)$ of zero-cycles of degree $0$ on $X$ is the direct sum of a divisible group and a finite group. This proves a conjecture of Raskind and Spiess and of Colliot-Th\\'{e}l\\`{e}ne and it is the first instance for $K3$ surfaces when this conjecture is proved in full. This class of $K3$'s includes, among others, the diagonal quartic surfaces. In the case of good ordinary reduction we describe many cases when the finite summand of $A_0(X)$ can be completely determined.   Using these results, we explore a local-to-global conjecture of Colliot-Th\\'{e}lene, Sansuc, Kato and Saito which, roughly speaking, predicts that the Brauer-Manin obstruction is the only obstruction to Weak Approximation for zero-cycles. We give examples of Kummer surfaces over a number field $F$ where the ramified places of good ordinary reduction contribute nontrivially to the Brauer set for zero-cycles of degree $0$ and we describe cases when an unconditional local-to-global principle can be proved, giving the first unconditional evidence for this conjecture in the case of $K3$ surfaces.","sentences":["Let $X$ be a $K3$ surface over a $p$-adic field $k$ such that for some abelian surface $A$ isogenous to a product of two elliptic curves, there is an isomorphism over the algebraic closure of $k$ between $X$ and the Kummer surface associated to $A$.","Under some assumptions on the reduction types of the elliptic curve factors of $A$, we prove that the Chow group $A_0(X)$ of zero-cycles of degree $0$ on $X$ is the direct sum of a divisible group and a finite group.","This proves a conjecture of Raskind and Spiess and of Colliot-Th\\'{e}l\\`{e}ne and it is the first instance for $K3$ surfaces when this conjecture is proved in full.","This class of $K3$'s includes, among others, the diagonal quartic surfaces.","In the case of good ordinary reduction we describe many cases when the finite summand of $A_0(X)$ can be completely determined.   ","Using these results, we explore a local-to-global conjecture of Colliot-Th\\'{e}lene, Sansuc, Kato and Saito which, roughly speaking, predicts that the Brauer-Manin obstruction is the only obstruction to Weak Approximation for zero-cycles.","We give examples of Kummer surfaces over a number field $F$ where the ramified places of good ordinary reduction contribute nontrivially to the Brauer set for zero-cycles of degree $0$ and we describe cases when an unconditional local-to-global principle can be proved, giving the first unconditional evidence for this conjecture in the case of $K3$ surfaces."],"url":"http://arxiv.org/abs/2402.12588v1","category":"math.AG"}
{"created":"2024-02-19 22:44:23","title":"On the Disentanglement of Tube Inequalities in Concentric Tube Continuum Robots","abstract":"Concentric tube continuum robots utilize nested tubes, which are subject to a set of inequalities. Current approaches to account for inequalities rely on branching methods such as if-else statements. It can introduce discontinuities, may result in a complicated decision tree, has a high wall-clock time, and cannot be vectorized. This affects the behavior and result of downstream methods in control, learning, workspace estimation, and path planning, among others.   In this paper, we investigate a mapping to mitigate branching methods. We derive a lower triangular transformation matrix to disentangle the inequalities and provide proof for the unique existence. It transforms the interdependent inequalities into independent box constraints. Further investigations are made for sampling, control, and workspace estimation. Approaches utilizing the proposed mapping are at least 14 times faster (up to 176 times faster), generate always valid joint configurations, are more interpretable, and are easier to extend.","sentences":["Concentric tube continuum robots utilize nested tubes, which are subject to a set of inequalities.","Current approaches to account for inequalities rely on branching methods such as if-else statements.","It can introduce discontinuities, may result in a complicated decision tree, has a high wall-clock time, and cannot be vectorized.","This affects the behavior and result of downstream methods in control, learning, workspace estimation, and path planning, among others.   ","In this paper, we investigate a mapping to mitigate branching methods.","We derive a lower triangular transformation matrix to disentangle the inequalities and provide proof for the unique existence.","It transforms the interdependent inequalities into independent box constraints.","Further investigations are made for sampling, control, and workspace estimation.","Approaches utilizing the proposed mapping are at least 14 times faster (up to 176 times faster), generate always valid joint configurations, are more interpretable, and are easier to extend."],"url":"http://arxiv.org/abs/2402.12587v1","category":"cs.RO"}
{"created":"2024-02-19 22:35:11","title":"Optimal Moments on Redundancies in Noisy Parallel Computing Setup","abstract":"We consider the problem of job assignment where a master server aims to compute some tasks and is provided a few child servers to compute under a uniform straggling pattern where each server is equally likely to straggle. We distribute tasks to the servers so that the master is able to receive most of the tasks even if a significant number of child servers fail to communicate. We first show that all \\textit{balanced} assignment schemes have the same expectation on the number of distinct tasks received and then study the variance. The variance or the second moment is a useful metric to study as there could be a high \\textit{variation} in the number of distinct tasks received. We show constructions using a generalization of ``Balanced Incomplete Block Design'' [11,40] minimizes the variance, and constructions based on repetition coding schemes attain the largest variance. Both minimum variance and maximum variance attaining designs have their own use cases depending on whether the master aims for a heavy-tailed or light-tailed distribution on the number of distinct jobs. We further show the equivalence between job and server-based assignment schemes when the number of jobs and child servers are equal.","sentences":["We consider the problem of job assignment where a master server aims to compute some tasks and is provided a few child servers to compute under a uniform straggling pattern where each server is equally likely to straggle.","We distribute tasks to the servers so that the master is able to receive most of the tasks even if a significant number of child servers fail to communicate.","We first show that all \\textit{balanced} assignment schemes have the same expectation on the number of distinct tasks received and then study the variance.","The variance or the second moment is a useful metric to study as there could be a high \\textit{variation} in the number of distinct tasks received.","We show constructions using a generalization of ``Balanced Incomplete Block Design''","[11,40] minimizes the variance, and constructions based on repetition coding schemes attain the largest variance.","Both minimum variance and maximum variance attaining designs have their own use cases depending on whether the master aims for a heavy-tailed or light-tailed distribution on the number of distinct jobs.","We further show the equivalence between job and server-based assignment schemes when the number of jobs and child servers are equal."],"url":"http://arxiv.org/abs/2402.12584v1","category":"cs.DC"}
{"created":"2024-02-19 22:06:09","title":"Proximal Byzantine Consensus","abstract":"Distributed control systems require high reliability and availability guarantees despite often being deployed at the edge of network infrastructure. Edge computing resources are less secure and less reliable than centralized resources in data centers. Replication and consensus protocols improve robustness to network faults and crashed or corrupted nodes, but these volatile environments can cause non-faulty nodes to temporarily diverge, increasing the time needed for replicas to converge on a consensus value, and give Byzantine attackers too much influence over the convergence process.   This paper proposes proximal Byzantine consensus, a new approximate consensus protocol where clients use statistical models of streaming computations to decide a consensus value. In addition, it provides an interval around the decision value and the probability that the true (non-faulty, noise-free) value falls within this interval. Proximal consensus (PC) tolerates unreliable network conditions, Byzantine behavior, and other sources of noise that cause honest replica states to diverge. We evaluate our approach for scalar values, and compare PC simulations against a vector consensus (VC) protocol simulation. Our simulations demonstrate that consensus values selected by PC have lower error and are more robust against Byzantine attacks. We formally characterize the security guarantees against Byzantine attacks and demonstrate attacker influence is bound with high probability. Additionally, an informal complexity analysis suggests PC scales better to higher dimensions than convex hull-based protocols such as VC.","sentences":["Distributed control systems require high reliability and availability guarantees despite often being deployed at the edge of network infrastructure.","Edge computing resources are less secure and less reliable than centralized resources in data centers.","Replication and consensus protocols improve robustness to network faults and crashed or corrupted nodes, but these volatile environments can cause non-faulty nodes to temporarily diverge, increasing the time needed for replicas to converge on a consensus value, and give Byzantine attackers too much influence over the convergence process.   ","This paper proposes proximal Byzantine consensus, a new approximate consensus protocol where clients use statistical models of streaming computations to decide a consensus value.","In addition, it provides an interval around the decision value and the probability that the true (non-faulty, noise-free) value falls within this interval.","Proximal consensus (PC) tolerates unreliable network conditions, Byzantine behavior, and other sources of noise that cause honest replica states to diverge.","We evaluate our approach for scalar values, and compare PC simulations against a vector consensus (VC) protocol simulation.","Our simulations demonstrate that consensus values selected by PC have lower error and are more robust against Byzantine attacks.","We formally characterize the security guarantees against Byzantine attacks and demonstrate attacker influence is bound with high probability.","Additionally, an informal complexity analysis suggests PC scales better to higher dimensions than convex hull-based protocols such as VC."],"url":"http://arxiv.org/abs/2402.12577v1","category":"cs.DC"}
{"created":"2024-02-19 21:55:41","title":"Cross-Market Mergers with Common Customers: When (and Why) Do They Increase Negotiated Prices?","abstract":"I examine the implications of cross-market mergers of suppliers to intermediaries that bundle products for consumers. These mergers are controversial. Some argue that suppliers' products will be substitutes for intermediaries, despite not being substitutes for consumers. Others contend that because bundling makes products complements for consumers, products must be complements for intermediaries. I contribute to this debate by showing that two products can be complements for consumers but substitutes for intermediaries when the products serve a similar role in attracting consumers to purchase the bundle. This result leads to new recommendations and helps explain why cross-market hospital mergers raise prices.","sentences":["I examine the implications of cross-market mergers of suppliers to intermediaries that bundle products for consumers.","These mergers are controversial.","Some argue that suppliers' products will be substitutes for intermediaries, despite not being substitutes for consumers.","Others contend that because bundling makes products complements for consumers, products must be complements for intermediaries.","I contribute to this debate by showing that two products can be complements for consumers but substitutes for intermediaries when the products serve a similar role in attracting consumers to purchase the bundle.","This result leads to new recommendations and helps explain why cross-market hospital mergers raise prices."],"url":"http://arxiv.org/abs/2402.12575v1","category":"econ.GN"}
{"created":"2024-02-19 21:51:01","title":"Choi-Defined Resource Theories","abstract":"The resource theories of separable entanglement, non-positive partial transpose entanglement, magic, and imaginarity share an interesting property: An operation is free if and only if its renormalized Choi matrix is a free state. In this letter, we refer to resource theories exhibiting this property as Choi-defined resource theories. We demonstrate how and under what conditions one can construct a Choi-defined resource theory, and we prove that when such a construction is possible, the free operations are all and only the completely resource non-generating operations.","sentences":["The resource theories of separable entanglement, non-positive partial transpose entanglement, magic, and imaginarity share an interesting property: An operation is free if and only if its renormalized Choi matrix is a free state.","In this letter, we refer to resource theories exhibiting this property as Choi-defined resource theories.","We demonstrate how and under what conditions one can construct a Choi-defined resource theory, and we prove that when such a construction is possible, the free operations are all and only the completely resource non-generating operations."],"url":"http://arxiv.org/abs/2402.12569v1","category":"quant-ph"}
{"created":"2024-02-19 21:36:54","title":"Dynamic Pricing and Learning with Long-term Reference Effects","abstract":"We consider a dynamic pricing problem where customer response to the current price is impacted by the customer price expectation, aka reference price. We study a simple and novel reference price mechanism where reference price is the average of the past prices offered by the seller. As opposed to the more commonly studied exponential smoothing mechanism, in our reference price mechanism the prices offered by seller have a longer term effect on the future customer expectations.   We show that under this mechanism, a markdown policy is near-optimal irrespective of the parameters of the model. This matches the common intuition that a seller may be better off by starting with a higher price and then decreasing it, as the customers feel like they are getting bargains on items that are ordinarily more expensive. For linear demand models, we also provide a detailed characterization of the near-optimal markdown policy along with an efficient way of computing it.   We then consider a more challenging dynamic pricing and learning problem, where the demand model parameters are apriori unknown, and the seller needs to learn them online from the customers' responses to the offered prices while simultaneously optimizing revenue. The objective is to minimize regret, i.e., the $T$-round revenue loss compared to a clairvoyant optimal policy. This task essentially amounts to learning a non-stationary optimal policy in a time-variant Markov Decision Process (MDP). For linear demand models, we provide an efficient learning algorithm with an optimal $\\tilde{O}(\\sqrt{T})$ regret upper bound.","sentences":["We consider a dynamic pricing problem where customer response to the current price is impacted by the customer price expectation, aka reference price.","We study a simple and novel reference price mechanism where reference price is the average of the past prices offered by the seller.","As opposed to the more commonly studied exponential smoothing mechanism, in our reference price mechanism the prices offered by seller have a longer term effect on the future customer expectations.   ","We show that under this mechanism, a markdown policy is near-optimal irrespective of the parameters of the model.","This matches the common intuition that a seller may be better off by starting with a higher price and then decreasing it, as the customers feel like they are getting bargains on items that are ordinarily more expensive.","For linear demand models, we also provide a detailed characterization of the near-optimal markdown policy along with an efficient way of computing it.   ","We then consider a more challenging dynamic pricing and learning problem, where the demand model parameters are apriori unknown, and the seller needs to learn them online from the customers' responses to the offered prices while simultaneously optimizing revenue.","The objective is to minimize regret, i.e., the $T$-round revenue loss compared to a clairvoyant optimal policy.","This task essentially amounts to learning a non-stationary optimal policy in a time-variant Markov Decision Process (MDP).","For linear demand models, we provide an efficient learning algorithm with an optimal $\\tilde{O}(\\sqrt{T})$ regret upper bound."],"url":"http://arxiv.org/abs/2402.12562v1","category":"cs.LG"}
{"created":"2024-02-19 21:36:20","title":"Robust Appointment Scheduling with Waiting Time Guarantees","abstract":"Appointment scheduling problems under uncertainty encounter a fundamental trade-off between cost minimization and customer waiting times. Most existing studies address this trade-off using a weighted sum approach, which puts little emphasis on individual waiting times and, thus, customer satisfaction. In contrast, we study how to minimize total cost while providing waiting time guarantees to all customers. Given box uncertainty sets for service times and no-shows, we introduce the Robust Appointment Scheduling Problem with Waiting Time Guarantees. We show that the problem is NP-hard in general and introduce a mixed-integer linear program that can be solved in reasonable computation time. For special cases, we prove that polynomial-time variants of the well-known Smallest-Variance-First sequencing rule and the Bailey-Welch scheduling rule are optimal. Furthermore, a case study with data from the radiology department of a large university hospital demonstrates that the approach not only guarantees acceptable waiting times but, compared to existing robust approaches, may simultaneously reduce costs incurred by idle time and overtime. This work suggests that limiting instead of minimizing customer waiting times is a win-win solution in the trade-off between customer satisfaction and cost minimization. Additionally, it provides an easy-to-implement and customizable appointment scheduling framework with waiting time guarantees.","sentences":["Appointment scheduling problems under uncertainty encounter a fundamental trade-off between cost minimization and customer waiting times.","Most existing studies address this trade-off using a weighted sum approach, which puts little emphasis on individual waiting times and, thus, customer satisfaction.","In contrast, we study how to minimize total cost while providing waiting time guarantees to all customers.","Given box uncertainty sets for service times and no-shows, we introduce the Robust Appointment Scheduling Problem with Waiting Time Guarantees.","We show that the problem is NP-hard in general and introduce a mixed-integer linear program that can be solved in reasonable computation time.","For special cases, we prove that polynomial-time variants of the well-known Smallest-Variance-First sequencing rule and the Bailey-Welch scheduling rule are optimal.","Furthermore, a case study with data from the radiology department of a large university hospital demonstrates that the approach not only guarantees acceptable waiting times but, compared to existing robust approaches, may simultaneously reduce costs incurred by idle time and overtime.","This work suggests that limiting instead of minimizing customer waiting times is a win-win solution in the trade-off between customer satisfaction and cost minimization.","Additionally, it provides an easy-to-implement and customizable appointment scheduling framework with waiting time guarantees."],"url":"http://arxiv.org/abs/2402.12561v1","category":"econ.GN"}
{"created":"2024-02-19 21:32:56","title":"Evaluation of Country Dietary Habits Using Machine Learning Techniques in Relation to Deaths from COVID-19","abstract":"COVID-19 disease has affected almost every country in the world. The large number of infected people and the different mortality rates between countries has given rise to many hypotheses about the key points that make the virus so lethal in some places. In this study, the eating habits of 170 countries were evaluated in order to find correlations between these habits and mortality rates caused by COVID-19 using machine learning techniques that group the countries together according to the different distribution of fat, energy, and protein across 23 different types of food, as well as the amount ingested in kilograms. Results shown how obesity and the high consumption of fats appear in countries with the highest death rates, whereas countries with a lower rate have a higher level of cereal consumption accompanied by a lower total average intake of kilocalories.","sentences":["COVID-19 disease has affected almost every country in the world.","The large number of infected people and the different mortality rates between countries has given rise to many hypotheses about the key points that make the virus so lethal in some places.","In this study, the eating habits of 170 countries were evaluated in order to find correlations between these habits and mortality rates caused by COVID-19 using machine learning techniques that group the countries together according to the different distribution of fat, energy, and protein across 23 different types of food, as well as the amount ingested in kilograms.","Results shown how obesity and the high consumption of fats appear in countries with the highest death rates, whereas countries with a lower rate have a higher level of cereal consumption accompanied by a lower total average intake of kilocalories."],"url":"http://arxiv.org/abs/2402.12558v1","category":"cs.LG"}
{"created":"2024-02-19 21:24:51","title":"Optimal Dynamic Treatment Regime Estimation in the Presence of Nonadherence","abstract":"Dynamic treatment regimes (DTRs) are sequences of functions that formalize the process of precision medicine. DTRs take as input patient information and output treatment recommendations. A major focus of the DTR literature has been on the estimation of optimal DTRs, the sequences of decision rules that result in the best outcome in expectation, across the complete population were they to be applied. While there is a rich literature on optimal DTR estimation, to date there has been minimal consideration of the impacts of nonadherence on these estimation procedures. Nonadherence refers to any process through that an individual's prescribed treatment does not match their true treatment. We explore the impacts of nonadherence and demonstrate that generally, when nonadherence is ignored, suboptimal regimes will be estimated. In light of these findings we propose a method for estimating optimal DTRs in the presence of nonadherence. The resulting estimators are consistent and asymptotically normal, with a double robustness property. Using simulations we demonstrate the reliability of these results, and illustrate comparable performance between the proposed estimation procedure adjusting for the impacts of nonadherence and estimators that are computed on data without nonadherence.","sentences":["Dynamic treatment regimes (DTRs) are sequences of functions that formalize the process of precision medicine.","DTRs take as input patient information and output treatment recommendations.","A major focus of the DTR literature has been on the estimation of optimal DTRs, the sequences of decision rules that result in the best outcome in expectation, across the complete population were they to be applied.","While there is a rich literature on optimal DTR estimation, to date there has been minimal consideration of the impacts of nonadherence on these estimation procedures.","Nonadherence refers to any process through that an individual's prescribed treatment does not match their true treatment.","We explore the impacts of nonadherence and demonstrate that generally, when nonadherence is ignored, suboptimal regimes will be estimated.","In light of these findings we propose a method for estimating optimal DTRs in the presence of nonadherence.","The resulting estimators are consistent and asymptotically normal, with a double robustness property.","Using simulations we demonstrate the reliability of these results, and illustrate comparable performance between the proposed estimation procedure adjusting for the impacts of nonadherence and estimators that are computed on data without nonadherence."],"url":"http://arxiv.org/abs/2402.12555v1","category":"stat.ME"}
{"created":"2024-02-19 21:19:46","title":"Composite likelihood inference for space-time point processes","abstract":"The dynamics of a rain forest is extremely complex involving births, deaths and growth of trees with complex interactions between trees, animals, climate, and environment. We consider the patterns of recruits (new trees) and dead trees between rain forest censuses. For a current census we specify regression models for the conditional intensity of recruits and the conditional probabilities of death given the current trees and spatial covariates. We estimate regression parameters using conditional composite likelihood functions that only involve the conditional first order properties of the data. When constructing assumption lean estimators of covariance matrices of parameter estimates we only need mild assumptions of decaying conditional correlations in space while assumptions regarding correlations over time are avoided by exploiting conditional centering of composite likelihood score functions. Time series of point patterns from rain forest censuses are quite short while each point pattern covers a fairly big spatial region. To obtain asymptotic results we therefore use a central limit theorem for the fixed timespan - increasing spatial domain asymptotic setting. This also allows us to handle the challenge of using stochastic covariates constructed from past point patterns. Conveniently, it suffices to impose weak dependence assumptions on the innovations of the space-time process. We investigate the proposed methodology by simulation studies and applications to rain forest data.","sentences":["The dynamics of a rain forest is extremely complex involving births, deaths and growth of trees with complex interactions between trees, animals, climate, and environment.","We consider the patterns of recruits (new trees) and dead trees between rain forest censuses.","For a current census we specify regression models for the conditional intensity of recruits and the conditional probabilities of death given the current trees and spatial covariates.","We estimate regression parameters using conditional composite likelihood functions that only involve the conditional first order properties of the data.","When constructing assumption lean estimators of covariance matrices of parameter estimates we only need mild assumptions of decaying conditional correlations in space while assumptions regarding correlations over time are avoided by exploiting conditional centering of composite likelihood score functions.","Time series of point patterns from rain forest censuses are quite short while each point pattern covers a fairly big spatial region.","To obtain asymptotic results we therefore use a central limit theorem for the fixed timespan - increasing spatial domain asymptotic setting.","This also allows us to handle the challenge of using stochastic covariates constructed from past point patterns.","Conveniently, it suffices to impose weak dependence assumptions on the innovations of the space-time process.","We investigate the proposed methodology by simulation studies and applications to rain forest data."],"url":"http://arxiv.org/abs/2402.12548v1","category":"stat.ME"}
{"created":"2024-02-19 21:06:55","title":"Tensor Decompositions with applications to LU and SLOCC equivalence of multipartite pure states","abstract":"We introduce a general lemma, of which one consequence is the higher order singular value decomposition (HOSVD) of tensors defined by DeLathauwer, DeMoor and Vandewalle (2000). By an analogous application of the lemma, we find a complex orthogonal version of the HOSVD. Kraus' (2010) algorithm used the HOSVD to compute normal forms of generic $n$-qubit pure states under the action of the local unitary group $\\operatorname{U}_2^{\\times n}$. Taking advantage of the double cover $\\operatorname{SL}_2(\\mathbb{C}) \\times \\operatorname{SL}_2(\\mathbb{C}) \\to \\operatorname{SO}_4({\\mathbb{C}})$, we produce similar algorithms (distinguished by the parity of $n$) that compute normal forms for tensors in $(\\mathbb{C}^2)^{\\otimes n}$ for the action of the SLOCC group $\\operatorname{SL}_2(\\mathbb{C})^{\\times n}$.","sentences":["We introduce a general lemma, of which one consequence is the higher order singular value decomposition (HOSVD) of tensors defined by DeLathauwer, DeMoor and Vandewalle (2000).","By an analogous application of the lemma, we find a complex orthogonal version of the HOSVD.","Kraus' (2010) algorithm used the HOSVD to compute normal forms of generic $n$-qubit pure states under the action of the local unitary group $\\operatorname{U}_2^{\\times n}$. Taking advantage of the double cover $\\operatorname{SL}_2(\\mathbb{C})","\\times \\operatorname{SL}_2(\\mathbb{C})","\\to \\operatorname{SO}_4({\\mathbb{C}})$, we produce similar algorithms (distinguished by the parity of $n$) that compute normal forms for tensors in $(\\mathbb{C}^2)^{\\otimes n}$ for the action of the SLOCC group $\\operatorname{SL}_2(\\mathbb{C})^{\\times n}$."],"url":"http://arxiv.org/abs/2402.12542v1","category":"quant-ph"}
{"created":"2024-02-19 21:05:55","title":"Leveraging Opposite Gender Interaction Ratio as a Path towards Fairness in Online Dating Recommendations Based on User Sexual Orientation","abstract":"Online dating platforms have gained widespread popularity as a means for individuals to seek potential romantic relationships. While recommender systems have been designed to improve the user experience in dating platforms by providing personalized recommendations, increasing concerns about fairness have encouraged the development of fairness-aware recommender systems from various perspectives (e.g., gender and race). However, sexual orientation, which plays a significant role in finding a satisfying relationship, is under-investigated. To fill this crucial gap, we propose a novel metric, Opposite Gender Interaction Ratio (OGIR), as a way to investigate potential unfairness for users with varying preferences towards the opposite gender. We empirically analyze a real online dating dataset and observe existing recommender algorithms could suffer from group unfairness according to OGIR. We further investigate the potential causes for such gaps in recommendation quality, which lead to the challenges of group quantity imbalance and group calibration imbalance. Ultimately, we propose a fair recommender system based on re-weighting and re-ranking strategies to respectively mitigate these associated imbalance challenges. Experimental results demonstrate both strategies improve fairness while their combination achieves the best performance towards maintaining model utility while improving fairness.","sentences":["Online dating platforms have gained widespread popularity as a means for individuals to seek potential romantic relationships.","While recommender systems have been designed to improve the user experience in dating platforms by providing personalized recommendations, increasing concerns about fairness have encouraged the development of fairness-aware recommender systems from various perspectives (e.g., gender and race).","However, sexual orientation, which plays a significant role in finding a satisfying relationship, is under-investigated.","To fill this crucial gap, we propose a novel metric, Opposite Gender Interaction Ratio (OGIR), as a way to investigate potential unfairness for users with varying preferences towards the opposite gender.","We empirically analyze a real online dating dataset and observe existing recommender algorithms could suffer from group unfairness according to OGIR.","We further investigate the potential causes for such gaps in recommendation quality, which lead to the challenges of group quantity imbalance and group calibration imbalance.","Ultimately, we propose a fair recommender system based on re-weighting and re-ranking strategies to respectively mitigate these associated imbalance challenges.","Experimental results demonstrate both strategies improve fairness while their combination achieves the best performance towards maintaining model utility while improving fairness."],"url":"http://arxiv.org/abs/2402.12541v1","category":"cs.IR"}
{"created":"2024-02-19 20:53:27","title":"Hierarchical Bayes Approach to Personalized Federated Unsupervised Learning","abstract":"Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics. Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored. We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework. We develop adaptive algorithms that discover the balance between using limited local data and collaborative information. We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models. We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity, local sample size). We also develop a theoretical framework for personalized diffusion models, which shows the benefits of collaboration even under heterogeneity. We finally evaluate our proposed algorithms using synthetic and real data, demonstrating the effective sample amplification for personalized tasks, induced through collaboration, despite data heterogeneity.","sentences":["Statistical heterogeneity of clients' local data is an important characteristic in federated learning, motivating personalized algorithms tailored to the local data statistics.","Though there has been a plethora of algorithms proposed for personalized supervised learning, discovering the structure of local data through personalized unsupervised learning is less explored.","We initiate a systematic study of such personalized unsupervised learning by developing algorithms based on optimization criteria inspired by a hierarchical Bayesian statistical framework.","We develop adaptive algorithms that discover the balance between using limited local data and collaborative information.","We do this in the context of two unsupervised learning tasks: personalized dimensionality reduction and personalized diffusion models.","We develop convergence analyses for our adaptive algorithms which illustrate the dependence on problem parameters (e.g., heterogeneity, local sample size).","We also develop a theoretical framework for personalized diffusion models, which shows the benefits of collaboration even under heterogeneity.","We finally evaluate our proposed algorithms using synthetic and real data, demonstrating the effective sample amplification for personalized tasks, induced through collaboration, despite data heterogeneity."],"url":"http://arxiv.org/abs/2402.12537v1","category":"cs.LG"}
{"created":"2024-02-19 20:50:55","title":"Designing High-Performing Networks for Multi-Scale Computer Vision","abstract":"Since the emergence of deep learning, the computer vision field has flourished with models improving at a rapid pace on more and more complex tasks. We distinguish three main ways to improve a computer vision model: (1) improving the data aspect by for example training on a large, more diverse dataset, (2) improving the training aspect by for example designing a better optimizer, and (3) improving the network architecture (or network for short). In this thesis, we chose to improve the latter, i.e. improving the network designs of computer vision models. More specifically, we investigate new network designs for multi-scale computer vision tasks, which are tasks requiring to make predictions about concepts at different scales. The goal of these new network designs is to outperform existing baseline designs from the literature. Specific care is taken to make sure the comparisons are fair, by guaranteeing that the different network designs were trained and evaluated with the same settings. Code is publicly available at https://github.com/CedricPicron/DetSeg.","sentences":["Since the emergence of deep learning, the computer vision field has flourished with models improving at a rapid pace on more and more complex tasks.","We distinguish three main ways to improve a computer vision model: (1) improving the data aspect by for example training on a large, more diverse dataset, (2) improving the training aspect by for example designing a better optimizer, and (3) improving the network architecture (or network for short).","In this thesis, we chose to improve the latter, i.e. improving the network designs of computer vision models.","More specifically, we investigate new network designs for multi-scale computer vision tasks, which are tasks requiring to make predictions about concepts at different scales.","The goal of these new network designs is to outperform existing baseline designs from the literature.","Specific care is taken to make sure the comparisons are fair, by guaranteeing that the different network designs were trained and evaluated with the same settings.","Code is publicly available at https://github.com/CedricPicron/DetSeg."],"url":"http://arxiv.org/abs/2402.12536v1","category":"cs.CV"}
{"created":"2024-02-19 20:33:01","title":"A VLBA-uGMRT search for candidate binary black holes: Study of six X-shaped radio galaxies with double-peaked emission lines","abstract":"Identifying methods to discover dual AGN has proven to be challenging. Several indirect tracers have been explored in the literature, including X/S-shaped radio morphologies and double-peaked (DP) emission lines in the optical spectra. However, the detection rates of confirmed dual AGN candidates from the individual methods remain extremely small. We search for binary black holes in a sample of six sources that exhibit both X-shaped radio morphology and DP emission lines using the VLBA. Three out of the six sources show dual VLBA compact components, making them strong candidates for binary black hole sources. In addition, we present deep uGMRT images revealing the exquisite details of the X-shaped wings in three sources. We present a detailed precession modeling analysis of these sources. The BH separations estimated from the simplistic geodetic precession model are incompatible with those estimated from emission line offsets and the VLBA separations. However, precession induced by a noncoplanar secondary black hole is a feasible mechanism for explaining the observed X-shaped radio morphologies and the black hole separations estimated from other methods. The black hole separations estimated from the double-peaked emission lines agree well with the VLBA compact component separations. Future multi-frequency VLBA observations will be critical in ruling out or confirming the binary black hole scenario in the three galaxies with dual component detections.","sentences":["Identifying methods to discover dual AGN has proven to be challenging.","Several indirect tracers have been explored in the literature, including X/S-shaped radio morphologies and double-peaked (DP) emission lines in the optical spectra.","However, the detection rates of confirmed dual AGN candidates from the individual methods remain extremely small.","We search for binary black holes in a sample of six sources that exhibit both X-shaped radio morphology and DP emission lines using the VLBA.","Three out of the six sources show dual VLBA compact components, making them strong candidates for binary black hole sources.","In addition, we present deep uGMRT images revealing the exquisite details of the X-shaped wings in three sources.","We present a detailed precession modeling analysis of these sources.","The BH separations estimated from the simplistic geodetic precession model are incompatible with those estimated from emission line offsets and the VLBA separations.","However, precession induced by a noncoplanar secondary black hole is a feasible mechanism for explaining the observed X-shaped radio morphologies and the black hole separations estimated from other methods.","The black hole separations estimated from the double-peaked emission lines agree well with the VLBA compact component separations.","Future multi-frequency VLBA observations will be critical in ruling out or confirming the binary black hole scenario in the three galaxies with dual component detections."],"url":"http://arxiv.org/abs/2402.12521v1","category":"astro-ph.GA"}
{"created":"2024-02-19 20:13:42","title":"Situating Data Sets: Making Public Data Actionable for Housing Justice","abstract":"Activists, governmentsm and academics regularly advocate for more open data. But how is data made open, and for whom is it made useful and usable? In this paper, we investigate and describe the work of making eviction data open to tenant organizers. We do this through an ethnographic description of ongoing work with a local housing activist organization. This work combines observation, direct participation in data work, and creating media artifacts, specifically digital maps. Our interpretation is grounded in D'Ignazio and Klein's Data Feminism, emphasizing standpoint theory. Through our analysis and discussion, we highlight how shifting positionalities from data intermediaries to data accomplices affects the design of data sets and maps. We provide HCI scholars with three design implications when situating data for grassroots organizers: becoming a domain beginner, striving for data actionability, and evaluating our design artifacts by the social relations they sustain rather than just their technical efficacy.","sentences":["Activists, governmentsm and academics regularly advocate for more open data.","But how is data made open, and for whom is it made useful and usable?","In this paper, we investigate and describe the work of making eviction data open to tenant organizers.","We do this through an ethnographic description of ongoing work with a local housing activist organization.","This work combines observation, direct participation in data work, and creating media artifacts, specifically digital maps.","Our interpretation is grounded in D'Ignazio and Klein's Data Feminism, emphasizing standpoint theory.","Through our analysis and discussion, we highlight how shifting positionalities from data intermediaries to data accomplices affects the design of data sets and maps.","We provide HCI scholars with three design implications when situating data for grassroots organizers: becoming a domain beginner, striving for data actionability, and evaluating our design artifacts by the social relations they sustain rather than just their technical efficacy."],"url":"http://arxiv.org/abs/2402.12505v1","category":"cs.HC"}
{"created":"2024-02-19 20:05:41","title":"Feudal Networks for Visual Navigation","abstract":"Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high-level manager, we learn a memory proxy map in a self supervised manner to record prior observations in a learned latent space and avoid the use of graphs and odometry. For the mid-level manager, we develop a waypoint network that outputs intermediate subgoals imitating human waypoint selection during local navigation. This waypoint network is pre-trained using a new, small set of teleoperation videos that we make publicly available, with training environments different from testing environments. The resulting feudal navigation network achieves near SOTA performance, while providing a novel no-RL, no-graph, no-odometry, no-metric map approach to the image goal navigation task.","sentences":["Visual navigation follows the intuition that humans can navigate without detailed maps.","A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning.","Recent variations learn from passive videos and can navigate using complex social and semantic cues.","However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized.","We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager.","Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales.","Two unique modules are developed in this framework.","For the high-level manager, we learn a memory proxy map in a self supervised manner to record prior observations in a learned latent space and avoid the use of graphs and odometry.","For the mid-level manager, we develop a waypoint network that outputs intermediate subgoals imitating human waypoint selection during local navigation.","This waypoint network is pre-trained using a new, small set of teleoperation videos that we make publicly available, with training environments different from testing environments.","The resulting feudal navigation network achieves near SOTA performance, while providing a novel no-RL, no-graph, no-odometry, no-metric map approach to the image goal navigation task."],"url":"http://arxiv.org/abs/2402.12498v1","category":"cs.CV"}
{"created":"2024-02-19 19:54:38","title":"Analytic electrical charged black holes in $F(R)$-ModMax theory","abstract":"Motivated by a new model of nonlinear electrodynamics known as Modified Maxwell (ModMax) theory, an exact analytical solution for black holes is obtained by coupling ModMax nonlinear electrodynamics and $F(R)$ gravity. Then, the effects of the system's parameters ($F(R)$-ModMax gravity parameters) on the event horizons are analyzed. The obtained black holes thermodynamic properties in the $F(R)$-ModMax theory are investigated by extracting their thermodynamic quantities such as Hawking temperature, electric charge, electric potential, entropy, and also total mass. The first law of thermodynamics for the system under study is evaluated. Next, by considering these black holes, the impact of various parameters on both the local stability and global stability are investigated by examining the heat capacity and the Helmholtz free energy, respectively. Finally, the thermodynamic geometry of the black hole in $F(R)$-ModMax gravity is investigated by applying the thermodynamic metric (the HPEM metric).","sentences":["Motivated by a new model of nonlinear electrodynamics known as Modified Maxwell (ModMax) theory, an exact analytical solution for black holes is obtained by coupling ModMax nonlinear electrodynamics and $F(R)$ gravity.","Then, the effects of the system's parameters ($F(R)$-ModMax gravity parameters) on the event horizons are analyzed.","The obtained black holes thermodynamic properties in the $F(R)$-ModMax theory are investigated by extracting their thermodynamic quantities such as Hawking temperature, electric charge, electric potential, entropy, and also total mass.","The first law of thermodynamics for the system under study is evaluated.","Next, by considering these black holes, the impact of various parameters on both the local stability and global stability are investigated by examining the heat capacity and the Helmholtz free energy, respectively.","Finally, the thermodynamic geometry of the black hole in $F(R)$-ModMax gravity is investigated by applying the thermodynamic metric (the HPEM metric)."],"url":"http://arxiv.org/abs/2402.12492v1","category":"gr-qc"}
{"created":"2024-02-19 19:49:29","title":"Do Pre-Trained Language Models Detect and Understand Semantic Underspecification? Ask the DUST!","abstract":"In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally. For example, to interpret the underspecified sentence \"Don't spend too much\", which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed. In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences. We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted. However, interpreting them correctly is much harder for any LMs. Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecification would predict. Overall, our study reveals limitations in current models' processing of sentence semantics and highlights the importance of using naturalistic data and communicative scenarios when evaluating LMs' language capabilities.","sentences":["In everyday language use, speakers frequently utter and interpret sentences that are semantically underspecified, namely, whose content is insufficient to fully convey their message or interpret them univocally.","For example, to interpret the underspecified sentence \"Don't spend too much\", which leaves implicit what (not) to spend, additional linguistic context or outside knowledge is needed.","In this work, we propose a novel Dataset of semantically Underspecified Sentences grouped by Type (DUST) and use it to study whether pre-trained language models (LMs) correctly identify and interpret underspecified sentences.","We find that newer LMs are reasonably able to identify underspecified sentences when explicitly prompted.","However, interpreting them correctly is much harder for any LMs.","Our experiments show that when interpreting underspecified sentences, LMs exhibit little uncertainty, contrary to what theoretical accounts of underspecification would predict.","Overall, our study reveals limitations in current models' processing of sentence semantics and highlights the importance of using naturalistic data and communicative scenarios when evaluating LMs' language capabilities."],"url":"http://arxiv.org/abs/2402.12486v1","category":"cs.CL"}
{"created":"2024-02-19 19:38:58","title":"Artifacts or Abduction: How Do LLMs Answer Multiple-Choice Questions Without the Question?","abstract":"Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs). To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices. In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain. To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference. Our key findings are threefold. First, we find no evidence that the choices-only accuracy stems from memorization alone. Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices. Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question. We hope to motivate the use of stronger baselines in MCQA benchmarks, the design of robust MCQA datasets, and further efforts to explain LLM decision-making.","sentences":["Multiple-choice question answering (MCQA) is often used to evaluate large language models (LLMs).","To see if MCQA assesses LLMs as intended, we probe if LLMs can perform MCQA with choices-only prompts, where models must select the correct answer only from the choices.","In three MCQA datasets and four LLMs, this prompt bests a majority baseline in 11/12 cases, with up to 0.33 accuracy gain.","To help explain this behavior, we conduct an in-depth, black-box analysis on memorization, choice dynamics, and question inference.","Our key findings are threefold.","First, we find no evidence that the choices-only accuracy stems from memorization alone.","Second, priors over individual choices do not fully explain choices-only accuracy, hinting that LLMs use the group dynamics of choices.","Third, LLMs have some ability to infer a relevant question from choices, and surprisingly can sometimes even match the original question.","We hope to motivate the use of stronger baselines in MCQA benchmarks, the design of robust MCQA datasets, and further efforts to explain LLM decision-making."],"url":"http://arxiv.org/abs/2402.12483v1","category":"cs.CL"}
{"created":"2024-02-19 19:21:45","title":"Diffeomorphism Neural Operator for various domains and parameters of partial differential equations","abstract":"Many science and engineering applications demand partial differential equations (PDE) evaluations that are traditionally computed with resource-intensive numerical solvers. Neural operator models provide an efficient alternative by learning the governing physical laws directly from data in a class of PDEs with different parameters, but constrained in a fixed boundary (domain). Many applications, such as design and manufacturing, would benefit from neural operators with flexible domains when studied at scale. Here we present a diffeomorphism neural operator learning framework towards developing domain-flexible models for physical systems with various and complex domains. Specifically, a neural operator trained in a shared domain mapped from various domains of fields by diffeomorphism is proposed, which transformed the problem of learning function mappings in varying domains (spaces) into the problem of learning operators on a shared diffeomorphic domain. Meanwhile, an index is provided to evaluate the generalization of diffeomorphism neural operators in different domains by the domain diffeomorphism similarity. Experiments on statics scenarios (Darcy flow, mechanics) and dynamic scenarios (pipe flow, airfoil flow) demonstrate the advantages of our approach for neural operator learning under various domains, where harmonic and volume parameterization are used as the diffeomorphism for 2D and 3D domains. Our diffeomorphism neural operator approach enables strong learning capability and robust generalization across varying domains and parameters.","sentences":["Many science and engineering applications demand partial differential equations (PDE) evaluations that are traditionally computed with resource-intensive numerical solvers.","Neural operator models provide an efficient alternative by learning the governing physical laws directly from data in a class of PDEs with different parameters, but constrained in a fixed boundary (domain).","Many applications, such as design and manufacturing, would benefit from neural operators with flexible domains when studied at scale.","Here we present a diffeomorphism neural operator learning framework towards developing domain-flexible models for physical systems with various and complex domains.","Specifically, a neural operator trained in a shared domain mapped from various domains of fields by diffeomorphism is proposed, which transformed the problem of learning function mappings in varying domains (spaces) into the problem of learning operators on a shared diffeomorphic domain.","Meanwhile, an index is provided to evaluate the generalization of diffeomorphism neural operators in different domains by the domain diffeomorphism similarity.","Experiments on statics scenarios (Darcy flow, mechanics) and dynamic scenarios (pipe flow, airfoil flow) demonstrate the advantages of our approach for neural operator learning under various domains, where harmonic and volume parameterization are used as the diffeomorphism for 2D and 3D domains.","Our diffeomorphism neural operator approach enables strong learning capability and robust generalization across varying domains and parameters."],"url":"http://arxiv.org/abs/2402.12475v1","category":"math.NA"}
{"created":"2024-02-19 19:18:06","title":"The dynamical evolution of star-forming regions measured with INDICATE","abstract":"Observations of star-forming regions provide snapshots in time of the star formation process, and can be compared with simulation data to constrain the initial conditions of star formation. In order to make robust inferences, different metrics must be used to quantify the spatial and kinematic distributions of stars. In this paper, we assess the suitability of the INDICATE (INdex to Define Inherent Clustering And TEndencies) method as a diagnostic to infer the initial conditions of star-forming regions that subsequently undergo dynamical evolution. We use INDICATE to measure the degree of clustering in N-body simulations of the evolution of star-forming regions with different initial conditions. We find that the clustering of individual stars, as measured by INDICATE, becomes significantly higher in simulations with higher initial stellar densities, and is higher in subvirial star-forming regions where significant amounts of dynamical mixing has occurred. We then combine INDICATE with other methods that measure the mass segregation, relative stellar surface density ratio and the morphology (Q-parameter) of star-forming regions, and show that the diagnostic capability of INDICATE increases when combined with these other metrics.","sentences":["Observations of star-forming regions provide snapshots in time of the star formation process, and can be compared with simulation data to constrain the initial conditions of star formation.","In order to make robust inferences, different metrics must be used to quantify the spatial and kinematic distributions of stars.","In this paper, we assess the suitability of the INDICATE (INdex to Define Inherent Clustering And TEndencies) method as a diagnostic to infer the initial conditions of star-forming regions that subsequently undergo dynamical evolution.","We use INDICATE to measure the degree of clustering in N-body simulations of the evolution of star-forming regions with different initial conditions.","We find that the clustering of individual stars, as measured by INDICATE, becomes significantly higher in simulations with higher initial stellar densities, and is higher in subvirial star-forming regions where significant amounts of dynamical mixing has occurred.","We then combine INDICATE with other methods that measure the mass segregation, relative stellar surface density ratio and the morphology (Q-parameter) of star-forming regions, and show that the diagnostic capability of INDICATE increases when combined with these other metrics."],"url":"http://arxiv.org/abs/2402.12472v1","category":"astro-ph.GA"}
{"created":"2024-02-19 19:17:30","title":"Field-extension statistics of charged semiflexible polymers stretched with uniform electric fields","abstract":"Single-molecule force-extension experiments have allowed quantitative measurements of the mechanical responses of biomolecules to applied forces explaining their roles in key biological functions. Electrophoretic stretching of charged polymers such as DNA in uniform electric fields is one such example, currently, used for sequencing purposes. Field-extension statistics of charged polymers differ from laser optical tweezer setups due to a non-uniform tension along the backbone of the chain, the effects of which remain poorly understood. In this paper, we modify an existing analytically tractable mean-field (MF) approach to account for the heterogeneity in tension for electric fields. Naively using this model for stretching of charged polymers such as DNA under electric fields results in local overstretching of the chain and gives inaccurate field-extension statistics. We improve this approach and account for the inhomogeneity in the tension by subdividing the chain into smaller segments while imposing the inextensibility of the chain. We find that the subdivided MF model shows better agreement with the simulations for the force-extension plots. We also show that using an isotropic mean-field model overestimates the longitudinal fluctuations both for tension forces as well as for fields. We correct the quantitative predictions for the fluctuations in the mean extension by numerically differentiating the field-extension plots. We also find that the subdivided MF model can accurately predict the statistics of experimentally relevant quantities, such as transverse fluctuations, due to the analytical tractability of the model. These field-extension predictions may be further used to introduce confinement effects in the subdivided MF model and develop a comprehensive understanding of sequencing technologies.","sentences":["Single-molecule force-extension experiments have allowed quantitative measurements of the mechanical responses of biomolecules to applied forces explaining their roles in key biological functions.","Electrophoretic stretching of charged polymers such as DNA in uniform electric fields is one such example, currently, used for sequencing purposes.","Field-extension statistics of charged polymers differ from laser optical tweezer setups due to a non-uniform tension along the backbone of the chain, the effects of which remain poorly understood.","In this paper, we modify an existing analytically tractable mean-field (MF) approach to account for the heterogeneity in tension for electric fields.","Naively using this model for stretching of charged polymers such as DNA under electric fields results in local overstretching of the chain and gives inaccurate field-extension statistics.","We improve this approach and account for the inhomogeneity in the tension by subdividing the chain into smaller segments while imposing the inextensibility of the chain.","We find that the subdivided MF model shows better agreement with the simulations for the force-extension plots.","We also show that using an isotropic mean-field model overestimates the longitudinal fluctuations both for tension forces as well as for fields.","We correct the quantitative predictions for the fluctuations in the mean extension by numerically differentiating the field-extension plots.","We also find that the subdivided MF model can accurately predict the statistics of experimentally relevant quantities, such as transverse fluctuations, due to the analytical tractability of the model.","These field-extension predictions may be further used to introduce confinement effects in the subdivided MF model and develop a comprehensive understanding of sequencing technologies."],"url":"http://arxiv.org/abs/2402.12470v1","category":"cond-mat.soft"}
{"created":"2024-02-19 19:15:13","title":"Optimal Rejection of Bounded Perturbations in Linear Leader-Following Consensus Protocol: Method Invariant Ellipsoid","abstract":"The objective of the invariant ellipsoid method is to minimize the smallest invariant and attractive set of a linear control system operating under the influence of bounded external disturbances. In this paper, this method is extended into the leader-following consensus problem. Initially, a linear control protocol is designed for the Multi-agent System without disturbances. Subsequently, in the presence of bounded disturbances, by employing a similar linear control protocol, a necessary and sufficient condition is introduced to derive the optimal control parameters for the MAS such that the state of followers converge and remain in an minimal invariant ellipsoid around the state of the leader.","sentences":["The objective of the invariant ellipsoid method is to minimize the smallest invariant and attractive set of a linear control system operating under the influence of bounded external disturbances.","In this paper, this method is extended into the leader-following consensus problem.","Initially, a linear control protocol is designed for the Multi-agent System without disturbances.","Subsequently, in the presence of bounded disturbances, by employing a similar linear control protocol, a necessary and sufficient condition is introduced to derive the optimal control parameters for the MAS such that the state of followers converge and remain in an minimal invariant ellipsoid around the state of the leader."],"url":"http://arxiv.org/abs/2402.12468v1","category":"math.OC"}
{"created":"2024-02-19 19:04:40","title":"Testing the double-logarithm asymptotic gluon density in ultraperipheral heavy ion collisions at the Large Hadron Collider","abstract":"In this work we analyze the application of the analytical gluon distribution based on the double asymptotic scaling for the photoproduction of vector mesons in coherent $pp$, $pA$ and $AA$ collisions at the LHC energies using the color dipole formalism. Predictions for the rapidity distribution are presented for $\\rho^0$ and $J/ \\psi$, $\\psi (2S)$ and $\\Upsilon (1S)$ photoproduction. An analysis on the uncertainties associated to different implementations of the dipole-proton amplitude is performed. The vector meson photoproduction accompanied by electromagnetic dissociation is also analyzed.","sentences":["In this work we analyze the application of the analytical gluon distribution based on the double asymptotic scaling for the photoproduction of vector mesons in coherent $pp$, $pA$ and $AA$ collisions at the LHC energies using the color dipole formalism.","Predictions for the rapidity distribution are presented for $\\rho^0$ and $J/ \\psi$, $\\psi (2S)$ and $\\Upsilon (1S)$ photoproduction.","An analysis on the uncertainties associated to different implementations of the dipole-proton amplitude is performed.","The vector meson photoproduction accompanied by electromagnetic dissociation is also analyzed."],"url":"http://arxiv.org/abs/2402.12458v1","category":"hep-ph"}
{"created":"2024-02-19 19:00:09","title":"DBNets: A publicly available deep learning tool to measure the masses of young planets in dusty protoplanetary discs","abstract":"Current methods to characterize embedded planets in protoplanetary disc observations are severely limited either in their ability to fully account for the observed complex physics or in their computational and time costs. To address this shortcoming, we developed DBNets: a deep learning tool, based on convolutional neural networks, that analyses substructures observed in the dust continuum emission of protoplanetary discs to quickly infer the mass of allegedly embedded planets. We focussed on developing a method to reliably quantify not only the planet mass, but also the associated uncertainty introduced by our modelling and adopted techniques. Our tests gave promising results achieving an 87% reduction of the log Mp mean squared error with respect to an analytical formula fitted on the same data (DBNets metrics: lmse 0.016, r2-score 97%). With the goal of providing the final user of DBNets with all the tools needed to interpret their measurements and decide on their significance, we extensively tested our tool on out-of-distribution data. We found that DBNets can identify inputs strongly outside its training scope returning an uncertainty above a specific threshold and we thus provided a rejection criterion that helps determine the significance of the results obtained. Additionally, we outlined some limitations of our tool: it can be reliably applied only on discs observed with inclinations below approximately 60{\\deg}, in the optically thin regime, with a resolution 8 times better than the gap radial location and with a signal-to-noise ratio higher than approximately ten. Finally, we applied DBNets to 33 actual observations of protoplanetary discs measuring the mass of 48 proposed planets and comparing our results with the available literature. We confirmed that most of the observed gaps imply planets in the sub-Jupiter regime. DBNets is publicly available at dbnets.fisica.unimi.it.","sentences":["Current methods to characterize embedded planets in protoplanetary disc observations are severely limited either in their ability to fully account for the observed complex physics or in their computational and time costs.","To address this shortcoming, we developed DBNets: a deep learning tool, based on convolutional neural networks, that analyses substructures observed in the dust continuum emission of protoplanetary discs to quickly infer the mass of allegedly embedded planets.","We focussed on developing a method to reliably quantify not only the planet mass, but also the associated uncertainty introduced by our modelling and adopted techniques.","Our tests gave promising results achieving an 87% reduction of the log Mp mean squared error with respect to an analytical formula fitted on the same data (DBNets metrics: lmse 0.016, r2-score 97%).","With the goal of providing the final user of DBNets with all the tools needed to interpret their measurements and decide on their significance, we extensively tested our tool on out-of-distribution data.","We found that DBNets can identify inputs strongly outside its training scope returning an uncertainty above a specific threshold and we thus provided a rejection criterion that helps determine the significance of the results obtained.","Additionally, we outlined some limitations of our tool: it can be reliably applied only on discs observed with inclinations below approximately 60{\\deg}, in the optically thin regime, with a resolution 8 times better than the gap radial location and with a signal-to-noise ratio higher than approximately ten.","Finally, we applied DBNets to 33 actual observations of protoplanetary discs measuring the mass of 48 proposed planets and comparing our results with the available literature.","We confirmed that most of the observed gaps imply planets in the sub-Jupiter regime.","DBNets is publicly available at dbnets.fisica.unimi.it."],"url":"http://arxiv.org/abs/2402.12448v1","category":"astro-ph.EP"}
{"created":"2024-02-19 19:00:02","title":"Relaxation of first-class constraints and the quantization of gauge theories: from \"matter without matter\" to the reappearance of time in quantum gravity","abstract":"We make a conceptual overview of a particular approach to the initial-value problem in canonical gauge theories. We stress how the first-class phase-space constraints may be relaxed if we interpret them as fixing the values of new degrees of freedom. This idea goes back to Fock and Stueckelberg, leading to restrictions of the gauge symmetry of a theory, and it corresponds, in certain cases, to promoting constants of Nature to physical fields. Recently, different versions of this formulation have gained considerable attention in the literature, with several independent iterations, particularly in classical and quantum descriptions of gravity, cosmology, and electromagnetism. In particular, in the case of canonical quantum gravity, the Fock--Stueckelberg approach is relevant to the so-called problem of time. Our overview recalls the work of Fock and Stueckelberg and its physical interpretation with the aim of conceptually unifying the different iterations of the idea that appear in the literature and of motivating further research.","sentences":["We make a conceptual overview of a particular approach to the initial-value problem in canonical gauge theories.","We stress how the first-class phase-space constraints may be relaxed if we interpret them as fixing the values of new degrees of freedom.","This idea goes back to Fock and Stueckelberg, leading to restrictions of the gauge symmetry of a theory, and it corresponds, in certain cases, to promoting constants of Nature to physical fields.","Recently, different versions of this formulation have gained considerable attention in the literature, with several independent iterations, particularly in classical and quantum descriptions of gravity, cosmology, and electromagnetism.","In particular, in the case of canonical quantum gravity, the Fock--Stueckelberg approach is relevant to the so-called problem of time.","Our overview recalls the work of Fock and Stueckelberg and its physical interpretation with the aim of conceptually unifying the different iterations of the idea that appear in the literature and of motivating further research."],"url":"http://arxiv.org/abs/2402.12437v1","category":"gr-qc"}
{"created":"2024-02-19 19:00:02","title":"Investigating the Chemically Homogeneous Evolution Channel and its Role in the Formation of the Enigmatic Binary Black Hole Progenitor Candidate HD 5980","abstract":"Chemically homogeneous evolution (CHE) is a promising channel for forming massive binary black holes. The enigmatic, massive Wolf-Rayet (WR) binary HD 5980 A&B has been proposed to have formed through this channel. We investigate this claim by comparing its observed parameters with CHE models. Using MESA, we simulate grids of close massive binaries then use a Bayesian approach to compare them with the stars' observed orbital period, masses, luminosities, and hydrogen surface abundances. The most probable models, given the observational data, have initial periods ~3 days, widening to the present-day ~20 day orbit as a result of mass loss -- correspondingly, they have very high initial stellar masses ($\\gtrsim$150 M$_\\odot$). We explore variations in stellar wind-mass loss and internal mixing efficiency, and find that models assuming enhanced mass-loss are greatly favored to explain HD 5980, while enhanced mixing is only slightly favoured over our fiducial assumptions. Our most probable models slightly underpredict the hydrogen surface abundances. Regardless of its prior history, this system is a likely binary black hole progenitor. We model its further evolution under our fiducial and enhanced wind assumptions, finding that both stars produce black holes with masses ~19-37 M$_\\odot$. The projected final orbit is too wide to merge within a Hubble time through gravitational waves alone. However, the system is thought to be part of a 2+2 hierarchical multiple. We speculate that secular effects with the (possible) third and fourth companions may drive the system to promptly become a gravitational-wave source.","sentences":["Chemically homogeneous evolution (CHE) is a promising channel for forming massive binary black holes.","The enigmatic, massive Wolf-Rayet (WR) binary HD 5980 A&B has been proposed to have formed through this channel.","We investigate this claim by comparing its observed parameters with CHE models.","Using MESA, we simulate grids of close massive binaries then use a Bayesian approach to compare them with the stars' observed orbital period, masses, luminosities, and hydrogen surface abundances.","The most probable models, given the observational data, have initial periods ~3 days, widening to the present-day ~20 day orbit as a result of mass loss -- correspondingly, they have very high initial stellar masses ($\\gtrsim$150 M$_\\odot$).","We explore variations in stellar wind-mass loss and internal mixing efficiency, and find that models assuming enhanced mass-loss are greatly favored to explain HD 5980, while enhanced mixing is only slightly favoured over our fiducial assumptions.","Our most probable models slightly underpredict the hydrogen surface abundances.","Regardless of its prior history, this system is a likely binary black hole progenitor.","We model its further evolution under our fiducial and enhanced wind assumptions, finding that both stars produce black holes with masses ~19-37 M$_\\odot$.","The projected final orbit is too wide to merge within a Hubble time through gravitational waves alone.","However, the system is thought to be part of a 2+2 hierarchical multiple.","We speculate that secular effects with the (possible) third and fourth companions may drive the system to promptly become a gravitational-wave source."],"url":"http://arxiv.org/abs/2402.12438v1","category":"astro-ph.SR"}
{"created":"2024-02-19 19:00:01","title":"Understanding Fine-grained Distortions in Reports of Scientific Findings","abstract":"Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions. Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial. Prior work focused on individual aspects of distortions or worked with unpaired data. In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt. four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes in these characteristics in both human-annotated and large-scale unlabeled data. Our results show that scientific findings frequently undergo subtle distortions when reported. Tweets distort findings more often than science news reports. Detecting fine-grained distortions automatically poses a challenging task. In our experiments, fine-tuned task-specific models consistently outperform few-shot LLM prompting.","sentences":["Distorted science communication harms individuals and society as it can lead to unhealthy behavior change and decrease trust in scientific institutions.","Given the rapidly increasing volume of science communication in recent years, a fine-grained understanding of how findings from scientific publications are reported to the general public, and methods to detect distortions from the original work automatically, are crucial.","Prior work focused on individual aspects of distortions or worked with unpaired data.","In this work, we make three foundational contributions towards addressing this problem: (1) annotating 1,600 instances of scientific findings from academic papers paired with corresponding findings as reported in news articles and tweets wrt.","four characteristics: causality, certainty, generality and sensationalism; (2) establishing baselines for automatically detecting these characteristics; and (3) analyzing the prevalence of changes in these characteristics in both human-annotated and large-scale unlabeled data.","Our results show that scientific findings frequently undergo subtle distortions when reported.","Tweets distort findings more often than science news reports.","Detecting fine-grained distortions automatically poses a challenging task.","In our experiments, fine-tuned task-specific models consistently outperform few-shot LLM prompting."],"url":"http://arxiv.org/abs/2402.12431v1","category":"cs.CL"}
{"created":"2024-02-19 19:00:01","title":"Utilizing Resource Estimation for the Development of Quantum Computing Applications","abstract":"Quantum computing has made considerable progress in recent years in both software and hardware. But to unlock the power of quantum computers in solving problems that cannot be efficiently solved classically, quantum computing at scale is necessary. Unfortunately, quantum simulators suffer from their exponential complexity and, at the same time, the currently available quantum computing hardware is still rather limited (even if roadmaps make intriguing promises). Hence, in order to evaluate quantum computing applications, end-users are still frequently restricted to toy-size problem instances (which additionally often do not take error correction into account). This substantially hinders the development and assessment of real-world quantum computing applications. In this work, we demonstrate how to utilize Resource Estimation to improve this situation. We show how the current workflow (relying on simulation and/or execution) can be complemented with an estimation step, allowing that end-users (1) actually can consider real-world problem instances already today (also considering error correction schemes and correspondingly required hardware resources), (2) can start exploring possible optimizations of those instances across the entire design space, and (3) can incorporate hypotheses of hardware development trends to derive more informed and, thus, better design space parameters. Overall, this enables end-users already today to check out the promises of possible future quantum computing applications, even if the corresponding hardware to execute them is not available yet.","sentences":["Quantum computing has made considerable progress in recent years in both software and hardware.","But to unlock the power of quantum computers in solving problems that cannot be efficiently solved classically, quantum computing at scale is necessary.","Unfortunately, quantum simulators suffer from their exponential complexity and, at the same time, the currently available quantum computing hardware is still rather limited (even if roadmaps make intriguing promises).","Hence, in order to evaluate quantum computing applications, end-users are still frequently restricted to toy-size problem instances (which additionally often do not take error correction into account).","This substantially hinders the development and assessment of real-world quantum computing applications.","In this work, we demonstrate how to utilize Resource Estimation to improve this situation.","We show how the current workflow (relying on simulation and/or execution) can be complemented with an estimation step, allowing that end-users (1) actually can consider real-world problem instances already today (also considering error correction schemes and correspondingly required hardware resources), (2) can start exploring possible optimizations of those instances across the entire design space, and (3) can incorporate hypotheses of hardware development trends to derive more informed and, thus, better design space parameters.","Overall, this enables end-users already today to check out the promises of possible future quantum computing applications, even if the corresponding hardware to execute them is not available yet."],"url":"http://arxiv.org/abs/2402.12434v1","category":"quant-ph"}
{"created":"2024-02-20 16:53:26","title":"The Hidden Space of Transformer Language Adapters","abstract":"We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.","sentences":["We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages.","We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model.","Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance.","Last, we show that adapters operate on top of the model's frozen representation space while largely preserving its structure, rather than on an 'isolated' subspace.","Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency."],"url":"http://arxiv.org/abs/2402.13137v1","category":"cs.CL"}
{"created":"2024-02-20 15:24:21","title":"Mode Estimation with Partial Feedback","abstract":"The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments. These new learning pipelines call for new theoretical frameworks. In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback. We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting. Finally, we combine those contributions into a statistically and computationally efficient solution to our problem.","sentences":["The combination of lightly supervised pre-training and online fine-tuning has played a key role in recent AI developments.","These new learning pipelines call for new theoretical frameworks.","In this paper, we formalize core aspects of weakly supervised and active learning with a simple problem: the estimation of the mode of a distribution using partial feedback.","We show how entropy coding allows for optimal information acquisition from partial feedback, develop coarse sufficient statistics for mode identification, and adapt bandit algorithms to our new setting.","Finally, we combine those contributions into a statistically and computationally efficient solution to our problem."],"url":"http://arxiv.org/abs/2402.13079v1","category":"stat.ML"}
{"created":"2024-02-20 12:35:23","title":"MapTrack: Tracking in the Map","abstract":"Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target. Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities. Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations. In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes. Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter. The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons. Trajectories of undetected targets that are still within the probability map are extended by state estimations directly. The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter. The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20. Despite its superior performance, our method remains simple, online, and real-time. The code will be open-sourced later.","sentences":["Multi-Object Tracking (MOT) aims to maintain stable and uninterrupted trajectories for each target.","Most state-of-the-art approaches first detect objects in each frame and then implement data association between new detections and existing tracks using motion models and appearance similarities.","Despite achieving satisfactory results, occlusion and crowds can easily lead to missing and distorted detections, followed by missing and false associations.","In this paper, we first revisit the classic tracker DeepSORT, enhancing its robustness over crowds and occlusion significantly by placing greater trust in predictions when detections are unavailable or of low quality in crowded and occluded scenes.","Specifically, we propose a new framework comprising of three lightweight and plug-and-play algorithms: the probability map, the prediction map, and the covariance adaptive Kalman filter.","The probability map identifies whether undetected objects have genuinely disappeared from view (e.g., out of the image or entered a building) or are only temporarily undetected due to occlusion or other reasons.","Trajectories of undetected targets that are still within the probability map are extended by state estimations directly.","The prediction map determines whether an object is in a crowd, and we prioritize state estimations over observations when severe deformation of observations occurs, accomplished through the covariance adaptive Kalman filter.","The proposed method, named MapTrack, achieves state-of-the-art results on popular multi-object tracking benchmarks such as MOT17 and MOT20.","Despite its superior performance, our method remains simple, online, and real-time.","The code will be open-sourced later."],"url":"http://arxiv.org/abs/2402.12968v1","category":"cs.CV"}
{"created":"2024-02-20 12:25:36","title":"Inferring Non-Failure Conditions for Declarative Programs","abstract":"Unintended failures during a computation are painful but frequent during software development. Failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers. Programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct. This paper presents an approach to verify such assumptions. For this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations. In the positive case, the absence of such failures is ensured. In the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again. Our method is fully automatic and can be applied to larger declarative programs. The results of an implementation for functional logic Curry programs are presented.","sentences":["Unintended failures during a computation are painful but frequent during software development.","Failures due to external reasons (e.g., missing files, no permissions) can be caught by exception handlers.","Programming failures, such as calling a partially defined operation with unintended arguments, are often not caught due to the assumption that the software is correct.","This paper presents an approach to verify such assumptions.","For this purpose, non-failure conditions for operations are inferred and then checked in all uses of partially defined operations.","In the positive case, the absence of such failures is ensured.","In the negative case, the programmer could adapt the program to handle possibly failing situations and check the program again.","Our method is fully automatic and can be applied to larger declarative programs.","The results of an implementation for functional logic Curry programs are presented."],"url":"http://arxiv.org/abs/2402.12960v1","category":"cs.PL"}
{"created":"2024-02-20 11:52:29","title":"Normalized Orthography for Tunisian Arabic","abstract":"Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences. This research introduces the \"Normalized Orthography for Tunisian Arabic\" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency. The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology. This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic.","sentences":["Tunisian Arabic (ISO 693-3: aeb) is a distinct linguistic variety native to Tunisia, initially stemmed from the Arabic language and enriched by a multitude of historical influences.","This research introduces the \"Normalized Orthography for Tunisian Arabic\" (NOTA), an adaptation of CODA* guidelines tailored for transcribing Tunisian Arabic using the Arabic script for language resource development purposes, with an emphasis on user-friendliness and consistency.","The updated standard seeks to address challenges related to accurately representing the unique characteristics of Tunisian phonology and morphology.","This will be achieved by rectifying problems arising from transcriptions based on resemblances to Modern Standard Arabic."],"url":"http://arxiv.org/abs/2402.12940v1","category":"cs.CL"}
{"created":"2024-02-20 11:50:27","title":"UniCell: Universal Cell Nucleus Classification via Prompt Learning","abstract":"The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis. Numerous pathological datasets are currently available, but their annotations are inconsistent. Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition. In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains. In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets. Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features. The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources. Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks. Code and models are available at https://github.com/lhaof/UniCell","sentences":["The recognition of multi-class cell nuclei can significantly facilitate the process of histopathological diagnosis.","Numerous pathological datasets are currently available, but their annotations are inconsistent.","Most existing methods require individual training on each dataset to deduce the relevant labels and lack the use of common knowledge across datasets, consequently restricting the quality of recognition.","In this paper, we propose a universal cell nucleus classification framework (UniCell), which employs a novel prompt learning mechanism to uniformly predict the corresponding categories of pathological images from different dataset domains.","In particular, our framework adopts an end-to-end architecture for nuclei detection and classification, and utilizes flexible prediction heads for adapting various datasets.","Moreover, we develop a Dynamic Prompt Module (DPM) that exploits the properties of multiple datasets to enhance features.","The DPM first integrates the embeddings of datasets and semantic categories, and then employs the integrated prompts to refine image representations, efficiently harvesting the shared knowledge among the related cell types and data sources.","Experimental results demonstrate that the proposed method effectively achieves the state-of-the-art results on four nucleus detection and classification benchmarks.","Code and models are available at https://github.com/lhaof/UniCell"],"url":"http://arxiv.org/abs/2402.12938v1","category":"cs.CV"}
{"created":"2024-02-20 11:34:18","title":"Enhancement of the critical current by surface irregularities in Fe-based superconductors","abstract":"The critical current $I_c$ of single crystals of the iron pnictide superconductor BaFe$_2$(As$_{1-x}$P$_x$)$_2$, has been studied through measurements of magnetic hysteresis cycles. We show that the introduction of surface irregularities in the $\\mu$m scale significantly increase $I_c$, primarily near the irreversibility magnetic field $H_{irr}$, where the surface currents are the main contribution to $I_c$. Such an increase is consistent with a theoretical estimate for the maximum non-dissipative current that a rough surface can sustain, based on Mathieu-Simon continuum theory for the vortex state.","sentences":["The critical current $I_c$ of single crystals of the iron pnictide superconductor BaFe$_2$(As$_{1-x}$P$_x$)$_2$, has been studied through measurements of magnetic hysteresis cycles.","We show that the introduction of surface irregularities in the $\\mu$m scale significantly increase $I_c$, primarily near the irreversibility magnetic field $H_{irr}$, where the surface currents are the main contribution to $I_c$. Such an increase is consistent with a theoretical estimate for the maximum non-dissipative current that a rough surface can sustain, based on Mathieu-Simon continuum theory for the vortex state."],"url":"http://arxiv.org/abs/2402.12933v1","category":"cond-mat.supr-con"}
{"created":"2024-02-20 09:30:48","title":"MoELoRA: Contrastive Learning Guided Mixture of Experts on Parameter-Efficient Fine-Tuning for Large Language Models","abstract":"Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks. Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research. However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks. In this work, we introduce a novel PEFT method: MoELoRA. We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to learn distinct features. We conducted experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks. With the same number of parameters, our approach outperforms LoRA significantly. In math reasoning, MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on several benchmarks.","sentences":["Fine-tuning is often necessary to enhance the adaptability of Large Language Models (LLM) to downstream tasks.","Nonetheless, the process of updating billions of parameters demands significant computational resources and training time, which poses a substantial obstacle to the widespread application of large-scale models in various scenarios.","To address this issue, Parameter-Efficient Fine-Tuning (PEFT) has emerged as a prominent paradigm in recent research.","However, current PEFT approaches that employ a limited set of global parameters (such as LoRA, which adds low-rank approximation matrices to all weights) face challenges in flexibly combining different computational modules in downstream tasks.","In this work, we introduce a novel PEFT method: MoELoRA.","We consider LoRA as Mixture of Experts (MoE), and to mitigate the random routing phenomenon observed in MoE, we propose the utilization of contrastive learning to encourage experts to learn distinct features.","We conducted experiments on 11 tasks in math reasoning and common-sense reasoning benchmarks.","With the same number of parameters, our approach outperforms LoRA significantly.","In math reasoning, MoELoRA achieved an average performance that was 4.2% higher than LoRA, and demonstrated competitive performance compared to the 175B GPT-3.5 on several benchmarks."],"url":"http://arxiv.org/abs/2402.12851v1","category":"cs.CL"}
{"created":"2024-02-20 08:33:41","title":"OMRA: Online Motion Resolution Adaptation to Remedy Domain Shift in Learned Hierarchical B-frame Coding","abstract":"Learned hierarchical B-frame coding aims to leverage bi-directional reference frames for better coding efficiency. However, the domain shift between training and test scenarios due to dataset limitations poses a challenge. This issue arises from training the codec with small groups of pictures (GOP) but testing it on large GOPs. Specifically, the motion estimation network, when trained on small GOPs, is unable to handle large motion at test time, incurring a negative impact on compression performance. To mitigate the domain shift, we present an online motion resolution adaptation (OMRA) method. It adapts the spatial resolution of video frames on a per-frame basis to suit the capability of the motion estimation network in a pre-trained B-frame codec. Our OMRA is an online, inference technique. It need not re-train the codec and is readily applicable to existing B-frame codecs that adopt hierarchical bi-directional prediction. Experimental results show that OMRA significantly enhances the compression performance of two state-of-the-art learned B-frame codecs on commonly used datasets.","sentences":["Learned hierarchical B-frame coding aims to leverage bi-directional reference frames for better coding efficiency.","However, the domain shift between training and test scenarios due to dataset limitations poses a challenge.","This issue arises from training the codec with small groups of pictures (GOP) but testing it on large GOPs.","Specifically, the motion estimation network, when trained on small GOPs, is unable to handle large motion at test time, incurring a negative impact on compression performance.","To mitigate the domain shift, we present an online motion resolution adaptation (OMRA) method.","It adapts the spatial resolution of video frames on a per-frame basis to suit the capability of the motion estimation network in a pre-trained B-frame codec.","Our OMRA is an online, inference technique.","It need not re-train the codec and is readily applicable to existing B-frame codecs that adopt hierarchical bi-directional prediction.","Experimental results show that OMRA significantly enhances the compression performance of two state-of-the-art learned B-frame codecs on commonly used datasets."],"url":"http://arxiv.org/abs/2402.12816v1","category":"eess.IV"}
{"created":"2024-02-20 08:27:50","title":"Learning Generalization and Regularization of Nonhomogeneous Temporal Poisson Processes","abstract":"The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications. Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods. In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem. We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited. We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters. Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness.","sentences":["The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is an essentially important counting process with numerous real-world applications.","Up to date, almost all works in the literature have been on the estimation of NHPPs with infinite data using non-data driven binning methods.","In this paper, we formulate the problem of estimation of NHPPs from finite and limited data as a learning generalization problem.","We mathematically show that while binning methods are essential for the estimation of NHPPs, they pose a threat of overfitting when the amount of data is limited.","We propose a framework for regularized learning of NHPPs with two new adaptive and data-driven binning methods that help to remove the ad-hoc tuning of binning parameters.","Our methods are experimentally tested on synthetic and real-world datasets and the results show their effectiveness."],"url":"http://arxiv.org/abs/2402.12808v1","category":"cs.LG"}
{"created":"2024-02-20 07:49:46","title":"Stochastic Graph Heat Modelling for Diffusion-based Connectivity Retrieval","abstract":"Heat diffusion describes the process by which heat flows from areas with higher temperatures to ones with lower temperatures. This concept was previously adapted to graph structures, whereby heat flows between nodes of a graph depending on the graph topology. Here, we combine the graph heat equation with the stochastic heat equation, which ultimately yields a model for multivariate time signals on a graph. We show theoretically how the model can be used to directly compute the diffusion-based connectivity structure from multivariate signals. Unlike other connectivity measures, our heat model-based approach is inherently multivariate and yields an absolute scaling factor, namely the graph thermal diffusivity, which captures the extent of heat-like graph propagation in the data. On two datasets, we show how the graph thermal diffusivity can be used to characterise Alzheimer's disease. We find that the graph thermal diffusivity is lower for Alzheimer's patients than healthy controls and correlates with dementia scores, suggesting structural impairment in patients in line with previous findings.","sentences":["Heat diffusion describes the process by which heat flows from areas with higher temperatures to ones with lower temperatures.","This concept was previously adapted to graph structures, whereby heat flows between nodes of a graph depending on the graph topology.","Here, we combine the graph heat equation with the stochastic heat equation, which ultimately yields a model for multivariate time signals on a graph.","We show theoretically how the model can be used to directly compute the diffusion-based connectivity structure from multivariate signals.","Unlike other connectivity measures, our heat model-based approach is inherently multivariate and yields an absolute scaling factor, namely the graph thermal diffusivity, which captures the extent of heat-like graph propagation in the data.","On two datasets, we show how the graph thermal diffusivity can be used to characterise Alzheimer's disease.","We find that the graph thermal diffusivity is lower for Alzheimer's patients than healthy controls and correlates with dementia scores, suggesting structural impairment in patients in line with previous findings."],"url":"http://arxiv.org/abs/2402.12785v1","category":"eess.SP"}
{"created":"2024-02-20 03:07:57","title":"On reduced modeling of the modulational dynamics in magnetohydrodynamics","abstract":"This paper explores structure formation in two-dimensional magnetohydrodynamic (MHD) turbulence as a modulational instability (MI) of turbulent fluctuations. We focus on the early stages of structure formation and consider simple backgrounds that allow for a tractable model of the MI while retaining the full chain of modulational harmonics. This approach allows us to systematically examine the validity of popular closures such as the quasilinear approximation and other low-order truncations. We find that, although such simple closures can provide quantitatively accurate approximations of the MI growth rates in some regimes, they can fail to capture the modulational dynamics in adjacent regimes even qualitatively, falsely predicting MI when the system is actually stable. We find that this discrepancy is due to the excitation of propagating spectral waves (PSWs) which can ballistically transport energy along the modulational spectrum, unimpeded until dissipative scales, thereby breaking the feedback loops that would otherwise sustain MIs. PSWs can be self-maintained as global modes with real frequencies and drain energy from the primary structure at a constant rate until the primary structure is depleted. To describe these waves within a reduced model, we propose an approximate spectral closure that captures them and MIs on the same footing. We also find that introducing corrections to ideal MHD, conservative or dissipative, can suppress PSWs and reinstate the accuracy of the quasilinear approximation. In this sense, ideal MHD is a `singular' system that is particularly sensitive to the accuracy of the closure within mean-field models.","sentences":["This paper explores structure formation in two-dimensional magnetohydrodynamic (MHD) turbulence as a modulational instability (MI) of turbulent fluctuations.","We focus on the early stages of structure formation and consider simple backgrounds that allow for a tractable model of the MI while retaining the full chain of modulational harmonics.","This approach allows us to systematically examine the validity of popular closures such as the quasilinear approximation and other low-order truncations.","We find that, although such simple closures can provide quantitatively accurate approximations of the MI growth rates in some regimes, they can fail to capture the modulational dynamics in adjacent regimes even qualitatively, falsely predicting MI when the system is actually stable.","We find that this discrepancy is due to the excitation of propagating spectral waves (PSWs) which can ballistically transport energy along the modulational spectrum, unimpeded until dissipative scales, thereby breaking the feedback loops that would otherwise sustain MIs.","PSWs can be self-maintained as global modes with real frequencies and drain energy from the primary structure at a constant rate until the primary structure is depleted.","To describe these waves within a reduced model, we propose an approximate spectral closure that captures them and MIs on the same footing.","We also find that introducing corrections to ideal MHD, conservative or dissipative, can suppress PSWs and reinstate the accuracy of the quasilinear approximation.","In this sense, ideal MHD is a `singular' system that is particularly sensitive to the accuracy of the closure within mean-field models."],"url":"http://arxiv.org/abs/2402.12680v1","category":"physics.plasm-ph"}
{"created":"2024-02-20 01:49:15","title":"Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation","abstract":"Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world. In this work, we study the correspondence between such decontextualized \"trick tests\" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations). We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation. We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation. We conduct each evaluation for seven instruction-tuned LLMs. For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises. We found no correspondence between trick tests and RUTEd evaluations. Specifically, selecting the least biased model based on the de-contextualized results coincides with selecting the model with the best performance on RUTEd evaluations only as often as random chance. We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms.","sentences":["Bias benchmarks are a popular method for studying the negative impacts of bias in LLMs, yet there has been little empirical investigation of whether these benchmarks are actually indicative of how real world harm may manifest in the real world.","In this work, we study the correspondence between such decontextualized \"trick tests\" and evaluations that are more grounded in Realistic Use and Tangible {Effects (i.e. RUTEd evaluations).","We explore this correlation in the context of gender-occupation bias--a popular genre of bias evaluation.","We compare three de-contextualized evaluations adapted from the current literature to three analogous RUTEd evaluations applied to long-form content generation.","We conduct each evaluation for seven instruction-tuned LLMs.","For the RUTEd evaluations, we conduct repeated trials of three text generation tasks: children's bedtime stories, user personas, and English language learning exercises.","We found no correspondence between trick tests and RUTEd evaluations.","Specifically, selecting the least biased model based on the de-contextualized results coincides with selecting the model with the best performance on RUTEd evaluations only as often as random chance.","We conclude that evaluations that are not based in realistic use are likely insufficient to mitigate and assess bias and real-world harms."],"url":"http://arxiv.org/abs/2402.12649v1","category":"cs.CL"}
{"created":"2024-02-19 23:08:12","title":"Ultrafast dynamics of a fermion chain in a terahertz field-driven optical cavity","abstract":"We study the effect of a terahertz field-driven single cavity mode for ultrafast control of a fermion chain with dissipation-induced nonlinearity and quadratic coupling to an infrared-active phonon mode. Without photon loss from the cavity, we uncover a first-order phase transition in the nonequilibrium steady state only for the lower phonon-polariton, accompanied by polaritons whose frequency response is asymmetric with respect to the photon frequency due to the direct laser-induced dressing effect on the photon. A weak laser field fails to induce the phase transition but renders the polaritons symmetrical. Finally, we show that sufficiently strong photon loss from the cavity eliminates the polaritons and the associated phase transition. The experimental feasibility of these phenomena is also proposed.","sentences":["We study the effect of a terahertz field-driven single cavity mode for ultrafast control of a fermion chain with dissipation-induced nonlinearity and quadratic coupling to an infrared-active phonon mode.","Without photon loss from the cavity, we uncover a first-order phase transition in the nonequilibrium steady state only for the lower phonon-polariton, accompanied by polaritons whose frequency response is asymmetric with respect to the photon frequency due to the direct laser-induced dressing effect on the photon.","A weak laser field fails to induce the phase transition but renders the polaritons symmetrical.","Finally, we show that sufficiently strong photon loss from the cavity eliminates the polaritons and the associated phase transition.","The experimental feasibility of these phenomena is also proposed."],"url":"http://arxiv.org/abs/2402.12591v1","category":"cond-mat.str-el"}
{"created":"2024-02-19 22:55:56","title":"On The Fourier Coefficients of High-Dimensional Random Geometric Graphs","abstract":"The random geometric graph $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ is formed by sampling $n$ i.i.d. vectors $\\{V_i\\}_{i = 1}^n$ uniformly on $\\mathbb{S}^{d-1}$ and placing an edge between pairs of vertices $i$ and $j$ for which $\\langle V_i,V_j\\rangle \\ge \\tau^p_d,$ where $\\tau^p_d$ is such that the expected density is $p.$ We study the low-degree Fourier coefficients of the distribution $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ and its Gaussian analogue.   Our main conceptual contribution is a novel two-step strategy for bounding Fourier coefficients which we believe is more widely applicable to studying latent space distributions. First, we localize the dependence among edges to few fragile edges. Second, we partition the space of latent vector configurations $(\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p))^{\\otimes n}$ based on the set of fragile edges and on each subset of configurations, we define a noise operator acting independently on edges not incident (in an appropriate sense) to fragile edges.   We apply the resulting bounds to: 1) Settle the low-degree polynomial complexity of distinguishing spherical and Gaussian random geometric graphs from Erdos-Renyi both in the case of observing a complete set of edges and in the non-adaptively chosen mask $\\mathcal{M}$ model recently introduced by [MVW24]; 2) Exhibit a statistical-computational gap for distinguishing $\\mathsf{RGG}$ and the planted coloring model [KVWX23] in a regime when $\\mathsf{RGG}$ is distinguishable from Erdos-Renyi; 3) Reprove known bounds on the second eigenvalue of random geometric graphs.","sentences":["The random geometric graph $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ is formed by sampling $n$ i.i.d. vectors $\\{V_i\\}_{i = 1}^n$ uniformly on $\\mathbb{S}^{d-1}$ and placing an edge between pairs of vertices $i$ and $j$ for which $\\langle V_i,V_j\\rangle \\ge \\tau^p_d,$ where $\\tau^p_d$ is such that the expected density is $p.$ We study the low-degree Fourier coefficients of the distribution $\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p)$ and its Gaussian analogue.   ","Our main conceptual contribution is a novel two-step strategy for bounding Fourier coefficients which we believe is more widely applicable to studying latent space distributions.","First, we localize the dependence among edges to few fragile edges.","Second, we partition the space of latent vector configurations $(\\mathsf{RGG}(n,\\mathbb{S}^{d-1}, p))^{\\otimes n}$ based on the set of fragile edges and on each subset of configurations, we define a noise operator acting independently on edges not incident (in an appropriate sense) to fragile edges.   ","We apply the resulting bounds to: 1) Settle the low-degree polynomial complexity of distinguishing spherical and Gaussian random geometric graphs from Erdos-Renyi both in the case of observing a complete set of edges and in the non-adaptively chosen mask $\\mathcal{M}$ model recently introduced by [MVW24]; 2) Exhibit a statistical-computational gap for distinguishing $\\mathsf{RGG}$ and the planted coloring model","[KVWX23] in a regime when $\\mathsf{RGG}$ is distinguishable from Erdos-Renyi; 3) Reprove known bounds on the second eigenvalue of random geometric graphs."],"url":"http://arxiv.org/abs/2402.12589v1","category":"math.ST"}
{"created":"2024-02-19 20:08:13","title":"Integrating kNN with Foundation Models for Adaptable and Privacy-Aware Image Classification","abstract":"Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes. Yet, this adaptability is vital for addressing user data privacy concerns. We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining. Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability. We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions. Quantitative experiments confirm improved classification across established benchmark datasets and the method's applicability to distinct medical image classification tasks. Additionally, we assess the method's robustness in continual learning and data removal scenarios. The approach exhibits great promise for bridging the gap between foundation models' performance and challenges tied to data privacy. The source code is available at https://github.com/TobArc/privacy-aware-image-classification-with-kNN.","sentences":["Traditional deep learning models implicity encode knowledge limiting their transparency and ability to adapt to data changes.","Yet, this adaptability is vital for addressing user data privacy concerns.","We address this limitation by storing embeddings of the underlying training data independently of the model weights, enabling dynamic data modifications without retraining.","Specifically, our approach integrates the $k$-Nearest Neighbor ($k$-NN) classifier with a vision-based foundation model, pre-trained self-supervised on natural images, enhancing interpretability and adaptability.","We share open-source implementations of a previously unpublished baseline method as well as our performance-improving contributions.","Quantitative experiments confirm improved classification across established benchmark datasets and the method's applicability to distinct medical image classification tasks.","Additionally, we assess the method's robustness in continual learning and data removal scenarios.","The approach exhibits great promise for bridging the gap between foundation models' performance and challenges tied to data privacy.","The source code is available at https://github.com/TobArc/privacy-aware-image-classification-with-kNN."],"url":"http://arxiv.org/abs/2402.12500v1","category":"cs.CV"}
{"created":"2024-02-19 19:10:23","title":"An Adaptive Cubic Regularization quasi-Newton Method on Riemannian Manifolds","abstract":"A quasi-Newton method with cubic regularization is designed for solving Riemannian unconstrained nonconvex optimization problems. The proposed algorithm is fully adaptive with at most ${\\cal O} (\\epsilon_g^{-3/2})$ iterations to achieve a gradient smaller than $\\epsilon_g$ for given $\\epsilon_g$, and at most $\\mathcal O(\\max\\{ \\epsilon_g^{-\\frac{3}{2}}, \\epsilon_H^{-3} \\})$ iterations to reach a second-order stationary point respectively. Notably, the proposed algorithm remains applicable even in cases of the gradient and Hessian of the objective function unknown. Numerical experiments are performed with gradient and Hessian being approximated by forward finite-differences to illustrate the theoretical results and numerical comparison.","sentences":["A quasi-Newton method with cubic regularization is designed for solving Riemannian unconstrained nonconvex optimization problems.","The proposed algorithm is fully adaptive with at most ${\\cal O} (\\epsilon_g^{-3/2})$ iterations to achieve a gradient smaller than $\\epsilon_g$ for given $\\epsilon_g$, and at most $\\mathcal O(\\max\\{ \\epsilon_g^{-\\frac{3}{2}}, \\epsilon_H^{-3} \\})$ iterations to reach a second-order stationary point respectively.","Notably, the proposed algorithm remains applicable even in cases of the gradient and Hessian of the objective function unknown.","Numerical experiments are performed with gradient and Hessian being approximated by forward finite-differences to illustrate the theoretical results and numerical comparison."],"url":"http://arxiv.org/abs/2402.12464v1","category":"math.OC"}
{"created":"2024-02-19 19:01:02","title":"Numerical Challenges in Modeling Gravothermal Collapse in Self-Interacting Dark Matter Halos","abstract":"When dark matter has a large cross section for self scattering, halos can undergo a process known as gravothermal core collapse, where the inner core rapidly increases in density and temperature. To date, several methods have been used to implement Self-Interacting Dark Matter~(SIDM) in N-body codes, but there has been no systematic study of these different methods or their accuracy in the core-collapse phase. In this paper, we compare three different numerical implementations of SIDM, including the standard methods from the GIZMO and Arepo codes, by simulating idealized dwarf halos undergoing significant dark matter self interactions ($\\sigma/m = 50$~cm$^2$/g). When simulating these halos, we also vary the mass resolution, time-stepping criteria, and gravitational force-softening scheme. The various SIDM methods lead to distinct differences in a halo's evolution during the core-collapse phase, as each results in slightly different scattering rates and spurious energy gains/losses. The use of adaptive force softening for gravity can lead to numerical heating that artificially accelerates core collapse, while an insufficiently small simulation time step can cause core evolution to stall or completely reverse. Additionally, particle numbers must be large enough to ensure that the simulated halos are not sensitive to noise in the initial conditions. Even for the highest-resolution simulations tested in this study ($10^6$ particles per halo), we find that variations of order $10\\%$ in collapse time are still present. The results of this work underscore the sensitivity of SIDM modeling on the choice of numerical implementation and motivate a careful study of how these results generalize to halos in a cosmological context.","sentences":["When dark matter has a large cross section for self scattering, halos can undergo a process known as gravothermal core collapse, where the inner core rapidly increases in density and temperature.","To date, several methods have been used to implement Self-Interacting Dark Matter~(SIDM) in N-body codes, but there has been no systematic study of these different methods or their accuracy in the core-collapse phase.","In this paper, we compare three different numerical implementations of SIDM, including the standard methods from the GIZMO and Arepo codes, by simulating idealized dwarf halos undergoing significant dark matter self interactions ($\\sigma/m = 50$~cm$^2$/g).","When simulating these halos, we also vary the mass resolution, time-stepping criteria, and gravitational force-softening scheme.","The various SIDM methods lead to distinct differences in a halo's evolution during the core-collapse phase, as each results in slightly different scattering rates and spurious energy gains/losses.","The use of adaptive force softening for gravity can lead to numerical heating that artificially accelerates core collapse, while an insufficiently small simulation time step can cause core evolution to stall or completely reverse.","Additionally, particle numbers must be large enough to ensure that the simulated halos are not sensitive to noise in the initial conditions.","Even for the highest-resolution simulations tested in this study ($10^6$ particles per halo), we find that variations of order $10\\%$ in collapse time are still present.","The results of this work underscore the sensitivity of SIDM modeling on the choice of numerical implementation and motivate a careful study of how these results generalize to halos in a cosmological context."],"url":"http://arxiv.org/abs/2402.12452v1","category":"astro-ph.CO"}
{"created":"2024-02-19 19:00:00","title":"On Binary Formation from Three Initially Unbound Bodies","abstract":"We explore three-body binary formation (3BBF), the formation of a bound system via gravitational scattering of three initially unbound bodies (3UB), using direct numerical integrations. For the first time, we consider systems with unequal masses, as well as finite-size and post-Newtonian effects. Our analytically derived encounter rates and numerical scattering results reproduce the 3BBF rate predicted by Goodman & Hut (1993) for hard binaries in dense star clusters. We find that 3BBF occurs overwhelmingly through nonresonant encounters and that the two most massive bodies are never the most likely to bind. Instead, 3BBF favors pairing the two least massive bodies (for wide binaries) or the most plus least massive bodies (for hard binaries). 3BBF overwhelmingly favors wide binary formation with super-thermal eccentricities, perhaps helping to explain the eccentric wide binaries observed by Gaia. Hard binaries form much more rarely, but with a thermal eccentricity distribution. The semimajor axis distribution scales cumulatively as $a^3$ for hard and slightly wider binaries. Though mergers are rare between black holes when including relativistic effects, direct collisions occur frequently between main-sequence stars -- more often than hard 3BBF. Yet, these collisions do not significantly suppress hard 3BBF at the low velocity dispersions typical of open or globular clusters. Energy dissipation through gravitational radiation leads to a small probability of a bound, hierarchical triple system forming directly from 3UB.","sentences":["We explore three-body binary formation (3BBF), the formation of a bound system via gravitational scattering of three initially unbound bodies (3UB), using direct numerical integrations.","For the first time, we consider systems with unequal masses, as well as finite-size and post-Newtonian effects.","Our analytically derived encounter rates and numerical scattering results reproduce the 3BBF rate predicted by Goodman & Hut (1993) for hard binaries in dense star clusters.","We find that 3BBF occurs overwhelmingly through nonresonant encounters and that the two most massive bodies are never the most likely to bind.","Instead, 3BBF favors pairing the two least massive bodies (for wide binaries) or the most plus least massive bodies (for hard binaries).","3BBF overwhelmingly favors wide binary formation with super-thermal eccentricities, perhaps helping to explain the eccentric wide binaries observed by Gaia.","Hard binaries form much more rarely, but with a thermal eccentricity distribution.","The semimajor axis distribution scales cumulatively as $a^3$ for hard and slightly wider binaries.","Though mergers are rare between black holes when including relativistic effects, direct collisions occur frequently between main-sequence stars -- more often than hard 3BBF.","Yet, these collisions do not significantly suppress hard 3BBF at the low velocity dispersions typical of open or globular clusters.","Energy dissipation through gravitational radiation leads to a small probability of a bound, hierarchical triple system forming directly from 3UB."],"url":"http://arxiv.org/abs/2402.12429v1","category":"astro-ph.SR"}
{"created":"2024-02-19 07:47:23","title":"Vehicle-group-based Crash Risk Formation and Propagation Analysis for Expressways","abstract":"Previous studies in predicting crash risk primarily associated the number or likelihood of crashes on a road segment with traffic parameters or geometric characteristics of the segment, usually neglecting the impact of vehicles' continuous movement and interactions with nearby vehicles. Advancements in communication technologies have empowered driving information collected from surrounding vehicles, enabling the study of group-based crash risks. Based on high-resolution vehicle trajectory data, this research focused on vehicle groups as the subject of analysis and explored risk formation and propagation mechanisms considering features of vehicle groups and road segments. Several key factors contributing to crash risks were identified, including past high-risk vehicle-group states, complex vehicle behaviors, high percentage of large vehicles, frequent lane changes within a vehicle group, and specific road geometries. A multinomial logistic regression model was developed to analyze the spatial risk propagation patterns, which were classified based on the trend of high-risk occurrences within vehicle groups. The results indicated that extended periods of high-risk states, increase in vehicle-group size, and frequent lane changes are associated with adverse risk propagation patterns. Conversely, smoother traffic flow and high initial crash risk values are linked to risk dissipation. Furthermore, the study conducted sensitivity analysis on different types of classifiers, prediction time intervalsss and adaptive TTC thresholds. The highest AUC value for vehicle-group risk prediction surpassed 0.93. The findings provide valuable insights to researchers and practitioners in understanding and prediction of vehicle-group safety, ultimately improving active traffic safety management and operations of Connected and Autonomous Vehicles.","sentences":["Previous studies in predicting crash risk primarily associated the number or likelihood of crashes on a road segment with traffic parameters or geometric characteristics of the segment, usually neglecting the impact of vehicles' continuous movement and interactions with nearby vehicles.","Advancements in communication technologies have empowered driving information collected from surrounding vehicles, enabling the study of group-based crash risks.","Based on high-resolution vehicle trajectory data, this research focused on vehicle groups as the subject of analysis and explored risk formation and propagation mechanisms considering features of vehicle groups and road segments.","Several key factors contributing to crash risks were identified, including past high-risk vehicle-group states, complex vehicle behaviors, high percentage of large vehicles, frequent lane changes within a vehicle group, and specific road geometries.","A multinomial logistic regression model was developed to analyze the spatial risk propagation patterns, which were classified based on the trend of high-risk occurrences within vehicle groups.","The results indicated that extended periods of high-risk states, increase in vehicle-group size, and frequent lane changes are associated with adverse risk propagation patterns.","Conversely, smoother traffic flow and high initial crash risk values are linked to risk dissipation.","Furthermore, the study conducted sensitivity analysis on different types of classifiers, prediction time intervalsss and adaptive TTC thresholds.","The highest AUC value for vehicle-group risk prediction surpassed 0.93.","The findings provide valuable insights to researchers and practitioners in understanding and prediction of vehicle-group safety, ultimately improving active traffic safety management and operations of Connected and Autonomous Vehicles."],"url":"http://arxiv.org/abs/2402.12415v1","category":"cs.LG"}
{"created":"2024-02-19 02:34:23","title":"Deep Structural Knowledge Exploitation and Synergy for Estimating Node Importance Value on Heterogeneous Information Networks","abstract":"Node importance estimation problem has been studied conventionally with homogeneous network topology analysis. To deal with network heterogeneity, a few recent methods employ graph neural models to automatically learn diverse sources of information. However, the major concern revolves around that their full adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability. In this work, we propose a novel learning framework: SKES. Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations. Based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its disparity against the reference. This establishes an interpretable node importance computation paradigm. Furthermore, SKES dives deep into the understanding that \"nodes with similar characteristics are prone to have similar importance values\" whilst guaranteeing that such informativeness disparity between any different nodes is orderly reflected by the embedding distance of their associated latent features. Extensive experiments on three widely-evaluated benchmarks demonstrate the performance superiority of SKES over several recent competing methods.","sentences":["Node importance estimation problem has been studied conventionally with homogeneous network topology analysis.","To deal with network heterogeneity, a few recent methods employ graph neural models to automatically learn diverse sources of information.","However, the major concern revolves around that their full adaptive learning process may lead to insufficient information exploration, thereby formulating the problem as the isolated node value prediction with underperformance and less interpretability.","In this work, we propose a novel learning framework: SKES.","Different from previous automatic learning designs, SKES exploits heterogeneous structural knowledge to enrich the informativeness of node representations.","Based on a sufficiently uninformative reference, SKES estimates the importance value for any input node, by quantifying its disparity against the reference.","This establishes an interpretable node importance computation paradigm.","Furthermore, SKES dives deep into the understanding that \"nodes with similar characteristics are prone to have similar importance values\" whilst guaranteeing that such informativeness disparity between any different nodes is orderly reflected by the embedding distance of their associated latent features.","Extensive experiments on three widely-evaluated benchmarks demonstrate the performance superiority of SKES over several recent competing methods."],"url":"http://arxiv.org/abs/2402.12411v1","category":"cs.SI"}
{"created":"2024-02-20 18:59:57","title":"How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey","abstract":"Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments. This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations. Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields. It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges.","sentences":["Over the past two decades, research in the field of Simultaneous Localization and Mapping (SLAM) has undergone a significant evolution, highlighting its critical role in enabling autonomous exploration of unknown environments.","This evolution ranges from hand-crafted methods, through the era of deep learning, to more recent developments focused on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) representations.","Recognizing the growing body of research and the absence of a comprehensive survey on the topic, this paper aims to provide the first comprehensive overview of SLAM progress through the lens of the latest advancements in radiance fields.","It sheds light on the background, evolutionary path, inherent strengths and limitations, and serves as a fundamental reference to highlight the dynamic progress and specific challenges."],"url":"http://arxiv.org/abs/2402.13255v1","category":"cs.CV"}
{"created":"2024-02-20 18:51:59","title":"Some obstructions to positive scalar curvature on a noncompact manifold","abstract":"We give obstructions for a noncompact manifold to admit a complete Riemannian metric with (nonuniformly) positive scalar curvature. We treat both the finite volume and infinite volume cases.","sentences":["We give obstructions for a noncompact manifold to admit a complete Riemannian metric with (nonuniformly) positive scalar curvature.","We treat both the finite volume and infinite volume cases."],"url":"http://arxiv.org/abs/2402.13239v1","category":"math.DG"}
{"created":"2024-02-20 18:50:25","title":"Towards audio language modeling -- an overview","abstract":"Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency. Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs). Numerous high-performance neural audio codecs and codec-based LMs have been developed. The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs.","sentences":["Neural audio codecs are initially introduced to compress audio data into compact codes to reduce transmission latency.","Researchers recently discovered the potential of codecs as suitable tokenizers for converting continuous audio into discrete codes, which can be employed to develop audio language models (LMs).","Numerous high-performance neural audio codecs and codec-based LMs have been developed.","The paper aims to provide a thorough and systematic overview of the neural audio codec models and codec-based LMs."],"url":"http://arxiv.org/abs/2402.13236v1","category":"eess.AS"}
{"created":"2024-02-20 18:44:31","title":"On Type II blowups of axisymmetric solutions to the Navier-Stokes equations","abstract":"In the note, the Euler scaling is used to study a certain scenario of potential Type II blowups of axisymmetric solutions to the Navier-Stokes equations.","sentences":["In the note, the Euler scaling is used to study a certain scenario of potential Type II blowups of axisymmetric solutions to the Navier-Stokes equations."],"url":"http://arxiv.org/abs/2402.13229v1","category":"math.AP"}
{"created":"2024-02-20 18:08:11","title":"Polygonal surfaces in pseudo-hyperbolic spaces","abstract":"A polygonal surface in the pseudo-hyperbolic space H 2,n is a complete maximal surface bounded by a lightlike polygon in the Einstein universe Ein 1,n with finitely many vertices. In this article, we give several characterizations of them. Polygonal surfaces are characterized by finiteness of their total curvature, by asymptotic flatness, and also by the fact of having parabolic type and polynomial quartic differential. Our result relies on a comparison between three ideal boundaries associated with a maximal surface, corresponding to three distinct distances naturally defined on the maximal surface.","sentences":["A polygonal surface in the pseudo-hyperbolic space H 2,n is a complete maximal surface bounded by a lightlike polygon in the Einstein universe Ein 1,n with finitely many vertices.","In this article, we give several characterizations of them.","Polygonal surfaces are characterized by finiteness of their total curvature, by asymptotic flatness, and also by the fact of having parabolic type and polynomial quartic differential.","Our result relies on a comparison between three ideal boundaries associated with a maximal surface, corresponding to three distinct distances naturally defined on the maximal surface."],"url":"http://arxiv.org/abs/2402.13197v1","category":"math.DG"}
{"created":"2024-02-20 17:53:02","title":"Effects of pair freeze-out on photon distributions in BBN epoch","abstract":"We investigate the evolution of non-extensivity in the photon distribution during the Big Bang Nucleosynthesis (BBN) epoch using Tsallis statistics. Assuming a minimal deviation from the Planck distribution, we construct the perturbed Boltzmann equation for photons, including the collision terms for pair creation and annihilation processes. We analyze the possibility that these collisions could cause a slight increase in the number of high-frequency photons within the BBN era, and consequently, the primordial plasma might be temporarily placed in a state of chemical non-equilibrium. We also discuss the restoration of the photon distribution to an equilibrium state as the Universe enters the matter-dominated era. These findings, which suggest possible changes in the photon distribution during the epoch between the BBN and the recombination, offer insights that support the previously proposed ansatz solution to the primordial lithium problem in arXiv:1812.09472.","sentences":["We investigate the evolution of non-extensivity in the photon distribution during the Big Bang Nucleosynthesis (BBN) epoch using Tsallis statistics.","Assuming a minimal deviation from the Planck distribution, we construct the perturbed Boltzmann equation for photons, including the collision terms for pair creation and annihilation processes.","We analyze the possibility that these collisions could cause a slight increase in the number of high-frequency photons within the BBN era, and consequently, the primordial plasma might be temporarily placed in a state of chemical non-equilibrium.","We also discuss the restoration of the photon distribution to an equilibrium state as the Universe enters the matter-dominated era.","These findings, which suggest possible changes in the photon distribution during the epoch between the BBN and the recombination, offer insights that support the previously proposed ansatz solution to the primordial lithium problem in arXiv:1812.09472."],"url":"http://arxiv.org/abs/2402.13186v1","category":"astro-ph.CO"}
{"created":"2024-02-20 17:22:37","title":"Nonadiabatic Dynamics of Molecules Interacting with Metal Surfaces: An Approach Based on Langevin Dynamics and the Hierarchical Equations of Motion","abstract":"A novel mixed quantum-classical approach to simulating nonadiabatic dynamics of molecules at metal surfaces is presented. The method combines the numerically exact hierarchical equations of motion approach for the quantum electronic degrees of freedom with Langevin dynamics for the classical degrees of freedom, namely low-frequency vibrational modes within the molecule. The approach extends previous mixed quantum-classical methods based on Langevin equations to models containing strong electron-electron or quantum electronic-vibrational interactions, while maintaining a nonperturbative and non-Markovian treatment of the molecule-metal coupling. To demonstrate the approach, nonequilibrium transport observables are calculated for a molecular nanojunction containing strong interactions.","sentences":["A novel mixed quantum-classical approach to simulating nonadiabatic dynamics of molecules at metal surfaces is presented.","The method combines the numerically exact hierarchical equations of motion approach for the quantum electronic degrees of freedom with Langevin dynamics for the classical degrees of freedom, namely low-frequency vibrational modes within the molecule.","The approach extends previous mixed quantum-classical methods based on Langevin equations to models containing strong electron-electron or quantum electronic-vibrational interactions, while maintaining a nonperturbative and non-Markovian treatment of the molecule-metal coupling.","To demonstrate the approach, nonequilibrium transport observables are calculated for a molecular nanojunction containing strong interactions."],"url":"http://arxiv.org/abs/2402.13161v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 16:32:47","title":"Embedded minimal surfaces in $\\mathbb{S}^3$ and $\\mathbb{B}^3$ via equivariant eigenvalue optimization","abstract":"In 1970, Lawson solved the topological realization problem for minimal surfaces in the sphere, showing that any closed orientable surface can be minimally embedded in $\\mathbb{S}^3$. The analogous problem for surfaces with boundary was posed by Fraser and Li in 2014, and it has attracted much attention in recent years, stimulating the development of many new constructions for free boundary minimal surfaces. In this paper, we resolve this problem by showing that any compact orientable surface with boundary can be embedded in $\\mathbb{B}^3$ as a free boundary minimal surface with area below $2\\pi$. Furthermore, we show that the number of minimal surfaces in $\\mathbb{S}^3$ of prescribed topology and area below $8\\pi$, and the number of free boundary minimal surfaces in $\\mathbb{B}^3$ with prescribed topology and area below $2\\pi$, grow at least linearly with the genus. This is achieved via a new method for producing minimal surfaces of prescribed topology in low-dimensional balls and spheres, based on the optimization of Laplace and Steklov eigenvalues in the presence of a discrete symmetry group.   As a key ingredient, we develop new techniques for proving the existence of maximizing metrics, which can be used to resolve the existence problem in many symmetric situations and provide at least partial existence results for classical eigenvalue optimization problems.","sentences":["In 1970, Lawson solved the topological realization problem for minimal surfaces in the sphere, showing that any closed orientable surface can be minimally embedded in $\\mathbb{S}^3$. The analogous problem for surfaces with boundary was posed by Fraser and Li in 2014, and it has attracted much attention in recent years, stimulating the development of many new constructions for free boundary minimal surfaces.","In this paper, we resolve this problem by showing that any compact orientable surface with boundary can be embedded in $\\mathbb{B}^3$ as a free boundary minimal surface with area below $2\\pi$. Furthermore, we show that the number of minimal surfaces in $\\mathbb{S}^3$ of prescribed topology and area below $8\\pi$, and the number of free boundary minimal surfaces in $\\mathbb{B}^3$ with prescribed topology and area below $2\\pi$, grow at least linearly with the genus.","This is achieved via a new method for producing minimal surfaces of prescribed topology in low-dimensional balls and spheres, based on the optimization of Laplace and Steklov eigenvalues in the presence of a discrete symmetry group.   ","As a key ingredient, we develop new techniques for proving the existence of maximizing metrics, which can be used to resolve the existence problem in many symmetric situations and provide at least partial existence results for classical eigenvalue optimization problems."],"url":"http://arxiv.org/abs/2402.13121v1","category":"math.DG"}
{"created":"2024-02-20 14:26:09","title":"N-MPC for Deep Neural Network-Based Collision Avoidance exploiting Depth Images","abstract":"This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs. The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view. Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon. The N-MPC achieves real time control of a UAV with a control frequency of 100Hz. The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments. The associated code is released open-source along with the training images.","sentences":["This paper introduces a Nonlinear Model Predictive Control (N-MPC) framework exploiting a Deep Neural Network for processing onboard-captured depth images for collision avoidance in trajectory-tracking tasks with UAVs.","The network is trained on simulated depth images to output a collision score for queried 3D points within the sensor field of view.","Then, this network is translated into an algebraic symbolic equation and included in the N-MPC, explicitly constraining predicted positions to be collision-free throughout the receding horizon.","The N-MPC achieves real time control of a UAV with a control frequency of 100Hz.","The proposed framework is validated through statistical analysis of the collision classifier network, as well as Gazebo simulations and real experiments to assess the resulting capabilities of the N-MPC to effectively avoid collisions in cluttered environments.","The associated code is released open-source along with the training images."],"url":"http://arxiv.org/abs/2402.13038v1","category":"cs.RO"}
{"created":"2024-02-20 14:02:36","title":"Dirichlet Problems in Perforated Domains","abstract":"In this paper we establish $W^{1,p}$ estimates for solutions $u_\\varepsilon$ to Laplace's equation with the Dirichlet condition in a bounded and perforated, not necessarily periodically, $C^1$ domain $\\Omega_{\\varepsilon, \\eta}$ in $\\mathbb{R}^d$. The bounding constants depend explicitly on two small parameters $\\varepsilon$ and $\\eta$, where $\\varepsilon$ represents the scale of the minimal distance between holes, and $\\eta$ denotes the ratio between the size of the holes and $\\varepsilon$. The proof relies on a large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$, whose proof is divided into two parts. In the first part, we show that as $\\varepsilon, \\eta $ approach zero, harmonic functions in $\\Omega_{\\varepsilon, \\eta}$ may be approximated by solutions of an intermediate problem for a Schr\\\"odinger operator in $\\Omega$. In the second part, a real-variable method is employed to establish the large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$ by using the approximation at scales above $\\varepsilon$. The results are sharp except in the case $d\\ge 3$ and $p=d$ or $d^\\prime$.","sentences":["In this paper we establish $W^{1,p}$ estimates for solutions $u_\\varepsilon$ to Laplace's equation with the Dirichlet condition in a bounded and perforated, not necessarily periodically, $C^1$ domain $\\Omega_{\\varepsilon, \\eta}$ in $\\mathbb{R}^d$. The bounding constants depend explicitly on two small parameters $\\varepsilon$ and $\\eta$, where $\\varepsilon$ represents the scale of the minimal distance between holes, and $\\eta$ denotes the ratio between the size of the holes and $\\varepsilon$. The proof relies on a large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$, whose proof is divided into two parts.","In the first part, we show that as $\\varepsilon, \\eta $ approach zero, harmonic functions in $\\Omega_{\\varepsilon, \\eta}$ may be approximated by solutions of an intermediate problem for a Schr\\\"odinger operator in $\\Omega$. In the second part, a real-variable method is employed to establish the large-scale $L^p$ estimate for $\\nabla u_\\varepsilon$ by using the approximation at scales above $\\varepsilon$. The results are sharp except in the case $d\\ge 3$ and $p=d$ or $d^\\prime$."],"url":"http://arxiv.org/abs/2402.13021v1","category":"math.AP"}
{"created":"2024-02-20 13:55:53","title":"Asymptotic behavior of the indicator function in the inverse problem of the wave equation for media with multiple types of cavities","abstract":"In this paper, the inverse problem of the wave equation by the enclosure method for a medium with multiple types of cavities is discussed. In the case considered here, the sign of the indicator function of the enclosure method is not determined and sign cancellation may occur, resulting in loss of information. By examining the top terms of the indicator function in detail, we show that the shortest distance to the cavities can be obtained even in such a case.","sentences":["In this paper, the inverse problem of the wave equation by the enclosure method for a medium with multiple types of cavities is discussed.","In the case considered here, the sign of the indicator function of the enclosure method is not determined and sign cancellation may occur, resulting in loss of information.","By examining the top terms of the indicator function in detail, we show that the shortest distance to the cavities can be obtained even in such a case."],"url":"http://arxiv.org/abs/2402.13012v1","category":"math.AP"}
{"created":"2024-02-20 13:32:00","title":"A unifying primary framework for quantum graph neural networks from quantum graph states","abstract":"Graph states are used to represent mathematical graphs as quantum states on quantum computers. They can be formulated through stabilizer codes or directly quantum gates and quantum states. In this paper we show that a quantum graph neural network model can be understood and realized based on graph states. We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers.","sentences":["Graph states are used to represent mathematical graphs as quantum states on quantum computers.","They can be formulated through stabilizer codes or directly quantum gates and quantum states.","In this paper we show that a quantum graph neural network model can be understood and realized based on graph states.","We show that they can be used either as a parameterized quantum circuits to represent neural networks or as an underlying structure to construct graph neural networks on quantum computers."],"url":"http://arxiv.org/abs/2402.13001v1","category":"quant-ph"}
{"created":"2024-02-20 13:14:02","title":"L\u00fcscher equation with long-range forces","abstract":"We derive the modified L\\\"uscher equation in the presence of the long-range force caused by the exchange of a light particle. It is shown that the use of this equation enables one to circumvent the problems related to the strong partial-wave mixing and the t-channel sub-threshold singularities. It is also demonstrated that the present method is intrinsically linked to the so-called modified effective-range expansion (MERE) in the infinite volume. A detailed comparison with the two recently proposed alternative approaches is provided.","sentences":["We derive the modified L\\\"uscher equation in the presence of the long-range force caused by the exchange of a light particle.","It is shown that the use of this equation enables one to circumvent the problems related to the strong partial-wave mixing and the t-channel sub-threshold singularities.","It is also demonstrated that the present method is intrinsically linked to the so-called modified effective-range expansion (MERE) in the infinite volume.","A detailed comparison with the two recently proposed alternative approaches is provided."],"url":"http://arxiv.org/abs/2402.12985v1","category":"hep-lat"}
{"created":"2024-02-20 13:00:38","title":"Minimisation of peak stresses with the shape derivative","abstract":"This paper is concerned with the minimisation of peak stresses occurring in linear elasticity. We propose to minimise the maximal von Mises stress of the elastic body. This leads to a nonsmooth shape functional. We derive the shape derivative and associate it with the Clarke sub-differential. Using a steepest descent algorithm we present numerical simulations. We compare our results to the usual $p$-norm regularisation and show that our algorithm performs better in the presented tests.","sentences":["This paper is concerned with the minimisation of peak stresses occurring in linear elasticity.","We propose to minimise the maximal von Mises stress of the elastic body.","This leads to a nonsmooth shape functional.","We derive the shape derivative and associate it with the Clarke sub-differential.","Using a steepest descent algorithm we present numerical simulations.","We compare our results to the usual $p$-norm regularisation and show that our algorithm performs better in the presented tests."],"url":"http://arxiv.org/abs/2402.12978v1","category":"math.OC"}
{"created":"2024-02-20 12:35:14","title":"Quantifying Privacy via Information Density","abstract":"We examine the relationship between privacy metrics that utilize information density to measure information leakage between a private and a disclosed random variable. Firstly, we prove that bounding the information density from above or below in turn implies a lower or upper bound on the information density, respectively. Using this result, we establish new relationships between local information privacy, asymmetric local information privacy, pointwise maximal leakage and local differential privacy. We further provide applications of these relations to privacy mechanism design. Furthermore, we provide statements showing the equivalence between a lower bound on information density and risk-averse adversaries. More specifically, we prove an equivalence between a guessing framework and a cost-function framework that result in the desired lower bound on the information density.","sentences":["We examine the relationship between privacy metrics that utilize information density to measure information leakage between a private and a disclosed random variable.","Firstly, we prove that bounding the information density from above or below in turn implies a lower or upper bound on the information density, respectively.","Using this result, we establish new relationships between local information privacy, asymmetric local information privacy, pointwise maximal leakage and local differential privacy.","We further provide applications of these relations to privacy mechanism design.","Furthermore, we provide statements showing the equivalence between a lower bound on information density and risk-averse adversaries.","More specifically, we prove an equivalence between a guessing framework and a cost-function framework that result in the desired lower bound on the information density."],"url":"http://arxiv.org/abs/2402.12967v1","category":"cs.IT"}
{"created":"2024-02-20 12:13:35","title":"Spectral collocation for functional and delay differential equations","abstract":"A framework for Chebyshev spectral collocation methods for the numerical solution of functional and delay differential equations (FDEs and DDEs) is described. The framework combines interpolation via the barycentric resampling matrix with a multidomain approach used to resolve isolated discontinuities propagated by non-smooth initial data. Geometric convergence is demonstrated for several examples of linear and nonlinear FDEs and DDEs with various delay types, including discrete, proportional, continuous, and state-dependent delay. The framework is a natural extension of standard spectral collocation methods based on polynomial interpolants and can be readily incorporated into existing spectral discretisations, such as in Chebfun/Chebop, allowing the automated and efficient solution of a wide class of nonlinear functional and delay differential equations.","sentences":["A framework for Chebyshev spectral collocation methods for the numerical solution of functional and delay differential equations (FDEs and DDEs) is described.","The framework combines interpolation via the barycentric resampling matrix with a multidomain approach used to resolve isolated discontinuities propagated by non-smooth initial data.","Geometric convergence is demonstrated for several examples of linear and nonlinear FDEs and DDEs with various delay types, including discrete, proportional, continuous, and state-dependent delay.","The framework is a natural extension of standard spectral collocation methods based on polynomial interpolants and can be readily incorporated into existing spectral discretisations, such as in Chebfun/Chebop, allowing the automated and efficient solution of a wide class of nonlinear functional and delay differential equations."],"url":"http://arxiv.org/abs/2402.12952v1","category":"math.NA"}
{"created":"2024-02-20 12:01:30","title":"Cell Graph Transformer for Nuclei Classification","abstract":"Nuclei classification is a critical step in computer-aided diagnosis with histopathology images. In the past, various methods have employed graph neural networks (GNN) to analyze cell graphs that model inter-cell relationships by considering nuclei as vertices. However, they are limited by the GNN mechanism that only passes messages among local nodes via fixed edges. To address the issue, we develop a cell graph transformer (CGT) that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes. Nevertheless, training the transformer with a cell graph presents another challenge. Poorly initialized features can lead to noisy self-attention scores and inferior convergence, particularly when processing the cell graphs with numerous connections. Thus, we further propose a novel topology-aware pretraining method that leverages a graph convolutional network (GCN) to learn a feature extractor. The pre-trained features may suppress unreasonable correlations and hence ease the finetuning of CGT. Experimental results suggest that the proposed cell graph transformer with topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance. Code and models are available at https://github.com/lhaof/CGT","sentences":["Nuclei classification is a critical step in computer-aided diagnosis with histopathology images.","In the past, various methods have employed graph neural networks (GNN) to analyze cell graphs that model inter-cell relationships by considering nuclei as vertices.","However, they are limited by the GNN mechanism that only passes messages among local nodes via fixed edges.","To address the issue, we develop a cell graph transformer (CGT) that treats nodes and edges as input tokens to enable learnable adjacency and information exchange among all nodes.","Nevertheless, training the transformer with a cell graph presents another challenge.","Poorly initialized features can lead to noisy self-attention scores and inferior convergence, particularly when processing the cell graphs with numerous connections.","Thus, we further propose a novel topology-aware pretraining method that leverages a graph convolutional network (GCN) to learn a feature extractor.","The pre-trained features may suppress unreasonable correlations and hence ease the finetuning of CGT.","Experimental results suggest that the proposed cell graph transformer with topology-aware pretraining significantly improves the nuclei classification results, and achieves the state-of-the-art performance.","Code and models are available at https://github.com/lhaof/CGT"],"url":"http://arxiv.org/abs/2402.12946v1","category":"cs.CV"}
{"created":"2024-02-20 12:00:25","title":"Stochastic Approximation Approach to Federated Machine Learning","abstract":"This paper examines Federated learning (FL) in a Stochastic Approximation (SA) framework. FL is a collaborative way to train neural network models across various participants or clients without centralizing their data. Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation. The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training. SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function. In this paper the clients use a stochastic approximation iterate to update the weights of its neural network. It is shown that the aggregated weights track an autonomous ODE. Numerical simulations are performed and the results are compared with standard algorithms like FedAvg and FedProx. It is observed that the proposed algorithm is robust and gives more reliable estimates of the weights, in particular when the clients data are not identically distributed.","sentences":["This paper examines Federated learning (FL) in a Stochastic Approximation (SA) framework.","FL is a collaborative way to train neural network models across various participants or clients without centralizing their data.","Each client will train a model on their respective data and send the weights across to a the server periodically for aggregation.","The server aggregates these weights which are then used by the clients to re-initialize their neural network and continue the training.","SA is an iterative algorithm that uses approximate sample gradients and tapering step size to locate a minimizer of a cost function.","In this paper the clients use a stochastic approximation iterate to update the weights of its neural network.","It is shown that the aggregated weights track an autonomous ODE.","Numerical simulations are performed and the results are compared with standard algorithms like FedAvg and FedProx.","It is observed that the proposed algorithm is robust and gives more reliable estimates of the weights, in particular when the clients data are not identically distributed."],"url":"http://arxiv.org/abs/2402.12945v1","category":"cs.LG"}
{"created":"2024-02-20 11:29:57","title":"Learning Exceptional Subgroups by End-to-End Maximizing KL-divergence","abstract":"Finding and describing sub-populations that are exceptional regarding a target property has important applications in many scientific disciplines, from identifying disadvantaged demographic groups in census data to finding conductive molecules within gold nanoparticles. Current approaches to finding such subgroups require pre-discretized predictive variables, do not permit non-trivial target distributions, do not scale to large datasets, and struggle to find diverse results.   To address these limitations, we propose Syflow, an end-to-end optimizable approach in which we leverage normalizing flows to model arbitrary target distributions, and introduce a novel neural layer that results in easily interpretable subgroup descriptions. We demonstrate on synthetic and real-world data, including a case study, that Syflow reliably finds highly exceptional subgroups accompanied by insightful descriptions.","sentences":["Finding and describing sub-populations that are exceptional regarding a target property has important applications in many scientific disciplines, from identifying disadvantaged demographic groups in census data to finding conductive molecules within gold nanoparticles.","Current approaches to finding such subgroups require pre-discretized predictive variables, do not permit non-trivial target distributions, do not scale to large datasets, and struggle to find diverse results.   ","To address these limitations, we propose Syflow, an end-to-end optimizable approach in which we leverage normalizing flows to model arbitrary target distributions, and introduce a novel neural layer that results in easily interpretable subgroup descriptions.","We demonstrate on synthetic and real-world data, including a case study, that Syflow reliably finds highly exceptional subgroups accompanied by insightful descriptions."],"url":"http://arxiv.org/abs/2402.12930v1","category":"cs.LG"}
{"created":"2024-02-20 10:52:23","title":"Fog enabled distributed training architecture for federated learning","abstract":"The amount of data being produced at every epoch of second is increasing every moment. Various sensors, cameras and smart gadgets produce continuous data throughout its installation. Processing and analyzing raw data at a cloud server faces several challenges such as bandwidth, congestion, latency, privacy and security. Fog computing brings computational resources closer to IoT that addresses some of these issues. These IoT devices have low computational capability, which is insufficient to train machine learning. Mining hidden patterns and inferential rules from continuously growing data is crucial for various applications. Due to growing privacy concerns, privacy preserving machine learning is another aspect that needs to be inculcated. In this paper, we have proposed a fog enabled distributed training architecture for machine learning tasks using resources constrained devices. The proposed architecture trains machine learning model on rapidly changing data using online learning. The network is inlined with privacy preserving federated learning training. Further, the learning capability of architecture is tested on a real world IIoT use case. We trained a neural network model for human position detection in IIoT setup on rapidly changing data.","sentences":["The amount of data being produced at every epoch of second is increasing every moment.","Various sensors, cameras and smart gadgets produce continuous data throughout its installation.","Processing and analyzing raw data at a cloud server faces several challenges such as bandwidth, congestion, latency, privacy and security.","Fog computing brings computational resources closer to IoT that addresses some of these issues.","These IoT devices have low computational capability, which is insufficient to train machine learning.","Mining hidden patterns and inferential rules from continuously growing data is crucial for various applications.","Due to growing privacy concerns, privacy preserving machine learning is another aspect that needs to be inculcated.","In this paper, we have proposed a fog enabled distributed training architecture for machine learning tasks using resources constrained devices.","The proposed architecture trains machine learning model on rapidly changing data using online learning.","The network is inlined with privacy preserving federated learning training.","Further, the learning capability of architecture is tested on a real world IIoT use case.","We trained a neural network model for human position detection in IIoT setup on rapidly changing data."],"url":"http://arxiv.org/abs/2402.12906v1","category":"cs.DC"}
{"created":"2024-02-20 10:45:46","title":"Explicit formula for the Benjamin--Ono equation with square integrable and real valued initial data and applications to the zero dispersion limit","abstract":"In this paper, we extend G{\\'e}rard's formula for the solution of the Benjamin--Ono equation on the line to square integrable and real valued initial data. Combined with this formula, we also extend the G{\\'e}rard's formula for the zero dispersion limit of the Benjamin--Ono equation on the line to more singular initial data. In the derivation of the extension of the formula for the zero dispersion limit, we also find an interesting integral equality, which might be useful in other contexts.","sentences":["In this paper, we extend G{\\'e}rard's formula for the solution of the Benjamin--Ono equation on the line to square integrable and real valued initial data.","Combined with this formula, we also extend the G{\\'e}rard's formula for the zero dispersion limit of the Benjamin--Ono equation on the line to more singular initial data.","In the derivation of the extension of the formula for the zero dispersion limit, we also find an interesting integral equality, which might be useful in other contexts."],"url":"http://arxiv.org/abs/2402.12898v1","category":"math.AP"}
{"created":"2024-02-20 10:06:30","title":"Deep, convergent, unrolled half-quadratic splitting for image deconvolution","abstract":"In recent years, algorithm unrolling has emerged as a powerful technique for designing interpretable neural networks based on iterative algorithms. Imaging inverse problems have particularly benefited from unrolling-based deep network design since many traditional model-based approaches rely on iterative optimization. Despite exciting progress, typical unrolling approaches heuristically design layer-specific convolution weights to improve performance. Crucially, convergence properties of the underlying iterative algorithm are lost once layer-specific parameters are learned from training data. We propose an unrolling technique that breaks the trade-off between retaining algorithm properties while simultaneously enhancing performance. We focus on image deblurring and unrolling the widely-applied Half-Quadratic Splitting (HQS) algorithm. We develop a new parametrization scheme which enforces layer-specific parameters to asymptotically approach certain fixed points. Through extensive experimental studies, we verify that our approach achieves competitive performance with state-of-the-art unrolled layer-specific learning and significantly improves over the traditional HQS algorithm. We further establish convergence of the proposed unrolled network as the number of layers approaches infinity, and characterize its convergence rate. Our experimental verification involves simulations that validate the analytical results as well as comparison with state-of-the-art non-blind deblurring techniques on benchmark datasets. The merits of the proposed convergent unrolled network are established over competing alternatives, especially in the regime of limited training.","sentences":["In recent years, algorithm unrolling has emerged as a powerful technique for designing interpretable neural networks based on iterative algorithms.","Imaging inverse problems have particularly benefited from unrolling-based deep network design since many traditional model-based approaches rely on iterative optimization.","Despite exciting progress, typical unrolling approaches heuristically design layer-specific convolution weights to improve performance.","Crucially, convergence properties of the underlying iterative algorithm are lost once layer-specific parameters are learned from training data.","We propose an unrolling technique that breaks the trade-off between retaining algorithm properties while simultaneously enhancing performance.","We focus on image deblurring and unrolling the widely-applied Half-Quadratic Splitting (HQS) algorithm.","We develop a new parametrization scheme which enforces layer-specific parameters to asymptotically approach certain fixed points.","Through extensive experimental studies, we verify that our approach achieves competitive performance with state-of-the-art unrolled layer-specific learning and significantly improves over the traditional HQS algorithm.","We further establish convergence of the proposed unrolled network as the number of layers approaches infinity, and characterize its convergence rate.","Our experimental verification involves simulations that validate the analytical results as well as comparison with state-of-the-art non-blind deblurring techniques on benchmark datasets.","The merits of the proposed convergent unrolled network are established over competing alternatives, especially in the regime of limited training."],"url":"http://arxiv.org/abs/2402.12872v1","category":"eess.IV"}
{"created":"2024-02-20 08:57:15","title":"Hidden-flavour four-quark states in the charm and bottom region","abstract":"We discuss the spectrum and the internal composition of ground and excited four-quark states in the charm and bottom energy region. To this end we extend previous calculations within the framework of the relativistic four-body Faddeev-Yakubovsky equation to include quantum numbers with $J^{P C} = 0^{++} , 0^{-+} , 1^{--} , 1^{+-}$ and $1^{+ +}$ and study their internal composition in terms of heavy-light meson pairs, hadroquarkonia and diquark-antidiquark clusters. We observe similar patterns in the charm and bottom energy region with different compositions of the four-quark states depending on $J^{P C}$ quantum numbers. Most notably, we find that all states with $C \\cdot P = +1$ are dominated by heavy-light meson contributions, whereas for axialvector states with $J^{P C} = 1^{+-}$ including the $Z_c (3900)$ we find a much more complicated picture depending on the flavour content. We systematically compare our results for the spectrum with existing experimental results and provide predictions for future analyses.","sentences":["We discuss the spectrum and the internal composition of ground and excited four-quark states in the charm and bottom energy region.","To this end we extend previous calculations within the framework of the relativistic four-body Faddeev-Yakubovsky equation to include quantum numbers with $J^{P C} = 0^{++} , 0^{-+} , 1^{--} , 1^{+-}$ and $1^{+ +}$ and study their internal composition in terms of heavy-light meson pairs, hadroquarkonia and diquark-antidiquark clusters.","We observe similar patterns in the charm and bottom energy region with different compositions of the four-quark states depending on $J^{P C}$ quantum numbers.","Most notably, we find that all states with $C \\cdot P = +1$ are dominated by heavy-light meson contributions, whereas for axialvector states with $J^{P C} = 1^{+-}$ including the $Z_c (3900)$ we find a much more complicated picture depending on the flavour content.","We systematically compare our results for the spectrum with existing experimental results and provide predictions for future analyses."],"url":"http://arxiv.org/abs/2402.12830v1","category":"hep-ph"}
{"created":"2024-02-20 08:19:30","title":"Radar-Based Recognition of Static Hand Gestures in American Sign Language","abstract":"In the fast-paced field of human-computer interaction (HCI) and virtual reality (VR), automatic gesture recognition has become increasingly essential. This is particularly true for the recognition of hand signs, providing an intuitive way to effortlessly navigate and control VR and HCI applications. Considering increased privacy requirements, radar sensors emerge as a compelling alternative to cameras. They operate effectively in low-light conditions without capturing identifiable human details, thanks to their lower resolution and distinct wavelength compared to visible light.   While previous works predominantly deploy radar sensors for dynamic hand gesture recognition based on Doppler information, our approach prioritizes classification using an imaging radar that operates on spatial information, e.g. image-like data. However, generating large training datasets required for neural networks (NN) is a time-consuming and challenging process, often falling short of covering all potential scenarios. Acknowledging these challenges, this study explores the efficacy of synthetic data generated by an advanced radar ray-tracing simulator. This simulator employs an intuitive material model that can be adjusted to introduce data diversity.   Despite exclusively training the NN on synthetic data, it demonstrates promising performance when put to the test with real measurement data. This emphasizes the practicality of our methodology in overcoming data scarcity challenges and advancing the field of automatic gesture recognition in VR and HCI applications.","sentences":["In the fast-paced field of human-computer interaction (HCI) and virtual reality (VR), automatic gesture recognition has become increasingly essential.","This is particularly true for the recognition of hand signs, providing an intuitive way to effortlessly navigate and control VR and HCI applications.","Considering increased privacy requirements, radar sensors emerge as a compelling alternative to cameras.","They operate effectively in low-light conditions without capturing identifiable human details, thanks to their lower resolution and distinct wavelength compared to visible light.   ","While previous works predominantly deploy radar sensors for dynamic hand gesture recognition based on Doppler information, our approach prioritizes classification using an imaging radar that operates on spatial information, e.g. image-like data.","However, generating large training datasets required for neural networks (NN) is a time-consuming and challenging process, often falling short of covering all potential scenarios.","Acknowledging these challenges, this study explores the efficacy of synthetic data generated by an advanced radar ray-tracing simulator.","This simulator employs an intuitive material model that can be adjusted to introduce data diversity.   ","Despite exclusively training the NN on synthetic data, it demonstrates promising performance when put to the test with real measurement data.","This emphasizes the practicality of our methodology in overcoming data scarcity challenges and advancing the field of automatic gesture recognition in VR and HCI applications."],"url":"http://arxiv.org/abs/2402.12800v1","category":"cs.CV"}
{"created":"2024-02-20 08:17:32","title":"Absence of small magic angles for disordered tunneling potentials in twisted bilayer graphene","abstract":"We consider small random perturbations of the standard high-symmetry tunneling potentials in the Bistritzer-MacDonald Hamiltonian describing twisted bilayer graphene. Using methods developed by Sj\\\"ostrand for studying the spectral asymptotics of non-selfadjoint pseudo-differential operators, we prove that for sufficiently small twisting angles the Hamiltonian will not exhibit a flat band with overwhelming probability, and hence the absence of the so-called \\textit{magic angels}. Moreover, we prove a probabilistic Weyl law for the eigenvalues of the non-selfadjoint tunneling operator, subject to small random perturbations, of the Bistritzer-MacDonald Hamiltonian in the chiral limit.","sentences":["We consider small random perturbations of the standard high-symmetry tunneling potentials in the Bistritzer-MacDonald Hamiltonian describing twisted bilayer graphene.","Using methods developed by Sj\\\"ostrand for studying the spectral asymptotics of non-selfadjoint pseudo-differential operators, we prove that for sufficiently small twisting angles the Hamiltonian will not exhibit a flat band with overwhelming probability, and hence the absence of the so-called \\textit{magic angels}.","Moreover, we prove a probabilistic Weyl law for the eigenvalues of the non-selfadjoint tunneling operator, subject to small random perturbations, of the Bistritzer-MacDonald Hamiltonian in the chiral limit."],"url":"http://arxiv.org/abs/2402.12799v1","category":"math-ph"}
{"created":"2024-02-20 08:14:53","title":"A Geometric Algorithm for Tubular Shape Reconstruction from Skeletal Representation","abstract":"We introduce a novel approach for the reconstruction of tubular shapes from skeletal representations. Our method processes all skeletal points as a whole, eliminating the need for splitting input structure into multiple segments. We represent the tubular shape as a truncated signed distance function (TSDF) in a voxel hashing manner, in which the signed distance between a voxel center and the object is computed through a simple geometric algorithm. Our method does not involve any surface sampling scheme or solving large matrix equations, and therefore is a faster and more elegant solution for tubular shape reconstruction compared to other approaches. Experiments demonstrate the efficiency and effectiveness of the proposed method. Code is avaliable at https://github.com/wlsdzyzl/Dragon.","sentences":["We introduce a novel approach for the reconstruction of tubular shapes from skeletal representations.","Our method processes all skeletal points as a whole, eliminating the need for splitting input structure into multiple segments.","We represent the tubular shape as a truncated signed distance function (TSDF) in a voxel hashing manner, in which the signed distance between a voxel center and the object is computed through a simple geometric algorithm.","Our method does not involve any surface sampling scheme or solving large matrix equations, and therefore is a faster and more elegant solution for tubular shape reconstruction compared to other approaches.","Experiments demonstrate the efficiency and effectiveness of the proposed method.","Code is avaliable at https://github.com/wlsdzyzl/Dragon."],"url":"http://arxiv.org/abs/2402.12797v1","category":"cs.CV"}
{"created":"2024-02-20 08:04:12","title":"OccFlowNet: Towards Self-supervised Occupancy Estimation via Differentiable Rendering and Occupancy Flow","abstract":"Semantic occupancy has recently gained significant traction as a prominent 3D scene representation. However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain. In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire. In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only. To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps. Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency. Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches. When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly. We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain.","sentences":["Semantic occupancy has recently gained significant traction as a prominent 3D scene representation.","However, most existing methods rely on large and costly datasets with fine-grained 3D voxel labels for training, which limits their practicality and scalability, increasing the need for self-monitored learning in this domain.","In this work, we present a novel approach to occupancy estimation inspired by neural radiance field (NeRF) using only 2D labels, which are considerably easier to acquire.","In particular, we employ differentiable volumetric rendering to predict depth and semantic maps and train a 3D network based on 2D supervision only.","To enhance geometric accuracy and increase the supervisory signal, we introduce temporal rendering of adjacent time steps.","Additionally, we introduce occupancy flow as a mechanism to handle dynamic objects in the scene and ensure their temporal consistency.","Through extensive experimentation we demonstrate that 2D supervision only is sufficient to achieve state-of-the-art performance compared to methods using 3D labels, while outperforming concurrent 2D approaches.","When combining 2D supervision with 3D labels, temporal rendering and occupancy flow we outperform all previous occupancy estimation models significantly.","We conclude that the proposed rendering supervision and occupancy flow advances occupancy estimation and further bridges the gap towards self-supervised learning in this domain."],"url":"http://arxiv.org/abs/2402.12792v1","category":"cs.CV"}
{"created":"2024-02-20 07:29:30","title":"Propagation of dark solitons of DNLS equation along a large-scale background","abstract":"We study dynamics of dark solitons in the theory of the DNLS equation by the method based on imposing the condition that this dynamics must be Hamiltonian. Combining this condition with Stokes' remark that relationships for harmonic linear waves and small-amplitude soliton tails satisfy the same linearized equations, so the corresponding solutions can be converted one into the other by replacement of the packet's wave number $k$ by $i\\kappa$, $\\kappa$ being the soliton's inverse half-width, we find the Hamiltonian and the canonical momentum of the soliton's motion. The Hamilton equations are reduced to the Newton equation whose solutions for some typical situations are compared with exact numerical solutions of the DNLS equation.","sentences":["We study dynamics of dark solitons in the theory of the DNLS equation by the method based on imposing the condition that this dynamics must be Hamiltonian.","Combining this condition with Stokes' remark that relationships for harmonic linear waves and small-amplitude soliton tails satisfy the same linearized equations, so the corresponding solutions can be converted one into the other by replacement of the packet's wave number $k$ by $i\\kappa$, $\\kappa$ being the soliton's inverse half-width, we find the Hamiltonian and the canonical momentum of the soliton's motion.","The Hamilton equations are reduced to the Newton equation whose solutions for some typical situations are compared with exact numerical solutions of the DNLS equation."],"url":"http://arxiv.org/abs/2402.12776v1","category":"nlin.PS"}
{"created":"2024-02-20 07:23:06","title":"Uniqueness and minimality of Euler's elastica with monotone curvature","abstract":"For an old problem of Euler's elastica we prove the novel global property that every planar elastica with non-constant monotone curvature is uniquely minimal subject to the clamped boundary condition. We also partly extend this unique minimality to the length-penalised case; this result is new even in view of local minimality. As an application we prove uniqueness of global minimisers in the straightening problem for generic boundary angles.","sentences":["For an old problem of Euler's elastica we prove the novel global property that every planar elastica with non-constant monotone curvature is uniquely minimal subject to the clamped boundary condition.","We also partly extend this unique minimality to the length-penalised case; this result is new even in view of local minimality.","As an application we prove uniqueness of global minimisers in the straightening problem for generic boundary angles."],"url":"http://arxiv.org/abs/2402.12771v1","category":"math.AP"}
{"created":"2024-02-20 07:12:58","title":"Lattice deformation induced by hot carriers in graphene","abstract":"Hot electrons formed in a graphene crystal by high-intensity short-duration laser pulses can exist for a time that is less than an electron-phonon energy relaxation time. During that time, electron-electron collisions cause the electrons to thermalize to a local effective temperature that propagates (diffuses) through graphene. The non-uniform nature of the electron temperature leads to a force acting on the graphene lattice. This force is the result of the electron-lattice interaction that exists even at times that are less than the electron-phonon scattering time. The force causes the lattice to deform. A Boltzmann equation description of the transient electron-lattice deformation causes by hot electrons in graphene is presented.","sentences":["Hot electrons formed in a graphene crystal by high-intensity short-duration laser pulses can exist for a time that is less than an electron-phonon energy relaxation time.","During that time, electron-electron collisions cause the electrons to thermalize to a local effective temperature that propagates (diffuses) through graphene.","The non-uniform nature of the electron temperature leads to a force acting on the graphene lattice.","This force is the result of the electron-lattice interaction that exists even at times that are less than the electron-phonon scattering time.","The force causes the lattice to deform.","A Boltzmann equation description of the transient electron-lattice deformation causes by hot electrons in graphene is presented."],"url":"http://arxiv.org/abs/2402.12766v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-20 06:42:48","title":"Critical Allard regularity: pointwise tilt-excess estimates","abstract":"The main results of this paper provide VMO-type estimates for the quadratic tilt-excess on varifolds with critical generalized mean curvature. These estimates apply to varifolds with \"almost-integral\" density which are close to a multiplicity one $m$-disc in a ball in the usual senses. The class of almost-integral varifolds allows for varifolds with non-perpendicular mean curvature. Moreover, the estimates hold \\emph{uniformly for every point} in a relatively open set in $\\text{spt}||V||$ and naturally imply a Reifenberg-type parametrization. The proof relies upon generalizing the $Q$-valued Lipschitz approximation and Sobolev-Poincar\\'e estimates of arXiv:0808.3660 to almost-integral rectifiable varifolds.","sentences":["The main results of this paper provide VMO-type estimates for the quadratic tilt-excess on varifolds with critical generalized mean curvature.","These estimates apply to varifolds with \"almost-integral\" density which are close to a multiplicity one $m$-disc in a ball in the usual senses.","The class of almost-integral varifolds allows for varifolds with non-perpendicular mean curvature.","Moreover, the estimates hold \\emph{uniformly for every point} in a relatively open set in $\\text{spt}||V||$ and naturally imply a Reifenberg-type parametrization.","The proof relies upon generalizing the $Q$-valued Lipschitz approximation and Sobolev-Poincar\\'e estimates of arXiv:0808.3660 to almost-integral rectifiable varifolds."],"url":"http://arxiv.org/abs/2402.12752v1","category":"math.DG"}
{"created":"2024-02-20 04:25:57","title":"MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for Single or Sparse-view 3D Object Reconstruction","abstract":"This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses. MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time. We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts. We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model.","sentences":["This paper presents a neural architecture MVDiffusion++ for 3D object reconstruction that synthesizes dense and high-resolution views of an object given one or a few images without camera poses.","MVDiffusion++ achieves superior flexibility and scalability with two surprisingly simple ideas: 1) A ``pose-free architecture'' where standard self-attention among 2D latent features learns 3D consistency across an arbitrary number of conditional and generation views without explicitly using camera pose information; and 2) A ``view dropout strategy'' that discards a substantial number of output views during training, which reduces the training-time memory footprint and enables dense and high-resolution view synthesis at test time.","We use the Objaverse for training and the Google Scanned Objects for evaluation with standard novel view synthesis and 3D reconstruction metrics, where MVDiffusion++ significantly outperforms the current state of the arts.","We also demonstrate a text-to-3D application example by combining MVDiffusion++ with a text-to-image generative model."],"url":"http://arxiv.org/abs/2402.12712v1","category":"cs.CV"}
{"created":"2024-02-20 03:59:52","title":"Quantitative uniqueness estimates for stochastic parabolic equations on the whole Euclidean space","abstract":"In this paper, a quantitative estimate of unique continuation for the stochastic heat equation with bounded potentials on the whole Euclidean space is established. This paper generalizes the earlier results in [29] and [17] from a bounded domain to an unbounded one. The proof is based on the locally parabolic-type frequency function method. An observability estimate from measurable sets in time for the same equation is also derived.","sentences":["In this paper, a quantitative estimate of unique continuation for the stochastic heat equation with bounded potentials on the whole Euclidean space is established.","This paper generalizes the earlier results in [29] and [17] from a bounded domain to an unbounded one.","The proof is based on the locally parabolic-type frequency function method.","An observability estimate from measurable sets in time for the same equation is also derived."],"url":"http://arxiv.org/abs/2402.12703v1","category":"math.AP"}
{"created":"2024-02-20 03:09:10","title":"Motion of Spinning Density Tensors in a Clifford Space","abstract":"A Clifford Space is counted to be a tempting approach to unify both micro-physics and macro-physics simultaneously. Such a tendency may be found in the realm of replacing vectors with poly-vectors. Accordingly, the problem of motion becomes essential to express the motion of extended particles rather than test particles. These equations are performed by using an equivalent Bazanski Lagrangian in a Clifford space. From this perspective, a generalized type an equation for spinning density tensors and spinning density deviation tensors are obtained. Spinning deviation tensors in a Clifford space may give a better performance to examine the problem of stability for spinning density tensors as expressed in terms of vectors defined in such a class of Riemannian geometry.","sentences":["A Clifford Space is counted to be a tempting approach to unify both micro-physics and macro-physics simultaneously.","Such a tendency may be found in the realm of replacing vectors with poly-vectors.","Accordingly, the problem of motion becomes essential to express the motion of extended particles rather than test particles.","These equations are performed by using an equivalent Bazanski Lagrangian in a Clifford space.","From this perspective, a generalized type an equation for spinning density tensors and spinning density deviation tensors are obtained.","Spinning deviation tensors in a Clifford space may give a better performance to examine the problem of stability for spinning density tensors as expressed in terms of vectors defined in such a class of Riemannian geometry."],"url":"http://arxiv.org/abs/2402.12681v1","category":"physics.gen-ph"}
{"created":"2024-02-20 02:46:32","title":"Iterated learning and multiscale modeling of history-dependent architectured metamaterials","abstract":"Neural network based models have emerged as a powerful tool in multiscale modeling of materials. One promising approach is to use a neural network based model, trained using data generated from repeated solution of an expensive small scale model, as a surrogate for the small scale model in application scale simulations. Such approaches have been shown to have the potential accuracy of concurrent multiscale methods like FE2, but at the cost comparable to empirical methods like classical constitutive models or parameter passing. A key question is to understand how much and what kind of data is necessary to obtain an accurate surrogate. This paper examines this question for history dependent elastic-plastic behavior of an architected metamaterial modeled as a truss. We introduce an iterative approach where we use the rich arbitrary class of trajectories to train an initial model, but then iteratively update the class of trajectories with those that arise in large scale simulation and use transfer learning to update the model. We show that such an approach converges to a highly accurate surrogate, and one that is transferable.","sentences":["Neural network based models have emerged as a powerful tool in multiscale modeling of materials.","One promising approach is to use a neural network based model, trained using data generated from repeated solution of an expensive small scale model, as a surrogate for the small scale model in application scale simulations.","Such approaches have been shown to have the potential accuracy of concurrent multiscale methods like FE2, but at the cost comparable to empirical methods like classical constitutive models or parameter passing.","A key question is to understand how much and what kind of data is necessary to obtain an accurate surrogate.","This paper examines this question for history dependent elastic-plastic behavior of an architected metamaterial modeled as a truss.","We introduce an iterative approach where we use the rich arbitrary class of trajectories to train an initial model, but then iteratively update the class of trajectories with those that arise in large scale simulation and use transfer learning to update the model.","We show that such an approach converges to a highly accurate surrogate, and one that is transferable."],"url":"http://arxiv.org/abs/2402.12674v1","category":"physics.app-ph"}
{"created":"2024-02-20 02:02:29","title":"PDEformer: Towards a Foundation Model for One-Dimensional Partial Differential Equations","abstract":"This paper introduces PDEformer, a neural solver for partial differential equations (PDEs) capable of simultaneously addressing various types of PDEs. We advocate representing the PDE in the form of a computational graph, facilitating the seamless integration of both symbolic and numerical information inherent in a PDE. A graph Transformer and an implicit neural representation (INR) are employed to generate mesh-free predicted solutions. Following pretraining on data exhibiting a certain level of diversity, our model achieves zero-shot accuracies on benchmark datasets that surpass those of adequately trained expert models. Additionally, PDEformer demonstrates promising results in the inverse problem of PDE coefficient recovery.","sentences":["This paper introduces PDEformer, a neural solver for partial differential equations (PDEs) capable of simultaneously addressing various types of PDEs.","We advocate representing the PDE in the form of a computational graph, facilitating the seamless integration of both symbolic and numerical information inherent in a PDE.","A graph Transformer and an implicit neural representation (INR) are employed to generate mesh-free predicted solutions.","Following pretraining on data exhibiting a certain level of diversity, our model achieves zero-shot accuracies on benchmark datasets that surpass those of adequately trained expert models.","Additionally, PDEformer demonstrates promising results in the inverse problem of PDE coefficient recovery."],"url":"http://arxiv.org/abs/2402.12652v1","category":"math.NA"}
{"created":"2024-02-20 01:52:55","title":"Null controllability for stochastic semi-discrete parabolic equations","abstract":"In this paper, we present a null controllability result for a class of stochastic semi-discrete parabolic equations. For this purpose, an observability estimate is established for backward stochastic semi-discrete parabolic equations, with an explicit observability constant that depends on the discretization parameter. This estimate is obtained by a new Carleman estimate for backward stochastic semi-discrete parabolic operators.","sentences":["In this paper, we present a null controllability result for a class of stochastic semi-discrete parabolic equations.","For this purpose, an observability estimate is established for backward stochastic semi-discrete parabolic equations, with an explicit observability constant that depends on the discretization parameter.","This estimate is obtained by a new Carleman estimate for backward stochastic semi-discrete parabolic operators."],"url":"http://arxiv.org/abs/2402.12651v1","category":"math.OC"}
{"created":"2024-02-20 01:32:45","title":"Invertibility of local geodesic transverse and mixed ray transforms II: higher order tensors","abstract":"Consider a compact Riemannian manifold in dimension $n$ with strictly convex boundary. We show the local invertibility near a boundary point of the transverse ray transform of $2$ tensors for $n\\geq 3$ and the mixed ray transform of $2+2$ tensors for $n=3$. When the manifold admits a strictly convex function, this local invertibility result leads to global invertibility.","sentences":["Consider a compact Riemannian manifold in dimension $n$ with strictly convex boundary.","We show the local invertibility near a boundary point of the transverse ray transform of $2$ tensors for $n\\geq 3$ and the mixed ray transform of $2+2$ tensors for $n=3$. When the manifold admits a strictly convex function, this local invertibility result leads to global invertibility."],"url":"http://arxiv.org/abs/2402.12640v1","category":"math.DG"}
{"created":"2024-02-20 01:25:39","title":"Scalar curvature rigidity of the four-dimensional sphere","abstract":"Let $(M,g)$ be a closed connected oriented (possibly non-spin) smooth four-dimensional manifold with scalar curvature bounded below by $n(n-1)$. In this paper, we prove that if $f$ is a smooth map of non-zero degree from $(M, g)$ to the unit four-sphere, then $f$ is an isometry. Following ideas of Gromov, we use $\\mu$-bubbles and a version with coefficients of the rigidity of the three-sphere to rule out the case of strict inequality. Our proof of rigidity is based on the harmonic map heat flow coupled with the Ricci flow.","sentences":["Let $(M,g)$ be a closed connected oriented (possibly non-spin)","smooth four-dimensional manifold with scalar curvature bounded below by $n(n-1)$. In this paper",", we prove that if $f$ is a smooth map of non-zero degree from $(M, g)$ to the unit four-sphere, then $f$ is an isometry.","Following ideas of Gromov, we use $\\mu$-bubbles and a version with coefficients of the rigidity of the three-sphere to rule out the case of strict inequality.","Our proof of rigidity is based on the harmonic map heat flow coupled with the Ricci flow."],"url":"http://arxiv.org/abs/2402.12633v1","category":"math.DG"}
{"created":"2024-02-20 01:10:12","title":"Compact NSGA-II for Multi-objective Feature Selection","abstract":"Feature selection is an expensive challenging task in machine learning and data mining aimed at removing irrelevant and redundant features. This contributes to an improvement in classification accuracy, as well as the budget and memory requirements for classification, or any other post-processing task conducted after feature selection. In this regard, we define feature selection as a multi-objective binary optimization task with the objectives of maximizing classification accuracy and minimizing the number of selected features. In order to select optimal features, we have proposed a binary Compact NSGA-II (CNSGA-II) algorithm. Compactness represents the population as a probability distribution to enhance evolutionary algorithms not only to be more memory-efficient but also to reduce the number of fitness evaluations. Instead of holding two populations during the optimization process, our proposed method uses several Probability Vectors (PVs) to generate new individuals. Each PV efficiently explores a region of the search space to find non-dominated solutions instead of generating candidate solutions from a small population as is the common approach in most evolutionary algorithms. To the best of our knowledge, this is the first compact multi-objective algorithm proposed for feature selection. The reported results for expensive optimization cases with a limited budget on five datasets show that the CNSGA-II performs more efficiently than the well-known NSGA-II method in terms of the hypervolume (HV) performance metric requiring less memory. The proposed method and experimental results are explained and analyzed in detail.","sentences":["Feature selection is an expensive challenging task in machine learning and data mining aimed at removing irrelevant and redundant features.","This contributes to an improvement in classification accuracy, as well as the budget and memory requirements for classification, or any other post-processing task conducted after feature selection.","In this regard, we define feature selection as a multi-objective binary optimization task with the objectives of maximizing classification accuracy and minimizing the number of selected features.","In order to select optimal features, we have proposed a binary Compact NSGA-II (CNSGA-II) algorithm.","Compactness represents the population as a probability distribution to enhance evolutionary algorithms not only to be more memory-efficient but also to reduce the number of fitness evaluations.","Instead of holding two populations during the optimization process, our proposed method uses several Probability Vectors (PVs) to generate new individuals.","Each PV efficiently explores a region of the search space to find non-dominated solutions instead of generating candidate solutions from a small population as is the common approach in most evolutionary algorithms.","To the best of our knowledge, this is the first compact multi-objective algorithm proposed for feature selection.","The reported results for expensive optimization cases with a limited budget on five datasets show that the CNSGA-II performs more efficiently than the well-known NSGA-II method in terms of the hypervolume (HV) performance metric requiring less memory.","The proposed method and experimental results are explained and analyzed in detail."],"url":"http://arxiv.org/abs/2402.12625v1","category":"cs.LG"}
{"created":"2024-02-20 00:55:11","title":"Nonlocal Symmetries of Two 2-component Equations of Camassa-Holm Type","abstract":"For a 2-component Camassa-Holm equation, as well as a 2-component generalization of the modified Camassa-Holm equation, nonlocal infinitesimal symmetries quadratically depending on eigenfunctions of linear spectral problems are constructed from functional gradients of spectral parameters. With appropriate pseudo-potentials, these nonlocal infinitesimal symmetries are prolonged to enlarged systems, and then explicitly integrated to generate symmetry transformations in finite form for enlarged systems. As implementations of these finite symmetry transformations, some kinds of nontrivial solutions and B\\\"{a}cklund transformations are derived for both equations.","sentences":["For a 2-component Camassa-Holm equation, as well as a 2-component generalization of the modified Camassa-Holm equation, nonlocal infinitesimal symmetries quadratically depending on eigenfunctions of linear spectral problems are constructed from functional gradients of spectral parameters.","With appropriate pseudo-potentials, these nonlocal infinitesimal symmetries are prolonged to enlarged systems, and then explicitly integrated to generate symmetry transformations in finite form for enlarged systems.","As implementations of these finite symmetry transformations, some kinds of nontrivial solutions and B\\\"{a}cklund transformations are derived for both equations."],"url":"http://arxiv.org/abs/2402.12618v1","category":"nlin.SI"}
{"created":"2024-02-20 00:50:26","title":"Multi-objective Binary Coordinate Search for Feature Selection","abstract":"A supervised feature selection method selects an appropriate but concise set of features to differentiate classes, which is highly expensive for large-scale datasets. Therefore, feature selection should aim at both minimizing the number of selected features and maximizing the accuracy of classification, or any other task. However, this crucial task is computationally highly demanding on many real-world datasets and requires a very efficient algorithm to reach a set of optimal features with a limited number of fitness evaluations. For this purpose, we have proposed the binary multi-objective coordinate search (MOCS) algorithm to solve large-scale feature selection problems. To the best of our knowledge, the proposed algorithm in this paper is the first multi-objective coordinate search algorithm. In this method, we generate new individuals by flipping a variable of the candidate solutions on the Pareto front. This enables us to investigate the effectiveness of each feature in the corresponding subset. In fact, this strategy can play the role of crossover and mutation operators to generate distinct subsets of features. The reported results indicate the significant superiority of our method over NSGA-II, on five real-world large-scale datasets, particularly when the computing budget is limited. Moreover, this simple hyper-parameter-free algorithm can solve feature selection much faster and more efficiently than NSGA-II.","sentences":["A supervised feature selection method selects an appropriate but concise set of features to differentiate classes, which is highly expensive for large-scale datasets.","Therefore, feature selection should aim at both minimizing the number of selected features and maximizing the accuracy of classification, or any other task.","However, this crucial task is computationally highly demanding on many real-world datasets and requires a very efficient algorithm to reach a set of optimal features with a limited number of fitness evaluations.","For this purpose, we have proposed the binary multi-objective coordinate search (MOCS) algorithm to solve large-scale feature selection problems.","To the best of our knowledge, the proposed algorithm in this paper is the first multi-objective coordinate search algorithm.","In this method, we generate new individuals by flipping a variable of the candidate solutions on the Pareto front.","This enables us to investigate the effectiveness of each feature in the corresponding subset.","In fact, this strategy can play the role of crossover and mutation operators to generate distinct subsets of features.","The reported results indicate the significant superiority of our method over NSGA-II, on five real-world large-scale datasets, particularly when the computing budget is limited.","Moreover, this simple hyper-parameter-free algorithm can solve feature selection much faster and more efficiently than NSGA-II."],"url":"http://arxiv.org/abs/2402.12616v1","category":"cs.LG"}
{"created":"2024-02-19 23:50:32","title":"Gravitational wave asteroseismology of dark matter hadronic stars","abstract":"The influence of the dark matter mass~($M_{\\chi}$) and the Fermi momentum~($k_{F}^{\\dm}$) on the $f_0$-mode oscillation frequency, damping time parameter, and tidal deformability of hadronic stars are studied by employing a numerical integration of hydrostatic equilibrium, nonradial oscillation, and tidal deformability equations. The matter inside the hadronic stars follows the NL3* equation of state. We obtain that the influence of $M_{\\chi}$ and $k_F^{\\dm}$ is observed in the $f_0$-mode, damping tome parameter, and tidal deformability. Finally, the correlation between the tidal deformability of the GW$170817$ event with $M_{\\chi}$ and $k_F^{\\dm}$ are also investigated.","sentences":["The influence of the dark matter mass~($M_{\\chi}$) and the Fermi momentum~($k_{F}^{\\dm}$) on the $f_0$-mode oscillation frequency, damping time parameter, and tidal deformability of hadronic stars are studied by employing a numerical integration of hydrostatic equilibrium, nonradial oscillation, and tidal deformability equations.","The matter inside the hadronic stars follows the NL3* equation of state.","We obtain that the influence of $M_{\\chi}$ and $k_F^{\\dm}$ is observed in the $f_0$-mode, damping tome parameter, and tidal deformability.","Finally, the correlation between the tidal deformability of the GW$170817$ event with $M_{\\chi}$ and $k_F^{\\dm}$ are also investigated."],"url":"http://arxiv.org/abs/2402.12600v1","category":"hep-ph"}
{"created":"2024-02-19 21:53:41","title":"Solving fluid flow problems in space-time with multiscale stabilization: formulation and examples","abstract":"We solve fluid flow problems through a space-time finite element method. The weak form of the Navier-Stokes equations is stabilized using the variational multi-scale formulation. The finite element problem is posed on the \"full\" space-time domain, considering time as another dimension. We apply this method on two benchmark problems in computational fluid dynamics, namely, lid-driven cavity flow and flow past a circular cylinder. We validate the current method with existing results from literature and show that very large space-time blocks can be solved using our approach.","sentences":["We solve fluid flow problems through a space-time finite element method.","The weak form of the Navier-Stokes equations is stabilized using the variational multi-scale formulation.","The finite element problem is posed on the \"full\" space-time domain, considering time as another dimension.","We apply this method on two benchmark problems in computational fluid dynamics, namely, lid-driven cavity flow and flow past a circular cylinder.","We validate the current method with existing results from literature and show that very large space-time blocks can be solved using our approach."],"url":"http://arxiv.org/abs/2402.12571v1","category":"math.NA"}
{"created":"2024-02-19 21:05:50","title":"Performance of using Mel-Frequency Cepstrum Based Features in Nonlinear Classifiers for Phonocardiography Recordings","abstract":"Cardiovascular system diseases can be identified by using a specialized diagnostic process utilizing a digital stethoscope. Digital stethoscopes provide phonocardiography (PCG) recordings for further inspection, besides filtering and amplification of heart sounds. In this paper, a framework that is useful to develop feature extraction and classification of PCG recordings is presented. This framework is built upon a previously proposed segmentation algorithm that processes a feature vector produced by the agglutinate application of Mel-frequency cepstrum and discrete wavelet transform (DWT). The performance of the segmentation algorithm is also tested on a new data set and compared to the previously reported results. After identifying the fundamental heart sounds and segmenting the PCG recordings, five principal features are extracted from the time domain signal and Mel-Frequency cepstral coefficients (MFCC) of each cardiac cycle. Classification outcomes are reported for three nonlinear models: k nearest neighbor (k-NN), support vector machine (SVM), and multilayer perceptrons (MLP) classifiers in comparison with a linear approach, namely Mahalanobis distance linear classifier. The results underline that although neural networks and linear classifier show compatible performance in basic classification problems, with the increase in the nonlinearity of the classification problem their performance significantly vary.","sentences":["Cardiovascular system diseases can be identified by using a specialized diagnostic process utilizing a digital stethoscope.","Digital stethoscopes provide phonocardiography (PCG) recordings for further inspection, besides filtering and amplification of heart sounds.","In this paper, a framework that is useful to develop feature extraction and classification of PCG recordings is presented.","This framework is built upon a previously proposed segmentation algorithm that processes a feature vector produced by the agglutinate application of Mel-frequency cepstrum and discrete wavelet transform (DWT).","The performance of the segmentation algorithm is also tested on a new data set and compared to the previously reported results.","After identifying the fundamental heart sounds and segmenting the PCG recordings, five principal features are extracted from the time domain signal and Mel-Frequency cepstral coefficients (MFCC) of each cardiac cycle.","Classification outcomes are reported for three nonlinear models: k nearest neighbor (k-NN), support vector machine (SVM), and multilayer perceptrons (MLP) classifiers in comparison with a linear approach, namely Mahalanobis distance linear classifier.","The results underline that although neural networks and linear classifier show compatible performance in basic classification problems, with the increase in the nonlinearity of the classification problem their performance significantly vary."],"url":"http://arxiv.org/abs/2402.12540v1","category":"eess.SP"}
{"created":"2024-02-19 20:48:09","title":"Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics","abstract":"This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR \\& AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (\\textbf{HEPT}), which combines E$^2$LSH with OR \\& AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance in two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at \\url{https://github.com/Graph-COM/HEPT}.","sentences":["This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics.","Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations.","One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers.","Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR \\& AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias.","Based on this finding, we propose LSH-based Efficient Point Transformer (\\textbf{HEPT}), which combines E$^2$LSH with OR \\& AND constructions and is built upon regular computations.","HEPT demonstrates remarkable performance in two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing.","Our code is available at \\url{https://github.com/Graph-COM/HEPT}."],"url":"http://arxiv.org/abs/2402.12535v1","category":"cs.LG"}
{"created":"2024-02-19 20:39:54","title":"Laplacians in spinor bundles over translation surfaces: self-adjoint extentions and regularized determinants","abstract":"We study the regularized determinants ${\\rm det}\\, \\Delta$ of various self-adjoint extensions of symmetric Laplacians acting in spinor bundles over compact Riemann surfaces with flat singular metrics $|\\omega|^2$, where $\\omega$ is a holomorphic one form on the Riemann surface. We find an explicit expression for ${\\rm det}\\, \\Delta$ for the so-called self-adjoint Szeg\\\"o extension through the Bergman tau-function on the moduli space of Abelian differentials and the theta-constants (corresponding to the spinor bundle). This expression can be considered as a version of the well-known spin-$1/2$ bosonization formula of Bost-Nelson for the case of flat conformal metrics with conical singularities and a higher genus generalization of the Ray-Singer formula for flat elliptic curves. We establish comparison formulas for the determinants of two different extensions (e. g., the Szeg\\\"o extension and the Friedrichs one). The paper answers a question raised by D'Hoker and Phong \\cite{DH-P} more than thirty years ago. We also reconsider the results from \\cite{DH-P} on the regularization of diverging determinant ratio for Mandelstam metrics (for any spin) proposing (and computing) a new regularization of this ratio.","sentences":["We study the regularized determinants ${\\rm det}\\, \\Delta$ of various self-adjoint extensions of symmetric Laplacians acting in spinor bundles over compact Riemann surfaces with flat singular metrics $|\\omega|^2$, where $\\omega$ is a holomorphic one form on the Riemann surface.","We find an explicit expression for ${\\rm det}\\, \\Delta$ for the so-called self-adjoint Szeg\\\"o extension through the Bergman tau-function on the moduli space of Abelian differentials and the theta-constants (corresponding to the spinor bundle).","This expression can be considered as a version of the well-known spin-$1/2$ bosonization formula of Bost-Nelson for the case of flat conformal metrics with conical singularities and a higher genus generalization of the Ray-Singer formula for flat elliptic curves.","We establish comparison formulas for the determinants of two different extensions (e. g., the Szeg\\\"o extension and the Friedrichs one).","The paper answers a question raised by D'Hoker and Phong \\cite{DH-P} more than thirty years ago.","We also reconsider the results from \\cite{DH-P} on the regularization of diverging determinant ratio for Mandelstam metrics (for any spin) proposing (and computing) a new regularization of this ratio."],"url":"http://arxiv.org/abs/2402.12529v1","category":"math.DG"}
{"created":"2024-02-19 20:29:49","title":"System Identification of Neural Systems: Going Beyond Images to Modelling Dynamics","abstract":"Vast literature has compared the recordings of biological neurons in the brain to deep neural networks. The ultimate goal is to interpret deep networks or to better understand and encode biological neural systems. Recently, there has been a debate on whether system identification is possible and how much it can tell us about the brain computation. System identification recognizes whether one model is more valid to represent the brain computation over another. Nonetheless, previous work did not consider the time aspect and how video and dynamics (e.g., motion) modelling in deep networks relate to these biological neural systems within a large-scale comparison. Towards this end, we propose a system identification study focused on comparing single image vs. video understanding models with respect to the visual cortex recordings. Our study encompasses two sets of experiments; a real environment setup and a simulated environment setup. The study also encompasses more than 30 models and, unlike prior works, we focus on convolutional vs. transformer-based, single vs. two-stream, and fully vs. self-supervised video understanding models. The goal is to capture a greater variety of architectures that model dynamics. As such, this signifies the first large-scale study of video understanding models from a neuroscience perspective. Our results in the simulated experiments, show that system identification can be attained to a certain level in differentiating image vs. video understanding models. Moreover, we provide key insights on how video understanding models predict visual cortex responses; showing video understanding better than image understanding models, convolutional models are better in the early-mid regions than transformer based except for multiscale transformers that are still good in predicting these regions, and that two-stream models are better than single stream.","sentences":["Vast literature has compared the recordings of biological neurons in the brain to deep neural networks.","The ultimate goal is to interpret deep networks or to better understand and encode biological neural systems.","Recently, there has been a debate on whether system identification is possible and how much it can tell us about the brain computation.","System identification recognizes whether one model is more valid to represent the brain computation over another.","Nonetheless, previous work did not consider the time aspect and how video and dynamics (e.g., motion) modelling in deep networks relate to these biological neural systems within a large-scale comparison.","Towards this end, we propose a system identification study focused on comparing single image vs. video understanding models with respect to the visual cortex recordings.","Our study encompasses two sets of experiments; a real environment setup and a simulated environment setup.","The study also encompasses more than 30 models and, unlike prior works, we focus on convolutional vs. transformer-based, single vs. two-stream, and fully vs. self-supervised video understanding models.","The goal is to capture a greater variety of architectures that model dynamics.","As such, this signifies the first large-scale study of video understanding models from a neuroscience perspective.","Our results in the simulated experiments, show that system identification can be attained to a certain level in differentiating image vs. video understanding models.","Moreover, we provide key insights on how video understanding models predict visual cortex responses; showing video understanding better than image understanding models, convolutional models are better in the early-mid regions than transformer based except for multiscale transformers that are still good in predicting these regions, and that two-stream models are better than single stream."],"url":"http://arxiv.org/abs/2402.12519v1","category":"cs.CV"}
{"created":"2024-02-19 20:20:00","title":"Function Class Learning with Genetic Programming: Towards Explainable Meta Learning for Tumor Growth Functionals","abstract":"Paragangliomas are rare, primarily slow-growing tumors for which the underlying growth pattern is unknown. Therefore, determining the best care for a patient is hard. Currently, if no significant tumor growth is observed, treatment is often delayed, as treatment itself is not without risk. However, by doing so, the risk of (irreversible) adverse effects due to tumor growth may increase. Being able to predict the growth accurately could assist in determining whether a patient will need treatment during their lifetime and, if so, the timing of this treatment. The aim of this work is to learn the general underlying growth pattern of paragangliomas from multiple tumor growth data sets, in which each data set contains a tumor's volume over time. To do so, we propose a novel approach based on genetic programming to learn a function class, i.e., a parameterized function that can be fit anew for each tumor. We do so in a unique, multi-modal, multi-objective fashion to find multiple potentially interesting function classes in a single run. We evaluate our approach on a synthetic and a real-world data set. By analyzing the resulting function classes, we can effectively explain the general patterns in the data.","sentences":["Paragangliomas are rare, primarily slow-growing tumors for which the underlying growth pattern is unknown.","Therefore, determining the best care for a patient is hard.","Currently, if no significant tumor growth is observed, treatment is often delayed, as treatment itself is not without risk.","However, by doing so, the risk of (irreversible) adverse effects due to tumor growth may increase.","Being able to predict the growth accurately could assist in determining whether a patient will need treatment during their lifetime and, if so, the timing of this treatment.","The aim of this work is to learn the general underlying growth pattern of paragangliomas from multiple tumor growth data sets, in which each data set contains a tumor's volume over time.","To do so, we propose a novel approach based on genetic programming to learn a function class, i.e., a parameterized function that can be fit anew for each tumor.","We do so in a unique, multi-modal, multi-objective fashion to find multiple potentially interesting function classes in a single run.","We evaluate our approach on a synthetic and a real-world data set.","By analyzing the resulting function classes, we can effectively explain the general patterns in the data."],"url":"http://arxiv.org/abs/2402.12510v1","category":"cs.NE"}
{"created":"2024-02-19 20:18:29","title":"SDEs for Minimax Optimization","abstract":"Minimax optimization problems have attracted a lot of attention over the past few years, with applications ranging from economics to machine learning. While advanced optimization methods exist for such problems, characterizing their dynamics in stochastic scenarios remains notably challenging. In this paper, we pioneer the use of stochastic differential equations (SDEs) to analyze and compare Minimax optimizers. Our SDE models for Stochastic Gradient Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient Descent are provable approximations of their algorithmic counterparts, clearly showcasing the interplay between hyperparameters, implicit regularization, and implicit curvature-induced noise. This perspective also allows for a unified and simplified analysis strategy based on the principles of It\\^o calculus. Finally, our approach facilitates the derivation of convergence conditions and closed-form solutions for the dynamics in simplified settings, unveiling further insights into the behavior of different optimizers.","sentences":["Minimax optimization problems have attracted a lot of attention over the past few years, with applications ranging from economics to machine learning.","While advanced optimization methods exist for such problems, characterizing their dynamics in stochastic scenarios remains notably challenging.","In this paper, we pioneer the use of stochastic differential equations (SDEs) to analyze and compare Minimax optimizers.","Our SDE models for Stochastic Gradient Descent-Ascent, Stochastic Extragradient, and Stochastic Hamiltonian Gradient Descent are provable approximations of their algorithmic counterparts, clearly showcasing the interplay between hyperparameters, implicit regularization, and implicit curvature-induced noise.","This perspective also allows for a unified and simplified analysis strategy based on the principles of It\\^o calculus.","Finally, our approach facilitates the derivation of convergence conditions and closed-form solutions for the dynamics in simplified settings, unveiling further insights into the behavior of different optimizers."],"url":"http://arxiv.org/abs/2402.12508v1","category":"cs.LG"}
{"created":"2024-02-19 20:11:46","title":"PARCv2: Physics-aware Recurrent Convolutional Neural Networks for Spatiotemporal Dynamics Modeling","abstract":"Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL). The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces. Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems. Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems. We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems. The extended model, referred to as PARCv2, is equipped with differential operators to model advection-reaction-diffusion equations, as well as a hybrid integral solver for stable, long-time predictions. PARCv2 is tested on both standard benchmark problems in fluid dynamics, namely Burgers and Navier-Stokes equations, and then applied to more complex shock-induced reaction problems in energetic materials. We evaluate the behavior of PARCv2 in comparison to other physics-informed and learning bias models and demonstrate its potential to model unsteady and advection-dominant dynamics regimes.","sentences":["Modeling unsteady, fast transient, and advection-dominated physics problems is a pressing challenge for physics-aware deep learning (PADL).","The physics of complex systems is governed by large systems of partial differential equations (PDEs) and ancillary constitutive models with nonlinear structures, as well as evolving state fields exhibiting sharp gradients and rapidly deforming material interfaces.","Here, we investigate an inductive bias approach that is versatile and generalizable to model generic nonlinear field evolution problems.","Our study focuses on the recent physics-aware recurrent convolutions (PARC), which incorporates a differentiator-integrator architecture that inductively models the spatiotemporal dynamics of generic physical systems.","We extend the capabilities of PARC to simulate unsteady, transient, and advection-dominant systems.","The extended model, referred to as PARCv2, is equipped with differential operators to model advection-reaction-diffusion equations, as well as a hybrid integral solver for stable, long-time predictions.","PARCv2 is tested on both standard benchmark problems in fluid dynamics, namely Burgers and Navier-Stokes equations, and then applied to more complex shock-induced reaction problems in energetic materials.","We evaluate the behavior of PARCv2 in comparison to other physics-informed and learning bias models and demonstrate its potential to model unsteady and advection-dominant dynamics regimes."],"url":"http://arxiv.org/abs/2402.12503v1","category":"cs.LG"}
{"created":"2024-02-19 20:09:42","title":"Euler-Maruyama schemes for stochastic differential equations driven by stable L\u00e9vy processes with i.i.d. stable components","abstract":"We study Euler-Maruyama numerical schemes of stochastic differential equations driven by stable L\\'{e}vy processes with i.i.d. stable components. We obtain a uniform-in-time approximation error in Wasserstein distance. Our approximation error has a linear dependence on the stepsize, which is expected to be tight, as can be seen from an explicit calculation for the case of an Ornstein-Uhlenbeck process. We also obtain a uniform-in-time approximation error when Pareto noises are used in the discretization scheme.","sentences":["We study Euler-Maruyama numerical schemes of stochastic differential equations driven by stable L\\'{e}vy processes with i.i.d. stable components.","We obtain a uniform-in-time approximation error in Wasserstein distance.","Our approximation error has a linear dependence on the stepsize, which is expected to be tight, as can be seen from an explicit calculation for the case of an Ornstein-Uhlenbeck process.","We also obtain a uniform-in-time approximation error when Pareto noises are used in the discretization scheme."],"url":"http://arxiv.org/abs/2402.12502v1","category":"math.PR"}
{"created":"2024-02-19 19:54:34","title":"Accretion discs onto supermassive compact objects: a portal to dark matter physics in active galaxies","abstract":"The study of the physics of accretion discs developed around the supermassive black hole (BH) candidates are essential theoretical tools to test their nature. Here, we study the accretion flow and associated emission using generalised $\\alpha$-discs on to horizonless dark compact objects, in order to compare with the traditional BH scenario. The BH alternative here proposed consists in a dense and highly degenerate core made of fermionic dark matter (DM) which is surrounded by a more diluted DM halo. Such a dense core -- diluted halo DM configuration is a solution of the Einstein equations of General Relativity (GR) in spherical symmetry, which naturally arises once the quantum nature of the DM fermions is dully accounted for. The methodology followed in this work consist in first generalising the theory of $\\alpha$-discs to work in the presence of regular and horizonless compact objects, and second, to apply it to the case of core-halo DM profiles typical of active-like galaxies. The fact that the compactness of the dense and transparent DM core scales with the particle mass, allows for the following key findings of this work: (i) it always exist a given core compacity -- i.e., corresponding particle mass -- which produces a luminosity spectrum which is basically indistinguishable from that of a Schwarzschild BH of the same mass as the DM core; (ii) the disc can enter deep inside the non-rotating DM core, allowing for accretion powered efficiencies as high as $28\\%$, thus comparable to that of a highly rotating Kerr BH. These results, together with the existence of a critical DM core mass of collapse into a supermassive BH, open new avenues of research for two seemingly unrelated topics such as AGN phenomenology and dark matter physics.","sentences":["The study of the physics of accretion discs developed around the supermassive black hole (BH) candidates are essential theoretical tools to test their nature.","Here, we study the accretion flow and associated emission using generalised $\\alpha$-discs on to horizonless dark compact objects, in order to compare with the traditional BH scenario.","The BH alternative here proposed consists in a dense and highly degenerate core made of fermionic dark matter (DM) which is surrounded by a more diluted DM halo.","Such a dense core -- diluted halo DM configuration is a solution of the Einstein equations of General Relativity (GR) in spherical symmetry, which naturally arises once the quantum nature of the DM fermions is dully accounted for.","The methodology followed in this work consist in first generalising the theory of $\\alpha$-discs to work in the presence of regular and horizonless compact objects, and second, to apply it to the case of core-halo DM profiles typical of active-like galaxies.","The fact that the compactness of the dense and transparent DM core scales with the particle mass, allows for the following key findings of this work: (i) it always exist a given core compacity -- i.e., corresponding particle mass -- which produces a luminosity spectrum which is basically indistinguishable from that of a Schwarzschild BH of the same mass as the DM core; (ii) the disc can enter deep inside the non-rotating DM core, allowing for accretion powered efficiencies as high as $28\\%$, thus comparable to that of a highly rotating Kerr BH.","These results, together with the existence of a critical DM core mass of collapse into a supermassive BH, open new avenues of research for two seemingly unrelated topics such as AGN phenomenology and dark matter physics."],"url":"http://arxiv.org/abs/2402.12491v1","category":"astro-ph.GA"}
{"created":"2024-02-19 19:24:58","title":"Exploring the UV and IR of a type-II holographic superconductor using a dyonic black hole","abstract":"In this study, we investigate a type-II holographic superconductor with a perturbative scalar field over a (3 + 1)-dimensional electric and magnetically charged planar AdS black hole. After consistently decoupling the scalar field sector from the complete Einstein-Maxwell-Scalar system, we delve into the thermodynamical properties of the background relevant for the dual description of the Ginzburg-Landau density of superconducting states. The adoption of a London gauge allowed us to consider the magnetic field as a uniform external field over which the holographic superconductor is subject. This consideration enables a consistent description of the appearance of Abrikosov vortex lattices typical in type-II superconductors. Thus, by matching near horizon and boundary expansions of the scalar field, we obtained an expression for the upper critical magnetic field as a function of temperature in both, the canonical and grand canonical ensemble. These novel results confirm that our perturbative scalar field model consistently reproduces the well-known temperature behavior of the upper critical magnetic field according to the Ginzburg-Landau theory and other Abelian-Higgs holographic developments for type-II superconductors. In addition, a new analysis of the scalar field equation in terms of a Schr\\\"odinger potential led us to observe the existence of potential wells distributed along the holographic coordinate. We interpret these regions with a local minimum as those in which bound states can exist, dual to the Cooper pairs density. These results provide evidence for the existence of an IR order parameter near the extremality. In view of this, we performed a closer inspection of the IR effective scalar equation in which the geometry adopts a Schwarzschild AdS$_{2}\\times \\mathbb{R}^{2}$ structure.","sentences":["In this study, we investigate a type-II holographic superconductor with a perturbative scalar field over a (3 + 1)-dimensional electric and magnetically charged planar AdS black hole.","After consistently decoupling the scalar field sector from the complete Einstein-Maxwell-Scalar system, we delve into the thermodynamical properties of the background relevant for the dual description of the Ginzburg-Landau density of superconducting states.","The adoption of a London gauge allowed us to consider the magnetic field as a uniform external field over which the holographic superconductor is subject.","This consideration enables a consistent description of the appearance of Abrikosov vortex lattices typical in type-II superconductors.","Thus, by matching near horizon and boundary expansions of the scalar field, we obtained an expression for the upper critical magnetic field as a function of temperature in both, the canonical and grand canonical ensemble.","These novel results confirm that our perturbative scalar field model consistently reproduces the well-known temperature behavior of the upper critical magnetic field according to the Ginzburg-Landau theory and other Abelian-Higgs holographic developments for type-II superconductors.","In addition, a new analysis of the scalar field equation in terms of a Schr\\\"odinger potential led us to observe the existence of potential wells distributed along the holographic coordinate.","We interpret these regions with a local minimum as those in which bound states can exist, dual to the Cooper pairs density.","These results provide evidence for the existence of an IR order parameter near the extremality.","In view of this, we performed a closer inspection of the IR effective scalar equation in which the geometry adopts a Schwarzschild AdS$_{2}\\times \\mathbb{R}^{2}$ structure."],"url":"http://arxiv.org/abs/2402.12476v1","category":"hep-th"}
{"created":"2024-02-19 19:18:03","title":"New geometric structures on 3-manifolds: surgery and generalized geometry","abstract":"Cosymplectic and normal almost contact structures are analogues of symplectic and complex structures that can be defined on 3-manifolds. Their existence imposes strong topological constraints. Generalized geometry offers a natural common generalization of these two structures: $B_3$-generalized complex structures. We prove that any closed orientable 3-manifold admits such a structure, which can be chosen to be stable, that is, generically cosymplectic up to generalized diffeomorphism.","sentences":["Cosymplectic and normal almost contact structures are analogues of symplectic and complex structures that can be defined on 3-manifolds.","Their existence imposes strong topological constraints.","Generalized geometry offers a natural common generalization of these two structures: $B_3$-generalized complex structures.","We prove that any closed orientable 3-manifold admits such a structure, which can be chosen to be stable, that is, generically cosymplectic up to generalized diffeomorphism."],"url":"http://arxiv.org/abs/2402.12471v1","category":"math.DG"}
{"created":"2024-02-19 19:09:13","title":"Non-perturbative Origin of the Electroweak Scale: RGE in Strongly-coupled Dark Gauge Theories via Dyson-Schwinger","abstract":"We propose a novel pathway to generate the electroweak scale (EW) via non-perturbative dynamics of a dark gauge sector based on the SU(N) gauge group. Imposing the scale invariance of the theory, we investigate the electroweak symmetry breaking (EWSB) which is triggered dynamically via the condensation of gauge fields. We provide a novel method to estimate a non-perturbative EW scale generation using the exact solution of the background equations of motion in Yang-Mills theory in terms of Jacobi elliptic functions and the exact beta-function valid in the strongly coupled regimes via the Dyson-Schwinger approach. Particularly, we find an analytical result for the Renormalization Group Equation (RGE) of the gauge coupling in the $SU(N)$ sector in the strongly-coupled regime. The dynamics studied in this paper pave the way to a more realistic model building with possible resolution to the hierarchy problem and, in general, dynamical generation of scales.","sentences":["We propose a novel pathway to generate the electroweak scale (EW) via non-perturbative dynamics of a dark gauge sector based on the SU(N) gauge group.","Imposing the scale invariance of the theory, we investigate the electroweak symmetry breaking (EWSB) which is triggered dynamically via the condensation of gauge fields.","We provide a novel method to estimate a non-perturbative EW scale generation using the exact solution of the background equations of motion in Yang-Mills theory in terms of Jacobi elliptic functions and the exact beta-function valid in the strongly coupled regimes via the Dyson-Schwinger approach.","Particularly, we find an analytical result for the Renormalization Group Equation (RGE) of the gauge coupling in the $SU(N)$ sector in the strongly-coupled regime.","The dynamics studied in this paper pave the way to a more realistic model building with possible resolution to the hierarchy problem and, in general, dynamical generation of scales."],"url":"http://arxiv.org/abs/2402.12462v1","category":"hep-ph"}
{"created":"2024-02-19 19:05:54","title":"On-shell recursion and holomorphic HQET for heavy quark hadronic resonances","abstract":"We develop a new theoretical framework for the treatment of heavy quark (HQ) resonances within heavy quark effective theory (HQET). This framework uses on-shell recursion techniques to express the resonant amplitude as a product of on-shell subamplitudes, which allows one to employ a form-factor representation of the hadronic matrix elements and to obtain an HQ expansion, but at the price of introducing complex momenta. We construct a generalized \"holomorphic HQET\" onto which such complex-momentum matrix elements can be matched, and we show that $PT$ symmetry ensures the Isgur-Wise functions (and the perturbative corrections) become holomorphic functions of the complex recoil parameter with real coefficients. They are thus an analytic continuation of the standard HQET description. This framework admits a HQ hadron (strong decay) width expansion. At second order, we show it is in good agreement with data for the $B_{1(2)}^{(*)}$ and $D_{1(2)}^{(*)}$ HQ doublets. Taking the $\\bar{B} \\to (D_1^*(1^-) \\to D\\pi)l\\nu$ system as an example, we compute the holomorphic HQET expansion to first order, as well as the complex-momentum on-shell subamplitudes. A toy numerical study of the resulting differential rates demonstrates that this framework generates HQ resonance lineshapes with large tails, resembling those seen in data.","sentences":["We develop a new theoretical framework for the treatment of heavy quark (HQ) resonances within heavy quark effective theory (HQET).","This framework uses on-shell recursion techniques to express the resonant amplitude as a product of on-shell subamplitudes, which allows one to employ a form-factor representation of the hadronic matrix elements and to obtain an HQ expansion, but at the price of introducing complex momenta.","We construct a generalized \"holomorphic HQET\" onto which such complex-momentum matrix elements can be matched, and we show that $PT$ symmetry ensures the Isgur-Wise functions (and the perturbative corrections) become holomorphic functions of the complex recoil parameter with real coefficients.","They are thus an analytic continuation of the standard HQET description.","This framework admits a HQ hadron (strong decay) width expansion.","At second order, we show it is in good agreement with data for the $B_{1(2)}^{(*)}$ and $D_{1(2)}^{(*)}$ HQ doublets.","Taking the $\\bar{B} \\to (D_1^*(1^-) \\to D\\pi)l\\nu$ system as an example, we compute the holomorphic HQET expansion to first order, as well as the complex-momentum on-shell subamplitudes.","A toy numerical study of the resulting differential rates demonstrates that this framework generates HQ resonance lineshapes with large tails, resembling those seen in data."],"url":"http://arxiv.org/abs/2402.12460v1","category":"hep-ph"}
{"created":"2024-02-19 19:03:21","title":"Existence of blow-up self-similar solutions for the supercritical quasilinear reaction-diffusion equation","abstract":"We establish the existence of self-similar solutions presenting finite time blow-up to the quasilinear reaction-diffusion equation $$ u_t=\\Delta u^m + u^p, $$ posed in dimension $N\\geq3$, $m>1$. More precisely, we show that there is always at least one solution in backward self-similar form if $p>p_s=m(N+2)/(N-2)$. In particular, this establishes \\emph{non-optimality of the Lepin critical exponent} introduced in \\cite{Le90} in the semilinear case $m=1$ and extended for $m>1$ in \\cite{GV97, GV02}, for the existence of self-similar blow-up solutions. We also prove that there are multiple solutions in the same range, provided $N$ is sufficiently large. This is in strong contrast with the semilinear case, where the Lepin critical exponent has been proved to be optimal.","sentences":["We establish the existence of self-similar solutions presenting finite time blow-up to the quasilinear reaction-diffusion equation $$ u_t=\\Delta u^m + u^p, $$ posed in dimension $N\\geq3$, $m>1$. More precisely, we show that there is always at least one solution in backward self-similar form if $p>p_s=","m(N+2)/(N-2)$. In particular, this establishes \\emph{non-optimality of the Lepin critical exponent} introduced in \\cite{Le90} in the semilinear case $m=1$ and extended for $m>1$ in \\cite{GV97, GV02}, for the existence of self-similar blow-up solutions.","We also prove that there are multiple solutions in the same range, provided $N$ is sufficiently large.","This is in strong contrast with the semilinear case, where the Lepin critical exponent has been proved to be optimal."],"url":"http://arxiv.org/abs/2402.12455v1","category":"math.AP"}
{"created":"2024-02-19 19:00:09","title":"Equivariant, Safe and Sensitive$\\unicode{x2013}$Graph Networks for New Physics","abstract":"This study introduces a novel Graph Neural Network (GNN) architecture that leverages infrared and collinear (IRC) safety and equivariance to enhance the analysis of collider data for Beyond the Standard Model (BSM) discoveries. By integrating equivariance in the rapidity-azimuth plane with IRC-safe principles, our model significantly reduces computational overhead while ensuring theoretical consistency in identifying BSM scenarios amidst Quantum Chromodynamics backgrounds. The proposed GNN architecture demonstrates superior performance in tagging semi-visible jets, highlighting its potential as a robust tool for advancing BSM search strategies at high-energy colliders.","sentences":["This study introduces a novel Graph Neural Network (GNN) architecture that leverages infrared and collinear (IRC) safety and equivariance to enhance the analysis of collider data for Beyond the Standard Model (BSM) discoveries.","By integrating equivariance in the rapidity-azimuth plane with IRC-safe principles, our model significantly reduces computational overhead while ensuring theoretical consistency in identifying BSM scenarios amidst Quantum Chromodynamics backgrounds.","The proposed GNN architecture demonstrates superior performance in tagging semi-visible jets, highlighting its potential as a robust tool for advancing BSM search strategies at high-energy colliders."],"url":"http://arxiv.org/abs/2402.12449v1","category":"hep-ph"}
{"created":"2024-02-19 19:00:04","title":"Generalized Rate Operator Quantum Jumps via Realization-Dependent Transformations","abstract":"The dynamics of open quantum systems is often solved by stochastic unravellings where the average over the state vector realizations reproduces the density matrix evolution. We focus on quantum jump descriptions based on the rate operator formalism. In addition to displaying and exploiting different equivalent ways of writing the master equation, we introduce state-dependent rate operator transformations within the framework of stochastic pure state realizations, allowing us to extend and generalize the previously developed formalism. As a consequence, this improves the controllability of the stochastic realizations and subsequently greatly benefits when searching for optimal simulation schemes to solve open system dynamics. At a fundamental level, intriguingly, our results show that it is possible to have positive unravellings -- without reverse quantum jumps and avoiding the use of auxiliary degrees freedom -- in a number of example cases even when the corresponding dynamical map breaks the property of P-divisibility, thus being in the strongly non-Markovian regime.","sentences":["The dynamics of open quantum systems is often solved by stochastic unravellings where the average over the state vector realizations reproduces the density matrix evolution.","We focus on quantum jump descriptions based on the rate operator formalism.","In addition to displaying and exploiting different equivalent ways of writing the master equation, we introduce state-dependent rate operator transformations within the framework of stochastic pure state realizations, allowing us to extend and generalize the previously developed formalism.","As a consequence, this improves the controllability of the stochastic realizations and subsequently greatly benefits when searching for optimal simulation schemes to solve open system dynamics.","At a fundamental level, intriguingly, our results show that it is possible to have positive unravellings -- without reverse quantum jumps and avoiding the use of auxiliary degrees freedom -- in a number of example cases even when the corresponding dynamical map breaks the property of P-divisibility, thus being in the strongly non-Markovian regime."],"url":"http://arxiv.org/abs/2402.12445v1","category":"quant-ph"}
{"created":"2024-02-19 19:00:01","title":"Emulating the interstellar medium chemistry with neural operators","abstract":"Galaxy formation and evolution critically depend on understanding the complex photo-chemical processes that govern the evolution and thermodynamics of the InterStellar Medium (ISM). Computationally, solving chemistry is among the most heavy tasks in cosmological and astrophysical simulations. The evolution of such non-equilibrium photo-chemical network relies on implicit, precise, computationally costly, ordinary differential equations (ODE) solvers. Here, we aim at substituting such procedural solvers with fast, pre-trained, emulators based on neural operators. We emulate a non-equilibrium chemical network up to H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism, i.e. by splitting the ODE solver operator that maps the initial conditions and time evolution into a tensor product of two neural networks. We use $\\texttt{KROME}$ to generate a training set spanning $-2\\leq \\log(n/\\mathrm{cm}^{-3}) \\leq 3.5$, $\\log(20) \\leq\\log(T/\\mathrm{K}) \\leq 5.5$, $-6 \\leq \\log(n_i/n) < 0$, and by adopting an incident radiation field $\\textbf{F}$ sampled in 10 energy bins with a continuity prior. We separately train the solver for $T$ and each $n_i$ for $\\simeq 4.34\\,\\rm GPUhrs$. Compared with the reference solutions obtained by $\\texttt{KROME}$ for single zone models, the typical precision obtained is of order $10^{-2}$, i.e. the $10 \\times$ better with a training that is $40 \\times$ less costly with respect to previous emulators which however considered only a fixed $\\mathbf{F}$. The present model achieves a speed-up of a factor of $128 \\times$ with respect to stiff ODE solvers. Our neural emulator represents a significant leap forward in the modeling of ISM chemistry, offering a good balance of precision, versatility, and computational efficiency.","sentences":["Galaxy formation and evolution critically depend on understanding the complex photo-chemical processes that govern the evolution and thermodynamics of the InterStellar Medium (ISM).","Computationally, solving chemistry is among the most heavy tasks in cosmological and astrophysical simulations.","The evolution of such non-equilibrium photo-chemical network relies on implicit, precise, computationally costly, ordinary differential equations (ODE) solvers.","Here, we aim at substituting such procedural solvers with fast, pre-trained, emulators based on neural operators.","We emulate a non-equilibrium chemical network up to H$_2$ formation (9 species, 52 reactions) by adopting the DeepONet formalism, i.e. by splitting the ODE solver operator that maps the initial conditions and time evolution into a tensor product of two neural networks.","We use $\\texttt{KROME}$ to generate a training set spanning $-2\\leq \\log(n/\\mathrm{cm}^{-3})","\\leq 3.5$, $\\log(20) \\leq\\log(T/\\mathrm{K})","\\leq 5.5$, $-6 \\leq \\log(n_i/n) < 0$, and by adopting an incident radiation field $\\textbf{F}$ sampled in 10 energy bins with a continuity prior.","We separately train the solver for $T$ and each $n_i$ for $\\simeq 4.34\\,\\rm GPUhrs$. Compared with the reference solutions obtained by $\\texttt{KROME}$ for single zone models, the typical precision obtained is of order $10^{-2}$, i.e. the $10 \\times$ better with a training that is $40 \\times$ less costly with respect to previous emulators which however considered only a fixed $\\mathbf{F}$. The present model achieves a speed-up of a factor of $128 \\times$ with respect to stiff ODE solvers.","Our neural emulator represents a significant leap forward in the modeling of ISM chemistry, offering a good balance of precision, versatility, and computational efficiency."],"url":"http://arxiv.org/abs/2402.12435v1","category":"astro-ph.GA"}
{"created":"2024-02-20 18:59:00","title":"FlashTex: Fast Relightable Mesh Texturing with LightControlNet","abstract":"Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators. We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt. Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment. We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model. Our text-to-texture pipeline then constructs the texture in two stages. The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet. The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting. Our pipeline is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures.","sentences":["Manually creating textures for 3D meshes is time-consuming, even for expert visual content creators.","We propose a fast approach for automatically texturing an input 3D mesh based on a user-provided text prompt.","Importantly, our approach disentangles lighting from surface material/reflectance in the resulting texture so that the mesh can be properly relit and rendered in any lighting environment.","We introduce LightControlNet, a new text-to-image model based on the ControlNet architecture, which allows the specification of the desired lighting as a conditioning image to the model.","Our text-to-texture pipeline then constructs the texture in two stages.","The first stage produces a sparse set of visually consistent reference views of the mesh using LightControlNet.","The second stage applies a texture optimization based on Score Distillation Sampling (SDS) that works with LightControlNet to increase the texture quality while disentangling surface material from lighting.","Our pipeline is significantly faster than previous text-to-texture methods, while producing high-quality and relightable textures."],"url":"http://arxiv.org/abs/2402.13251v1","category":"cs.GR"}
{"created":"2024-02-20 18:07:59","title":"Practical Kernel Tests of Conditional Independence","abstract":"We describe a data-efficient, kernel-based approach to statistical testing of conditional independence. A major challenge of conditional independence testing, absent in tests of unconditional independence, is to obtain the correct test level (the specified upper bound on the rate of false positives), while still attaining competitive test power. Excess false positives arise due to bias in the test statistic, which is obtained using nonparametric kernel ridge regression. We propose three methods for bias control to correct the test level, based on data splitting, auxiliary data, and (where possible) simpler function classes. We show these combined strategies are effective both for synthetic and real-world data.","sentences":["We describe a data-efficient, kernel-based approach to statistical testing of conditional independence.","A major challenge of conditional independence testing, absent in tests of unconditional independence, is to obtain the correct test level (the specified upper bound on the rate of false positives), while still attaining competitive test power.","Excess false positives arise due to bias in the test statistic, which is obtained using nonparametric kernel ridge regression.","We propose three methods for bias control to correct the test level, based on data splitting, auxiliary data, and (where possible) simpler function classes.","We show these combined strategies are effective both for synthetic and real-world data."],"url":"http://arxiv.org/abs/2402.13196v1","category":"cs.LG"}
{"created":"2024-02-20 17:48:11","title":"DINOBot: Robot Manipulation via Retrieval and Alignment with Vision Foundation Models","abstract":"We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO. When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction. Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation. Videos and code are available at https://www.robot-learning.uk/dinobot.","sentences":["We propose DINOBot, a novel imitation learning framework for robot manipulation, which leverages the image-level and pixel-level capabilities of features extracted from Vision Transformers trained with DINO.","When interacting with a novel object, DINOBot first uses these features to retrieve the most visually similar object experienced during human demonstrations, and then uses this object to align its end-effector with the novel object to enable effective interaction.","Through a series of real-world experiments on everyday tasks, we show that exploiting both the image-level and pixel-level properties of vision foundation models enables unprecedented learning efficiency and generalisation.","Videos and code are available at https://www.robot-learning.uk/dinobot."],"url":"http://arxiv.org/abs/2402.13181v1","category":"cs.RO"}
{"created":"2024-02-20 16:00:41","title":"Photon Classification with Gradient Boosted Trees at CLAS12","abstract":"Dihadron semi-inclusive deep inelastic scattering (SIDIS) of 10.6 GeV longitudinally polarized electrons off the proton has been measured using the CLAS12 detector at Jefferson Lab. Two separate channels, $\\pi^+\\pi^0$ and $\\pi^-\\pi^0$, were analyzed, requiring the reconstruction of diphoton pairs. In this analysis, we addressed the problem of false neutral particles being reconstructed by CLAS12's event builder, polluting the otherwise physical combinatorial background underneath the $\\pi^0$ peak. A photon classifier using a Gradient Boosted Trees (GBTs) architecture was trained with Monte Carlo simulations to reduce the amount of background $\\pi^0$'s. We show that the nearest-neighbor features learned by the model lead to a substantial increase in signal vs. background discrimination compared to previous CLAS12 $\\pi^0$ analyses. The machine learning approach recovers several times more dihadron statistics for the dataset.","sentences":["Dihadron semi-inclusive deep inelastic scattering (SIDIS) of 10.6 GeV longitudinally polarized electrons off the proton has been measured using the CLAS12 detector at Jefferson Lab.","Two separate channels, $\\pi^+\\pi^0$ and $\\pi^-\\pi^0$, were analyzed, requiring the reconstruction of diphoton pairs.","In this analysis, we addressed the problem of false neutral particles being reconstructed by CLAS12's event builder, polluting the otherwise physical combinatorial background underneath the $\\pi^0$ peak.","A photon classifier using a Gradient Boosted Trees (GBTs) architecture was trained with Monte Carlo simulations to reduce the amount of background $\\pi^0$'s.","We show that the nearest-neighbor features learned by the model lead to a substantial increase in signal vs. background discrimination compared to previous CLAS12 $\\pi^0$ analyses.","The machine learning approach recovers several times more dihadron statistics for the dataset."],"url":"http://arxiv.org/abs/2402.13105v1","category":"hep-ex"}
{"created":"2024-02-20 15:58:45","title":"Multivariate Functional Linear Discriminant Analysis for the Classification of Short Time Series with Missing Data","abstract":"Functional linear discriminant analysis (FLDA) is a powerful tool that extends LDA-mediated multiclass classification and dimension reduction to univariate time-series functions. However, in the age of large multivariate and incomplete data, statistical dependencies between features must be estimated in a computationally tractable way, while also dealing with missing data. There is a need for a computationally tractable approach that considers the statistical dependencies between features and can handle missing values. We here develop a multivariate version of FLDA (MUDRA) to tackle this issue and describe an efficient expectation/conditional-maximization (ECM) algorithm to infer its parameters. We assess its predictive power on the \"Articulary Word Recognition\" data set and show its improvement over the state-of-the-art, especially in the case of missing data. MUDRA allows interpretable classification of data sets with large proportions of missing data, which will be particularly useful for medical or psychological data sets.","sentences":["Functional linear discriminant analysis (FLDA) is a powerful tool that extends LDA-mediated multiclass classification and dimension reduction to univariate time-series functions.","However, in the age of large multivariate and incomplete data, statistical dependencies between features must be estimated in a computationally tractable way, while also dealing with missing data.","There is a need for a computationally tractable approach that considers the statistical dependencies between features and can handle missing values.","We here develop a multivariate version of FLDA (MUDRA) to tackle this issue and describe an efficient expectation/conditional-maximization (ECM) algorithm to infer its parameters.","We assess its predictive power on the \"Articulary Word Recognition\" data set and show its improvement over the state-of-the-art, especially in the case of missing data.","MUDRA allows interpretable classification of data sets with large proportions of missing data, which will be particularly useful for medical or psychological data sets."],"url":"http://arxiv.org/abs/2402.13103v1","category":"cs.LG"}
{"created":"2024-02-20 15:40:07","title":"A Lightweight Machine Learning Approach for Delay-Aware Cell-Switching in 6G HAPS Networks","abstract":"This study investigates the integration of a high altitude platform station (HAPS), a non-terrestrial network (NTN) node, into the cell-switching paradigm for energy saving. By doing so, the sustainability and ubiquitous connectivity targets can be achieved. Besides, a delay-aware approach is also adopted, where the delay profiles of users are respected in such a way that we attempt to meet the latency requirements of users with a best-effort strategy. To this end, a novel, simple, and lightweight Q-learning algorithm is designed to address the cell-switching optimization problem. During the simulation campaigns, different interference scenarios and delay situations between base stations are examined in terms of energy consumption and quality-of-service (QoS), and the results confirm the efficacy of the proposed Q-learning algorithm.","sentences":["This study investigates the integration of a high altitude platform station (HAPS), a non-terrestrial network (NTN) node, into the cell-switching paradigm for energy saving.","By doing so, the sustainability and ubiquitous connectivity targets can be achieved.","Besides, a delay-aware approach is also adopted, where the delay profiles of users are respected in such a way that we attempt to meet the latency requirements of users with a best-effort strategy.","To this end, a novel, simple, and lightweight Q-learning algorithm is designed to address the cell-switching optimization problem.","During the simulation campaigns, different interference scenarios and delay situations between base stations are examined in terms of energy consumption and quality-of-service (QoS), and the results confirm the efficacy of the proposed Q-learning algorithm."],"url":"http://arxiv.org/abs/2402.13096v1","category":"cs.NI"}
{"created":"2024-02-20 15:28:00","title":"Kleene Theorems for Lasso Languages and $\u03c9$-Languages","abstract":"Automata operating on pairs of words were introduced as an alternative way of capturing acceptance of regular $\\omega$-languages. Families of DFAs and lasso automata operating on such pairs followed, giving rise to minimisation algorithms, a Myhill-Nerode theorem and language learning algorithms. Yet Kleene theorems for such a well-established class are still missing. Here, we introduce rational lasso languages and expressions, show a Kleene theorem for lasso languages and explore the connection between rational lasso and $\\omega$-expressions, which yields a Kleene theorem for $\\omega$-languages with respect to saturated lasso automata. For one direction of the Kleene theorems, we also provide a Brzozowski construction for lasso automata from rational lasso expressions.","sentences":["Automata operating on pairs of words were introduced as an alternative way of capturing acceptance of regular $\\omega$-languages.","Families of DFAs and lasso automata operating on such pairs followed, giving rise to minimisation algorithms, a Myhill-Nerode theorem and language learning algorithms.","Yet Kleene theorems for such a well-established class are still missing.","Here, we introduce rational lasso languages and expressions, show a Kleene theorem for lasso languages and explore the connection between rational lasso and $\\omega$-expressions, which yields a Kleene theorem for $\\omega$-languages with respect to saturated lasso automata.","For one direction of the Kleene theorems, we also provide a Brzozowski construction for lasso automata from rational lasso expressions."],"url":"http://arxiv.org/abs/2402.13085v1","category":"cs.FL"}
{"created":"2024-02-20 15:22:25","title":"Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition","abstract":"Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience. This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time factor.","sentences":["Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience.","This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models.","We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory.","Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models.","These guidelines focus on minimizing power use without substantially affecting accuracy.","Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods.","It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time factor."],"url":"http://arxiv.org/abs/2402.13076v1","category":"cs.SD"}
{"created":"2024-02-20 14:56:28","title":"Toward Fairness via Maximum Mean Discrepancy Regularization on Logits Space","abstract":"Fairness has become increasingly pivotal in machine learning for high-risk applications such as machine learning in healthcare and facial recognition. However, we see the deficiency in the previous logits space constraint methods. Therefore, we propose a novel framework, Logits-MMD, that achieves the fairness condition by imposing constraints on output logits with Maximum Mean Discrepancy. Moreover, quantitative analysis and experimental results show that our framework has a better property that outperforms previous methods and achieves state-of-the-art on two facial recognition datasets and one animal dataset. Finally, we show experimental results and demonstrate that our debias approach achieves the fairness condition effectively.","sentences":["Fairness has become increasingly pivotal in machine learning for high-risk applications such as machine learning in healthcare and facial recognition.","However, we see the deficiency in the previous logits space constraint methods.","Therefore, we propose a novel framework, Logits-MMD, that achieves the fairness condition by imposing constraints on output logits with Maximum Mean Discrepancy.","Moreover, quantitative analysis and experimental results show that our framework has a better property that outperforms previous methods and achieves state-of-the-art on two facial recognition datasets and one animal dataset.","Finally, we show experimental results and demonstrate that our debias approach achieves the fairness condition effectively."],"url":"http://arxiv.org/abs/2402.13061v1","category":"cs.CV"}
{"created":"2024-02-20 13:59:12","title":"Understanding the effects of language-specific class imbalance in multilingual fine-tuning","abstract":"We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages. We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features. We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects. These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task.","sentences":["We study the effect of one type of imbalance often present in real-life multilingual classification datasets: an uneven distribution of labels across languages.","We show evidence that fine-tuning a transformer-based Large Language Model (LLM) on a dataset with this imbalance leads to worse performance, a more pronounced separation of languages in the latent space, and the promotion of uninformative features.","We modify the traditional class weighing approach to imbalance by calculating class weights separately for each language and show that this helps mitigate those detrimental effects.","These results create awareness of the negative effects of language-specific class imbalance in multilingual fine-tuning and the way in which the model learns to rely on the separation of languages to perform the task."],"url":"http://arxiv.org/abs/2402.13016v1","category":"cs.CL"}
{"created":"2024-02-20 13:18:33","title":"Tactile Perception in Upper Limb Prostheses: Mechanical Characterization, Human Experiments, and Computational Findings","abstract":"Our research investigates vibrotactile perception in four prosthetic hands with distinct kinematics and mechanical characteristics. We found that rigid and simple socket-based prosthetic devices can transmit tactile information and surprisingly enable users to identify the stimulated finger with high reliability. This ability decreases with more advanced prosthetic hands with additional articulations and softer mechanics. We conducted experiments to understand the underlying mechanisms. We assessed a prosthetic user's ability to discriminate finger contacts based on vibrations transmitted through the four prosthetic hands. We also performed numerical and mechanical vibration tests on the prostheses and used a machine learning classifier to identify the contacted finger. Our results show that simpler and rigid prosthetic hands facilitate contact discrimination (for instance, a user of a purely cosmetic hand can distinguish a contact on the index finger from other fingers with 83% accuracy), but all tested hands, including soft advanced ones, performed above chance level. Despite advanced hands reducing vibration transmission, a machine learning algorithm still exceeded human performance in discriminating finger contacts. These findings suggest the potential for enhancing vibrotactile feedback in advanced prosthetic hands and lay the groundwork for future integration of such feedback in prosthetic devices.","sentences":["Our research investigates vibrotactile perception in four prosthetic hands with distinct kinematics and mechanical characteristics.","We found that rigid and simple socket-based prosthetic devices can transmit tactile information and surprisingly enable users to identify the stimulated finger with high reliability.","This ability decreases with more advanced prosthetic hands with additional articulations and softer mechanics.","We conducted experiments to understand the underlying mechanisms.","We assessed a prosthetic user's ability to discriminate finger contacts based on vibrations transmitted through the four prosthetic hands.","We also performed numerical and mechanical vibration tests on the prostheses and used a machine learning classifier to identify the contacted finger.","Our results show that simpler and rigid prosthetic hands facilitate contact discrimination (for instance, a user of a purely cosmetic hand can distinguish a contact on the index finger from other fingers with 83% accuracy), but all tested hands, including soft advanced ones, performed above chance level.","Despite advanced hands reducing vibration transmission, a machine learning algorithm still exceeded human performance in discriminating finger contacts.","These findings suggest the potential for enhancing vibrotactile feedback in advanced prosthetic hands and lay the groundwork for future integration of such feedback in prosthetic devices."],"url":"http://arxiv.org/abs/2402.12989v1","category":"cs.RO"}
{"created":"2024-02-20 11:15:13","title":"Right on Time: Revising Time Series Models by Constraining their Explanations","abstract":"The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT). Our method enables interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets.","sentences":["The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results.","Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this.","To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT).","Our method enables interactions with model explanations across both the time and frequency domain.","Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors.","The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets.","We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets."],"url":"http://arxiv.org/abs/2402.12921v1","category":"cs.LG"}
{"created":"2024-02-20 10:34:19","title":"More Discriminative Sentence Embeddings via Semantic Graph Smoothing","abstract":"This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion. Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks. Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks.","sentences":["This paper explores an empirical approach to learn more discriminantive sentence representations in an unsupervised fashion.","Leveraging semantic graph smoothing, we enhance sentence embeddings obtained from pretrained models to improve results for the text clustering and classification tasks.","Our method, validated on eight benchmarks, demonstrates consistent improvements, showcasing the potential of semantic graph smoothing in improving sentence embeddings for the supervised and unsupervised document categorization tasks."],"url":"http://arxiv.org/abs/2402.12890v1","category":"cs.CL"}
{"created":"2024-02-20 10:25:44","title":"A Bound on the Maximal Marginal Degrees of Freedom","abstract":"Common kernel ridge regression is expensive in memory allocation and computation time. This paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties. The fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable. The bound relates the effective dimension with the largest statistical leverage score. We characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel. This growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the Nystr\\\"om method.","sentences":["Common kernel ridge regression is expensive in memory allocation and computation time.","This paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties.","The fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable.","The bound relates the effective dimension with the largest statistical leverage score.","We characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel.","This growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the Nystr\\\"om method."],"url":"http://arxiv.org/abs/2402.12885v1","category":"stat.ML"}
{"created":"2024-02-20 10:13:44","title":"Federated Multi-Task Learning on Non-IID Data Silos: An Experimental Study","abstract":"The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets. However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field. This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm. This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios. We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption. Through our exhaustive experiments, we aim to provide valuable insights into the strengths and limitations of existing baseline methods, contributing to the ongoing discourse on optimal FMTL application in practical scenarios. The source code will be made available for results replication.","sentences":["The innovative Federated Multi-Task Learning (FMTL) approach consolidates the benefits of Federated Learning (FL) and Multi-Task Learning (MTL), enabling collaborative model training on multi-task learning datasets.","However, a comprehensive evaluation method, integrating the unique features of both FL and MTL, is currently absent in the field.","This paper fills this void by introducing a novel framework, FMTL-Bench, for systematic evaluation of the FMTL paradigm.","This benchmark covers various aspects at the data, model, and optimization algorithm levels, and comprises seven sets of comparative experiments, encapsulating a wide array of non-independent and identically distributed (Non-IID) data partitioning scenarios.","We propose a systematic process for comparing baselines of diverse indicators and conduct a case study on communication expenditure, time, and energy consumption.","Through our exhaustive experiments, we aim to provide valuable insights into the strengths and limitations of existing baseline methods, contributing to the ongoing discourse on optimal FMTL application in practical scenarios.","The source code will be made available for results replication."],"url":"http://arxiv.org/abs/2402.12876v1","category":"cs.LG"}
{"created":"2024-02-20 10:09:00","title":"Skill or Luck? Return Decomposition via Advantage Functions","abstract":"Learning from off-policy data is essential for sample-efficient reinforcement learning. In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck). Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE). The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions. We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important. Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance.","sentences":["Learning from off-policy data is essential for sample-efficient reinforcement learning.","In the present work, we build on the insight that the advantage function can be understood as the causal effect of an action on the return, and show that this allows us to decompose the return of a trajectory into parts caused by the agent's actions (skill) and parts outside of the agent's control (luck).","Furthermore, this decomposition enables us to naturally extend Direct Advantage Estimation (DAE) to off-policy settings (Off-policy DAE).","The resulting method can learn from off-policy trajectories without relying on importance sampling techniques or truncating off-policy actions.","We draw connections between Off-policy DAE and previous methods to demonstrate how it can speed up learning and when the proposed off-policy corrections are important.","Finally, we use the MinAtar environments to illustrate how ignoring off-policy corrections can lead to suboptimal policy optimization performance."],"url":"http://arxiv.org/abs/2402.12874v1","category":"cs.LG"}
{"created":"2024-02-20 09:31:03","title":"CCFC++: Enhancing Federated Clustering through Feature Decorrelation","abstract":"In federated clustering, multiple data-holding clients collaboratively group data without exchanging raw data. This field has seen notable advancements through its marriage with contrastive learning, exemplified by Cluster-Contrastive Federated Clustering (CCFC). However, CCFC suffers from heterogeneous data across clients, leading to poor and unrobust performance. Our study conducts both empirical and theoretical analyses to understand the impact of heterogeneous data on CCFC. Findings indicate that increased data heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased correlations across multiple dimensions of the learned representations. To address this, we introduce a decorrelation regularizer to CCFC. Benefiting from the regularizer, the improved method effectively mitigates the detrimental effects of data heterogeneity, and achieves superior performance, as evidenced by a marked increase in NMI scores, with the gain reaching as high as 0.32 in the most pronounced case.","sentences":["In federated clustering, multiple data-holding clients collaboratively group data without exchanging raw data.","This field has seen notable advancements through its marriage with contrastive learning, exemplified by Cluster-Contrastive Federated Clustering (CCFC).","However, CCFC suffers from heterogeneous data across clients, leading to poor and unrobust performance.","Our study conducts both empirical and theoretical analyses to understand the impact of heterogeneous data on CCFC.","Findings indicate that increased data heterogeneity exacerbates dimensional collapse in CCFC, evidenced by increased correlations across multiple dimensions of the learned representations.","To address this, we introduce a decorrelation regularizer to CCFC.","Benefiting from the regularizer, the improved method effectively mitigates the detrimental effects of data heterogeneity, and achieves superior performance, as evidenced by a marked increase in NMI scores, with the gain reaching as high as 0.32 in the most pronounced case."],"url":"http://arxiv.org/abs/2402.12852v1","category":"cs.LG"}
{"created":"2024-02-20 08:31:42","title":"Scaling Laws Behind Code Understanding Model","abstract":"The scaling law is becoming a fundamental law in many machine learning areas. That is, test error falls off with the power law when increasing training data, model size, and computing resource. However, whether this law is suitable for the task of code understanding is not well studied, and most current language models for code understanding are about 100M parameters, which are relatively \"small\" compared to large language models. In this paper, we conduct extensive experiments to investigate the scaling law for the code understanding task by varying training data, model size, and computing resource. We validate that the test error of code understanding models falls off with the power law when using larger models, indicating that the scaling law is suitable for the code understanding task. Besides, we apply different scales of models to two downstream code understanding tasks, and find that the performance increases with larger scale of models. Finally, we train a large-scale code understanding model named CoLSBERT with 1.5B parameters on a large dataset using more computing resource, which outperforms previous work by a large margin. We will release our code and the CoLSBERT model when our paper is published.","sentences":["The scaling law is becoming a fundamental law in many machine learning areas.","That is, test error falls off with the power law when increasing training data, model size, and computing resource.","However, whether this law is suitable for the task of code understanding is not well studied, and most current language models for code understanding are about 100M parameters, which are relatively \"small\" compared to large language models.","In this paper, we conduct extensive experiments to investigate the scaling law for the code understanding task by varying training data, model size, and computing resource.","We validate that the test error of code understanding models falls off with the power law when using larger models, indicating that the scaling law is suitable for the code understanding task.","Besides, we apply different scales of models to two downstream code understanding tasks, and find that the performance increases with larger scale of models.","Finally, we train a large-scale code understanding model named CoLSBERT with 1.5B parameters on a large dataset using more computing resource, which outperforms previous work by a large margin.","We will release our code and the CoLSBERT model when our paper is published."],"url":"http://arxiv.org/abs/2402.12813v1","category":"cs.SE"}
{"created":"2024-02-20 07:58:04","title":"From Movements to Metrics: Evaluating Explainable AI Methods in Skeleton-Based Human Activity Recognition","abstract":"The advancement of deep learning in human activity recognition (HAR) using 3D skeleton data is critical for applications in healthcare, security, sports, and human-computer interaction. This paper tackles a well-known gap in the field, which is the lack of testing in the applicability and reliability of XAI evaluation metrics in the skeleton-based HAR domain. We have tested established XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this problem. The study also introduces a perturbation method that respects human biomechanical constraints to ensure realistic variations in human movement. Our findings indicate that \\textit{faithfulness} may not be a reliable metric in certain contexts, such as with the EfficientGCN model. Conversely, stability emerges as a more dependable metric when there is slight input data perturbations. CAM and Grad-CAM are also found to produce almost identical explanations, leading to very similar XAI metric performance. This calls for the need for more diversified metrics and new XAI methods applied in skeleton-based HAR.","sentences":["The advancement of deep learning in human activity recognition (HAR) using 3D skeleton data is critical for applications in healthcare, security, sports, and human-computer interaction.","This paper tackles a well-known gap in the field, which is the lack of testing in the applicability and reliability of XAI evaluation metrics in the skeleton-based HAR domain.","We have tested established XAI metrics namely faithfulness and stability on Class Activation Mapping (CAM) and Gradient-weighted Class Activation Mapping (Grad-CAM) to address this problem.","The study also introduces a perturbation method that respects human biomechanical constraints to ensure realistic variations in human movement.","Our findings indicate that \\textit{faithfulness} may not be a reliable metric in certain contexts, such as with the EfficientGCN model.","Conversely, stability emerges as a more dependable metric when there is slight input data perturbations.","CAM and Grad-CAM are also found to produce almost identical explanations, leading to very similar XAI metric performance.","This calls for the need for more diversified metrics and new XAI methods applied in skeleton-based HAR."],"url":"http://arxiv.org/abs/2402.12790v1","category":"cs.LG"}
{"created":"2024-02-20 07:31:33","title":"Application of Quantum Extreme Learning Machines for QoS Prediction of Elevators' Software in an Industrial Context","abstract":"Quantum Extreme Learning Machine (QELM) is an emerging technique that utilizes quantum dynamics and an easy-training strategy to solve problems such as classification and regression efficiently. Although QELM has many potential benefits, its real-world applications remain limited. To this end, we present QELM's industrial application in the context of elevators, by proposing an approach called QUELL. In QUELL, we use QELM for the waiting time prediction related to the scheduling software of elevators, with applications for software regression testing, elevator digital twins, and real-time performance prediction. The scheduling software has been implemented by our industrial partner Orona, a globally recognized leader in elevator technology. We demonstrate that QUELL can efficiently predict waiting times, with prediction quality significantly better than that of classical ML models employed in a state-of-the-practice approach. Moreover, we show that the prediction quality of QUELL does not degrade when using fewer features. Based on our industrial application, we further provide insights into using QELM in other applications in Orona, and discuss how QELM could be applied to other industrial applications.","sentences":["Quantum Extreme Learning Machine (QELM) is an emerging technique that utilizes quantum dynamics and an easy-training strategy to solve problems such as classification and regression efficiently.","Although QELM has many potential benefits, its real-world applications remain limited.","To this end, we present QELM's industrial application in the context of elevators, by proposing an approach called QUELL.","In QUELL, we use QELM for the waiting time prediction related to the scheduling software of elevators, with applications for software regression testing, elevator digital twins, and real-time performance prediction.","The scheduling software has been implemented by our industrial partner Orona, a globally recognized leader in elevator technology.","We demonstrate that QUELL can efficiently predict waiting times, with prediction quality significantly better than that of classical ML models employed in a state-of-the-practice approach.","Moreover, we show that the prediction quality of QUELL does not degrade when using fewer features.","Based on our industrial application, we further provide insights into using QELM in other applications in Orona, and discuss how QELM could be applied to other industrial applications."],"url":"http://arxiv.org/abs/2402.12777v1","category":"cs.SE"}
{"created":"2024-02-20 06:22:02","title":"Surrogate Models for Vibrational Entropy Based on a Spatial Decomposition","abstract":"The temperature-dependent behavior of defect densities within a crystalline structure is intricately linked to the phenomenon of vibrational entropy. Traditional methods for evaluating vibrational entropy are computationally intensive, limiting their practical utility. We show that total entropy can be decomposed into atomic site contributions and rigorously estimate the locality of site entropy. This analysis suggests that vibrational entropy can be effectively predicted using a surrogate model for site entropy. We employ machine learning to develop such a surrogate models employing the Atomic Cluster Expansion model. We supplement our rigorous analysis with an empirical convergence study. In addition we demonstrate the performance of our method for predicting vibrational formation entropy and attempt frequency of the transition rates, on point defects such as vacancies and interstitials.","sentences":["The temperature-dependent behavior of defect densities within a crystalline structure is intricately linked to the phenomenon of vibrational entropy.","Traditional methods for evaluating vibrational entropy are computationally intensive, limiting their practical utility.","We show that total entropy can be decomposed into atomic site contributions and rigorously estimate the locality of site entropy.","This analysis suggests that vibrational entropy can be effectively predicted using a surrogate model for site entropy.","We employ machine learning to develop such a surrogate models employing the Atomic Cluster Expansion model.","We supplement our rigorous analysis with an empirical convergence study.","In addition we demonstrate the performance of our method for predicting vibrational formation entropy and attempt frequency of the transition rates, on point defects such as vacancies and interstitials."],"url":"http://arxiv.org/abs/2402.12744v1","category":"physics.comp-ph"}
{"created":"2024-02-20 06:04:44","title":"Guarantee Regions for Local Explanations","abstract":"Interpretability methods that utilise local surrogate models (e.g. LIME) are very good at describing the behaviour of the predictive model at a point of interest, but they are not guaranteed to extrapolate to the local region surrounding the point. However, overfitting to the local curvature of the predictive model and malicious tampering can significantly limit extrapolation. We propose an anchor-based algorithm for identifying regions in which local explanations are guaranteed to be correct by explicitly describing those intervals along which the input features can be trusted. Our method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model. We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines. We also show how our method can identify misleading local explanations with significantly poorer guarantee regions.","sentences":["Interpretability methods that utilise local surrogate models (e.g. LIME) are very good at describing the behaviour of the predictive model at a point of interest, but they are not guaranteed to extrapolate to the local region surrounding the point.","However, overfitting to the local curvature of the predictive model and malicious tampering can significantly limit extrapolation.","We propose an anchor-based algorithm for identifying regions in which local explanations are guaranteed to be correct by explicitly describing those intervals along which the input features can be trusted.","Our method produces an interpretable feature-aligned box where the prediction of the local surrogate model is guaranteed to match the predictive model.","We demonstrate that our algorithm can be used to find explanations with larger guarantee regions that better cover the data manifold compared to existing baselines.","We also show how our method can identify misleading local explanations with significantly poorer guarantee regions."],"url":"http://arxiv.org/abs/2402.12737v1","category":"cs.LG"}
{"created":"2024-02-20 04:40:00","title":"Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules","abstract":"Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experimental evaluations on a diverse group of benchmarks, including ligand binding affinity prediction, molecular property prediction, and protein property prediction, show that EPT significantly outperforms previous SOTA methods for affinity prediction, and achieves the best or comparable performance with existing domain-specific pretraining models for other tasks.","sentences":["Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications.","However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge.","To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins.","To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom.","Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures.","Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins.","Experimental evaluations on a diverse group of benchmarks, including ligand binding affinity prediction, molecular property prediction, and protein property prediction, show that EPT significantly outperforms previous SOTA methods for affinity prediction, and achieves the best or comparable performance with existing domain-specific pretraining models for other tasks."],"url":"http://arxiv.org/abs/2402.12714v1","category":"cs.LG"}
{"created":"2024-02-20 04:21:13","title":"Achieving Near-Optimal Regret for Bandit Algorithms with Uniform Last-Iterate Guarantee","abstract":"Existing performance measures for bandit algorithms such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications. This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms. Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t. (large) round t, preventing revisits to bad arms when sufficient samples are available. We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures. To examine the achievability of ULI in the finite arm setting, we first provide two positive results that some elimination-based algorithms and high-probability adversarial algorithms with stronger analysis or additional designs, can attain near-optimal ULI guarantees. Then, we also provide a negative result, indicating that optimistic algorithms cannot achieve a near-optimal ULI guarantee. Finally, we propose an efficient algorithm for linear bandits with infinitely many arms, which achieves the ULI guarantee, given access to an optimization oracle.","sentences":["Existing performance measures for bandit algorithms such as regret, PAC bounds, or uniform-PAC (Dann et al., 2017), typically evaluate the cumulative performance, while allowing the play of an arbitrarily bad arm at any finite time t. Such a behavior can be highly detrimental in high-stakes applications.","This paper introduces a stronger performance measure, the uniform last-iterate (ULI) guarantee, capturing both cumulative and instantaneous performance of bandit algorithms.","Specifically, ULI characterizes the instantaneous performance since it ensures that the per-round regret of the played arm is bounded by a function, monotonically decreasing w.r.t.","(large) round t, preventing revisits to bad arms when sufficient samples are available.","We demonstrate that a near-optimal ULI guarantee directly implies near-optimal cumulative performance across aforementioned performance measures.","To examine the achievability of ULI in the finite arm setting, we first provide two positive results that some elimination-based algorithms and high-probability adversarial algorithms with stronger analysis or additional designs, can attain near-optimal ULI guarantees.","Then, we also provide a negative result, indicating that optimistic algorithms cannot achieve a near-optimal ULI guarantee.","Finally, we propose an efficient algorithm for linear bandits with infinitely many arms, which achieves the ULI guarantee, given access to an optimization oracle."],"url":"http://arxiv.org/abs/2402.12711v1","category":"cs.LG"}
{"created":"2024-02-20 04:06:28","title":"Quantum Embedding with Transformer for High-dimensional Data","abstract":"Quantum embedding with transformers is a novel and promising architecture for quantum machine learning to deliver exceptional capability on near-term devices or simulators. The research incorporated a vision transformer (ViT) to advance quantum significantly embedding ability and results for a single qubit classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a challenging high-dimensional dataset. The study showcases and analyzes empirical evidence that our transformer-based architecture is a highly versatile and practical approach to modern quantum machine learning problems.","sentences":["Quantum embedding with transformers is a novel and promising architecture for quantum machine learning to deliver exceptional capability on near-term devices or simulators.","The research incorporated a vision transformer (ViT) to advance quantum significantly embedding ability and results for a single qubit classifier with around 3 percent in the median F1 score on the BirdCLEF-2021, a challenging high-dimensional dataset.","The study showcases and analyzes empirical evidence that our transformer-based architecture is a highly versatile and practical approach to modern quantum machine learning problems."],"url":"http://arxiv.org/abs/2402.12704v1","category":"quant-ph"}
{"created":"2024-02-20 03:57:16","title":"wmh_seg: Transformer based U-Net for Robust and Automatic White Matter Hyperintensity Segmentation across 1.5T, 3T and 7T","abstract":"White matter hyperintensity (WMH) remains the top imaging biomarker for neurodegenerative diseases. Robust and accurate segmentation of WMH holds paramount significance for neuroimaging studies. The growing shift from 3T to 7T MRI necessitates robust tools for harmonized segmentation across field strengths and artifacts. Recent deep learning models exhibit promise in WMH segmentation but still face challenges, including diverse training data representation and limited analysis of MRI artifacts' impact. To address these, we introduce wmh_seg, a novel deep learning model leveraging a transformer-based encoder from SegFormer. wmh_seg is trained on an unmatched dataset, including 1.5T, 3T, and 7T FLAIR images from various sources, alongside with artificially added MR artifacts. Our approach bridges gaps in training diversity and artifact analysis. Our model demonstrated stable performance across magnetic field strengths, scanner manufacturers, and common MR imaging artifacts. Despite the unique inhomogeneity artifacts on ultra-high field MR images, our model still offers robust and stable segmentation on 7T FLAIR images. Our model, to date, is the first that offers quality white matter lesion segmentation on 7T FLAIR images.","sentences":["White matter hyperintensity (WMH) remains the top imaging biomarker for neurodegenerative diseases.","Robust and accurate segmentation of WMH holds paramount significance for neuroimaging studies.","The growing shift from 3T to 7T MRI necessitates robust tools for harmonized segmentation across field strengths and artifacts.","Recent deep learning models exhibit promise in WMH segmentation but still face challenges, including diverse training data representation and limited analysis of MRI artifacts' impact.","To address these, we introduce wmh_seg, a novel deep learning model leveraging a transformer-based encoder from SegFormer.","wmh_seg is trained on an unmatched dataset, including 1.5T, 3T, and 7T FLAIR images from various sources, alongside with artificially added MR artifacts.","Our approach bridges gaps in training diversity and artifact analysis.","Our model demonstrated stable performance across magnetic field strengths, scanner manufacturers, and common MR imaging artifacts.","Despite the unique inhomogeneity artifacts on ultra-high field MR images, our model still offers robust and stable segmentation on 7T FLAIR images.","Our model, to date, is the first that offers quality white matter lesion segmentation on 7T FLAIR images."],"url":"http://arxiv.org/abs/2402.12701v1","category":"eess.IV"}
{"created":"2024-02-20 03:45:59","title":"Revitalizing Multivariate Time Series Forecasting: Learnable Decomposition with Inter-Series Dependencies and Intra-Series Variations Modeling","abstract":"Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations. Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data. Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably. Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention. To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods. Through the comparison results, our Leddam (LEarnable Decomposition and Dual Attention Module) not only demonstrates significant advancements in predictive performance, but also the proposed decomposition strategy can be plugged into other methods with a large performance-boosting, from 11.87% to 48.56% MSE error degradation.","sentences":["Predicting multivariate time series is crucial, demanding precise modeling of intricate patterns, including inter-series dependencies and intra-series variations.","Distinctive trend characteristics in each time series pose challenges, and existing methods, relying on basic moving average kernels, may struggle with the non-linear structure and complex trends in real-world data.","Given that, we introduce a learnable decomposition strategy to capture dynamic trend information more reasonably.","Additionally, we propose a dual attention module tailored to capture inter-series dependencies and intra-series variations simultaneously for better time series forecasting, which is implemented by channel-wise self-attention and autoregressive self-attention.","To evaluate the effectiveness of our method, we conducted experiments across eight open-source datasets and compared it with the state-of-the-art methods.","Through the comparison results, our Leddam (LEarnable Decomposition and Dual Attention Module) not only demonstrates significant advancements in predictive performance, but also the proposed decomposition strategy can be plugged into other methods with a large performance-boosting, from 11.87% to 48.56% MSE error degradation."],"url":"http://arxiv.org/abs/2402.12694v1","category":"cs.LG"}
{"created":"2024-02-20 03:37:24","title":"Tree-Planted Transformers: Large Language Models with Implicit Syntactic Supervision","abstract":"Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency. In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability. Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM). In this paper, we propose a novel method dubbed tree-planting: implicitly \"plant\" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language. Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via continual learning with syntactic scaffolding. Targeted syntactic evaluations on the SyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit syntactic supervision, significantly outperformed various SLMs with explicit syntactic supervision that generate hundreds of syntactic structures in parallel, suggesting that tree-planting and TPTs are the promising foundation for SLLMs.","sentences":["Large Language Models (LLMs) have achieved remarkable success thanks to scalability on large text corpora, but have some drawback in training efficiency.","In contrast, Syntactic Language Models (SLMs) can be trained efficiently to reach relatively high performance thanks to syntactic supervision, but have trouble with scalability.","Thus, given these complementary advantages of LLMs and SLMs, it is necessary to develop an architecture that integrates the scalability of LLMs with the training efficiency of SLMs, namely Syntactic Large Language Models (SLLM).","In this paper, we propose a novel method dubbed tree-planting: implicitly \"plant\" trees into attention weights of Transformer LMs to reflect syntactic structures of natural language.","Specifically, Transformer LMs trained with tree-planting will be called Tree-Planted Transformers (TPT), which learn syntax on small treebanks via tree-planting and then scale on large text corpora via continual learning with syntactic scaffolding.","Targeted syntactic evaluations on the SyntaxGym benchmark demonstrated that TPTs, despite the lack of explicit syntactic supervision, significantly outperformed various SLMs with explicit syntactic supervision that generate hundreds of syntactic structures in parallel, suggesting that tree-planting and TPTs are the promising foundation for SLLMs."],"url":"http://arxiv.org/abs/2402.12691v1","category":"cs.CL"}
{"created":"2024-02-20 02:48:58","title":"Advancing Monocular Video-Based Gait Analysis Using Motion Imitation with Physics-Based Simulation","abstract":"Gait analysis from videos obtained from a smartphone would open up many clinical opportunities for detecting and quantifying gait impairments. However, existing approaches for estimating gait parameters from videos can produce physically implausible results. To overcome this, we train a policy using reinforcement learning to control a physics simulation of human movement to replicate the movement seen in video. This forces the inferred movements to be physically plausible, while improving the accuracy of the inferred step length and walking velocity.","sentences":["Gait analysis from videos obtained from a smartphone would open up many clinical opportunities for detecting and quantifying gait impairments.","However, existing approaches for estimating gait parameters from videos can produce physically implausible results.","To overcome this, we train a policy using reinforcement learning to control a physics simulation of human movement to replicate the movement seen in video.","This forces the inferred movements to be physically plausible, while improving the accuracy of the inferred step length and walking velocity."],"url":"http://arxiv.org/abs/2402.12676v1","category":"cs.CV"}
{"created":"2024-02-20 02:36:26","title":"Randomization Can Reduce Both Bias and Variance: A Case Study in Random Forests","abstract":"We study the often overlooked phenomenon, first noted in \\cite{breiman2001random}, that random forests appear to reduce bias compared to bagging. Motivated by an interesting paper by \\cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (SNR) settings, we explore how random forests can uncover patterns in the data missed by bagging. We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when SNR is high. Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split. Our investigations also yield practical insights into the importance of tuning $mtry$ in random forests.","sentences":["We study the often overlooked phenomenon, first noted in \\cite{breiman2001random}, that random forests appear to reduce bias compared to bagging.","Motivated by an interesting paper by \\cite{mentch2020randomization}, where the authors argue that random forests reduce effective degrees of freedom and only outperform bagging ensembles in low signal-to-noise ratio (SNR) settings, we explore how random forests can uncover patterns in the data missed by bagging.","We empirically demonstrate that in the presence of such patterns, random forests reduce bias along with variance and increasingly outperform bagging ensembles when SNR is high.","Our observations offer insights into the real-world success of random forests across a range of SNRs and enhance our understanding of the difference between random forests and bagging ensembles with respect to the randomization injected into each split.","Our investigations also yield practical insights into the importance of tuning $mtry$ in random forests."],"url":"http://arxiv.org/abs/2402.12668v1","category":"stat.ML"}
{"created":"2024-02-20 02:35:15","title":"Remote Possibilities: Where there is a WIL, is there a Way? AI Education for Remote Learners in a New Era of Work-Integrated-Learning","abstract":"Increasing diversity in educational settings is challenging in part due to the lack of access to resources for non-traditional learners in remote communities. Post-pandemic platforms designed specifically for remote and hybrid learning -- supporting team-based collaboration online -- are positioned to bridge this gap. Our work combines the use of these new platforms with co-creation and collaboration tools for AI assisted remote Work-Integrated-Learning (WIL) opportunities, including efforts in community and with the public library system. This paper outlines some of our experiences to date, and proposes methods to further integrate AI education into community-driven applications for remote WIL.","sentences":["Increasing diversity in educational settings is challenging in part due to the lack of access to resources for non-traditional learners in remote communities.","Post-pandemic platforms designed specifically for remote and hybrid learning -- supporting team-based collaboration online -- are positioned to bridge this gap.","Our work combines the use of these new platforms with co-creation and collaboration tools for AI assisted remote Work-Integrated-Learning (WIL) opportunities, including efforts in community and with the public library system.","This paper outlines some of our experiences to date, and proposes methods to further integrate AI education into community-driven applications for remote WIL."],"url":"http://arxiv.org/abs/2402.12667v1","category":"cs.CY"}
{"created":"2024-02-20 02:23:15","title":"SoftQE: Learned Representations of Queries Expanded by LLMs","abstract":"We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time. SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries. While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks.","sentences":["We investigate the integration of Large Language Models (LLMs) into query encoders to improve dense retrieval without increasing latency and cost, by circumventing the dependency on LLMs at inference time.","SoftQE incorporates knowledge from LLMs by mapping embeddings of input queries to those of the LLM-expanded queries.","While improvements over various strong baselines on in-domain MS-MARCO metrics are marginal, SoftQE improves performance by 2.83 absolute percentage points on average on five out-of-domain BEIR tasks."],"url":"http://arxiv.org/abs/2402.12663v1","category":"cs.CL"}
{"created":"2024-02-20 02:16:24","title":"SingVisio: Visual Analytics of Diffusion Model for Singing Voice Conversion","abstract":"In this study, we present SingVisio, an interactive visual analysis system that aims to explain the diffusion model used in singing voice conversion. SingVisio provides a visual display of the generation process in diffusion models, showcasing the step-by-step denoising of the noisy spectrum and its transformation into a clean spectrum that captures the desired singer's timbre. The system also facilitates side-by-side comparisons of different conditions, such as source content, melody, and target timbre, highlighting the impact of these conditions on the diffusion generation process and resulting conversions. Through comprehensive evaluations, SingVisio demonstrates its effectiveness in terms of system design, functionality, explainability, and user-friendliness. It offers users of various backgrounds valuable learning experiences and insights into the diffusion model for singing voice conversion.","sentences":["In this study, we present SingVisio, an interactive visual analysis system that aims to explain the diffusion model used in singing voice conversion.","SingVisio provides a visual display of the generation process in diffusion models, showcasing the step-by-step denoising of the noisy spectrum and its transformation into a clean spectrum that captures the desired singer's timbre.","The system also facilitates side-by-side comparisons of different conditions, such as source content, melody, and target timbre, highlighting the impact of these conditions on the diffusion generation process and resulting conversions.","Through comprehensive evaluations, SingVisio demonstrates its effectiveness in terms of system design, functionality, explainability, and user-friendliness.","It offers users of various backgrounds valuable learning experiences and insights into the diffusion model for singing voice conversion."],"url":"http://arxiv.org/abs/2402.12660v1","category":"cs.SD"}
{"created":"2024-02-20 02:14:45","title":"Guiding the underwater acoustic target recognition with interpretable contrastive learning","abstract":"Recognizing underwater targets from acoustic signals is a challenging task owing to the intricate ocean environments and variable underwater channels. While deep learning-based systems have become the mainstream approach for underwater acoustic target recognition, they have faced criticism for their lack of interpretability and weak generalization performance in practical applications. In this work, we apply the class activation mapping (CAM) to generate visual explanations for the predictions of a spectrogram-based recognition system. CAM can help to understand the behavior of recognition models by highlighting the regions of the input features that contribute the most to the prediction. Our explorations reveal that recognition models tend to focus on the low-frequency line spectrum and high-frequency periodic modulation information of underwater signals. Based on the observation, we propose an interpretable contrastive learning (ICL) strategy that employs two encoders to learn from acoustic features with different emphases (line spectrum and modulation information). By imposing constraints between encoders, the proposed strategy can enhance the generalization performance of the recognition system. Our experiments demonstrate that the proposed contrastive learning approach can improve the recognition accuracy and bring significant improvements across various underwater databases.","sentences":["Recognizing underwater targets from acoustic signals is a challenging task owing to the intricate ocean environments and variable underwater channels.","While deep learning-based systems have become the mainstream approach for underwater acoustic target recognition, they have faced criticism for their lack of interpretability and weak generalization performance in practical applications.","In this work, we apply the class activation mapping (CAM) to generate visual explanations for the predictions of a spectrogram-based recognition system.","CAM can help to understand the behavior of recognition models by highlighting the regions of the input features that contribute the most to the prediction.","Our explorations reveal that recognition models tend to focus on the low-frequency line spectrum and high-frequency periodic modulation information of underwater signals.","Based on the observation, we propose an interpretable contrastive learning (ICL) strategy that employs two encoders to learn from acoustic features with different emphases (line spectrum and modulation information).","By imposing constraints between encoders, the proposed strategy can enhance the generalization performance of the recognition system.","Our experiments demonstrate that the proposed contrastive learning approach can improve the recognition accuracy and bring significant improvements across various underwater databases."],"url":"http://arxiv.org/abs/2402.12658v1","category":"cs.SD"}
{"created":"2024-02-20 01:35:23","title":"YOLO-Ant: A Lightweight Detector via Depthwise Separable Convolutional and Large Kernel Design for Antenna Interference Source Detection","abstract":"In the era of 5G communication, removing interference sources that affect communication is a resource-intensive task. The rapid development of computer vision has enabled unmanned aerial vehicles to perform various high-altitude detection tasks. Because the field of object detection for antenna interference sources has not been fully explored, this industry lacks dedicated learning samples and detection models for this specific task. In this article, an antenna dataset is created to address important antenna interference source detection issues and serves as the basis for subsequent research. We introduce YOLO-Ant, a lightweight CNN and transformer hybrid detector specifically designed for antenna interference source detection. Specifically, we initially formulated a lightweight design for the network depth and width, ensuring that subsequent investigations were conducted within a lightweight framework. Then, we propose a DSLK-Block module based on depthwise separable convolution and large convolution kernels to enhance the network's feature extraction ability, effectively improving small object detection. To address challenges such as complex backgrounds and large interclass differences in antenna detection, we construct DSLKVit-Block, a powerful feature extraction module that combines DSLK-Block and transformer structures. Considering both its lightweight design and accuracy, our method not only achieves optimal performance on the antenna dataset but also yields competitive results on public datasets.","sentences":["In the era of 5G communication, removing interference sources that affect communication is a resource-intensive task.","The rapid development of computer vision has enabled unmanned aerial vehicles to perform various high-altitude detection tasks.","Because the field of object detection for antenna interference sources has not been fully explored, this industry lacks dedicated learning samples and detection models for this specific task.","In this article, an antenna dataset is created to address important antenna interference source detection issues and serves as the basis for subsequent research.","We introduce YOLO-Ant, a lightweight CNN and transformer hybrid detector specifically designed for antenna interference source detection.","Specifically, we initially formulated a lightweight design for the network depth and width, ensuring that subsequent investigations were conducted within a lightweight framework.","Then, we propose a DSLK-Block module based on depthwise separable convolution and large convolution kernels to enhance the network's feature extraction ability, effectively improving small object detection.","To address challenges such as complex backgrounds and large interclass differences in antenna detection, we construct DSLKVit-Block, a powerful feature extraction module that combines DSLK-Block and transformer structures.","Considering both its lightweight design and accuracy, our method not only achieves optimal performance on the antenna dataset but also yields competitive results on public datasets."],"url":"http://arxiv.org/abs/2402.12641v1","category":"cs.CV"}
{"created":"2024-02-20 01:28:34","title":"StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing","abstract":"Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track. Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability. To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level. It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate embeddings to improve the overall style expression; And (3) a phoneme-guided lip aligner to maintain lip sync. Extensive experiments on two of the primary benchmarks, V2C and Grid, demonstrate the favorable performance of the proposed method as compared to the current state-of-the-art.","sentences":["Given a script, the challenge in Movie Dubbing (Visual Voice Cloning, V2C) is to generate speech that aligns well with the video in both time and emotion, based on the tone of a reference audio track.","Existing state-of-the-art V2C models break the phonemes in the script according to the divisions between video frames, which solves the temporal alignment problem but leads to incomplete phoneme pronunciation and poor identity stability.","To address this problem, we propose StyleDubber, which switches dubbing learning from the frame level to phoneme level.","It contains three main components: (1) A multimodal style adaptor operating at the phoneme level to learn pronunciation style from the reference audio, and generate intermediate representations informed by the facial emotion presented in the video; (2) An utterance-level style learning module, which guides both the mel-spectrogram decoding and the refining processes from the intermediate embeddings to improve the overall style expression; And (3) a phoneme-guided lip aligner to maintain lip sync.","Extensive experiments on two of the primary benchmarks, V2C and Grid, demonstrate the favorable performance of the proposed method as compared to the current state-of-the-art."],"url":"http://arxiv.org/abs/2402.12636v1","category":"cs.CL"}
{"created":"2024-02-20 01:22:04","title":"FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML","abstract":"We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models $\\sim$2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines \\citep{nori2019interpretml}. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.","sentences":["We present FAST, an optimization framework for fast additive segmentation.","FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models.","The framework leverages a novel optimization procedure to fit these models $\\sim$2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines \\citep{nori2019interpretml}.","We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well.","Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models."],"url":"http://arxiv.org/abs/2402.12630v1","category":"stat.ML"}
{"created":"2024-02-19 23:19:15","title":"Truncated Polynomial Expansion-Based Detection in Massive MIMO: A Model-Driven Deep Learning Approach","abstract":"In this paper, we propose a deep learning (DL)-based approach for efficiently computing the inverse of Hermitian matrices using truncated polynomial expansion (TPE). Our model-driven approach involves optimizing the coefficients of the TPE during an offline training procedure for a given number of TPE terms. We apply this method to signal detection in uplink massive multiple-input multiple-output (MIMO) systems, where the matrix inverse operation required by linear detectors, such as zero-forcing (ZF) and minimum mean square error (MMSE), is approximated using TPE. Our simulation results demonstrate that the proposed learned TPE-based method outperforms the conventional TPE method with optimal coefficients in terms of asymptotic convergence speed and reduces the computational complexity of the online detection stage, albeit at the expense of the offline training stage. However, the limited number of trainable parameters leads to a swift offline training process.","sentences":["In this paper, we propose a deep learning (DL)-based approach for efficiently computing the inverse of Hermitian matrices using truncated polynomial expansion (TPE).","Our model-driven approach involves optimizing the coefficients of the TPE during an offline training procedure for a given number of TPE terms.","We apply this method to signal detection in uplink massive multiple-input multiple-output (MIMO) systems, where the matrix inverse operation required by linear detectors, such as zero-forcing (ZF) and minimum mean square error (MMSE), is approximated using TPE.","Our simulation results demonstrate that the proposed learned TPE-based method outperforms the conventional TPE method with optimal coefficients in terms of asymptotic convergence speed and reduces the computational complexity of the online detection stage, albeit at the expense of the offline training stage.","However, the limited number of trainable parameters leads to a swift offline training process."],"url":"http://arxiv.org/abs/2402.12595v1","category":"eess.SP"}
{"created":"2024-02-20 18:59:02","title":"Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields","abstract":"In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision. First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions. Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization. Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead. To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask. Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization.","sentences":["In this paper, we propose an algorithm that allows joint refinement of camera pose and scene geometry represented by decomposed low-rank tensor, using only 2D images as supervision.","First, we conduct a pilot study based on a 1D signal and relate our findings to 3D scenarios, where the naive joint pose optimization on voxel-based NeRFs can easily lead to sub-optimal solutions.","Moreover, based on the analysis of the frequency spectrum, we propose to apply convolutional Gaussian filters on 2D and 3D radiance fields for a coarse-to-fine training schedule that enables joint camera pose optimization.","Leveraging the decomposition property in decomposed low-rank tensor, our method achieves an equivalent effect to brute-force 3D convolution with only incurring little computational overhead.","To further improve the robustness and stability of joint optimization, we also propose techniques of smoothed 2D supervision, randomly scaled kernel parameters, and edge-guided loss mask.","Extensive quantitative and qualitative evaluations demonstrate that our proposed framework achieves superior performance in novel view synthesis as well as rapid convergence for optimization."],"url":"http://arxiv.org/abs/2402.13252v1","category":"cs.CV"}
{"created":"2024-02-20 17:22:11","title":"Barking dogs: A Fr\u00e9chet distance variant for detour detection","abstract":"Imagine you are a dog behind a fence $Q$ and a hiker is passing by at constant speed along the hiking path $P$. In order to fulfil your duties as a watchdog, you desire to bark as long as possible at the human. However, your barks can only be heard in a fixed radius $\\rho$ and, as a dog, you have bounded speed $s$. Can you optimize your route along the fence $Q$ in order to maximize the barking time with radius $\\rho$, assuming you can run backwards and forward at speed at most $s$?   We define the barking distance from a polyline $P$ on $n$ vertices to a polyline $Q$ on $m$ vertices as the time that the hiker stays in your barking radius if you run optimally along $Q$. This asymmetric similarity measure between two curves can be used to detect outliers in $Q$ compared to $P$ that other established measures like the Fr\\'echet distance and Dynamic Time Warping fail to capture at times. We consider this measure in three different settings. In the discrete setting, the traversals of $P$ and $Q$ are both discrete. For this case we show that the barking distance from $P$ to $Q$ can be computed in $O(nm\\log s)$ time. In the semi-discrete setting, the traversal of $Q$ is continuous while the one of $P$ is again discrete. Here, we show how to compute the barking distance in time $O(nm\\log (nm))$. Finally, in the continuous setting in which both traversals are continuous, we show that the problem can be solved in polynomial time. For all the settings we show that, assuming SETH, no truly subquadratic algorithm can exist.","sentences":["Imagine you are a dog behind a fence $Q$ and a hiker is passing by at constant speed along the hiking path $P$.","In order to fulfil your duties as a watchdog, you desire to bark as long as possible at the human.","However, your barks can only be heard in a fixed radius $\\rho$ and, as a dog, you have bounded speed $s$. Can you optimize your route along the fence $Q$ in order to maximize the barking time with radius $\\rho$, assuming you can run backwards and forward at speed at most $s$?   ","We define the barking distance from a polyline $P$ on $n$ vertices to a polyline $Q$ on $m$ vertices as the time that the hiker stays in your barking radius if you run optimally along $Q$. This asymmetric similarity measure between two curves can be used to detect outliers in $Q$ compared to $P$ that other established measures like the Fr\\'echet distance and Dynamic Time Warping fail to capture at times.","We consider this measure in three different settings.","In the discrete setting, the traversals of $P$ and $Q$ are both discrete.","For this case we show that the barking distance from $P$ to $Q$ can be computed in $O(nm\\log s)$ time.","In the semi-discrete setting, the traversal of $Q$ is continuous while the one of $P$ is again discrete.","Here, we show how to compute the barking distance in time $O(nm\\log (nm))$. Finally, in the continuous setting in which both traversals are continuous, we show that the problem can be solved in polynomial time.","For all the settings we show that, assuming SETH, no truly subquadratic algorithm can exist."],"url":"http://arxiv.org/abs/2402.13159v1","category":"cs.CG"}
{"created":"2024-02-20 15:13:38","title":"Codec-SUPERB: An In-Depth Analysis of Sound Codec Models","abstract":"The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance. Recent years have witnessed significant developments in codec models. The ideal sound codec should preserve content, paralinguistics, speakers, and audio information. However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings. This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark. It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs. Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons. Finally, we will release codes, the leaderboard, and data to accelerate progress within the community.","sentences":["The sound codec's dual roles in minimizing data transmission latency and serving as tokenizers underscore its critical importance.","Recent years have witnessed significant developments in codec models.","The ideal sound codec should preserve content, paralinguistics, speakers, and audio information.","However, the question of which codec achieves optimal sound information preservation remains unanswered, as in different papers, models are evaluated on their selected experimental settings.","This study introduces Codec-SUPERB, an acronym for Codec sound processing Universal PERformance Benchmark.","It is an ecosystem designed to assess codec models across representative sound applications and signal-level metrics rooted in sound domain knowledge.","Codec-SUPERB simplifies result sharing through an online leaderboard, promoting collaboration within a community-driven benchmark database, thereby stimulating new development cycles for codecs.","Furthermore, we undertake an in-depth analysis to offer insights into codec models from both application and signal perspectives, diverging from previous codec papers mainly concentrating on signal-level comparisons.","Finally, we will release codes, the leaderboard, and data to accelerate progress within the community."],"url":"http://arxiv.org/abs/2402.13071v1","category":"eess.AS"}
{"created":"2024-02-20 02:58:03","title":"Min-Max Optimization for Robust Nonlinear Least Squares Problems","abstract":"This paper considers min-max optimization for a class of robust nonlinear least squares problems. We show via an example that solving the first order optimality conditions defined by gradients of the objective function can lead to incorrect solutions of min-max problems. We give an explicit formula for the value function of the inner maximization problem. Using the formula, we show that finding a first order $\\epsilon$-approximate necessary minimax point of the min-max problem needs at most $O(|\\log\\epsilon| +\\epsilon^{-2}) $ evaluations of the function value and gradients of the objective function. Moreover, we establish error bounds from any solution of the nonlinear least squares problem to the solution set of the robust nonlinear least squares problem.","sentences":["This paper considers min-max optimization for a class of robust nonlinear least squares problems.","We show via an example that solving the first order optimality conditions defined by gradients of the objective function can lead to incorrect solutions of min-max problems.","We give an explicit formula for the value function of the inner maximization problem.","Using the formula, we show that finding a first order $\\epsilon$-approximate necessary minimax point of the min-max problem needs at most $O(|\\log\\epsilon| +\\epsilon^{-2}) $ evaluations of the function value and gradients of the objective function.","Moreover, we establish error bounds from any solution of the nonlinear least squares problem to the solution set of the robust nonlinear least squares problem."],"url":"http://arxiv.org/abs/2402.12679v1","category":"math.OC"}
{"created":"2024-02-20 01:46:30","title":"Optimal PSPACE-hardness of Approximating Set Cover Reconfiguration","abstract":"In the Minmax Set Cover Reconfiguration problem, given a set system $\\mathcal{F}$ over a universe and its two covers $\\mathcal{C}^\\mathsf{start}$ and $\\mathcal{C}^\\mathsf{goal}$ of size $k$, we wish to transform $\\mathcal{C}^\\mathsf{start}$ into $\\mathcal{C}^\\mathsf{goal}$ by repeatedly adding or removing a single set of $\\mathcal{F}$ while covering the universe in any intermediate state. Then, the objective is to minimize the maximize size of any intermediate cover during transformation. We prove that Minmax Set Cover Reconfiguration and Minmax Dominating Set Reconfiguration are $\\mathsf{PSPACE}$-hard to approximate within a factor of $2-\\frac{1}{\\operatorname{polyloglog} N}$, where $N$ is the size of the universe and the number of vertices in a graph, respectively, improving upon Ohsaka (SODA 2024) and Karthik C. S. and Manurangsi (2023). This is the first result that exhibits a sharp threshold for the approximation factor of any reconfiguration problem because both problems admit a $2$-factor approximation algorithm as per Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (Theor. Comput. Sci., 2011). Our proof is based on a reconfiguration analogue of the FGLSS reduction from Probabilistically Checkable Reconfiguration Proofs of Hirahara and Ohsaka (2024). We also prove that for any constant $\\varepsilon \\in (0,1)$, Minmax Hypergraph Vertex Cover Reconfiguration on $\\operatorname{poly}(\\varepsilon^{-1})$-uniform hypergraphs is $\\mathsf{PSPACE}$-hard to approximate within a factor of $2-\\varepsilon$.","sentences":["In the Minmax Set Cover Reconfiguration problem, given a set system $\\mathcal{F}$ over a universe and its two covers $\\mathcal{C}^\\mathsf{start}$ and $\\mathcal{C}^\\mathsf{goal}$ of size $k$, we wish to transform $\\mathcal{C}^\\mathsf{start}$ into $\\mathcal{C}^\\mathsf{goal}$ by repeatedly adding or removing a single set of $\\mathcal{F}$ while covering the universe in any intermediate state.","Then, the objective is to minimize the maximize size of any intermediate cover during transformation.","We prove that Minmax Set Cover Reconfiguration and Minmax Dominating Set Reconfiguration are $\\mathsf{PSPACE}$-hard to approximate within a factor of $2-\\frac{1}{\\operatorname{polyloglog} N}$, where $N$ is the size of the universe and the number of vertices in a graph, respectively, improving upon Ohsaka (SODA 2024) and Karthik C. S. and Manurangsi (2023).","This is the first result that exhibits a sharp threshold for the approximation factor of any reconfiguration problem because both problems admit a $2$-factor approximation algorithm as per Ito, Demaine, Harvey, Papadimitriou, Sideri, Uehara, and Uno (Theor.","Comput.","Sci., 2011).","Our proof is based on a reconfiguration analogue of the FGLSS reduction from Probabilistically Checkable Reconfiguration Proofs of Hirahara and Ohsaka (2024).","We also prove that for any constant $\\varepsilon \\in (0,1)$, Minmax Hypergraph Vertex Cover Reconfiguration on $\\operatorname{poly}(\\varepsilon^{-1})$-uniform hypergraphs is $\\mathsf{PSPACE}$-hard to approximate within a factor of $2-\\varepsilon$."],"url":"http://arxiv.org/abs/2402.12645v1","category":"cs.CC"}
{"created":"2024-02-20 01:36:08","title":"Rampo: A CEGAR-based Integration of Binary Code Analysis and System Falsification for Cyber-Kinetic Vulnerability Detection","abstract":"This paper presents a novel tool, named Rampo, that can perform binary code analysis to identify cyber kinetic vulnerabilities in CPS. The tool takes as input a Signal Temporal Logic (STL) formula that describes the kinetic effect, i.e., the behavior of the physical system, that one wants to avoid. The tool then searches the possible cyber trajectories in the binary code that may lead to such physical behavior. This search integrates binary code analysis tools and hybrid systems falsification tools using a Counter-Example Guided Abstraction Refinement (CEGAR) approach. Rampo starts by analyzing the binary code to extract symbolic constraints that represent the different paths in the code. These symbolic constraints are then passed to a Satisfiability Modulo Theories (SMT) solver to extract the range of control signals that can be produced by each path in the code. The next step is to search over possible physical trajectories using a hybrid systems falsification tool that adheres to the behavior of the cyber paths and yet leads to violations of the STL formula. Since the number of cyber paths that need to be explored increases exponentially with the length of physical trajectories, we iteratively perform refinement of the cyber path constraints based on the previous falsification result and traverse the abstract path tree obtained from the control program to explore the search space of the system. To illustrate the practical utility of binary code analysis in identifying cyber kinetic vulnerabilities, we present case studies from diverse CPS domains, showcasing how they can be discovered in their control programs. Our tool could compute the same number of vulnerabilities while leading to a speedup that ranges from 3x to 98x.","sentences":["This paper presents a novel tool, named Rampo, that can perform binary code analysis to identify cyber kinetic vulnerabilities in CPS.","The tool takes as input a Signal Temporal Logic (STL) formula that describes the kinetic effect, i.e., the behavior of the physical system, that one wants to avoid.","The tool then searches the possible cyber trajectories in the binary code that may lead to such physical behavior.","This search integrates binary code analysis tools and hybrid systems falsification tools using a Counter-Example Guided Abstraction Refinement (CEGAR) approach.","Rampo starts by analyzing the binary code to extract symbolic constraints that represent the different paths in the code.","These symbolic constraints are then passed to a Satisfiability Modulo Theories (SMT) solver to extract the range of control signals that can be produced by each path in the code.","The next step is to search over possible physical trajectories using a hybrid systems falsification tool that adheres to the behavior of the cyber paths and yet leads to violations of the STL formula.","Since the number of cyber paths that need to be explored increases exponentially with the length of physical trajectories, we iteratively perform refinement of the cyber path constraints based on the previous falsification result and traverse the abstract path tree obtained from the control program to explore the search space of the system.","To illustrate the practical utility of binary code analysis in identifying cyber kinetic vulnerabilities, we present case studies from diverse CPS domains, showcasing how they can be discovered in their control programs.","Our tool could compute the same number of vulnerabilities while leading to a speedup that ranges from 3x to 98x."],"url":"http://arxiv.org/abs/2402.12642v1","category":"cs.CR"}
{"created":"2024-02-20 01:22:07","title":"Guarantees on Warm-Started QAOA: Single-Round Approximation Ratios for 3-Regular MAXCUT and Higher-Round Scaling Limits","abstract":"We generalize Farhi et al.'s 0.6924-approximation result technique of the Max-Cut Quantum Approximate Optimization Algorithm (QAOA) on 3-regular graphs to obtain provable lower bounds on the approximation ratio for warm-started QAOA. Given an initialization angle $\\theta$, we consider warm-starts where the initial state is a product state where each qubit position is angle $\\theta$ away from either the north or south pole of the Bloch sphere; of the two possible qubit positions the position of each qubit is decided by some classically obtained cut encoded as a bitstring $b$. We illustrate through plots how the properties of $b$ and the initialization angle $\\theta$ influence the bound on the approximation ratios of warm-started QAOA. We consider various classical algorithms (and the cuts they produce which we use to generate the warm-start). Our results strongly suggest that there does not exist any choice of initialization angle that yields a (worst-case) approximation ratio that simultaneously beats standard QAOA and the classical algorithm used to create the warm-start.   Additionally, we show that at $\\theta=60^\\circ$, warm-started QAOA is able to (effectively) recover the cut used to generate the warm-start, thus suggesting that in practice, this value could be a promising starting angle to explore alternate solutions in a heuristic fashion. Finally, for any combinatorial optimization problem with integer-valued objective values, we provide bounds on the required circuit depth needed for warm-started QAOA to achieve some change in approximation ratio; more specifically, we show that for small $\\theta$, the bound is roughly proportional to $1/\\theta$.","sentences":["We generalize Farhi et al.'s 0.6924-approximation result technique of the Max-Cut Quantum Approximate Optimization Algorithm (QAOA) on 3-regular graphs to obtain provable lower bounds on the approximation ratio for warm-started QAOA.","Given an initialization angle $\\theta$, we consider warm-starts where the initial state is a product state where each qubit position is angle $\\theta$ away from either the north or south pole of the Bloch sphere; of the two possible qubit positions the position of each qubit is decided by some classically obtained cut encoded as a bitstring $b$. We illustrate through plots how the properties of $b$ and the initialization angle $\\theta$ influence the bound on the approximation ratios of warm-started QAOA.","We consider various classical algorithms (and the cuts they produce which we use to generate the warm-start).","Our results strongly suggest that there does not exist any choice of initialization angle that yields a (worst-case) approximation ratio that simultaneously beats standard QAOA and the classical algorithm used to create the warm-start.   ","Additionally, we show that at $\\theta=60^\\circ$, warm-started QAOA is able to (effectively) recover the cut used to generate the warm-start, thus suggesting that in practice, this value could be a promising starting angle to explore alternate solutions in a heuristic fashion.","Finally, for any combinatorial optimization problem with integer-valued objective values, we provide bounds on the required circuit depth needed for warm-started QAOA to achieve some change in approximation ratio; more specifically, we show that for small $\\theta$, the bound is roughly proportional to $1/\\theta$."],"url":"http://arxiv.org/abs/2402.12631v1","category":"quant-ph"}
{"created":"2024-02-20 01:07:07","title":"Effective Edge Ranking via Random Walk with Restart","abstract":"Given a network G, edge centrality is a metric used to evaluate the importance of edges in G, which is a key concept in analyzing networks and finds vast applications involving edge ranking. In spite of a wealth of research on devising edge centrality measures, they incur either prohibitively high computation costs or varied deficiencies that lead to sub-optimal ranking quality.   To overcome their limitations, this paper proposes EdgeRAKE, a new centrality measure for edge ranking that leverages the novel notion of the edgewise random walk with restart. Based thereon, we present a linear-complexity algorithm for EdgeRAKE approximation, followed by an in-depth theoretical analysis in terms of various aspects. Extensive experiments comparing EdgeRAKE against six edge centrality metrics in graph analytics tasks on real networks showcase that EdgeRAKE offers superior practical effectiveness without significantly reducing computation efficiency","sentences":["Given a network G, edge centrality is a metric used to evaluate the importance of edges in G, which is a key concept in analyzing networks and finds vast applications involving edge ranking.","In spite of a wealth of research on devising edge centrality measures, they incur either prohibitively high computation costs or varied deficiencies that lead to sub-optimal ranking quality.   ","To overcome their limitations, this paper proposes EdgeRAKE, a new centrality measure for edge ranking that leverages the novel notion of the edgewise random walk with restart.","Based thereon, we present a linear-complexity algorithm for EdgeRAKE approximation, followed by an in-depth theoretical analysis in terms of various aspects.","Extensive experiments comparing EdgeRAKE against six edge centrality metrics in graph analytics tasks on real networks showcase that EdgeRAKE offers superior practical effectiveness without significantly reducing computation efficiency"],"url":"http://arxiv.org/abs/2402.12623v1","category":"cs.SI"}
{"created":"2024-02-19 21:23:00","title":"Triglobal resolvent-analysis-based control of separated flows around low-aspect-ratio wings","abstract":"We perform direct numerical simulations (DNS) of actively controlled laminar separated wakes around low-aspect-ratio wings with two primary goals: (i) reducing the size of the separation bubble and (ii) attenuating the wing tip vortex. Instead of preventing separation, we aim to modify the three-dimensional ($3$-D) dynamics to exploit wake vortices for aerodynamic enhancements. A direct wake modification is considered using optimal harmonic forcing modes from triglobal resolvent analysis. For this study, we consider wings at angles of attack of $14^\\circ$ and $22^\\circ$, taper ratios $0.27$ and $1$, and leading edge sweep angles of $0^\\circ$ and $30^\\circ$, at a mean-chord-based Reynolds number of $600$. The wakes behind these wings exhibit $3$-D reversed-flow bubble and large-scale vortical structures. For swept and tapered wings, the diversity of wake vortices increases substantially, posing a challenge for flow control. To achieve the first control objective, a root-based actuation at the shedding frequency reduces the reversed-flow bubble size and capitalizes on wake vortices to significantly enhance the aerodynamic performance of the wing. For tapered and swept wings, actuation modifies the stalled flow, increasing the root contribution to lift. For the goal of controlling the tip vortex, we demonstrate the effectiveness of actuation with high-frequency perturbations near the tip. This study shows how insights from resolvent analysis for unsteady actuation lead to the global modification of $3$-D separated wakes and improved aerodynamics of wings.","sentences":["We perform direct numerical simulations (DNS) of actively controlled laminar separated wakes around low-aspect-ratio wings with two primary goals: (i) reducing the size of the separation bubble and (ii) attenuating the wing tip vortex.","Instead of preventing separation, we aim to modify the three-dimensional ($3$-D) dynamics to exploit wake vortices for aerodynamic enhancements.","A direct wake modification is considered using optimal harmonic forcing modes from triglobal resolvent analysis.","For this study, we consider wings at angles of attack of $14^\\circ$ and $22^\\circ$, taper ratios $0.27$ and $1$, and leading edge sweep angles of $0^\\circ$ and $30^\\circ$, at a mean-chord-based Reynolds number of $600$. The wakes behind these wings exhibit $3$-D reversed-flow bubble and large-scale vortical structures.","For swept and tapered wings, the diversity of wake vortices increases substantially, posing a challenge for flow control.","To achieve the first control objective, a root-based actuation at the shedding frequency reduces the reversed-flow bubble size and capitalizes on wake vortices to significantly enhance the aerodynamic performance of the wing.","For tapered and swept wings, actuation modifies the stalled flow, increasing the root contribution to lift.","For the goal of controlling the tip vortex, we demonstrate the effectiveness of actuation with high-frequency perturbations near the tip.","This study shows how insights from resolvent analysis for unsteady actuation lead to the global modification of $3$-D separated wakes and improved aerodynamics of wings."],"url":"http://arxiv.org/abs/2402.12553v1","category":"physics.flu-dyn"}
{"created":"2024-02-19 20:45:10","title":"Exterior Nonlocal Variational Inequalities","abstract":"This paper introduces a new class of variational inequalities where the obstacle is placed in the exterior domain that is disjoint from the observation domain. This is carried out with the help of nonlocal fractional operators. The need for such novel variational inequalities stems from the fact that the classical approach only allows placing the obstacle either inside the observation domain or on the boundary. A complete analysis of the continuous problem is provided. Additionally, perturbation arguments to approximate the problem are discussed.","sentences":["This paper introduces a new class of variational inequalities where the obstacle is placed in the exterior domain that is disjoint from the observation domain.","This is carried out with the help of nonlocal fractional operators.","The need for such novel variational inequalities stems from the fact that the classical approach only allows placing the obstacle either inside the observation domain or on the boundary.","A complete analysis of the continuous problem is provided.","Additionally, perturbation arguments to approximate the problem are discussed."],"url":"http://arxiv.org/abs/2402.12533v1","category":"math.AP"}
{"created":"2024-02-19 19:55:03","title":"On Averaging and Extrapolation for Gradient Descent","abstract":"This work considers the effect of averaging, and more generally extrapolation, of the iterates of gradient descent in smooth convex optimization. After running the method, rather than reporting the final iterate, one can report either a convex combination of the iterates (averaging) or a generic combination of the iterates (extrapolation). For several common stepsize sequences, including recently developed accelerated periodically long stepsize schemes, we show averaging cannot improve gradient descent's worst-case performance and is, in fact, strictly worse than simply returning the last iterate. In contrast, we prove a conceptually simple and computationally cheap extrapolation scheme strictly improves the worst-case convergence rate: when initialized at the origin, reporting $(1+1/\\sqrt{16N\\log(N)})x_N$ rather than $x_N$ improves the best possible worst-case performance by the same amount as conducting $O(\\sqrt{N/\\log(N)})$ more gradient steps. Our analysis and characterizations of the best-possible convergence guarantees are computer-aided, using performance estimation problems. Numerically, we find similar (small) benefits from such simple extrapolation for a range of gradient methods.","sentences":["This work considers the effect of averaging, and more generally extrapolation, of the iterates of gradient descent in smooth convex optimization.","After running the method, rather than reporting the final iterate, one can report either a convex combination of the iterates (averaging) or a generic combination of the iterates (extrapolation).","For several common stepsize sequences, including recently developed accelerated periodically long stepsize schemes, we show averaging cannot improve gradient descent's worst-case performance and is, in fact, strictly worse than simply returning the last iterate.","In contrast, we prove a conceptually simple and computationally cheap extrapolation scheme strictly improves the worst-case convergence rate: when initialized at the origin, reporting $(1+1/\\sqrt{16N\\log(N)})x_N$ rather than $x_N$ improves the best possible worst-case performance by the same amount as conducting $O(\\sqrt{N/\\log(N)})$ more gradient steps.","Our analysis and characterizations of the best-possible convergence guarantees are computer-aided, using performance estimation problems.","Numerically, we find similar (small) benefits from such simple extrapolation for a range of gradient methods."],"url":"http://arxiv.org/abs/2402.12493v1","category":"math.OC"}
{"created":"2024-02-19 19:44:45","title":"Quantum Shortcut to Adiabaticity for State Preparation in a Finite-Sized Jaynes-Cummings Lattice","abstract":"In noisy quantum systems, achieving high-fidelity state preparation using the adiabatic approach faces a dilemma: either extending the evolution time to reduce diabatic transitions or shortening it to mitigate decoherence effects. Here, we present a quantum shortcut to adiabaticity for state preparation in a finite-sized Jaynes-Cummings lattice by applying a counter-diabatic (CD) driving along given adiabatic trajectories. Leveraging the symmetry of eigenstates in this system, we derive a simplified CD Hamiltonian that only involves local qubit-cavity couplings for a two-site lattice with one polariton excitation. Additionally, we derive the analytical form of the CD Hamiltonian for this lattice with two excitations. Our numerical results demonstrate that this scheme is robust against circuit errors and environmental noise, with characterization achievable through qubit detection. The simplified CD Hamiltonian can be implemented in physical systems with realistic parameters. This approach can lead to a promising pathway to high-fidelity state preparation within a significantly reduced timescale compared to conventional adiabatic methods.","sentences":["In noisy quantum systems, achieving high-fidelity state preparation using the adiabatic approach faces a dilemma: either extending the evolution time to reduce diabatic transitions or shortening it to mitigate decoherence effects.","Here, we present a quantum shortcut to adiabaticity for state preparation in a finite-sized Jaynes-Cummings lattice by applying a counter-diabatic (CD) driving along given adiabatic trajectories.","Leveraging the symmetry of eigenstates in this system, we derive a simplified CD Hamiltonian that only involves local qubit-cavity couplings for a two-site lattice with one polariton excitation.","Additionally, we derive the analytical form of the CD Hamiltonian for this lattice with two excitations.","Our numerical results demonstrate that this scheme is robust against circuit errors and environmental noise, with characterization achievable through qubit detection.","The simplified CD Hamiltonian can be implemented in physical systems with realistic parameters.","This approach can lead to a promising pathway to high-fidelity state preparation within a significantly reduced timescale compared to conventional adiabatic methods."],"url":"http://arxiv.org/abs/2402.12485v1","category":"quant-ph"}
{"created":"2024-02-19 09:55:32","title":"EBFT: Effective and Block-Wise Fine-Tuning for Sparse LLMs","abstract":"Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs. Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions. To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error. Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions. Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines. For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14. Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a perplexity of 16.27, outperforming LoRA (perplexity 16.44). Furthermore, the fine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes, and the entire framework can be executed on a single 16GB GPU. The source code is available at https://github.com/sunggo/EBFT.","sentences":["Existing methods for fine-tuning sparse LLMs often suffer from resource-intensive requirements and high retraining costs.","Additionally, many fine-tuning methods often rely on approximations or heuristic optimization strategies, which may lead to suboptimal solutions.","To address these issues, we propose an efficient and fast framework for fine-tuning sparse LLMs based on minimizing reconstruction error.","Our approach involves sampling a small dataset for calibration and utilizing backpropagation to iteratively optimize block-wise reconstruction error, on a block-by-block basis, aiming for optimal solutions.","Extensive experiments on various benchmarks consistently demonstrate the superiority of our method over other baselines.","For instance, on the Wikitext2 dataset with LlamaV1-7B at 70% sparsity, our proposed EBFT achieves a perplexity of 16.88, surpassing the state-of-the-art DSnoT with a perplexity of 75.14.","Moreover, with a structured sparsity ratio of 26\\%, EBFT achieves a perplexity of 16.27, outperforming LoRA (perplexity 16.44).","Furthermore, the fine-tuning process of EBFT for LlamaV1-7B only takes approximately 30 minutes, and the entire framework can be executed on a single 16GB GPU.","The source code is available at https://github.com/sunggo/EBFT."],"url":"http://arxiv.org/abs/2402.12419v1","category":"cs.LG"}
