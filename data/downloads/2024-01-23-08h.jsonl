{"created":"2024-01-22 18:59:29","title":"Exploring Simple Open-Vocabulary Semantic Segmentation","abstract":"Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts. In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements. S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets. Contrary to prior works, our model directly trains for pixel-level features and language alignment. Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning. In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training. We believe that our simple yet effective approach will serve as a solid baseline for future research.","sentences":["Open-vocabulary semantic segmentation models aim to accurately assign a semantic label to each pixel in an image from a set of arbitrary open-vocabulary texts.","In order to learn such pixel-level alignment, current approaches typically rely on a combination of (i) image-level VL model (e.g. CLIP), (ii) ground truth masks, and (iii) custom grouping encoders.","In this paper, we introduce S-Seg, a novel model that can achieve surprisingly strong performance without depending on any of the above elements.","S-Seg leverages pseudo-mask and language to train a MaskFormer, and can be easily trained from publicly available image-text datasets.","Contrary to prior works, our model directly trains for pixel-level features and language alignment.","Once trained, S-Seg generalizes well to multiple testing datasets without requiring fine-tuning.","In addition, S-Seg has the extra benefits of scalability with data and consistently improvement when augmented with self-training.","We believe that our simple yet effective approach will serve as a solid baseline for future research."],"url":"http://arxiv.org/abs/2401.12217v1","category":"cs.CV"}
{"created":"2024-01-22 18:59:12","title":"Mitigating Covariate Shift in Misspecified Regression with Applications to Reinforcement Learning","abstract":"A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ. As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects. In this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily. We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions. As our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates. As applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage.","sentences":["A pervasive phenomenon in machine learning applications is distribution shift, where training and deployment conditions for a machine learning model differ.","As distribution shift typically results in a degradation in performance, much attention has been devoted to algorithmic interventions that mitigate these detrimental effects.","In this paper, we study the effect of distribution shift in the presence of model misspecification, specifically focusing on $L_{\\infty}$-misspecified regression and adversarial covariate shift, where the regression target remains fixed while the covariate distribution changes arbitrarily.","We show that empirical risk minimization, or standard least squares regression, can result in undesirable misspecification amplification where the error due to misspecification is amplified by the density ratio between the training and testing distributions.","As our main result, we develop a new algorithm -- inspired by robust optimization techniques -- that avoids this undesirable behavior, resulting in no misspecification amplification while still obtaining optimal statistical rates.","As applications, we use this regression procedure to obtain new guarantees in offline and online reinforcement learning with misspecification and establish new separations between previously studied structural conditions and notions of coverage."],"url":"http://arxiv.org/abs/2401.12216v1","category":"stat.ML"}
{"created":"2024-01-22 18:59:07","title":"Less Could Be Better: Parameter-efficient Fine-tuning Advances Medical Vision Foundation Models","abstract":"Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks. However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored. As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models. Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets. Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters. Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14. We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks. Code and models are available at https://github.com/RL4M/MED-PEFT.","sentences":["Parameter-efficient fine-tuning (PEFT) that was initially developed for exploiting pre-trained large language models has recently emerged as an effective approach to perform transfer learning on computer vision tasks.","However, the effectiveness of PEFT on medical vision foundation models is still unclear and remains to be explored.","As a proof of concept, we conducted a detailed empirical study on applying PEFT to chest radiography foundation models.","Specifically, we delved into LoRA, a representative PEFT method, and compared it against full-parameter fine-tuning (FFT) on two self-supervised radiography foundation models across three well-established chest radiograph datasets.","Our results showed that LoRA outperformed FFT in 13 out of 18 transfer learning tasks by at most 2.9% using fewer than 1% tunable parameters.","Combining LoRA with foundation models, we set up new state-of-the-art on a range of data-efficient learning tasks, such as an AUROC score of 80.6% using 1% labeled data on NIH ChestX-ray14.","We hope this study can evoke more attention from the community in the use of PEFT for transfer learning on medical imaging tasks.","Code and models are available at https://github.com/RL4M/MED-PEFT."],"url":"http://arxiv.org/abs/2401.12215v1","category":"cs.CV"}
{"created":"2024-01-22 18:56:46","title":"Identifying gap-closings in open non-Hermitian systems by Biorthogonal Polarization","abstract":"We investigate gap-closings in one- and two-dimensional tight-binding models with two bands, containing non-Hermitian hopping terms, and open boundary conditions (OBCs) imposed along one direction. We compare the bulk OBC spectra with the periodic boundary condition (PBC) spectra, pointing out that they do not coincide, which is an intrinsic characteristic of non-Hermitian systems. The non-Hermiticity thus results in the failure of the familiar notions of bulk-boundary correspondence found for Hermitian systems. This necessitates the search for topological invariants which can characterize gap-closings in open non-Hermitian systems correctly and unambiguously. We elucidate the behaviour of two possible candidates applicable for one-dimensional slices -- (1) the sum of winding numbers for the two bands defined on a generalized Brillouin zone and (2) the biorthogonal polarization (BP). While the former shows jumps/discontinuities for some of the non-Hermitian systems studied here, at points when an edge mode enters the bulk states and becomes delocalized, it does not maintain quantized values in a given topological phase. On the contrary, BP shows jumps and at phase transitions takes the quantized value of one or zero, which corresponds to whether an actual edge mode exists or whether that mode is delocalized and absorbed within the bulk (not being an edge mode anymore).","sentences":["We investigate gap-closings in one-","and two-dimensional tight-binding models with two bands, containing non-Hermitian hopping terms, and open boundary conditions (OBCs) imposed along one direction.","We compare the bulk OBC spectra with the periodic boundary condition (PBC) spectra, pointing out that they do not coincide, which is an intrinsic characteristic of non-Hermitian systems.","The non-Hermiticity thus results in the failure of the familiar notions of bulk-boundary correspondence found for Hermitian systems.","This necessitates the search for topological invariants which can characterize gap-closings in open non-Hermitian systems correctly and unambiguously.","We elucidate the behaviour of two possible candidates applicable for one-dimensional slices -- (1) the sum of winding numbers for the two bands defined on a generalized Brillouin zone and (2) the biorthogonal polarization (BP).","While the former shows jumps/discontinuities for some of the non-Hermitian systems studied here, at points when an edge mode enters the bulk states and becomes delocalized, it does not maintain quantized values in a given topological phase.","On the contrary, BP shows jumps and at phase transitions takes the quantized value of one or zero, which corresponds to whether an actual edge mode exists or whether that mode is delocalized and absorbed within the bulk (not being an edge mode anymore)."],"url":"http://arxiv.org/abs/2401.12213v1","category":"quant-ph"}
{"created":"2024-01-22 18:54:20","title":"Genericity Through Stratification","abstract":"A fundamental issue in the $\\lambda$-calculus is to find appropriate notions for meaningfulness. Inspired by well-known results for the call-by-name $\\lambda$-calculus (CbN), where meaningful terms are identified to the solvable ones, this paper validates the challenging claim that the notion of potential valuability (aka scrutability), previously introduced in the literature, adequately represents meaningfulness in the call-by-value $\\lambda$-calculus (CbV). Akin to CbN, this claim is corroborated by proving two essential properties. The first one is genericity, stating that meaningless subterms have no bearing on evaluating normalizing terms. To prove this, we use a novel approach based on stratified reduction, indifferently applicable to CbN and CbV. The second property concerns consistency of the smallest congruence relation resulting from equating all meaningless terms (without equating all terms). We also show that such a congruence has a unique consistent and maximal extension, which coincides with a natural notion of observational equivalence. Our results thus supply the formal concepts and tools that validate the informal notion of meaningfulness underlying CbN and CbV.","sentences":["A fundamental issue in the $\\lambda$-calculus is to find appropriate notions for meaningfulness.","Inspired by well-known results for the call-by-name $\\lambda$-calculus (CbN), where meaningful terms are identified to the solvable ones, this paper validates the challenging claim that the notion of potential valuability (aka scrutability), previously introduced in the literature, adequately represents meaningfulness in the call-by-value $\\lambda$-calculus (CbV).","Akin to CbN, this claim is corroborated by proving two essential properties.","The first one is genericity, stating that meaningless subterms have no bearing on evaluating normalizing terms.","To prove this, we use a novel approach based on stratified reduction, indifferently applicable to CbN and CbV.","The second property concerns consistency of the smallest congruence relation resulting from equating all meaningless terms (without equating all terms).","We also show that such a congruence has a unique consistent and maximal extension, which coincides with a natural notion of observational equivalence.","Our results thus supply the formal concepts and tools that validate the informal notion of meaningfulness underlying CbN and CbV."],"url":"http://arxiv.org/abs/2401.12212v1","category":"cs.LO"}
{"created":"2024-01-22 18:52:51","title":"Connecting the Dots: Leveraging Spatio-Temporal Graph Neural Networks for Accurate Bangla Sign Language Recognition","abstract":"Recent advances in Deep Learning and Computer Vision have been successfully leveraged to serve marginalized communities in various contexts. One such area is Sign Language - a primary means of communication for the deaf community. However, so far, the bulk of research efforts and investments have gone into American Sign Language, and research activity into low-resource sign languages - especially Bangla Sign Language - has lagged significantly. In this research paper, we present a new word-level Bangla Sign Language dataset - BdSL40 - consisting of 611 videos over 40 words, along with two different approaches: one with a 3D Convolutional Neural Network model and another with a novel Graph Neural Network approach for the classification of BdSL40 dataset. This is the first study on word-level BdSL recognition, and the dataset was transcribed from Indian Sign Language (ISL) using the Bangla Sign Language Dictionary (1997). The proposed GNN model achieved an F1 score of 89%. The study highlights the significant lexical and semantic similarity between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature. We release the dataset and source code to stimulate further research.","sentences":["Recent advances in Deep Learning and Computer Vision have been successfully leveraged to serve marginalized communities in various contexts.","One such area is Sign Language - a primary means of communication for the deaf community.","However, so far, the bulk of research efforts and investments have gone into American Sign Language, and research activity into low-resource sign languages - especially Bangla Sign Language - has lagged significantly.","In this research paper, we present a new word-level Bangla Sign Language dataset - BdSL40 - consisting of 611 videos over 40 words, along with two different approaches: one with a 3D Convolutional Neural Network model and another with a novel Graph Neural Network approach for the classification of BdSL40 dataset.","This is the first study on word-level BdSL recognition, and the dataset was transcribed from Indian Sign Language (ISL) using the Bangla Sign Language Dictionary (1997).","The proposed GNN model achieved an F1 score of 89%.","The study highlights the significant lexical and semantic similarity between BdSL, West Bengal Sign Language, and ISL, and the lack of word-level datasets for BdSL in the literature.","We release the dataset and source code to stimulate further research."],"url":"http://arxiv.org/abs/2401.12210v1","category":"cs.CV"}
{"created":"2024-01-22 18:51:23","title":"A Single Photon Source based on a Long-Range Interacting Room Temperature Vapor","abstract":"We report on the current development of a single photon source based on a long-range interacting room temperature rubidium vapor. We discuss the history of the project, the production of vapor cells, and the observation of Rabi-oscillations in the four-wave-mixing excitation scheme.","sentences":["We report on the current development of a single photon source based on a long-range interacting room temperature rubidium vapor.","We discuss the history of the project, the production of vapor cells, and the observation of Rabi-oscillations in the four-wave-mixing excitation scheme."],"url":"http://arxiv.org/abs/2401.12209v1","category":"physics.atom-ph"}
{"created":"2024-01-22 18:51:07","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation","abstract":"Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice. Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes. However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation. In this work, we address these challenges by first introducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets. We then present \\emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs. To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities. Finally, we introduce \\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks. Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities. Our project is at \\url{https://stanford-aimi.github.io/chexagent.html}.","sentences":["Chest X-rays (CXRs) are the most frequently performed imaging test in clinical practice.","Recent advances in the development of vision-language foundation models (FMs) give rise to the possibility of performing automated CXR interpretation, which can assist physicians with clinical decision-making and improve patient outcomes.","However, developing FMs that can accurately interpret CXRs is challenging due to the (1) limited availability of large-scale vision-language datasets in the medical image domain, (2) lack of vision and language encoders that can capture the complexities of medical data, and (3) absence of evaluation frameworks for benchmarking the abilities of FMs on CXR interpretation.","In this work, we address these challenges by first introducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset curated from 28 publicly-available datasets.","We then present \\emph{CheXagent} - an instruction-tuned FM capable of analyzing and summarizing CXRs.","To build CheXagent, we design a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities.","Finally, we introduce \\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks.","Extensive quantitative evaluations and qualitative reviews with five expert radiologists demonstrate that CheXagent outperforms previously-developed general- and medical-domain FMs on CheXbench tasks.","Furthermore, in an effort to improve model transparency, we perform a fairness evaluation across factors of sex, race and age to highlight potential performance disparities.","Our project is at \\url{https://stanford-aimi.github.io/chexagent.html}."],"url":"http://arxiv.org/abs/2401.12208v1","category":"cs.CV"}
{"created":"2024-01-22 18:49:56","title":"Rate-Distortion-Perception Tradeoff Based on the Conditional-Distribution Perception Measure","abstract":"We study the rate-distortion-perception (RDP) tradeoff for a memoryless source model in the asymptotic limit of large block-lengths. Our perception measure is based on a divergence between the distributions of the source and reconstruction sequences conditioned on the encoder output, which was first proposed in [1], [2]. We consider the case when there is no shared randomness between the encoder and the decoder. For the case of discrete memoryless sources we derive a single-letter characterization of the RDP function, thus settling a problem that remains open for the marginal metric introduced in Blau and Michaeli [3] (with no shared randomness). Our achievability scheme is based on lossy source coding with a posterior reference map proposed in [4]. For the case of continuous valued sources under squared error distortion measure and squared quadratic Wasserstein perception measure we also derive a single-letter characterization and show that a noise-adding mechanism at the decoder suffices to achieve the optimal representation. For the case of zero perception loss, we show that our characterization interestingly coincides with the results for the marginal metric derived in [5], [6] and again demonstrate that zero perception loss can be achieved with a $3$-dB penalty in the minimum distortion. Finally we specialize our results to the case of Gaussian sources. We derive the RDP function for vector Gaussian sources and propose a waterfilling type solution. We also partially characterize the RDP function for a mixture of vector Gaussians.","sentences":["We study the rate-distortion-perception (RDP) tradeoff for a memoryless source model in the asymptotic limit of large block-lengths.","Our perception measure is based on a divergence between the distributions of the source and reconstruction sequences conditioned on the encoder output, which was first proposed in [1], [2].","We consider the case when there is no shared randomness between the encoder and the decoder.","For the case of discrete memoryless sources we derive a single-letter characterization of the RDP function, thus settling a problem that remains open for the marginal metric introduced in Blau and Michaeli","[3] (with no shared randomness).","Our achievability scheme is based on lossy source coding with a posterior reference map proposed in [4].","For the case of continuous valued sources under squared error distortion measure and squared quadratic Wasserstein perception measure we also derive a single-letter characterization and show that a noise-adding mechanism at the decoder suffices to achieve the optimal representation.","For the case of zero perception loss, we show that our characterization interestingly coincides with the results for the marginal metric derived in [5], [6] and again demonstrate that zero perception loss can be achieved with a $3$-dB penalty in the minimum distortion.","Finally we specialize our results to the case of Gaussian sources.","We derive the RDP function for vector Gaussian sources and propose a waterfilling type solution.","We also partially characterize the RDP function for a mixture of vector Gaussians."],"url":"http://arxiv.org/abs/2401.12207v1","category":"cs.IT"}
{"created":"2024-01-22 18:46:30","title":"Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization","abstract":"Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates. The process involves a sequential application of logic minimization heuristics (``synthesis recipe\"), with their arrangement significantly impacting crucial metrics such as area and delay. Addressing the challenge posed by the broad spectrum of design complexities - from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) - requires a nuanced `synthesis recipe` guided by human expertise and intuition. This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory. We present ABC-RL, a meticulously tuned $\\alpha$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process. Computed based on similarity scores through nearest neighbor retrieval from the training dataset, ABC-RL yields superior synthesis recipes tailored for a wide array of hardware designs. Our findings showcase substantial enhancements in the Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to 24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies.","sentences":["Logic synthesis, a pivotal stage in chip design, entails optimizing chip specifications encoded in hardware description languages like Verilog into highly efficient implementations using Boolean logic gates.","The process involves a sequential application of logic minimization heuristics (``synthesis recipe\"), with their arrangement significantly impacting crucial metrics such as area and delay.","Addressing the challenge posed by the broad spectrum of design complexities - from variations of past designs (e.g., adders and multipliers) to entirely novel configurations (e.g., innovative processor instructions) - requires a nuanced `synthesis recipe` guided by human expertise and intuition.","This study conducts a thorough examination of learning and search techniques for logic synthesis, unearthing a surprising revelation: pre-trained agents, when confronted with entirely novel designs, may veer off course, detrimentally affecting the search trajectory.","We present ABC-RL, a meticulously tuned $\\alpha$ parameter that adeptly adjusts recommendations from pre-trained agents during the search process.","Computed based on similarity scores through nearest neighbor retrieval from the training dataset, ABC-RL yields superior synthesis recipes tailored for a wide array of hardware designs.","Our findings showcase substantial enhancements in the Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to 24.8% compared to state-of-the-art techniques.","Furthermore, ABC-RL achieves an impressive up to 9x reduction in runtime (iso-QoR) when compared to current state-of-the-art methodologies."],"url":"http://arxiv.org/abs/2401.12205v1","category":"cs.LG"}
{"created":"2024-01-22 18:42:31","title":"Unsupervised Machine Learning for the Classification of Astrophysical X-ray Sources","abstract":"The automatic classification of X-ray detections is a necessary step in extracting astrophysical information from compiled catalogs of astrophysical sources. Classification is useful for the study of individual objects, statistics for population studies, as well as for anomaly detection, i.e., the identification of new unexplored phenomena, including transients and spectrally extreme sources. Despite the importance of this task, classification remains challenging in X-ray astronomy due to the lack of optical counterparts and representative training sets. We develop an alternative methodology that employs an unsupervised machine learning approach to provide probabilistic classes to Chandra Source Catalog sources with a limited number of labeled sources, and without ancillary information from optical and infrared catalogs. We provide a catalog of probabilistic classes for 8,756 sources, comprising a total of 14,507 detections, and demonstrate the success of the method at identifying emission from young stellar objects, as well as distinguishing between small-scale and large-scale compact accretors with a significant level of confidence. We investigate the consistency between the distribution of features among classified objects and well-established astrophysical hypotheses such as the unified AGN model. This provides interpretability to the probabilistic classifier. Code and tables are available publicly through GitHub. We provide a web playground for readers to explore our final classification at https://umlcaxs-playground.streamlit.app.","sentences":["The automatic classification of X-ray detections is a necessary step in extracting astrophysical information from compiled catalogs of astrophysical sources.","Classification is useful for the study of individual objects, statistics for population studies, as well as for anomaly detection, i.e., the identification of new unexplored phenomena, including transients and spectrally extreme sources.","Despite the importance of this task, classification remains challenging in X-ray astronomy due to the lack of optical counterparts and representative training sets.","We develop an alternative methodology that employs an unsupervised machine learning approach to provide probabilistic classes to Chandra Source Catalog sources with a limited number of labeled sources, and without ancillary information from optical and infrared catalogs.","We provide a catalog of probabilistic classes for 8,756 sources, comprising a total of 14,507 detections, and demonstrate the success of the method at identifying emission from young stellar objects, as well as distinguishing between small-scale and large-scale compact accretors with a significant level of confidence.","We investigate the consistency between the distribution of features among classified objects and well-established astrophysical hypotheses such as the unified AGN model.","This provides interpretability to the probabilistic classifier.","Code and tables are available publicly through GitHub.","We provide a web playground for readers to explore our final classification at https://umlcaxs-playground.streamlit.app."],"url":"http://arxiv.org/abs/2401.12203v1","category":"astro-ph.IM"}
{"created":"2024-01-22 18:42:20","title":"OK-Robot: What Really Matters in Integrating Open-Knowledge Models for Robotics","abstract":"Remarkable progress has been made in recent years in the fields of vision, language, and robotics. We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects. Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping. In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot. By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training. To evaluate its performance, we run OK-Robot in 10 real-world home environments. The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered environments, OK-Robot's performance increases to 82%. However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules. Videos of our experiments are available on our website: https://ok-robot.github.io","sentences":["Remarkable progress has been made in recent years in the fields of vision, language, and robotics.","We now have vision models capable of recognizing objects based on language queries, navigation systems that can effectively control mobile systems, and grasping models that can handle a wide range of objects.","Despite these advancements, general-purpose applications of robotics still lag behind, even though they rely on these fundamental capabilities of recognition, navigation, and grasping.","In this paper, we adopt a systems-first approach to develop a new Open Knowledge-based robotics framework called OK-Robot.","By combining Vision-Language Models (VLMs) for object detection, navigation primitives for movement, and grasping primitives for object manipulation, OK-Robot offers a integrated solution for pick-and-drop operations without requiring any training.","To evaluate its performance, we run OK-Robot in 10 real-world home environments.","The results demonstrate that OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks, representing a new state-of-the-art in Open Vocabulary Mobile Manipulation (OVMM) with nearly 1.8x the performance of prior work.","On cleaner, uncluttered environments, OK-Robot's performance increases to 82%.","However, the most important insight gained from OK-Robot is the critical role of nuanced details when combining Open Knowledge systems like VLMs with robotic modules.","Videos of our experiments are available on our website: https://ok-robot.github.io"],"url":"http://arxiv.org/abs/2401.12202v1","category":"cs.RO"}
{"created":"2024-01-22 18:39:40","title":"APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference","abstract":"Fine-tuning and inference with large Language Models (LM) are generally known to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency. Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time. To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs. At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency. Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%.","sentences":["Fine-tuning and inference with large Language Models (LM) are generally known to be expensive.","Parameter-efficient fine-tuning over pretrained LMs reduces training memory by updating a small number of LM parameters but does not improve inference efficiency.","Structured pruning improves LM inference efficiency by removing consistent parameter blocks, yet often increases training memory and time.","To improve both training and inference efficiency, we introduce APT that adaptively prunes and tunes parameters for the LMs.","At the early stage of fine-tuning, APT dynamically adds salient tuning parameters for fast and accurate convergence while discarding unimportant parameters for efficiency.","Compared to baselines, our experiments show that APT maintains up to 98% task performance when pruning RoBERTa and T5 models with 40% parameters left while keeping 86.4% LLaMA models' performance with 70% parameters remained.","Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces large LMs memory training footprint by up to 70%."],"url":"http://arxiv.org/abs/2401.12200v1","category":"cs.CL"}
{"created":"2024-01-22 18:38:44","title":"LONEStar: The Lunar Flashlight Optical Navigation Experiment","abstract":"This paper documents the results from the highly successful Lunar flashlight Optical Navigation Experiment with a Star tracker (LONEStar). Launched in December 2022, Lunar Flashlight (LF) was a NASA-funded technology demonstration mission. After a propulsion system anomaly prevented capture in lunar orbit, LF was ejected from the Earth-Moon system and into heliocentric space. NASA subsequently transferred ownership of LF to Georgia Tech to conduct an unfunded extended mission to demonstrate further advanced technology objectives, including LONEStar. From August-December 2023, the LONEStar team performed on-orbit calibration of the optical instrument and a number of different OPNAV experiments. This campaign included the processing of nearly 400 images of star fields, Earth and Moon, and four other planets (Mercury, Mars, Jupiter, and Saturn). LONEStar provided the first on-orbit demonstrations of heliocentric navigation using only optical observations of planets. Of special note is the successful in-flight demonstration of (1) instantaneous triangulation with simultaneous sightings of two planets with the LOST algorithm and (2) dynamic triangulation with sequential sightings of multiple planets.","sentences":["This paper documents the results from the highly successful Lunar flashlight Optical Navigation Experiment with a Star tracker (LONEStar).","Launched in December 2022, Lunar Flashlight (LF) was a NASA-funded technology demonstration mission.","After a propulsion system anomaly prevented capture in lunar orbit, LF was ejected from the Earth-Moon system and into heliocentric space.","NASA subsequently transferred ownership of LF to Georgia Tech to conduct an unfunded extended mission to demonstrate further advanced technology objectives, including LONEStar.","From August-December 2023, the LONEStar team performed on-orbit calibration of the optical instrument and a number of different OPNAV experiments.","This campaign included the processing of nearly 400 images of star fields, Earth and Moon, and four other planets (Mercury, Mars, Jupiter, and Saturn).","LONEStar provided the first on-orbit demonstrations of heliocentric navigation using only optical observations of planets.","Of special note is the successful in-flight demonstration of (1) instantaneous triangulation with simultaneous sightings of two planets with the LOST algorithm and (2) dynamic triangulation with sequential sightings of multiple planets."],"url":"http://arxiv.org/abs/2401.12198v1","category":"cs.CV"}
{"created":"2024-01-22 18:35:02","title":"Programmable EM Sensor Array for Golden-Model Free Run-time Trojan Detection and Localization","abstract":"Side-channel analysis has been proven effective at detecting hardware Trojans in integrated circuits (ICs). However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans. Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures. To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification. PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors' shape, size, and location. Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan. The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors. We fabricated an AES-128 test chip with four AES Hardware Trojans. They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis.","sentences":["Side-channel analysis has been proven effective at detecting hardware Trojans in integrated circuits (ICs).","However, most detection techniques rely on large external probes and antennas for data collection and require a long measurement time to detect Trojans.","Such limitations make these techniques impractical for run-time deployment and ineffective in detecting small Trojans with subtle side-channel signatures.","To overcome these challenges, we propose a Programmable Sensor Array (PSA) for run-time hardware Trojan detection, localization, and identification.","PSA is a tampering-resilient integrated on-chip magnetic field sensor array that can be re-programmed to change the sensors' shape, size, and location.","Using PSA, EM side-channel measurement results collected from sensors at different locations on an IC can be analyzed to localize and identify the Trojan.","The PSA has better performance than conventional external magnetic probes and state-of-the-art on-chip single-coil magnetic field sensors.","We fabricated an AES-128 test chip with four AES Hardware Trojans.","They were successfully detected, located, and identified with the proposed on-chip PSA within 10 milliseconds using our proposed cross-domain analysis."],"url":"http://arxiv.org/abs/2401.12193v1","category":"cs.CR"}
{"created":"2024-01-22 18:34:42","title":"Text Embedding Inversion Attacks on Multilingual Language Models","abstract":"Representing textual information as real-numbered embeddings has become the norm in NLP. Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model. This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them. However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors. %As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security. To this end, this work investigates LLM security from the perspective of multilingual embedding inversion. Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario. Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts. This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori. To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security.","sentences":["Representing textual information as real-numbered embeddings has become the norm in NLP.","Moreover, with the rise of public interest in large language models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a business model.","This is not without outstanding security risks, as previous research has demonstrated that sensitive data can be reconstructed from embeddings, even without knowledge of the underlying model that generated them.","However, such work is limited by its sole focus on English, leaving all other languages vulnerable to attacks by malicious actors.","%As many international and multilingual companies leverage EaaS, there is an urgent need for research into multilingual LLM security.","To this end, this work investigates LLM security from the perspective of multilingual embedding inversion.","Concretely, we define the problem of black-box multilingual and cross-lingual inversion attacks, with special attention to a cross-domain scenario.","Our findings reveal that multilingual models are potentially more vulnerable to inversion attacks than their monolingual counterparts.","This stems from the reduced data requirements for achieving comparable inversion performance in settings where the underlying language is not known a-priori.","To our knowledge, this work is the first to delve into multilinguality within the context of inversion attacks, and our findings highlight the need for further investigation and enhanced defenses in the area of NLP Security."],"url":"http://arxiv.org/abs/2401.12192v1","category":"cs.CL"}
{"created":"2024-01-22 18:32:47","title":"Information Problem in Black Holes and Cosmology and Ghosts in Quadratic Gravity","abstract":"Black hole information problem is the question about unitarity of the evolution operator during the collapse and evaporation of the black hole. One can ask the same question about unitarity of quantum and inflationary cosmology. In this paper we argue that in both cases, for black holes and for cosmology, the answer is negative and we face non-unitarity.   Such a question can not be addressed by using the fixed classical gravitational background since one has to take into account the backreaction. To his end one uses the semi-classical gravity, which includes the expectation value of the energy - momentum tensor operator of the matter fields. One has to renormalize the energy-momentum tensor and one gets an effective action which contains quadratic terms in scalar curvature and Ricci tensor. Such quadratic gravity contains ghosts which in fact lead to violation of unitarity in black holes and cosmology. We discuss the question whether black holes will emit ghosts.   One can try to restrict ourselves to the $f(R)$ gravity that seems is a good approximation to the semi-classical gravity and widely used in cosmology. The black hole entropy in $f(R)$ gravity is different from the Bekenstein-Hawking entropy and from entanglement island entropy. The black hole entropy in $R+R^2$ gravity goes to a constant during the evaporation process. This can be interpreted as another indication to the possible non-unitarity in black holes and cosmology","sentences":["Black hole information problem is the question about unitarity of the evolution operator during the collapse and evaporation of the black hole.","One can ask the same question about unitarity of quantum and inflationary cosmology.","In this paper we argue that in both cases, for black holes and for cosmology, the answer is negative and we face non-unitarity.   ","Such a question can not be addressed by using the fixed classical gravitational background since one has to take into account the backreaction.","To his end one uses the semi-classical gravity, which includes the expectation value of the energy - momentum tensor operator of the matter fields.","One has to renormalize the energy-momentum tensor and one gets an effective action which contains quadratic terms in scalar curvature and Ricci tensor.","Such quadratic gravity contains ghosts which in fact lead to violation of unitarity in black holes and cosmology.","We discuss the question whether black holes will emit ghosts.   ","One can try to restrict ourselves to the $f(R)$ gravity that seems is a good approximation to the semi-classical gravity and widely used in cosmology.","The black hole entropy in $f(R)$ gravity is different from the Bekenstein-Hawking entropy and from entanglement island entropy.","The black hole entropy in $R+R^2$ gravity goes to a constant during the evaporation process.","This can be interpreted as another indication to the possible non-unitarity in black holes and cosmology"],"url":"http://arxiv.org/abs/2401.12191v1","category":"hep-th"}
{"created":"2024-01-22 18:27:08","title":"WARM: On the Benefits of Weight Averaged Reward Models","abstract":"Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives. We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences. As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space. This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training. By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies. Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM.","sentences":["Aligning large language models (LLMs) with human preferences through reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit failures in the reward model (RM) to achieve seemingly high rewards without meeting the underlying objectives.","We identify two primary challenges when designing RMs to mitigate reward hacking: distribution shifts during the RL process and inconsistencies in human preferences.","As a solution, we propose Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then averaging them in the weight space.","This strategy follows the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training.","By averaging weights, WARM improves efficiency compared to the traditional ensembling of predictions, while improving reliability under distribution shifts and robustness to preference inconsistencies.","Our experiments on summarization tasks, using best-of-N and RL methods, shows that WARM improves the overall quality and alignment of LLM predictions; for example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy RL fine-tuned with a single RM."],"url":"http://arxiv.org/abs/2401.12187v1","category":"cs.LG"}
{"created":"2024-01-22 18:24:41","title":"Is Your Kettle Smarter Than a Hacker? A Scalable Tool for Assessing Replay Attack Vulnerabilities on Consumer IoT Devices","abstract":"Consumer Internet of Things (IoT) devices often leverage the local network to communicate with the corresponding companion app or other devices. This has benefits in terms of efficiency since it offloads the cloud. ENISA and NIST security guidelines underscore the importance of enabling default local communication for safety and reliability. Indeed, an IoT device should continue to function in case the cloud connection is not available. While the security of cloud-device connections is typically strengthened through the usage of standard protocols, local connectivity security is frequently overlooked. Neglecting the security of local communication opens doors to various threats, including replay attacks. In this paper, we investigate this class of attacks by designing a systematic methodology for automatically testing IoT devices vulnerability to replay attacks. Specifically, we propose a tool, named REPLIOT, able to test whether a replay attack is successful or not, without prior knowledge of the target devices. We perform thousands of automated experiments using popular commercial devices spanning various vendors and categories. Notably, our study reveals that among these devices, 51% of them do not support local connectivity, thus they are not compliant with the reliability and safety requirements of the ENISA/NIST guidelines. We find that 75% of the remaining devices are vulnerable to replay attacks with REPLIOT having a detection accuracy of 0.98-1. Finally, we investigate the possible causes of this vulnerability, discussing possible mitigation strategies.","sentences":["Consumer Internet of Things (IoT) devices often leverage the local network to communicate with the corresponding companion app or other devices.","This has benefits in terms of efficiency since it offloads the cloud.","ENISA and NIST security guidelines underscore the importance of enabling default local communication for safety and reliability.","Indeed, an IoT device should continue to function in case the cloud connection is not available.","While the security of cloud-device connections is typically strengthened through the usage of standard protocols, local connectivity security is frequently overlooked.","Neglecting the security of local communication opens doors to various threats, including replay attacks.","In this paper, we investigate this class of attacks by designing a systematic methodology for automatically testing IoT devices vulnerability to replay attacks.","Specifically, we propose a tool, named REPLIOT, able to test whether a replay attack is successful or not, without prior knowledge of the target devices.","We perform thousands of automated experiments using popular commercial devices spanning various vendors and categories.","Notably, our study reveals that among these devices, 51% of them do not support local connectivity, thus they are not compliant with the reliability and safety requirements of the ENISA/NIST guidelines.","We find that 75% of the remaining devices are vulnerable to replay attacks with REPLIOT having a detection accuracy of 0.98-1.","Finally, we investigate the possible causes of this vulnerability, discussing possible mitigation strategies."],"url":"http://arxiv.org/abs/2401.12184v1","category":"cs.CR"}
{"created":"2024-01-22 18:18:51","title":"Observation of discrete charge states of a coherent two-level system in a superconducting qubit","abstract":"We report observations of discrete charge states of a coherent dielectric two-level system (TLS) that is strongly coupled to an offset-charge-sensitive superconducting transmon qubit. We measure an offset charge of 0.072$e$ associated with the two TLS eigenstates, which have a transition frequency of 2.9 GHz and a relaxation time exceeding 3 ms. Combining measurements in the strong dispersive and resonant regime, we quantify both transverse and longitudinal couplings of the TLS-qubit interaction. We further perform joint tracking of TLS transitions and quasiparticle tunneling dynamics but find no intrinsic correlations. This study demonstrates microwave-frequency TLS as a source of low-frequency charge noise.","sentences":["We report observations of discrete charge states of a coherent dielectric two-level system (TLS) that is strongly coupled to an offset-charge-sensitive superconducting transmon qubit.","We measure an offset charge of 0.072$e$ associated with the two TLS eigenstates, which have a transition frequency of 2.9 GHz and a relaxation time exceeding 3 ms.","Combining measurements in the strong dispersive and resonant regime, we quantify both transverse and longitudinal couplings of the TLS-qubit interaction.","We further perform joint tracking of TLS transitions and quasiparticle tunneling dynamics but find no intrinsic correlations.","This study demonstrates microwave-frequency TLS as a source of low-frequency charge noise."],"url":"http://arxiv.org/abs/2401.12183v1","category":"quant-ph"}
{"created":"2024-01-22 18:13:44","title":"Tracking before detection using partial orders and optimization","abstract":"This article addresses the problem of multi-object tracking by using a non-deterministic model of target behaviors with hard constraints. To capture the evolution of target features as well as their locations, we permit objects to lie in a general topological target configuration space, rather than a Euclidean space. We obtain tracker performance bounds based on sample rates, and derive a flexible, agnostic tracking algorithm. We demonstrate our algorithm on two scenarios involving laboratory and field data.","sentences":["This article addresses the problem of multi-object tracking by using a non-deterministic model of target behaviors with hard constraints.","To capture the evolution of target features as well as their locations, we permit objects to lie in a general topological target configuration space, rather than a Euclidean space.","We obtain tracker performance bounds based on sample rates, and derive a flexible, agnostic tracking algorithm.","We demonstrate our algorithm on two scenarios involving laboratory and field data."],"url":"http://arxiv.org/abs/2401.12182v1","category":"math.DS"}
{"created":"2024-01-22 18:11:01","title":"Universal Neurons in GPT2 Language Models","abstract":"A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms. In other words, are neural mechanisms universal across different models? In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable. In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5\\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs. We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families. We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set.","sentences":["A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms.","In other words, are neural mechanisms universal across different models?","In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable.","In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5\\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs.","We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families.","We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set."],"url":"http://arxiv.org/abs/2401.12181v1","category":"cs.LG"}
{"created":"2024-01-22 18:10:10","title":"DITTO: Diffusion Inference-Time T-Optimization for Music Generation","abstract":"We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents. Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency. We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model. When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models. Sound examples can be found at https://DITTO-Music.github.io/web/.","sentences":["We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose frame-work for controlling pre-trained text-to-music diffusion models at inference-time via optimizing initial noise latents.","Our method can be used to optimize through any differentiable feature matching loss to achieve a target (stylized) output and leverages gradient checkpointing for memory efficiency.","We demonstrate a surprisingly wide-range of applications for music generation including inpainting, outpainting, and looping as well as intensity, melody, and musical structure control - all without ever fine-tuning the underlying model.","When we compare our approach against related training, guidance, and optimization-based methods, we find DITTO achieves state-of-the-art performance on nearly all tasks, including outperforming comparable approaches on controllability, audio quality, and computational efficiency, thus opening the door for high-quality, flexible, training-free control of diffusion models.","Sound examples can be found at https://DITTO-Music.github.io/web/."],"url":"http://arxiv.org/abs/2401.12179v1","category":"cs.SD"}
{"created":"2024-01-22 18:09:52","title":"In-Context Learning for Extreme Multi-Label Classification","abstract":"Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt. We propose a general program, $\\texttt{Infer--Retrieve--Rank}$, that defines multi-step interactions between LMs and retrievers to efficiently tackle such problems. We implement this program using the $\\texttt{DSPy}$ programming model, which specifies in-context systems in a declarative manner, and use $\\texttt{DSPy}$ optimizers to tune it towards specific datasets by bootstrapping only tens of few-shot examples. Our primary extreme classification program, optimized separately for each task, attains state-of-the-art results across three benchmarks (HOUSE, TECH, TECHWOLF). We apply the same program to a benchmark with vastly different characteristics and attain competitive performance as well (BioDEX). Unlike prior work, our proposed solution requires no finetuning, is easily applicable to new tasks, alleviates prompt engineering, and requires only tens of labeled examples. Our code is public at https://github.com/KarelDO/xmc.dspy.","sentences":["Multi-label classification problems with thousands of classes are hard to solve with in-context learning alone, as language models (LMs) might lack prior knowledge about the precise classes or how to assign them, and it is generally infeasible to demonstrate every class in a prompt.","We propose a general program, $\\texttt{Infer--Retrieve--Rank}$, that defines multi-step interactions between LMs and retrievers to efficiently tackle such problems.","We implement this program using the $\\texttt{DSPy}$ programming model, which specifies in-context systems in a declarative manner, and use $\\texttt{DSPy}$ optimizers to tune it towards specific datasets by bootstrapping only tens of few-shot examples.","Our primary extreme classification program, optimized separately for each task, attains state-of-the-art results across three benchmarks (HOUSE, TECH, TECHWOLF).","We apply the same program to a benchmark with vastly different characteristics and attain competitive performance as well (BioDEX).","Unlike prior work, our proposed solution requires no finetuning, is easily applicable to new tasks, alleviates prompt engineering, and requires only tens of labeled examples.","Our code is public at https://github.com/KarelDO/xmc.dspy."],"url":"http://arxiv.org/abs/2401.12178v1","category":"cs.CL"}
{"created":"2024-01-22 18:09:15","title":"Broiler-Net: A Deep Convolutional Framework for Broiler Behavior Analysis in Poultry Houses","abstract":"Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, minimizing economic losses and bolstering profitability. This paper presents a novel real-time framework for analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors. Specifically, two significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study. The proposed framework comprises three key steps: (1) chicken detection utilizing a state-of-the-art deep learning model, (2) tracking individual chickens across consecutive frames with a fast tracker module, and (3) detecting abnormal behaviors within the video stream. Experimental studies are conducted to evaluate the efficacy of the proposed algorithm in accurately assessing chicken behavior. The results illustrate that our framework provides a precise and efficient solution for real-time anomaly detection, facilitating timely interventions to maintain chicken health and enhance overall productivity on poultry farms. Github: https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis","sentences":["Detecting anomalies in poultry houses is crucial for maintaining optimal chicken health conditions, minimizing economic losses and bolstering profitability.","This paper presents a novel real-time framework for analyzing chicken behavior in cage-free poultry houses to detect abnormal behaviors.","Specifically, two significant abnormalities, namely inactive broiler and huddling behavior, are investigated in this study.","The proposed framework comprises three key steps: (1) chicken detection utilizing a state-of-the-art deep learning model, (2) tracking individual chickens across consecutive frames with a fast tracker module, and (3) detecting abnormal behaviors within the video stream.","Experimental studies are conducted to evaluate the efficacy of the proposed algorithm in accurately assessing chicken behavior.","The results illustrate that our framework provides a precise and efficient solution for real-time anomaly detection, facilitating timely interventions to maintain chicken health and enhance overall productivity on poultry farms.","Github:","https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis"],"url":"http://arxiv.org/abs/2401.12176v1","category":"cs.CV"}
{"created":"2024-01-22 18:08:22","title":"Single-View 3D Human Digitalization with Large Reconstruction Models","abstract":"In this paper, we introduce Human-LRM, a single-stage feed-forward Large Reconstruction Model designed to predict human Neural Radiance Fields (NeRF) from a single image. Our approach demonstrates remarkable adaptability in training using extensive datasets containing 3D scans and multi-view capture. Furthermore, to enhance the model's applicability for in-the-wild scenarios especially with occlusions, we propose a novel strategy that distills multi-view reconstruction into single-view via a conditional triplane diffusion model. This generative extension addresses the inherent variations in human body shapes when observed from a single view, and makes it possible to reconstruct the full body human from an occluded image. Through extensive experiments, we show that Human-LRM surpasses previous methods by a significant margin on several benchmarks.","sentences":["In this paper, we introduce Human-LRM, a single-stage feed-forward Large Reconstruction Model designed to predict human Neural Radiance Fields (NeRF) from a single image.","Our approach demonstrates remarkable adaptability in training using extensive datasets containing 3D scans and multi-view capture.","Furthermore, to enhance the model's applicability for in-the-wild scenarios especially with occlusions, we propose a novel strategy that distills multi-view reconstruction into single-view via a conditional triplane diffusion model.","This generative extension addresses the inherent variations in human body shapes when observed from a single view, and makes it possible to reconstruct the full body human from an occluded image.","Through extensive experiments, we show that Human-LRM surpasses previous methods by a significant margin on several benchmarks."],"url":"http://arxiv.org/abs/2401.12175v1","category":"cs.CV"}
{"created":"2024-01-22 18:05:33","title":"Robust stability analysis of an energy-efficient control in a Networked Control System with application to Unmanned Ground Vehicles","abstract":"In this paper, the robust stability and disturbance rejection performance analysis of an energy-efficient control is addressed in the framework of Networked Control System (NCS). The control scheme under study integrates periodic event-triggered control, packet-based control, time-varying Kalman filter, dual-rate control and prediction techniques, whose design is aimed at reducing energy consumption and bandwidth usage. The robust stability against time-varying model uncertainties is analyzed by means of a suficient condition based on Linear Matrix Inequalities (LMI). Finally, the effectiveness of the proposed approach is experimentally validated in a tracking control for an Unmanned Ground Vehicle (UGV), which is a battery-constrained mobile device with limited computation capacities.","sentences":["In this paper, the robust stability and disturbance rejection performance analysis of an energy-efficient control is addressed in the framework of Networked Control System (NCS).","The control scheme under study integrates periodic event-triggered control, packet-based control, time-varying Kalman filter, dual-rate control and prediction techniques, whose design is aimed at reducing energy consumption and bandwidth usage.","The robust stability against time-varying model uncertainties is analyzed by means of a suficient condition based on Linear Matrix Inequalities (LMI).","Finally, the effectiveness of the proposed approach is experimentally validated in a tracking control for an Unmanned Ground Vehicle (UGV), which is a battery-constrained mobile device with limited computation capacities."],"url":"http://arxiv.org/abs/2401.12172v1","category":"cs.SY"}
{"created":"2024-01-22 18:04:26","title":"Natural Strategic Ability in Stochastic Multi-Agent Systems","abstract":"Strategies synthesized using formal methods can be complex and often require infinite memory, which does not correspond to the expected behavior when trying to model Multi-Agent Systems (MAS). To capture such behaviors, natural strategies are a recently proposed framework striking a balance between the ability of agents to strategize with memory and the model-checking complexity, but until now has been restricted to fully deterministic settings. For the first time, we consider the probabilistic temporal logics PATL and PATL* under natural strategies (NatPATL and NatPATL*, resp.). As main result we show that, in stochastic MAS, NatPATL model-checking is NP-complete when the active coalition is restricted to deterministic strategies. We also give a 2NEXPTIME complexity result for NatPATL* with the same restriction. In the unrestricted case, we give an EXPSPACE complexity for NatPATL and 3EXPSPACE complexity for NatPATL*.","sentences":["Strategies synthesized using formal methods can be complex and often require infinite memory, which does not correspond to the expected behavior when trying to model Multi-Agent Systems (MAS).","To capture such behaviors, natural strategies are a recently proposed framework striking a balance between the ability of agents to strategize with memory and the model-checking complexity, but until now has been restricted to fully deterministic settings.","For the first time, we consider the probabilistic temporal logics PATL and PATL* under natural strategies (NatPATL and NatPATL*, resp.).","As main result we show that, in stochastic MAS, NatPATL model-checking is NP-complete when the active coalition is restricted to deterministic strategies.","We also give a 2NEXPTIME complexity result for NatPATL* with the same restriction.","In the unrestricted case, we give an EXPSPACE complexity for NatPATL and 3EXPSPACE complexity for NatPATL*."],"url":"http://arxiv.org/abs/2401.12170v1","category":"cs.LO"}
{"created":"2024-01-22 18:01:01","title":"SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities","abstract":"Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/","sentences":["Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics.","While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences.","We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data.","To this end, we present a system to facilitate this approach.","We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images.","We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture.","Our work features the first internet-scale 3D spatial reasoning dataset in metric space.","By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA.","Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability.","Project website: https://spatial-vlm.github.io/"],"url":"http://arxiv.org/abs/2401.12168v1","category":"cs.CV"}
{"created":"2024-01-22 17:56:07","title":"Semi-supervised segmentation of land cover images using nonlinear canonical correlation analysis with multiple features and t-SNE","abstract":"Image segmentation is a clustering task whereby each pixel is assigned a cluster label. Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available. This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate. Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical. In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed. Initially, over all pixels, an image data matrix is created in high dimensional feature space. Then, t-SNE projects the high dimensional data onto 3D embedding. By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set. The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results.","sentences":["Image segmentation is a clustering task whereby each pixel is assigned a cluster label.","Remote sensing data usually consists of multiple bands of spectral images in which there exist semantically meaningful land cover subregions, co-registered with other source data such as LIDAR (LIght Detection And Ranging) data, where available.","This suggests that, in order to account for spatial correlation between pixels, a feature vector associated with each pixel may be a vectorized tensor representing the multiple bands and a local patch as appropriate.","Similarly, multiple types of texture features based on a pixel's local patch would also be beneficial for encoding locally statistical information and spatial variations, without necessarily labelling pixel-wise a large amount of ground truth, then training a supervised model, which is sometimes impractical.","In this work, by resorting to label only a small quantity of pixels, a new semi-supervised segmentation approach is proposed.","Initially, over all pixels, an image data matrix is created in high dimensional feature space.","Then, t-SNE projects the high dimensional data onto 3D embedding.","By using radial basis functions as input features, which use the labelled data samples as centres, to pair with the output class labels, a modified canonical correlation analysis algorithm, referred to as RBF-CCA, is introduced which learns the associated projection matrix via the small labelled data set.","The associated canonical variables, obtained for the full image, are applied by k-means clustering algorithm.","The proposed semi-supervised RBF-CCA algorithm has been implemented on several remotely sensed multispectral images, demonstrating excellent segmentation results."],"url":"http://arxiv.org/abs/2401.12164v1","category":"cs.CV"}
{"created":"2024-01-22 17:56:06","title":"On the Evolution During Growth of Regular Boundaries of Bodies into Fractals","abstract":"Generalizing smooth volumetric growth to the singular case, using de Rham currents and flat chains, we demonstrate how regular boundaries of bodies may evolve to fractals.","sentences":["Generalizing smooth volumetric growth to the singular case, using de Rham currents and flat chains, we demonstrate how regular boundaries of bodies may evolve to fractals."],"url":"http://arxiv.org/abs/2401.12163v1","category":"math-ph"}
{"created":"2024-01-22 17:55:16","title":"Automated facial recognition system using deep learning for pain assessment in adults with cerebral palsy","abstract":"Background: Pain assessment in individuals with neurological conditions, especially those with limited self-report ability and altered facial expressions, presents challenges. Existing measures, relying on direct observation by caregivers, lack sensitivity and specificity. In cerebral palsy, pain is a common comorbidity and a reliable evaluation protocol is crucial. Thus, having an automatic system that recognizes facial expressions could be of enormous help when diagnosing pain in this type of patient.   Objectives: 1) to build a dataset of facial pain expressions in individuals with cerebral palsy, and 2) to develop an automated facial recognition system based on deep learning for pain assessment addressed to this population.   Methods: Ten neural networks were trained on three pain image databases, including the UNBC-McMaster Shoulder Pain Expression Archive Database, the Multimodal Intensity Pain Dataset, and the Delaware Pain Database. Additionally, a curated dataset (CPPAIN) was created, consisting of 109 preprocessed facial pain expression images from individuals with cerebral palsy, categorized by two physiotherapists using the Facial Action Coding System observational scale.   Results: InceptionV3 exhibited promising performance on the CP-PAIN dataset, achieving an accuracy of 62.67% and an F1 score of 61.12%. Explainable artificial intelligence techniques revealed consistent essential features for pain identification across models.   Conclusion: This study demonstrates the potential of deep learning models for robust pain detection in populations with neurological conditions and communication disabilities. The creation of a larger dataset specific to cerebral palsy would further enhance model accuracy, offering a valuable tool for discerning subtle and idiosyncratic pain expressions. The insights gained could extend to other complex neurological conditions.","sentences":["Background: Pain assessment in individuals with neurological conditions, especially those with limited self-report ability and altered facial expressions, presents challenges.","Existing measures, relying on direct observation by caregivers, lack sensitivity and specificity.","In cerebral palsy, pain is a common comorbidity and a reliable evaluation protocol is crucial.","Thus, having an automatic system that recognizes facial expressions could be of enormous help when diagnosing pain in this type of patient.   ","Objectives: 1) to build a dataset of facial pain expressions in individuals with cerebral palsy, and 2) to develop an automated facial recognition system based on deep learning for pain assessment addressed to this population.   ","Methods: Ten neural networks were trained on three pain image databases, including the UNBC-McMaster Shoulder Pain Expression Archive Database, the Multimodal Intensity Pain Dataset, and the Delaware Pain Database.","Additionally, a curated dataset (CPPAIN) was created, consisting of 109 preprocessed facial pain expression images from individuals with cerebral palsy, categorized by two physiotherapists using the Facial Action Coding System observational scale.   ","Results: InceptionV3 exhibited promising performance on the CP-PAIN dataset, achieving an accuracy of 62.67% and an F1 score of 61.12%.","Explainable artificial intelligence techniques revealed consistent essential features for pain identification across models.   ","Conclusion: This study demonstrates the potential of deep learning models for robust pain detection in populations with neurological conditions and communication disabilities.","The creation of a larger dataset specific to cerebral palsy would further enhance model accuracy, offering a valuable tool for discerning subtle and idiosyncratic pain expressions.","The insights gained could extend to other complex neurological conditions."],"url":"http://arxiv.org/abs/2401.12161v1","category":"cs.CV"}
{"created":"2024-01-22 17:54:07","title":"Transcending To Notions","abstract":"Social identities play an important role in the dynamics of human societies, and it can be argued that some sense of identification with a larger cause or idea plays a critical role in making humans act responsibly. Often social activists strive to get populations to identify with some cause or notion -- like green energy, diversity, etc. in order to bring about desired social changes. We explore the problem of designing computational models for social identities in the context of autonomous AI agents. For this, we propose an agent model that enables agents to identify with certain notions and show how this affects collective outcomes. We also contrast between associations of identity with rational preferences. The proposed model is simulated in an application context of urban mobility, where we show how changes in social identity affect mobility patterns and collective outcomes.","sentences":["Social identities play an important role in the dynamics of human societies, and it can be argued that some sense of identification with a larger cause or idea plays a critical role in making humans act responsibly.","Often social activists strive to get populations to identify with some cause or notion -- like green energy, diversity, etc. in order to bring about desired social changes.","We explore the problem of designing computational models for social identities in the context of autonomous AI agents.","For this, we propose an agent model that enables agents to identify with certain notions and show how this affects collective outcomes.","We also contrast between associations of identity with rational preferences.","The proposed model is simulated in an application context of urban mobility, where we show how changes in social identity affect mobility patterns and collective outcomes."],"url":"http://arxiv.org/abs/2401.12159v1","category":"cs.MA"}
{"created":"2024-01-22 17:40:15","title":"Uncoded Storage Coded Transmission Elastic Computing with Straggler Tolerance in Heterogeneous Systems","abstract":"In 2018, Yang et al. introduced a novel and effective approach, using maximum distance separable (MDS) codes, to mitigate the impact of elasticity in cloud computing systems. This approach is referred to as coded elastic computing. Some limitations of this approach include that it assumes all virtual machines have the same computing speeds and storage capacities, and it cannot tolerate stragglers for matrix-matrix multiplications. In order to resolve these limitations, in this paper, we introduce a new combinatorial optimization framework, named uncoded storage coded transmission elastic computing (USCTEC), for heterogeneous speeds and storage constraints, aiming to minimize the expected computation time for matrix-matrix multiplications, under the consideration of straggler tolerance. Within this framework, we propose optimal solutions with straggler tolerance under relaxed storage constraints. Moreover, we propose a heuristic algorithm that considers the heterogeneous storage constraints. Our results demonstrate that the proposed algorithm outperforms baseline solutions utilizing cyclic storage placements, in terms of both expected computation time and storage size.","sentences":["In 2018, Yang et al. introduced a novel and effective approach, using maximum distance separable (MDS) codes, to mitigate the impact of elasticity in cloud computing systems.","This approach is referred to as coded elastic computing.","Some limitations of this approach include that it assumes all virtual machines have the same computing speeds and storage capacities, and it cannot tolerate stragglers for matrix-matrix multiplications.","In order to resolve these limitations, in this paper, we introduce a new combinatorial optimization framework, named uncoded storage coded transmission elastic computing (USCTEC), for heterogeneous speeds and storage constraints, aiming to minimize the expected computation time for matrix-matrix multiplications, under the consideration of straggler tolerance.","Within this framework, we propose optimal solutions with straggler tolerance under relaxed storage constraints.","Moreover, we propose a heuristic algorithm that considers the heterogeneous storage constraints.","Our results demonstrate that the proposed algorithm outperforms baseline solutions utilizing cyclic storage placements, in terms of both expected computation time and storage size."],"url":"http://arxiv.org/abs/2401.12151v1","category":"cs.IT"}
{"created":"2024-01-22 17:36:23","title":"Personalized Over-the-Air Federated Learning with Personalized Reconfigurable Intelligent Surfaces","abstract":"Over-the-air federated learning (OTA-FL) provides bandwidth-efficient learning by leveraging the inherent superposition property of wireless channels. Personalized federated learning balances performance for users with diverse datasets, addressing real-life data heterogeneity. We propose the first personalized OTA-FL scheme through multi-task learning, assisted by personal reconfigurable intelligent surfaces (RIS) for each user. We take a cross-layer approach that optimizes communication and computation resources for global and personalized tasks in time-varying channels with imperfect channel state information, using multi-task learning for non-i.i.d data. Our PROAR-PFed algorithm adaptively designs power, local iterations, and RIS configurations. We present convergence analysis for non-convex objectives and demonstrate that PROAR-PFed outperforms state-of-the-art on the Fashion-MNIST dataset.","sentences":["Over-the-air federated learning (OTA-FL) provides bandwidth-efficient learning by leveraging the inherent superposition property of wireless channels.","Personalized federated learning balances performance for users with diverse datasets, addressing real-life data heterogeneity.","We propose the first personalized OTA-FL scheme through multi-task learning, assisted by personal reconfigurable intelligent surfaces (RIS) for each user.","We take a cross-layer approach that optimizes communication and computation resources for global and personalized tasks in time-varying channels with imperfect channel state information, using multi-task learning for non-i.i.d data.","Our PROAR-PFed algorithm adaptively designs power, local iterations, and RIS configurations.","We present convergence analysis for non-convex objectives and demonstrate that PROAR-PFed outperforms state-of-the-art on the Fashion-MNIST dataset."],"url":"http://arxiv.org/abs/2401.12149v1","category":"cs.IT"}
{"created":"2024-01-22 17:30:49","title":"An Efficient Finite Difference-based Implicit Solver for Phase-Field Equations with Spatially and Temporally Varying Parameters","abstract":"The phase field method is an effective tool for modeling microstructure evolution in materials. Many efficient implicit numerical solvers have been proposed for phase field simulations under uniform and time-invariant model parameters. We use Eyre's theorem to develop an unconditionally stable implicit solver for spatially non-uniform and time-varying model parameters. The accuracy, unconditional stability, and efficiency of the solver is validated against benchmarking examples. In its current form, the solver requires a uniform mesh and may only be applied to problems with periodic, Neumann, or mixed periodic and Neumann boundary conditions.","sentences":["The phase field method is an effective tool for modeling microstructure evolution in materials.","Many efficient implicit numerical solvers have been proposed for phase field simulations under uniform and time-invariant model parameters.","We use Eyre's theorem to develop an unconditionally stable implicit solver for spatially non-uniform and time-varying model parameters.","The accuracy, unconditional stability, and efficiency of the solver is validated against benchmarking examples.","In its current form, the solver requires a uniform mesh and may only be applied to problems with periodic, Neumann, or mixed periodic and Neumann boundary conditions."],"url":"http://arxiv.org/abs/2401.12147v1","category":"math.NA"}
{"created":"2024-01-22 17:26:55","title":"Anisotropy Is Inherent to Self-Attention in Transformers","abstract":"The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers. In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity). Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens. We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences. We also show that the anisotropy problem extends to Transformers trained on other modalities. Our observations suggest that anisotropy is actually inherent to Transformers-based models.","sentences":["The representation degeneration problem is a phenomenon that is widely observed among self-supervised learning methods based on Transformers.","In NLP, it takes the form of anisotropy, a singular property of hidden representations which makes them unexpectedly close to each other in terms of angular distance (cosine-similarity).","Some recent works tend to show that anisotropy is a consequence of optimizing the cross-entropy loss on long-tailed distributions of tokens.","We show in this paper that anisotropy can also be observed empirically in language models with specific objectives that should not suffer directly from the same consequences.","We also show that the anisotropy problem extends to Transformers trained on other modalities.","Our observations suggest that anisotropy is actually inherent to Transformers-based models."],"url":"http://arxiv.org/abs/2401.12143v1","category":"cs.CL"}
{"created":"2024-01-22 17:19:08","title":"Spin Wave Threshold Gate","abstract":"While Spin Waves (SW) interaction provides natural support for low power Majority (MAJ) gate implementations many hurdles still exists on the road towards the realization of practically relevant SW circuits. In this paper we leave the SW interaction avenue and propose Threshold Logic (TL) inspired SW computing, which relies on successive phase rotations applied to one single SW instead of on the interference of an odd number of SWs. After providing a short TL inside we introduce the SW TL gate concept and discuss the way to mirror TL gate weight and threshold values into physical phase-shifter parameters. Subsequently, we design and demonstrate proper operation of a SW TL based Full Adder (FA) by means of micro-magnetic simulations. We conclude the paper by providing inside on the potential advantages of our proposal by means of a conceptual comparison of MAJ and TL based FA implementations.","sentences":["While Spin Waves (SW) interaction provides natural support for low power Majority (MAJ) gate implementations many hurdles still exists on the road towards the realization of practically relevant SW circuits.","In this paper we leave the SW interaction avenue and propose Threshold Logic (TL) inspired SW computing, which relies on successive phase rotations applied to one single SW instead of on the interference of an odd number of SWs.","After providing a short TL inside we introduce the SW TL gate concept and discuss the way to mirror TL gate weight and threshold values into physical phase-shifter parameters.","Subsequently, we design and demonstrate proper operation of a SW TL based Full Adder (FA) by means of micro-magnetic simulations.","We conclude the paper by providing inside on the potential advantages of our proposal by means of a conceptual comparison of MAJ and TL based FA implementations."],"url":"http://arxiv.org/abs/2401.12136v1","category":"cs.ET"}
{"created":"2024-01-22 17:18:53","title":"Accelerating Continuous Variable Coherent Ising Machines via Momentum","abstract":"The Coherent Ising Machine (CIM) is a non-conventional architecture that takes inspiration from physical annealing processes to solve Ising problems heuristically. Its dynamics are naturally continuous and described by a set of ordinary differential equations that have been proven to be useful for the optimization of continuous variables non-convex quadratic optimization problems. The dynamics of such Continuous Variable CIMs (CV-CIM) encourage optimization via optical pulses whose amplitudes are determined by the negative gradient of the objective; however, standard gradient descent is known to be trapped by local minima and hampered by poor problem conditioning. In this work, we propose to modify the CV-CIM dynamics using more sophisticated pulse injections based on tried-and-true optimization techniques such as momentum and Adam. Through numerical experiments, we show that the momentum and Adam updates can significantly speed up the CV-CIM's convergence and improve sample diversity over the original CV-CIM dynamics. We also find that the Adam-CV-CIM's performance is more stable as a function of feedback strength, especially on poorly conditioned instances, resulting in an algorithm that is more robust, reliable, and easily tunable. More broadly, we identify the CIM dynamical framework as a fertile opportunity for exploring the intersection of classical optimization and modern analog computing.","sentences":["The Coherent Ising Machine (CIM) is a non-conventional architecture that takes inspiration from physical annealing processes to solve Ising problems heuristically.","Its dynamics are naturally continuous and described by a set of ordinary differential equations that have been proven to be useful for the optimization of continuous variables non-convex quadratic optimization problems.","The dynamics of such Continuous Variable CIMs (CV-CIM) encourage optimization via optical pulses whose amplitudes are determined by the negative gradient of the objective; however, standard gradient descent is known to be trapped by local minima and hampered by poor problem conditioning.","In this work, we propose to modify the CV-CIM dynamics using more sophisticated pulse injections based on tried-and-true optimization techniques such as momentum and Adam.","Through numerical experiments, we show that the momentum and Adam updates can significantly speed up the CV-CIM's convergence and improve sample diversity over the original CV-CIM dynamics.","We also find that the Adam-CV-CIM's performance is more stable as a function of feedback strength, especially on poorly conditioned instances, resulting in an algorithm that is more robust, reliable, and easily tunable.","More broadly, we identify the CIM dynamical framework as a fertile opportunity for exploring the intersection of classical optimization and modern analog computing."],"url":"http://arxiv.org/abs/2401.12135v1","category":"math.OC"}
{"created":"2024-01-22 17:18:50","title":"The Early Universe as an Open Quantum System: Complexity and Decoherence","abstract":"In this work, we extend previous results, demonstrating how complexity in an open quantum system can identify decoherence between two fields, even in the presence of an accelerating background. Using the curved-space Caldeira-Leggett two-field model in de Sitter as our toy model, we discover a distinctive feature in the growth of complexity of purification, providing an alternative diagnostic for studying decoherence when the adiabatic perturbation is coupled to a heavy field. This paper initiates a new pathway to explore the features of quantum complexity in an accelerating background, thereby expanding our understanding of the evolution of primordial cosmological perturbations in the early universe.","sentences":["In this work, we extend previous results, demonstrating how complexity in an open quantum system can identify decoherence between two fields, even in the presence of an accelerating background.","Using the curved-space Caldeira-Leggett two-field model in de Sitter as our toy model, we discover a distinctive feature in the growth of complexity of purification, providing an alternative diagnostic for studying decoherence when the adiabatic perturbation is coupled to a heavy field.","This paper initiates a new pathway to explore the features of quantum complexity in an accelerating background, thereby expanding our understanding of the evolution of primordial cosmological perturbations in the early universe."],"url":"http://arxiv.org/abs/2401.12134v1","category":"hep-th"}
{"created":"2024-01-22 17:15:02","title":"VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear Responses in VR Stand-up Interactive Games","abstract":"Understanding and recognizing emotions are important and challenging issues in the metaverse era. Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications. In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players. We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively. We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets. The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope. We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments. Moreover, we discussed the implications of this work for communities and applications. The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD.","sentences":["Understanding and recognizing emotions are important and challenging issues in the metaverse era.","Understanding, identifying, and predicting fear, which is one of the fundamental human emotions, in virtual reality (VR) environments plays an essential role in immersive game development, scene development, and next-generation virtual human-computer interaction applications.","In this article, we used VR horror games as a medium to analyze fear emotions by collecting multi-modal data (posture, audio, and physiological signals) from 23 players.","We used an LSTM-based model to predict fear with accuracies of 65.31% and 90.47% under 6-level classification (no fear and five different levels of fear) and 2-level classification (no fear and fear), respectively.","We constructed a multi-modal natural behavior dataset of immersive human fear responses (VRMN-bD) and compared it with existing relevant advanced datasets.","The results show that our dataset has fewer limitations in terms of collection method, data scale and audience scope.","We are unique and advanced in targeting multi-modal datasets of fear and behavior in VR stand-up interactive environments.","Moreover, we discussed the implications of this work for communities and applications.","The dataset and pre-trained model are available at https://github.com/KindOPSTAR/VRMN-bD."],"url":"http://arxiv.org/abs/2401.12133v1","category":"cs.HC"}
{"created":"2024-01-22 17:14:47","title":"Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis Using Sequential Multisequence MRI","abstract":"Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures. Our hypothesis is that quantum models will provide competitive performance. Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS. These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT). Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss. Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20. Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631). Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001). Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time. Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging.","sentences":["Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term Memory (LSTM) models were studied to provide sequential relationships for each timepoint in MRIs of patients with Multiple Sclerosis (MS).","In this pilot study, we compared three QCNN-LSTM models for binary classification of MS disability benchmarked against classical neural network architectures.","Our hypothesis is that quantum models will provide competitive performance.","Methods Matrix Product State (MPS), reverse Multistate Entanglement Renormalization Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM layer to process near-annual MRI data of patients diagnosed with MS.","These were benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision Transformer (ViViT).","Predicted logits were measured against ground truth labels of each patient's Extended Disability Severity Score (EDSS) using binary cross-entropy loss.","Training/validation/holdout testing was partitioned using 5-fold cross validation with a total split of 60:20:20.","Levene's test of variance was used to measure statistical difference and Student's t-test for paired model differences in mean.","Results The MPS-LSTM, reverse MERA-LSTM, and TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively (p-value 0.915).","VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73 and 0.77, respectively (p-value 0.631).","Overall variance and mean were not statistically significant (p-value 0.713), however, time to train was significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218, respectively, p-value <0.001).","Conclusion QCNN-LSTM models perform competitively to their classical counterparts with greater efficiency in train time.","Clinically, these can add value in terms of efficiency to time-dependent deep learning prediction of disease progression based upon medical imaging."],"url":"http://arxiv.org/abs/2401.12132v1","category":"cs.LG"}
{"created":"2024-01-22 17:13:50","title":"NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis","abstract":"We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for reactive synthesis. At the core of the solver lies a seamless integration of neural and symbolic approaches to solving the reactive synthesis problem. To ensure soundness, the neural engine is coupled with model checkers verifying the predictions of the underlying neural models. The open-source implementation of NeuroSynt provides an integration framework for reactive synthesis in which new neural and state-of-the-art symbolic approaches can be seamlessly integrated. Extensive experiments demonstrate its efficacy in handling challenging specifications, enhancing the state-of-the-art reactive synthesis solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP benchmarks.","sentences":["We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for reactive synthesis.","At the core of the solver lies a seamless integration of neural and symbolic approaches to solving the reactive synthesis problem.","To ensure soundness, the neural engine is coupled with model checkers verifying the predictions of the underlying neural models.","The open-source implementation of NeuroSynt provides an integration framework for reactive synthesis in which new neural and state-of-the-art symbolic approaches can be seamlessly integrated.","Extensive experiments demonstrate its efficacy in handling challenging specifications, enhancing the state-of-the-art reactive synthesis solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP benchmarks."],"url":"http://arxiv.org/abs/2401.12131v1","category":"cs.LO"}
{"created":"2024-01-22 17:11:01","title":"Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy","abstract":"As deep neural networks become adopted in high-stakes domains, it is crucial to be able to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration despite high confidence. Among many others, existing methods use the following two scores to do so without training on any apriori OOD examples: a learned temperature and an energy score. In this paper we introduce Ablated Learned Temperature Energy (or \"AbeT\" for short), a method which combines these prior methods in novel ways with effective modifications. Due to these contributions, AbeT lowers the False Positive Rate at $95\\%$ True Positive Rate (FPR@95) by $35.39\\%$ in classification (averaged across all ID and OOD datasets measured) compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes. We additionally provide empirical insights as to how our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time. Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively - with an AUROC increase of $5.15\\%$ in object detection and both a decrease in FPR@95 of $41.48\\%$ and an increase in AUPRC of $34.20\\%$ on average in semantic segmentation compared to previous state of the art.","sentences":["As deep neural networks become adopted in high-stakes domains, it is crucial to be able to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration despite high confidence.","Among many others, existing methods use the following two scores to do so without training on any apriori OOD examples: a learned temperature and an energy score.","In this paper we introduce Ablated Learned Temperature Energy (or \"AbeT\" for short), a method which combines these prior methods in novel ways with effective modifications.","Due to these contributions, AbeT lowers the False Positive Rate at $95\\%$ True Positive Rate (FPR@95) by $35.39\\%$ in classification (averaged across all ID and OOD datasets measured) compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes.","We additionally provide empirical insights as to how our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time.","Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively - with an AUROC increase of $5.15\\%$ in object detection and both a decrease in FPR@95 of $41.48\\%$ and an increase in AUPRC of $34.20\\%$ on average in semantic segmentation compared to previous state of the art."],"url":"http://arxiv.org/abs/2401.12129v1","category":"cs.CV"}
{"created":"2024-01-22 17:08:54","title":"CodeTailor: Personalized Parsons Puzzles are Preferred Over AI-Generated Solutions to Support Learning","abstract":"Programming can be challenging for novices, but it is difficult to provide high-quality, comprehensive, and timely support at scale. Generative AI and its products, like ChatGPT, can create a solution for most introductory programming problems. However, students may become overly reliant on these tools for quick code generation and homework completion, leading to reduced engagement and limited learning. In this work, we present \\sys{}, a system that utilizes large language models (LLM) while still promoting students' cognitive engagement. \\sys{} provides a personalized Parsons puzzle to support struggling students. In a Parsons puzzle, students place mixed-up code blocks in the correct order to solve a problem. A technical evaluation with 800 incorrect student code demonstrated that \\sys{} can efficiently create high-quality (correct, personalized, and concise) Parsons puzzles for students. In a within-subjects experiment with 18 novice programmers, most students rated using \\sys{} as more engaging, and they preferred \\sys{} for learning rather than simply receiving an AI-generated solution. Additionally, students recalled more new elements from the supported practice to the posttest after using \\sys{}, compared to when they simply received a direct solution. Qualitative observations and interviews provided evidence for the benefits of \\sys{} including emphasizing algorithmic thinking, fostering continuity in learning, promoting metacognitive reflection, and boosting student confidence. We conclude by suggesting future designs for applying generative AI in a way that minimizes over-reliance and enhances learning.","sentences":["Programming can be challenging for novices, but it is difficult to provide high-quality, comprehensive, and timely support at scale.","Generative AI and its products, like ChatGPT, can create a solution for most introductory programming problems.","However, students may become overly reliant on these tools for quick code generation and homework completion, leading to reduced engagement and limited learning.","In this work, we present \\sys{}, a system that utilizes large language models (LLM) while still promoting students' cognitive engagement.","\\sys{} provides a personalized Parsons puzzle to support struggling students.","In a Parsons puzzle, students place mixed-up code blocks in the correct order to solve a problem.","A technical evaluation with 800 incorrect student code demonstrated that \\sys{} can efficiently create high-quality (correct, personalized, and concise) Parsons puzzles for students.","In a within-subjects experiment with 18 novice programmers, most students rated using \\sys{} as more engaging, and they preferred \\sys{} for learning rather than simply receiving an AI-generated solution.","Additionally, students recalled more new elements from the supported practice to the posttest after using \\sys{}, compared to when they simply received a direct solution.","Qualitative observations and interviews provided evidence for the benefits of \\sys{} including emphasizing algorithmic thinking, fostering continuity in learning, promoting metacognitive reflection, and boosting student confidence.","We conclude by suggesting future designs for applying generative AI in a way that minimizes over-reliance and enhances learning."],"url":"http://arxiv.org/abs/2401.12125v1","category":"cs.CY"}
{"created":"2024-01-22 17:05:16","title":"Improving genetic algorithms performance via deterministic population shrinkage","abstract":"Despite the intuition that the same population size is not needed throughout the run of an Evolutionary Algorithm (EA), most EAs use a fixed population size. This paper presents an empirical study on the possible benefits of a Simple Variable Population Sizing (SVPS) scheme on the performance of Genetic Algorithms (GAs). It consists in decreasing the population for a GA run following a predetermined schedule, configured by a speed and a severity parameter. The method uses as initial population size an estimation of the minimum size needed to supply enough building blocks, using a fixed-size selectorecombinative GA converging within some confidence interval toward good solutions for a particular problem. Following this methodology, a scalability analysis is conducted on deceptive, quasi-deceptive, and non-deceptive trap functions in order to assess whether SVPS-GA improves performances compared to a fixed-size GA under different problem instances and difficulty levels. Results show several combinations of speed-severity where SVPS-GA preserves the solution quality while improving performances, by reducing the number of evaluations needed for success.","sentences":["Despite the intuition that the same population size is not needed throughout the run of an Evolutionary Algorithm (EA), most EAs use a fixed population size.","This paper presents an empirical study on the possible benefits of a Simple Variable Population Sizing (SVPS) scheme on the performance of Genetic Algorithms (GAs).","It consists in decreasing the population for a GA run following a predetermined schedule, configured by a speed and a severity parameter.","The method uses as initial population size an estimation of the minimum size needed to supply enough building blocks, using a fixed-size selectorecombinative GA converging within some confidence interval toward good solutions for a particular problem.","Following this methodology, a scalability analysis is conducted on deceptive, quasi-deceptive, and non-deceptive trap functions in order to assess whether SVPS-GA improves performances compared to a fixed-size GA under different problem instances and difficulty levels.","Results show several combinations of speed-severity where SVPS-GA preserves the solution quality while improving performances, by reducing the number of evaluations needed for success."],"url":"http://arxiv.org/abs/2401.12121v1","category":"cs.NE"}
{"created":"2024-01-22 17:05:04","title":"Centralization in Block Building and Proposer-Builder Separation","abstract":"The goal of this paper is to rigorously interrogate conventional wisdom about centralization in block-building (due to, e.g., MEV and private order flow) and the outsourcing of block-building by validators to specialists (i.e., proposer-builder separation):   1. Does heterogeneity in skills and knowledge across block producers inevitably lead to centralization?   2. Does proposer-builder separation eliminate heterogeneity and preserve decentralization among proposers?   This paper develops mathematical models and results that offer answers to these questions:   1. In a game-theoretic model with endogenous staking, heterogeneous block producer rewards, and staking costs, we quantify the extent to which heterogeneous rewards lead to concentration in the equilibrium staking distribution.   2. In a stochastic model in which heterogeneous block producers repeatedly reinvest rewards into staking, we quantify, as a function of the block producer heterogeneity, the rate at which stake concentrates on the most sophisticated block producers.   3. In a model with heterogeneous proposers and specialized builders, we quantify, as a function of the competitiveness of the builder ecosystem, the extent to which proposer-builder separation reduces the heterogeneity in rewards across different proposers.   Our models and results take advantage of connections to contest design, P\\'olya urn processes, and auction theory.","sentences":["The goal of this paper is to rigorously interrogate conventional wisdom about centralization in block-building (due to, e.g., MEV and private order flow) and the outsourcing of block-building by validators to specialists (i.e., proposer-builder separation):   1.","Does heterogeneity in skills and knowledge across block producers inevitably lead to centralization?   ","2. Does proposer-builder separation eliminate heterogeneity and preserve decentralization among proposers?   ","This paper develops mathematical models and results that offer answers to these questions:   1.","In a game-theoretic model with endogenous staking, heterogeneous block producer rewards, and staking costs, we quantify the extent to which heterogeneous rewards lead to concentration in the equilibrium staking distribution.   ","2.","In a stochastic model in which heterogeneous block producers repeatedly reinvest rewards into staking, we quantify, as a function of the block producer heterogeneity, the rate at which stake concentrates on the most sophisticated block producers.   ","3.","In a model with heterogeneous proposers and specialized builders, we quantify, as a function of the competitiveness of the builder ecosystem, the extent to which proposer-builder separation reduces the heterogeneity in rewards across different proposers.   ","Our models and results take advantage of connections to contest design, P\\'olya urn processes, and auction theory."],"url":"http://arxiv.org/abs/2401.12120v1","category":"cs.GT"}
{"created":"2024-01-22 17:04:43","title":"Temperature as Joules per Bit","abstract":"Boltzmann's constant reflects a historical misunderstanding of the concept of entropy, whose informational nature is obfuscated when expressed in J/K. We suggest that the development of temperature and energy, historically prior to that of entropy, does not amount to their logical priority: Temperature should be defined in terms of entropy, not vice versa. Following the precepts of information theory, entropy is measured in bits, and coincides with information capacity at thermodynamic equilibrium. Consequently, not only is the temperature of an equilibrated system expressed in J/bit, but it acquires an operational meaning: It is the cost in energy to increase its information capacity by 1 bit. Our proposal also supports the notion of available capacity, analogous to free energy. Finally, it simplifies Landauer's cost and clarifies that it is a cost of displacement, not of erasure.","sentences":["Boltzmann's constant reflects a historical misunderstanding of the concept of entropy, whose informational nature is obfuscated when expressed in J/K. We suggest that the development of temperature and energy, historically prior to that of entropy, does not amount to their logical priority: Temperature should be defined in terms of entropy, not vice versa.","Following the precepts of information theory, entropy is measured in bits, and coincides with information capacity at thermodynamic equilibrium.","Consequently, not only is the temperature of an equilibrated system expressed in J/bit, but it acquires an operational meaning: It is the cost in energy to increase its information capacity by 1 bit.","Our proposal also supports the notion of available capacity, analogous to free energy.","Finally, it simplifies Landauer's cost and clarifies that it is a cost of displacement, not of erasure."],"url":"http://arxiv.org/abs/2401.12119v1","category":"quant-ph"}
{"created":"2024-01-22 16:57:05","title":"The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models","abstract":"While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with various methods, such as Chain-of-Thought prompting, resulting in a significant (up to 100%) boost in performance.","sentences":["While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs).","These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities.","However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited.","In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices.","Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models.","We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings.","Finally, to improve MLLMs' performance, we experiment with various methods, such as Chain-of-Thought prompting, resulting in a significant (up to 100%) boost in performance."],"url":"http://arxiv.org/abs/2401.12117v1","category":"cs.CL"}
{"created":"2024-01-22 16:51:26","title":"Improved accuracy of continuum surface flux models for metal additive manufacturing melt pool simulations","abstract":"Computational modeling of the melt pool dynamics in laser-based powder bed fusion metal additive manufacturing (PBF-LB/M) promises to shed light on fundamental defect generation mechanisms. These processes are typically accompanied by rapid evaporation so that the evaporation-induced recoil pressure and cooling arise as major driving forces for fluid dynamics and temperature evolution. The magnitude of these interface fluxes depends exponentially on the melt pool surface temperature, which, therefore, must be predicted with high accuracy. The present work utilizes a diffuse interface model based on a continuum surface flux (CSF) description on the interfaces to study dimensionally reduced thermal two-phase problems representing PBF-LB/M in a finite element framework. It is demonstrated that the extreme temperature gradients combined with the high ratios of material properties between metal and ambient gas lead to significant errors in the interface temperatures and fluxes when classical CSF approaches, along with typical interface thicknesses and discretizations, are applied. A novel parameter-scaled CSF approach is proposed, which is constructed to yield a smoother temperature rate in the diffuse interface region, significantly increasing the solution accuracy. The interface thickness required to predict the temperature field with a given level of accuracy is less restrictive by at least one order of magnitude for the proposed parameter-scaled CSF approach compared to classical CSF, drastically reducing computational costs. Finally, we showcased the general applicability of the parameter-scaled CSF to a three-dimensional simulation of stationary laser melting of PBF-LB/M considering the fully coupled thermo-hydrodynamic multi-phase problem, including phase change.","sentences":["Computational modeling of the melt pool dynamics in laser-based powder bed fusion metal additive manufacturing (PBF-LB/M) promises to shed light on fundamental defect generation mechanisms.","These processes are typically accompanied by rapid evaporation so that the evaporation-induced recoil pressure and cooling arise as major driving forces for fluid dynamics and temperature evolution.","The magnitude of these interface fluxes depends exponentially on the melt pool surface temperature, which, therefore, must be predicted with high accuracy.","The present work utilizes a diffuse interface model based on a continuum surface flux (CSF) description on the interfaces to study dimensionally reduced thermal two-phase problems representing PBF-LB/M in a finite element framework.","It is demonstrated that the extreme temperature gradients combined with the high ratios of material properties between metal and ambient gas lead to significant errors in the interface temperatures and fluxes when classical CSF approaches, along with typical interface thicknesses and discretizations, are applied.","A novel parameter-scaled CSF approach is proposed, which is constructed to yield a smoother temperature rate in the diffuse interface region, significantly increasing the solution accuracy.","The interface thickness required to predict the temperature field with a given level of accuracy is less restrictive by at least one order of magnitude for the proposed parameter-scaled CSF approach compared to classical CSF, drastically reducing computational costs.","Finally, we showcased the general applicability of the parameter-scaled CSF to a three-dimensional simulation of stationary laser melting of PBF-LB/M considering the fully coupled thermo-hydrodynamic multi-phase problem, including phase change."],"url":"http://arxiv.org/abs/2401.12114v1","category":"cs.CE"}
{"created":"2024-01-22 16:51:01","title":"Extracting Formulae in Many-Valued Logic from Deep Neural Networks","abstract":"We propose a new perspective on deep ReLU networks, namely as circuit counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV) generalization of Boolean logic. An algorithm for extracting formulae in MV logic from deep ReLU networks is presented. As the algorithm applies to networks with general, in particular also real-valued, weights, it can be used to extract logical formulae from deep ReLU networks trained on data.","sentences":["We propose a new perspective on deep ReLU networks, namely as circuit counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV) generalization of Boolean logic.","An algorithm for extracting formulae in MV logic from deep ReLU networks is presented.","As the algorithm applies to networks with general, in particular also real-valued, weights, it can be used to extract logical formulae from deep ReLU networks trained on data."],"url":"http://arxiv.org/abs/2401.12113v1","category":"cs.LG"}
{"created":"2024-01-22 16:49:17","title":"Constrained Multi-Tildes: Derived Term and Position Automata","abstract":"Multi-tildes are regular operators that were introduced to enhance the factorization power of regular expressions, allowing us to add the empty word in several factors of a catenation product of languages. In addition to multi-bars, which dually remove the empty word, they allow representing any acyclic automaton by a linear-sized expression, whereas the lower bound is exponential in the classic case.   In this paper, we extend multi-tildes from disjunctive combinations to any Boolean combination, allowing us to exponentially enhance the factorization power of tildes expressions. Moreover, we show how to convert these expressions into finite automata and give a Haskell implementation of them using advanced techniques of functional programming.","sentences":["Multi-tildes are regular operators that were introduced to enhance the factorization power of regular expressions, allowing us to add the empty word in several factors of a catenation product of languages.","In addition to multi-bars, which dually remove the empty word, they allow representing any acyclic automaton by a linear-sized expression, whereas the lower bound is exponential in the classic case.   ","In this paper, we extend multi-tildes from disjunctive combinations to any Boolean combination, allowing us to exponentially enhance the factorization power of tildes expressions.","Moreover, we show how to convert these expressions into finite automata and give a Haskell implementation of them using advanced techniques of functional programming."],"url":"http://arxiv.org/abs/2401.12111v1","category":"cs.FL"}
{"created":"2024-01-22 16:46:00","title":"Weak second-order quantum state diffusion unraveling of the Lindblad master equation","abstract":"Abstract Simulating mixed-state evolution in open quantum systems is crucial for various chemical physics, quantum optics, and computer science applications. These simulations typically follow the Lindblad master equation dynamics. An alternative approach known as quantum state diffusion unraveling is based on the trajectories of pure states generated by random wave functions, which evolve according to a nonlinear It\\^o-Schr\\\"odinger equation (ISE). This study introduces weak first- and second-order solvers for the ISE based on directly applying the It\\^o-Taylor expansion with exact derivatives in the interaction picture. We tested the method on free and driven Morse oscillators coupled to a thermal environment and found that both orders allowed practical estimation with a few dozen iterations. The variance was relatively small compared to the linear unraveling and did not grow with time. The second-order solver delivers much higher accuracy and stability with bigger time steps than the first-order scheme, with a small additional workload. However, the second-order algorithm has quadratic complexity with the number of Lindblad operators as opposed to the linear complexity of the first-order algorithm.","sentences":["Abstract Simulating mixed-state evolution in open quantum systems is crucial for various chemical physics, quantum optics, and computer science applications.","These simulations typically follow the Lindblad master equation dynamics.","An alternative approach known as quantum state diffusion unraveling is based on the trajectories of pure states generated by random wave functions, which evolve according to a nonlinear It\\^o-Schr\\\"odinger equation (ISE).","This study introduces weak first- and second-order solvers for the ISE based on directly applying the It\\^o-Taylor expansion with exact derivatives in the interaction picture.","We tested the method on free and driven Morse oscillators coupled to a thermal environment and found that both orders allowed practical estimation with a few dozen iterations.","The variance was relatively small compared to the linear unraveling and did not grow with time.","The second-order solver delivers much higher accuracy and stability with bigger time steps than the first-order scheme, with a small additional workload.","However, the second-order algorithm has quadratic complexity with the number of Lindblad operators as opposed to the linear complexity of the first-order algorithm."],"url":"http://arxiv.org/abs/2401.12109v1","category":"quant-ph"}
{"created":"2024-01-22 16:45:15","title":"On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using Streaming Data","abstract":"In parcel delivery, the \"last mile\" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival. Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes. In crowdshipping, private citizens (\"the crowd\") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives. However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals. Leveraging crowdshipping for time-sensitive deliveries remains an open challenge. In this paper, we present an agent-based approach to on-time parcel delivery with crowds. Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays. Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby. Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach.","sentences":["In parcel delivery, the \"last mile\" from the parcel hub to the customer is costly, especially for time-sensitive delivery tasks that have to be completed within hours after arrival.","Recently, crowdshipping has attracted increased attention as a new alternative to traditional delivery modes.","In crowdshipping, private citizens (\"the crowd\") perform short detours in their daily lives to contribute to parcel delivery in exchange for small incentives.","However, achieving desirable crowd behavior is challenging as the crowd is highly dynamic and consists of autonomous, self-interested individuals.","Leveraging crowdshipping for time-sensitive deliveries remains an open challenge.","In this paper, we present an agent-based approach to on-time parcel delivery with crowds.","Our system performs data stream processing on the couriers' smartphone sensor data to predict delivery delays.","Whenever a delay is predicted, the system attempts to forge an agreement for transferring the parcel from the current deliverer to a more promising courier nearby.","Our experiments show that through accurate delay predictions and purposeful task transfers many delays can be prevented that would occur without our approach."],"url":"http://arxiv.org/abs/2401.12108v1","category":"cs.AI"}
{"created":"2024-01-22 16:41:36","title":"Energy-aware Trajectory Optimization for UAV-mounted RIS and Full-duplex Relay","abstract":"In the evolving landscape of sixth-generation (6G) wireless networks, unmanned aerial vehicles (UAVs) have emerged as transformative tools for dynamic and adaptive connectivity. However, dynamically adjusting their position to offer favorable communication channels introduces operational challenges in terms of energy consumption, especially when integrating advanced communication technologies like reconfigurable intelligent surfaces (RISs) and full-duplex relays (FDRs). To this end, by recognizing the pivotal role of UAV mobility, the paper introduces an energy-aware trajectory design for UAV-mounted RISs and UAV-mounted FDRs using the decode and forward (DF) protocol, aiming to maximize the network minimum rate and enhance user fairness, while taking into consideration the available on-board energy. Specifically, this work highlights their distinct energy consumption characteristics and their associated integration challenges by developing appropriate energy consumption models for both UAV-mounted RISs and FDRs that capture the intricate relationship between key factors such as weight, and their operational characteristics. Furthermore, a joint time-division multiple access (TDMA) user scheduling-UAV trajectory optimization problem is formulated, considering the power dynamics of both systems, while assuring that the UAV energy is not depleted mid-air. Finally, simulation results underscore the importance of energy considerations in determining the optimal trajectory and scheduling and provide insights into the performance comparison of UAV-mounted RISs and FDRs in UAV-assisted wireless networks.","sentences":["In the evolving landscape of sixth-generation (6G) wireless networks, unmanned aerial vehicles (UAVs) have emerged as transformative tools for dynamic and adaptive connectivity.","However, dynamically adjusting their position to offer favorable communication channels introduces operational challenges in terms of energy consumption, especially when integrating advanced communication technologies like reconfigurable intelligent surfaces (RISs) and full-duplex relays (FDRs).","To this end, by recognizing the pivotal role of UAV mobility, the paper introduces an energy-aware trajectory design for UAV-mounted RISs and UAV-mounted FDRs using the decode and forward (DF) protocol, aiming to maximize the network minimum rate and enhance user fairness, while taking into consideration the available on-board energy.","Specifically, this work highlights their distinct energy consumption characteristics and their associated integration challenges by developing appropriate energy consumption models for both UAV-mounted RISs and FDRs that capture the intricate relationship between key factors such as weight, and their operational characteristics.","Furthermore, a joint time-division multiple access (TDMA) user scheduling-UAV trajectory optimization problem is formulated, considering the power dynamics of both systems, while assuring that the UAV energy is not depleted mid-air.","Finally, simulation results underscore the importance of energy considerations in determining the optimal trajectory and scheduling and provide insights into the performance comparison of UAV-mounted RISs and FDRs in UAV-assisted wireless networks."],"url":"http://arxiv.org/abs/2401.12107v1","category":"cs.IT"}
{"created":"2024-01-22 16:41:00","title":"Geometric Phase of a Transmon in a Dissipative Quantum Circuit","abstract":"Superconducting circuits reveal themselves as promising physical devices with multiple uses. Within those uses, the fundamental concept of the geometric phase accumulated by the state of a system shows up recurrently, as, for example, in the construction of geometric gates. Given this framework, we study the geometric phases acquired by a paradigmatic setup: a transmon coupled to a superconductor resonating cavity. We do so both for the case in which the evolution is unitary and when it is subjected to dissipative effects. These models offer a comprehensive quantum description of an anharmonic system interacting with a single mode of the electromagnetic field within a perfect or dissipative cavity, respectively. In the dissipative model, the non-unitary effects arise from dephasing, relaxation, and decay of the transmon coupled to its environment. Our approach enables a comparison of the geometric phases obtained in these models, leading to a thorough understanding of the corrections introduced by the presence of the environment.","sentences":["Superconducting circuits reveal themselves as promising physical devices with multiple uses.","Within those uses, the fundamental concept of the geometric phase accumulated by the state of a system shows up recurrently, as, for example, in the construction of geometric gates.","Given this framework, we study the geometric phases acquired by a paradigmatic setup: a transmon coupled to a superconductor resonating cavity.","We do so both for the case in which the evolution is unitary and when it is subjected to dissipative effects.","These models offer a comprehensive quantum description of an anharmonic system interacting with a single mode of the electromagnetic field within a perfect or dissipative cavity, respectively.","In the dissipative model, the non-unitary effects arise from dephasing, relaxation, and decay of the transmon coupled to its environment.","Our approach enables a comparison of the geometric phases obtained in these models, leading to a thorough understanding of the corrections introduced by the presence of the environment."],"url":"http://arxiv.org/abs/2401.12106v1","category":"quant-ph"}
{"created":"2024-01-22 16:40:01","title":"Magic Can Enhance the Quantum Capacity of Channels","abstract":"We investigate the role of magic in the quantum capacity of channels. We consider the quantum channel of the recently proposed discrete beam splitter with the fixed environment state. We find that if the fixed environment state is a stabilizer state, then the quantum capacity is zero. Moreover, we find that the quantum capacity is nonzero for some magic states, and the quantum capacity increases linearly with respect to the number of single-qudit magic states in the environment. These results suggest that magic can increase the quantum capacity of channels, which sheds new insight into the role of stabilizer and magic states in quantum communication.","sentences":["We investigate the role of magic in the quantum capacity of channels.","We consider the quantum channel of the recently proposed discrete beam splitter with the fixed environment state.","We find that if the fixed environment state is a stabilizer state, then the quantum capacity is zero.","Moreover, we find that the quantum capacity is nonzero for some magic states, and the quantum capacity increases linearly with respect to the number of single-qudit magic states in the environment.","These results suggest that magic can increase the quantum capacity of channels, which sheds new insight into the role of stabilizer and magic states in quantum communication."],"url":"http://arxiv.org/abs/2401.12105v1","category":"quant-ph"}
{"created":"2024-01-22 16:39:52","title":"Ground and Excited States from Ensemble Variational Principles","abstract":"The extension of the Rayleigh-Ritz variational principle to ensemble states $\\rho_{\\mathbf{w}}\\equiv\\sum_k w_k |\\Psi_k\\rangle \\langle\\Psi_k|$ with fixed weights $w_k$ lies ultimately at the heart of several recent methodological developments for targeting excitation energies by variational means. Prominent examples are density and density matrix functional theory, Monte Carlo sampling, state-average complete active space self-consistent field methods and variational quantum eigensolvers. In order to provide a sound basis for all these methods and to improve their current implementations, we prove the validity of the underlying critical hypothesis: Whenever the ensemble energy is well-converged, the same holds true for the ensemble state $\\rho_{\\mathbf{w}}$ as well as the individual eigenstates $|\\Psi_k\\rangle$ and eigenenergies $E_k$. To be more specific, we derive linear bounds $d_-\\Delta{E}_{\\mathbf{w}} \\leq \\Delta Q \\leq d_+ \\Delta\\Delta{E}_{\\mathbf{w}}$ on the errors $\\Delta Q $ of these sought-after quantities. A subsequent analytical analysis and numerical illustration proves the tightness of our universal inequalities. Our results and particularly the explicit form of $d_{\\pm}\\equiv d_{\\pm}^{(Q)}(\\mathbf{w},\\mathbf{E})$ provide valuable insights into the optimal choice of the auxiliary weights $w_k$ in practical applications.","sentences":["The extension of the Rayleigh-Ritz variational principle to ensemble states $\\rho_{\\mathbf{w}}\\equiv\\sum_k w_k |\\Psi_k\\rangle \\langle\\Psi_k|$ with fixed weights $w_k$ lies ultimately at the heart of several recent methodological developments for targeting excitation energies by variational means.","Prominent examples are density and density matrix functional theory, Monte Carlo sampling, state-average complete active space self-consistent field methods and variational quantum eigensolvers.","In order to provide a sound basis for all these methods and to improve their current implementations, we prove the validity of the underlying critical hypothesis:","Whenever the ensemble energy is well-converged, the same holds true for the ensemble state $\\rho_{\\mathbf{w}}$ as well as the individual eigenstates $|\\Psi_k\\rangle$ and eigenenergies $E_k$. To be more specific, we derive linear bounds $d_-\\Delta{E}_{\\mathbf{w}} \\leq \\Delta Q \\leq d_+ \\Delta\\Delta{E}_{\\mathbf{w}}$ on the errors $\\Delta Q $ of these sought-after quantities.","A subsequent analytical analysis and numerical illustration proves the tightness of our universal inequalities.","Our results and particularly the explicit form of $d_{\\pm}\\equiv d_{\\pm}^{(Q)}(\\mathbf{w},\\mathbf{E})$ provide valuable insights into the optimal choice of the auxiliary weights $w_k$ in practical applications."],"url":"http://arxiv.org/abs/2401.12104v1","category":"quant-ph"}
{"created":"2024-01-22 16:38:33","title":"LearnedWMP: Workload Memory Prediction Using Distribution of Query Templates","abstract":"In a modern DBMS, working memory is frequently the limiting factor when processing in-memory analytic query operations such as joins, sorting, and aggregation. Existing resource estimation approaches for a DBMS estimate the resource consumption of a query by computing an estimate of each individual database operator in the query execution plan. Such an approach is slow and error-prone as it relies upon simplifying assumptions, such as uniformity and independence of the underlying data. Additionally, the existing approach focuses on individual queries separately and does not factor in other queries in the workload that may be executed concurrently. In this research, we are interested in query performance optimization under concurrent execution of a batch of queries (a workload). Specifically, we focus on predicting the memory demand for a workload rather than providing separate estimates for each query within it. We introduce the problem of workload memory prediction and formalize it as a distribution regression problem. We propose Learned Workload Memory Prediction (LearnedWMP) to improve and simplify estimating the working memory demands of workloads. Through a comprehensive experimental evaluation, we show that LearnedWMP reduces the memory estimation error of the state-of-the-practice method by up to 47.6%. Compared to an alternative single-query model, during training and inferencing, the LearnedWMP model and its variants were 3x to 10x faster. Moreover, LearnedWMP-based models were at least 50% smaller in most cases. Overall, the results demonstrate the advantages of the LearnedWMP approach and its potential for a broader impact on query performance optimization.","sentences":["In a modern DBMS, working memory is frequently the limiting factor when processing in-memory analytic query operations such as joins, sorting, and aggregation.","Existing resource estimation approaches for a DBMS estimate the resource consumption of a query by computing an estimate of each individual database operator in the query execution plan.","Such an approach is slow and error-prone as it relies upon simplifying assumptions, such as uniformity and independence of the underlying data.","Additionally, the existing approach focuses on individual queries separately and does not factor in other queries in the workload that may be executed concurrently.","In this research, we are interested in query performance optimization under concurrent execution of a batch of queries (a workload).","Specifically, we focus on predicting the memory demand for a workload rather than providing separate estimates for each query within it.","We introduce the problem of workload memory prediction and formalize it as a distribution regression problem.","We propose Learned Workload Memory Prediction (LearnedWMP) to improve and simplify estimating the working memory demands of workloads.","Through a comprehensive experimental evaluation, we show that LearnedWMP reduces the memory estimation error of the state-of-the-practice method by up to 47.6%.","Compared to an alternative single-query model, during training and inferencing, the LearnedWMP model and its variants were 3x to 10x faster.","Moreover, LearnedWMP-based models were at least 50% smaller in most cases.","Overall, the results demonstrate the advantages of the LearnedWMP approach and its potential for a broader impact on query performance optimization."],"url":"http://arxiv.org/abs/2401.12103v1","category":"cs.DB"}
{"created":"2024-01-22 16:37:16","title":"Effective Abelian Lattice Gauge Field Theories for scalar-matter-monopole interactions","abstract":"We present a gauge and Lorentz invariant effective field theory model for the interaction of a charged scalar matter field with a magnetic monopole source, described by an external magnetic current. The quantum fluctuations of the monopole field are described effectively by a strongly-coupled ``dual'' $U_{\\rm d}(1)$ gauge field, which is independent of the electromagnetic $U_{\\rm em}(1)$ gauge field. The effective interactions of the charged matter with the monopole source are described by a gauge invariant mixed Chern-Simons-like (Pontryagin-density) term between the two $U(1)$ gauge fields. The latter interaction coupling is left free, and a Lattice study of the system is performed with the aim of determining the phase structure of this effective theory. Our study shows that, in the spontaneously-broken-symmetry phase, the monopole source triggers, via the mixed Chern-Simons term, which is non-trivial in its presence, the generation of a dynamical singular configuration (magnetic-monopole-like) for the respective gauge fields. The scalar field also behaves in the broken phase in a way similar to that of the scalar sector of the `t Hooft-Polyakov monopole.","sentences":["We present a gauge and Lorentz invariant effective field theory model for the interaction of a charged scalar matter field with a magnetic monopole source, described by an external magnetic current.","The quantum fluctuations of the monopole field are described effectively by a strongly-coupled ``dual'' $U_{\\rm d}(1)$ gauge field, which is independent of the electromagnetic $U_{\\rm em}(1)$ gauge field.","The effective interactions of the charged matter with the monopole source are described by a gauge invariant mixed Chern-Simons-like (Pontryagin-density) term between the two $U(1)$ gauge fields.","The latter interaction coupling is left free, and a Lattice study of the system is performed with the aim of determining the phase structure of this effective theory.","Our study shows that, in the spontaneously-broken-symmetry phase, the monopole source triggers, via the mixed Chern-Simons term, which is non-trivial in its presence, the generation of a dynamical singular configuration (magnetic-monopole-like) for the respective gauge fields.","The scalar field also behaves in the broken phase in a way similar to that of the scalar sector of the `t Hooft-Polyakov monopole."],"url":"http://arxiv.org/abs/2401.12101v1","category":"hep-th"}
{"created":"2024-01-22 16:35:00","title":"An Empirical Analysis of In-context Learning Abilities of LLMs for MT","abstract":"In-context learning (ICL) has consistently demonstrated superior performance over zero-shot performance in large language models (LLMs). However, the understanding of the dynamics of ICL and the aspects that influence downstream performance remains limited, especially for natural language generation (NLG) tasks. This work aims to address this gap by investigating the ICL capabilities of LLMs and studying the impact of different aspects of the in-context demonstrations for the task of machine translation (MT). Our preliminary investigations aim to discern whether in-context learning (ICL) is predominantly influenced by demonstrations or instructions by applying diverse perturbations to in-context demonstrations while preserving the task instruction. We observe varying behavior to perturbed examples across different model families, notably with BLOOM-7B derivatives being severely influenced by noise, whereas Llama 2 derivatives not only exhibit robustness but also tend to show enhancements over the clean baseline when subject to perturbed demonstrations. This suggests that the robustness of ICL may be governed by several factors, including the type of noise, perturbation direction (source or target), the extent of pretraining of the specific model, and fine-tuning for downstream tasks if applicable. Further investigation is warranted to develop a comprehensive understanding of these factors in future research.","sentences":["In-context learning (ICL) has consistently demonstrated superior performance over zero-shot performance in large language models (LLMs).","However, the understanding of the dynamics of ICL and the aspects that influence downstream performance remains limited, especially for natural language generation (NLG) tasks.","This work aims to address this gap by investigating the ICL capabilities of LLMs and studying the impact of different aspects of the in-context demonstrations for the task of machine translation (MT).","Our preliminary investigations aim to discern whether in-context learning (ICL) is predominantly influenced by demonstrations or instructions by applying diverse perturbations to in-context demonstrations while preserving the task instruction.","We observe varying behavior to perturbed examples across different model families, notably with BLOOM-7B derivatives being severely influenced by noise, whereas Llama 2 derivatives not only exhibit robustness but also tend to show enhancements over the clean baseline when subject to perturbed demonstrations.","This suggests that the robustness of ICL may be governed by several factors, including the type of noise, perturbation direction (source or target), the extent of pretraining of the specific model, and fine-tuning for downstream tasks if applicable.","Further investigation is warranted to develop a comprehensive understanding of these factors in future research."],"url":"http://arxiv.org/abs/2401.12097v1","category":"cs.CL"}
{"created":"2024-01-22 16:34:48","title":"The inverse problem for a class of implicit differential equations and the coisotropic embedding theorem","abstract":"We carry on the approach used in [Sch] to provide a solution for the inverse problem of the calculus of variations for Maxwell equations in vacuum and we provide an abstract theory including all implicit differential equations that can be formulated in terms of vector fields over pre-symplectic manifolds.","sentences":["We carry on the approach used in [Sch] to provide a solution for the inverse problem of the calculus of variations for Maxwell equations in vacuum and we provide an abstract theory including all implicit differential equations that can be formulated in terms of vector fields over pre-symplectic manifolds."],"url":"http://arxiv.org/abs/2401.12096v1","category":"math.AP"}
{"created":"2024-01-22 16:33:04","title":"CLIQUE as an AND of Polynomial-Sized Monotone Constant-Depth Circuits","abstract":"This paper shows that calculating $k$-CLIQUE on $n$ vertex graphs, requires the AND of at least $2^{n/4k}$ monotone, constant-depth, and polynomial-sized circuits, for sufficiently large values of $k$. The proof relies on a new, monotone, one-sided switching lemma, designed for cliques.","sentences":["This paper shows that calculating $k$-CLIQUE on $n$ vertex graphs, requires the AND of at least $2^{n/4k}$ monotone, constant-depth, and polynomial-sized circuits, for sufficiently large values of $k$. The proof relies on a new, monotone, one-sided switching lemma, designed for cliques."],"url":"http://arxiv.org/abs/2401.12094v1","category":"cs.CC"}
{"created":"2024-01-22 16:31:45","title":"Monitoring the Future of Smart Contracts","abstract":"Blockchains are decentralized systems that provide trustable execution guarantees. Smart contracts are programs written in specialized programming languages running on blockchains that govern how tokens and cryptocurrency are sent and received. Smart contracts can invoke other smart contracts during the execution of transactions always initiated by external users.   Once deployed, smart contracts cannot be modified, so techniques like runtime verification are very appealing for improving their reliability. However, the conventional model of computation of smart contracts is transactional: once operations commit, their effects are permanent and cannot be undone.   In this paper, we proposed the concept of future monitors which allows monitors to remain waiting for future transactions to occur before committing or aborting. This is inspired by optimistic rollups, which are modern blockchain implementations that increase efficiency (and reduce cost) by delaying transaction effects. We exploit this delay to propose a model of computation that allows (bounded) future monitors. We show our monitors correct respect of legacy transactions, how they implement future bounded monitors and how they guarantee progress. We illustrate the use of future bounded monitors to implement correctly multi-transaction flash loans.","sentences":["Blockchains are decentralized systems that provide trustable execution guarantees.","Smart contracts are programs written in specialized programming languages running on blockchains that govern how tokens and cryptocurrency are sent and received.","Smart contracts can invoke other smart contracts during the execution of transactions always initiated by external users.   ","Once deployed, smart contracts cannot be modified, so techniques like runtime verification are very appealing for improving their reliability.","However, the conventional model of computation of smart contracts is transactional: once operations commit, their effects are permanent and cannot be undone.   ","In this paper, we proposed the concept of future monitors which allows monitors to remain waiting for future transactions to occur before committing or aborting.","This is inspired by optimistic rollups, which are modern blockchain implementations that increase efficiency (and reduce cost) by delaying transaction effects.","We exploit this delay to propose a model of computation that allows (bounded) future monitors.","We show our monitors correct respect of legacy transactions, how they implement future bounded monitors and how they guarantee progress.","We illustrate the use of future bounded monitors to implement correctly multi-transaction flash loans."],"url":"http://arxiv.org/abs/2401.12093v1","category":"cs.LO"}
{"created":"2024-01-22 16:29:08","title":"Quantum Eigensolver for General Matrices","abstract":"The eigenvalue problem, a cornerstone in linear algebra, provides profound insights into studying matrix properties. Quantum algorithms addressing this problem have hitherto been constrained to special normal matrices assuming spectral decomposition, leaving the extension to general matrices an open challenge. In this work, we present a novel family of quantum algorithms tailored for solving the eigenvalue problem for general matrices, encompassing scenarios with complex eigenvalues or even defective matrices. Our approach begins by tackling the task of searching for an eigenvalue without additional constraints. For diagonalizable matrices, our algorithm has $\\tilde O(\\varepsilon^{-1})$ complexity with an error $\\varepsilon$, achieving the nearly Heisenberg scaling. Subsequently, we study the identification of eigenvalues closest to a specified point or line, extending the results for ground energy and energy gap problems in Hermitian matrices. We achieve an accuracy scaling of $\\tilde O(\\varepsilon^{-2})$ for general diagonalizable matrices, further refining to $\\tilde O(\\varepsilon^{-1})$ under the condition of real eigenvalues or constant distance from the reference point. The algorithm's foundation lies in the synergy of three techniques: the relationship between eigenvalues of matrix $A$ and the minimum singular value of $A-\\mu I$, quantum singular value threshold subroutine extended from quantum singular-value estimation, and problem-specific searching algorithms. Our algorithms find applications in diverse domains, including estimating the relaxation time of Markov chains, solving Liouvillian gaps in open quantum systems, and verifying PT-symmetry broken/unbroken phases. These applications underscore the significance of our quantum eigensolvers for problems across various disciplines.","sentences":["The eigenvalue problem, a cornerstone in linear algebra, provides profound insights into studying matrix properties.","Quantum algorithms addressing this problem have hitherto been constrained to special normal matrices assuming spectral decomposition, leaving the extension to general matrices an open challenge.","In this work, we present a novel family of quantum algorithms tailored for solving the eigenvalue problem for general matrices, encompassing scenarios with complex eigenvalues or even defective matrices.","Our approach begins by tackling the task of searching for an eigenvalue without additional constraints.","For diagonalizable matrices, our algorithm has $\\tilde O(\\varepsilon^{-1})$ complexity with an error $\\varepsilon$, achieving the nearly Heisenberg scaling.","Subsequently, we study the identification of eigenvalues closest to a specified point or line, extending the results for ground energy and energy gap problems in Hermitian matrices.","We achieve an accuracy scaling of $\\tilde O(\\varepsilon^{-2})$ for general diagonalizable matrices, further refining to $\\tilde O(\\varepsilon^{-1})$ under the condition of real eigenvalues or constant distance from the reference point.","The algorithm's foundation lies in the synergy of three techniques: the relationship between eigenvalues of matrix $A$ and the minimum singular value of $A-\\mu I$, quantum singular value threshold subroutine extended from quantum singular-value estimation, and problem-specific searching algorithms.","Our algorithms find applications in diverse domains, including estimating the relaxation time of Markov chains, solving Liouvillian gaps in open quantum systems, and verifying PT-symmetry broken/unbroken phases.","These applications underscore the significance of our quantum eigensolvers for problems across various disciplines."],"url":"http://arxiv.org/abs/2401.12091v1","category":"quant-ph"}
{"created":"2024-01-22 16:27:14","title":"Trainability of a quantum-classical machine in the NISQ era","abstract":"Advancements in classical computing have significantly enhanced machine learning applications, yet inherent limitations persist in terms of energy, resource and speed. Quantum machine learning algorithms offer a promising avenue to overcome these limitations but bring along their own challenges. This experimental study explores the limits of trainability of a real experimental quantum classical hybrid system implementing supervised training protocols, in an ion trap platform. Challenges associated with ion trap-coupled classical processor are addressed, highlighting the robustness of the genetic algorithm as a classical optimizer in navigating complex optimization landscape inherent in binary classification problems with many local minima. Experimental results, focused on a binary classification problem, reveal the superior efficiency and accuracy of the genetic algorithm compared to gradient-based optimizers. We intricately discuss why gradient-based optimizers may not be suitable in the NISQ era through thorough analysis. These findings contribute insights into the performance of quantum-classical hybrid systems, emphasizing the significance of efficient training strategies and hardware considerations for practical quantum machine learning applications. This work not only advances the understanding of hybrid quantum-classical systems but also underscores the potential impact on real-world challenges through the convergence of quantum and classical computing paradigms operating without the aid of classical simulators.","sentences":["Advancements in classical computing have significantly enhanced machine learning applications, yet inherent limitations persist in terms of energy, resource and speed.","Quantum machine learning algorithms offer a promising avenue to overcome these limitations but bring along their own challenges.","This experimental study explores the limits of trainability of a real experimental quantum classical hybrid system implementing supervised training protocols, in an ion trap platform.","Challenges associated with ion trap-coupled classical processor are addressed, highlighting the robustness of the genetic algorithm as a classical optimizer in navigating complex optimization landscape inherent in binary classification problems with many local minima.","Experimental results, focused on a binary classification problem, reveal the superior efficiency and accuracy of the genetic algorithm compared to gradient-based optimizers.","We intricately discuss why gradient-based optimizers may not be suitable in the NISQ era through thorough analysis.","These findings contribute insights into the performance of quantum-classical hybrid systems, emphasizing the significance of efficient training strategies and hardware considerations for practical quantum machine learning applications.","This work not only advances the understanding of hybrid quantum-classical systems but also underscores the potential impact on real-world challenges through the convergence of quantum and classical computing paradigms operating without the aid of classical simulators."],"url":"http://arxiv.org/abs/2401.12089v1","category":"quant-ph"}
{"created":"2024-01-22 16:25:47","title":"Unsupervised Learning of Graph from Recipes","abstract":"Cooking recipes are one of the most readily available kinds of procedural text. They consist of natural language instructions that can be challenging to interpret. In this paper, we propose a model to identify relevant information from recipes and generate a graph to represent the sequence of actions in the recipe. In contrast with other approaches, we use an unsupervised approach. We iteratively learn the graph structure and the parameters of a $\\mathsf{GNN}$ encoding the texts (text-to-graph) one sequence at a time while providing the supervision by decoding the graph into text (graph-to-text) and comparing the generated text to the input. We evaluate the approach by comparing the identified entities with annotated datasets, comparing the difference between the input and output texts, and comparing our generated graphs with those generated by state of the art methods.","sentences":["Cooking recipes are one of the most readily available kinds of procedural text.","They consist of natural language instructions that can be challenging to interpret.","In this paper, we propose a model to identify relevant information from recipes and generate a graph to represent the sequence of actions in the recipe.","In contrast with other approaches, we use an unsupervised approach.","We iteratively learn the graph structure and the parameters of a $\\mathsf{GNN}$ encoding the texts (text-to-graph) one sequence at a time while providing the supervision by decoding the graph into text (graph-to-text) and comparing the generated text to the input.","We evaluate the approach by comparing the identified entities with annotated datasets, comparing the difference between the input and output texts, and comparing our generated graphs with those generated by state of the art methods."],"url":"http://arxiv.org/abs/2401.12088v1","category":"cs.CL"}
{"created":"2024-01-22 16:25:27","title":"Revisiting Demonstration Selection Strategies in In-Context Learning","abstract":"Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice. In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent. We further proposed a data- and model-dependent demonstration selection method, \\textbf{TopK + ConE}, based on the assumption that \\textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code will be released.","sentences":["Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model.","However, the performance of ICL varies significantly with the choice of demonstrations, and it is still unclear why this happens or what factors will influence its choice.","In this work, we first revisit the factors contributing to this variance from both data and model aspects, and find that the choice of demonstration is both data- and model-dependent.","We further proposed a data- and model-dependent demonstration selection method, \\textbf{TopK + ConE}, based on the assumption that \\textit{the performance of a demonstration positively correlates with its contribution to the model's understanding of the test samples}, resulting in a simple and effective recipe for ICL.","Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales.","Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods.","Code will be released."],"url":"http://arxiv.org/abs/2401.12087v1","category":"cs.CL"}
{"created":"2024-01-22 16:24:43","title":"West-of-N: Synthetic Preference Generation for Improved Reward Modeling","abstract":"The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model. In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs. Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training. This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query. Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data. This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges.","sentences":["The success of reinforcement learning from human feedback (RLHF) in language model alignment is strongly dependent on the quality of the underlying reward model.","In this paper, we present a novel approach to improve reward model quality by generating synthetic preference data, thereby augmenting the training dataset with on-policy, high-quality preference pairs.","Motivated by the promising results of Best-of-N sampling strategies in language model training, we extend their application to reward model training.","This results in a self-training strategy to generate preference pairs by selecting the best and worst candidates in a pool of responses to a given query.","Empirically, we find that this approach improves the performance of any reward model, with an effect comparable to the addition of a similar quantity of human preference data.","This work opens up new avenues of research for improving RLHF for language model alignment, by offering synthetic preference generation as a solution to reward modeling challenges."],"url":"http://arxiv.org/abs/2401.12086v1","category":"cs.CL"}
{"created":"2024-01-22 16:21:19","title":"Collaborative Reinforcement Learning Based Unmanned Aerial Vehicle (UAV) Trajectory Design for 3D UAV Tracking","abstract":"In this paper, the problem of using one active unmanned aerial vehicle (UAV) and four passive UAVs to localize a 3D target UAV in real time is investigated. In the considered model, each passive UAV receives reflection signals from the target UAV, which are initially transmitted by the active UAV. The received reflection signals allow each passive UAV to estimate the signal transmission distance which will be transmitted to a base station (BS) for the estimation of the position of the target UAV. Due to the movement of the target UAV, each active/passive UAV must optimize its trajectory to continuously localize the target UAV. Meanwhile, since the accuracy of the distance estimation depends on the signal-to-noise ratio of the transmission signals, the active UAV must optimize its transmit power. This problem is formulated as an optimization problem whose goal is to jointly optimize the transmit power of the active UAV and trajectories of both active and passive UAVs so as to maximize the target UAV positioning accuracy. To solve this problem, a Z function decomposition based reinforcement learning (ZD-RL) method is proposed. Compared to value function decomposition based RL (VD-RL), the proposed method can find the probability distribution of the sum of future rewards to accurately estimate the expected value of the sum of future rewards thus finding better transmit power of the active UAV and trajectories for both active and passive UAVs and improving target UAV positioning accuracy. Simulation results show that the proposed ZD-RL method can reduce the positioning errors by up to 39.4% and 64.6%, compared to VD-RL and independent deep RL methods, respectively.","sentences":["In this paper, the problem of using one active unmanned aerial vehicle (UAV) and four passive UAVs to localize a 3D target UAV in real time is investigated.","In the considered model, each passive UAV receives reflection signals from the target UAV, which are initially transmitted by the active UAV.","The received reflection signals allow each passive UAV to estimate the signal transmission distance which will be transmitted to a base station (BS) for the estimation of the position of the target UAV.","Due to the movement of the target UAV, each active/passive UAV must optimize its trajectory to continuously localize the target UAV.","Meanwhile, since the accuracy of the distance estimation depends on the signal-to-noise ratio of the transmission signals, the active UAV must optimize its transmit power.","This problem is formulated as an optimization problem whose goal is to jointly optimize the transmit power of the active UAV and trajectories of both active and passive UAVs so as to maximize the target UAV positioning accuracy.","To solve this problem, a Z function decomposition based reinforcement learning (ZD-RL) method is proposed.","Compared to value function decomposition based RL (VD-RL), the proposed method can find the probability distribution of the sum of future rewards to accurately estimate the expected value of the sum of future rewards thus finding better transmit power of the active UAV and trajectories for both active and passive UAVs and improving target UAV positioning accuracy.","Simulation results show that the proposed ZD-RL method can reduce the positioning errors by up to 39.4% and 64.6%, compared to VD-RL and independent deep RL methods, respectively."],"url":"http://arxiv.org/abs/2401.12079v1","category":"cs.MA"}
{"created":"2024-01-22 16:20:14","title":"Temporal Blind Spots in Large Language Models","abstract":"Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks. These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance. However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations. Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents. In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding. We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets. Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information. In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates. Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks. The code is available\\footnote{https://github.com/jwallat/temporalblindspots}.","sentences":["Large language models (LLMs) have recently gained significant attention due to their unparalleled ability to perform various natural language processing tasks.","These models, benefiting from their advanced natural language understanding capabilities, have demonstrated impressive zero-shot performance.","However, the pre-training data utilized in LLMs is often confined to a specific corpus, resulting in inherent freshness and temporal scope limitations.","Consequently, this raises concerns regarding the effectiveness of LLMs for tasks involving temporal intents.","In this study, we aim to investigate the underlying limitations of general-purpose LLMs when deployed for tasks that require a temporal understanding.","We pay particular attention to handling factual temporal knowledge through three popular temporal QA datasets.","Specifically, we observe low performance on detailed questions about the past and, surprisingly, for rather new information.","In manual and automatic testing, we find multiple temporal errors and characterize the conditions under which QA performance deteriorates.","Our analysis contributes to understanding LLM limitations and offers valuable insights into developing future models that can better cater to the demands of temporally-oriented tasks.","The code is available\\footnote{https://github.com/jwallat/temporalblindspots}."],"url":"http://arxiv.org/abs/2401.12078v1","category":"cs.CL"}
{"created":"2024-01-22 16:14:57","title":"Human Impression of Humanoid Robots Mirroring Social Cues","abstract":"Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people's perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people's perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. We discovered that the iCub robot was perceived as more humanlike than the Pepper robot when mirroring affect. A vision-based controlled iCub outperformed the IMU-based controlled one in the movement mirroring task. Our findings suggest that different robotic platforms impact people's perception of robots' mirroring during HRI. The control method also contributes to the robot's mirroring performance. Our work sheds light on the design and application of different humanoid robots in the real world.","sentences":["Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world.","The robotic platforms and control methods also impact people's perception of human-robot interaction.","However, limited studies have compared robot imitation across different platforms and control methods.","Our research addresses this gap by conducting two experiments comparing people's perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control.","We discovered that the iCub robot was perceived as more humanlike than the Pepper robot when mirroring affect.","A vision-based controlled iCub outperformed the IMU-based controlled one in the movement mirroring task.","Our findings suggest that different robotic platforms impact people's perception of robots' mirroring during HRI.","The control method also contributes to the robot's mirroring performance.","Our work sheds light on the design and application of different humanoid robots in the real world."],"url":"http://arxiv.org/abs/2401.12076v1","category":"cs.RO"}
{"created":"2024-01-22 16:14:27","title":"NLP-based Relation Extraction Methods in RE","abstract":"Mobile app repositories have been largely used in scientific research as large-scale, highly adaptive crowdsourced information systems. These software platforms can potentially nourish multiple software and requirements engineering tasks based on user reviews and other natural language documents, including feedback analysis, recommender systems and topic modelling. Consequently, researchers often endeavour to overcome domain-specific challenges, including integration of heterogeneous data sources, large-scale data collection and adaptation of a publicly available data set for a given research scenario. In this paper, we present MApp-KG, a combination of software resources and data artefacts in the field of mobile app repositories to support extended knowledge generation tasks. Our contribution aims to provide a framework for automatically constructing a knowledge graph modelling a domain-specific catalogue of mobile apps. Complementarily, we distribute MApp-KG in a public triplestore and as a static data snapshot, which may be promptly employed for future research and reproduction of our findings.","sentences":["Mobile app repositories have been largely used in scientific research as large-scale, highly adaptive crowdsourced information systems.","These software platforms can potentially nourish multiple software and requirements engineering tasks based on user reviews and other natural language documents, including feedback analysis, recommender systems and topic modelling.","Consequently, researchers often endeavour to overcome domain-specific challenges, including integration of heterogeneous data sources, large-scale data collection and adaptation of a publicly available data set for a given research scenario.","In this paper, we present MApp-KG, a combination of software resources and data artefacts in the field of mobile app repositories to support extended knowledge generation tasks.","Our contribution aims to provide a framework for automatically constructing a knowledge graph modelling a domain-specific catalogue of mobile apps.","Complementarily, we distribute MApp-KG in a public triplestore and as a static data snapshot, which may be promptly employed for future research and reproduction of our findings."],"url":"http://arxiv.org/abs/2401.12075v1","category":"cs.SE"}
{"created":"2024-01-22 16:14:04","title":"The time slot allocation problem in liberalised passenger railway markets: a multi-objective approach","abstract":"The liberalisation of the European passenger railway markets through the European Directive EU 91/440/EEC states a new scenario where different Railway Undertakings compete with each other in a bidding process for time slots. The infrastructure resources are provided by the Infrastructure Manager, who analyses and assesses the bids received, allocating the resources to each Railway Undertaking. Time slot allocation is a fact that drastically influences the market equilibrium. In this paper, we address the time slot allocation problem within the context of a liberalized passenger railway market as a multi-objective model. The Infrastructure Manager is tasked with selecting a point from the Pareto front as the solution to the time slot allocation problem. We propose two criteria for making this selection: the first one allocates time slots to each company according to a set of priorities, while the second one introduces a criterion of fairness in the treatment of companies to incentive competition. The assessment of the impact of these rules on market equilibrium has been conducted on a liberalized high-speed corridor within the Spanish railway network.","sentences":["The liberalisation of the European passenger railway markets through the European Directive EU 91/440/EEC states a new scenario where different Railway Undertakings compete with each other in a bidding process for time slots.","The infrastructure resources are provided by the Infrastructure Manager, who analyses and assesses the bids received, allocating the resources to each Railway Undertaking.","Time slot allocation is a fact that drastically influences the market equilibrium.","In this paper, we address the time slot allocation problem within the context of a liberalized passenger railway market as a multi-objective model.","The Infrastructure Manager is tasked with selecting a point from the Pareto front as the solution to the time slot allocation problem.","We propose two criteria for making this selection: the first one allocates time slots to each company according to a set of priorities, while the second one introduces a criterion of fairness in the treatment of companies to incentive competition.","The assessment of the impact of these rules on market equilibrium has been conducted on a liberalized high-speed corridor within the Spanish railway network."],"url":"http://arxiv.org/abs/2401.12073v1","category":"cs.CE"}
{"created":"2024-01-22 16:13:45","title":"Cross-lingual Transfer Learning for Javanese Dependency Parsing","abstract":"While structure learning achieves remarkable performance in high-resource languages, the situation differs for under-represented languages due to the scarcity of annotated data. This study focuses on assessing the efficacy of transfer learning in enhancing dependency parsing for Javanese, a language spoken by 80 million individuals but characterized by limited representation in natural language processing. We utilized the Universal Dependencies dataset consisting of dependency treebanks from more than 100 languages, including Javanese. We propose two learning strategies to train the model: transfer learning (TL) and hierarchical transfer learning (HTL). While TL only uses a source language to pre-train the model, the HTL method uses a source language and an intermediate language in the learning process. The results show that our best model uses the HTL method, which improves performance with an increase of 10% for both UAS and LAS evaluations compared to the baseline model.","sentences":["While structure learning achieves remarkable performance in high-resource languages, the situation differs for under-represented languages due to the scarcity of annotated data.","This study focuses on assessing the efficacy of transfer learning in enhancing dependency parsing for Javanese, a language spoken by 80 million individuals but characterized by limited representation in natural language processing.","We utilized the Universal Dependencies dataset consisting of dependency treebanks from more than 100 languages, including Javanese.","We propose two learning strategies to train the model: transfer learning (TL) and hierarchical transfer learning (HTL).","While TL only uses a source language to pre-train the model, the HTL method uses a source language and an intermediate language in the learning process.","The results show that our best model uses the HTL method, which improves performance with an increase of 10% for both UAS and LAS evaluations compared to the baseline model."],"url":"http://arxiv.org/abs/2401.12072v1","category":"cs.CL"}
{"created":"2024-01-22 16:11:11","title":"An Irredundant and Compressed Data Layout to Optimize Bandwidth Utilization of FPGA Accelerators","abstract":"Memory bandwidth is known to be a performance bottleneck for FPGA accelerators, especially when they deal with large multi-dimensional data-sets. A large body of work focuses on reducing of off-chip transfers, but few authors try to improve the efficiency of transfers. This paper addresses the later issue by proposing (i) a compiler-based approach to accelerator's data layout to maximize contiguous access to off-chip memory, and (ii) data packing and runtime compression techniques that take advantage of this layout to further improve memory performance. We show that our approach can decrease the I/O cycles up to $7\\times$ compared to un-optimized memory accesses.","sentences":["Memory bandwidth is known to be a performance bottleneck for FPGA accelerators, especially when they deal with large multi-dimensional data-sets.","A large body of work focuses on reducing of off-chip transfers, but few authors try to improve the efficiency of transfers.","This paper addresses the later issue by proposing (i) a compiler-based approach to accelerator's data layout to maximize contiguous access to off-chip memory, and (ii) data packing and runtime compression techniques that take advantage of this layout to further improve memory performance.","We show that our approach can decrease the I/O cycles up to $7\\times$ compared to un-optimized memory accesses."],"url":"http://arxiv.org/abs/2401.12071v1","category":"cs.AR"}
{"created":"2024-01-22 16:09:47","title":"Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text","abstract":"Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data.","sentences":["Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors.","However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text.","Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs.","The method, called Binoculars, achieves state-of-the-art accuracy without any training data.","It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications.","We comprehensively evaluate Binoculars on a number of text sources and in varied situations.","Over a wide range of document types, Binoculars detects over 90% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being trained on any ChatGPT data."],"url":"http://arxiv.org/abs/2401.12070v1","category":"cs.CL"}
{"created":"2024-01-22 16:08:41","title":"Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions for Tree Ensembles","abstract":"While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models. As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions. The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models. Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order. In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models. TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets.","sentences":["While shallow decision trees may be interpretable, larger ensemble models like gradient-boosted trees, which often set the state of the art in machine learning problems involving tabular data, still remain black box models.","As a remedy, the Shapley value (SV) is a well-known concept in explainable artificial intelligence (XAI) research for quantifying additive feature attributions of predictions.","The model-specific TreeSHAP methodology solves the exponential complexity for retrieving exact SVs from tree-based models.","Expanding beyond individual feature attribution, Shapley interactions reveal the impact of intricate feature interactions of any order.","In this work, we present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley interactions for predictions of tree-based models.","TreeSHAP-IQ is supported by a mathematical framework that exploits polynomial arithmetic to compute the interaction scores in a single recursive traversal of the tree, akin to Linear TreeSHAP.","We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore interactions on well-established benchmark datasets."],"url":"http://arxiv.org/abs/2401.12069v1","category":"cs.LG"}
{"created":"2024-01-22 16:05:30","title":"Resource-constrained stereo singing voice cancellation","abstract":"We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix. We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation. Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead. In practice, this is realised by adapting an existing mono model to handle stereo input. Improvements in quality are obtained by tuning model parameters and expanding the training set. Moreover, we highlight the benefits a stereo model brings by introducing a new metric which detects attenuation inconsistencies between channels. Our approach is evaluated using objective offline metrics and a large-scale MUSHRA trial, confirming the effectiveness of our techniques in stringent listening tests.","sentences":["We study the problem of stereo singing voice cancellation, a subtask of music source separation, whose goal is to estimate an instrumental background from a stereo mix.","We explore how to achieve performance similar to large state-of-the-art source separation networks starting from a small, efficient model for real-time speech separation.","Such a model is useful when memory and compute are limited and singing voice processing has to run with limited look-ahead.","In practice, this is realised by adapting an existing mono model to handle stereo input.","Improvements in quality are obtained by tuning model parameters and expanding the training set.","Moreover, we highlight the benefits a stereo model brings by introducing a new metric which detects attenuation inconsistencies between channels.","Our approach is evaluated using objective offline metrics and a large-scale MUSHRA trial, confirming the effectiveness of our techniques in stringent listening tests."],"url":"http://arxiv.org/abs/2401.12068v1","category":"cs.SD"}
{"created":"2024-01-22 16:04:51","title":"A concise proof of Commoner's theorem","abstract":"The textbook proofs of Commoner's theorem characterizing liveness in free-choice Petri nets are given in contexts of technical notions and claims that make the proofs look a bit long. The aim of this note is to give a concise self-contained proof.","sentences":["The textbook proofs of Commoner's theorem characterizing liveness in free-choice Petri nets are given in contexts of technical notions and claims that make the proofs look a bit long.","The aim of this note is to give a concise self-contained proof."],"url":"http://arxiv.org/abs/2401.12067v1","category":"cs.LO"}
{"created":"2024-01-22 15:59:38","title":"Probing the Spin-Induced Quadrupole Moment of Massive Black Holes with the Inspiral of Binary Black Holes","abstract":"One of the most important sources for the space-borne gravitational wave detectors such as TianQin and LISA, is the merger of massivie black hole binaries. Using the inspiral signals, we can probe the properties of the massive black holes, such as the spin-induced multipole moments. By verifying the relation among the mass, spin and quadrupole moment, the no-hair theoreom can be tested. In this work, we analysed the capability on probing the spin-induced quadrupole moment with the inspiral signal of massive black hole binaries using space-borne gravitational wave detectors. Using the Fisher information matrix, we find that the deviation of quadrupole moment can be constrained to the level of $10^{-1}$, and the events with higher mass-ratio will give a better constraint. We also find that the late inspiral part will dominate the result of parameter estimation. The result of Bayesian analysis shows that the capability will be improved by a few times due to the consideration of higher modes. We also calculate the Bayes factor, and the results indicate that the model of black hole and Boson star can be distinguished undoubtedly.","sentences":["One of the most important sources for the space-borne gravitational wave detectors such as TianQin and LISA, is the merger of massivie black hole binaries.","Using the inspiral signals, we can probe the properties of the massive black holes, such as the spin-induced multipole moments.","By verifying the relation among the mass, spin and quadrupole moment, the no-hair theoreom can be tested.","In this work, we analysed the capability on probing the spin-induced quadrupole moment with the inspiral signal of massive black hole binaries using space-borne gravitational wave detectors.","Using the Fisher information matrix, we find that the deviation of quadrupole moment can be constrained to the level of $10^{-1}$, and the events with higher mass-ratio will give a better constraint.","We also find that the late inspiral part will dominate the result of parameter estimation.","The result of Bayesian analysis shows that the capability will be improved by a few times due to the consideration of higher modes.","We also calculate the Bayes factor, and the results indicate that the model of black hole and Boson star can be distinguished undoubtedly."],"url":"http://arxiv.org/abs/2401.12066v1","category":"gr-qc"}
{"created":"2024-01-22 15:57:56","title":"Backward wave optical parametric oscillation in a waveguide","abstract":"A backward wave optical parametric oscillator (BWOPO) waveguide in periodically poled Rb-doped KTP is presented. The waveguide exhibits low loss (0.16 dB/cm) and has an oscillation threshold, almost 20 times lower than the corresponding bulk device. The backward wave has a narrow linewidth of 21 GHz at 1514.6 nm while the forward wave at 1688.7 nm has a spectrum replicating the pump. The unique spectral features of the BWOPO will unlock novel opportunities in low-power nonlinear integrated optics. A conversion efficiency of 8.4% was obtained limited by the emergence of backward stimulated polariton scattering.","sentences":["A backward wave optical parametric oscillator (BWOPO) waveguide in periodically poled Rb-doped KTP is presented.","The waveguide exhibits low loss (0.16 dB/cm) and has an oscillation threshold, almost 20 times lower than the corresponding bulk device.","The backward wave has a narrow linewidth of 21 GHz at 1514.6 nm while the forward wave at 1688.7 nm has a spectrum replicating the pump.","The unique spectral features of the BWOPO will unlock novel opportunities in low-power nonlinear integrated optics.","A conversion efficiency of 8.4% was obtained limited by the emergence of backward stimulated polariton scattering."],"url":"http://arxiv.org/abs/2401.12063v1","category":"physics.optics"}
{"created":"2024-01-22 15:54:47","title":"Scalable Automated Verification for Cyber-Physical Systems in Isabelle/HOL","abstract":"We formally introduce IsaVODEs (Isabelle verification with Ordinary Differential Equations), a framework for the verification of cyber-physical systems. We describe the semantic foundations of the framework's formalisation in the Isabelle/HOL proof assistant. A user-friendly language specification based on a robust state model makes our framework flexible and adaptable to various engineering workflows. New additions to the framework increase both its expressivity and proof automation. Specifically, formalisations related to forward diamond correctness specifications, certification of unique solutions to ordinary differential equations (ODEs) as flows, and invariant reasoning for systems of ODEs contribute to the framework's scalability and usability. Various examples and an evaluation validate the effectiveness of our framework.","sentences":["We formally introduce IsaVODEs","(Isabelle verification with Ordinary Differential Equations), a framework for the verification of cyber-physical systems.","We describe the semantic foundations of the framework's formalisation in the Isabelle/HOL proof assistant.","A user-friendly language specification based on a robust state model makes our framework flexible and adaptable to various engineering workflows.","New additions to the framework increase both its expressivity and proof automation.","Specifically, formalisations related to forward diamond correctness specifications, certification of unique solutions to ordinary differential equations (ODEs) as flows, and invariant reasoning for systems of ODEs contribute to the framework's scalability and usability.","Various examples and an evaluation validate the effectiveness of our framework."],"url":"http://arxiv.org/abs/2401.12061v1","category":"cs.LO"}
{"created":"2024-01-22 15:53:52","title":"SEDAC: A CVAE-Based Data Augmentation Method for Security Bug Report Identification","abstract":"Bug tracking systems store many bug reports, some of which are related to security. Identifying those security bug reports (SBRs) may help us predict some security-related bugs and solve security issues promptly so that the project can avoid threats and attacks. However, in the real world, the ratio of security bug reports is severely low; thus, directly training a prediction model with raw data may result in inaccurate results. Faced with the massive challenge of data imbalance, many researchers in the past have attempted to use text filtering or clustering methods to minimize the proportion of non-security bug reports (NSBRs) or apply oversampling methods to synthesize SBRs to make the dataset as balanced as possible. Nevertheless, there are still two challenges to those methods: 1) They ignore long-distance contextual information. 2) They fail to generate an utterly balanced dataset. To tackle these two challenges, we propose SEDAC, a new SBR identification method that generates similar bug report vectors to solve data imbalance problems and accurately detect security bug reports. Unlike previous studies, it first converts bug reports into individual bug report vectors with distilBERT, which are based on word2vec. Then, it trains a generative model through conditional variational auto-encoder (CVAE) to generate similar vectors with security labels, which makes the number of SBRs equal to NSBRs'. Finally, balanced data are used to train a security bug report classifier. To evaluate the effectiveness of our framework, we conduct it on 45,940 bug reports from Chromium and four Apache projects. The experimental results show that SEDAC outperforms all the baselines in g-measure with improvements of around 14.24%-50.10%.","sentences":["Bug tracking systems store many bug reports, some of which are related to security.","Identifying those security bug reports (SBRs) may help us predict some security-related bugs and solve security issues promptly so that the project can avoid threats and attacks.","However, in the real world, the ratio of security bug reports is severely low; thus, directly training a prediction model with raw data may result in inaccurate results.","Faced with the massive challenge of data imbalance, many researchers in the past have attempted to use text filtering or clustering methods to minimize the proportion of non-security bug reports (NSBRs) or apply oversampling methods to synthesize SBRs to make the dataset as balanced as possible.","Nevertheless, there are still two challenges to those methods: 1) They ignore long-distance contextual information.","2) They fail to generate an utterly balanced dataset.","To tackle these two challenges, we propose SEDAC, a new SBR identification method that generates similar bug report vectors to solve data imbalance problems and accurately detect security bug reports.","Unlike previous studies, it first converts bug reports into individual bug report vectors with distilBERT, which are based on word2vec.","Then, it trains a generative model through conditional variational auto-encoder (CVAE) to generate similar vectors with security labels, which makes the number of SBRs equal to NSBRs'.","Finally, balanced data are used to train a security bug report classifier.","To evaluate the effectiveness of our framework, we conduct it on 45,940 bug reports from Chromium and four Apache projects.","The experimental results show that SEDAC outperforms all the baselines in g-measure with improvements of around 14.24%-50.10%."],"url":"http://arxiv.org/abs/2401.12060v1","category":"cs.CR"}
{"created":"2024-01-22 15:50:32","title":"The Dimension Strikes Back with Gradients: Generalization of Gradient Methods in Stochastic Convex Optimization","abstract":"We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence. First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\\Omega(1)$ population excess risk. Our bound translates to a lower bound of $\\Omega (\\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable. Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\\Omega(\\sqrt{d})$ lower bound for the sample complexity of SGD to reach a non-trivial empirical error, despite achieving optimal test performance. This again provides an exponential improvement in the dimension dependence compared to previous work (Koren, Livni, Mansour, and Sherman, 2022), resolving an open question left therein.","sentences":["We study the generalization performance of gradient methods in the fundamental stochastic convex optimization setting, focusing on its dimension dependence.","First, for full-batch gradient descent (GD) we give a construction of a learning problem in dimension $d=O(n^2)$, where the canonical version of GD (tuned for optimal performance of the empirical risk) trained with $n$ training examples converges, with constant probability, to an approximate empirical risk minimizer with $\\Omega(1)$ population excess risk.","Our bound translates to a lower bound of $\\Omega (\\sqrt{d})$ on the number of training examples required for standard GD to reach a non-trivial test error, answering an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b) and showing that a non-trivial dimension dependence is unavoidable.","Furthermore, for standard one-pass stochastic gradient descent (SGD), we show that an application of the same construction technique provides a similar $\\Omega(\\sqrt{d})$ lower bound for the sample complexity of SGD to reach a non-trivial empirical error, despite achieving optimal test performance.","This again provides an exponential improvement in the dimension dependence compared to previous work (Koren, Livni, Mansour, and Sherman, 2022), resolving an open question left therein."],"url":"http://arxiv.org/abs/2401.12058v1","category":"cs.LG"}
{"created":"2024-01-22 15:47:05","title":"NEUROSEC: FPGA-Based Neuromorphic Audio Security","abstract":"Neuromorphic systems, inspired by the complexity and functionality of the human brain, have gained interest in academic and industrial attention due to their unparalleled potential across a wide range of applications. While their capabilities herald innovation, it is imperative to underscore that these computational paradigms, analogous to their traditional counterparts, are not impervious to security threats. Although the exploration of neuromorphic methodologies for image and video processing has been rigorously pursued, the realm of neuromorphic audio processing remains in its early stages. Our results highlight the robustness and precision of our FPGA-based neuromorphic system. Specifically, our system showcases a commendable balance between desired signal and background noise, efficient spike rate encoding, and unparalleled resilience against adversarial attacks such as FGSM and PGD. A standout feature of our framework is its detection rate of 94%, which, when compared to other methodologies, underscores its greater capability in identifying and mitigating threats within 5.39 dB, a commendable SNR ratio. Furthermore, neuromorphic computing and hardware security serve many sensor domains in mission-critical and privacy-preserving applications.","sentences":["Neuromorphic systems, inspired by the complexity and functionality of the human brain, have gained interest in academic and industrial attention due to their unparalleled potential across a wide range of applications.","While their capabilities herald innovation, it is imperative to underscore that these computational paradigms, analogous to their traditional counterparts, are not impervious to security threats.","Although the exploration of neuromorphic methodologies for image and video processing has been rigorously pursued, the realm of neuromorphic audio processing remains in its early stages.","Our results highlight the robustness and precision of our FPGA-based neuromorphic system.","Specifically, our system showcases a commendable balance between desired signal and background noise, efficient spike rate encoding, and unparalleled resilience against adversarial attacks such as FGSM and PGD.","A standout feature of our framework is its detection rate of 94%, which, when compared to other methodologies, underscores its greater capability in identifying and mitigating threats within 5.39 dB, a commendable SNR ratio.","Furthermore, neuromorphic computing and hardware security serve many sensor domains in mission-critical and privacy-preserving applications."],"url":"http://arxiv.org/abs/2401.12055v1","category":"cs.CR"}
{"created":"2024-01-22 15:46:40","title":"Twisting asymptotic symmetries and algebraically special vacuum solutions","abstract":"In this paper, we study asymptotic symmetries and algebraically special exact solutions in the Newman-Penrose formalism. Removing the hypersurface orthogonal condition in the well studied Newman-Unti gauge, we obtain a generic asymptotic solution space which includes all possible origins of propagating degree of freedom. The asymptotic symmetry of the generalized system extends the Weyl-BMS symmetry by two independent local Lorentz transformations with non-trivial boundary charges, which reveals new boundary degrees of freedom. The generalized Newman-Unti gauge includes algebraically special condition in its most convenient form. Remarkably, the generic solutions satisfying the algebraically special condition truncate in the inverse power of radial expansions and the non-radial Newman-Penrose equations are explicitly solved at any order. Hence, we provide the most general algebraically special solution space and the derivation is self-contained in the Newman-Penrose formalism. The asymptotic symmetry with respect to the algebraically special condition is the standard Weyl-BMS symmetry and the symmetry parameters consist only the integration constant order. We present the Kerr solution and Taub-NUT solution in the generalized Newman-Unti gauge in a simple form.","sentences":["In this paper, we study asymptotic symmetries and algebraically special exact solutions in the Newman-Penrose formalism.","Removing the hypersurface orthogonal condition in the well studied Newman-Unti gauge, we obtain a generic asymptotic solution space which includes all possible origins of propagating degree of freedom.","The asymptotic symmetry of the generalized system extends the Weyl-BMS symmetry by two independent local Lorentz transformations with non-trivial boundary charges, which reveals new boundary degrees of freedom.","The generalized Newman-Unti gauge includes algebraically special condition in its most convenient form.","Remarkably, the generic solutions satisfying the algebraically special condition truncate in the inverse power of radial expansions and the non-radial Newman-Penrose equations are explicitly solved at any order.","Hence, we provide the most general algebraically special solution space and the derivation is self-contained in the Newman-Penrose formalism.","The asymptotic symmetry with respect to the algebraically special condition is the standard Weyl-BMS symmetry and the symmetry parameters consist only the integration constant order.","We present the Kerr solution and Taub-NUT solution in the generalized Newman-Unti gauge in a simple form."],"url":"http://arxiv.org/abs/2401.12054v1","category":"gr-qc"}
{"created":"2024-01-22 15:43:14","title":"From Trust to Disagreement: disentangling the interplay of Misinformation and Polarisation in the News Ecosystem","abstract":"The increasing pervasiveness of fruitless disagreement poses a considerable risk to social cohesion and constructive public discourse. While polarised discussions can exhibit significant distrust in the news, it is still largely unclear whether disagreement is somehow linked to misinformation. In this work, we exploit the results of `Cartesio', an online experiment to rate the trustworthiness of Italian news articles annotated for reliability by expert evaluators. We developed a metric for disagreement that allows for correct comparisons between news with different mean trust values. Our findings indicate that, though misinformation receives lower trust ratings than accurate information, it does not appear to be more controversial. Additionally, we examined the relationship between these findings and Facebook user engagement with news articles. Our results show that disagreement correlates with an increased likelihood of commenting, probably linked to inconclusive and long discussions. The emerging scenario is one in which fighting disinformation seems ineffective in countering polarisation. Disagreement focuses more on the divergence of opinions, trust, and their effects on social cohesion. This study offers a foundation for unsupervised news item analysis independent of expert annotation. Incorporating similar principles into the design of news distribution platforms and social media systems can enhance online interactions and foster the development of a less divisive news ecosystem.","sentences":["The increasing pervasiveness of fruitless disagreement poses a considerable risk to social cohesion and constructive public discourse.","While polarised discussions can exhibit significant distrust in the news, it is still largely unclear whether disagreement is somehow linked to misinformation.","In this work, we exploit the results of `Cartesio', an online experiment to rate the trustworthiness of Italian news articles annotated for reliability by expert evaluators.","We developed a metric for disagreement that allows for correct comparisons between news with different mean trust values.","Our findings indicate that, though misinformation receives lower trust ratings than accurate information, it does not appear to be more controversial.","Additionally, we examined the relationship between these findings and Facebook user engagement with news articles.","Our results show that disagreement correlates with an increased likelihood of commenting, probably linked to inconclusive and long discussions.","The emerging scenario is one in which fighting disinformation seems ineffective in countering polarisation.","Disagreement focuses more on the divergence of opinions, trust, and their effects on social cohesion.","This study offers a foundation for unsupervised news item analysis independent of expert annotation.","Incorporating similar principles into the design of news distribution platforms and social media systems can enhance online interactions and foster the development of a less divisive news ecosystem."],"url":"http://arxiv.org/abs/2401.12053v1","category":"physics.soc-ph"}
{"created":"2024-01-22 15:42:21","title":"CloSe: A 3D Clothing Segmentation Dataset and Model","abstract":"3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries. Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization. To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds. CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work. The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data. We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing. We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels. Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data. Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/.","sentences":["3D Clothing modeling and datasets play crucial role in the entertainment, animation, and digital fashion industries.","Existing work often lacks detailed semantic understanding or uses synthetic datasets, lacking realism and personalization.","To address this, we first introduce CloSe-D: a novel large-scale dataset containing 3D clothing segmentation of 3167 scans, covering a range of 18 distinct clothing classes.","Additionally, we propose CloSe-Net, the first learning-based 3D clothing segmentation model for fine-grained segmentation from colored point clouds.","CloSe-Net uses local point features, body-clothing correlation, and a garment-class and point features-based attention module, improving performance over baselines and prior work.","The proposed attention module enables our model to learn appearance and geometry-dependent clothing prior from data.","We further validate the efficacy of our approach by successfully segmenting publicly available datasets of people in clothing.","We also introduce CloSe-T, a 3D interactive tool for refining segmentation labels.","Combining the tool with CloSe-T in a continual learning setup demonstrates improved generalization on real-world data.","Dataset, model, and tool can be found at https://virtualhumans.mpi-inf.mpg.de/close3dv24/."],"url":"http://arxiv.org/abs/2401.12051v1","category":"cs.CV"}
{"created":"2024-01-22 15:40:24","title":"HomeRobot Open Vocabulary Mobile Manipulation Challenge 2023 Participant Report (Team KuzHum)","abstract":"We report an improvements to NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation (OVMM) Challenge reinforcement learning baseline. More specifically, we propose more accurate semantic segmentation module, along with better place skill policy, and high-level heuristic that outperforms the baseline by 2.4% of overall success rate (sevenfold improvement) and 8.2% of partial success rate (1.75 times improvement) on Test Standard split of the challenge dataset. With aforementioned enhancements incorporated our agent scored 3rd place in the challenge on both simulation and real-world stages.","sentences":["We report an improvements to NeurIPS 2023 HomeRobot: Open Vocabulary Mobile Manipulation (OVMM) Challenge reinforcement learning baseline.","More specifically, we propose more accurate semantic segmentation module, along with better place skill policy, and high-level heuristic that outperforms the baseline by 2.4% of overall success rate (sevenfold improvement) and 8.2% of partial success rate (1.75 times improvement) on Test Standard split of the challenge dataset.","With aforementioned enhancements incorporated our agent scored 3rd place in the challenge on both simulation and real-world stages."],"url":"http://arxiv.org/abs/2401.12048v1","category":"cs.RO"}
{"created":"2024-01-22 15:38:37","title":"Fast degree-preserving rewiring of complex networks","abstract":"In this paper we introduce a new fast degree-preserving network rewiring algorithm. Commonly used existing algorithms require a large number of iterations, in particular in the case of large dense networks. This can especially be problematic when we wish to study ensembles of networks. In this paper we focus on degree assortative rewiring, and overcome aforementioned scalability problems by performing a rewiring of all edges at once to achieve a very high assorativity value before rewiring samples of edges at once to reduce this high assortativity value to the target value. The proposed method performs better than existing methods by several orders of magnitude for a range of structurally diverse complex networks, both in terms of the number of iterations taken, and time taken to reach a given assortativity value. Here we test networks up to $\\approx 4,000$ nodes and $\\approx 88,000$ edges and find that the relative improvements in speed remain.","sentences":["In this paper we introduce a new fast degree-preserving network rewiring algorithm.","Commonly used existing algorithms require a large number of iterations, in particular in the case of large dense networks.","This can especially be problematic when we wish to study ensembles of networks.","In this paper we focus on degree assortative rewiring, and overcome aforementioned scalability problems by performing a rewiring of all edges at once to achieve a very high assorativity value before rewiring samples of edges at once to reduce this high assortativity value to the target value.","The proposed method performs better than existing methods by several orders of magnitude for a range of structurally diverse complex networks, both in terms of the number of iterations taken, and time taken to reach a given assortativity value.","Here we test networks up to $\\approx 4,000$ nodes and $\\approx 88,000$ edges and find that the relative improvements in speed remain."],"url":"http://arxiv.org/abs/2401.12047v1","category":"physics.soc-ph"}
{"created":"2024-01-22 15:38:29","title":"Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D","abstract":"Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions. Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments. In this work, we propose Fourier Transporter (\\ours{}) which leverages the two-fold $\\SE(d)\\times\\SE(d)$ symmetry in the pick-place problem to achieve much higher sample efficiency. \\ours{} is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new environments. \\ours{} is constrained to incorporate symmetries of the pick and place actions independently. Our method utilizes a fiber space Fourier transformation that allows for memory-efficient construction. We test our proposed network on the RLbench benchmark and achieve state-of-the-art results across various tasks.","sentences":["Many complex robotic manipulation tasks can be decomposed as a sequence of pick and place actions.","Training a robotic agent to learn this sequence over many different starting conditions typically requires many iterations or demonstrations, especially in 3D environments.","In this work, we propose Fourier Transporter (\\ours{}) which leverages the two-fold $\\SE(d)\\times\\SE(d)$ symmetry in the pick-place problem to achieve much higher sample efficiency.","\\ours{} is an open-loop behavior cloning method trained using expert demonstrations to predict pick-place actions on new environments.","\\ours{} is constrained to incorporate symmetries of the pick and place actions independently.","Our method utilizes a fiber space Fourier transformation that allows for memory-efficient construction.","We test our proposed network on the RLbench benchmark and achieve state-of-the-art results across various tasks."],"url":"http://arxiv.org/abs/2401.12046v1","category":"cs.RO"}
{"created":"2024-01-22 15:36:15","title":"Toward QCD on Quantum Computer: Orbifold Lattice Approach","abstract":"We propose an orbifold lattice formulation of QCD suitable for quantum simulations. The advantages come from the use of noncompact variables that makes qubitization and truncated Hamiltonian very simple. It is shown that SU(3) gauge group and quarks in fundamental representation can be implemented straightforwardly.","sentences":["We propose an orbifold lattice formulation of QCD suitable for quantum simulations.","The advantages come from the use of noncompact variables that makes qubitization and truncated Hamiltonian very simple.","It is shown that SU(3) gauge group and quarks in fundamental representation can be implemented straightforwardly."],"url":"http://arxiv.org/abs/2401.12045v1","category":"hep-th"}
{"created":"2024-01-22 15:29:35","title":"Trade-off between Bagging and Boosting for quantum separability-entanglement classification","abstract":"Certifying whether an arbitrary quantum system is entangled or not, is, in general, an NP-hard problem. Though various necessary and sufficient conditions have already been explored in this regard for lower dimensional systems, it is hard to extend them to higher dimensions. Recently, an ensemble bagging and convex hull approximation (CHA) approach (together, BCHA) was proposed and it strongly suggests employing a machine learning technique for the separability-entanglement classification problem. However, BCHA does only incorporate the balanced dataset for classification tasks which results in lower average accuracy. In order to solve the data imbalance problem in the present literature, an exploration of the Boosting technique has been carried out, and a trade-off between the Boosting and Bagging-based ensemble classifier is explored for quantum separability problems. For the two-qubit and two-qutrit quantum systems, the pros and cons of the proposed random under-sampling boost CHA (RUSBCHA) for the quantum separability problem are compared with the state-of-the-art CHA and BCHA approaches. As the data is highly unbalanced, performance measures such as overall accuracy, average accuracy, F-measure, and G-mean are evaluated for a fair comparison. The outcomes suggest that RUSBCHA is an alternative to the BCHA approach. Also, for several cases, performance improvements are observed for RUSBCHA since the data is imbalanced.","sentences":["Certifying whether an arbitrary quantum system is entangled or not, is, in general, an NP-hard problem.","Though various necessary and sufficient conditions have already been explored in this regard for lower dimensional systems, it is hard to extend them to higher dimensions.","Recently, an ensemble bagging and convex hull approximation (CHA) approach (together, BCHA) was proposed and it strongly suggests employing a machine learning technique for the separability-entanglement classification problem.","However, BCHA does only incorporate the balanced dataset for classification tasks which results in lower average accuracy.","In order to solve the data imbalance problem in the present literature, an exploration of the Boosting technique has been carried out, and a trade-off between the Boosting and Bagging-based ensemble classifier is explored for quantum separability problems.","For the two-qubit and two-qutrit quantum systems, the pros and cons of the proposed random under-sampling boost CHA (RUSBCHA) for the quantum separability problem are compared with the state-of-the-art CHA and BCHA approaches.","As the data is highly unbalanced, performance measures such as overall accuracy, average accuracy, F-measure, and G-mean are evaluated for a fair comparison.","The outcomes suggest that RUSBCHA is an alternative to the BCHA approach.","Also, for several cases, performance improvements are observed for RUSBCHA since the data is imbalanced."],"url":"http://arxiv.org/abs/2401.12041v1","category":"quant-ph"}
{"created":"2024-01-22 15:27:36","title":"Construction of Chiral Cosmological Models Unifying Inflation and Primordial Black Hole Formation","abstract":"We propose the method for construction of $F(R,\\xi)$ gravity model, unifying inflation and primordial black hole formation. The proposed models are based on the Starobinsky $R+R^2$ inflationary model, so, the function $F(R,\\xi)$ is a quadratic polynomial of the Ricci scalar $R$. We show that the potential of the corresponding two-field chiral cosmological model in the Einstein frame can be always found in terms of the elementary functions. The special choice of the function $F(R,\\xi)$ allows us to get such a generalization of the hybrid inflation that can describe both inflation, and the primordial black hole formation.","sentences":["We propose the method for construction of $F(R,\\xi)$ gravity model, unifying inflation and primordial black hole formation.","The proposed models are based on the Starobinsky $R+R^2$ inflationary model, so, the function $F(R,\\xi)$ is a quadratic polynomial of the Ricci scalar $R$. We show that the potential of the corresponding two-field chiral cosmological model in the Einstein frame can be always found in terms of the elementary functions.","The special choice of the function $F(R,\\xi)$ allows us to get such a generalization of the hybrid inflation that can describe both inflation, and the primordial black hole formation."],"url":"http://arxiv.org/abs/2401.12040v1","category":"gr-qc"}
{"created":"2024-01-22 15:26:01","title":"Look, Listen and Recognise: Character-Aware Audio-Visual Subtitling","abstract":"The goal of this paper is automatic character-aware subtitle generation. Given a video and a minimal amount of metadata, we propose an audio-visual method that generates a full transcript of the dialogue, with precise speech timestamps, and the character speaking identified. The key idea is to first use audio-visual cues to select a set of high-precision audio exemplars for each character, and then use these exemplars to classify all speech segments by speaker identity. Notably, the method does not require face detection or tracking. We evaluate the method over a variety of TV sitcoms, including Seinfeld, Fraiser and Scrubs. We envision this system being useful for the automatic generation of subtitles to improve the accessibility of the vast amount of videos available on modern streaming services. Project page : \\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}","sentences":["The goal of this paper is automatic character-aware subtitle generation.","Given a video and a minimal amount of metadata, we propose an audio-visual method that generates a full transcript of the dialogue, with precise speech timestamps, and the character speaking identified.","The key idea is to first use audio-visual cues to select a set of high-precision audio exemplars for each character, and then use these exemplars to classify all speech segments by speaker identity.","Notably, the method does not require face detection or tracking.","We evaluate the method over a variety of TV sitcoms, including Seinfeld, Fraiser and Scrubs.","We envision this system being useful for the automatic generation of subtitles to improve the accessibility of the vast amount of videos available on modern streaming services.","Project page : \\url{https://www.robots.ox.ac.uk/~vgg/research/look-listen-recognise/}"],"url":"http://arxiv.org/abs/2401.12039v1","category":"cs.CV"}
{"created":"2024-01-22 15:22:54","title":"Joint Near-Field Target Tracking and Communications with Full Duplex Holographic MIMO","abstract":"In this paper, we present a simultaneous target tracking and multi-user communications system realized by a full duplex holographic Multiple-Input Multiple-Output (MIMO) node equipped with Dynamic Metasurface Antennas (DMAs) at both its communication ends. Focusing on the near-field regime, we extend Fresnel's approximation to metasurfaces and devise a subspace tracking scheme with DMA-based hybrid Analog and Digital (A/D) reception as well as hybrid A/D transmission with a DMA for sum-rate maximization. The presented simulation results corroborate the efficiency of the proposed framework for various system parameters.","sentences":["In this paper, we present a simultaneous target tracking and multi-user communications system realized by a full duplex holographic Multiple-Input Multiple-Output (MIMO) node equipped with Dynamic Metasurface Antennas (DMAs) at both its communication ends.","Focusing on the near-field regime, we extend Fresnel's approximation to metasurfaces and devise a subspace tracking scheme with DMA-based hybrid Analog and Digital (A/D) reception as well as hybrid A/D transmission with a DMA for sum-rate maximization.","The presented simulation results corroborate the efficiency of the proposed framework for various system parameters."],"url":"http://arxiv.org/abs/2401.12036v1","category":"cs.IT"}
{"created":"2024-01-22 15:19:18","title":"Momentum-SAM: Sharpness Aware Minimization without Computational Overhead","abstract":"The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss. While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam. We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization. Code is available at https://github.com/MarlonBecker/MSAM.","sentences":["The recently proposed optimization algorithm for deep neural networks Sharpness Aware Minimization (SAM) suggests perturbing parameters before gradient calculation by a gradient ascent step to guide the optimization into parameter space regions of flat loss.","While significant generalization improvements and thus reduction of overfitting could be demonstrated, the computational costs are doubled due to the additionally needed gradient calculation, making SAM unfeasible in case of limited computationally capacities.","Motivated by Nesterov Accelerated Gradient (NAG) we propose Momentum-SAM (MSAM), which perturbs parameters in the direction of the accumulated momentum vector to achieve low sharpness without significant computational overhead or memory demands over SGD or Adam.","We evaluate MSAM in detail and reveal insights on separable mechanisms of NAG, SAM and MSAM regarding training optimization and generalization.","Code is available at https://github.com/MarlonBecker/MSAM."],"url":"http://arxiv.org/abs/2401.12033v1","category":"cs.LG"}
{"created":"2024-01-22 15:17:54","title":"MINT: A wrapper to make multi-modal and multi-image AI models interactive","abstract":"During the diagnostic process, doctors incorporate multimodal information including imaging and the medical history - and similarly medical AI development has increasingly become multimodal. In this paper we tackle a more subtle challenge: doctors take a targeted medical history to obtain only the most pertinent pieces of information; how do we enable AI to do the same? We develop a wrapper method named MINT (Make your model INTeractive) that automatically determines what pieces of information are most valuable at each step, and ask for only the most useful information. We demonstrate the efficacy of MINT wrapping a skin disease prediction model, where multiple images and a set of optional answers to $25$ standard metadata questions (i.e., structured medical history) are used by a multi-modal deep network to provide a differential diagnosis. We show that MINT can identify whether metadata inputs are needed and if so, which question to ask next. We also demonstrate that when collecting multiple images, MINT can identify if an additional image would be beneficial, and if so, which type of image to capture. We showed that MINT reduces the number of metadata and image inputs needed by 82% and 36.2% respectively, while maintaining predictive performance. Using real-world AI dermatology system data, we show that needing fewer inputs can retain users that may otherwise fail to complete the system submission and drop off without a diagnosis. Qualitative examples show MINT can closely mimic the step-by-step decision making process of a clinical workflow and how this is different for straight forward cases versus more difficult, ambiguous cases. Finally we demonstrate how MINT is robust to different underlying multi-model classifiers and can be easily adapted to user requirements without significant model re-training.","sentences":["During the diagnostic process, doctors incorporate multimodal information including imaging and the medical history - and similarly medical AI development has increasingly become multimodal.","In this paper we tackle a more subtle challenge: doctors take a targeted medical history to obtain only the most pertinent pieces of information; how do we enable AI to do the same?","We develop a wrapper method named MINT (Make your model INTeractive) that automatically determines what pieces of information are most valuable at each step, and ask for only the most useful information.","We demonstrate the efficacy of MINT wrapping a skin disease prediction model, where multiple images and a set of optional answers to $25$ standard metadata questions (i.e., structured medical history) are used by a multi-modal deep network to provide a differential diagnosis.","We show that MINT can identify whether metadata inputs are needed and if so, which question to ask next.","We also demonstrate that when collecting multiple images, MINT can identify if an additional image would be beneficial, and if so, which type of image to capture.","We showed that MINT reduces the number of metadata and image inputs needed by 82% and 36.2% respectively, while maintaining predictive performance.","Using real-world AI dermatology system data, we show that needing fewer inputs can retain users that may otherwise fail to complete the system submission and drop off without a diagnosis.","Qualitative examples show MINT can closely mimic the step-by-step decision making process of a clinical workflow and how this is different for straight forward cases versus more difficult, ambiguous cases.","Finally we demonstrate how MINT is robust to different underlying multi-model classifiers and can be easily adapted to user requirements without significant model re-training."],"url":"http://arxiv.org/abs/2401.12032v1","category":"cs.HC"}
{"created":"2024-01-22 15:16:19","title":"Near-Field Localization with $1$-bit Quantized Hybrid A/D Reception","abstract":"In this paper, we consider a hybrid Analog and Digital (A/D) receiver architecture with an extremely large Dynamic Metasurface Antenna (DMA) and an $1$-bit resolution Analog-to-Digital Converter (ADC) at each of its reception radio-frequency chains, and present a localization approach for User Equipment (UE) lying in its near-field regime. The proposed algorithm scans the UE area of interest to identify the DMA-based analog combining configuration resulting to the peak in a received pseudo-spectrum, yielding the UE position estimation in three dimensions. Our simulation results demonstrate the validity of the proposed scheme, especially for increasing DMA sizes, and showcase the interplay among various system parameters.","sentences":["In this paper, we consider a hybrid Analog and Digital (A/D) receiver architecture with an extremely large Dynamic Metasurface Antenna (DMA) and an $1$-bit resolution Analog-to-Digital Converter (ADC) at each of its reception radio-frequency chains, and present a localization approach for User Equipment (UE) lying in its near-field regime.","The proposed algorithm scans the UE area of interest to identify the DMA-based analog combining configuration resulting to the peak in a received pseudo-spectrum, yielding the UE position estimation in three dimensions.","Our simulation results demonstrate the validity of the proposed scheme, especially for increasing DMA sizes, and showcase the interplay among various system parameters."],"url":"http://arxiv.org/abs/2401.12029v1","category":"cs.IT"}
{"created":"2024-01-22 15:15:18","title":"Quantum Characteristics Near Event Horizons","abstract":"We investigate the genuine multipartite entanglement, global entanglement, and quantum coherence among different configurations of a penta-partite system involving particles inside and outside the event horizon of a Schwarzschild black hole. We consider and analyze different scenarios based on how many particles are accessible. In each scenario, we evaluate first-order coherence, concurrence fill, and global concurrence under varying Hawking temperature and Dirac particle mode frequency. For the fully accessible scenario with all particles outside the event horizon, the measures exhibit non-monotonic behavior with a discernible trade-off. In the partially accessible scenarios with one particle inside the event horizon, monotonic variations and clear trade-offs are observed. Finally, in the scenario when two particles are inside the event horizon, concurrence fill becomes complex, attributed to the violation of the entanglement polygon inequality in curved space-time. This result reveals intricate relationships between entanglement and coherence around the event horizon of Schwarzchild black holes. Our findings suggest reevaluating entanglement polygon inequalities and concurrence fill for applicability in flat and curved space-times. These insights contribute to our understanding of quantum information dynamics and gravitational impacts on entanglement in extreme environments.","sentences":["We investigate the genuine multipartite entanglement, global entanglement, and quantum coherence among different configurations of a penta-partite system involving particles inside and outside the event horizon of a Schwarzschild black hole.","We consider and analyze different scenarios based on how many particles are accessible.","In each scenario, we evaluate first-order coherence, concurrence fill, and global concurrence under varying Hawking temperature and Dirac particle mode frequency.","For the fully accessible scenario with all particles outside the event horizon, the measures exhibit non-monotonic behavior with a discernible trade-off.","In the partially accessible scenarios with one particle inside the event horizon, monotonic variations and clear trade-offs are observed.","Finally, in the scenario when two particles are inside the event horizon, concurrence fill becomes complex, attributed to the violation of the entanglement polygon inequality in curved space-time.","This result reveals intricate relationships between entanglement and coherence around the event horizon of Schwarzchild black holes.","Our findings suggest reevaluating entanglement polygon inequalities and concurrence fill for applicability in flat and curved space-times.","These insights contribute to our understanding of quantum information dynamics and gravitational impacts on entanglement in extreme environments."],"url":"http://arxiv.org/abs/2401.12028v1","category":"quant-ph"}
{"created":"2024-01-22 15:12:40","title":"A Survey of Advances in Optimization Methods for Wireless Communication System Design","abstract":"Mathematical optimization is now widely regarded as an indispensable modeling and solution tool for the design of wireless communications systems. While optimization has played a significant role in the revolutionary progress in wireless communication and networking technologies from 1G to 5G and onto the future 6G, the innovations in wireless technologies have also substantially transformed the nature of the underlying mathematical optimization problems upon which the system designs are based and have sparked significant innovations in the development of methodologies to understand, to analyze, and to solve those problems. In this paper, we provide a comprehensive survey of recent advances in mathematical optimization theory and algorithms for wireless communication system design. We begin by illustrating common features of mathematical optimization problems arising in wireless communication system design. We discuss various scenarios and use cases and their associated mathematical structures from an optimization perspective. We then provide an overview of recent advances in mathematical optimization theory and algorithms, from nonconvex optimization, global optimization, and integer programming, to distributed optimization and learning-based optimization. The key to successful solution of mathematical optimization problems is in carefully choosing and/or developing suitable optimization algorithms (or neural network architectures) that can exploit the underlying problem structure. We conclude the paper by identifying several open research challenges and outlining future research directions.","sentences":["Mathematical optimization is now widely regarded as an indispensable modeling and solution tool for the design of wireless communications systems.","While optimization has played a significant role in the revolutionary progress in wireless communication and networking technologies from 1G to 5G and onto the future 6G, the innovations in wireless technologies have also substantially transformed the nature of the underlying mathematical optimization problems upon which the system designs are based and have sparked significant innovations in the development of methodologies to understand, to analyze, and to solve those problems.","In this paper, we provide a comprehensive survey of recent advances in mathematical optimization theory and algorithms for wireless communication system design.","We begin by illustrating common features of mathematical optimization problems arising in wireless communication system design.","We discuss various scenarios and use cases and their associated mathematical structures from an optimization perspective.","We then provide an overview of recent advances in mathematical optimization theory and algorithms, from nonconvex optimization, global optimization, and integer programming, to distributed optimization and learning-based optimization.","The key to successful solution of mathematical optimization problems is in carefully choosing and/or developing suitable optimization algorithms (or neural network architectures) that can exploit the underlying problem structure.","We conclude the paper by identifying several open research challenges and outlining future research directions."],"url":"http://arxiv.org/abs/2401.12025v1","category":"cs.IT"}
{"created":"2024-01-22 15:11:57","title":"Multimodal Visual-Tactile Representation Learning through Self-Supervised Contrastive Pre-Training","abstract":"The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities. Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments. Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans.This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion. By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction. Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques. In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction. Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments.","sentences":["The rapidly evolving field of robotics necessitates methods that can facilitate the fusion of multiple modalities.","Specifically, when it comes to interacting with tangible objects, effectively combining visual and tactile sensory data is key to understanding and navigating the complex dynamics of the physical world, enabling a more nuanced and adaptable response to changing environments.","Nevertheless, much of the earlier work in merging these two sensory modalities has relied on supervised methods utilizing datasets labeled by humans.","This paper introduces MViTac, a novel methodology that leverages contrastive learning to integrate vision and touch sensations in a self-supervised fashion.","By availing both sensory inputs, MViTac leverages intra and inter-modality losses for learning representations, resulting in enhanced material property classification and more adept grasping prediction.","Through a series of experiments, we showcase the effectiveness of our method and its superiority over existing state-of-the-art self-supervised and supervised techniques.","In evaluating our methodology, we focus on two distinct tasks: material classification and grasping success prediction.","Our results indicate that MViTac facilitates the development of improved modality encoders, yielding more robust representations as evidenced by linear probing assessments."],"url":"http://arxiv.org/abs/2401.12024v1","category":"cs.RO"}
{"created":"2024-01-22 15:10:56","title":"A Simulation of Optimal Dryness When Moving in the Rain or Snow Using MATLAB","abstract":"The classic question of whether one should walk or run in the rain to remain the least wet has inspired a myriad of solutions ranging from physically performing test runs in raining conditions to mathematically modeling human movement through rain. This manuscript approaches the classical problem by simulating movement through rainfall using MATLAB. Our simulation was generalizable to include snowfall as well. An increase in walking speed resulted in a corresponding decrease in raindrop and snowflake collisions. When raindrops or snowflakes were given a horizontal movement vector due to wind, a local minimum in collisions was achieved when moving in parallel with the same horizontal speed as the raindrop; no local minimum was detected with antiparallel movement. In general, our simulation revealed that the faster one moves, the drier one remains.","sentences":["The classic question of whether one should walk or run in the rain to remain the least wet has inspired a myriad of solutions ranging from physically performing test runs in raining conditions to mathematically modeling human movement through rain.","This manuscript approaches the classical problem by simulating movement through rainfall using MATLAB.","Our simulation was generalizable to include snowfall as well.","An increase in walking speed resulted in a corresponding decrease in raindrop and snowflake collisions.","When raindrops or snowflakes were given a horizontal movement vector due to wind, a local minimum in collisions was achieved when moving in parallel with the same horizontal speed as the raindrop; no local minimum was detected with antiparallel movement.","In general, our simulation revealed that the faster one moves, the drier one remains."],"url":"http://arxiv.org/abs/2401.12023v1","category":"cs.DM"}
{"created":"2024-01-22 15:05:05","title":"Stereo-Matching Knowledge Distilled Monocular Depth Estimation Filtered by Multiple Disparity Consistency","abstract":"In stereo-matching knowledge distillation methods of the self-supervised monocular depth estimation, the stereo-matching network's knowledge is distilled into a monocular depth network through pseudo-depth maps. In these methods, the learning-based stereo-confidence network is generally utilized to identify errors in the pseudo-depth maps to prevent transferring the errors. However, the learning-based stereo-confidence networks should be trained with ground truth (GT), which is not feasible in a self-supervised setting. In this paper, we propose a method to identify and filter errors in the pseudo-depth map using multiple disparity maps by checking their consistency without the need for GT and a training process. Experimental results show that the proposed method outperforms the previous methods and works well on various configurations by filtering out erroneous areas where the stereo-matching is vulnerable, especially such as textureless regions, occlusion boundaries, and reflective surfaces.","sentences":["In stereo-matching knowledge distillation methods of the self-supervised monocular depth estimation, the stereo-matching network's knowledge is distilled into a monocular depth network through pseudo-depth maps.","In these methods, the learning-based stereo-confidence network is generally utilized to identify errors in the pseudo-depth maps to prevent transferring the errors.","However, the learning-based stereo-confidence networks should be trained with ground truth (GT), which is not feasible in a self-supervised setting.","In this paper, we propose a method to identify and filter errors in the pseudo-depth map using multiple disparity maps by checking their consistency without the need for GT and a training process.","Experimental results show that the proposed method outperforms the previous methods and works well on various configurations by filtering out erroneous areas where the stereo-matching is vulnerable, especially such as textureless regions, occlusion boundaries, and reflective surfaces."],"url":"http://arxiv.org/abs/2401.12019v1","category":"cs.CV"}
{"created":"2024-01-22 15:04:46","title":"PairwiseHist: Fast, Accurate and Space-Efficient Approximate Query Processing with Data Compression","abstract":"Exponential growth in data collection is creating significant challenges for data storage and analytics latency.Approximate Query Processing (AQP) has long been touted as a solution for accelerating analytics on large datasets, however, there is still room for improvement across all key performance criteria. In this paper, we propose a novel histogram-based data synopsis called PairwiseHist that uses recursive hypothesis testing to ensure accurate histograms and can be built on top of data compressed using Generalized Deduplication (GD). We thus show that GD data compression can contribute to AQP. Compared to state-of-the-art AQP approaches, PairwiseHist achieves better performance across all key metrics, including 2.6$ \\times $ higher accuracy, 3.5$ \\times $ lower latency, 24$ \\times $ smaller synopses and 1.5--4$ \\times $ faster construction time.","sentences":["Exponential growth in data collection is creating significant challenges for data storage and analytics latency.","Approximate Query Processing (AQP) has long been touted as a solution for accelerating analytics on large datasets, however, there is still room for improvement across all key performance criteria.","In this paper, we propose a novel histogram-based data synopsis called PairwiseHist that uses recursive hypothesis testing to ensure accurate histograms and can be built on top of data compressed using Generalized Deduplication (GD).","We thus show that GD data compression can contribute to AQP.","Compared to state-of-the-art AQP approaches, PairwiseHist achieves better performance across all key metrics, including 2.6$ \\times $ higher accuracy, 3.5$ \\times $ lower latency, 24$ \\times $ smaller synopses and 1.5--4$ \\times $ faster construction time."],"url":"http://arxiv.org/abs/2401.12018v1","category":"cs.DB"}
{"created":"2024-01-22 15:03:57","title":"Fault tolerance of stabilizer channels","abstract":"Stabilizer channels, which are stabilizer circuits that implement logical operations while mapping from an input stabilizer code to an output stabilizer code, are ubiquitous for fault tolerant quantum computing not just with surface codes, but with general LDPC codes and Floquet codes. We introduce a rigorous and general formalism to analyze the fault tolerance properties of any stabilizer channel under a broad class of noise models. We provide rigorous but easy-to-work-with definitions and algorithms for the fault distance and hook faults for stabilizer channels. Additionally, we establish necessary conditions such that channel composition preserves the fault distance. We apply our framework to design and analyze fault tolerant stabilizer channels for surface codes, revealing novel aspects of fault tolerant circuits.","sentences":["Stabilizer channels, which are stabilizer circuits that implement logical operations while mapping from an input stabilizer code to an output stabilizer code, are ubiquitous for fault tolerant quantum computing not just with surface codes, but with general LDPC codes and Floquet codes.","We introduce a rigorous and general formalism to analyze the fault tolerance properties of any stabilizer channel under a broad class of noise models.","We provide rigorous but easy-to-work-with definitions and algorithms for the fault distance and hook faults for stabilizer channels.","Additionally, we establish necessary conditions such that channel composition preserves the fault distance.","We apply our framework to design and analyze fault tolerant stabilizer channels for surface codes, revealing novel aspects of fault tolerant circuits."],"url":"http://arxiv.org/abs/2401.12017v1","category":"quant-ph"}
{"created":"2024-01-22 15:00:32","title":"Robustness to distribution shifts of compressed networks for edge devices","abstract":"It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources. However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained. It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations. In this study, we discover that compressed models are less robust to distribution shifts than their original networks. Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks. Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks. Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness.","sentences":["It is necessary to develop efficient DNNs deployed on edge devices with limited computation resources.","However, the compressed networks often execute new tasks in the target domain, which is different from the source domain where the original network is trained.","It is important to investigate the robustness of compressed networks in two types of data distribution shifts: domain shifts and adversarial perturbations.","In this study, we discover that compressed models are less robust to distribution shifts than their original networks.","Interestingly, larger networks are more vulnerable to losing robustness than smaller ones, even when they are compressed to a similar size as the smaller networks.","Furthermore, compact networks obtained by knowledge distillation are much more robust to distribution shifts than pruned networks.","Finally, post-training quantization is a reliable method for achieving significant robustness to distribution shifts, and it outperforms both pruned and distilled models in terms of robustness."],"url":"http://arxiv.org/abs/2401.12014v1","category":"cs.LG"}
{"created":"2024-01-22 14:59:11","title":"TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for Lazy Clients","abstract":"Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years. In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data. Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity. The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical. In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are \"lazy\" and train their models solely for few epochs for next global aggregation. TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings. We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution. Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC.","sentences":["Federated learning is a distributed collaborative machine learning paradigm that has gained strong momentum in recent years.","In federated learning, a central server periodically coordinates models with clients and aggregates the models trained locally by clients without necessitating access to local data.","Despite its potential, the implementation of federated learning continues to encounter several challenges, predominantly the slow convergence that is largely due to data heterogeneity.","The slow convergence becomes particularly problematic in cross-device federated learning scenarios where clients may be strongly limited by computing power and storage space, and hence counteracting methods that induce additional computation or memory cost on the client side such as auxiliary objective terms and larger training iterations can be impractical.","In this paper, we propose a novel federated aggregation strategy, TurboSVM-FL, that poses no additional computation burden on the client side and can significantly accelerate convergence for federated classification task, especially when clients are \"lazy\" and train their models solely for few epochs for next global aggregation.","TurboSVM-FL extensively utilizes support vector machine to conduct selective aggregation and max-margin spread-out regularization on class embeddings.","We evaluate TurboSVM-FL on multiple datasets including FEMNIST, CelebA, and Shakespeare using user-independent validation with non-iid data distribution.","Our results show that TurboSVM-FL can significantly outperform existing popular algorithms on convergence rate and reduce communication rounds while delivering better test metrics including accuracy, F1 score, and MCC."],"url":"http://arxiv.org/abs/2401.12012v1","category":"cs.LG"}
{"created":"2024-01-22 14:58:54","title":"Architecting Data-Intensive Applications : From Data Architecture Design to Its Quality Assurance","abstract":"Context - The exponential growth of data is becoming a significant concern. Managing this data has become incredibly challenging, especially when dealing with various sources in different formats and speeds. Moreover, Ensuring data quality has become increasingly crucial for effective decision-making and operational processes. Data Architecture is crucial in describing, collecting, storing, processing, and analyzing data to meet business needs. Providing an abstract view of data-intensive applications is essential to ensure that the data is transformed into valuable information. We must take these challenges seriously to ensure we can effectively manage and use the data to our advantage. Objective - To establish an architecture framework that enables a comprehensive description of the data architecture and effectively streamlines data quality monitoring. Method - The architecture framework utilizes Model Driven Engineering (MDE) techniques. Its backing of data-intensive architecture descriptions empowers with an automated generation for data quality checks. Result - The Framework offers a comprehensive solution for data-intensive applications to model their architecture efficiently and monitor the quality of their data. It automates the entire process and ensures precision and consistency in data. With DAT, architects and analysts gain access to a powerful tool that simplifies their workflow and empowers them to make informed decisions based on reliable data insights. Conclusion - We have evaluated the DAT on more than five cases within various industry domains, demonstrating its exceptional adaptability and effectiveness.","sentences":["Context - The exponential growth of data is becoming a significant concern.","Managing this data has become incredibly challenging, especially when dealing with various sources in different formats and speeds.","Moreover, Ensuring data quality has become increasingly crucial for effective decision-making and operational processes.","Data Architecture is crucial in describing, collecting, storing, processing, and analyzing data to meet business needs.","Providing an abstract view of data-intensive applications is essential to ensure that the data is transformed into valuable information.","We must take these challenges seriously to ensure we can effectively manage and use the data to our advantage.","Objective - To establish an architecture framework that enables a comprehensive description of the data architecture and effectively streamlines data quality monitoring.","Method -","The architecture framework utilizes Model Driven Engineering (MDE) techniques.","Its backing of data-intensive architecture descriptions empowers with an automated generation for data quality checks.","Result -","The Framework offers a comprehensive solution for data-intensive applications to model their architecture efficiently and monitor the quality of their data.","It automates the entire process and ensures precision and consistency in data.","With DAT, architects and analysts gain access to a powerful tool that simplifies their workflow and empowers them to make informed decisions based on reliable data insights.","Conclusion - We have evaluated the DAT on more than five cases within various industry domains, demonstrating its exceptional adaptability and effectiveness."],"url":"http://arxiv.org/abs/2401.12011v1","category":"cs.SE"}
{"created":"2024-01-22 14:58:11","title":"On a class of interdiction problems with partition matroids: complexity and polynomial-time algorithms","abstract":"In this study, we consider a class of linear matroid interdiction problems, where the feasible sets for the upper-level decision-maker (referred to as the leader) and the lower-level decision-maker (referred to as the follower) are given by partition matroids with a common ground set. In contrast to classical network interdiction models where the leader is subject to a single budget constraint, in our setting, both the leader and the follower are subject to several independent cardinality constraints and engage in a zero-sum game. While a single-level linear integer programming problem over a partition matroid is known to be polynomially solvable, we prove that the considered bilevel problem is NP-hard, even when the objective function coefficients are all binary. On a positive note, it turns out that, if the number of cardinality constraints is fixed for either the leader or the follower, then the considered class of bilevel problems admits several polynomial-time solution schemes. Specifically, these schemes are based on a single-level dual reformulation, a dynamic programming-based approach, and a 2-flip local search algorithm for the leader.","sentences":["In this study, we consider a class of linear matroid interdiction problems, where the feasible sets for the upper-level decision-maker (referred to as the leader) and the lower-level decision-maker (referred to as the follower) are given by partition matroids with a common ground set.","In contrast to classical network interdiction models where the leader is subject to a single budget constraint, in our setting, both the leader and the follower are subject to several independent cardinality constraints and engage in a zero-sum game.","While a single-level linear integer programming problem over a partition matroid is known to be polynomially solvable, we prove that the considered bilevel problem is NP-hard, even when the objective function coefficients are all binary.","On a positive note, it turns out that, if the number of cardinality constraints is fixed for either the leader or the follower, then the considered class of bilevel problems admits several polynomial-time solution schemes.","Specifically, these schemes are based on a single-level dual reformulation, a dynamic programming-based approach, and a 2-flip local search algorithm for the leader."],"url":"http://arxiv.org/abs/2401.12010v1","category":"cs.CC"}
{"created":"2024-01-22 14:57:17","title":"Four Gluon Vertex from Lattice QCD","abstract":"A lattice QCD calculation for the four gluon one-particle irreducible Green function in the Landau gauge is discussed. Results for some of the associated form factors are reported for kinematical configurations with a single momentum scale. Our results show that the computation of this Green function requires large statistical ensembles with 10K or larger number of gauge configurations. The simulations considered herein have a clear Monte Carlo signal for momenta up to $\\sim 1$ GeV. The form factors show an hierarchy, with the form factor associated with the tree level Feynman rule being dominant and essentially constant for the range of momenta accessed. The remaining form factors seem to increase as the momentum decreases, suggesting that a possible $\\log$ divergence may occur. The computed form factors are, at least, in qualitative agreement with the results obtained with continuum approaches to this vertex, when available.","sentences":["A lattice QCD calculation for the four gluon one-particle irreducible Green function in the Landau gauge is discussed.","Results for some of the associated form factors are reported for kinematical configurations with a single momentum scale.","Our results show that the computation of this Green function requires large statistical ensembles with 10K or larger number of gauge configurations.","The simulations considered herein have a clear Monte Carlo signal for momenta up to $\\sim 1$ GeV. The form factors show an hierarchy, with the form factor associated with the tree level Feynman rule being dominant and essentially constant for the range of momenta accessed.","The remaining form factors seem to increase as the momentum decreases, suggesting that a possible $\\log$ divergence may occur.","The computed form factors are, at least, in qualitative agreement with the results obtained with continuum approaches to this vertex, when available."],"url":"http://arxiv.org/abs/2401.12008v1","category":"hep-lat"}
{"created":"2024-01-22 14:55:01","title":"Tensor-view Topological Graph Neural Network","abstract":"Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation learning modules that disentangle feature tensor aggregation and transformation and learn to preserve multi-modal structure with less computation. Theoretically, we derive high probability bounds on both the out-of-sample and in-sample mean squared approximation errors for our proposed Tensor Transformation Layer (TTL). Real data experiments show that the proposed TTG-NN outperforms 20 state-of-the-art methods on various graph benchmarks.","sentences":["Graph classification is an important learning task for graph-structured data.","Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems.","Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation.","To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations.","This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels.","Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation learning modules that disentangle feature tensor aggregation and transformation and learn to preserve multi-modal structure with less computation.","Theoretically, we derive high probability bounds on both the out-of-sample and in-sample mean squared approximation errors for our proposed Tensor Transformation Layer (TTL).","Real data experiments show that the proposed TTG-NN outperforms 20 state-of-the-art methods on various graph benchmarks."],"url":"http://arxiv.org/abs/2401.12007v1","category":"cs.LG"}
{"created":"2024-01-22 14:53:59","title":"ALMs: Authorial Language Models for Authorship Attribution","abstract":"In this paper, we introduce an authorship attribution method called Authorial Language Models (ALMs) that involves identifying the most likely author of a questioned document based on the perplexity of the questioned document calculated for a set of causal language models fine-tuned on the writings of a set of candidate author. We benchmarked ALMs against state-of-art-systems using the CCAT50 dataset and the Blogs50 datasets. We find that ALMs achieves a macro-average accuracy score of 83.6% on Blogs50, outperforming all other methods, and 74.9% on CCAT50, matching the performance of the best method. To assess the performance of ALMs on shorter texts, we also conducted text ablation testing. We found that to reach a macro-average accuracy of 70%, ALMs needs 40 tokens on Blogs50 and 400 tokens on CCAT50, while to reach 60% ALMs requires 20 tokens on Blogs50 and 70 tokens on CCAT50.","sentences":["In this paper, we introduce an authorship attribution method called Authorial Language Models (ALMs) that involves identifying the most likely author of a questioned document based on the perplexity of the questioned document calculated for a set of causal language models fine-tuned on the writings of a set of candidate author.","We benchmarked ALMs against state-of-art-systems using the CCAT50 dataset and the Blogs50 datasets.","We find that ALMs achieves a macro-average accuracy score of 83.6% on Blogs50, outperforming all other methods, and 74.9% on CCAT50, matching the performance of the best method.","To assess the performance of ALMs on shorter texts, we also conducted text ablation testing.","We found that to reach a macro-average accuracy of 70%, ALMs needs 40 tokens on Blogs50 and 400 tokens on CCAT50, while to reach 60% ALMs requires 20 tokens on Blogs50 and 70 tokens on CCAT50."],"url":"http://arxiv.org/abs/2401.12005v1","category":"cs.CL"}
{"created":"2024-01-22 14:53:21","title":"NLCG-Net: A Model-Based Zero-Shot Learning Framework for Undersampled Quantitative MRI Reconstruction","abstract":"Typical quantitative MRI (qMRI) methods estimate parameter maps after image reconstructing, which is prone to biases and error propagation. We propose a Nonlinear Conjugate Gradient (NLCG) optimizer for model-based T2/T1 estimation, which incorporates U-Net regularization trained in a scan-specific manner. This end-to-end method directly estimates qMRI maps from undersampled k-space data using mono-exponential signal modeling with zero-shot scan-specific neural network regularization to enable high fidelity T1 and T2 mapping. T2 and T1 mapping results demonstrate the ability of the proposed NLCG-Net to improve estimation quality compared to subspace reconstruction at high accelerations.","sentences":["Typical quantitative MRI (qMRI) methods estimate parameter maps after image reconstructing, which is prone to biases and error propagation.","We propose a Nonlinear Conjugate Gradient (NLCG) optimizer for model-based T2/T1 estimation, which incorporates U-Net regularization trained in a scan-specific manner.","This end-to-end method directly estimates qMRI maps from undersampled k-space data using mono-exponential signal modeling with zero-shot scan-specific neural network regularization to enable high fidelity T1 and T2 mapping.","T2 and T1 mapping results demonstrate the ability of the proposed NLCG-Net to improve estimation quality compared to subspace reconstruction at high accelerations."],"url":"http://arxiv.org/abs/2401.12004v1","category":"eess.IV"}
{"created":"2024-01-22 14:52:34","title":"HgbNet: predicting hemoglobin level/anemia degree from EHR data","abstract":"Anemia is a prevalent medical condition that typically requires invasive blood tests for diagnosis and monitoring. Electronic health records (EHRs) have emerged as valuable data sources for numerous medical studies. EHR-based hemoglobin level/anemia degree prediction is non-invasive and rapid but still faces some challenges due to the fact that EHR data is typically an irregular multivariate time series containing a significant number of missing values and irregular time intervals. To address these issues, we introduce HgbNet, a machine learning-based prediction model that emulates clinicians' decision-making processes for hemoglobin level/anemia degree prediction. The model incorporates a NanDense layer with a missing indicator to handle missing values and employs attention mechanisms to account for both local irregularity and global irregularity. We evaluate the proposed method using two real-world datasets across two use cases. In our first use case, we predict hemoglobin level/anemia degree at moment T+1 by utilizing records from moments prior to T+1. In our second use case, we integrate all historical records with additional selected test results at moment T+1 to predict hemoglobin level/anemia degree at the same moment, T+1. HgbNet outperforms the best baseline results across all datasets and use cases. These findings demonstrate the feasibility of estimating hemoglobin levels and anemia degree from EHR data, positioning HgbNet as an effective non-invasive anemia diagnosis solution that could potentially enhance the quality of life for millions of affected individuals worldwide. To our knowledge, HgbNet is the first machine learning model leveraging EHR data for hemoglobin level/anemia degree prediction.","sentences":["Anemia is a prevalent medical condition that typically requires invasive blood tests for diagnosis and monitoring.","Electronic health records (EHRs) have emerged as valuable data sources for numerous medical studies.","EHR-based hemoglobin level/anemia degree prediction is non-invasive and rapid but still faces some challenges due to the fact that EHR data is typically an irregular multivariate time series containing a significant number of missing values and irregular time intervals.","To address these issues, we introduce HgbNet, a machine learning-based prediction model that emulates clinicians' decision-making processes for hemoglobin level/anemia degree prediction.","The model incorporates a NanDense layer with a missing indicator to handle missing values and employs attention mechanisms to account for both local irregularity and global irregularity.","We evaluate the proposed method using two real-world datasets across two use cases.","In our first use case, we predict hemoglobin level/anemia degree at moment T+1","by utilizing records from moments prior to T+1.","In our second use case, we integrate all historical records with additional selected test results at moment T+1 to predict hemoglobin level/anemia degree at the same moment, T+1.","HgbNet outperforms the best baseline results across all datasets and use cases.","These findings demonstrate the feasibility of estimating hemoglobin levels and anemia degree from EHR data, positioning HgbNet as an effective non-invasive anemia diagnosis solution that could potentially enhance the quality of life for millions of affected individuals worldwide.","To our knowledge, HgbNet is the first machine learning model leveraging EHR data for hemoglobin level/anemia degree prediction."],"url":"http://arxiv.org/abs/2401.12002v1","category":"cs.LG"}
{"created":"2024-01-22 14:52:08","title":"Modeling Stereo-Confidence Out of the End-to-End Stereo-Matching Network via Disparity Plane Sweep","abstract":"We propose a novel stereo-confidence that can be measured externally to various stereo-matching networks, offering an alternative input modality choice of the cost volume for learning-based approaches, especially in safety-critical systems. Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method is built upon the idea that any shift in a stereo-image pair should be updated in a corresponding amount shift in the disparity map. Based on this idea, the proposed stereo-confidence method can be summarized in three folds. 1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume), like the cost volume is constructed. 2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile at every spatial point. 3) By comparing the desirable and predicted disparity profiles, we can quantify the level of matching ambiguity between left and right images for confidence measurement. Extensive experimental results using various stereo-matching networks and datasets demonstrate that the proposed stereo-confidence method not only shows competitive performance on its own but also consistent performance improvements when it is used as an input modality for learning-based stereo-confidence methods.","sentences":["We propose a novel stereo-confidence that can be measured externally to various stereo-matching networks, offering an alternative input modality choice of the cost volume for learning-based approaches, especially in safety-critical systems.","Grounded in the foundational concepts of disparity definition and the disparity plane sweep, the proposed stereo-confidence method is built upon the idea that any shift in a stereo-image pair should be updated in a corresponding amount shift in the disparity map.","Based on this idea, the proposed stereo-confidence method can be summarized in three folds.","1) Using the disparity plane sweep, multiple disparity maps can be obtained and treated as a 3-D volume (predicted disparity volume), like the cost volume is constructed.","2) One of these disparity maps serves as an anchor, allowing us to define a desirable (or ideal) disparity profile at every spatial point.","3) By comparing the desirable and predicted disparity profiles, we can quantify the level of matching ambiguity between left and right images for confidence measurement.","Extensive experimental results using various stereo-matching networks and datasets demonstrate that the proposed stereo-confidence method not only shows competitive performance on its own but also consistent performance improvements when it is used as an input modality for learning-based stereo-confidence methods."],"url":"http://arxiv.org/abs/2401.12001v1","category":"cs.CV"}
{"created":"2024-01-22 14:51:01","title":"Integrating Statistical Significance and Discriminative Power in Pattern Discovery","abstract":"Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains. Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power. Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality. We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria. To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\\delta$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search. Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures.   Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license.","sentences":["Pattern discovery plays a central role in both descriptive and predictive tasks across multiple domains.","Actionable patterns must meet rigorous statistical significance criteria and, in the presence of target variables, further uphold discriminative power.","Our work addresses the underexplored area of guiding pattern discovery by integrating statistical significance and discriminative power criteria into state-of-the-art algorithms while preserving pattern quality.","We also address how pattern quality thresholds, imposed by some algorithms, can be rectified to accommodate these additional criteria.","To test the proposed methodology, we select the triclustering task as the guiding pattern discovery case and extend well-known greedy and multi-objective optimization triclustering algorithms, $\\delta$-Trimax and TriGen, that use various pattern quality criteria, such as Mean Squared Residual (MSR), Least Squared Lines (LSL), and Multi Slope Measure (MSL).","Results from three case studies show the role of the proposed methodology in discovering patterns with pronounced improvements of discriminative power and statistical significance without quality deterioration, highlighting its importance in supervisedly guiding the search.","Although the proposed methodology is motivated over multivariate time series data, it can be straightforwardly extended to pattern discovery tasks involving multivariate, N-way (N>3), transactional, and sequential data structures.   ","Availability: The code is freely available at https://github.com/JupitersMight/MOF_Triclustering under the MIT license."],"url":"http://arxiv.org/abs/2401.12000v1","category":"cs.LG"}
{"created":"2024-01-22 14:46:41","title":"Expert-Driven Monitoring of Operational ML Models","abstract":"We propose Expert Monitoring, an approach that leverages domain expertise to enhance the detection and mitigation of concept drift in machine learning (ML) models. Our approach supports practitioners by consolidating domain expertise related to concept drift-inducing events, making this expertise accessible to on-call personnel, and enabling automatic adaptability with expert oversight.","sentences":["We propose Expert Monitoring, an approach that leverages domain expertise to enhance the detection and mitigation of concept drift in machine learning (ML) models.","Our approach supports practitioners by consolidating domain expertise related to concept drift-inducing events, making this expertise accessible to on-call personnel, and enabling automatic adaptability with expert oversight."],"url":"http://arxiv.org/abs/2401.11993v1","category":"cs.LG"}
{"created":"2024-01-22 14:44:07","title":"Tight Bounds on the Message Complexity of Distributed Tree Verification","abstract":"We consider the message complexity of verifying whether a given subgraph of the communication network forms a tree with specific properties both in the KT-$\\rho$ (nodes know their $\\rho$-hop neighborhood, including node IDs) and the KT-$0$ (nodes do not have this knowledge) models. We develop a rather general framework that helps in establishing tight lower bounds for various tree verification problems. We also consider two different verification requirements: namely that every node detects in the case the input is incorrect, as well as the requirement that at least one node detects. The results are stronger than previous ones in the sense that we assume that each node knows the number $n$ of nodes in the graph (in some cases) or an $\\alpha$ approximation of $n$ (in other cases). For spanning tree verification, we show that the message complexity inherently depends on the quality of the given approximation of $n$: We show a tight lower bound of $\\Omega(n^2)$ for the case $\\alpha \\ge \\sqrt{2}$ and a much better upper bound (i.e., $O(n \\log n)$) when nodes are given a tighter approximation. On the other hand, our framework also yields an $\\Omega(n^2)$ lower bound on the message complexity of verifying a minimum spanning tree (MST), which reveals a polynomial separation between ST verification and MST verification. This result holds for randomized algorithms with perfect knowledge of the network size, and even when just one node detects illegal inputs, thus improving over the work of Kor, Korman, and Peleg (2013). For verifying a $d$-approximate BFS tree, we show that the same lower bound holds even if nodes know $n$ exactly, however, the lower bound is sensitive to $d$, which is the stretch parameter.","sentences":["We consider the message complexity of verifying whether a given subgraph of the communication network forms a tree with specific properties both in the KT-$\\rho$ (nodes know their $\\rho$-hop neighborhood, including node IDs) and the KT-$0$ (nodes do not have this knowledge) models.","We develop a rather general framework that helps in establishing tight lower bounds for various tree verification problems.","We also consider two different verification requirements: namely that every node detects in the case the input is incorrect, as well as the requirement that at least one node detects.","The results are stronger than previous ones in the sense that we assume that each node knows the number $n$ of nodes in the graph (in some cases) or an $\\alpha$ approximation of $n$ (in other cases).","For spanning tree verification, we show that the message complexity inherently depends on the quality of the given approximation of $n$: We show a tight lower bound of $\\Omega(n^2)$ for the case $\\alpha \\ge \\sqrt{2}$ and a much better upper bound (i.e., $O(n \\log n)$) when nodes are given a tighter approximation.","On the other hand, our framework also yields an $\\Omega(n^2)$ lower bound on the message complexity of verifying a minimum spanning tree (MST), which reveals a polynomial separation between ST verification and MST verification.","This result holds for randomized algorithms with perfect knowledge of the network size, and even when just one node detects illegal inputs, thus improving over the work of Kor, Korman, and Peleg (2013).","For verifying a $d$-approximate BFS tree, we show that the same lower bound holds even if nodes know $n$ exactly, however, the lower bound is sensitive to $d$, which is the stretch parameter."],"url":"http://arxiv.org/abs/2401.11991v1","category":"cs.DC"}
{"created":"2024-01-22 14:39:30","title":"Versatile quadrature antenna for precise control of large electron spin ensembles in diamond","abstract":"We present an easily reproducible inexpensive microwave antenna that can generate a strong and homogeneous magnetic field of arbitrary polarization, which enables fast and coherent control of electron spins over a large volume. Unlike preceding works, we present a resonant antenna that maintains its resonant behaviour regardless of the proximity of other experimental hardware components. This robustness is crucial as it enables, amongst others, using microscope objectives with short working distances to perform wide field imaging/sensing with bulk diamonds. The antenna generates a magnetic field strength of 22.3 A/m for 1 W total driving power, which doubles the power efficiency compared with previously reported patch antenna designs. The magnetic field homogeneity in a volume of $1 \\text{mm}^3$ is within 6.6\\%. The antenna has a full width at half maximum bandwidth of $\\sim$160 MHz and its resonant frequency can be tuned over a 400 MHz range via four capacitors or varactors. The antenna has been tested and found to remain within safe handling temperatures during continuous-wave operation at 8 W. The files required to reproduce this antenna, which can be built on a standard and affordable double sided PCB, are provided open-source. This work facilitates a robust and versatile piece of instrumentation, being particularly appealing for applications such as high sensitivity magnetometry and wide field imaging/sensing with Nitrogen Vacancy centers.","sentences":["We present an easily reproducible inexpensive microwave antenna that can generate a strong and homogeneous magnetic field of arbitrary polarization, which enables fast and coherent control of electron spins over a large volume.","Unlike preceding works, we present a resonant antenna that maintains its resonant behaviour regardless of the proximity of other experimental hardware components.","This robustness is crucial as it enables, amongst others, using microscope objectives with short working distances to perform wide field imaging/sensing with bulk diamonds.","The antenna generates a magnetic field strength of 22.3 A/m for 1 W total driving power, which doubles the power efficiency compared with previously reported patch antenna designs.","The magnetic field homogeneity in a volume of $1 \\text{mm}^3$ is within 6.6\\%.","The antenna has a full width at half maximum bandwidth of $\\sim$160 MHz and its resonant frequency can be tuned over a 400 MHz range via four capacitors or varactors.","The antenna has been tested and found to remain within safe handling temperatures during continuous-wave operation at 8 W. The files required to reproduce this antenna, which can be built on a standard and affordable double sided PCB, are provided open-source.","This work facilitates a robust and versatile piece of instrumentation, being particularly appealing for applications such as high sensitivity magnetometry and wide field imaging/sensing with Nitrogen Vacancy centers."],"url":"http://arxiv.org/abs/2401.11986v1","category":"physics.ins-det"}
{"created":"2024-01-22 14:38:25","title":"Scaling Face Interaction Graph Networks to Real World Scenes","abstract":"Accurately simulating real world object dynamics is essential for various applications such as robotics, engineering, graphics, and design. To better capture complex real dynamics such as contact and friction, learned simulators based on graph networks have recently shown great promise. However, applying these learned simulators to real scenes comes with two major challenges: first, scaling learned simulators to handle the complexity of real world scenes which can involve hundreds of objects each with complicated 3D shapes, and second, handling inputs from perception rather than 3D state information. Here we introduce a method which substantially reduces the memory required to run graph-based learned simulators. Based on this memory-efficient simulation model, we then present a perceptual interface in the form of editable NeRFs which can convert real-world scenes into a structured representation that can be processed by graph network simulator. We show that our method uses substantially less memory than previous graph-based simulators while retaining their accuracy, and that the simulators learned in synthetic environments can be applied to real world scenes captured from multiple camera angles. This paves the way for expanding the application of learned simulators to settings where only perceptual information is available at inference time.","sentences":["Accurately simulating real world object dynamics is essential for various applications such as robotics, engineering, graphics, and design.","To better capture complex real dynamics such as contact and friction, learned simulators based on graph networks have recently shown great promise.","However, applying these learned simulators to real scenes comes with two major challenges: first, scaling learned simulators to handle the complexity of real world scenes which can involve hundreds of objects each with complicated 3D shapes, and second, handling inputs from perception rather than 3D state information.","Here we introduce a method which substantially reduces the memory required to run graph-based learned simulators.","Based on this memory-efficient simulation model, we then present a perceptual interface in the form of editable NeRFs which can convert real-world scenes into a structured representation that can be processed by graph network simulator.","We show that our method uses substantially less memory than previous graph-based simulators while retaining their accuracy, and that the simulators learned in synthetic environments can be applied to real world scenes captured from multiple camera angles.","This paves the way for expanding the application of learned simulators to settings where only perceptual information is available at inference time."],"url":"http://arxiv.org/abs/2401.11985v1","category":"cs.LG"}
{"created":"2024-01-22 14:36:01","title":"Lightweight Protection for Privacy in Offloaded Speech Understanding","abstract":"Speech is a common input method for mobile embedded devices, but cloud-based speech recognition systems pose privacy risks. Disentanglement-based encoders, designed to safeguard user privacy by filtering sensitive information from speech signals, unfortunately require substantial memory and computational resources, which limits their use in less powerful devices. To overcome this, we introduce a novel system, XXX, optimized for such devices. XXX is built on the insight that speech understanding primarily relies on understanding the entire utterance's long-term dependencies, while privacy concerns are often linked to short-term details. Therefore, XXX focuses on selectively masking these short-term elements, preserving the quality of long-term speech understanding. The core of XXX is an innovative differential mask generator, grounded in interpretable learning, which fine-tunes the masking process. We tested XXX on the STM32H7 microcontroller, assessing its performance in various potential attack scenarios. The results show that XXX maintains speech understanding accuracy and privacy at levels comparable to existing encoders, but with a significant improvement in efficiency, achieving up to 53.3$\\times$ faster processing and a 134.1$\\times$ smaller memory footprint.","sentences":["Speech is a common input method for mobile embedded devices, but cloud-based speech recognition systems pose privacy risks.","Disentanglement-based encoders, designed to safeguard user privacy by filtering sensitive information from speech signals, unfortunately require substantial memory and computational resources, which limits their use in less powerful devices.","To overcome this, we introduce a novel system, XXX, optimized for such devices.","XXX is built on the insight that speech understanding primarily relies on understanding the entire utterance's long-term dependencies, while privacy concerns are often linked to short-term details.","Therefore, XXX focuses on selectively masking these short-term elements, preserving the quality of long-term speech understanding.","The core of XXX is an innovative differential mask generator, grounded in interpretable learning, which fine-tunes the masking process.","We tested XXX on the STM32H7 microcontroller, assessing its performance in various potential attack scenarios.","The results show that XXX maintains speech understanding accuracy and privacy at levels comparable to existing encoders, but with a significant improvement in efficiency, achieving up to 53.3$\\times$ faster processing and a 134.1$\\times$ smaller memory footprint."],"url":"http://arxiv.org/abs/2401.11983v1","category":"cs.SD"}
{"created":"2024-01-22 14:32:20","title":"Learning Analytics in Higher Education -- Exploring Students and Teachers Expectations in Germany","abstract":"Technology enhanced learning analytics has the potential to play a significant role in higher education in the future. Opinions and expectations towards technology and learning analytics, thus, are vital to consider for institutional developments in higher education institutions. The Sheila framework offers instruments to yield exploratory knowledge about stakeholder aspirations towards technology, such as learning analytics in higher education. The sample of the study consists of students (N = 1169) and teachers (N = 497) at a higher education institution in Germany. Using self-report questionnaires, we assessed students and teachers attitudes towards learning analytics in higher education teaching, comparing ideal and expected circumstances. We report results on the attitudes of students, teachers, as well as comparisons of the two groups and different disciplines. We discuss the results with regard to practical implications for the implementation and further developments of learning analytics in higher education.","sentences":["Technology enhanced learning analytics has the potential to play a significant role in higher education in the future.","Opinions and expectations towards technology and learning analytics, thus, are vital to consider for institutional developments in higher education institutions.","The Sheila framework offers instruments to yield exploratory knowledge about stakeholder aspirations towards technology, such as learning analytics in higher education.","The sample of the study consists of students (N = 1169) and teachers (N = 497) at a higher education institution in Germany.","Using self-report questionnaires, we assessed students and teachers attitudes towards learning analytics in higher education teaching, comparing ideal and expected circumstances.","We report results on the attitudes of students, teachers, as well as comparisons of the two groups and different disciplines.","We discuss the results with regard to practical implications for the implementation and further developments of learning analytics in higher education."],"url":"http://arxiv.org/abs/2401.11981v1","category":"cs.CY"}
{"created":"2024-01-22 14:31:46","title":"On the uniqueness of compiling graphs under the parity transformation","abstract":"In this article, we establish a mathematical framework that utilizes concepts from graph theory to define the parity transformation as a mapping that encompasses all possible compiled hypergraphs, and investigate uniqueness properties of this mapping in more detail. By introducing so-called loop labelings, we derive an alternative expression of the preimage of any set of compiled hypergraphs under this encoding procedure when all equivalences classes of graphs are being considered. We then deduce equivalent conditions for the injectivity of the parity transformation on any subset of all equivalences classes of graphs. Moreover, we show concrete examples of optimization problems demonstrating that the parity transformation is not an injective mapping, and also introduce an important class of plaquette layouts and their corresponding set of constraints whose preimage is uniquely determined. In addition, we provide an algorithm which is based on classical algorithms from theoretical computer science and computes a compiled physical layout in this class in polynomial time.","sentences":["In this article, we establish a mathematical framework that utilizes concepts from graph theory to define the parity transformation as a mapping that encompasses all possible compiled hypergraphs, and investigate uniqueness properties of this mapping in more detail.","By introducing so-called loop labelings, we derive an alternative expression of the preimage of any set of compiled hypergraphs under this encoding procedure when all equivalences classes of graphs are being considered.","We then deduce equivalent conditions for the injectivity of the parity transformation on any subset of all equivalences classes of graphs.","Moreover, we show concrete examples of optimization problems demonstrating that the parity transformation is not an injective mapping, and also introduce an important class of plaquette layouts and their corresponding set of constraints whose preimage is uniquely determined.","In addition, we provide an algorithm which is based on classical algorithms from theoretical computer science and computes a compiled physical layout in this class in polynomial time."],"url":"http://arxiv.org/abs/2401.11980v1","category":"quant-ph"}
{"created":"2024-01-22 14:28:00","title":"Adaptive Motion Planning for Multi-fingered Functional Grasp via Force Feedback","abstract":"Enabling multi-fingered robots to grasp and manipulate objects with human-like dexterity is especially challenging during the dynamic, continuous hand-object interactions. Closed-loop feedback control is essential for dexterous hands to dynamically finetune hand poses when performing precise functional grasps. This work proposes an adaptive motion planning method based on deep reinforcement learning to adjust grasping poses according to real-time feedback from joint torques from pre-grasp to goal grasp. We find the multi-joint torques of the dexterous hand can sense object positions through contacts and collisions, enabling real-time adjustment of grasps to generate varying grasping trajectories for objects in different positions. In our experiments, the performance gap with and without force feedback reveals the important role of force feedback in adaptive manipulation. Our approach utilizing force feedback preliminarily exhibits human-like flexibility, adaptability, and precision.","sentences":["Enabling multi-fingered robots to grasp and manipulate objects with human-like dexterity is especially challenging during the dynamic, continuous hand-object interactions.","Closed-loop feedback control is essential for dexterous hands to dynamically finetune hand poses when performing precise functional grasps.","This work proposes an adaptive motion planning method based on deep reinforcement learning to adjust grasping poses according to real-time feedback from joint torques from pre-grasp to goal grasp.","We find the multi-joint torques of the dexterous hand can sense object positions through contacts and collisions, enabling real-time adjustment of grasps to generate varying grasping trajectories for objects in different positions.","In our experiments, the performance gap with and without force feedback reveals the important role of force feedback in adaptive manipulation.","Our approach utilizing force feedback preliminarily exhibits human-like flexibility, adaptability, and precision."],"url":"http://arxiv.org/abs/2401.11977v1","category":"cs.RO"}
{"created":"2024-01-22 14:26:14","title":"Crossing the singularity of a gravitational wave collision","abstract":"A reformulation of general relativity inspired by the Belinski-Khalatnikov-Lifshitz conjecture had been introduced by Ashtekar, Henderson and Sloan which is based on variables closely related to the basic variables of loop quantum gravity, thereby providing a way of classically analyzing singularities that may be carried over to the quantum theory. It is reasonable to expect that these variables are regular at generic spacelike singularities. This has been shown on various examples -- particularly, cosmological spacetimes. In this study we extend this analysis to the singularities of gravitational wave collision spacetimes, which are the result of the mutual focusing of the two waves. We focus on two specific examples and explicitly confirm that the said variables are regular at the singularity and can be smoothly continued beyond it.","sentences":["A reformulation of general relativity inspired by the Belinski-Khalatnikov-Lifshitz conjecture had been introduced by Ashtekar, Henderson and Sloan which is based on variables closely related to the basic variables of loop quantum gravity, thereby providing a way of classically analyzing singularities that may be carried over to the quantum theory.","It is reasonable to expect that these variables are regular at generic spacelike singularities.","This has been shown on various examples -- particularly, cosmological spacetimes.","In this study we extend this analysis to the singularities of gravitational wave collision spacetimes, which are the result of the mutual focusing of the two waves.","We focus on two specific examples and explicitly confirm that the said variables are regular at the singularity and can be smoothly continued beyond it."],"url":"http://arxiv.org/abs/2401.11975v1","category":"gr-qc"}
{"created":"2024-01-22 14:26:02","title":"Cross-Validation Conformal Risk Control","abstract":"Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees. Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate. The original CRC requires the available data set to be split between training and validation data sets. This can be problematic when data availability is limited, resulting in inefficient set predictors. In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC. The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions. CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor. Furthermore, numerical experiments show that CV-CRC can reduce the average set size with respect to CRC when the available data are limited.","sentences":["Conformal risk control (CRC) is a recently proposed technique that applies post-hoc to a conventional point predictor to provide calibration guarantees.","Generalizing conformal prediction (CP), with CRC, calibration is ensured for a set predictor that is extracted from the point predictor to control a risk function such as the probability of miscoverage or the false negative rate.","The original CRC requires the available data set to be split between training and validation data sets.","This can be problematic when data availability is limited, resulting in inefficient set predictors.","In this paper, a novel CRC method is introduced that is based on cross-validation, rather than on validation as the original CRC.","The proposed cross-validation CRC (CV-CRC) extends a version of the jackknife-minmax from CP to CRC, allowing for the control of a broader range of risk functions.","CV-CRC is proved to offer theoretical guarantees on the average risk of the set predictor.","Furthermore, numerical experiments show that CV-CRC can reduce the average set size with respect to CRC when the available data are limited."],"url":"http://arxiv.org/abs/2401.11974v1","category":"cs.LG"}
{"created":"2024-01-22 14:24:03","title":"Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid Approaches to Natural Language Processing","abstract":"The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP). While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks. Meanwhile, the symbolic methods excel in representing knowledge-rich data. However, they struggle to adapt dynamic data and generalize the knowledge. Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths. Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks. In this paper, we present an overview of hybrid approaches used for NLP. Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning. Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges, offering a roadmap for future directions.","sentences":["The advancement of machine learning and symbolic approaches have underscored their strengths and weaknesses in Natural Language Processing (NLP).","While machine learning approaches are powerful in identifying patterns in data, they often fall short in learning commonsense and the factual knowledge required for the NLP tasks.","Meanwhile, the symbolic methods excel in representing knowledge-rich data.","However, they struggle to adapt dynamic data and generalize the knowledge.","Bridging these two paradigms through hybrid approaches enables the alleviation of weaknesses in both while preserving their strengths.","Recent studies extol the virtues of this union, showcasing promising results in a wide range of NLP tasks.","In this paper, we present an overview of hybrid approaches used for NLP.","Specifically, we delve into the state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks requiring natural language understanding, generation, and reasoning.","Furthermore, we discuss the existing resources available for hybrid approaches for NLP along with the challenges, offering a roadmap for future directions."],"url":"http://arxiv.org/abs/2401.11972v1","category":"cs.CL"}
{"created":"2024-01-22 14:17:03","title":"Claim Detection for Automated Fact-checking: A Survey on Monolingual, Multilingual and Cross-Lingual Research","abstract":"Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms. This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims. This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods. This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue. Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation. Focusing on multilingual misinformation, we present a comprehensive survey of existing multilingual claim detection research. We present state-of-the-art multilingual claim detection research categorized into three key factors of the problem, verifiability, priority, and similarity. Further, we present a detailed overview of the existing multilingual datasets along with the challenges and suggest possible future advancements.","sentences":["Automated fact-checking has drawn considerable attention over the past few decades due to the increase in the diffusion of misinformation on online platforms.","This is often carried out as a sequence of tasks comprising (i) the detection of sentences circulating in online platforms which constitute claims needing verification, followed by (ii) the verification process of those claims.","This survey focuses on the former, by discussing existing efforts towards detecting claims needing fact-checking, with a particular focus on multilingual data and methods.","This is a challenging and fertile direction where existing methods are yet far from matching human performance due to the profoundly challenging nature of the issue.","Especially, the dissemination of information across multiple social platforms, articulated in multiple languages and modalities demands more generalized solutions for combating misinformation.","Focusing on multilingual misinformation, we present a comprehensive survey of existing multilingual claim detection research.","We present state-of-the-art multilingual claim detection research categorized into three key factors of the problem, verifiability, priority, and similarity.","Further, we present a detailed overview of the existing multilingual datasets along with the challenges and suggest possible future advancements."],"url":"http://arxiv.org/abs/2401.11969v1","category":"cs.CL"}
{"created":"2024-01-22 14:16:37","title":"Effective Intrusion Detection in Heterogeneous Internet-of-Things Networks via Ensemble Knowledge Distillation-based Federated Learning","abstract":"With the rapid development of low-cost consumer electronics and cloud computing, Internet-of-Things (IoT) devices are widely adopted for supporting next-generation distributed systems such as smart cities and industrial control systems. IoT devices are often susceptible to cyber attacks due to their open deployment environment and limited computing capabilities for stringent security controls. Hence, Intrusion Detection Systems (IDS) have emerged as one of the effective ways of securing IoT networks by monitoring and detecting abnormal activities. However, existing IDS approaches rely on centralized servers to generate behaviour profiles and detect anomalies, causing high response time and large operational costs due to communication overhead. Besides, sharing of behaviour data in an open and distributed IoT network environment may violate on-device privacy requirements. Additionally, various IoT devices tend to capture heterogeneous data, which complicates the training of behaviour models. In this paper, we introduce Federated Learning (FL) to collaboratively train a decentralized shared model of IDS, without exposing training data to others. Furthermore, we propose an effective method called Federated Learning Ensemble Knowledge Distillation (FLEKD) to mitigate the heterogeneity problems across various clients. FLEKD enables a more flexible aggregation method than conventional model fusion techniques. Experiment results on the public dataset CICIDS2019 demonstrate that the proposed approach outperforms local training and traditional FL in terms of both speed and performance and significantly improves the system's ability to detect unknown attacks. Finally, we evaluate our proposed framework's performance in three potential real-world scenarios and show FLEKD has a clear advantage in experimental results.","sentences":["With the rapid development of low-cost consumer electronics and cloud computing, Internet-of-Things (IoT) devices are widely adopted for supporting next-generation distributed systems such as smart cities and industrial control systems.","IoT devices are often susceptible to cyber attacks due to their open deployment environment and limited computing capabilities for stringent security controls.","Hence, Intrusion Detection Systems (IDS) have emerged as one of the effective ways of securing IoT networks by monitoring and detecting abnormal activities.","However, existing IDS approaches rely on centralized servers to generate behaviour profiles and detect anomalies, causing high response time and large operational costs due to communication overhead.","Besides, sharing of behaviour data in an open and distributed IoT network environment may violate on-device privacy requirements.","Additionally, various IoT devices tend to capture heterogeneous data, which complicates the training of behaviour models.","In this paper, we introduce Federated Learning (FL) to collaboratively train a decentralized shared model of IDS, without exposing training data to others.","Furthermore, we propose an effective method called Federated Learning Ensemble Knowledge Distillation (FLEKD) to mitigate the heterogeneity problems across various clients.","FLEKD enables a more flexible aggregation method than conventional model fusion techniques.","Experiment results on the public dataset CICIDS2019 demonstrate that the proposed approach outperforms local training and traditional FL in terms of both speed and performance and significantly improves the system's ability to detect unknown attacks.","Finally, we evaluate our proposed framework's performance in three potential real-world scenarios and show FLEKD has a clear advantage in experimental results."],"url":"http://arxiv.org/abs/2401.11968v1","category":"cs.CR"}
{"created":"2024-01-22 14:15:55","title":"Exploring descriptors for titanium microstructure via digital fingerprints from variational autoencoders","abstract":"Microstructure is key to controlling and understanding the properties of metallic materials, but traditional approaches to describing microstructure capture only a small number of features. To enable data-centric approaches to materials discovery, allow efficient storage of microstructural data and assist in quality control in metals processing, we require more complete descriptors of microstructure. The concept of microstructural fingerprinting, using machine learning (ML) to develop quantitative, low-dimensional descriptors of microstructures, has recently attracted significant attention. However, it is difficult to interpret conclusions drawn by ML algorithms, which are commonly referred to as \"black boxes\".   Here we explore variational autoencoders (VAEs), which can be trained to produce microstructural fingerprints in a continuous latent space. VAEs enable the reconstruction of images from fingerprints, allowing us to explore how key features of microstructure are encoded. We develop a VAE architecture based on ResNet18 and train it on Ti-6Al-4V optical micrographs as an example of an industrially important alloy where microstructural control is critical to performance. The latent space is explored in several ways, including by supplying interpolated and randomly perturbed fingerprints to the trained decoder and via dimensionality reduction to explore the distribution of microstructural features within the latent space of fingerprints. We show that the VAE fingerprints exhibit smooth, interpolable behaviour with stability to local perturbations, supporting their suitability as general purpose descriptors for microstructure. We also show that key properties of the microstructures are strongly correlated with position in the latent space, supporting the use of VAE fingerprints for quantitative exploration of process-structure-property relationships.","sentences":["Microstructure is key to controlling and understanding the properties of metallic materials, but traditional approaches to describing microstructure capture only a small number of features.","To enable data-centric approaches to materials discovery, allow efficient storage of microstructural data and assist in quality control in metals processing, we require more complete descriptors of microstructure.","The concept of microstructural fingerprinting, using machine learning (ML) to develop quantitative, low-dimensional descriptors of microstructures, has recently attracted significant attention.","However, it is difficult to interpret conclusions drawn by ML algorithms, which are commonly referred to as \"black boxes\".   ","Here we explore variational autoencoders (VAEs), which can be trained to produce microstructural fingerprints in a continuous latent space.","VAEs enable the reconstruction of images from fingerprints, allowing us to explore how key features of microstructure are encoded.","We develop a VAE architecture based on ResNet18 and train it on Ti-6Al-4V optical micrographs as an example of an industrially important alloy where microstructural control is critical to performance.","The latent space is explored in several ways, including by supplying interpolated and randomly perturbed fingerprints to the trained decoder and via dimensionality reduction to explore the distribution of microstructural features within the latent space of fingerprints.","We show that the VAE fingerprints exhibit smooth, interpolable behaviour with stability to local perturbations, supporting their suitability as general purpose descriptors for microstructure.","We also show that key properties of the microstructures are strongly correlated with position in the latent space, supporting the use of VAE fingerprints for quantitative exploration of process-structure-property relationships."],"url":"http://arxiv.org/abs/2401.11967v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-01-22 14:11:57","title":"Not all Probability Density Functions are Tomograms","abstract":"This paper delves into the significance of the tomographic probability density function (pdf) representation of quantum states, shedding light on the special classes of pdfs that can be tomograms. Instead of using wave functions or density operators on Hilbert spaces, tomograms, which are the true pdfs, are used to completely describe the states of quantum systems. Unlike quasi-pdfs, like the Wigner function, tomograms can be analysed using all the tools of classical probability theory for pdf estimation, which can allow a better quality of state reconstruction. This is particularly useful when dealing with non-Gaussian states where the pdfs are multi-mode. The knowledge of the family of distributions plays an important role in the application of both parametric and non-parametric density estimation methods. We show that not all pdfs can play the role of tomograms of quantum states and introduce the conditions that must be fulfilled by pdfs to be \"quantum\".","sentences":["This paper delves into the significance of the tomographic probability density function (pdf) representation of quantum states, shedding light on the special classes of pdfs that can be tomograms.","Instead of using wave functions or density operators on Hilbert spaces, tomograms, which are the true pdfs, are used to completely describe the states of quantum systems.","Unlike quasi-pdfs, like the Wigner function, tomograms can be analysed using all the tools of classical probability theory for pdf estimation, which can allow a better quality of state reconstruction.","This is particularly useful when dealing with non-Gaussian states where the pdfs are multi-mode.","The knowledge of the family of distributions plays an important role in the application of both parametric and non-parametric density estimation methods.","We show that not all pdfs can play the role of tomograms of quantum states and introduce the conditions that must be fulfilled by pdfs to be \"quantum\"."],"url":"http://arxiv.org/abs/2401.11966v1","category":"quant-ph"}
{"created":"2024-01-22 14:06:37","title":"Bridging Evolutionary Algorithms and Reinforcement Learning: A Comprehensive Survey","abstract":"Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements. By fusing the strengths of both approaches, ERL has emerged as a promising research direction. This survey offers a comprehensive overview of the diverse research branches in ERL. Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL. Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches. We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges. In conclusion, we discuss potential challenges and prospective future research directions across various research directions.","sentences":["Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has demonstrated remarkable performance advancements.","By fusing the strengths of both approaches, ERL has emerged as a promising research direction.","This survey offers a comprehensive overview of the diverse research branches in ERL.","Specifically, we systematically summarize recent advancements in relevant algorithms and identify three primary research directions: EA-assisted optimization of RL, RL-assisted optimization of EA, and synergistic optimization of EA and RL.","Following that, we conduct an in-depth analysis of each research direction, organizing multiple research branches.","We elucidate the problems that each branch aims to tackle and how the integration of EA and RL addresses these challenges.","In conclusion, we discuss potential challenges and prospective future research directions across various research directions."],"url":"http://arxiv.org/abs/2401.11963v1","category":"cs.NE"}
{"created":"2024-01-22 14:03:15","title":"Enhancing Safety in Nonlinear Systems: Design and Stability Analysis of Adaptive Cruise Control","abstract":"The safety of autonomous driving systems, particularly self-driving vehicles, remains of paramount concern. These systems exhibit affine nonlinear dynamics and face the challenge of executing predefined control tasks while adhering to state and input constraints to mitigate risks. However, achieving safety control within the framework of control input constraints, such as collision avoidance and maintaining system states within secure boundaries, presents challenges due to limited options. In this study, we introduce a novel approach to address safety concerns by transforming safety conditions into control constraints with a relative degree of 1. This transformation is facilitated through the design of control barrier functions, enabling the creation of a safety control system for affine nonlinear networks. Subsequently, we formulate a robust control strategy that incorporates safety protocols and conduct a comprehensive analysis of its stability and reliability. To illustrate the effectiveness of our approach, we apply it to a specific problem involving adaptive cruise control. Through simulations, we validate the efficiency of our model in ensuring safety without compromising control performance. Our approach signifies significant progress in the field, providing a practical solution to enhance safety for autonomous driving systems operating within the context of affine nonlinear dynamics.","sentences":["The safety of autonomous driving systems, particularly self-driving vehicles, remains of paramount concern.","These systems exhibit affine nonlinear dynamics","and","face the challenge of executing predefined control tasks while adhering to state and input constraints to mitigate risks.","However, achieving safety control within the framework of control input constraints, such as collision avoidance and maintaining system states within secure boundaries, presents challenges due to limited options.","In this study, we introduce a novel approach to address safety concerns by transforming safety conditions into control constraints with a relative degree of 1.","This transformation is facilitated through the design of control barrier functions, enabling the creation of a safety control system for affine nonlinear networks.","Subsequently, we formulate a robust control strategy that incorporates safety protocols and conduct a comprehensive analysis of its stability and reliability.","To illustrate the effectiveness of our approach, we apply it to a specific problem involving adaptive cruise control.","Through simulations, we validate the efficiency of our model in ensuring safety without compromising control performance.","Our approach signifies significant progress in the field, providing a practical solution to enhance safety for autonomous driving systems operating within the context of affine nonlinear dynamics."],"url":"http://arxiv.org/abs/2401.11961v1","category":"cs.SY"}
{"created":"2024-01-22 14:02:56","title":"Observation-Guided Meteorological Field Downscaling at Station Scale: A Benchmark and a New Method","abstract":"Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting. Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales. However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations. In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field. Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors. Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field. Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables. Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods. We will release the dataset and code subsequently.","sentences":["Downscaling (DS) of meteorological variables involves obtaining high-resolution states from low-resolution meteorological fields and is an important task in weather forecasting.","Previous methods based on deep learning treat downscaling as a super-resolution task in computer vision and utilize high-resolution gridded meteorological fields as supervision to improve resolution at specific grid scales.","However, this approach has struggled to align with the continuous distribution characteristics of meteorological fields, leading to an inherent systematic bias between the downscaled results and the actual observations at meteorological stations.","In this paper, we extend meteorological downscaling to arbitrary scattered station scales, establish a brand new benchmark and dataset, and retrieve meteorological states at any given station location from a coarse-resolution meteorological field.","Inspired by data assimilation techniques, we integrate observational data into the downscaling process, providing multi-scale observational priors.","Building on this foundation, we propose a new downscaling model based on hypernetwork architecture, namely HyperDS, which efficiently integrates different observational information into the model training, achieving continuous scale modeling of the meteorological field.","Through extensive experiments, our proposed method outperforms other specially designed baseline models on multiple surface variables.","Notably, the mean squared error (MSE) for wind speed and surface pressure improved by 67% and 19.5% compared to other methods.","We will release the dataset and code subsequently."],"url":"http://arxiv.org/abs/2401.11960v1","category":"cs.CV"}
{"created":"2024-01-22 13:57:41","title":"Gravitational collapse of matter in the presence of non-minimally coupled Quintessence and Phantom-like scalar fields","abstract":"This paper explores the evolution of the over-dense region of dark matter in the presence of a non-minimally coupled scalar field which is used to model quintessence and phantom-like dark energy. We focus on algebraic coupling, where the interaction Lagrangian is independent of the derivatives of the scalar field. To make our model more relativistic, like the minimal coupling scenario we studied earlier, we consider a spacetime structure that is internally closed Friedmann-Lemaitre-Robertson-Walker (FLRW) spacetime and externally the generalized Vaidya spacetime. This structure allows non-zero matter flux at the boundary of the over-dense region. Our investigation reveals that an increment of the coupling strength causes dark energy to cluster with dark matter at a certain cosmological scale where the influence of dark energy cannot be ignored. This phenomenon arises from the specific nature of the non-minimal coupling considered in this paper. While the evolution of matter's energy density remains unchanged, the scalar field's Klein-Gordon equation is modified, causing dark energy to deviate from its homogeneous state and cluster with dark matter. Similar to minimal coupling scenarios, closed spherical regions do not collapse within certain parameter ranges, exhibiting eternal expansion within the spatially flat FLRW spacetime acting as voids with decreasing matter density. The study extends our understanding of the cosmological scenarios where the virialization of the over-dense regions of dark matter is influenced by the non-minimally coupled dark energy.","sentences":["This paper explores the evolution of the over-dense region of dark matter in the presence of a non-minimally coupled scalar field which is used to model quintessence and phantom-like dark energy.","We focus on algebraic coupling, where the interaction Lagrangian is independent of the derivatives of the scalar field.","To make our model more relativistic, like the minimal coupling scenario we studied earlier, we consider a spacetime structure that is internally closed Friedmann-Lemaitre-Robertson-Walker (FLRW) spacetime and externally the generalized Vaidya spacetime.","This structure allows non-zero matter flux at the boundary of the over-dense region.","Our investigation reveals that an increment of the coupling strength causes dark energy to cluster with dark matter at a certain cosmological scale where the influence of dark energy cannot be ignored.","This phenomenon arises from the specific nature of the non-minimal coupling considered in this paper.","While the evolution of matter's energy density remains unchanged, the scalar field's Klein-Gordon equation is modified, causing dark energy to deviate from its homogeneous state and cluster with dark matter.","Similar to minimal coupling scenarios, closed spherical regions do not collapse within certain parameter ranges, exhibiting eternal expansion within the spatially flat FLRW spacetime acting as voids with decreasing matter density.","The study extends our understanding of the cosmological scenarios where the virialization of the over-dense regions of dark matter is influenced by the non-minimally coupled dark energy."],"url":"http://arxiv.org/abs/2401.11957v1","category":"gr-qc"}
{"created":"2024-01-22 13:54:26","title":"RUMBoost: Gradient Boosted Random Utility Models","abstract":"This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of Random Utility Models (RUMs) with the generalisation and predictive ability of deep learning methods. We obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a RUM with an ensemble of gradient boosted regression trees. This enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables. We introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space. Furthermore, we introduce an optimisation-based smoothing technique that replaces the piece-wise constant utility values of alternative attributes with monotonic piece-wise cubic splines to identify non-linear parameters with defined gradient. We demonstrate the potential of the RUMBoost model compared to various ML and Random Utility benchmark models for revealed preference mode choice data from London. The results highlight the great predictive performance and the direct interpretability of our proposed approach. Furthermore, the smoothed attribute utility functions allow for the calculation of various behavioural indicators and marginal utilities. Finally, we demonstrate the flexibility of our methodology by showing how the RUMBoost model can be extended to complex model specifications, including attribute interactions, correlation within alternative error terms and heterogeneity within the population.","sentences":["This paper introduces the RUMBoost model, a novel discrete choice modelling approach that combines the interpretability and behavioural robustness of Random Utility Models (RUMs) with the generalisation and predictive ability of deep learning methods.","We obtain the full functional form of non-linear utility specifications by replacing each linear parameter in the utility functions of a RUM with an ensemble of gradient boosted regression trees.","This enables piece-wise constant utility values to be imputed for all alternatives directly from the data for any possible combination of input variables.","We introduce additional constraints on the ensembles to ensure three crucial features of the utility specifications: (i) dependency of the utilities of each alternative on only the attributes of that alternative, (ii) monotonicity of marginal utilities, and (iii) an intrinsically interpretable functional form, where the exact response of the model is known throughout the entire input space.","Furthermore, we introduce an optimisation-based smoothing technique that replaces the piece-wise constant utility values of alternative attributes with monotonic piece-wise cubic splines to identify non-linear parameters with defined gradient.","We demonstrate the potential of the RUMBoost model compared to various ML and Random Utility benchmark models for revealed preference mode choice data from London.","The results highlight the great predictive performance and the direct interpretability of our proposed approach.","Furthermore, the smoothed attribute utility functions allow for the calculation of various behavioural indicators and marginal utilities.","Finally, we demonstrate the flexibility of our methodology by showing how the RUMBoost model can be extended to complex model specifications, including attribute interactions, correlation within alternative error terms and heterogeneity within the population."],"url":"http://arxiv.org/abs/2401.11954v1","category":"cs.LG"}
{"created":"2024-01-22 13:54:20","title":"On the steadiness of symmetric solutions to two dimensional dispersive models","abstract":"In this paper, we consider the steadiness of symmetric solutions to two dispersive models in shallow water and hyperelastic mechanics, respectively. These models are derived previously in the two-dimensional setting and can be viewed as the generalization of the Camassa-Holm and Kadomtsev-Petviashvili equations. For these two models, we prove that symmetry of classical solutions implies steadiness in the horizontal direction. We also confirm the such connection between symmetry and steadiness in weak formulation, which includes in particular the peaked solutions.","sentences":["In this paper, we consider the steadiness of symmetric solutions to two dispersive models in shallow water and hyperelastic mechanics, respectively.","These models are derived previously in the two-dimensional setting and can be viewed as the generalization of the Camassa-Holm and Kadomtsev-Petviashvili equations.","For these two models, we prove that symmetry of classical solutions implies steadiness in the horizontal direction.","We also confirm the such connection between symmetry and steadiness in weak formulation, which includes in particular the peaked solutions."],"url":"http://arxiv.org/abs/2401.11953v1","category":"math.AP"}
{"created":"2024-01-22 13:38:24","title":"Feature Denoising Diffusion Model for Blind Image Quality Assessment","abstract":"Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks. Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning. However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features. In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features. Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model. (ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions. Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC).","sentences":["Blind Image Quality Assessment (BIQA) aims to evaluate image quality in line with human perception, without reference benchmarks.","Currently, deep learning BIQA methods typically depend on using features from high-level tasks for transfer learning.","However, the inherent differences between BIQA and these high-level tasks inevitably introduce noise into the quality-aware features.","In this paper, we take an initial step towards exploring the diffusion model for feature denoising in BIQA, namely Perceptual Feature Diffusion for IQA (PFD-IQA), which aims to remove noise from quality-aware features.","Specifically, (i) We propose a {Perceptual Prior Discovery and Aggregation module to establish two auxiliary tasks to discover potential low-level features in images that are used to aggregate perceptual text conditions for the diffusion model.","(ii) We propose a Perceptual Prior-based Feature Refinement strategy, which matches noisy features to predefined denoising trajectories and then performs exact feature denoising based on text conditions.","Extensive experiments on eight standard BIQA datasets demonstrate the superior performance to the state-of-the-art BIQA methods, i.e., achieving the PLCC values of 0.935 ( vs. 0.905 in KADID) and 0.922 ( vs. 0.894 in LIVEC)."],"url":"http://arxiv.org/abs/2401.11949v1","category":"cs.CV"}
{"created":"2024-01-22 13:35:27","title":"A Dynamic YOLO-Based Sequence-Matching Model for Efficient Coverless Image Steganography","abstract":"Many existing coverless steganography methods establish a mapping relationship between cover images and hidden data. There exists an issue that the number of images stored in the database grows exponentially as the steganographic capacity rises. The need for a high steganographic capacity makes it challenging to build an image database. To improve the image library utilization and anti-attack capability of the steganography system, we present an efficient coverless scheme based on dynamically matched substrings. YOLO is employed for selecting optimal objects, and a mapping dictionary is established between these objects and scrambling factors. With the aid of this dictionary, each image is effectively assigned to a specific scrambling factor, which is used to scramble the receiver's sequence key. To achieve sufficient steganography capability based on a limited image library, all substrings of the scrambled sequences hold the potential to hide data. After completing the secret information matching, the ideal number of stego images will be obtained from the database. According to experimental results, this technology outperforms most previous works on data load, transmission security, and hiding capacity. Under typical geometric attacks, it can recover 79.85\\% of secret information on average. Furthermore, only approximately 200 random images are needed to meet a capacity of 19 bits per image.","sentences":["Many existing coverless steganography methods establish a mapping relationship between cover images and hidden data.","There exists an issue that the number of images stored in the database grows exponentially as the steganographic capacity rises.","The need for a high steganographic capacity makes it challenging to build an image database.","To improve the image library utilization and anti-attack capability of the steganography system, we present an efficient coverless scheme based on dynamically matched substrings.","YOLO is employed for selecting optimal objects, and a mapping dictionary is established between these objects and scrambling factors.","With the aid of this dictionary, each image is effectively assigned to a specific scrambling factor, which is used to scramble the receiver's sequence key.","To achieve sufficient steganography capability based on a limited image library, all substrings of the scrambled sequences hold the potential to hide data.","After completing the secret information matching, the ideal number of stego images will be obtained from the database.","According to experimental results, this technology outperforms most previous works on data load, transmission security, and hiding capacity.","Under typical geometric attacks, it can recover 79.85\\% of secret information on average.","Furthermore, only approximately 200 random images are needed to meet a capacity of 19 bits per image."],"url":"http://arxiv.org/abs/2401.11946v1","category":"cs.CR"}
{"created":"2024-01-22 13:35:00","title":"The Effect of Predictive Formal Modelling at Runtime on Performance in Human-Swarm Interaction","abstract":"Formal Modelling is often used as part of the design and testing process of software development to ensure that components operate within suitable bounds even in unexpected circumstances. In this paper, we use predictive formal modelling (PFM) at runtime in a human-swarm mission and show that this integration can be used to improve the performance of human-swarm teams. We recruited 60 participants to operate a simulated aerial swarm to deliver parcels to target locations. In the PFM condition, operators were informed of the estimated completion times given the number of drones deployed, whereas in the No-PFM condition, operators did not have this information. The operators could control the mission by adding or removing drones from the mission and thereby, increasing or decreasing the overall mission cost. The evaluation of human-swarm performance relied on four key metrics: the time taken to complete tasks, the number of agents involved, the total number of tasks accomplished, and the overall cost associated with the human-swarm task. Our results show that PFM modelling at runtime improves mission performance without significantly affecting the operator's workload or the system's usability.","sentences":["Formal Modelling is often used as part of the design and testing process of software development to ensure that components operate within suitable bounds even in unexpected circumstances.","In this paper, we use predictive formal modelling (PFM) at runtime in a human-swarm mission and show that this integration can be used to improve the performance of human-swarm teams.","We recruited 60 participants to operate a simulated aerial swarm to deliver parcels to target locations.","In the PFM condition, operators were informed of the estimated completion times given the number of drones deployed, whereas in the No-PFM condition, operators did not have this information.","The operators could control the mission by adding or removing drones from the mission and thereby, increasing or decreasing the overall mission cost.","The evaluation of human-swarm performance relied on four key metrics: the time taken to complete tasks, the number of agents involved, the total number of tasks accomplished, and the overall cost associated with the human-swarm task.","Our results show that PFM modelling at runtime improves mission performance without significantly affecting the operator's workload or the system's usability."],"url":"http://arxiv.org/abs/2401.11945v1","category":"cs.RO"}
{"created":"2024-01-22 13:34:34","title":"CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding Benchmark","abstract":"As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need. Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context. CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU. These questions span 30 subjects and comprise 39 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.   CMMMU focuses on complex perception and reasoning with domain-specific knowledge in the Chinese context. We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement. CMMMU will boost the community to build the next-generation LMMs towards expert artificial intelligence and promote the democratization of LMMs by providing diverse language contexts.","sentences":["As the capabilities of large multimodal models (LMMs) continue to advance, evaluating the performance of LMMs emerges as an increasing need.","Additionally, there is an even larger gap in evaluating the advanced knowledge and reasoning abilities of LMMs in non-English contexts such as Chinese.","We introduce CMMMU, a new Chinese Massive Multi-discipline Multimodal Understanding benchmark designed to evaluate LMMs on tasks demanding college-level subject knowledge and deliberate reasoning in a Chinese context.","CMMMU is inspired by and strictly follows the annotation and analysis pattern of MMMU.   ","CMMMU includes 12k manually collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines:","Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering, like its companion, MMMU.","These questions span 30 subjects and comprise 39 highly heterogeneous image types, such as charts, diagrams, maps, tables, music sheets, and chemical structures.   ","CMMMU focuses on complex perception and reasoning with domain-specific knowledge in the Chinese context.","We evaluate 11 open-source LLMs and one proprietary GPT-4V(ision).","Even GPT-4V only achieves accuracies of 42%, indicating a large space for improvement.","CMMMU will boost the community to build the next-generation LMMs towards expert artificial intelligence and promote the democratization of LMMs by providing diverse language contexts."],"url":"http://arxiv.org/abs/2401.11944v1","category":"cs.CL"}
{"created":"2024-01-22 13:33:53","title":"Benchmarking Large Multimodal Models against Common Corruptions","abstract":"This technical report aims to fill a deficiency in the assessment of large multimodal models (LMMs) by specifically examining the self-consistency of their outputs when subjected to common corruptions. We investigate the cross-modal interactions between text, image, and speech, encompassing four essential generation tasks: text-to-image, image-to-text, text-to-speech, and speech-to-text. We create a comprehensive benchmark, named MMCBench, that covers more than 100 popular LMMs (totally over 150 model checkpoints). A thorough evaluation under common corruptions is critical for practical deployment and facilitates a better understanding of the reliability of cutting-edge LMMs. The benchmarking code is available at https://github.com/sail-sg/MMCBench","sentences":["This technical report aims to fill a deficiency in the assessment of large multimodal models (LMMs) by specifically examining the self-consistency of their outputs when subjected to common corruptions.","We investigate the cross-modal interactions between text, image, and speech, encompassing four essential generation tasks: text-to-image, image-to-text, text-to-speech, and speech-to-text.","We create a comprehensive benchmark, named MMCBench, that covers more than 100 popular LMMs (totally over 150 model checkpoints).","A thorough evaluation under common corruptions is critical for practical deployment and facilitates a better understanding of the reliability of cutting-edge LMMs.","The benchmarking code is available at https://github.com/sail-sg/MMCBench"],"url":"http://arxiv.org/abs/2401.11943v1","category":"cs.LG"}
{"created":"2024-01-22 13:30:11","title":"Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent","abstract":"This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank. Even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance. A series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error.","sentences":["This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements.","Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors.","Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method.","Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD).","This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements.","We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations.","Additionally, it is worth noting that our method does not require the precise estimation of the tensor tubal-rank.","Even in cases where the tubal-rank is slightly overestimated, our approach continues to demonstrate robust performance.","A series of experiments have been carried out to demonstrate that, as compared to other popular ones, our approach exhibits superior performance in multiple scenarios, in terms of the faster computational speed and the smaller convergence error."],"url":"http://arxiv.org/abs/2401.11940v1","category":"cs.LG"}
{"created":"2024-01-22 13:28:04","title":"The Definition of a Photon Surface in an Invariant Spin Frame","abstract":"This paper defines the photon surface conditions using Cartan scalars within an invariant spin frame, offering a comprehensive description of the local spacetime geometry. By employing this approach, we gain novel insights into the geometry and dynamics of photon surfaces, independent of the global spacetime structure. We first discuss the photon surface conditions in a Petrov type-D spacetime manifold, and then we simplify those conditions assuming the existence of spherical symmetry. Finally, employing the simplified, spherically symmetric photon surface conditions, we explore the dynamics of photon surfaces in static, collapsing Lemaitre-Tolman-Bondi (LTB) spacetimes, and Vaidya spacetimes. Notably, we show that photon surfaces can emerge from the central singularity during the collapse of an inhomogeneous dust cloud modeled by a LTB spacetime. This underscores the significance of our findings in comprehending the potential observational implications of the physics near the ultra-high gravity region.","sentences":["This paper defines the photon surface conditions using Cartan scalars within an invariant spin frame, offering a comprehensive description of the local spacetime geometry.","By employing this approach, we gain novel insights into the geometry and dynamics of photon surfaces, independent of the global spacetime structure.","We first discuss the photon surface conditions in a Petrov type-D spacetime manifold, and then we simplify those conditions assuming the existence of spherical symmetry.","Finally, employing the simplified, spherically symmetric photon surface conditions, we explore the dynamics of photon surfaces in static, collapsing Lemaitre-Tolman-Bondi (LTB) spacetimes, and Vaidya spacetimes.","Notably, we show that photon surfaces can emerge from the central singularity during the collapse of an inhomogeneous dust cloud modeled by a LTB spacetime.","This underscores the significance of our findings in comprehending the potential observational implications of the physics near the ultra-high gravity region."],"url":"http://arxiv.org/abs/2401.11936v1","category":"gr-qc"}
{"created":"2024-01-22 13:25:08","title":"Systematic Performance Evaluation Framework for LEO Mega-Constellation Satellite Networks","abstract":"Low Earth orbit (LEO) mega-constellation satellite networks have shown great potential to extend the coverage capability of conventional terrestrial networks. How to systematically define, quantify, and assess the technical performance of LEO mega-constellation satellite networks remains an open issue. In this paper, we propose a comprehensive key performance indicator (KPI) framework for mega-constellation based LEO satellite networks. An efficient LEO constellation oriented performance evaluation methodology is then carefully designed by resorting to the concept of interfering area and spherical geographic cell. We have carried out rigorous system-level simulations and provided numerical results to assess the KPI framework. It can be observed that the achieved area traffic capacity of the reference LEO constellation is around 4 Kbps/km2, with service availability ranging from 0.36 to 0.39. Besides, the average access success probability and handover failure rate is approximate to 96% and 10%, respectively, in the nearest satellite association scheme.","sentences":["Low Earth orbit (LEO) mega-constellation satellite networks have shown great potential to extend the coverage capability of conventional terrestrial networks.","How to systematically define, quantify, and assess the technical performance of LEO mega-constellation satellite networks remains an open issue.","In this paper, we propose a comprehensive key performance indicator (KPI) framework for mega-constellation based LEO satellite networks.","An efficient LEO constellation oriented performance evaluation methodology is then carefully designed by resorting to the concept of interfering area and spherical geographic cell.","We have carried out rigorous system-level simulations and provided numerical results to assess the KPI framework.","It can be observed that the achieved area traffic capacity of the reference LEO constellation is around 4 Kbps/km2, with service availability ranging from 0.36 to 0.39.","Besides, the average access success probability and handover failure rate is approximate to 96% and 10%, respectively, in the nearest satellite association scheme."],"url":"http://arxiv.org/abs/2401.11934v1","category":"cs.PF"}
{"created":"2024-01-22 13:24:25","title":"Large deviation full counting statistics in adiabatic open quantum dynamics","abstract":"The state of an open quantum system undergoing an adiabatic process evolves by following the instantaneous stationary state of its time-dependent generator. This observation allows one to characterize, for a generic adiabatic evolution, the average dynamics of the open system. However, information about fluctuations of dynamical observables, such as the number of photons emitted or the time-integrated stochastic entropy production in single experimental runs, requires controlling the whole spectrum of the generator and not only the stationary state. Here, we show how such information can be obtained in adiabatic open quantum dynamics by exploiting tools from large deviation theory. We prove an adiabatic theorem for deformed generators, which allows us to encode, in a biased quantum state, the full counting statistics of generic time-integrated dynamical observables. We further compute the probability associated with an arbitrary \"rare\" time-history of the observable and derive a dynamics which realizes it in its typical behavior. Our results provide a way to characterize and engineer adiabatic open quantum dynamics and to control their fluctuations.","sentences":["The state of an open quantum system undergoing an adiabatic process evolves by following the instantaneous stationary state of its time-dependent generator.","This observation allows one to characterize, for a generic adiabatic evolution, the average dynamics of the open system.","However, information about fluctuations of dynamical observables, such as the number of photons emitted or the time-integrated stochastic entropy production in single experimental runs, requires controlling the whole spectrum of the generator and not only the stationary state.","Here, we show how such information can be obtained in adiabatic open quantum dynamics by exploiting tools from large deviation theory.","We prove an adiabatic theorem for deformed generators, which allows us to encode, in a biased quantum state, the full counting statistics of generic time-integrated dynamical observables.","We further compute the probability associated with an arbitrary \"rare\" time-history of the observable and derive a dynamics which realizes it in its typical behavior.","Our results provide a way to characterize and engineer adiabatic open quantum dynamics and to control their fluctuations."],"url":"http://arxiv.org/abs/2401.11933v1","category":"cond-mat.stat-mech"}
{"created":"2024-01-22 13:23:38","title":"Accelerating Causal Algorithms for Industrial-scale Data: A Distributed Computing Approach with Ray Framework","abstract":"The increasing need for causal analysis in large-scale industrial datasets necessitates the development of efficient and scalable causal algorithms for real-world applications. This paper addresses the challenge of scaling causal algorithms in the context of conducting causal analysis on extensive datasets commonly encountered in industrial settings. Our proposed solution involves enhancing the scalability of causal algorithm libraries, such as EconML, by leveraging the parallelism capabilities offered by the distributed computing framework Ray. We explore the potential of parallelizing key iterative steps within causal algorithms to significantly reduce overall runtime, supported by a case study that examines the impact on estimation times and costs. Through this approach, we aim to provide a more effective solution for implementing causal analysis in large-scale industrial applications.","sentences":["The increasing need for causal analysis in large-scale industrial datasets necessitates the development of efficient and scalable causal algorithms for real-world applications.","This paper addresses the challenge of scaling causal algorithms in the context of conducting causal analysis on extensive datasets commonly encountered in industrial settings.","Our proposed solution involves enhancing the scalability of causal algorithm libraries, such as EconML, by leveraging the parallelism capabilities offered by the distributed computing framework Ray.","We explore the potential of parallelizing key iterative steps within causal algorithms to significantly reduce overall runtime, supported by a case study that examines the impact on estimation times and costs.","Through this approach, we aim to provide a more effective solution for implementing causal analysis in large-scale industrial applications."],"url":"http://arxiv.org/abs/2401.11932v1","category":"cs.DC"}
{"created":"2024-01-22 13:15:40","title":"The Bigger the Better? Rethinking the Effective Model Scale in Long-term Time Series Forecasting","abstract":"Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches. While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity. These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms. This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity. Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data. Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with hierarchical decomposition. This novel architecture not only inverts the prevailing trend toward model expansion but also accomplishes precise forecasting with drastically fewer computations and parameters. Remarkably, HDformer outperforms existing state-of-the-art LTSF models, while requiring over 99\\% fewer parameters. Through this work, we advocate a paradigm shift in LTSF, emphasizing the importance to tailor the model to the inherent dynamics of time series data-a timely reminder that in the realm of LTSF, bigger is not invariably better.","sentences":["Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, distinguished by its focus on extensive input sequences, in contrast to the constrained lengths typical of traditional approaches.","While longer sequences inherently convey richer information, potentially enhancing predictive precision, prevailing techniques often respond by escalating model complexity.","These intricate models can inflate into millions of parameters, incorporating parameter-intensive elements like positional encodings, feed-forward networks and self-attention mechanisms.","This complexity, however, leads to prohibitive model scale, particularly given the time series data's semantic simplicity.","Motivated by the pursuit of parsimony, our research employs conditional correlation and auto-correlation as investigative tools, revealing significant redundancies within the input data.","Leveraging these insights, we introduce the HDformer, a lightweight Transformer variant enhanced with hierarchical decomposition.","This novel architecture not only inverts the prevailing trend toward model expansion but also accomplishes precise forecasting with drastically fewer computations and parameters.","Remarkably, HDformer outperforms existing state-of-the-art LTSF models, while requiring over 99\\% fewer parameters.","Through this work, we advocate a paradigm shift in LTSF, emphasizing the importance to tailor the model to the inherent dynamics of time series data-a timely reminder that in the realm of LTSF, bigger is not invariably better."],"url":"http://arxiv.org/abs/2401.11929v1","category":"cs.LG"}
{"created":"2024-01-22 13:12:46","title":"Large deviations of the empirical spectral measure of supercritical sparse Wigner matrices","abstract":"Let $\\Xi$ be the adjacency matrix of an Erd\\H{o}s-R\\'enyi graph on $n$ vertices and with parameter $p$ and consider $A$ a $n\\times n$ centered random symmetric matrix with bounded i.i.d. entries above the diagonal. When the mean degree $np$ diverges, the empirical spectral measure of the normalized Hadamard product $(A \\circ \\Xi)/\\sqrt{np}$ converges weakly in probability to the semicircle law. In the regime where $p\\ll 1$ and $ np \\gg \\log n$, we prove a large deviations principle for the empirical spectral measure with speed $n^2p$ and with a good rate function solution of a certain variational problem. The rate function reveals in particular that the only possible deviations at the exponential scale $n^2p$ are around measures coming from Quadratic Vector Equations. As a byproduct, we obtain a large deviations principle for the empirical spectral measure of supercritical Erd\\H{o}s-R\\'enyi graphs.","sentences":["Let $\\Xi$ be the adjacency matrix of an Erd\\H{o}s-R\\'enyi graph on $n$ vertices and with parameter $p$ and consider $A$ a $n\\times n$ centered random symmetric matrix with bounded i.i.d. entries above the diagonal.","When the mean degree $np$ diverges, the empirical spectral measure of the normalized Hadamard product $(A \\circ \\Xi)/\\sqrt{np}$ converges weakly in probability to the semicircle law.","In the regime where $p\\ll 1$ and $ np \\gg \\log n$, we prove a large deviations principle for the empirical spectral measure with speed $n^2p$ and with a good rate function solution of a certain variational problem.","The rate function reveals in particular that the only possible deviations at the exponential scale $n^2p$ are around measures coming from Quadratic Vector Equations.","As a byproduct, we obtain a large deviations principle for the empirical spectral measure of supercritical Erd\\H{o}s-R\\'enyi graphs."],"url":"http://arxiv.org/abs/2401.11925v1","category":"math.PR"}
{"created":"2024-01-22 13:10:23","title":"VirtuWander: Enhancing Multi-modal Interaction for Virtual Tour Guidance through Large Language Models","abstract":"Tour guidance in virtual museums encourages multi-modal interactions to boost user experiences, concerning engagement, immersion, and spatial awareness. Nevertheless, achieving the goal is challenging due to the complexity of comprehending diverse user needs and accommodating personalized user preferences. Informed by a formative study that characterizes guidance-seeking contexts, we establish a multi-modal interaction design framework for virtual tour guidance. We then design VirtuWander, a two-stage innovative system using domain-oriented large language models to transform user inquiries into diverse guidance-seeking contexts and facilitate multi-modal interactions. The feasibility and versatility of VirtuWander are demonstrated with virtual guiding examples that encompass various touring scenarios and cater to personalized preferences. We further evaluate VirtuWander through a user study within an immersive simulated museum. The results suggest that our system enhances engaging virtual tour experiences through personalized communication and knowledgeable assistance, indicating its potential for expanding into real-world scenarios.","sentences":["Tour guidance in virtual museums encourages multi-modal interactions to boost user experiences, concerning engagement, immersion, and spatial awareness.","Nevertheless, achieving the goal is challenging due to the complexity of comprehending diverse user needs and accommodating personalized user preferences.","Informed by a formative study that characterizes guidance-seeking contexts, we establish a multi-modal interaction design framework for virtual tour guidance.","We then design VirtuWander, a two-stage innovative system using domain-oriented large language models to transform user inquiries into diverse guidance-seeking contexts and facilitate multi-modal interactions.","The feasibility and versatility of VirtuWander are demonstrated with virtual guiding examples that encompass various touring scenarios and cater to personalized preferences.","We further evaluate VirtuWander through a user study within an immersive simulated museum.","The results suggest that our system enhances engaging virtual tour experiences through personalized communication and knowledgeable assistance, indicating its potential for expanding into real-world scenarios."],"url":"http://arxiv.org/abs/2401.11923v1","category":"cs.HC"}
{"created":"2024-01-22 13:10:20","title":"Universal scaling laws of quantum spatial search in complex networks","abstract":"Since quantum spatial searches on complex networks have a strong network dependence, the question arises whether the universal perspective exists in this quantum algorithm for complex networks. Here, we uncover the universal scaling laws of the quantum spatial search on complex networks such as small-world and scale-free networks. The average path length, a key quantity in the complex network science, is useful to expose this universal feature, where the collapse plot can be generated for the optimal time, the maximal finding probability and the optimal hopping parameter. Based on the path integral method, we also clarify that the probability amplitude in the continuous-time quantum walk can be determined by the path length distribution. Our results demonstrate a new link between the quantum physics and the complex networks.","sentences":["Since quantum spatial searches on complex networks have a strong network dependence, the question arises whether the universal perspective exists in this quantum algorithm for complex networks.","Here, we uncover the universal scaling laws of the quantum spatial search on complex networks such as small-world and scale-free networks.","The average path length, a key quantity in the complex network science, is useful to expose this universal feature, where the collapse plot can be generated for the optimal time, the maximal finding probability and the optimal hopping parameter.","Based on the path integral method, we also clarify that the probability amplitude in the continuous-time quantum walk can be determined by the path length distribution.","Our results demonstrate a new link between the quantum physics and the complex networks."],"url":"http://arxiv.org/abs/2401.11922v1","category":"quant-ph"}
{"created":"2024-01-22 13:09:30","title":"Maximizing Spectral and Energy Efficiency in Multi-user MIMO OFDM Systems with RIS and Hardware Impairment","abstract":"An emerging technology to enhance the spectral efficiency (SE) and energy efficiency (EE) of wireless communication systems is reconfigurable intelligent surface (RIS), which is shown to be very powerful in single-carrier systems. However, in multi-user orthogonal frequency division multiplexing (OFDM) systems, RIS may not be as promising as in single-carrier systems since an independent optimization of RIS elements at each sub-carrier is impossible in multi-carrier systems. Thus, this paper investigates the performance of various RIS technologies like regular (reflective and passive), simultaneously transmit and reflect (STAR), and multi-sector beyond diagonal (BD) RIS in multi-user multiple-input multiple-output (MIMO) OFDM broadcast channels (BC). This requires to formulate and solve a joint MIMO precoding and RIS optimization problem. The obtained solution reveals that RIS can significantly improve the system performance even when the number of RIS elements is relatively low. Moreover, we develop resource allocation schemes for STAR-RIS and multi-sector BD-RIS in MIMO OFDM BCs, and show that these RIS technologies can outperform a regular RIS, especially when the regular RIS cannot assist the communications for all the users.","sentences":["An emerging technology to enhance the spectral efficiency (SE) and energy efficiency (EE) of wireless communication systems is reconfigurable intelligent surface (RIS), which is shown to be very powerful in single-carrier systems.","However, in multi-user orthogonal frequency division multiplexing (OFDM) systems, RIS may not be as promising as in single-carrier systems since an independent optimization of RIS elements at each sub-carrier is impossible in multi-carrier systems.","Thus, this paper investigates the performance of various RIS technologies like regular (reflective and passive), simultaneously transmit and reflect (STAR), and multi-sector beyond diagonal (BD) RIS in multi-user multiple-input multiple-output (MIMO) OFDM broadcast channels (BC).","This requires to formulate and solve a joint MIMO precoding and RIS optimization problem.","The obtained solution reveals that RIS can significantly improve the system performance even when the number of RIS elements is relatively low.","Moreover, we develop resource allocation schemes for STAR-RIS and multi-sector BD-RIS in MIMO OFDM BCs, and show that these RIS technologies can outperform a regular RIS, especially when the regular RIS cannot assist the communications for all the users."],"url":"http://arxiv.org/abs/2401.11921v1","category":"cs.IT"}
{"created":"2024-01-22 13:01:49","title":"Secure Multi-hop Telemetry Broadcasts for UAV Swarm Communication","abstract":"Unmanned Aerial Vehicles (UAVs) are evolving as adaptable platforms for a wide range of applications such as precise inspections, emergency response, and remote sensing. Autonomous UAV swarms require efficient and stable communication during deployment for a successful mission execution. For instance, the periodic exchange of telemetry data between all swarm members provides the foundation for formation flight and collision avoidance. However, due to the mobility of the vehicles and instability of wireless transmissions, maintaining a secure and reliable all-to-all communication remains challenging. This paper investigates encrypted and authenticated multi-hop broadcast communication based on the transmission of custom IEEE 802.11 Wi-Fi data frames.","sentences":["Unmanned Aerial Vehicles (UAVs) are evolving as adaptable platforms for a wide range of applications such as precise inspections, emergency response, and remote sensing.","Autonomous UAV swarms require efficient and stable communication during deployment for a successful mission execution.","For instance, the periodic exchange of telemetry data between all swarm members provides the foundation for formation flight and collision avoidance.","However, due to the mobility of the vehicles and instability of wireless transmissions, maintaining a secure and reliable all-to-all communication remains challenging.","This paper investigates encrypted and authenticated multi-hop broadcast communication based on the transmission of custom IEEE 802.11 Wi-Fi data frames."],"url":"http://arxiv.org/abs/2401.11915v1","category":"cs.CR"}
{"created":"2024-01-22 13:01:35","title":"A Saliency Enhanced Feature Fusion based multiscale RGB-D Salient Object Detection Network","abstract":"Multiscale convolutional neural network (CNN) has demonstrated remarkable capabilities in solving various vision problems. However, fusing features of different scales alwaysresults in large model sizes, impeding the application of multiscale CNNs in RGB-D saliency detection. In this paper, we propose a customized feature fusion module, called Saliency Enhanced Feature Fusion (SEFF), for RGB-D saliency detection. SEFF utilizes saliency maps of the neighboring scales to enhance the necessary features for fusing, resulting in more representative fused features. Our multiscale RGB-D saliency detector uses SEFF and processes images with three different scales. SEFF is used to fuse the features of RGB and depth images, as well as the features of decoders at different scales. Extensive experiments on five benchmark datasets have demonstrated the superiority of our method over ten SOTA saliency detectors.","sentences":["Multiscale convolutional neural network (CNN) has demonstrated remarkable capabilities in solving various vision problems.","However, fusing features of different scales alwaysresults in large model sizes, impeding the application of multiscale CNNs in RGB-D saliency detection.","In this paper, we propose a customized feature fusion module, called Saliency Enhanced Feature Fusion (SEFF), for RGB-D saliency detection.","SEFF utilizes saliency maps of the neighboring scales to enhance the necessary features for fusing, resulting in more representative fused features.","Our multiscale RGB-D saliency detector uses SEFF and processes images with three different scales.","SEFF is used to fuse the features of RGB and depth images, as well as the features of decoders at different scales.","Extensive experiments on five benchmark datasets have demonstrated the superiority of our method over ten SOTA saliency detectors."],"url":"http://arxiv.org/abs/2401.11914v1","category":"cs.CV"}
