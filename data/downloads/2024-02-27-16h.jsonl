{"created":"2024-02-26 13:39:04","title":"Q-FOX Learning: Breaking Tradition in Reinforcement Learning","abstract":"Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (steps). Q-FOX has been evaluated on two OpenAI Gym environment control tasks: Cart Pole and Frozen Lake. It exposed greater cumulative rewards than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly selected HP. The cumulative reward for the Cart Pole task was 32.08, and for the Frozen Lake task was 0.95. Despite the robustness of Q-FOX, it has limitations. It cannot be used directly in real-word problems before choosing the HP in a simulation environment because its processes work iteratively, making it time-consuming. The results indicate that Q-FOX has played an essential role in HP tuning for RL algorithms to effectively solve different control tasks.","sentences":["Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision.","Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms.","Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards.","In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed.","This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning.","Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (steps).","Q-FOX has been evaluated on two OpenAI Gym environment control tasks: Cart Pole and Frozen Lake.","It exposed greater cumulative rewards than HP tuning with other optimizers, such as PSO, GA, Bee, or randomly selected HP.","The cumulative reward for the Cart Pole task was 32.08, and for the Frozen Lake task was 0.95.","Despite the robustness of Q-FOX, it has limitations.","It cannot be used directly in real-word problems before choosing the HP in a simulation environment because its processes work iteratively, making it time-consuming.","The results indicate that Q-FOX has played an essential role in HP tuning for RL algorithms to effectively solve different control tasks."],"url":"http://arxiv.org/abs/2402.16562v1","category":"cs.LG"}
{"created":"2024-02-26 13:37:46","title":"Data depth functions for non-standard data by use of formal concept analysis","abstract":"Data depth functions have been intensively studied for normed vector spaces. However, a discussion on depth functions on data where one specific data structure cannot be presupposed is lacking. In this article, we introduce a notion of depth functions for data types that are not given in statistical standard data formats and therefore we do not have one specific data structure. We call such data in general non-standard data. To achieve this, we represent the data via formal concept analysis which leads to a unified data representation. Besides introducing depth functions for non-standard data using formal concept analysis, we give a systematic basis by introducing structural properties. Furthermore, we embed the generalised Tukey depth into our concept of data depth and analyse it using the introduced structural properties. Thus, this article provides the mathematical formalisation of centrality and outlyingness for non-standard data and therefore increases the spaces centrality is currently discussed. In particular, it gives a basis to define further depth functions and statistical inference methods for non-standard data.","sentences":["Data depth functions have been intensively studied for normed vector spaces.","However, a discussion on depth functions on data where one specific data structure cannot be presupposed is lacking.","In this article, we introduce a notion of depth functions for data types that are not given in statistical standard data formats and therefore we do not have one specific data structure.","We call such data in general non-standard data.","To achieve this, we represent the data via formal concept analysis which leads to a unified data representation.","Besides introducing depth functions for non-standard data using formal concept analysis, we give a systematic basis by introducing structural properties.","Furthermore, we embed the generalised Tukey depth into our concept of data depth and analyse it using the introduced structural properties.","Thus, this article provides the mathematical formalisation of centrality and outlyingness for non-standard data and therefore increases the spaces centrality is currently discussed.","In particular, it gives a basis to define further depth functions and statistical inference methods for non-standard data."],"url":"http://arxiv.org/abs/2402.16560v1","category":"math.ST"}
{"created":"2024-02-26 13:32:38","title":"A randomized algorithm for simultaneously diagonalizing symmetric matrices by congruence","abstract":"A family of symmetric matrices $A_1,\\ldots, A_d$ is SDC (simultaneous diagonalization by congruence) if there is an invertible matrix $X$ such that every $X^T A_k X$ is diagonal. In this work, a novel randomized SDC (RSDC) algorithm is proposed that reduces SDC to a generalized eigenvalue problem by considering two (random) linear combinations of the family. We establish exact recovery: RSDC achieves diagonalization with probability $1$ if the family is exactly SDC. Under a mild regularity assumption, robust recovery is also established: Given a family that is $\\epsilon$-close to SDC then RSDC diagonalizes, with high probability, the family up to an error of norm $\\mathcal{O}(\\epsilon)$. Under a positive definiteness assumption, which often holds in applications, stronger results are established, including a bound on the condition number of the transformation matrix. For practical use, we suggest to combine RSDC with an optimization algorithm. The performance of the resulting method is verified for synthetic data, image separation and EEG analysis tasks. It turns out that our newly developed method outperforms existing optimization-based methods in terms of efficiency while achieving a comparable level of accuracy.","sentences":["A family of symmetric matrices $A_1,\\ldots, A_d$ is SDC (simultaneous diagonalization by congruence) if there is an invertible matrix $X$ such that every $X^T A_k X$ is diagonal.","In this work, a novel randomized SDC (RSDC) algorithm is proposed that reduces SDC to a generalized eigenvalue problem by considering two (random) linear combinations of the family.","We establish exact recovery: RSDC achieves diagonalization with probability $1$ if the family is exactly SDC.","Under a mild regularity assumption, robust recovery is also established: Given a family that is $\\epsilon$-close to SDC then RSDC diagonalizes, with high probability, the family up to an error of norm $\\mathcal{O}(\\epsilon)$. Under a positive definiteness assumption, which often holds in applications, stronger results are established, including a bound on the condition number of the transformation matrix.","For practical use, we suggest to combine RSDC with an optimization algorithm.","The performance of the resulting method is verified for synthetic data, image separation and EEG analysis tasks.","It turns out that our newly developed method outperforms existing optimization-based methods in terms of efficiency while achieving a comparable level of accuracy."],"url":"http://arxiv.org/abs/2402.16557v1","category":"math.NA"}
{"created":"2024-02-26 13:26:34","title":"Contracts with Inspections","abstract":"In the classical principal-agent hidden-action model, a principal delegates the execution of a costly task to an agent for which he can choose among actions with different costs and different success probabilities to accomplish the task. To incentivize the agent to exert effort, the principal can commit to a contract, which is the amount of payment based on the task's success. A crucial assumption of this model is that the principal can only base the payment on the outcome but not on the agent's chosen action.   In this work, we relax the hidden-action assumption and introduce a new model where the principal is allowed to inspect subsets of actions at some cost that depends on the inspected subset. If the principal discovers that the agent did not select the agreed-upon action through the inspection, the principal can withhold payment. This relaxation of the model introduces a broader strategy space for the principal, who now faces a tradeoff between positive incentives (increasing payment) and negative incentives (increasing inspection).   We show how to find the best deterministic incentive-compatible inspection scheme for all monotone inspection cost functions. We then turn to randomized inspection schemes and show that one can efficiently find the best randomized incentive-compatible inspection scheme when the inspection cost function is submodular. We complement this result by showing that it is impossible to efficiently find the optimal randomized inspection scheme for the more general case of XOS inspection cost functions.","sentences":["In the classical principal-agent hidden-action model, a principal delegates the execution of a costly task to an agent for which he can choose among actions with different costs and different success probabilities to accomplish the task.","To incentivize the agent to exert effort, the principal can commit to a contract, which is the amount of payment based on the task's success.","A crucial assumption of this model is that the principal can only base the payment on the outcome but not on the agent's chosen action.   ","In this work, we relax the hidden-action assumption and introduce a new model where the principal is allowed to inspect subsets of actions at some cost that depends on the inspected subset.","If the principal discovers that the agent did not select the agreed-upon action through the inspection, the principal can withhold payment.","This relaxation of the model introduces a broader strategy space for the principal, who now faces a tradeoff between positive incentives (increasing payment) and negative incentives (increasing inspection).   ","We show how to find the best deterministic incentive-compatible inspection scheme for all monotone inspection cost functions.","We then turn to randomized inspection schemes and show that one can efficiently find the best randomized incentive-compatible inspection scheme when the inspection cost function is submodular.","We complement this result by showing that it is impossible to efficiently find the optimal randomized inspection scheme for the more general case of XOS inspection cost functions."],"url":"http://arxiv.org/abs/2402.16553v1","category":"cs.GT"}
{"created":"2024-02-26 13:22:07","title":"Delayed-feedback oscillators replicate the dynamics of multiplex networks: wavefront propagation and stochastic resonance","abstract":"The widespread development and use of neural networks have significantly enriched a wide range of computer algorithms and promise higher speed at lower cost. However, the imitation of neural networks by means of modern computing substrates is highly inefficient, whereas physical realization of large scale networks remains challenging. Fortunately, delayed-feedback oscillators, being much easier to realize experimentally, represent promising candidates for the empirical implementation of neural networks and next generation computing architectures. In the current research, we demonstrate that coupled bistable delayed-feedback oscillators emulate a multilayer network, where one single-layer network is connected to another single-layer network through coupling between replica nodes, i.e. the multiplex network. We show that all the aspects of the multiplexing impact on wavefront propagation and stochastic resonance identified in multilayer networks of bistable oscillators are entirely reproduced in the dynamics of time-delay oscillators. In particular, varying the coupling strength allows suppressing and enhancing the effect of stochastic resonance, as well as controlling the speed and direction of both deterministic and stochastic wavefront propagation. All the considered effects are studied in numerical simulations and confirmed in physical experiments, showing an excellent correspondence and disclosing thereby the robustness of the observed phenomena.","sentences":["The widespread development and use of neural networks have significantly enriched a wide range of computer algorithms and promise higher speed at lower cost.","However, the imitation of neural networks by means of modern computing substrates is highly inefficient, whereas physical realization of large scale networks remains challenging.","Fortunately, delayed-feedback oscillators, being much easier to realize experimentally, represent promising candidates for the empirical implementation of neural networks and next generation computing architectures.","In the current research, we demonstrate that coupled bistable delayed-feedback oscillators emulate a multilayer network, where one single-layer network is connected to another single-layer network through coupling between replica nodes, i.e. the multiplex network.","We show that all the aspects of the multiplexing impact on wavefront propagation and stochastic resonance identified in multilayer networks of bistable oscillators are entirely reproduced in the dynamics of time-delay oscillators.","In particular, varying the coupling strength allows suppressing and enhancing the effect of stochastic resonance, as well as controlling the speed and direction of both deterministic and stochastic wavefront propagation.","All the considered effects are studied in numerical simulations and confirmed in physical experiments, showing an excellent correspondence and disclosing thereby the robustness of the observed phenomena."],"url":"http://arxiv.org/abs/2402.16551v1","category":"nlin.AO"}
{"created":"2024-02-26 13:08:44","title":"Beyond Accuracy: An Empirical Study on Unit Testing in Open-source Deep Learning Projects","abstract":"Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness. However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems. Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub. We find that: 1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests, 2) 68% of the sampled DL projects are not unit tested at all, 3) the layer and utilities (utils) of DL models have the most unit tests. Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects. We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability. The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area.","sentences":["Deep Learning (DL) models have rapidly advanced, focusing on achieving high performance through testing model accuracy and robustness.","However, it is unclear whether DL projects, as software systems, are tested thoroughly or functionally correct when there is a need to treat and test them like other software systems.","Therefore, we empirically study the unit tests in open-source DL projects, analyzing 9,129 projects from GitHub.","We find that: 1) unit tested DL projects have positive correlation with the open-source project metrics and have a higher acceptance rate of pull requests, 2) 68% of the sampled DL projects are not unit tested at all, 3) the layer and utilities (utils) of DL models have the most unit tests.","Based on these findings and previous research outcomes, we built a mapping taxonomy between unit tests and faults in DL projects.","We discuss the implications of our findings for developers and researchers and highlight the need for unit testing in open-source DL projects to ensure their reliability and stability.","The study contributes to this community by raising awareness of the importance of unit testing in DL projects and encouraging further research in this area."],"url":"http://arxiv.org/abs/2402.16546v1","category":"cs.SE"}
{"created":"2024-02-26 13:01:28","title":"RoboGrind: Intuitive and Interactive Surface Treatment with Industrial Robots","abstract":"Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate. We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots. It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment. RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades.","sentences":["Surface treatment tasks such as grinding, sanding or polishing are a vital step of the value chain in many industries, but are notoriously challenging to automate.","We present RoboGrind, an integrated system for the intuitive, interactive automation of surface treatment tasks with industrial robots.","It combines a sophisticated 3D perception pipeline for surface scanning and automatic defect identification, an interactive voice-controlled wizard system for the AI-assisted bootstrapping and parameterization of robot programs, and an automatic planning and execution pipeline for force-controlled robotic surface treatment.","RoboGrind is evaluated both under laboratory and real-world conditions in the context of refabricating fiberglass wind turbine blades."],"url":"http://arxiv.org/abs/2402.16542v1","category":"cs.RO"}
{"created":"2024-02-26 12:55:51","title":"Integrating Large Language Models with Graphical Session-Based Recommendation","abstract":"With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems. While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity. SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors. The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs. In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously integrating LLMs with Graph Neural Networks (GNNs) for SBR tasks. This integration seeks to leverage the complementary strengths of LLMs in natural language understanding and GNNs in relational data processing, leading to a more powerful session-based recommender system that can understand and recommend items within a session. Moreover, to endow the LLM with the capability to empower SBR tasks, we design a series of prompts for both auxiliary and major instruction tuning tasks. These prompts are crafted to assist the LLM in understanding graph-structured data and align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by LLM architectures. Extensive experiments on three real-world datasets demonstrate that LLMGR outperforms several competitive baselines, indicating its effectiveness in enhancing SBR tasks and its potential as a research direction for future exploration.","sentences":["With the rapid development of Large Language Models (LLMs), various explorations have arisen to utilize LLMs capability of context understanding on recommender systems.","While pioneering strategies have primarily transformed traditional recommendation tasks into challenges of natural language generation, there has been a relative scarcity of exploration in the domain of session-based recommendation (SBR) due to its specificity.","SBR has been primarily dominated by Graph Neural Networks, which have achieved many successful outcomes due to their ability to capture both the implicit and explicit relationships between adjacent behaviors.","The structural nature of graphs contrasts with the essence of natural language, posing a significant adaptation gap for LLMs.","In this paper, we introduce large language models with graphical Session-Based recommendation, named LLMGR, an effective framework that bridges the aforementioned gap by harmoniously integrating LLMs with Graph Neural Networks (GNNs) for SBR tasks.","This integration seeks to leverage the complementary strengths of LLMs in natural language understanding and GNNs in relational data processing, leading to a more powerful session-based recommender system that can understand and recommend items within a session.","Moreover, to endow the LLM with the capability to empower SBR tasks, we design a series of prompts for both auxiliary and major instruction tuning tasks.","These prompts are crafted to assist the LLM in understanding graph-structured data and align textual information with nodes, effectively translating nuanced user interactions into a format that can be understood and utilized by LLM architectures.","Extensive experiments on three real-world datasets demonstrate that LLMGR outperforms several competitive baselines, indicating its effectiveness in enhancing SBR tasks and its potential as a research direction for future exploration."],"url":"http://arxiv.org/abs/2402.16539v1","category":"cs.IR"}
{"created":"2024-02-26 12:53:44","title":"Learning to Maximize (Expected) Utility","abstract":"We study if participants in a choice experiment learn to behave in ways that are closer to the predictions of ordinal and expected utility theory as they make decisions from the same menus repeatedly and without receiving feedback of any kind. We designed and implemented a non-forced-choice lab experiment with money lotteries and five repetitions per menu that aimed to test this hypothesis from many behavioural angles. In our data from 308 subjects in the UK and Germany, significantly more individuals were ordinal- and expected-utility maximizers in their last 15 than in their first 15 identical decision problems. Furthermore, around a quarter and a fifth of all subjects, respectively, decided in those modes throughout the experiment, with nearly half revealing non-trivial indifferences. A considerable overlap was found between those consistently rational individuals and the ones who satisfied core principles of random utility theory. Finally, in addition to finding that choice consistency is positively correlated with cognitive ability, we document that subjects who learned to maximize utility were more cognitively able than those who did not. We discuss potential implications of our analysis.","sentences":["We study if participants in a choice experiment learn to behave in ways that are closer to the predictions of ordinal and expected utility theory as they make decisions from the same menus repeatedly and without receiving feedback of any kind.","We designed and implemented a non-forced-choice lab experiment with money lotteries and five repetitions per menu that aimed to test this hypothesis from many behavioural angles.","In our data from 308 subjects in the UK and Germany, significantly more individuals were ordinal- and expected-utility maximizers in their last 15 than in their first 15 identical decision problems.","Furthermore, around a quarter and a fifth of all subjects, respectively, decided in those modes throughout the experiment, with nearly half revealing non-trivial indifferences.","A considerable overlap was found between those consistently rational individuals and the ones who satisfied core principles of random utility theory.","Finally, in addition to finding that choice consistency is positively correlated with cognitive ability, we document that subjects who learned to maximize utility were more cognitively able than those who did not.","We discuss potential implications of our analysis."],"url":"http://arxiv.org/abs/2402.16538v1","category":"econ.GN"}
{"created":"2024-02-26 12:51:43","title":"Tests of Macrorealism in Discrete and Continuous Variable Systems","abstract":"I study several aspects of tests of macrorealism (MR), which for a given data set serves to give a quantitative signal of the presence of a specific notion of non-classical behaviour. The insufficiency of classical understanding underpins both the paradoxes of quantum mechanics, its future technological promise, and so these tests are of interest both foundationally and pragmatically. I derive generalisations of the Leggett-Garg (LG) inequalities and Fine's theorem, which together establish the necessary and sufficient conditions for macrorealism. First, I extend these conditions to tests involving an arbitrary number of measurement times. Secondly, I generalise them beyond the standard dichotomic variable, to systems described by many-valued variables. I also perform a quantum mechanical analysis examining the interplay of different conditions of MR. I then develop the theoretical framework to support tests of macrorealism in continuous variable systems, where I define variables based on coarse-grainings of position. I calculate temporal correlators for general bound systems, and analyse LG violations within the quantum harmonic oscillator (QHO), in its energy eigenstates and coherent states. I analyse the precise physical mechanisms underpinning the violations in terms of probability currents, Bohm trajectories. Staying within continuous variable systems, we outline a different approach to meeting the invasiveness requirement of LG tests. Reasoning that we may approximately non-invasively measure whether a particle crosses the axis, we measure an object which is related to the standard correlators, and derive a set of macrorealistic inequalities for these modified correlators. We demonstrate violations of these modified LG inequalities for several states within the QHO.","sentences":["I study several aspects of tests of macrorealism (MR), which for a given data set serves to give a quantitative signal of the presence of a specific notion of non-classical behaviour.","The insufficiency of classical understanding underpins both the paradoxes of quantum mechanics, its future technological promise, and so these tests are of interest both foundationally and pragmatically.","I derive generalisations of the Leggett-Garg (LG) inequalities and Fine's theorem, which together establish the necessary and sufficient conditions for macrorealism.","First, I extend these conditions to tests involving an arbitrary number of measurement times.","Secondly, I generalise them beyond the standard dichotomic variable, to systems described by many-valued variables.","I also perform a quantum mechanical analysis examining the interplay of different conditions of MR.","I then develop the theoretical framework to support tests of macrorealism in continuous variable systems, where I define variables based on coarse-grainings of position.","I calculate temporal correlators for general bound systems, and analyse LG violations within the quantum harmonic oscillator (QHO), in its energy eigenstates and coherent states.","I analyse the precise physical mechanisms underpinning the violations in terms of probability currents, Bohm trajectories.","Staying within continuous variable systems, we outline a different approach to meeting the invasiveness requirement of LG tests.","Reasoning that we may approximately non-invasively measure whether a particle crosses the axis, we measure an object which is related to the standard correlators, and derive a set of macrorealistic inequalities for these modified correlators.","We demonstrate violations of these modified LG inequalities for several states within the QHO."],"url":"http://arxiv.org/abs/2402.16537v1","category":"quant-ph"}
{"created":"2024-02-26 12:31:11","title":"Visualization of frequency structures in gravitational wave signals","abstract":"The gravitational wave signals produced by the coalescence of compact binaries progress through three stages: inspiral, merger, and postmerger. The evolution of their frequency follows a slow build up during the inspiral that peaks at merger, forming the characteristic \"chirp\" pattern in the signal's time-frequency map. Herein we introduce a framework for localizing further characteristic structures in the time-frequency space of gravitational wave signals using the continuous wavelet transform. We consider two example cases where there are specific patterns in the postmerger stage of the signal that are rich with information on the physical nature of the source: highly-inclined black hole binaries with asymmetric mass ratio, and neutron star binaries with postmerger remnant oscillations. It is demonstrated that the choice of quality factor $Q$ plays a central role in distinguishing the postmerger features from that of the inspiral, with black hole systems preferring lower $Q$ and neutron star systems preferring higher $Q$. Furthermore, we demonstrate the use of chirplets as the wavelet transform basis, which allow for manipulation of structure in the time-frequency map.","sentences":["The gravitational wave signals produced by the coalescence of compact binaries progress through three stages: inspiral, merger, and postmerger.","The evolution of their frequency follows a slow build up during the inspiral that peaks at merger, forming the characteristic \"chirp\" pattern in the signal's time-frequency map.","Herein we introduce a framework for localizing further characteristic structures in the time-frequency space of gravitational wave signals using the continuous wavelet transform.","We consider two example cases where there are specific patterns in the postmerger stage of the signal that are rich with information on the physical nature of the source: highly-inclined black hole binaries with asymmetric mass ratio, and neutron star binaries with postmerger remnant oscillations.","It is demonstrated that the choice of quality factor $Q$ plays a central role in distinguishing the postmerger features from that of the inspiral, with black hole systems preferring lower $Q$ and neutron star systems preferring higher $Q$.","Furthermore, we demonstrate the use of chirplets as the wavelet transform basis, which allow for manipulation of structure in the time-frequency map."],"url":"http://arxiv.org/abs/2402.16533v1","category":"gr-qc"}
{"created":"2024-02-26 12:26:06","title":"On the directional asymptotic approach in optimization theory","abstract":"As a starting point of our research, we show that, for a fixed order $\\gamma\\geq 1$, each local minimizer of a rather general nonsmooth optimization problem in Euclidean spaces is either M-stationary in the classical sense (corresponding to stationarity of order $1$), satisfies stationarity conditions in terms of a coderivative construction of order $\\gamma$, or is asymptotically stationary with respect to a critical direction as well as order $\\gamma$ in a certain sense. By ruling out the latter case with a constraint qualification not stronger than directional metric subregularity, we end up with new necessary optimality conditions comprising a mixture of limiting variational tools of orders $1$ and $\\gamma$. These abstract findings are carved out for the broad class of geometric constraints and $\\gamma:=2$, and visualized by examples from complementarity-constrained and nonlinear semidefinite optimization. As a byproduct of the particular setting $\\gamma:=1$, our general approach yields new so-called directional asymptotic regularity conditions which serve as constraint qualifications guaranteeing M-stationarity of local minimizers. We compare these new regularity conditions with standard constraint qualifications from nonsmooth optimization. Further, we extend directional concepts of pseudo- and quasi-normality to arbitrary set-valued mappings. It is shown that these properties provide sufficient conditions for the validity of directional asymptotic regularity. Finally, a novel coderivative-like variational tool is used to construct sufficient conditions for the presence of directional asymptotic regularity. For geometric constraints, it is illustrated that all appearing objects can be calculated in terms of initial problem data.","sentences":["As a starting point of our research, we show that, for a fixed order $\\gamma\\geq 1$, each local minimizer of a rather general nonsmooth optimization problem in Euclidean spaces is either M-stationary in the classical sense (corresponding to stationarity of order $1$), satisfies stationarity conditions in terms of a coderivative construction of order $\\gamma$, or is asymptotically stationary with respect to a critical direction as well as order $\\gamma$ in a certain sense.","By ruling out the latter case with a constraint qualification not stronger than directional metric subregularity, we end up with new necessary optimality conditions comprising a mixture of limiting variational tools of orders $1$ and $\\gamma$.","These abstract findings are carved out for the broad class of geometric constraints and $\\gamma:=2$, and visualized by examples from complementarity-constrained and nonlinear semidefinite optimization.","As a byproduct of the particular setting $\\gamma:=1$, our general approach yields new so-called directional asymptotic regularity conditions which serve as constraint qualifications guaranteeing M-stationarity of local minimizers.","We compare these new regularity conditions with standard constraint qualifications from nonsmooth optimization.","Further, we extend directional concepts of pseudo- and quasi-normality to arbitrary set-valued mappings.","It is shown that these properties provide sufficient conditions for the validity of directional asymptotic regularity.","Finally, a novel coderivative-like variational tool is used to construct sufficient conditions for the presence of directional asymptotic regularity.","For geometric constraints, it is illustrated that all appearing objects can be calculated in terms of initial problem data."],"url":"http://arxiv.org/abs/2402.16530v1","category":"math.OC"}
{"created":"2024-02-26 12:25:56","title":"Black hole in a combined magnetic field: ionized accretion disks in the jetlike and looplike configurations","abstract":"Magnetic fields surrounding black holes are responsible for various astrophysical phenomena related to accretion processes and relativistic jets. Depending on the source, the configuration of the field lines may differ significantly, affecting the trajectories of charged particles and the corresponding observables. Usually, the magnetic fields around black holes are modeled within a single source or current generating the field. However, magnetic fields can have more than a single origin, being a combination of different fields, such as, e.g., that of an accretion disk and external large-scale or Galactic ones. In this paper, we propose a combined magnetic field solution given by the superposition of the uniform and Blandford-Znajek split-monopole magnetic fields in a strong gravity regime of the Schwarzschild black hole. We show that when the combined magnetic field components are aligned, the resulting field is of a paraboloidal jetlike shape. Such a configuration is supported by relativistic jet observations and is often utilized in general relativistic magnetohydrodynamical simulations. In the opposite orientation of the two field components, we observe looplike field structures magnetically connecting the black hole with an accretion disk and the magnetic null points, which can be related to the regions of magnetic reconnection. In the combined magnetic field configurations, we analyze the dynamics of charged particles, study their stability conditions, and find the locations of stable off-equatorial structures close to the symmetry axis. We consider an ionization of Keplerian accretion disk as a particular scenario of particle scattering. From the numerical experiments, we conclude that charged particles in the jetlike combination show a strong tendency to escape from the black hole. In contrast, the looplike combination supports accretion of charged particles into the black hole.","sentences":["Magnetic fields surrounding black holes are responsible for various astrophysical phenomena related to accretion processes and relativistic jets.","Depending on the source, the configuration of the field lines may differ significantly, affecting the trajectories of charged particles and the corresponding observables.","Usually, the magnetic fields around black holes are modeled within a single source or current generating the field.","However, magnetic fields can have more than a single origin, being a combination of different fields, such as, e.g., that of an accretion disk and external large-scale or Galactic ones.","In this paper, we propose a combined magnetic field solution given by the superposition of the uniform and Blandford-Znajek split-monopole magnetic fields in a strong gravity regime of the Schwarzschild black hole.","We show that when the combined magnetic field components are aligned, the resulting field is of a paraboloidal jetlike shape.","Such a configuration is supported by relativistic jet observations and is often utilized in general relativistic magnetohydrodynamical simulations.","In the opposite orientation of the two field components, we observe looplike field structures magnetically connecting the black hole with an accretion disk and the magnetic null points, which can be related to the regions of magnetic reconnection.","In the combined magnetic field configurations, we analyze the dynamics of charged particles, study their stability conditions, and find the locations of stable off-equatorial structures close to the symmetry axis.","We consider an ionization of Keplerian accretion disk as a particular scenario of particle scattering.","From the numerical experiments, we conclude that charged particles in the jetlike combination show a strong tendency to escape from the black hole.","In contrast, the looplike combination supports accretion of charged particles into the black hole."],"url":"http://arxiv.org/abs/2402.16529v1","category":"astro-ph.HE"}
{"created":"2024-02-26 12:17:59","title":"Defocus-integration Interferometric Scattering Microscopy for Speckle Suppression and Enhancing Nanoparticle Detection on Substrate","abstract":"Direct optical detection and imaging of single nanoparticles on substrate in wide field underpin vast applications across different research fields. However, the speckles originating from the unavoidable random surface undulations of the substrate ultimately limit the size of the decipherable nanoparticles by the current optical techniques, including the ultrasensitive interferometric scattering microscopy (iSCAT). Here we report a defocus-integration iSCAT to suppress the speckle noise and to enhance the detection and imaging of single nanoparticles on ultra-flat glass substrate and silicon wafer. In particular, we discover distinct symmetry properties of the scattering phase between the nanoparticle and the surface undulations that cause the speckles. Consequently, we develop the defocus-integration technique to suppress the speckles.We experimentally achieve an enhancement of the signal to noise ratio by 6.9 dB for the nanoparticle detection. We demonstrate that the technique is generally applicable for nanoparticles of various materials and for both low and high refractive-index substrates.","sentences":["Direct optical detection and imaging of single nanoparticles on substrate in wide field underpin vast applications across different research fields.","However, the speckles originating from the unavoidable random surface undulations of the substrate ultimately limit the size of the decipherable nanoparticles by the current optical techniques, including the ultrasensitive interferometric scattering microscopy (iSCAT).","Here we report a defocus-integration iSCAT to suppress the speckle noise and to enhance the detection and imaging of single nanoparticles on ultra-flat glass substrate and silicon wafer.","In particular, we discover distinct symmetry properties of the scattering phase between the nanoparticle and the surface undulations that cause the speckles.","Consequently, we develop the defocus-integration technique to suppress the speckles.","We experimentally achieve an enhancement of the signal to noise ratio by 6.9 dB for the nanoparticle detection.","We demonstrate that the technique is generally applicable for nanoparticles of various materials and for both low and high refractive-index substrates."],"url":"http://arxiv.org/abs/2402.16527v1","category":"physics.optics"}
{"created":"2024-02-26 12:09:34","title":"Large Anomalous Hall Effect at Room Temperature in a Fermi-Level-Tuned Kagome Antiferromagnet","abstract":"The recent discoveries of surperisingly large anomalous Hall effect in chiral antiderromagnets have triggered extensive research efforts in various fields, ranging from topological condensed-matter physics to antiferromagnetic spintronics, and energy harvesting technology. However, such AHE-hosting antiferromagnetic materials are rare in nature. Herein, we demonstrate that Mn2.4Ga, a Fermi-level-tuned kagome antiferromagnet, has a large anomalous Hall conductivity of about 150 {\\Omega}-1cm-1 at room temperature that surpasses the usual high values (i.e.,20-50 {\\Omega}-1cm-1) observed so far in two outstanding kagome antiferromagnets, Mn3Sn and Mn3Ge. The spin triangular structure of Mn2.4Ga guarantees a nonzero Berry curvature while generates only a weak net moment in the kagome plane.Moreover, the anomalous Hall conductivity exhibits a sign reversal with the rotation of a small magnetic field, which can be ascribed to the field-controlled chirality of the spin triangular structure. Our theoretical calculation indicate that the large AHE in Mn2.4Ga originates from a significantly enhanced Berry curvature associated wiht the tuning of the Fermi level close to the Weyl points. These properties, together with the ability to manipulate moment orientations using a moderate external magnetic field, make Mn2.4Ga extremely exciting for future antiferromagnetic spintronics.","sentences":["The recent discoveries of surperisingly large anomalous Hall effect in chiral antiderromagnets have triggered extensive research efforts in various fields, ranging from topological condensed-matter physics to antiferromagnetic spintronics, and energy harvesting technology.","However, such AHE-hosting antiferromagnetic materials are rare in nature.","Herein, we demonstrate that Mn2.4Ga, a Fermi-level-tuned kagome antiferromagnet, has a large anomalous Hall conductivity of about 150 {\\Omega}-1cm-1 at room temperature that surpasses the usual high values (i.e.,20-50 {\\Omega}-1cm-1) observed so far in two outstanding kagome antiferromagnets, Mn3Sn and Mn3Ge.","The spin triangular structure of Mn2.4Ga guarantees a nonzero Berry curvature while generates only a weak net moment in the kagome plane.","Moreover, the anomalous Hall conductivity exhibits a sign reversal with the rotation of a small magnetic field, which can be ascribed to the field-controlled chirality of the spin triangular structure.","Our theoretical calculation indicate that the large AHE in Mn2.4Ga originates from a significantly enhanced Berry curvature associated wiht the tuning of the Fermi level close to the Weyl points.","These properties, together with the ability to manipulate moment orientations using a moderate external magnetic field, make Mn2.4Ga extremely exciting for future antiferromagnetic spintronics."],"url":"http://arxiv.org/abs/2402.16521v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-26 12:04:23","title":"Sequential design for surrogate modeling in Bayesian inverse problems","abstract":"Sequential design is a highly active field of research in active learning which provides a general framework for the design of computer experiments to make the most of a low computational budget. It has been widely used to generate efficient surrogate models able to replace complex computer codes, most notably for uncertainty quantification, Bayesian optimization, reliability analysis or model calibration tasks. In this work, a sequential design strategy is developed for Bayesian inverse problems, in which a Gaussian process surrogate model serves as an emulator for a costly computer code. The proposed strategy is based on a goal-oriented I-optimal criterion adapted to the Stepwise Uncertainty Reduction (SUR) paradigm. In SUR strategies, a new design point is chosen by minimizing the expectation of an uncertainty metric with respect to the yet unknown new data point. These methods have attracted increasing interest as they provide an accessible framework for the sequential design of experiments while including almost-sure convergence for the most-widely used metrics. In this paper, a weighted integrated mean square prediction error is introduced and serves as a metric of uncertainty for the newly proposed IP-SUR (Inverse Problem Stepwise Uncertainty Reduction) sequential design strategy derived from SUR methods. This strategy is shown to be tractable for both scalar and multi-output Gaussian process surrogate models with continuous sample paths, and comes with theoretical guarantee for the almost-sure convergence of the metric of uncertainty. The premises of this work are highlighted on various test cases in which the newly derived strategy is compared to other naive and sequential designs (D-optimal designs, Bayes risk minimization).","sentences":["Sequential design is a highly active field of research in active learning which provides a general framework for the design of computer experiments to make the most of a low computational budget.","It has been widely used to generate efficient surrogate models able to replace complex computer codes, most notably for uncertainty quantification, Bayesian optimization, reliability analysis or model calibration tasks.","In this work, a sequential design strategy is developed for Bayesian inverse problems, in which a Gaussian process surrogate model serves as an emulator for a costly computer code.","The proposed strategy is based on a goal-oriented I-optimal criterion adapted to the Stepwise Uncertainty Reduction (SUR) paradigm.","In SUR strategies, a new design point is chosen by minimizing the expectation of an uncertainty metric with respect to the yet unknown new data point.","These methods have attracted increasing interest as they provide an accessible framework for the sequential design of experiments while including almost-sure convergence for the most-widely used metrics.","In this paper, a weighted integrated mean square prediction error is introduced and serves as a metric of uncertainty for the newly proposed IP-SUR (Inverse Problem Stepwise Uncertainty Reduction) sequential design strategy derived from SUR methods.","This strategy is shown to be tractable for both scalar and multi-output Gaussian process surrogate models with continuous sample paths, and comes with theoretical guarantee for the almost-sure convergence of the metric of uncertainty.","The premises of this work are highlighted on various test cases in which the newly derived strategy is compared to other naive and sequential designs (D-optimal designs, Bayes risk minimization)."],"url":"http://arxiv.org/abs/2402.16520v1","category":"stat.ME"}
{"created":"2024-02-26 11:58:02","title":"Discovering Artificial Viscosity Models for Discontinuous Galerkin Approximation of Conservation Laws using Physics-Informed Machine Learning","abstract":"Finite element-based high-order solvers of conservation laws offer large accuracy but face challenges near discontinuities due to the Gibbs phenomenon. Artificial viscosity is a popular and effective solution to this problem based on physical insight. In this work, we present a physics-informed machine learning algorithm to automate the discovery of artificial viscosity models in a non-supervised paradigm. The algorithm is inspired by reinforcement learning and trains a neural network acting cell-by-cell (the viscosity model) by minimizing a loss defined as the difference with respect to a reference solution thanks to automatic differentiation. This enables a dataset-free training procedure. We prove that the algorithm is effective by integrating it into a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase several numerical tests on scalar and vectorial problems, such as Burgers' and Euler's equations in one and two dimensions. Results demonstrate that the proposed approach trains a model that is able to outperform classical viscosity models. Moreover, we show that the learnt artificial viscosity model is able to generalize across different problems and parameters.","sentences":["Finite element-based high-order solvers of conservation laws offer large accuracy but face challenges near discontinuities due to the Gibbs phenomenon.","Artificial viscosity is a popular and effective solution to this problem based on physical insight.","In this work, we present a physics-informed machine learning algorithm to automate the discovery of artificial viscosity models in a non-supervised paradigm.","The algorithm is inspired by reinforcement learning and trains a neural network acting cell-by-cell (the viscosity model) by minimizing a loss defined as the difference with respect to a reference solution thanks to automatic differentiation.","This enables a dataset-free training procedure.","We prove that the algorithm is effective by integrating it into a state-of-the-art Runge-Kutta discontinuous Galerkin solver.","We showcase several numerical tests on scalar and vectorial problems, such as Burgers' and Euler's equations in one and two dimensions.","Results demonstrate that the proposed approach trains a model that is able to outperform classical viscosity models.","Moreover, we show that the learnt artificial viscosity model is able to generalize across different problems and parameters."],"url":"http://arxiv.org/abs/2402.16517v1","category":"math.NA"}
{"created":"2024-02-26 11:54:54","title":"Generative Pretrained Hierarchical Transformer for Time Series Forecasting","abstract":"Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies. Nevertheless, existing approaches still exhibit two critical drawbacks. Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data. Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT. There are two aspects of key designs in GPHT. On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various datasets from diverse data scenarios. This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets. On the other hand, GPHT employs an auto-regressive forecasting approach under the channel-independent assumption, effectively modeling temporal dependencies in the output series. Importantly, no customized forecasting head is required, enabling a single model to forecast at arbitrary horizon settings. We conduct sufficient experiments on eight datasets with mainstream self-supervised pretraining models and supervised models. The results demonstrated that GPHT surpasses the baseline models across various fine-tuning and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretrained time series large models.","sentences":["Recent efforts have been dedicated to enhancing time series forecasting accuracy by introducing advanced network architectures and self-supervised pretraining strategies.","Nevertheless, existing approaches still exhibit two critical drawbacks.","Firstly, these methods often rely on a single dataset for training, limiting the model's generalizability due to the restricted scale of the training data.","Secondly, the one-step generation schema is widely followed, which necessitates a customized forecasting head and overlooks the temporal dependencies in the output series, and also leads to increased training costs under different horizon length settings.   ","To address these issues, we propose a novel generative pretrained hierarchical transformer architecture for forecasting, named GPHT.","There are two aspects of key designs in GPHT.","On the one hand, we advocate for constructing a mixed dataset for pretraining our model, comprising various datasets from diverse data scenarios.","This approach significantly expands the scale of training data, allowing our model to uncover commonalities in time series data and facilitating improved transfer to specific datasets.","On the other hand, GPHT employs an auto-regressive forecasting approach under the channel-independent assumption, effectively modeling temporal dependencies in the output series.","Importantly, no customized forecasting head is required, enabling a single model to forecast at arbitrary horizon settings.","We conduct sufficient experiments on eight datasets with mainstream self-supervised pretraining models and supervised models.","The results demonstrated that GPHT surpasses the baseline models across various fine-tuning and zero/few-shot learning settings in the traditional long-term forecasting task, providing support for verifying the feasibility of pretrained time series large models."],"url":"http://arxiv.org/abs/2402.16516v1","category":"cs.LG"}
{"created":"2024-02-26 11:52:55","title":"LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification","abstract":"As sufficient data are not always publically accessible for model training, researchers exploit limited data with advanced learning algorithms or expand the dataset via data augmentation (DA). Conducting DA in private domain requires private protection approaches (i.e. anonymization and perturbation), but those methods cannot provide protection guarantees. Differential privacy (DP) learning methods theoretically bound the protection but are not skilled at generating pseudo text samples with large models. In this paper, we transfer DP-based pseudo sample generation task to DP-based generated samples discrimination task, where we propose a DP-based DA method with a LLM and a DP-based discriminator for text classification on private domains. We construct a knowledge distillation model as the DP-based discriminator: teacher models, accessing private data, teaches students how to select private samples with calibrated noise to achieve DP. To constrain the distribution of DA's generation, we propose a DP-based tutor that models the noised private distribution and controls samples' generation with a low privacy cost. We theoretically analyze our model's privacy protection and empirically verify our model.","sentences":["As sufficient data are not always publically accessible for model training, researchers exploit limited data with advanced learning algorithms or expand the dataset via data augmentation (DA).","Conducting DA in private domain requires private protection approaches (i.e. anonymization and perturbation), but those methods cannot provide protection guarantees.","Differential privacy (DP) learning methods theoretically bound the protection but are not skilled at generating pseudo text samples with large models.","In this paper, we transfer DP-based pseudo sample generation task to DP-based generated samples discrimination task, where we propose a DP-based DA method with a LLM and a DP-based discriminator for text classification on private domains.","We construct a knowledge distillation model as the DP-based discriminator: teacher models, accessing private data, teaches students how to select private samples with calibrated noise to achieve DP.","To constrain the distribution of DA's generation, we propose a DP-based tutor that models the noised private distribution and controls samples' generation with a low privacy cost.","We theoretically analyze our model's privacy protection and empirically verify our model."],"url":"http://arxiv.org/abs/2402.16515v1","category":"cs.CL"}
{"created":"2024-02-26 11:50:42","title":"Enhancement of 3D Camera Synthetic Training Data with Noise Models","abstract":"The goal of this paper is to assess the impact of noise in 3D camera-captured data by modeling the noise of the imaging process and applying it on synthetic training data. We compiled a dataset of specifically constructed scenes to obtain a noise model. We specifically model lateral noise, affecting the position of captured points in the image plane, and axial noise, affecting the position along the axis perpendicular to the image plane. The estimated models can be used to emulate noise in synthetic training data. The added benefit of adding artificial noise is evaluated in an experiment with rendered data for object segmentation. We train a series of neural networks with varying levels of noise in the data and measure their ability to generalize on real data. The results show that using too little or too much noise can hurt the networks' performance indicating that obtaining a model of noise from real scanners is beneficial for synthetic data generation.","sentences":["The goal of this paper is to assess the impact of noise in 3D camera-captured data by modeling the noise of the imaging process and applying it on synthetic training data.","We compiled a dataset of specifically constructed scenes to obtain a noise model.","We specifically model lateral noise, affecting the position of captured points in the image plane, and axial noise, affecting the position along the axis perpendicular to the image plane.","The estimated models can be used to emulate noise in synthetic training data.","The added benefit of adding artificial noise is evaluated in an experiment with rendered data for object segmentation.","We train a series of neural networks with varying levels of noise in the data and measure their ability to generalize on real data.","The results show that using too little or too much noise can hurt the networks' performance indicating that obtaining a model of noise from real scanners is beneficial for synthetic data generation."],"url":"http://arxiv.org/abs/2402.16514v1","category":"cs.CV"}
{"created":"2024-02-26 11:50:12","title":"Photonic Neural Network Fabricated on Thin Film Lithium Niobate for High-Fidelity and Power-Efficient Matrix Computation","abstract":"Photonic neural networks (PNNs) have emerged as a promising platform to address the energy consumption issue that comes with the advancement of artificial intelligence technology, and thin film lithium niobate (TFLN) offers an attractive solution as a material platform mainly for its combined characteristics of low optical loss and large electro-optic (EO) coefficients. Here, we present the first implementation of an EO tunable PNN based on the TFLN platform. Our device features ultra-high fidelity, high computation speed, and exceptional power efficiency. We benchmark the performance of our device with several deep learning missions including in-situ training of Circle and Moons nonlinear datasets classification, Iris flower species recognition, and handwriting digits recognition. Our work paves the way for sustainable up-scaling of high-speed, energy-efficient PNNs.","sentences":["Photonic neural networks (PNNs) have emerged as a promising platform to address the energy consumption issue that comes with the advancement of artificial intelligence technology, and thin film lithium niobate (TFLN) offers an attractive solution as a material platform mainly for its combined characteristics of low optical loss and large electro-optic (EO) coefficients.","Here, we present the first implementation of an EO tunable PNN based on the TFLN platform.","Our device features ultra-high fidelity, high computation speed, and exceptional power efficiency.","We benchmark the performance of our device with several deep learning missions including in-situ training of Circle and Moons nonlinear datasets classification, Iris flower species recognition, and handwriting digits recognition.","Our work paves the way for sustainable up-scaling of high-speed, energy-efficient PNNs."],"url":"http://arxiv.org/abs/2402.16513v1","category":"physics.optics"}
{"created":"2024-02-26 11:48:37","title":"Ergodicity in planar slow-fast systems through slow relation functions","abstract":"In this paper, we study ergodic properties of the slow relation function (or entry-exit function) in planar slow-fast systems. It is well known that zeros of the slow divergence integral associated with canard limit periodic sets give candidates for limit cycles. We present a new approach to detect the zeros of the slow divergence integral by studying the structure of the set of all probability measures invariant under the corresponding slow relation function. Using the slow relation function, we also show how to estimate (in terms of weak convergence) the transformation of families of probability measures that describe initial point distribution of canard orbits during the passage near a slow-fast Hopf point (or a more general turning point). We provide formulas to compute exit densities for given entry densities and the slow relation function. We apply our results to slow-fast Li\\'{e}nard equations.","sentences":["In this paper, we study ergodic properties of the slow relation function (or entry-exit function) in planar slow-fast systems.","It is well known that zeros of the slow divergence integral associated with canard limit periodic sets give candidates for limit cycles.","We present a new approach to detect the zeros of the slow divergence integral by studying the structure of the set of all probability measures invariant under the corresponding slow relation function.","Using the slow relation function, we also show how to estimate (in terms of weak convergence) the transformation of families of probability measures that describe initial point distribution of canard orbits during the passage near a slow-fast Hopf point (or a more general turning point).","We provide formulas to compute exit densities for given entry densities and the slow relation function.","We apply our results to slow-fast Li\\'{e}nard equations."],"url":"http://arxiv.org/abs/2402.16511v1","category":"math.DS"}
{"created":"2024-02-26 13:34:57","title":"Open Your Ears to Take a Look: A State-of-the-Art Report on the Integration of Sonification and Visualization","abstract":"The research communities studying visualization and sonification for data display and analysis share exceptionally similar goals, essentially making data of any kind interpretable to humans. One community does so by using visual representations of data, the other community does so by employing auditory (non-speech) representations of data. While the two communities have a lot in common, they developed mostly in parallel over the course of the last few decades. With this STAR, we discuss a collection of work that bridges the borders of the two communities, hence a collection of work that aims to integrate the two techniques to one form of audiovisual display, which we argue to be \"more than the sum of the two.\" We introduce and motivate a classification system applicable to such audiovisual displays and categorize a corpus of 57 academic publications that appeared between 2011 and 2023 in categories such as reading level, dataset type, or evaluation system, to mention a few. The corpus also enables a meta-analysis of the field, including regularly occurring design patterns such as type of visualization and sonification techniques, or the use of visual and auditory channels, and the analysis of a co-author network of the field which shows individual teams without much interconnection. The body of work covered in this STAR also relates to three adjacent topics: audiovisual monitoring, accessibility, and audiovisual data art. These three topics are discussed individually in addition to the systematically conducted part of this research. The findings of this report may be used by researchers from both fields to understand the potentials and challenges of such integrated designs, while inspiring them for future collaboration with experts from the respective other field.","sentences":["The research communities studying visualization and sonification for data display and analysis share exceptionally similar goals, essentially making data of any kind interpretable to humans.","One community does so by using visual representations of data, the other community does so by employing auditory (non-speech) representations of data.","While the two communities have a lot in common, they developed mostly in parallel over the course of the last few decades.","With this STAR, we discuss a collection of work that bridges the borders of the two communities, hence a collection of work that aims to integrate the two techniques to one form of audiovisual display, which we argue to be \"more than the sum of the two.\"","We introduce and motivate a classification system applicable to such audiovisual displays and categorize a corpus of 57 academic publications that appeared between 2011 and 2023 in categories such as reading level, dataset type, or evaluation system, to mention a few.","The corpus also enables a meta-analysis of the field, including regularly occurring design patterns such as type of visualization and sonification techniques, or the use of visual and auditory channels, and the analysis of a co-author network of the field which shows individual teams without much interconnection.","The body of work covered in this STAR also relates to three adjacent topics: audiovisual monitoring, accessibility, and audiovisual data art.","These three topics are discussed individually in addition to the systematically conducted part of this research.","The findings of this report may be used by researchers from both fields to understand the potentials and challenges of such integrated designs, while inspiring them for future collaboration with experts from the respective other field."],"url":"http://arxiv.org/abs/2402.16558v1","category":"cs.HC"}
{"created":"2024-02-26 13:36:47","title":"Normal approximations of commuting square-summable matrix families","abstract":"For any square-summable commuting family $(A_i)_{i\\in I}$ of complex $n\\times n$ matrices there is a normal commuting family $(B_i)_i$ no farther from it, in squared normalized $\\ell^2$ distance, than the diameter of the numerical range of $\\sum_i A_i^* A_i$. Specializing in one direction (limiting case of the inequality for finite $I$) this recovers a result of M. Fraas: if $\\sum_{i=1}^{\\ell} A_i^* A_i$ is scalar for commuting $A_i\\in M_n(\\mathbb{C})$ then the $A_i$ are normal; specializing in another (singleton $I$) retrieves the well-known fact that close-to-isometric matrices are close to isometries.","sentences":["For any square-summable commuting family $(A_i)_{i\\in I}$ of complex $n\\times n$ matrices there is a normal commuting family $(B_i)_i$ no farther from it, in squared normalized $\\ell^2$ distance, than the diameter of the numerical range of $\\sum_i A_i^* A_i$. Specializing in one direction (limiting case of the inequality for finite $I$) this recovers a result of M. Fraas: if $\\sum_{i=1}^{\\ell} A_i^*","A_i$ is scalar for commuting $A_i\\in M_n(\\mathbb{C})$ then the $A_i$ are normal; specializing in another (singleton $I$) retrieves the well-known fact that close-to-isometric matrices are close to isometries."],"url":"http://arxiv.org/abs/2402.16559v1","category":"math.OA"}
{"created":"2024-02-26 13:32:11","title":"Boundary-induced transitions in M\u00f6bius quenches of holographic BCFT","abstract":"Boundary effects play an interesting role in finite-size physical systems. In this work, we study the boundary-induced properties of 1+1-dimensional critical systems driven by inhomogeneous M\\\"obius-like quenches. We focus on the entanglement entropy in BCFTs with a large central charge and a sparse spectrum of low-dimensional operators. We find that the choice of boundary conditions leads to different scenarios of dynamical phase transitions. We also derive these results in a holographic description in terms of intersecting branes in AdS$_3$, and find a precise match.","sentences":["Boundary effects play an interesting role in finite-size physical systems.","In this work, we study the boundary-induced properties of 1+1-dimensional critical systems driven by inhomogeneous M\\\"obius-like quenches.","We focus on the entanglement entropy in BCFTs with a large central charge and a sparse spectrum of low-dimensional operators.","We find that the choice of boundary conditions leads to different scenarios of dynamical phase transitions.","We also derive these results in a holographic description in terms of intersecting branes in AdS$_3$, and find a precise match."],"url":"http://arxiv.org/abs/2402.16555v1","category":"hep-th"}
{"created":"2024-02-26 13:16:07","title":"Point collocation with mollified piecewise polynomial approximants for high-order partial differential equations","abstract":"The solution approximation for partial differential equations (PDEs) can be substantially improved using smooth basis functions. The recently introduced mollified basis functions are constructed through mollification, or convolution, of cell-wise defined piecewise polynomials with a smooth mollifier of certain characteristics. The properties of the mollified basis functions are governed by the order of the piecewise functions and the smoothness of the mollifier. In this work, we exploit the high-order and high-smoothness properties of the mollified basis functions for solving PDEs through the point collocation method. The basis functions are evaluated at a set of collocation points in the domain. In addition, boundary conditions are imposed at a set of boundary collocation points distributed over the domain boundaries. To ensure the stability of the resulting linear system of equations, the number of collocation points is set larger than the total number of basis functions. The resulting linear system is overdetermined and is solved using the least square technique. The presented numerical examples confirm the convergence of the proposed approximation scheme for Poisson, linear elasticity, and biharmonic problems. We study in particular the influence of the mollifier and the spatial distribution of the collocation points.","sentences":["The solution approximation for partial differential equations (PDEs) can be substantially improved using smooth basis functions.","The recently introduced mollified basis functions are constructed through mollification, or convolution, of cell-wise defined piecewise polynomials with a smooth mollifier of certain characteristics.","The properties of the mollified basis functions are governed by the order of the piecewise functions and the smoothness of the mollifier.","In this work, we exploit the high-order and high-smoothness properties of the mollified basis functions for solving PDEs through the point collocation method.","The basis functions are evaluated at a set of collocation points in the domain.","In addition, boundary conditions are imposed at a set of boundary collocation points distributed over the domain boundaries.","To ensure the stability of the resulting linear system of equations, the number of collocation points is set larger than the total number of basis functions.","The resulting linear system is overdetermined and is solved using the least square technique.","The presented numerical examples confirm the convergence of the proposed approximation scheme for Poisson, linear elasticity, and biharmonic problems.","We study in particular the influence of the mollifier and the spatial distribution of the collocation points."],"url":"http://arxiv.org/abs/2402.16548v1","category":"math.NA"}
{"created":"2024-02-26 13:01:45","title":"Model-based deep reinforcement learning for accelerated learning from flow simulations","abstract":"In recent years, deep reinforcement learning has emerged as a technique to solve closed-loop flow control problems. Employing simulation-based environments in reinforcement learning enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms. While reinforcement learning has been applied successfully in a number of rather simple flow control benchmarks, a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow simulations. In this contribution, we demonstrate the benefits of model-based reinforcement learning for flow control applications. Specifically, we optimize the policy by alternating between trajectories sampled from flow simulations and trajectories sampled from an ensemble of environment models. The model-based learning reduces the overall training time by up to $85\\%$ for the fluidic pinball test case. Even larger savings are expected for more demanding flow simulations.","sentences":["In recent years, deep reinforcement learning has emerged as a technique to solve closed-loop flow control problems.","Employing simulation-based environments in reinforcement learning enables a priori end-to-end optimization of the control system, provides a virtual testbed for safety-critical control applications, and allows to gain a deep understanding of the control mechanisms.","While reinforcement learning has been applied successfully in a number of rather simple flow control benchmarks, a major bottleneck toward real-world applications is the high computational cost and turnaround time of flow simulations.","In this contribution, we demonstrate the benefits of model-based reinforcement learning for flow control applications.","Specifically, we optimize the policy by alternating between trajectories sampled from flow simulations and trajectories sampled from an ensemble of environment models.","The model-based learning reduces the overall training time by up to $85\\%$ for the fluidic pinball test case.","Even larger savings are expected for more demanding flow simulations."],"url":"http://arxiv.org/abs/2402.16543v1","category":"physics.flu-dyn"}
{"created":"2024-02-26 12:59:20","title":"Integer Programming Using A Single Atom","abstract":"Integer programming (IP), as the name suggests is an integer-variable-based approach commonly used to formulate real-world optimization problems with constraints. Currently, quantum algorithms reformulate the IP into an unconstrained form through the use of binary variables, which is an indirect and resource-consuming way of solving it. We develop an algorithm that maps and solves an IP problem in its original form to any quantum system that possesses a large number of accessible internal degrees of freedom which can be controlled with sufficient accuracy. Using a single Rydberg atom as an example, we associate the integer values to electronic states belonging to different manifolds and implement a selective superposition of these different states to solve the full IP problem. The optimal solution is found within 2-40{\\mu}s for a few prototypical IP problems with up to eight variables and up to four constraints including a non-linear IP problem, which is usually harder to solve with classical algorithms when compared with linear IP problems. Our algorithm for solving IP is benchmarked using the Branch & Bound approach and it outperforms the classical algorithm in terms of the number of steps needed to converge and carries the potential to improve the bounds provided by the classical algorithm for larger problems.","sentences":["Integer programming (IP), as the name suggests is an integer-variable-based approach commonly used to formulate real-world optimization problems with constraints.","Currently, quantum algorithms reformulate the IP into an unconstrained form through the use of binary variables, which is an indirect and resource-consuming way of solving it.","We develop an algorithm that maps and solves an IP problem in its original form to any quantum system that possesses a large number of accessible internal degrees of freedom which can be controlled with sufficient accuracy.","Using a single Rydberg atom as an example, we associate the integer values to electronic states belonging to different manifolds and implement a selective superposition of these different states to solve the full IP problem.","The optimal solution is found within 2-40{\\mu}s for a few prototypical IP problems with up to eight variables and up to four constraints including a non-linear IP problem, which is usually harder to solve with classical algorithms when compared with linear IP problems.","Our algorithm for solving IP is benchmarked using the Branch & Bound approach and it outperforms the classical algorithm in terms of the number of steps needed to converge and carries the potential to improve the bounds provided by the classical algorithm for larger problems."],"url":"http://arxiv.org/abs/2402.16541v1","category":"quant-ph"}
{"created":"2024-02-26 12:57:58","title":"Quasi Directed Jonsson Operations Imply Bounded Width (For fo-expansions of symmetric binary cores with free amalgamation)","abstract":"Every CSP(B) for a finite structure B is either in P or it is NP-complete but the proofs of the finite-domain CSP dichotomy by Andrei Bulatov and Dimitryi Zhuk not only show the computational complexity separation but also confirm the algebraic tractability conjecture stating that tractability origins from a certain system of operations preserving B. The establishment of the dichotomy was in fact preceded by a number of similar results for stronger conditions of this type, i.e. for system of operations covering not necessarily all tractable finite-domain CSPs.   A similar, infinite-domain algebraic tractability conjecture is known for first-order reducts of countably infinite finitely bounded homogeneous structures and is currently wide open. In particular, with an exception of a quasi near-unanimity operation there are no known systems of operations implying tractability in this regime. This paper changes the state-of-the-art and provides a proof that a chain of quasi directed Jonsson operations imply tractability and bounded width for a large and natural class of infinite structures.","sentences":["Every CSP(B) for a finite structure B is either in P or it is NP-complete but the proofs of the finite-domain CSP dichotomy by Andrei Bulatov and Dimitryi Zhuk not only show the computational complexity separation but also confirm the algebraic tractability conjecture stating that tractability origins from a certain system of operations preserving B.","The establishment of the dichotomy was in fact preceded by a number of similar results for stronger conditions of this type, i.e. for system of operations covering not necessarily all tractable finite-domain CSPs.   ","A similar, infinite-domain algebraic tractability conjecture is known for first-order reducts of countably infinite finitely bounded homogeneous structures and is currently wide open.","In particular, with an exception of a quasi near-unanimity operation there are no known systems of operations implying tractability in this regime.","This paper changes the state-of-the-art and provides a proof that a chain of quasi directed Jonsson operations imply tractability and bounded width for a large and natural class of infinite structures."],"url":"http://arxiv.org/abs/2402.16540v1","category":"cs.LO"}
{"created":"2024-02-26 12:45:59","title":"Global well-posedness of the 3D Patlak-Keller-Segel system near a straight line","abstract":"We consider the Cauchy problem of the three-dimensional parabolic-elliptic Patlak-Keller-Segel chemotactic model. The initial data is almost a Dirac measure supported on a straight line with mass less than $8\\pi$. We prove that if the data is sufficiently close to the straight line, then global well-posedness holds. This result is parallel to the work on vortex filament solutions of the Navier-Stokes equations by Bedrossian, Germain and Harrop-Griffiths \\cite{filament}.","sentences":["We consider the Cauchy problem of the three-dimensional parabolic-elliptic Patlak-Keller-Segel chemotactic model.","The initial data is almost a Dirac measure supported on a straight line with mass less than $8\\pi$. We prove that if the data is sufficiently close to the straight line, then global well-posedness holds.","This result is parallel to the work on vortex filament solutions of the Navier-Stokes equations by Bedrossian, Germain and Harrop-Griffiths \\cite{filament}."],"url":"http://arxiv.org/abs/2402.16536v1","category":"math.AP"}
{"created":"2024-02-26 12:38:51","title":"Weak-linearity, globality and in-place update","abstract":"Computational interpretations of linear logic allow static control of memory resources: the data produced by the program are endowed through its type with attributes that determine its life cycle. This has promoted numerous investigations into safe introduction of in-place update. Various type systems have been proposed for this aim, but linearity and correctness of in-place update are properties that are not fully compatible. The main achievement of this work is to establish a simple theoretical framework that will allow us to clarify the potential (and limits) of linearity to guarantee the process of transforming a functional program into an imperative one.","sentences":["Computational interpretations of linear logic allow static control of memory resources: the data produced by the program are endowed through its type with attributes that determine its life cycle.","This has promoted numerous investigations into safe introduction of in-place update.","Various type systems have been proposed for this aim, but linearity and correctness of in-place update are properties that are not fully compatible.","The main achievement of this work is to establish a simple theoretical framework that will allow us to clarify the potential (and limits) of linearity to guarantee the process of transforming a functional program into an imperative one."],"url":"http://arxiv.org/abs/2402.16534v1","category":"cs.PL"}
{"created":"2024-02-26 12:17:57","title":"Discovery of magnetically guided metal accretion onto a polluted white dwarf","abstract":"Dynamically active planetary systems orbit a significant fraction of white dwarf stars. These stars often exhibit surface metals accreted from debris disks, which are detected through infrared excess or transiting structures. However, the full journey of a planetesimal from star-grazing orbit to final dissolution in the host star is poorly understood. Here, we report the discovery that the cool metal polluted star WD0816-310 has cannibalized heavy elements from a planetary body similar in size to Vesta, and where accretion and horizontal mixing processes have clearly been controlled by the stellar magnetic field. Our observations unveil periodic and synchronized variations in metal line strength and magnetic field intensity, implying a correlation between the local surface density of metals and the magnetic field structure. Specifically, the data point to a likely persistent concentration of metals near a magnetic pole. These findings demonstrate that magnetic fields may play a fundamental role in the final stages of exoplanetary bodies that are recycled into their white dwarf hosts.","sentences":["Dynamically active planetary systems orbit a significant fraction of white dwarf stars.","These stars often exhibit surface metals accreted from debris disks, which are detected through infrared excess or transiting structures.","However, the full journey of a planetesimal from star-grazing orbit to final dissolution in the host star is poorly understood.","Here, we report the discovery that the cool metal polluted star WD0816-310 has cannibalized heavy elements from a planetary body similar in size to Vesta, and where accretion and horizontal mixing processes have clearly been controlled by the stellar magnetic field.","Our observations unveil periodic and synchronized variations in metal line strength and magnetic field intensity, implying a correlation between the local surface density of metals and the magnetic field structure.","Specifically, the data point to a likely persistent concentration of metals near a magnetic pole.","These findings demonstrate that magnetic fields may play a fundamental role in the final stages of exoplanetary bodies that are recycled into their white dwarf hosts."],"url":"http://arxiv.org/abs/2402.16526v1","category":"astro-ph.SR"}
{"created":"2024-02-26 12:13:06","title":"Uniform large deviations and metastability of random dynamical systems","abstract":"In this paper, we first provide a criterion on uniform large deviation principles (ULDP) of stochastic differential equations under Lyapunov conditions on the coefficients, which can be applied to stochastic systems with coefficients of polynomial growth and possible degenerate driving noises. In the second part, using the ULDP criterion we preclude the concentration of limiting measures of invariant measures of stochastic dynamical systems on repellers and acyclic saddle chains and extend Freidlin and Wentzell's asymptotics theorem to stochastic systems with unbounded coefficients. Of particular interest, we determine the limiting measures of the invariant measures of the famous stochastic van der Pol equation and van der Pol Duffing equation whose noises are naturally degenerate. We also construct two examples to match the global phase portraits of Freidlin and Wentzell's unperturbed systems and to explicitly compute their transition difficulty matrices. Other applications include stochastic May-Leonard system and random systems with infinitely many equivalent classes.","sentences":["In this paper, we first provide a criterion on uniform large deviation principles (ULDP) of stochastic differential equations under Lyapunov conditions on the coefficients, which can be applied to stochastic systems with coefficients of polynomial growth and possible degenerate driving noises.","In the second part, using the ULDP criterion we preclude the concentration of limiting measures of invariant measures of stochastic dynamical systems on repellers and acyclic saddle chains and extend Freidlin and Wentzell's asymptotics theorem to stochastic systems with unbounded coefficients.","Of particular interest, we determine the limiting measures of the invariant measures of the famous stochastic van der Pol equation and van der Pol Duffing equation whose noises are naturally degenerate.","We also construct two examples to match the global phase portraits of Freidlin and Wentzell's unperturbed systems and to explicitly compute their transition difficulty matrices.","Other applications include stochastic May-Leonard system and random systems with infinitely many equivalent classes."],"url":"http://arxiv.org/abs/2402.16522v1","category":"math.PR"}
{"created":"2024-02-26 11:49:15","title":"Periodically driven thermodynamic systems under vanishingly small viscous drives","abstract":"Periodically driven thermodynamic systems support stable non-equilibrium oscillating states with properties drastically different from equilibrium. They exhibit even more exotic features for low viscous drives, which is a regime that is hard to probe due to singular behavior of the underlying Langevin dynamics near vanishing viscosity. We propose a method, based on singular perturbation and Floquet theories, that allows us to obtain oscillating states in this limit. We then find two distinct classes of distributions, each exhibiting interesting features that can be exploited for a range of practical applicability, including cooling a system and triggering chemical reactions through weakly interacting driven environments.","sentences":["Periodically driven thermodynamic systems support stable non-equilibrium oscillating states with properties drastically different from equilibrium.","They exhibit even more exotic features for low viscous drives, which is a regime that is hard to probe due to singular behavior of the underlying Langevin dynamics near vanishing viscosity.","We propose a method, based on singular perturbation and Floquet theories, that allows us to obtain oscillating states in this limit.","We then find two distinct classes of distributions, each exhibiting interesting features that can be exploited for a range of practical applicability, including cooling a system and triggering chemical reactions through weakly interacting driven environments."],"url":"http://arxiv.org/abs/2402.16512v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-26 11:46:17","title":"Scaling and flow profiles in magnetically confined liquid-in-liquid channels","abstract":"Ferrofluids kept in place by permanent magnet quadrupoles can act as liquid walls to surround a second non-magnetic inside, resulting in a liquid fluidic channel with diameter size ranging from mm down to less than 10 micrometer. Micro particle tracking velocimetry (micro PTV) experiments and modeling show that near ideal plug flow is possible in such liquid-in-liquid channels due to the reduced friction at the walls. The measured fluids velocity profiles agree with the predictions of a hydrodynamic model of cylindrical symmetry with a minimal set of hypotheses. By introducing symmetry breaking elements in the system, we show how unique velocity and flow properties can be obtained. Our liquid-in-liquid confinement opens new possibilities for < 10 micrometer-sized microfluidics with low pressures and low shear, with flow characteristics not attainable in comparable solid-wall devices.","sentences":["Ferrofluids kept in place by permanent magnet quadrupoles can act as liquid walls to surround a second non-magnetic inside, resulting in a liquid fluidic channel with diameter size ranging from mm down to less than 10 micrometer.","Micro particle tracking velocimetry (micro PTV) experiments and modeling show that near ideal plug flow is possible in such liquid-in-liquid channels due to the reduced friction at the walls.","The measured fluids velocity profiles agree with the predictions of a hydrodynamic model of cylindrical symmetry with a minimal set of hypotheses.","By introducing symmetry breaking elements in the system, we show how unique velocity and flow properties can be obtained.","Our liquid-in-liquid confinement opens new possibilities for < 10 micrometer-sized microfluidics with low pressures and low shear, with flow characteristics not attainable in comparable solid-wall devices."],"url":"http://arxiv.org/abs/2402.16510v1","category":"physics.flu-dyn"}
{"created":"2024-02-26 13:24:39","title":"Exact relations between the conductivities and their connection to the chemical composition of QCD matter","abstract":"We present exact relations between the diffusion coefficients or conductivities, $\\kappa_{qq'}/T = \\sigma_{qq'}$, of strongly-interacting matter. We show that once the diagonal entries are known in two different charge representations, the off-diagonal coefficients are functions of the diagonal entries once isospin symmetry applies. As an important example, we infer the conductivities on the basis of available calculations from lattice quantum chromodynamics (LQCD) and argue that these computations suffer under the approximations made to achieve them. Further, we argue that the representation of the conductivities w.r.t. to the conserved quark-flavors may deliver more insight into the chemical composition of strongly-interacting matter.","sentences":["We present exact relations between the diffusion coefficients or conductivities, $\\kappa_{qq'}/T = \\sigma_{qq'}$, of strongly-interacting matter.","We show that once the diagonal entries are known in two different charge representations, the off-diagonal coefficients are functions of the diagonal entries once isospin symmetry applies.","As an important example, we infer the conductivities on the basis of available calculations from lattice quantum chromodynamics (LQCD) and argue that these computations suffer under the approximations made to achieve them.","Further, we argue that the representation of the conductivities w.r.t.","to the conserved quark-flavors may deliver more insight into the chemical composition of strongly-interacting matter."],"url":"http://arxiv.org/abs/2402.16552v1","category":"hep-ph"}
{"created":"2024-02-26 13:03:26","title":"Label Learning Method Based on Tensor Projection","abstract":"Multi-view clustering method based on anchor graph has been widely concerned due to its high efficiency and effectiveness. In order to avoid post-processing, most of the existing anchor graph-based methods learn bipartite graphs with connected components. However, such methods have high requirements on parameters, and in some cases it may not be possible to obtain bipartite graphs with clear connected components. To end this, we propose a label learning method based on tensor projection (LLMTP). Specifically, we project anchor graph into the label space through an orthogonal projection matrix to obtain cluster labels directly. Considering that the spatial structure information of multi-view data may be ignored to a certain extent when projected in different views separately, we extend the matrix projection transformation to tensor projection, so that the spatial structure information between views can be fully utilized. In addition, we introduce the tensor Schatten $p$-norm regularization to make the clustering label matrices of different views as consistent as possible. Extensive experiments have proved the effectiveness of the proposed method.","sentences":["Multi-view clustering method based on anchor graph has been widely concerned due to its high efficiency and effectiveness.","In order to avoid post-processing, most of the existing anchor graph-based methods learn bipartite graphs with connected components.","However, such methods have high requirements on parameters, and in some cases it may not be possible to obtain bipartite graphs with clear connected components.","To end this, we propose a label learning method based on tensor projection (LLMTP).","Specifically, we project anchor graph into the label space through an orthogonal projection matrix to obtain cluster labels directly.","Considering that the spatial structure information of multi-view data may be ignored to a certain extent when projected in different views separately, we extend the matrix projection transformation to tensor projection, so that the spatial structure information between views can be fully utilized.","In addition, we introduce the tensor Schatten $p$-norm regularization to make the clustering label matrices of different views as consistent as possible.","Extensive experiments have proved the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2402.16544v1","category":"cs.LG"}
{"created":"2024-02-26 11:28:53","title":"An NS-NS basis for odd-parity couplings at order $\u03b1'^3$","abstract":"In this study, we thoroughly investigate the covariant and $B$-field gauge invariant odd-parity NS-NS couplings at order $\\alpha'^3$, while considering the removal of field redefinitions, Bianchi identities, and total derivative freedoms. Our comprehensive analysis reveals the existence of 477 independent couplings. To establish a specific basis, we construct it in such a way that none of the couplings contain terms involving structures such as $R$, $R_{\\mu\\nu}$, $\\nabla_\\mu H^{\\mu\\alpha\\beta}$, $\\nabla_\\mu\\nabla^\\mu\\Phi$, or terms with more than two derivatives, except for one term that possesses three derivatives on $H$. Interestingly, the mentioned coupling with the four-derivative on the $B$-field is rendered zero by the sphere-level three-point S-matrix element.   Furthermore, we demonstrate that the remaining 476 parameters in type II superstring theory are fixed to zero by imposing the requirement that the circular reduction of the couplings remains invariant under $O(1,1,\\MZ)$ T-duality transformations. This result is consistent with our expectations and highlights the crucial role played by the $O(1,1,\\MZ)$ symmetry in constraining the parameter space of the classical effective actions in string theory.","sentences":["In this study, we thoroughly investigate the covariant and $B$-field gauge invariant odd-parity NS-NS couplings at order $\\alpha'^3$, while considering the removal of field redefinitions, Bianchi identities, and total derivative freedoms.","Our comprehensive analysis reveals the existence of 477 independent couplings.","To establish a specific basis, we construct it in such a way that none of the couplings contain terms involving structures such as $R$, $R_{\\mu\\nu}$, $\\nabla_\\mu H^{\\mu\\alpha\\beta}$, $\\nabla_\\mu\\nabla^\\mu\\Phi$, or terms with more than two derivatives, except for one term that possesses three derivatives on $H$. Interestingly, the mentioned coupling with the four-derivative on the $B$-field is rendered zero by the sphere-level three-point S-matrix element.   ","Furthermore, we demonstrate that the remaining 476 parameters in type II superstring theory are fixed to zero by imposing the requirement that the circular reduction of the couplings remains invariant under $O(1,1,\\MZ)$ T-duality transformations.","This result is consistent with our expectations and highlights the crucial role played by the $O(1,1,\\MZ)$ symmetry in constraining the parameter space of the classical effective actions in string theory."],"url":"http://arxiv.org/abs/2402.16496v1","category":"hep-th"}
{"created":"2024-02-26 10:54:26","title":"Edge Detectors Can Make Deep Convolutional Neural Networks More Robust","abstract":"Deep convolutional neural networks (DCNN for short) are vulnerable to examples with small perturbations. Improving DCNN's robustness is of great significance to the safety-critical applications, such as autonomous driving and industry automation. Inspired by the principal way that human eyes recognize objects, i.e., largely relying on the shape features, this paper first employs the edge detectors as layer kernels and designs a binary edge feature branch (BEFB for short) to learn the binary edge features, which can be easily integrated into any popular backbone. The four edge detectors can learn the horizontal, vertical, positive diagonal, and negative diagonal edge features, respectively, and the branch is stacked by multiple Sobel layers (using edge detectors as kernels) and one threshold layer. The binary edge features learned by the branch, concatenated with the texture features learned by the backbone, are fed into the fully connected layers for classification. We integrate the proposed branch into VGG16 and ResNet34, respectively, and conduct experiments on multiple datasets. Experimental results demonstrate the BEFB is lightweight and has no side effects on training. And the accuracy of the BEFB integrated models is better than the original ones on all datasets when facing FGSM, PGD, and C\\&W attacks. Besides, BEFB integrated models equipped with the robustness enhancing techniques can achieve better classification accuracy compared to the original models. The work in this paper for the first time shows it is feasible to enhance the robustness of DCNNs through combining both shape-like features and texture features.","sentences":["Deep convolutional neural networks (DCNN for short) are vulnerable to examples with small perturbations.","Improving DCNN's robustness is of great significance to the safety-critical applications, such as autonomous driving and industry automation.","Inspired by the principal way that human eyes recognize objects, i.e., largely relying on the shape features, this paper first employs the edge detectors as layer kernels and designs a binary edge feature branch (BEFB for short) to learn the binary edge features, which can be easily integrated into any popular backbone.","The four edge detectors can learn the horizontal, vertical, positive diagonal, and negative diagonal edge features, respectively, and the branch is stacked by multiple Sobel layers (using edge detectors as kernels) and one threshold layer.","The binary edge features learned by the branch, concatenated with the texture features learned by the backbone, are fed into the fully connected layers for classification.","We integrate the proposed branch into VGG16 and ResNet34, respectively, and conduct experiments on multiple datasets.","Experimental results demonstrate the BEFB is lightweight and has no side effects on training.","And the accuracy of the BEFB integrated models is better than the original ones on all datasets when facing FGSM, PGD, and C\\&W attacks.","Besides, BEFB integrated models equipped with the robustness enhancing techniques can achieve better classification accuracy compared to the original models.","The work in this paper for the first time shows it is feasible to enhance the robustness of DCNNs through combining both shape-like features and texture features."],"url":"http://arxiv.org/abs/2402.16479v1","category":"cs.CV"}
{"created":"2024-02-26 10:48:41","title":"Spin Effect induced Momentum Spiral and Asymmetry Degree in Pair Production","abstract":"Spin effect on the pair production under circularly polarized fields are investigated. Significantly different from what momentum spirals caused by two counter-rotating fields with a time delay, we find for the first time that the spirals can also be induced due to the particles spin effect even if in a single field. We further examine the bichromatic combinational fields, the inhomogeneous spiral structures can be observed in the momentum spectrum, in particular, the spiral not only does exist in two cases of spin but also is about two orders of magnitude amplifier than that in the single field. Meanwhile, the spin asymmetry degree on the momentum distributions is investigated and found that there exist the effect of spin flip with increasing time delay between two fields. The spin asymmetry degree on the number density can reach to $98\\%$ in a certain of condition. These results indicate that the signatures of created particles, especially the spiral structures are strongly associated with the information of laser field as well as the created particle spin, which can deepen the understanding of vacuum pair production.","sentences":["Spin effect on the pair production under circularly polarized fields are investigated.","Significantly different from what momentum spirals caused by two counter-rotating fields with a time delay, we find for the first time that the spirals can also be induced due to the particles spin effect even if in a single field.","We further examine the bichromatic combinational fields, the inhomogeneous spiral structures can be observed in the momentum spectrum, in particular, the spiral not only does exist in two cases of spin but also is about two orders of magnitude amplifier than that in the single field.","Meanwhile, the spin asymmetry degree on the momentum distributions is investigated and found that there exist the effect of spin flip with increasing time delay between two fields.","The spin asymmetry degree on the number density can reach to $98\\%$ in a certain of condition.","These results indicate that the signatures of created particles, especially the spiral structures are strongly associated with the information of laser field as well as the created particle spin, which can deepen the understanding of vacuum pair production."],"url":"http://arxiv.org/abs/2402.16476v1","category":"hep-ph"}
{"created":"2024-02-26 09:56:48","title":"Multiscale Experiments and Predictive Modelling for Inverse Design and Failure Mitigation in Additively Manufactured Lattices","abstract":"Additive manufacturing (AM) enables the development of high-performance architected cellular materials, emphasizing the growing importance of establishing programmable and predictable energy absorption capabilities. This study evaluates the impact of a precisely tuned fused filament fabrication (FFF) AM process on the energy absorption and failure characteristics of thermoplastic lattice materials through multiscale experiments and predictive modelling. Lattices with four distinct unit cell topologies and three varying relative densities are manufactured, and their in-plane mechanical response under quasi-static compression is measured. Macroscale testing and micro-CT imaging reveal relative density-dependent damage mechanisms and failure modes, prompting the development of a robust predictive modelling framework to capture process-induced performance variation and damage. For lower relative density lattices, an FE model based on the extended Drucker-Prager material model, incorporating Bridgman correction with crazing failure criteria, accurately captures the crushing response. As lattice density increases, interfacial damage along bead-bead interfaces becomes predominant, necessitating the enrichment of the model with a microscale cohesive zone model to capture interfacial debonding. All proposed models are validated, highlighting inter-bead damage as the primary factor limiting energy absorption performance in FFF-printed lattices. Finally, the predictive modelling introduces an enhancement factor, providing a straightforward approach to assess the influence of the AM process on energy absorption performance, facilitating the inverse design of FFF-printed lattices. This approach enables a critical evaluation of how FFF processes can be improved to achieve the highest attainable performance and mitigate failures in architected cellular materials.","sentences":["Additive manufacturing (AM) enables the development of high-performance architected cellular materials, emphasizing the growing importance of establishing programmable and predictable energy absorption capabilities.","This study evaluates the impact of a precisely tuned fused filament fabrication (FFF) AM process on the energy absorption and failure characteristics of thermoplastic lattice materials through multiscale experiments and predictive modelling.","Lattices with four distinct unit cell topologies and three varying relative densities are manufactured, and their in-plane mechanical response under quasi-static compression is measured.","Macroscale testing and micro-CT imaging reveal relative density-dependent damage mechanisms and failure modes, prompting the development of a robust predictive modelling framework to capture process-induced performance variation and damage.","For lower relative density lattices, an FE model based on the extended Drucker-Prager material model, incorporating Bridgman correction with crazing failure criteria, accurately captures the crushing response.","As lattice density increases, interfacial damage along bead-bead interfaces becomes predominant, necessitating the enrichment of the model with a microscale cohesive zone model to capture interfacial debonding.","All proposed models are validated, highlighting inter-bead damage as the primary factor limiting energy absorption performance in FFF-printed lattices.","Finally, the predictive modelling introduces an enhancement factor, providing a straightforward approach to assess the influence of the AM process on energy absorption performance, facilitating the inverse design of FFF-printed lattices.","This approach enables a critical evaluation of how FFF processes can be improved to achieve the highest attainable performance and mitigate failures in architected cellular materials."],"url":"http://arxiv.org/abs/2402.16452v1","category":"physics.app-ph"}
{"created":"2024-02-26 09:36:31","title":"Solving Nonlinear Absolute Value Equations","abstract":"In this work we show that several problems naturally modeled as Nonlinear Absolute Value Equations (NAVE), can be restated as Nonlinear Complementarity Problems (NCP) and solved efficiently using smoothing regularizing techniques under mild assumptions. Applications include ridge optimization and resolution of nonlinear ordinary differential equations.","sentences":["In this work we show that several problems naturally modeled as Nonlinear Absolute Value Equations (NAVE), can be restated as Nonlinear Complementarity Problems (NCP) and solved efficiently using smoothing regularizing techniques under mild assumptions.","Applications include ridge optimization and resolution of nonlinear ordinary differential equations."],"url":"http://arxiv.org/abs/2402.16439v1","category":"math.OC"}
{"created":"2024-02-26 09:30:55","title":"RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions","abstract":"Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (\\textit{adversarial context method}) to further boost the robustness of the LLMs. Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural language instructions. For example, with gpt-3.5-turbo, our method achieves an improvement of 5.68\\% in test set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).","sentences":["Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions.","However, recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples.","In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions.","Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs.","Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (\\textit{adversarial context method}) to further boost the robustness of the LLMs.","Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural language instructions.","For example, with gpt-3.5-turbo, our method achieves an improvement of 5.68\\% in test set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR)."],"url":"http://arxiv.org/abs/2402.16431v1","category":"cs.CL"}
{"created":"2024-02-26 09:27:05","title":"Electronic phase transitions and superconductivity in ferroelectric Sn$_2$P$_2$Se$_6$ under pressure","abstract":"Since there is both strong electron-phonon coupling during a ferroelectric/FE transition and superconducting/SC transition, it has been an important topic to explore superconductivity from the FE instability. Sn$_2$P$_2$Se$_6$ arouses broad attention due to its unique FE properties. Here, we reported the electronic phase transitions and superconductivity in this compound based on high-pressure electrical transport measurement, optical absorption spectroscopy and Raman based structural analysis. Upon compression, the conductivity of Sn$_2$P$_2$Se$_6$ was elevated monotonously, an electronic phase transition occurred near 5.4 GPa, revealed by optical absorption spectroscopy, and the insulating state is estimated to be fully suppressed near 15 GPa. Then, it started to show the signature of superconductivity near 15.3 GPa. The zero-resistance state was presented from 19.4 GPa, and the superconductivity was enhanced with pressure continuously. The magnetic field effect further confirmed the SC behavior and this compound had a $T_c$ of 5.4 K at 41.8 GPa with a zero temperature upper critical field of 6.55 T. The Raman spectra confirmed the structural origin of the electronic transition near 5.4 GPa, which should due to the transition from the paraelectric phase to the incommensurate phase, and suggested a possible first-order phase transition when the sample underwent the semiconductor-metal transition near 15 GPa. This work demonstrates the versatile physical properties in ferroelectrics and inspires the further investigation on the correlation between FE instability and SC in M$_2$P$_2$X$_6$ family.","sentences":["Since there is both strong electron-phonon coupling during a ferroelectric/FE transition and superconducting/SC transition, it has been an important topic to explore superconductivity from the FE instability.","Sn$_2$P$_2$Se$_6$ arouses broad attention due to its unique FE properties.","Here, we reported the electronic phase transitions and superconductivity in this compound based on high-pressure electrical transport measurement, optical absorption spectroscopy and Raman based structural analysis.","Upon compression, the conductivity of Sn$_2$P$_2$Se$_6$ was elevated monotonously, an electronic phase transition occurred near 5.4 GPa, revealed by optical absorption spectroscopy, and the insulating state is estimated to be fully suppressed near 15 GPa.","Then, it started to show the signature of superconductivity near 15.3 GPa.","The zero-resistance state was presented from 19.4 GPa, and the superconductivity was enhanced with pressure continuously.","The magnetic field effect further confirmed the SC behavior and this compound had a $T_c$ of 5.4 K at 41.8 GPa with a zero temperature upper critical field of 6.55 T. The Raman spectra confirmed the structural origin of the electronic transition near 5.4 GPa, which should due to the transition from the paraelectric phase to the incommensurate phase, and suggested a possible first-order phase transition when the sample underwent the semiconductor-metal transition near 15 GPa.","This work demonstrates the versatile physical properties in ferroelectrics and inspires the further investigation on the correlation between FE instability and SC in M$_2$P$_2$X$_6$ family."],"url":"http://arxiv.org/abs/2402.16427v1","category":"cond-mat.supr-con"}
{"created":"2024-02-26 09:05:13","title":"Renormalisation Group Methods for Effective Epidemiological Models","abstract":"Epidemiological models describe the spread of an infectious disease within a population. They capture microscopic details on how the disease is passed on among individuals in various different ways, while making predictions about the state of the entirety of the population. However, the type and structure of the specific model considered typically depend on the size of the population under consideration. To analyse this effect, we study a family of effective epidemiological models in space and time that are related to each other through scaling transformations. Inspired by a similar treatment of diffusion processes, we interpret the latter as renormalisation group transformations, both at the level of the underlying differential equations and their solutions. We show that in the large scale limit, the microscopic details of the infection process become irrelevant, safe for a simple real number, which plays the role of the infection rate in a basic compartmental model.","sentences":["Epidemiological models describe the spread of an infectious disease within a population.","They capture microscopic details on how the disease is passed on among individuals in various different ways, while making predictions about the state of the entirety of the population.","However, the type and structure of the specific model considered typically depend on the size of the population under consideration.","To analyse this effect, we study a family of effective epidemiological models in space and time that are related to each other through scaling transformations.","Inspired by a similar treatment of diffusion processes, we interpret the latter as renormalisation group transformations, both at the level of the underlying differential equations and their solutions.","We show that in the large scale limit, the microscopic details of the infection process become irrelevant, safe for a simple real number, which plays the role of the infection rate in a basic compartmental model."],"url":"http://arxiv.org/abs/2402.16409v1","category":"q-bio.PE"}
{"created":"2024-02-26 06:59:27","title":"On convergence of forecasts in prediction markets","abstract":"We propose a dynamic model of a prediction market in which agents predict the values of a sequence of random vectors. The main result shows that if there are agents who make correct (or asymptotically correct) next-period forecasts, then the aggregated market forecasts converge to the next-period conditional expectations of the random vectors.","sentences":["We propose a dynamic model of a prediction market in which agents predict the values of a sequence of random vectors.","The main result shows that if there are agents who make correct (or asymptotically correct) next-period forecasts, then the aggregated market forecasts converge to the next-period conditional expectations of the random vectors."],"url":"http://arxiv.org/abs/2402.16345v1","category":"math.PR"}
{"created":"2024-02-26 04:43:50","title":"Conformalized Selective Regression","abstract":"Should prediction models always deliver a prediction? In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty. Selective regression, also known as the \"reject option,\" allows models to abstain from predictions in cases of considerable uncertainty. Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance. However, this focus neglects the significant influence of model-specific biases on a model's performance. In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases. In addition, we propose a standardized evaluation framework to allow proper comparison of selective regression approaches. Via an extensive experimental approach, we demonstrate how our proposed approach, conformalized selective regression, demonstrates an advantage over multiple state-of-the-art baselines.","sentences":["Should prediction models always deliver a prediction?","In the pursuit of maximum predictive performance, critical considerations of reliability and fairness are often overshadowed, particularly when it comes to the role of uncertainty.","Selective regression, also known as the \"reject option,\" allows models to abstain from predictions in cases of considerable uncertainty.","Initially proposed seven decades ago, approaches to selective regression have mostly focused on distribution-based proxies for measuring uncertainty, particularly conditional variance.","However, this focus neglects the significant influence of model-specific biases on a model's performance.","In this paper, we propose a novel approach to selective regression by leveraging conformal prediction, which provides grounded confidence measures for individual predictions based on model-specific biases.","In addition, we propose a standardized evaluation framework to allow proper comparison of selective regression approaches.","Via an extensive experimental approach, we demonstrate how our proposed approach, conformalized selective regression, demonstrates an advantage over multiple state-of-the-art baselines."],"url":"http://arxiv.org/abs/2402.16300v1","category":"cs.LG"}
{"created":"2024-02-26 04:06:05","title":"A Comparison of Deep Learning Models for Proton Background Rejection with the AMS Electromagnetic Calorimeter","abstract":"The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector onboard the International Space Station containing six different subdetectors. The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are used to separate electrons/positrons from the abundant cosmic-ray proton background.   The positron flux measured in space by AMS falls with a power law which unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several theoretical models try to explain these phenomena, and a purer measurement of positrons at higher energies is needed to help test them. The currently used methods to reject the proton background at high energies involve extrapolating shower features from the ECAL to use as inputs for boosted decision tree and likelihood classifiers. We present a new approach for particle identification with the AMS ECAL using deep learning (DL). By taking the energy deposition within all the ECAL cells as an input and treating them as pixels in an image-like format, we train an MLP, a CNN, and multiple ResNets and Convolutional vision Transformers (CvTs) as shower classifiers.   Proton rejection performance is evaluated using Monte Carlo (MC) events and ISS data separately. For MC, using events with a reconstructed energy between 0.2 - 2 TeV, at 90% electron accuracy, the proton rejection power of our CvT model is more than 5 times that of the other DL models. Similarly, for ISS data with a reconstructed energy between 50 - 70 GeV, the proton rejection power of our CvT model is more than 2.5 times that of the other DL models.","sentences":["The Alpha Magnetic Spectrometer (AMS) is a high-precision particle detector onboard the International Space Station containing six different subdetectors.","The Transition Radiation Detector and Electromagnetic Calorimeter (ECAL) are used to separate electrons/positrons from the abundant cosmic-ray proton background.   ","The positron flux measured in space by AMS falls with a power law which unexpectedly softens above 25 GeV and then hardens above 280 GeV. Several theoretical models try to explain these phenomena, and a purer measurement of positrons at higher energies is needed to help test them.","The currently used methods to reject the proton background at high energies involve extrapolating shower features from the ECAL to use as inputs for boosted decision tree and likelihood classifiers.","We present a new approach for particle identification with the AMS ECAL using deep learning (DL).","By taking the energy deposition within all the ECAL cells as an input and treating them as pixels in an image-like format, we train an MLP, a CNN, and multiple ResNets and Convolutional vision Transformers (CvTs) as shower classifiers.   ","Proton rejection performance is evaluated using Monte Carlo (MC) events and ISS data separately.","For MC, using events with a reconstructed energy between 0.2 - 2 TeV, at 90% electron accuracy, the proton rejection power of our CvT model is more than 5 times that of the other DL models.","Similarly, for ISS data with a reconstructed energy between 50 - 70 GeV, the proton rejection power of our CvT model is more than 2.5 times that of the other DL models."],"url":"http://arxiv.org/abs/2402.16285v1","category":"hep-ex"}
{"created":"2024-02-26 03:08:01","title":"Infrared and visible Image Fusion with Language-driven Loss in CLIP Embedding Space","abstract":"Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities. Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically. As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited. In this paper, we first propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance. For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the multi-modal embedding space using CLIP. A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors to represent the fusion objective and input image modalities. Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via supervised training. Experiments show that our method can obtain much better fusion results than existing techniques.","sentences":["Infrared-visible image fusion (IVIF) has attracted much attention owing to the highly-complementary properties of the two image modalities.","Due to the lack of ground-truth fused images, the fusion output of current deep-learning based methods heavily depends on the loss functions defined mathematically.","As it is hard to well mathematically define the fused image without ground truth, the performance of existing fusion methods is limited.","In this paper, we first propose to use natural language to express the objective of IVIF, which can avoid the explicit mathematical modeling of fusion output in current losses, and make full use of the advantage of language expression to improve the fusion performance.","For this purpose, we present a comprehensive language-expressed fusion objective, and encode relevant texts into the multi-modal embedding space using CLIP.","A language-driven fusion model is then constructed in the embedding space, by establishing the relationship among the embedded vectors to represent the fusion objective and input image modalities.","Finally, a language-driven loss is derived to make the actual IVIF aligned with the embedded language-driven fusion model via supervised training.","Experiments show that our method can obtain much better fusion results than existing techniques."],"url":"http://arxiv.org/abs/2402.16267v1","category":"cs.CV"}
{"created":"2024-02-26 02:42:41","title":"A novel method for determining the resistivity of compressed superconducting materials","abstract":"The resistivity of a superconductor in its normal state plays a critical role in determining its superconducting ground state. However, measuring the resistivity of a material under high pressure has long presented a significant technical challenge due to pressure-induced changes in the crystallographic directions, especially for samples with anisotropic layered structures like high-Tc superconductors and other intriguing quantum materials. Here, we are the first to propose a novel and effective method for determining high-pressure resistivity, which relies on the ambient-pressure resistivity, initial sample sizes, lattice parameters, high-pressure resistance, and lattice parameters measured from the same sample. Its validity has been confirmed through our investigations of pressurized copper-oxide superconductors, which demonstrates that this method provides new possibilities for researchers conducting high-pressure studies related to resistivity of these materials.","sentences":["The resistivity of a superconductor in its normal state plays a critical role in determining its superconducting ground state.","However, measuring the resistivity of a material under high pressure has long presented a significant technical challenge due to pressure-induced changes in the crystallographic directions, especially for samples with anisotropic layered structures like high-Tc superconductors and other intriguing quantum materials.","Here, we are the first to propose a novel and effective method for determining high-pressure resistivity, which relies on the ambient-pressure resistivity, initial sample sizes, lattice parameters, high-pressure resistance, and lattice parameters measured from the same sample.","Its validity has been confirmed through our investigations of pressurized copper-oxide superconductors, which demonstrates that this method provides new possibilities for researchers conducting high-pressure studies related to resistivity of these materials."],"url":"http://arxiv.org/abs/2402.16257v1","category":"cond-mat.str-el"}
{"created":"2024-02-26 02:01:00","title":"High-Frequency-aware Hierarchical Contrastive Selective Coding for Representation Learning on Text-attributed Graphs","abstract":"We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information. Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs. Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture. To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model. Different from previous \"cascaded architectures\" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual enhancement between network and text signals in diverse granularities. Moreover, we show that existing contrastive objective learns the low-frequency component of the augmentation graph and propose a high-frequency component (HFC)-aware contrastive learning objective that makes the learned embeddings more distinctive. Extensive experiments on six real-world benchmarks substantiate the efficacy of our proposed approach. In addition, theoretical analysis and item embedding visualization provide insights into our model interoperability.","sentences":["We investigate node representation learning on text-attributed graphs (TAGs), where nodes are associated with text information.","Although recent studies on graph neural networks (GNNs) and pretrained language models (PLMs) have exhibited their power in encoding network and text signals, respectively, less attention has been paid to delicately coupling these two types of models on TAGs.","Specifically, existing GNNs rarely model text in each node in a contextualized way; existing PLMs can hardly be applied to characterize graph structures due to their sequence architecture.","To address these challenges, we propose HASH-CODE, a High-frequency Aware Spectral Hierarchical Contrastive Selective Coding method that integrates GNNs and PLMs into a unified model.","Different from previous \"cascaded architectures\" that directly add GNN layers upon a PLM, our HASH-CODE relies on five self-supervised optimization objectives to facilitate thorough mutual enhancement between network and text signals in diverse granularities.","Moreover, we show that existing contrastive objective learns the low-frequency component of the augmentation graph and propose a high-frequency component (HFC)-aware contrastive learning objective that makes the learned embeddings more distinctive.","Extensive experiments on six real-world benchmarks substantiate the efficacy of our proposed approach.","In addition, theoretical analysis and item embedding visualization provide insights into our model interoperability."],"url":"http://arxiv.org/abs/2402.16240v1","category":"cs.IR"}
{"created":"2024-02-25 23:49:39","title":"Orbital Stability of Soliton for the Derivative Nonlinear Schr\u00f6dinger Equation in the $L^2$ Space","abstract":"In this paper, we establish the $L^2$-orbital stability of 1-soliton solution for the derivative nonlinear Schr\\\"odinger equation under perturbations in $L^2(\\mathbb{R})$. We demonstrate this stability by utilizing the B\\\"acklund transformation associated with the spectral problem of the Lax pair and by applying the first conservation quantity in $L^2(\\mathbb{R})$.","sentences":["In this paper, we establish the $L^2$-orbital stability of 1-soliton solution for the derivative nonlinear Schr\\\"odinger equation under perturbations in $L^2(\\mathbb{R})$. We demonstrate this stability by utilizing the B\\\"acklund transformation associated with the spectral problem of the Lax pair and by applying the first conservation quantity in $L^2(\\mathbb{R})$."],"url":"http://arxiv.org/abs/2402.16222v1","category":"math.AP"}
{"created":"2024-02-25 20:27:20","title":"ARIN: Adaptive Resampling and Instance Normalization for Robust Blind Inpainting of Dunhuang Cave Paintings","abstract":"Image enhancement algorithms are very useful for real world computer vision tasks where image resolution is often physically limited by the sensor size. While state-of-the-art deep neural networks show impressive results for image enhancement, they often struggle to enhance real-world images. In this work, we tackle a real-world setting: inpainting of images from Dunhuang caves. The Dunhuang dataset consists of murals, half of which suffer from corrosion and aging. These murals feature a range of rich content, such as Buddha statues, bodhisattvas, sponsors, architecture, dance, music, and decorative patterns designed by different artists spanning ten centuries, which makes manual restoration challenging. We modify two different existing methods (CAR, HINet) that are based upon state-of-the-art (SOTA) super resolution and deblurring networks. We show that those can successfully inpaint and enhance these deteriorated cave paintings. We further show that a novel combination of CAR and HINet, resulting in our proposed inpainting network (ARIN), is very robust to external noise, especially Gaussian noise. To this end, we present a quantitative and qualitative comparison of our proposed approach with existing SOTA networks and winners of the Dunhuang challenge. One of the proposed methods HINet) represents the new state of the art and outperforms the 1st place of the Dunhuang Challenge, while our combination ARIN, which is robust to noise, is comparable to the 1st place. We also present and discuss qualitative results showing the impact of our method for inpainting on Dunhuang cave images.","sentences":["Image enhancement algorithms are very useful for real world computer vision tasks where image resolution is often physically limited by the sensor size.","While state-of-the-art deep neural networks show impressive results for image enhancement, they often struggle to enhance real-world images.","In this work, we tackle a real-world setting: inpainting of images from Dunhuang caves.","The Dunhuang dataset consists of murals, half of which suffer from corrosion and aging.","These murals feature a range of rich content, such as Buddha statues, bodhisattvas, sponsors, architecture, dance, music, and decorative patterns designed by different artists spanning ten centuries, which makes manual restoration challenging.","We modify two different existing methods (CAR, HINet) that are based upon state-of-the-art (SOTA) super resolution and deblurring networks.","We show that those can successfully inpaint and enhance these deteriorated cave paintings.","We further show that a novel combination of CAR and HINet, resulting in our proposed inpainting network (ARIN), is very robust to external noise, especially Gaussian noise.","To this end, we present a quantitative and qualitative comparison of our proposed approach with existing SOTA networks and winners of the Dunhuang challenge.","One of the proposed methods HINet) represents the new state of the art and outperforms the 1st place of the Dunhuang Challenge, while our combination ARIN, which is robust to noise, is comparable to the 1st place.","We also present and discuss qualitative results showing the impact of our method for inpainting on Dunhuang cave images."],"url":"http://arxiv.org/abs/2402.16188v1","category":"cs.CV"}
{"created":"2024-02-25 19:49:52","title":"A convergence result for a minimizing movement scheme for mean curvature flow with prescribed contact angle in a curved domain","abstract":"We consider a minimizing movement scheme of Chambolle type for the mean curvature flow equation with prescribed contact angle condition in a smooth bounded domain in $\\mathbb{R}^d$ ($d\\geq2$). We prove that an approximate solution constructed by the proposed scheme converges to the level-set mean curvature flow with prescribed contact angle provided that the domain is convex and that the contact angle is away from zero under some control of derivatives of given prescribed angle. We actually prove that an auxiliary function corresponding to the scheme uniformly converges to a unique viscosity solution to the level-set equation with an oblique {derivative} boundary condition corresponding to the prescribed boundary condition.","sentences":["We consider a minimizing movement scheme of Chambolle type for the mean curvature flow equation with prescribed contact angle condition in a smooth bounded domain in $\\mathbb{R}^d$ ($d\\geq2$).","We prove that an approximate solution constructed by the proposed scheme converges to the level-set mean curvature flow with prescribed contact angle provided that the domain is convex and that the contact angle is away from zero under some control of derivatives of given prescribed angle.","We actually prove that an auxiliary function corresponding to the scheme uniformly converges to a unique viscosity solution to the level-set equation with an oblique {derivative} boundary condition corresponding to the prescribed boundary condition."],"url":"http://arxiv.org/abs/2402.16180v1","category":"math.AP"}
{"created":"2024-02-25 19:48:22","title":"Simultaneously preperiodic points for a family of polynomials in positive characteristic","abstract":"In the goundbreaking paper [BD11] (which opened a wide avenue of research regarding unlikely intersections in arithmetic dynamics), Baker and DeMarco prove that for the family of polynomials $f_\\lambda(x):=x^d+\\lambda$ (parameterized by $\\lambda\\in\\mathbb{C}$), given two starting points $a$ and $b$ in $\\mathbb{C}$, if there exist infinitely many $\\lambda\\in\\mathbb{C}$ such that both $a$ and $b$ are preperiodic under the action of $f_\\lambda$, then $a^d=b^d$. In this paper we study the same question, this time working in a field of characteristic $p>0$. The answer in positive characteristic is more nuanced, as there are three distinct cases: (i) both starting points $a$ and $b$ live in $\\Fpbar$; (ii) $d$ is a power of $p$; and (iii) not both $a$ and $b$ live in $\\Fpbar$, while $d$ is not a power of $p$. Only in case~(iii), one derives the same conclusion as in characteristic $0$, i.e., that $a^d=b^d$. In case~(i), one has that for each $\\lambda\\in\\Fpbar$, both $a$ and $b$ are preperiodic under the action of $f_\\lambda$, while in case~(ii), one obtains that \\emph{also} whenever $a-b\\in\\Fpbar$, then for each parameter $\\lambda$, we have that $a$ is preperiodic under the action of $f_\\lambda$ if and only if $b$ is preperiodic under the action of $f_\\lambda$.","sentences":["In the goundbreaking paper [BD11] (which opened a wide avenue of research regarding unlikely intersections in arithmetic dynamics), Baker and DeMarco prove that for the family of polynomials $f_\\lambda(x):=x^d+\\lambda$ (parameterized by $\\lambda\\in\\mathbb{C}$), given two starting points $a$ and $b$ in $\\mathbb{C}$, if there exist infinitely many $\\lambda\\in\\mathbb{C}$ such that both $a$ and $b$ are preperiodic under the action of $f_\\lambda$, then $a^d=b^d$. In this paper we study the same question, this time working in a field of characteristic $p>0$. The answer in positive characteristic is more nuanced, as there are three distinct cases: (i) both starting points $a$ and $b$ live in $\\Fpbar$; (ii) $d$ is a power of $p$; and (iii) not both $a$ and $b$ live in $\\Fpbar$, while $d$ is not a power of $p$. Only in case~(iii), one derives the same conclusion as in characteristic $0$, i.e., that $a^d=b^d$. In case~(i), one has that for each $\\lambda\\in\\Fpbar$, both $a$ and $b$ are preperiodic under the action of $f_\\lambda$, while in case~(ii), one obtains that \\emph{also} whenever $a-b\\in\\Fpbar$, then for each parameter $\\lambda$, we have that $a$ is preperiodic under the action of $f_\\lambda$ if and only if $b$ is preperiodic under the action of $f_\\lambda$."],"url":"http://arxiv.org/abs/2402.16179v1","category":"math.NT"}
{"created":"2024-02-25 19:47:04","title":"Enhancing the Environmental Stability of Perovskite Thin Films via PMMA and AZ5214-Photoresist Coatings","abstract":"We introduce a pioneering strategy to enhance the environmental stability of perovskite thin films, a critical step forward in advancing their application in optoelectronics. Through the innovative application of matrix encapsulation techniques, we focus on the stabilization of methylammonium lead iodide (MAPbI3) and methylammonium lead bromide (MAPbBr3) films. These films are meticulously prepared via a two-step solution deposition method under controlled ambient conditions. Our approach involves the application of poly(methyl methacrylate) (PMMA) and AZ5214 photoresist layers through spin-coating, aimed at singularly encapsulating the perovskite films. This encapsulation acts as a robust hydrophobic barrier, significantly mitigating moisture ingress and simultaneously addressing the challenge of pinhole presence within the perovskite structure. Through a series of detailed characterizations-spanning scanning electron microscopy (SEM), X-ray diffraction (XRD), and photoluminescence (PL) spectroscopy-we demonstrate that, despite the thicker nature of the AZ5214 photoresist compared to the PMMA layer, it exhibits markedly enhanced stability. Notably, the integrity and optical properties of the perovskite films are preserved for extended periods of up to 960 hours under environmental exposure. This breakthrough highlights the superior performance of AZ5214 photoresist over PMMA in prolonging the operational life of perovskite thin films, thereby offering a promising avenue for their deployment in a wide range of optoelectronic devices.","sentences":["We introduce a pioneering strategy to enhance the environmental stability of perovskite thin films, a critical step forward in advancing their application in optoelectronics.","Through the innovative application of matrix encapsulation techniques, we focus on the stabilization of methylammonium lead iodide (MAPbI3) and methylammonium lead bromide (MAPbBr3) films.","These films are meticulously prepared via a two-step solution deposition method under controlled ambient conditions.","Our approach involves the application of poly(methyl methacrylate) (PMMA) and AZ5214 photoresist layers through spin-coating, aimed at singularly encapsulating the perovskite films.","This encapsulation acts as a robust hydrophobic barrier, significantly mitigating moisture ingress and simultaneously addressing the challenge of pinhole presence within the perovskite structure.","Through a series of detailed characterizations-spanning scanning electron microscopy (SEM), X-ray diffraction (XRD), and photoluminescence (PL) spectroscopy-we demonstrate that, despite the thicker nature of the AZ5214 photoresist compared to the PMMA layer, it exhibits markedly enhanced stability.","Notably, the integrity and optical properties of the perovskite films are preserved for extended periods of up to 960 hours under environmental exposure.","This breakthrough highlights the superior performance of AZ5214 photoresist over PMMA in prolonging the operational life of perovskite thin films, thereby offering a promising avenue for their deployment in a wide range of optoelectronic devices."],"url":"http://arxiv.org/abs/2402.16177v1","category":"physics.app-ph"}
{"created":"2024-02-25 16:21:23","title":"Ultra-High precision Compton polarimetry at 2 GeV","abstract":"We report a high precision measurement of electron beam polarization using Compton polarimetry. The measurement was made in experimental Hall A at Jefferson Lab during the CREX experiment in 2020. A total uncertainty of dP/P=0.36% was achieved detecting the back-scattered photons from the Compton scattering process. This is the highest accuracy in a measurement of electron beam polarization using Compton scattering ever reported, surpassing the ground-breaking measurement from the SLAC Large Detector (SLD) Compton polarimeter. Such uncertainty reaches the level required for the future flagship measurements to be made by the MOLLER and SoLID experiments.","sentences":["We report a high precision measurement of electron beam polarization using Compton polarimetry.","The measurement was made in experimental Hall A at Jefferson Lab during the CREX experiment in 2020.","A total uncertainty of dP/P=0.36% was achieved detecting the back-scattered photons from the Compton scattering process.","This is the highest accuracy in a measurement of electron beam polarization using Compton scattering ever reported, surpassing the ground-breaking measurement from the SLAC Large Detector (SLD) Compton polarimeter.","Such uncertainty reaches the level required for the future flagship measurements to be made by the MOLLER and SoLID experiments."],"url":"http://arxiv.org/abs/2402.16135v1","category":"physics.ins-det"}
{"created":"2024-02-25 16:16:37","title":"Dynamics of the temperature regime of permafrost soil in the vicinity of the main gas pipeline taking into account climate warming","abstract":"An initial-boundary value problem for an unsteady two-dimensional heat conduction equation in a bounded domain modeling the unsteady temperature distribution of permafrost soil in the vicinity of a main gas pipeline, taking into account climate warming, is investigated. The parameters of the mathematical model are selected in accordance with experimental data on gas transportation in permafrost areas. The problem is solved numerically by the finite element method. Modeling of the temperature field has been carried out for 30 years since the start of the gas pipeline operation. Calculations are carried out until the periodic temperature regime of the soil around the gas pipeline is practically established. Under the initial conditions adopted in the work, a periodic temperature regime at the top and bottom of the pipe is established in approximately 12 years, and a periodic temperature regime in depth is established in approximately 22 years. Two scenarios of climate warming are considered: moderate RCP2.6 and more negative RCP8.5. It is shown that significant changes in the ground temperature regime occur in the vicinity of the pipe under both warming scenarios. Nevertheless, the calculations demonstrate the preservation of permafrost even in a negative scenario of climate warming.","sentences":["An initial-boundary value problem for an unsteady two-dimensional heat conduction equation in a bounded domain modeling the unsteady temperature distribution of permafrost soil in the vicinity of a main gas pipeline, taking into account climate warming, is investigated.","The parameters of the mathematical model are selected in accordance with experimental data on gas transportation in permafrost areas.","The problem is solved numerically by the finite element method.","Modeling of the temperature field has been carried out for 30 years since the start of the gas pipeline operation.","Calculations are carried out until the periodic temperature regime of the soil around the gas pipeline is practically established.","Under the initial conditions adopted in the work, a periodic temperature regime at the top and bottom of the pipe is established in approximately 12 years, and a periodic temperature regime in depth is established in approximately 22 years.","Two scenarios of climate warming are considered: moderate RCP2.6 and more negative RCP8.5.","It is shown that significant changes in the ground temperature regime occur in the vicinity of the pipe under both warming scenarios.","Nevertheless, the calculations demonstrate the preservation of permafrost even in a negative scenario of climate warming."],"url":"http://arxiv.org/abs/2402.16134v1","category":"physics.geo-ph"}
{"created":"2024-02-25 15:12:24","title":"On the approximation of $G$-Diffusions by discrete-time $G$-Markov chains","abstract":"In this paper, we introduce discrete-time $G$-Markov chains, which are families of sublinear expectations corresponding to Markov chains with uncertain transition probabilities. Our main result investigates the convergence to $G$-diffusions, which are extensions of Peng's $G$-expectation with a random $G$-function. More specifically, we establish convergence of the uncertainty sets in the Hausdorff metric topology as well as weak convergence of the corresponding sublinear expectations. This resembles a result from Stroock and Varadhan for the linear case without uncertainty. As an application, we discuss an approximation of Peng's $G$-expectation and some extensions.","sentences":["In this paper, we introduce discrete-time $G$-Markov chains, which are families of sublinear expectations corresponding to Markov chains with uncertain transition probabilities.","Our main result investigates the convergence to $G$-diffusions, which are extensions of Peng's $G$-expectation with a random $G$-function.","More specifically, we establish convergence of the uncertainty sets in the Hausdorff metric topology as well as weak convergence of the corresponding sublinear expectations.","This resembles a result from Stroock and Varadhan for the linear case without uncertainty.","As an application, we discuss an approximation of Peng's $G$-expectation and some extensions."],"url":"http://arxiv.org/abs/2402.16108v1","category":"math.PR"}
{"created":"2024-02-25 13:05:25","title":"Beyond Spatio-Temporal Representations: Evolving Fourier Transform for Temporal Graphs","abstract":"We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs. We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain. We view the problem as an optimization over the Laplacian of the continuous time dynamic graph. Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient. The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs. Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra. We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance.","sentences":["We present the Evolving Graph Fourier Transform (EFT), the first invertible spectral transform that captures evolving representations on temporal graphs.","We motivate our work by the inadequacy of existing methods for capturing the evolving graph spectra, which are also computationally expensive due to the temporal aspect along with the graph vertex domain.","We view the problem as an optimization over the Laplacian of the continuous time dynamic graph.","Additionally, we propose pseudo-spectrum relaxations that decompose the transformation process, making it highly computationally efficient.","The EFT method adeptly captures the evolving graph's structural and positional properties, making it effective for downstream tasks on evolving graphs.","Hence, as a reference implementation, we develop a simple neural model induced with EFT for capturing evolving graph spectra.","We empirically validate our theoretical findings on a number of large-scale and standard temporal graph benchmarks and demonstrate that our model achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2402.16078v1","category":"cs.LG"}
{"created":"2024-02-25 11:49:48","title":"Demonstration of 3 V Programmable Josephson Junction Arrays Using Non-Integer-Multiple Logic","abstract":"This article demonstrates a new kind of programmable logic for the representation of an integer that can be used for the programmable Josephson voltage standard. It can enable the numbers of junctions in most bits to be variable integer values, which is different from normal binary logic or ternary logic. Consequently, missing junctions due to superconducting short circuits can be tolerated under this logic. This logic can also have nearly the same segmentation efficiency as ternary logic. The completeness of the sequences using this logic is proven by the recursive method in mathematics in this paper. After that, a new algorithm for the representation of integers is presented according to the proven process, and an analysis of the number of fault-tolerant junctions for each bit is provided. Although the first and second bits are not tolerant to missing junctions, bits beyond these can tolerate one to hundreds of missing junctions. Due to the non-fixed multiples between the bits of the sequence, this logic is called non-integer-multiple logic. Finally, the design and fabrication of a 3 V programmable Josephson junction array using this logic are described, and the measurements and analysis of the characteristic parameters are presented.","sentences":["This article demonstrates a new kind of programmable logic for the representation of an integer that can be used for the programmable Josephson voltage standard.","It can enable the numbers of junctions in most bits to be variable integer values, which is different from normal binary logic or ternary logic.","Consequently, missing junctions due to superconducting short circuits can be tolerated under this logic.","This logic can also have nearly the same segmentation efficiency as ternary logic.","The completeness of the sequences using this logic is proven by the recursive method in mathematics in this paper.","After that, a new algorithm for the representation of integers is presented according to the proven process, and an analysis of the number of fault-tolerant junctions for each bit is provided.","Although the first and second bits are not tolerant to missing junctions, bits beyond these can tolerate one to hundreds of missing junctions.","Due to the non-fixed multiples between the bits of the sequence, this logic is called non-integer-multiple logic.","Finally, the design and fabrication of a 3 V programmable Josephson junction array using this logic are described, and the measurements and analysis of the characteristic parameters are presented."],"url":"http://arxiv.org/abs/2402.16072v1","category":"cs.ET"}
{"created":"2024-02-25 10:36:35","title":"Reducing multivariate independence testing to two bivariate means comparisons","abstract":"Testing for independence between two random vectors is a fundamental problem in statistics. It is observed from empirical studies that many existing omnibus consistent tests may not work well for some strongly nonmonotonic and nonlinear relationships. To explore the reasons behind this issue, we novelly transform the multivariate independence testing problem equivalently into checking the equality of two bivariate means. An important observation we made is that the power loss is mainly due to cancellation of positive and negative terms in dependence metrics, making them very close to zero. Motivated by this observation, we propose a class of consistent metrics with a positive integer $\\gamma$ that exactly characterize independence. Theoretically, we show that the metrics with even and infinity $\\gamma$ can effectively avoid the cancellation, and have high powers under the alternatives that two mean differences offset each other. Since we target at a wide range of dependence scenarios in practice, we further suggest to combine the p-values of test statistics with different $\\gamma$'s through the Fisher's method. We illustrate the advantages of our proposed tests through extensive numerical studies.","sentences":["Testing for independence between two random vectors is a fundamental problem in statistics.","It is observed from empirical studies that many existing omnibus consistent tests may not work well for some strongly nonmonotonic and nonlinear relationships.","To explore the reasons behind this issue, we novelly transform the multivariate independence testing problem equivalently into checking the equality of two bivariate means.","An important observation we made is that the power loss is mainly due to cancellation of positive and negative terms in dependence metrics, making them very close to zero.","Motivated by this observation, we propose a class of consistent metrics with a positive integer $\\gamma$ that exactly characterize independence.","Theoretically, we show that the metrics with even and infinity $\\gamma$ can effectively avoid the cancellation, and have high powers under the alternatives that two mean differences offset each other.","Since we target at a wide range of dependence scenarios in practice, we further suggest to combine the p-values of test statistics with different $\\gamma$'s through the Fisher's method.","We illustrate the advantages of our proposed tests through extensive numerical studies."],"url":"http://arxiv.org/abs/2402.16053v1","category":"stat.ME"}
{"created":"2024-02-25 08:58:37","title":"Four-Channel WDM Graphene Optical Receiver using Mechanically Exfoliated Graphene","abstract":"Silicon photonics with the advantages of low power consumption, low cost, and high yield is a crucial technology for facilitating high-capacity optical communications and interconnects. The graphene photodetectors (GPDs) featuring broadband operation, high speed, and low integration cost can be good additions to the conventional SiGe photodetectors, supporting silicon-integrated on-chip photodetection in new wavelength bands beyond 1.6 microns (e.g., U-band and 2 microns). Here we realize a silicon-integrated four-channel wavelength division multiplexing (WDM) optical receiver based on a micro-ring resonator (MRR) array and four p-n homojunction GPDs by using mechanically exfoliated monolayer graphene. These GPDs based on the photo-thermoelectric (PTE) effect operating under zero (current) bias exhibit responsivities of ~1.1 V W^-1 and flat frequency responses up to 67 GHz which is set-up limited. The GPDs show good consistence benefiting from the compact active region array (0.006 mm^2) covered by a single hBN/graphene/hBN stack. Moreover, the WDM graphene optical receiver realized the 4x16 Gbps non-return to zero (NRZ) optical signal transmission. To the best of our knowledge, it is the first GPD-array-based optical receiver using mechanically exfoliated graphene and edge graphene-metal conduct. This work shed light on the large-scale integration of GPDs by using the high-quality mechanically exfoliated graphene, promoting the application and development of the graphene photonic devices.","sentences":["Silicon photonics with the advantages of low power consumption, low cost, and high yield is a crucial technology for facilitating high-capacity optical communications and interconnects.","The graphene photodetectors (GPDs) featuring broadband operation, high speed, and low integration cost can be good additions to the conventional SiGe photodetectors, supporting silicon-integrated on-chip photodetection in new wavelength bands beyond 1.6 microns (e.g., U-band and 2 microns).","Here we realize a silicon-integrated four-channel wavelength division multiplexing (WDM) optical receiver based on a micro-ring resonator (MRR) array and four p-n homojunction GPDs by using mechanically exfoliated monolayer graphene.","These GPDs based on the photo-thermoelectric (PTE) effect operating under zero (current) bias exhibit responsivities of ~1.1 V W^-1 and flat frequency responses up to 67 GHz which is set-up limited.","The GPDs show good consistence benefiting from the compact active region array (0.006 mm^2) covered by a single hBN/graphene/hBN stack.","Moreover, the WDM graphene optical receiver realized the 4x16 Gbps non-return to zero (NRZ) optical signal transmission.","To the best of our knowledge, it is the first GPD-array-based optical receiver using mechanically exfoliated graphene and edge graphene-metal conduct.","This work shed light on the large-scale integration of GPDs by using the high-quality mechanically exfoliated graphene, promoting the application and development of the graphene photonic devices."],"url":"http://arxiv.org/abs/2402.16032v1","category":"physics.optics"}
{"created":"2024-02-25 07:12:51","title":"Semi-supervised Open-World Object Detection","abstract":"Conventional open-world object detection (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown objects when introduced with labels in the subsequent tasks. However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages. Such reliance on run-time makes this formulation less realistic in a real-world deployment. To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner. We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting. Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the object query representations between the original and augmented images to leverage the large unlabeled and few labeled data. We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder object queries to capture object-specific information. We demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing object detection, proposing carefully curated splits and baseline performance evaluations. Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach. Our source code, models and splits are available here - https://github.com/sahalshajim/SS-OWFormer","sentences":["Conventional open-world object detection (OWOD) problem setting first distinguishes known and unknown classes and then later incrementally learns the unknown objects when introduced with labels in the subsequent tasks.","However, the current OWOD formulation heavily relies on the external human oracle for knowledge input during the incremental learning stages.","Such reliance on run-time makes this formulation less realistic in a real-world deployment.","To address this, we introduce a more realistic formulation, named semi-supervised open-world detection (SS-OWOD), that reduces the annotation cost by casting the incremental learning stages of OWOD in a semi-supervised manner.","We demonstrate that the performance of the state-of-the-art OWOD detector dramatically deteriorates in the proposed SS-OWOD setting.","Therefore, we introduce a novel SS-OWOD detector, named SS-OWFormer, that utilizes a feature-alignment scheme to better align the object query representations between the original and augmented images to leverage the large unlabeled and few labeled data.","We further introduce a pseudo-labeling scheme for unknown detection that exploits the inherent capability of decoder object queries to capture object-specific information.","We demonstrate the effectiveness of our SS-OWOD problem setting and approach for remote sensing object detection, proposing carefully curated splits and baseline performance evaluations.","Our experiments on 4 datasets including MS COCO, PASCAL, Objects365 and DOTA demonstrate the effectiveness of our approach.","Our source code, models and splits are available here - https://github.com/sahalshajim/SS-OWFormer"],"url":"http://arxiv.org/abs/2402.16013v1","category":"cs.CV"}
{"created":"2024-02-25 07:03:37","title":"Deep Contrastive Graph Learning with Clustering-Oriented Guidance","abstract":"Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering. To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN. Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features. Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph. To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering. Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features. On this basis, feature-level contrastive learning is introduced to enhance the discriminative capacity, and the relationship between samples and centroids is employed as the clustering-oriented guidance. Afterward, a two-branch graph learning mechanism is designed to extract the local and global structural relationships, which are further embedded into a unified graph under the cluster-level contrastive guidance. Experimental results on several benchmark datasets demonstrate the superiority of DCGL against state-of-the-art algorithms.","sentences":["Graph Convolutional Network (GCN) has exhibited remarkable potential in improving graph-based clustering.","To handle the general clustering scenario without a prior graph, these models estimate an initial graph beforehand to apply GCN.","Throughout the literature, we have witnessed that 1) most models focus on the initial graph while neglecting the original features.","Therefore, the discriminability of the learned representation may be corrupted by a low-quality initial graph; 2) the training procedure lacks effective clustering guidance, which may lead to the incorporation of clustering-irrelevant information into the learned graph.","To tackle these problems, the Deep Contrastive Graph Learning (DCGL) model is proposed for general data clustering.","Specifically, we establish a pseudo-siamese network, which incorporates auto-encoder with GCN to emphasize both the graph structure and the original features.","On this basis, feature-level contrastive learning is introduced to enhance the discriminative capacity, and the relationship between samples and centroids is employed as the clustering-oriented guidance.","Afterward, a two-branch graph learning mechanism is designed to extract the local and global structural relationships, which are further embedded into a unified graph under the cluster-level contrastive guidance.","Experimental results on several benchmark datasets demonstrate the superiority of DCGL against state-of-the-art algorithms."],"url":"http://arxiv.org/abs/2402.16012v1","category":"cs.LG"}
{"created":"2024-02-25 06:46:27","title":"From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation of Text Embeddings","abstract":"The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties. Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs. This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models. We conducted experiments on LLMs such as LLaMa2, Vicuna and using the Advbench dataset's harmful instructions. The results indicate that our method achieves a much better attack success rate to existing techniques, while significantly enhancing the textual fluency of the prompts. In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini. As a result, the prompts generated through our method exhibit enriched semantic diversity, which potentially provides more adversarial examples for LLM defense methods.","sentences":["The safety defense methods of Large language models(LLMs) stays limited because the dangerous prompts are manually curated to just few known attack types, which fails to keep pace with emerging varieties.","Recent studies found that attaching suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous outputs.","This method, while effective, leaves a gap in understanding the underlying mechanics of such adversarial suffix due to the non-readability and it can be relatively easily seen through by common defense methods such as perplexity filters.","To cope with this challenge, in this paper, we propose an Adversarial Suffixes Embedding Translation Framework(ASETF) that are able to translate the unreadable adversarial suffixes into coherent, readable text, which makes it easier to understand and analyze the reasons behind harmful content generation by large language models.","We conducted experiments on LLMs such as LLaMa2, Vicuna and using the Advbench dataset's harmful instructions.","The results indicate that our method achieves a much better attack success rate to existing techniques, while significantly enhancing the textual fluency of the prompts.","In addition, our approach can be generalized into a broader method for generating transferable adversarial suffixes that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT and Gemini.","As a result, the prompts generated through our method exhibit enriched semantic diversity, which potentially provides more adversarial examples for LLM defense methods."],"url":"http://arxiv.org/abs/2402.16006v1","category":"cs.CL"}
{"created":"2024-02-25 04:13:00","title":"Spherical Geometry Algorithm for Space-borne Synthetic Aperture Radar Imaging","abstract":"Higher spatial resolution and larger imaging scene are always the goals pursued by advanced space-borne SAR system.High resolution and wide swath SAR imaging can provide more information about the illuminated scene of interest on one hand,but also come with some new challenges on the other hand.The induced new challenging problems include curved orbit,Earth rotation,and spherical ground surface,etc.Most existing image formation algorithms suffer from performance deficiency in these challenging cases,either in focus accuracy or computational efficiency.In this paper,an accurate Fourier transform relationship between the phase history domain data and the scene reflectivity function is derived under arbitrary radar trajectory by exploiting the spherical geometry property of the space-borne SAR data collection.Using the derived new data model,an image reconstruction algorithm based on Fourier inversion is proposed.The new algorithm has the inherent capability of correcting for the curved orbit and spherical ground surface effect.Meanwhile,the out-of-plane motion effect induced by the Earth's rotation can also be compensated by a two-step phase correction and data projection procedure embedded in the Fourier inversion reconstruction.The new algorithm inherits the merit of both time domain and frequency domain algorithms,has excellent performance in both focus accuracy and computational efficiency.Both simulation and real data processing results validate the effectiveness of the proposed imaging algorithm.","sentences":["Higher spatial resolution and larger imaging scene are always the goals pursued by advanced space-borne SAR system.","High resolution and wide swath SAR imaging can provide more information about the illuminated scene of interest on one hand,but also come with some new challenges on the other hand.","The induced new challenging problems include curved orbit,Earth rotation,and spherical ground surface,etc.Most existing image formation algorithms suffer from performance deficiency in these challenging cases,either in focus accuracy or computational efficiency.","In this paper,an accurate Fourier transform relationship between the phase history domain data and the scene reflectivity function is derived under arbitrary radar trajectory by exploiting the spherical geometry property of the space-borne SAR data collection.","Using the derived new data model,an image reconstruction algorithm based on Fourier inversion is proposed.","The new algorithm has the inherent capability of correcting for the curved orbit and spherical ground surface effect.","Meanwhile,the out-of-plane motion effect induced by the Earth's rotation can also be compensated by a two-step phase correction and data projection procedure embedded in the Fourier inversion reconstruction.","The new algorithm inherits the merit of both time domain and frequency domain algorithms,has excellent performance in both focus accuracy and computational efficiency.","Both simulation and real data processing results validate the effectiveness of the proposed imaging algorithm."],"url":"http://arxiv.org/abs/2402.15982v1","category":"eess.SP"}
{"created":"2024-02-25 03:48:13","title":"Shaving Weights with Occam's Razor: Bayesian Sparsification for Neural Networks Using the Marginal Likelihood","abstract":"Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\\\"ively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.","sentences":["Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to na\\\"ively deploy on consumer hardware.","While much work has focused on different weight pruning criteria, the overall sparsifiability of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked.","We present Sparsifiability via the Marginal likelihood (SpaM), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable.","Our approach implements an automatic Occam's razor that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification.","In addition, we demonstrate that the pre-computed posterior Hessian approximation used in the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches.","We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets."],"url":"http://arxiv.org/abs/2402.15978v1","category":"cs.LG"}
{"created":"2024-02-25 02:07:50","title":"Convolution and Cross-Correlation of Count Sketches Enables Fast Cardinality Estimation of Multi-Join Queries","abstract":"With the increasing rate of data generated by critical systems, estimating functions on streaming data has become essential. This demand has driven numerous advancements in algorithms designed to efficiently query and analyze one or more data streams while operating under memory constraints. The primary challenge arises from the rapid influx of new items, requiring algorithms that enable efficient incremental processing of streams in order to keep up. A prominent algorithm in this domain is the AMS sketch. Originally developed to estimate the second frequency moment of a data stream, it can also estimate the cardinality of the equi-join between two relations. Since then, two important advancements are the Count sketch, a method which significantly improves upon the sketch update time, and secondly, an extension of the AMS sketch to accommodate multi-join queries. However, combining the strengths of these methods to maintain sketches for multi-join queries while ensuring fast update times is a non-trivial task, and has remained an open problem for decades as highlighted in the existing literature. In this work, we successfully address this problem by introducing a novel sketching method which has fast updates, even for sketches capable of accurately estimating the cardinality of complex multi-join queries. We prove that our estimator is unbiased and has the same error guarantees as the AMS-based method. Our experimental results confirm the significant improvement in update time complexity, resulting in orders of magnitude faster estimates, with equal or better estimation accuracy.","sentences":["With the increasing rate of data generated by critical systems, estimating functions on streaming data has become essential.","This demand has driven numerous advancements in algorithms designed to efficiently query and analyze one or more data streams while operating under memory constraints.","The primary challenge arises from the rapid influx of new items, requiring algorithms that enable efficient incremental processing of streams in order to keep up.","A prominent algorithm in this domain is the AMS sketch.","Originally developed to estimate the second frequency moment of a data stream, it can also estimate the cardinality of the equi-join between two relations.","Since then, two important advancements are the Count sketch, a method which significantly improves upon the sketch update time, and secondly, an extension of the AMS sketch to accommodate multi-join queries.","However, combining the strengths of these methods to maintain sketches for multi-join queries while ensuring fast update times is a non-trivial task, and has remained an open problem for decades as highlighted in the existing literature.","In this work, we successfully address this problem by introducing a novel sketching method which has fast updates, even for sketches capable of accurately estimating the cardinality of complex multi-join queries.","We prove that our estimator is unbiased and has the same error guarantees as the AMS-based method.","Our experimental results confirm the significant improvement in update time complexity, resulting in orders of magnitude faster estimates, with equal or better estimation accuracy."],"url":"http://arxiv.org/abs/2402.15953v1","category":"cs.DB"}
{"created":"2024-02-25 01:31:28","title":"Criticality measure-based error estimates for infinite dimensional optimization","abstract":"Motivated by optimization with differential equations, we consider optimization problems with Hilbert spaces as decision spaces. As a consequence of their infinite dimensionality, the numerical solution necessitates finite dimensional approximations and discretizations. We develop an approximation framework and demonstrate criticality measure-based error estimates. We consider criticality measures inspired by those used within optimization methods, such as semismooth Newton and (conditional) gradient methods. Furthermore, we show that our error estimates are order-optimal. Our findings augment existing distance-based error estimates, but do not rely on strong convexity or second-order sufficient optimality conditions. Moreover, our error estimates can be used for code verification and validation. We illustrate our theoretical convergence rates on linear, semilinear, and bilinear PDE-constrained optimization.","sentences":["Motivated by optimization with differential equations, we consider optimization problems with Hilbert spaces as decision spaces.","As a consequence of their infinite dimensionality, the numerical solution necessitates finite dimensional approximations and discretizations.","We develop an approximation framework and demonstrate criticality measure-based error estimates.","We consider criticality measures inspired by those used within optimization methods, such as semismooth Newton and (conditional) gradient methods.","Furthermore, we show that our error estimates are order-optimal.","Our findings augment existing distance-based error estimates, but do not rely on strong convexity or second-order sufficient optimality conditions.","Moreover, our error estimates can be used for code verification and validation.","We illustrate our theoretical convergence rates on linear, semilinear, and bilinear PDE-constrained optimization."],"url":"http://arxiv.org/abs/2402.15948v1","category":"math.OC"}
{"created":"2024-02-25 01:05:39","title":"On A Class of Greedy Sparse Recovery Algorithms -- A High Dimensional Approach","abstract":"Sparse signal recovery deals with finding the sparest solution of an under-determined linear system $x = Qs$. In this paper, we propose a novel greedy approach to addressing the challenges from such a problem. Such an approach is based on a characterization of solutions to the system, which allows us to work on the sparse recovery in the $s$-space directly with a given measure. With $l_2$-based measure, two OMP-type algorithms are proposed, which significantly outperform the classical OMP algorithm in terms of recovery accuracy while maintaining comparable computational complexity. An $l_1$-based algorithm, denoted as $\\text{Alg}_{GBP}$ (greedy basis pursuit) algorithm, is derived. Such an algorithm significantly outperforms the classical BP algorithm. A CoSaMP-type algorithm is also proposed to further enhance the performance of the two proposed OMP-type algorithms. The superior performance of our proposed algorithms is demonstrated through extensive numerical simulations using synthetic data as well as video signals, highlighting their potential for various applications in compressed sensing and signal processing.","sentences":["Sparse signal recovery deals with finding the sparest solution of an under-determined linear system $x = Qs$.","In this paper, we propose a novel greedy approach to addressing the challenges from such a problem.","Such an approach is based on a characterization of solutions to the system, which allows us to work on the sparse recovery in the $s$-space directly with a given measure.","With $l_2$-based measure, two OMP-type algorithms are proposed, which significantly outperform the classical OMP algorithm in terms of recovery accuracy while maintaining comparable computational complexity.","An $l_1$-based algorithm, denoted as $\\text{Alg}_{GBP}$ (greedy basis pursuit) algorithm, is derived.","Such an algorithm significantly outperforms the classical BP algorithm.","A CoSaMP-type algorithm is also proposed to further enhance the performance of the two proposed OMP-type algorithms.","The superior performance of our proposed algorithms is demonstrated through extensive numerical simulations using synthetic data as well as video signals, highlighting their potential for various applications in compressed sensing and signal processing."],"url":"http://arxiv.org/abs/2402.15944v1","category":"cs.IT"}
{"created":"2024-02-24 23:52:43","title":"Optimizing Neural Networks for Bermudan Option Pricing: Convergence Acceleration, Future Exposure Evaluation and Interpolation in Counterparty Credit Risk","abstract":"This paper presents a Monte-Carlo-based artificial neural network framework for pricing Bermudan options, offering several notable advantages. These advantages encompass the efficient static hedging of the target Bermudan option and the effective generation of exposure profiles for risk management. We also introduce a novel optimisation algorithm designed to expedite the convergence of the neural network framework proposed by Lokeshwar et al. (2022) supported by a comprehensive error convergence analysis. We conduct an extensive comparative analysis of the Present Value (PV) distribution under Markovian and no-arbitrage assumptions. We compare the proposed neural network model in conjunction with the approach initially introduced by Longstaff and Schwartz (2001) and benchmark it against the COS model, the pricing model pioneered by Fang and Oosterlee (2009), across all Bermudan exercise time points. Additionally, we evaluate exposure profiles, including Expected Exposure and Potential Future Exposure, generated by our proposed model and the Longstaff-Schwartz model, comparing them against the COS model. We also derive exposure profiles at finer non-standard grid points or risk horizons using the proposed approach, juxtaposed with the Longstaff Schwartz method with linear interpolation and benchmark against the COS method. In addition, we explore the effectiveness of various interpolation schemes within the context of the Longstaff-Schwartz method for generating exposures at finer grid horizons.","sentences":["This paper presents a Monte-Carlo-based artificial neural network framework for pricing Bermudan options, offering several notable advantages.","These advantages encompass the efficient static hedging of the target Bermudan option and the effective generation of exposure profiles for risk management.","We also introduce a novel optimisation algorithm designed to expedite the convergence of the neural network framework proposed by Lokeshwar et al. (2022) supported by a comprehensive error convergence analysis.","We conduct an extensive comparative analysis of the Present Value (PV) distribution under Markovian and no-arbitrage assumptions.","We compare the proposed neural network model in conjunction with the approach initially introduced by Longstaff and Schwartz (2001) and benchmark it against the COS model, the pricing model pioneered by Fang and Oosterlee (2009), across all Bermudan exercise time points.","Additionally, we evaluate exposure profiles, including Expected Exposure and Potential Future Exposure, generated by our proposed model and the Longstaff-Schwartz model, comparing them against the COS model.","We also derive exposure profiles at finer non-standard grid points or risk horizons using the proposed approach, juxtaposed with the Longstaff Schwartz method with linear interpolation and benchmark against the COS method.","In addition, we explore the effectiveness of various interpolation schemes within the context of the Longstaff-Schwartz method for generating exposures at finer grid horizons."],"url":"http://arxiv.org/abs/2402.15936v1","category":"q-fin.CP"}
{"created":"2024-02-24 23:23:06","title":"Frustratingly Simple Prompting-based Text Denoising","abstract":"This paper introduces a novel perspective on the automated essay scoring (AES) task, challenging the conventional view of the ASAP dataset as a static entity. Employing simple text denoising techniques using prompting, we explore the dynamic potential within the dataset. While acknowledging the previous emphasis on building regression systems, our paper underscores how making minor changes to a dataset through text denoising can enhance the final results.","sentences":["This paper introduces a novel perspective on the automated essay scoring (AES) task, challenging the conventional view of the ASAP dataset as a static entity.","Employing simple text denoising techniques using prompting, we explore the dynamic potential within the dataset.","While acknowledging the previous emphasis on building regression systems, our paper underscores how making minor changes to a dataset through text denoising can enhance the final results."],"url":"http://arxiv.org/abs/2402.15931v1","category":"cs.CL"}
{"created":"2024-02-24 23:17:56","title":"Evaluating Prompting Strategies for Grammatical Error Correction Based on Language Proficiency","abstract":"The writing examples of English language learners may be different from those of native speakers. Given that there is a significant differences in second language (L2) learners' error types by their proficiency levels, this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency. Our method focuses on zero-shot and few-shot prompting and fine-tuning models for GEC for learners of English as a foreign language based on the different proficiency. We investigate GEC results and find that overcorrection happens primarily in advanced language learners' writing (proficiency C) rather than proficiency A (a beginner level) and proficiency B (an intermediate level). Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures. To make our claim concrete, we conduct a comprehensive examination of GEC outcomes and their evaluation results based on language proficiency.","sentences":["The writing examples of English language learners may be different from those of native speakers.","Given that there is a significant differences in second language (L2) learners' error types by their proficiency levels, this paper attempts to reduce overcorrection by examining the interaction between LLM's performance and L2 language proficiency.","Our method focuses on zero-shot and few-shot prompting and fine-tuning models for GEC for learners of English as a foreign language based on the different proficiency.","We investigate GEC results and find that overcorrection happens primarily in advanced language learners' writing (proficiency C) rather than proficiency A (a beginner level) and proficiency B (an intermediate level).","Fine-tuned LLMs, and even few-shot prompting with writing examples of English learners, actually tend to exhibit decreased recall measures.","To make our claim concrete, we conduct a comprehensive examination of GEC outcomes and their evaluation results based on language proficiency."],"url":"http://arxiv.org/abs/2402.15930v1","category":"cs.CL"}
{"created":"2024-02-24 23:10:28","title":"Large Stepsize Gradient Descent for Logistic Loss: Non-Monotonicity of the Loss Improves Optimization Efficiency","abstract":"We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\\eta$ is so large that the loss initially oscillates. We show that GD exits this initial oscillatory phase rapidly -- in $\\mathcal{O}(\\eta)$ steps -- and subsequently achieves an $\\tilde{\\mathcal{O}}(1 / (\\eta t) )$ convergence rate after $t$ additional steps. Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\\tilde{\\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\\eta:= \\Theta( T)$, without any use of momentum or variable stepsize schedulers. Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\\tilde{\\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under suitable separability conditions.","sentences":["We consider gradient descent (GD) with a constant stepsize applied to logistic regression with linearly separable data, where the constant stepsize $\\eta$ is so large that the loss initially oscillates.","We show that GD exits this initial oscillatory phase rapidly -- in $\\mathcal{O}(\\eta)$ steps -- and subsequently achieves an $\\tilde{\\mathcal{O}}(1 / (\\eta t) )","$ convergence rate after $t$ additional steps.","Our results imply that, given a budget of $T$ steps, GD can achieve an accelerated loss of $\\tilde{\\mathcal{O}}(1/T^2)$ with an aggressive stepsize $\\eta:= \\Theta( T)$, without any use of momentum or variable stepsize schedulers.","Our proof technique is versatile and also handles general classification loss functions (where exponential tails are needed for the $\\tilde{\\mathcal{O}}(1/T^2)$ acceleration), nonlinear predictors in the neural tangent kernel regime, and online stochastic gradient descent (SGD) with a large stepsize, under suitable separability conditions."],"url":"http://arxiv.org/abs/2402.15926v1","category":"cs.LG"}
{"created":"2024-02-24 22:33:08","title":"Barrow black hole variable parameter model connected to information theory","abstract":"One of the greatest challenges of theoretical physics today is to unveil the quantum information theory concerning what happens when one bit of information enters the black hole (BH) horizon. The Landauer principle showed that a certain amount of energy is generated when one-bit of information is erased as it enters the event horizon system. In this paper we used the recently developed Barrow BH model to calculate the addition to the area of the event horizon of his toy model by using the Landauer concept. Besides we make this computation considering $\\Delta$ as a constant and a variable parameter. We formulate the Barrow parameter ($\\Delta$) as a function of the energy/mass, which is new in the Barrow BH literature. We will investigate the differences between the Bekenstein-Hawking entropy ($\\Delta=0$) and the fractal ($\\Delta=1$) cases concerning the addition in the area of the BH. The asymptotical analysis is also mentioned and we will see that it affects only the fractal case. All the results accomplished here are new concerning BHs in general and the Barrow model literature in particular.","sentences":["One of the greatest challenges of theoretical physics today is to unveil the quantum information theory concerning what happens when one bit of information enters the black hole (BH) horizon.","The Landauer principle showed that a certain amount of energy is generated when one-bit of information is erased as it enters the event horizon system.","In this paper we used the recently developed Barrow BH model to calculate the addition to the area of the event horizon of his toy model by using the Landauer concept.","Besides we make this computation considering $\\Delta$ as a constant and a variable parameter.","We formulate the Barrow parameter ($\\Delta$) as a function of the energy/mass, which is new in the Barrow BH literature.","We will investigate the differences between the Bekenstein-Hawking entropy ($\\Delta=0$) and the fractal ($\\Delta=1$) cases concerning the addition in the area of the BH.","The asymptotical analysis is also mentioned and we will see that it affects only the fractal case.","All the results accomplished here are new concerning BHs in general and the Barrow model literature in particular."],"url":"http://arxiv.org/abs/2402.15922v1","category":"gr-qc"}
{"created":"2024-02-24 22:02:26","title":"The statistics of Rayleigh-Levy flight extrema","abstract":"Rayleigh-Levy flights have played a significant role in cosmology as simplified models for understanding how matter distributes itself under gravitational influence. These models also exhibit numerous remarkable properties that enable the prediction of a wide range of characteristics. Here, we derive the one and two point statistics of extreme points within Rayleigh-Levy flights spanning one to three dimensions, stemming directly from fundamental principles. In the context of the mean field limit, we provide straightforward closed-form expressions for Euler counts and their correlations, particularly in relation to their clustering behaviour over long distances. Additionally, quadratures allow for the computation of extreme value number densities. A comparison between theoretical predictions in 1D and Monte Carlo measurements shows remarkable agreement. Given the widespread use of Rayleigh-Levy processes, these comprehensive findings offer significant promise not only in astrophysics but also in broader applications beyond the field.","sentences":["Rayleigh-Levy flights have played a significant role in cosmology as simplified models for understanding how matter distributes itself under gravitational influence.","These models also exhibit numerous remarkable properties that enable the prediction of a wide range of characteristics.","Here, we derive the one and two point statistics of extreme points within Rayleigh-Levy flights spanning one to three dimensions, stemming directly from fundamental principles.","In the context of the mean field limit, we provide straightforward closed-form expressions for Euler counts and their correlations, particularly in relation to their clustering behaviour over long distances.","Additionally, quadratures allow for the computation of extreme value number densities.","A comparison between theoretical predictions in 1D and Monte Carlo measurements shows remarkable agreement.","Given the widespread use of Rayleigh-Levy processes, these comprehensive findings offer significant promise not only in astrophysics but also in broader applications beyond the field."],"url":"http://arxiv.org/abs/2402.15915v1","category":"astro-ph.CO"}
{"created":"2024-02-24 21:50:49","title":"Role of quantum correlations in daemonic expected utility","abstract":"Fluctuations can challenge the possibility of improving work extraction from quantum correlations. This uncertainty in the work extraction process can be addressed resorting to the expected utility hypothesis which can provide an optimal method for work extraction. We study a bipartite quantum system and examine the role of quantum correlations in a daemonic work extraction performed by certain local operations and classical communication. Specifically, we demonstrate and explain how, depending on the so-called absolute risk aversion, a non-neutral risk agent, influenced by fluctuations, views quantum correlations differently from a neutral risk agent who is affected solely by the average work.","sentences":["Fluctuations can challenge the possibility of improving work extraction from quantum correlations.","This uncertainty in the work extraction process can be addressed resorting to the expected utility hypothesis which can provide an optimal method for work extraction.","We study a bipartite quantum system and examine the role of quantum correlations in a daemonic work extraction performed by certain local operations and classical communication.","Specifically, we demonstrate and explain how, depending on the so-called absolute risk aversion, a non-neutral risk agent, influenced by fluctuations, views quantum correlations differently from a neutral risk agent who is affected solely by the average work."],"url":"http://arxiv.org/abs/2402.15912v1","category":"quant-ph"}
{"created":"2024-02-24 20:01:15","title":"Concurrent Learning of Policy and Unknown Safety Constraints in Reinforcement Learning","abstract":"Reinforcement learning (RL) has revolutionized decision-making across a wide range of domains over the past few decades. Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety. Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process. However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable. Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment. Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy optimization, using a Lagrangian-variant of the twin delayed deep deterministic policy gradient (TD3) algorithm, with Bayesian optimization for optimizing parameters for the given pSTL safety specification. Through experimentation in comprehensive case studies, we validate the efficacy of this approach across varying forms of environmental constraints, consistently yielding safe RL policies with high returns. Furthermore, our findings indicate successful learning of STL safety constraint parameters, exhibiting a high degree of conformity with true environmental safety constraints. The performance of our model closely mirrors that of an ideal scenario that possesses complete prior knowledge of safety constraints, demonstrating its proficiency in accurately identifying environmental safety constraints and learning safe policies that adhere to those constraints.","sentences":["Reinforcement learning (RL) has revolutionized decision-making across a wide range of domains over the past few decades.","Yet, deploying RL policies in real-world scenarios presents the crucial challenge of ensuring safety.","Traditional safe RL approaches have predominantly focused on incorporating predefined safety constraints into the policy learning process.","However, this reliance on predefined safety constraints poses limitations in dynamic and unpredictable real-world settings where such constraints may not be available or sufficiently adaptable.","Bridging this gap, we propose a novel approach that concurrently learns a safe RL control policy and identifies the unknown safety constraint parameters of a given environment.","Initializing with a parametric signal temporal logic (pSTL) safety specification and a small initial labeled dataset, we frame the problem as a bilevel optimization task, intricately integrating constrained policy optimization, using a Lagrangian-variant of the twin delayed deep deterministic policy gradient (TD3) algorithm, with Bayesian optimization for optimizing parameters for the given pSTL safety specification.","Through experimentation in comprehensive case studies, we validate the efficacy of this approach across varying forms of environmental constraints, consistently yielding safe RL policies with high returns.","Furthermore, our findings indicate successful learning of STL safety constraint parameters, exhibiting a high degree of conformity with true environmental safety constraints.","The performance of our model closely mirrors that of an ideal scenario that possesses complete prior knowledge of safety constraints, demonstrating its proficiency in accurately identifying environmental safety constraints and learning safe policies that adhere to those constraints."],"url":"http://arxiv.org/abs/2402.15893v1","category":"eess.SY"}
{"created":"2024-02-24 18:33:19","title":"Lorentz invariance and quantum mechanics","abstract":"Bohmian mechanics and spontaneous collapse models are theories that overcome the quantum measurement problem. While they are naturally formulated for non-relativistic systems, it has proven difficult to formulate Lorentz invariant extensions, primarily due to the inherent non-locality, which is unavoidable due to Bell's theorem. There are trivial ways to make space-time theories Lorentz invariant, but the challenge is to achieve what Bell dubbed ``serious Lorentz invariance''. However, this notion is hard to make precise. This is reminiscent of the debate on the meaning of general invariance in Einstein's theory of general relativity. The issue there is whether the requirement of general invariance is physically vacuous (in the sense that any space-time theory can be made generally invariant) or whether it is a fundamental physical principle. Here, we want to consider two of the more promising avenues that have emerged from that debate in order to explore what serious Lorentz invariance could mean. First, we will consider Anderson's approach based on the identification of absolute objects. Second, we will consider a relativity principle for isolated subsystems. Using these criteria, we will evaluate a number of Lorentz invariant Bohmian models and a spontaneous collapse model, finding that the latter satisfies both criteria, while there are some Bohmian models that violate the criteria. However, some Bohmian models that satisfy both criteria still do not seem seriously Lorentz invariant. While these notions may hence still not capture exactly what serious Lorentz invariance ought to be, they clarify what aspects of relativity theory (in addition to locality) may need to be given up in passing from classical to quantum theory.","sentences":["Bohmian mechanics and spontaneous collapse models are theories that overcome the quantum measurement problem.","While they are naturally formulated for non-relativistic systems, it has proven difficult to formulate Lorentz invariant extensions, primarily due to the inherent non-locality, which is unavoidable due to Bell's theorem.","There are trivial ways to make space-time theories Lorentz invariant, but the challenge is to achieve what Bell dubbed ``serious Lorentz invariance''.","However, this notion is hard to make precise.","This is reminiscent of the debate on the meaning of general invariance in Einstein's theory of general relativity.","The issue there is whether the requirement of general invariance is physically vacuous (in the sense that any space-time theory can be made generally invariant) or whether it is a fundamental physical principle.","Here, we want to consider two of the more promising avenues that have emerged from that debate in order to explore what serious Lorentz invariance could mean.","First, we will consider Anderson's approach based on the identification of absolute objects.","Second, we will consider a relativity principle for isolated subsystems.","Using these criteria, we will evaluate a number of Lorentz invariant Bohmian models and a spontaneous collapse model, finding that the latter satisfies both criteria, while there are some Bohmian models that violate the criteria.","However, some Bohmian models that satisfy both criteria still do not seem seriously Lorentz invariant.","While these notions may hence still not capture exactly what serious Lorentz invariance ought to be, they clarify what aspects of relativity theory (in addition to locality) may need to be given up in passing from classical to quantum theory."],"url":"http://arxiv.org/abs/2402.15881v1","category":"quant-ph"}
{"created":"2024-02-24 18:14:13","title":"Introduction to Variational Quantum Algorithms","abstract":"This document is a pdf version of the series of blogposts about variational quantum algorithms (VQA) I originally posted on my blog Musty Thoughts. It provides an explanation of the basic variational algorithms, such as Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA), as well as a more general framework for VQAs. It also describes some more advanced techniques that can be used to make these algorithms more efficient, as well as the challenges associated with using them.","sentences":["This document is a pdf version of the series of blogposts about variational quantum algorithms (VQA) I originally posted on my blog Musty Thoughts.","It provides an explanation of the basic variational algorithms, such as Variational Quantum Eigensolver (VQE) and Quantum Approximate Optimization Algorithm (QAOA), as well as a more general framework for VQAs.","It also describes some more advanced techniques that can be used to make these algorithms more efficient, as well as the challenges associated with using them."],"url":"http://arxiv.org/abs/2402.15879v1","category":"quant-ph"}
{"created":"2024-02-24 17:56:21","title":"Automorphic density estimates and optimal Diophantine exponents","abstract":"The present paper is devoted to establishing an optimal approximation exponent for the action of an irreducible uniform lattice subgroup of a product group on its proper factors. Previously optimal approximation exponents for lattice actions on homogeneous spaces were established under the assumption that the restriction of the automorphic representation to the stability group is suitably tempered. However, for irreducible lattices in semisimple algebraic groups, either this property does not hold or it amounts to an instance of the Ramanujan-Petersson-Selberg conjecture. Sarnak's Density Hypothesis and its variants bounding the multiplicities of irreducible representations occurring in the decomposition of the automorphic representation can be viewed as a weakening of the temperedness property. A refined form of this hypothesis has recently been established for uniform irreducible arithmetic congruence lattices arising from quaternion algebras. We employ this result in order to establish - unconditionally - an optimal approximation exponent for the actions of these lattices on the associated symmetric spaces. We also give a general spectral criterion for the optimality of the approximation exponent for irreducible uniform lattices in a product of arbitrary Gelfand pairs. Our methods involve utilizing the multiplicity bounds in the pre-trace formula, establishing refined estimates of the spherical transforms, and carrying out an elaborate spectral analysis that bounds the Hilbert-Schmidt norms of carefully balanced geometric convolution operators.","sentences":["The present paper is devoted to establishing an optimal approximation exponent for the action of an irreducible uniform lattice subgroup of a product group on its proper factors.","Previously optimal approximation exponents for lattice actions on homogeneous spaces were established under the assumption that the restriction of the automorphic representation to the stability group is suitably tempered.","However, for irreducible lattices in semisimple algebraic groups, either this property does not hold or it amounts to an instance of the Ramanujan-Petersson-Selberg conjecture.","Sarnak's Density Hypothesis and its variants bounding the multiplicities of irreducible representations occurring in the decomposition of the automorphic representation can be viewed as a weakening of the temperedness property.","A refined form of this hypothesis has recently been established for uniform irreducible arithmetic congruence lattices arising from quaternion algebras.","We employ this result in order to establish - unconditionally - an optimal approximation exponent for the actions of these lattices on the associated symmetric spaces.","We also give a general spectral criterion for the optimality of the approximation exponent for irreducible uniform lattices in a product of arbitrary Gelfand pairs.","Our methods involve utilizing the multiplicity bounds in the pre-trace formula, establishing refined estimates of the spherical transforms, and carrying out an elaborate spectral analysis that bounds the Hilbert-Schmidt norms of carefully balanced geometric convolution operators."],"url":"http://arxiv.org/abs/2402.15875v1","category":"math.NT"}
{"created":"2024-02-24 17:47:10","title":"Evaluating Classification Algorithms: Exoplanet Detection using Kepler Time Series Data","abstract":"This study presents a comprehensive evaluation of various classification algorithms used for the detection of exoplanets using labeled time series data from the Kepler mission. The study investigates the performance of six commonly employed algorithms, namely Random Forest, Support Vector Machine, Logistic Regression, K-Nearest Neighbors, Naive Bayes, and Decision Tree. The evaluation process involves analyzing a dataset that consists of time series measurements of star brightness, accompanied by labels indicating the presence or absence of exoplanets. To assess the effectiveness of each algorithm in accurately identifying exoplanets, performance metrics such as accuracy, precision, recall, and F1 score are employed. The results demonstrate that the Random Forest algorithm achieves the highest accuracy of 94.2\\%, followed closely by the Support Vector Machine with 93.8 percent accuracy. The Logistic Regression algorithm achieves an accuracy of 91.5 percent, while the K-Nearest Neighbors, Naive Bayes, and Decision Tree algorithms achieve accuracies of 89.6\\%, 87.3\\%, and 85.9\\% respectively. Furthermore, the precision, recall, and F1 score metrics provide insights into the strengths and weaknesses of each classifier. The Random Forest algorithm exhibits a precision of 0.92, recall of 0.95, and F1 score of 0.93, indicating a balanced performance in correctly identifying both positive and negative instances. The Support Vector Machine also demonstrates strong performance with precision, recall, and F1 score values of 0.91, 0.94, and 0.92 respectively. The evaluation demonstrates that Random Forest and Support Vector Machine algorithms are well-suited for exoplanet detection using Kepler time series data. These findings enhance our understanding of the detection process and assist in selecting suitable algorithms for future studies.","sentences":["This study presents a comprehensive evaluation of various classification algorithms used for the detection of exoplanets using labeled time series data from the Kepler mission.","The study investigates the performance of six commonly employed algorithms, namely Random Forest, Support Vector Machine, Logistic Regression, K-Nearest Neighbors, Naive Bayes, and Decision Tree.","The evaluation process involves analyzing a dataset that consists of time series measurements of star brightness, accompanied by labels indicating the presence or absence of exoplanets.","To assess the effectiveness of each algorithm in accurately identifying exoplanets, performance metrics such as accuracy, precision, recall, and F1 score are employed.","The results demonstrate that the Random Forest algorithm achieves the highest accuracy of 94.2\\%, followed closely by the Support Vector Machine with 93.8 percent accuracy.","The Logistic Regression algorithm achieves an accuracy of 91.5 percent, while the K-Nearest Neighbors, Naive Bayes, and Decision Tree algorithms achieve accuracies of 89.6\\%, 87.3\\%, and 85.9\\% respectively.","Furthermore, the precision, recall, and F1 score metrics provide insights into the strengths and weaknesses of each classifier.","The Random Forest algorithm exhibits a precision of 0.92, recall of 0.95, and F1 score of 0.93, indicating a balanced performance in correctly identifying both positive and negative instances.","The Support Vector Machine also demonstrates strong performance with precision, recall, and F1 score values of 0.91, 0.94, and 0.92 respectively.","The evaluation demonstrates that Random Forest and Support Vector Machine algorithms are well-suited for exoplanet detection using Kepler time series data.","These findings enhance our understanding of the detection process and assist in selecting suitable algorithms for future studies."],"url":"http://arxiv.org/abs/2402.15874v1","category":"astro-ph.EP"}
{"created":"2024-02-24 17:26:47","title":"Regular resolution effectively simulates resolution","abstract":"Regular resolution is a refinement of the resolution proof system requiring that no variable be resolved on more than once along any path in the proof. It is known that there exist sequences of formulas that require exponential-size proofs in regular resolution while admitting polynomial-size proofs in resolution. Thus, with respect to the usual notion of simulation, regular resolution is separated from resolution. An alternative, and weaker, notion for comparing proof systems is that of an \"effective simulation,\" which allows the translation of the formula along with the proof when moving between proof systems. We prove that regular resolution is equivalent to resolution under effective simulations. As a corollary, we recover in a black-box fashion a recent result on the hardness of automating regular resolution.","sentences":["Regular resolution is a refinement of the resolution proof system requiring that no variable be resolved on more than once along any path in the proof.","It is known that there exist sequences of formulas that require exponential-size proofs in regular resolution while admitting polynomial-size proofs in resolution.","Thus, with respect to the usual notion of simulation, regular resolution is separated from resolution.","An alternative, and weaker, notion for comparing proof systems is that of an \"effective simulation,\" which allows the translation of the formula along with the proof when moving between proof systems.","We prove that regular resolution is equivalent to resolution under effective simulations.","As a corollary, we recover in a black-box fashion a recent result on the hardness of automating regular resolution."],"url":"http://arxiv.org/abs/2402.15871v1","category":"cs.LO"}
{"created":"2024-02-24 17:21:11","title":"Perfect fluid spacetimes and $k$-almost yamabe solitons","abstract":"In this article, we presumed that a perfect fluid is the source of the gravitational field while analyzing the solutions to the Einstein field equations. With this new and creative approach, here we study $k$-almost yamabe solitons and gradient $k$-almost yamabe solitons. First, two examples are constructed to ensure the existence of gradient $k$-almost Yamabe solitons. Then we show that if a perfect fluid spacetime admits a $k$-almost yamabe soliton, then its potential vector field is Killing if and only if the divergence of the potential vector field vanishes. Besides, we prove that if a perfect fluid spacetime permit a $k$-almost yamabe soliton ($g,k,\\rho,\\lambda$), then the integral curves of the vector field $\\rho$ are geodesics, the spacetime becomes stationary and the isotopic pressure and energy density remain invariant under the velocity vector field $\\rho$. Also, we establish that if the potential vector field is pointwise collinear with the velocity vector field and $\\rho(a)=0$ where a is a scalar, then either the perfect fluid spacetime represents phantom era, or the potential function $\\Phi$ is invariant under the velocity vector field $\\rho$. Finally, we prove that if a perfect fluid spacetime permits a gradient $k$-almost yamabe soliton ($g,k,D\\Phi,\\lambda$) and $R, \\lambda, k$ are invariant under $\\rho$, then the vorticity of the fluid vanishes.","sentences":["In this article, we presumed that a perfect fluid is the source of the gravitational field while analyzing the solutions to the Einstein field equations.","With this new and creative approach, here we study $k$-almost yamabe solitons and gradient $k$-almost yamabe solitons.","First, two examples are constructed to ensure the existence of gradient $k$-almost Yamabe solitons.","Then we show that if a perfect fluid spacetime admits a $k$-almost yamabe soliton, then its potential vector field is Killing if and only if the divergence of the potential vector field vanishes.","Besides, we prove that if a perfect fluid spacetime permit a $k$-almost yamabe soliton ($g,k,\\rho,\\lambda$), then the integral curves of the vector field $\\rho$ are geodesics, the spacetime becomes stationary and the isotopic pressure and energy density remain invariant under the velocity vector field $\\rho$. Also, we establish that if the potential vector field is pointwise collinear with the velocity vector field and $\\rho(a)=0$ where a is a scalar, then either the perfect fluid spacetime represents phantom era, or the potential function $\\Phi$ is invariant under the velocity vector field $\\rho$. Finally, we prove that if a perfect fluid spacetime permits a gradient $k$-almost yamabe soliton ($g,k,D\\Phi,\\lambda$) and $R, \\lambda, k$ are invariant under $\\rho$, then the vorticity of the fluid vanishes."],"url":"http://arxiv.org/abs/2402.15868v1","category":"math.DG"}
{"created":"2024-02-24 17:16:02","title":"Local moment matching with Erlang mixtures under automatic roughness penalization","abstract":"We consider the class of Erlang mixtures for the task of density estimation on the positive real line when the only available information is given as local moments, a histogram with potentially higher order moments in some bins. By construction, the obtained moment problem is ill-posed and requires regularization. Several penalties can be used for such a task, such as a lasso penalty for sparsity of the representation, but we focus here on a simplified roughness penalty from the P-splines literature. We show that the corresponding hyperparameter can be selected without cross-validation through the computation of the so-called effective dimension of the estimator, which makes the estimator practical and adapted to these summarized information settings. The flexibility of the local moments representations allows interesting additions such as the enforcement of Value-at-Risk and Tail Value-at-Risk constraints on the resulting estimator, making the procedure suitable for the estimation of heavy-tailed densities.","sentences":["We consider the class of Erlang mixtures for the task of density estimation on the positive real line when the only available information is given as local moments, a histogram with potentially higher order moments in some bins.","By construction, the obtained moment problem is ill-posed and requires regularization.","Several penalties can be used for such a task, such as a lasso penalty for sparsity of the representation, but we focus here on a simplified roughness penalty from the P-splines literature.","We show that the corresponding hyperparameter can be selected without cross-validation through the computation of the so-called effective dimension of the estimator, which makes the estimator practical and adapted to these summarized information settings.","The flexibility of the local moments representations allows interesting additions such as the enforcement of Value-at-Risk and Tail Value-at-Risk constraints on the resulting estimator, making the procedure suitable for the estimation of heavy-tailed densities."],"url":"http://arxiv.org/abs/2402.15866v1","category":"math.ST"}
{"created":"2024-02-24 17:08:45","title":"MATHWELL: Generating Educational Math Word Problems at Scale","abstract":"Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise. We suggest that language models can support K-8 math education by automatically generating problems at scale. To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate. Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators. We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation. Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems. 3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate.","sentences":["Math word problems are critical K-8 educational tools, but writing them is time-consuming and requires domain expertise.","We suggest that language models can support K-8 math education by automatically generating problems at scale.","To be educational, generated problems must be 1) solvable, 2) accurate, and 3) appropriate.","Existing datasets are unlabeled for these criteria, making them ill-suited for training problem generators.","We introduce MATHWELL, a Llama-2 (70B) model iteratively finetuned to generate K-8 math word problems using data from expert annotation.","Using MATHWELL, we generate the largest English word problem dataset to date, containing 20,490 problems.","3,484 are scored by domain experts who find MATHWELL has a 40% higher share of problems that have executable solutions and meet all criteria than alternatives, with 74% of its problems with executable solutions being solvable, accurate, and appropriate."],"url":"http://arxiv.org/abs/2402.15861v1","category":"cs.CL"}
{"created":"2024-02-24 16:59:51","title":"Characterizations of a spacetime of quasi-constant sectional curvature and $\\mathcal{F}(\\mathcal{R})$-gravity","abstract":"The main aim of this article is to investigate a spacetime of quasi-constant sectional curvature. At first, the existence of such a spacetime is established by several examples. We have shown that a spacetime of quasi-constant sectional curvature agrees with the present state of the universe and it represents a Robertson Walker spacetime. Moreover, if the spacetime is Ricci semi-symmetric or Ricci symmetric, then either the spacetime represents a spacetime of constant sectional curvature, or the spacetime represents phantom era. Also, we prove that a Ricci symmetric spacetime of quasi-constant sectional curvature represents a static spacetime and the spacetime under consideration is of Petrov type I, D or O. Finally, we concentrate on a quasi-constant sectional curvature spacetime solution in $\\mathcal{F}(\\mathcal{R})$-gravity. As a result, various energy conditions are studied and analysed our obtained outcomes in terms of a $\\mathcal{F}(\\mathcal{R})$-gravity model.","sentences":["The main aim of this article is to investigate a spacetime of quasi-constant sectional curvature.","At first, the existence of such a spacetime is established by several examples.","We have shown that a spacetime of quasi-constant sectional curvature agrees with the present state of the universe and it represents a Robertson Walker spacetime.","Moreover, if the spacetime is Ricci semi-symmetric or Ricci symmetric, then either the spacetime represents a spacetime of constant sectional curvature, or the spacetime represents phantom era.","Also, we prove that a Ricci symmetric spacetime of quasi-constant sectional curvature represents a static spacetime and the spacetime under consideration is of Petrov type I, D or","O. Finally, we concentrate on a quasi-constant sectional curvature spacetime solution in $\\mathcal{F}(\\mathcal{R})$-gravity.","As a result, various energy conditions are studied and analysed our obtained outcomes in terms of a $\\mathcal{F}(\\mathcal{R})$-gravity model."],"url":"http://arxiv.org/abs/2402.15859v1","category":"math.DG"}
{"created":"2024-02-24 16:52:54","title":"Protocols for Quantum Weak Coin Flipping","abstract":"Weak coin flipping is an important cryptographic primitive -- it is the strongest known secure two-party computation primitive that classically becomes secure only under certain assumptions (e.g. computational hardness), while quantumly there exist protocols that achieve arbitrarily close to perfect security. This breakthrough result was established by Mochon in 2007 [arXiv:0711.4114]. However, his proof relied on the existence of certain unitary operators which was established by a non-constructive argument. Consequently, explicit protocols have remained elusive. In this work, we give exact constructions of related unitary operators. These, together with a new formalism, yield a family of protocols approaching perfect security thereby also simplifying Mochon's proof of existence. We illustrate the construction of explicit weak coin flipping protocols by considering concrete examples (from the aforementioned family of protocols) that are more secure than all previously known protocols.","sentences":["Weak coin flipping is an important cryptographic primitive -- it is the strongest known secure two-party computation primitive that classically becomes secure only under certain assumptions (e.g. computational hardness), while quantumly there exist protocols that achieve arbitrarily close to perfect security.","This breakthrough result was established by Mochon in 2007","[arXiv:0711.4114].","However, his proof relied on the existence of certain unitary operators which was established by a non-constructive argument.","Consequently, explicit protocols have remained elusive.","In this work, we give exact constructions of related unitary operators.","These, together with a new formalism, yield a family of protocols approaching perfect security thereby also simplifying Mochon's proof of existence.","We illustrate the construction of explicit weak coin flipping protocols by considering concrete examples (from the aforementioned family of protocols) that are more secure than all previously known protocols."],"url":"http://arxiv.org/abs/2402.15855v1","category":"quant-ph"}
{"created":"2024-02-24 16:36:48","title":"On the Redistribution of Maximal Extractable Value: A Dynamic Mechanism","abstract":"Maximal Extractable Value (MEV) has emerged as a new frontier in the design of blockchain systems. The marriage between decentralization and finance gives the power to block producers (a.k.a., miners) not only to select and add transactions to the blockchain but, crucially, also to order them so as to extract as much financial gain as possible for themselves. Whilst this price may be unavoidable for the service provided by block producers, users of the chain may in the long run prefer to use less predatory systems. In this paper, we propose to make the MEV extraction rate part of the protocol design space. Our aim is to leverage this parameter to maintain a healthy balance between miners (who need to be compensated) and users (who need to feel encouraged to transact). Inspired by the principles introduced by EIP-1559 for transaction fees, we design a dynamic mechanism which updates the MEV extraction rate with the goal of stabilizing it at a target value. We analyse the evolution of this dynamic mechanism under various market conditions and provide formal guarantees about its long-term performance. Our results show that even when the system behavior is provably chaotic, the dynamics guarantee long-term liveness (survival) and robustness of the system. The main takeaway from our work is that the proposed system exhibits desirable behavior (near-optimal performance) even when it operates in out of equilibrium conditions that are often met in practice. Our work establishes, the first to our knowledge, dynamic framework for the integral problem of MEV sharing between extractors and users.","sentences":["Maximal Extractable Value (MEV) has emerged as a new frontier in the design of blockchain systems.","The marriage between decentralization and finance gives the power to block producers (a.k.a., miners) not only to select and add transactions to the blockchain but, crucially, also to order them so as to extract as much financial gain as possible for themselves.","Whilst this price may be unavoidable for the service provided by block producers, users of the chain may in the long run prefer to use less predatory systems.","In this paper, we propose to make the MEV extraction rate part of the protocol design space.","Our aim is to leverage this parameter to maintain a healthy balance between miners (who need to be compensated) and users (who need to feel encouraged to transact).","Inspired by the principles introduced by EIP-1559 for transaction fees, we design a dynamic mechanism which updates the MEV extraction rate with the goal of stabilizing it at a target value.","We analyse the evolution of this dynamic mechanism under various market conditions and provide formal guarantees about its long-term performance.","Our results show that even when the system behavior is provably chaotic, the dynamics guarantee long-term liveness (survival) and robustness of the system.","The main takeaway from our work is that the proposed system exhibits desirable behavior (near-optimal performance) even when it operates in out of equilibrium conditions that are often met in practice.","Our work establishes, the first to our knowledge, dynamic framework for the integral problem of MEV sharing between extractors and users."],"url":"http://arxiv.org/abs/2402.15849v1","category":"cs.GT"}
{"created":"2024-02-26 10:06:41","title":"Matrix weighted modulation spaces","abstract":"Given a matrix-weight $W$ in the Muckenhoupt class $\\mathbf{A}_p(\\mathbb{R}^n)$, $1\\leq p<\\infty$, we introduce corresponding vector-valued continuous and discrete $\\alpha$-modulation spaces $M^{s,\\alpha}_{p,q}(W)$ and $m^{s,\\alpha}_{p,q}(W)$ and prove their equivalence through the use of adapted tight frames. Compatible notions of molecules and almost diagonal matrices are also introduced, and an application to the study of pseudo-differential operators on vector valued spaces is given.","sentences":["Given a matrix-weight $W$ in the Muckenhoupt class $\\mathbf{A}_p(\\mathbb{R}^n)$, $1\\leq p<\\infty$, we introduce corresponding vector-valued continuous and discrete $\\alpha$-modulation spaces $M^{s,\\alpha}_{p,q}(W)$ and $m^{s,\\alpha}_{p,q}(W)$ and prove their equivalence through the use of adapted tight frames.","Compatible notions of molecules and almost diagonal matrices are also introduced, and an application to the study of pseudo-differential operators on vector valued spaces is given."],"url":"http://arxiv.org/abs/2402.16461v1","category":"math.FA"}
{"created":"2024-02-26 09:17:13","title":"Two-stage Information Spreading Evolution on The Control Role of Announcements","abstract":"Modern social media networks have become an important platform for information competition among countries, regions, companies and other parties. This paper utilizes the research method of spread dynamics to investigate the influence of the control role of announcements in social networks on the spreading process. This paper distinguishes two spreading phases using the authentication intervention as a boundary: the unconfirmed spreading phase and the confirmed spreading phase. Based on the actual rules of spreading in online social networks, two kinds of verification results are defined: true information and false information. The Two-stage information spreading dynamics model is developed to analyze the changes in spreading effects due to different validation results. The impact of the intervention time on the overall spread process is analyzed by combining important control factors such as response cost and time-sensitivity. The validity of the model is verified by comparing the model simulation results with real cases and the adaptive capacity experiments. This work is analyzed and visualized from multiple perspectives, providing more quantitative results. The research content will provide a scientific basis for the intervention behavior of information management control by relevant departments or authorities.","sentences":["Modern social media networks have become an important platform for information competition among countries, regions, companies and other parties.","This paper utilizes the research method of spread dynamics to investigate the influence of the control role of announcements in social networks on the spreading process.","This paper distinguishes two spreading phases using the authentication intervention as a boundary: the unconfirmed spreading phase and the confirmed spreading phase.","Based on the actual rules of spreading in online social networks, two kinds of verification results are defined: true information and false information.","The Two-stage information spreading dynamics model is developed to analyze the changes in spreading effects due to different validation results.","The impact of the intervention time on the overall spread process is analyzed by combining important control factors such as response cost and time-sensitivity.","The validity of the model is verified by comparing the model simulation results with real cases and the adaptive capacity experiments.","This work is analyzed and visualized from multiple perspectives, providing more quantitative results.","The research content will provide a scientific basis for the intervention behavior of information management control by relevant departments or authorities."],"url":"http://arxiv.org/abs/2402.16416v1","category":"cs.SI"}
{"created":"2024-02-26 08:22:14","title":"Nonlocal-to-local limit in linearized viscoelasticity","abstract":"We study the quasistatic evolution of a linear peridynamic Kelvin-Voigt viscoelastic material. More specifically, we consider the gradient flow of a nonlocal elastic energy with respect to a nonlocal viscous dissipation. Following an evolutionary $\\Gamma$-convergence approach, we prove that the solutions of the nonlocal problem converge to the solution of the local problem, when the peridynamic horizon tends to $0$, that is, in the nonlocal-to-local limit.","sentences":["We study the quasistatic evolution of a linear peridynamic Kelvin-Voigt viscoelastic material.","More specifically, we consider the gradient flow of a nonlocal elastic energy with respect to a nonlocal viscous dissipation.","Following an evolutionary $\\Gamma$-convergence approach, we prove that the solutions of the nonlocal problem converge to the solution of the local problem, when the peridynamic horizon tends to $0$, that is, in the nonlocal-to-local limit."],"url":"http://arxiv.org/abs/2402.16386v1","category":"math.AP"}
{"created":"2024-02-26 07:49:40","title":"Adaptive Online Learning of Separable Path Graph Transforms for Intra-prediction","abstract":"Current video coding standards, including H.264/AVC, HEVC, and VVC, employ discrete cosine transform (DCT), discrete sine transform (DST), and secondary to Karhunen-Loeve transforms (KLTs) decorrelate the intra-prediction residuals. However, the efficiency of these transforms in decorrelation can be limited when the signal has a non-smooth and non-periodic structure, such as those occurring in textures with intricate patterns. This paper introduces a novel adaptive separable path graph-based transform (GBT) that can provide better decorrelation than the DCT for intra-predicted texture data. The proposed GBT is learned in an online scenario with sequential K-means clustering, which groups similar blocks during encoding and decoding to adaptively learn the GBT for the current block from previously reconstructed areas with similar characteristics. A signaling overhead is added to the bitstream of each coding block to indicate the usage of the proposed graph-based transform. We assess the performance of this method combined with H.264/AVC intra-coding tools and demonstrate that it can significantly outperform H.264/AVC DCT for intra-predicted texture data.","sentences":["Current video coding standards, including H.264/AVC, HEVC, and VVC, employ discrete cosine transform (DCT), discrete sine transform (DST), and secondary to Karhunen-Loeve transforms (KLTs) decorrelate the intra-prediction residuals.","However, the efficiency of these transforms in decorrelation can be limited when the signal has a non-smooth and non-periodic structure, such as those occurring in textures with intricate patterns.","This paper introduces a novel adaptive separable path graph-based transform (GBT) that can provide better decorrelation than the DCT for intra-predicted texture data.","The proposed GBT is learned in an online scenario with sequential K-means clustering, which groups similar blocks during encoding and decoding to adaptively learn the GBT for the current block from previously reconstructed areas with similar characteristics.","A signaling overhead is added to the bitstream of each coding block to indicate the usage of the proposed graph-based transform.","We assess the performance of this method combined with H.264/AVC intra-coding tools and demonstrate that it can significantly outperform H.264/AVC DCT for intra-predicted texture data."],"url":"http://arxiv.org/abs/2402.16371v1","category":"eess.IV"}
{"created":"2024-02-26 06:26:21","title":"Efficient calculation of magnetocrystalline anisotropy energy using symmetry-adapted Wannier functions","abstract":"Magnetocrystalline anisotropy, a crucial factor in magnetic properties and applications like magnetoresistive random-access memory, often requires extensive $k$-point mesh in first-principles calculations. In this study, we develop a Wannier orbital tight-binding model incorporating crystal and spin symmetries and utilize time-reversal symmetry to divide magnetization components. This model enables efficient computation of magnetocrystalline anisotropy. Applying this method to $\\mathrm{L1_0}$ $\\mathrm{FePt}$ and $\\mathrm{FeNi}$, we calculate the dependence of the anisotropic energy on $k$-point mesh size, chemical potential, spin-orbit interaction, and magnetization direction. The results validate the practicality of the models to the energy order of $10~[\\mathrm{\\mu eV}/f.u.]$.","sentences":["Magnetocrystalline anisotropy, a crucial factor in magnetic properties and applications like magnetoresistive random-access memory, often requires extensive $k$-point mesh in first-principles calculations.","In this study, we develop a Wannier orbital tight-binding model incorporating crystal and spin symmetries and utilize time-reversal symmetry to divide magnetization components.","This model enables efficient computation of magnetocrystalline anisotropy.","Applying this method to $\\mathrm{L1_0}$ $\\mathrm{FePt}$ and $\\mathrm{FeNi}$, we calculate the dependence of the anisotropic energy on $k$-point mesh size, chemical potential, spin-orbit interaction, and magnetization direction.","The results validate the practicality of the models to the energy order of $10~[\\mathrm{\\mu eV}/f.u.]$."],"url":"http://arxiv.org/abs/2402.16331v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-26 05:50:43","title":"Gradient-Guided Modality Decoupling for Missing-Modality Robustness","abstract":"Multimodal learning with incomplete input data (missing modality) is practical and challenging. In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance. Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario. In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities. Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance. In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available. We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis. The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions. Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling.","sentences":["Multimodal learning with incomplete input data (missing modality) is practical and challenging.","In this work, we conduct an in-depth analysis of this challenge and find that modality dominance has a significant negative impact on the model training, greatly degrading the missing modality performance.","Motivated by Grad-CAM, we introduce a novel indicator, gradients, to monitor and reduce modality dominance which widely exists in the missing-modality scenario.","In aid of this indicator, we present a novel Gradient-guided Modality Decoupling (GMD) method to decouple the dependency on dominating modalities.","Specifically, GMD removes the conflicted gradient components from different modalities to achieve this decoupling, significantly improving the performance.","In addition, to flexibly handle modal-incomplete data, we design a parameter-efficient Dynamic Sharing (DS) framework which can adaptively switch on/off the network parameters based on whether one modality is available.","We conduct extensive experiments on three popular multimodal benchmarks, including BraTS 2018 for medical segmentation, CMU-MOSI, and CMU-MOSEI for sentiment analysis.","The results show that our method can significantly outperform the competitors, showing the effectiveness of the proposed solutions.","Our code is released here: https://github.com/HaoWang420/Gradient-guided-Modality-Decoupling."],"url":"http://arxiv.org/abs/2402.16318v1","category":"cs.CV"}
{"created":"2024-02-26 03:13:09","title":"Peak finding algorithm for cluster counting with domain adaptation","abstract":"Cluster counting in drift chamber is the most promising breakthrough in particle identification (PID) technique in particle physics experiment. Reconstruction algorithm is one of the key challenges in cluster counting. In this paper, a semi-supervised domain adaptation (DA) algorithm is developed and applied on the peak finding problem in cluster counting. The algorithm uses optimal transport (OT), which provides geometric metric between distributions, to align the samples between the source (simulation) and target (data) samples, and performs semi-supervised learning with the samples in target domain that are partially labeled with the continuous wavelet transform (CWT) algorithm. The model is validated by the pseudo data with labels, which achieves performance close to the fully supervised model. When applying the algorithm on real experimental data, taken at CERN with a 180 GeV/c muon beam, it shows better classification power than the traditional derivative-based algorithm, and the performance is stable for experimental data samples across varying track lengths.","sentences":["Cluster counting in drift chamber is the most promising breakthrough in particle identification (PID) technique in particle physics experiment.","Reconstruction algorithm is one of the key challenges in cluster counting.","In this paper, a semi-supervised domain adaptation (DA) algorithm is developed and applied on the peak finding problem in cluster counting.","The algorithm uses optimal transport (OT), which provides geometric metric between distributions, to align the samples between the source (simulation) and target (data) samples, and performs semi-supervised learning with the samples in target domain that are partially labeled with the continuous wavelet transform (CWT) algorithm.","The model is validated by the pseudo data with labels, which achieves performance close to the fully supervised model.","When applying the algorithm on real experimental data, taken at CERN with a 180 GeV/c muon beam, it shows better classification power than the traditional derivative-based algorithm, and the performance is stable for experimental data samples across varying track lengths."],"url":"http://arxiv.org/abs/2402.16270v1","category":"physics.ins-det"}
{"created":"2024-02-26 00:25:03","title":"Entropic Cohesion in Vitrimers","abstract":"Vitrimers are polymer networks that can undergo bond exchange reactions. They dynamically rearrange their structures while maintaining their overall integrity, thus resulting in unique properties such as self-healing, reprocessability, shape memory and adaptability. Here, we show that the introduction of dynamic bonds directly impacts the polymer density. For a limiting case, where the dynamic bonds are the same size as the polymer chain bonds, simulations and theory show an enhancement in the density, because these bonds induce an increased cohesive force in the liquid, which is entropic in origin. The crosslinks are well mixed in the bulk but are depleted from the air and polymer interface. These findings implicate density as a key variable in polymers with dynamic crosslinkers, one that can be used to facilely tune their properties.","sentences":["Vitrimers are polymer networks that can undergo bond exchange reactions.","They dynamically rearrange their structures while maintaining their overall integrity, thus resulting in unique properties such as self-healing, reprocessability, shape memory and adaptability.","Here, we show that the introduction of dynamic bonds directly impacts the polymer density.","For a limiting case, where the dynamic bonds are the same size as the polymer chain bonds, simulations and theory show an enhancement in the density, because these bonds induce an increased cohesive force in the liquid, which is entropic in origin.","The crosslinks are well mixed in the bulk but are depleted from the air and polymer interface.","These findings implicate density as a key variable in polymers with dynamic crosslinkers, one that can be used to facilely tune their properties."],"url":"http://arxiv.org/abs/2402.16226v1","category":"cond-mat.soft"}
{"created":"2024-02-25 23:55:29","title":"Scalable Multipartite Entanglement of Remote Rare-earth Ion Qubits","abstract":"Single photon emitters with internal spin are leading contenders for developing quantum repeater networks, enabling long-range entanglement distribution for transformational technologies in communications and sensing. However, scaling beyond current few-node networks will require radical improvements to quantum link efficiencies and fidelities. Solid-state emitters are particularly promising due to their crystalline environment, enabling nanophotonic integration and providing spins for memory and processing. However, inherent spatial and temporal variations in host crystals give rise to static shifts and dynamic fluctuations in optical transition frequencies, posing formidable challenges in establishing large-scale, multipartite entanglement. Here, we introduce a scalable approach to quantum networking that utilizes frequency erasing photon detection in conjunction with adaptive, real-time quantum control. This enables frequency multiplexed entanglement distribution that is also insensitive to deleterious optical frequency fluctuations. Single rare-earth ions are an ideal platform for implementing this protocol due to their long spin coherence, narrow optical inhomogeneous distributions, and long photon lifetimes. Using two 171Yb:YVO4 ions in remote nanophotonic cavities we herald bipartite entanglement and probabilistically teleport quantum states. Then, we extend this protocol to include a third ion and prepare a tripartite W state: a useful input for advanced quantum networking applications. Our results provide a practical route to overcoming universal limitations imposed by non-uniformity and instability in solid-state emitters, whilst also showcasing single rare-earth ions as a scalable platform for the future quantum internet.","sentences":["Single photon emitters with internal spin are leading contenders for developing quantum repeater networks, enabling long-range entanglement distribution for transformational technologies in communications and sensing.","However, scaling beyond current few-node networks will require radical improvements to quantum link efficiencies and fidelities.","Solid-state emitters are particularly promising due to their crystalline environment, enabling nanophotonic integration and providing spins for memory and processing.","However, inherent spatial and temporal variations in host crystals give rise to static shifts and dynamic fluctuations in optical transition frequencies, posing formidable challenges in establishing large-scale, multipartite entanglement.","Here, we introduce a scalable approach to quantum networking that utilizes frequency erasing photon detection in conjunction with adaptive, real-time quantum control.","This enables frequency multiplexed entanglement distribution that is also insensitive to deleterious optical frequency fluctuations.","Single rare-earth ions are an ideal platform for implementing this protocol due to their long spin coherence, narrow optical inhomogeneous distributions, and long photon lifetimes.","Using two 171Yb:YVO4 ions in remote nanophotonic cavities we herald bipartite entanglement and probabilistically teleport quantum states.","Then, we extend this protocol to include a third ion and prepare a tripartite W state: a useful input for advanced quantum networking applications.","Our results provide a practical route to overcoming universal limitations imposed by non-uniformity and instability in solid-state emitters, whilst also showcasing single rare-earth ions as a scalable platform for the future quantum internet."],"url":"http://arxiv.org/abs/2402.16224v1","category":"quant-ph"}
{"created":"2024-02-25 16:43:41","title":"PeriodicLoRA: Breaking the Low-Rank Bottleneck in LoRA Optimization","abstract":"Supervised fine-tuning is the most common method to adapt large language models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive computational resources. Recently, parameter-efficient fine-tuning (PEFT) methods have been widely studied due to its cost-effectiveness. LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional. Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices. In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank. PLoRA has multiple training stages. During each stage, we still update only the LoRA weights. However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then reinitialize the LoRA states. Experimental results show that PLoRA has stronger learning ability, approximately 1.8 times that of LoRA's learning ability at most, but it does not increase memory usage. Further, we introduce a momentum-based unloading strategy for PLoRA to mitigate the training instability.","sentences":["Supervised fine-tuning is the most common method to adapt large language models (LLMs) to downstream tasks, but full fine-tuning LLMs requires massive computational resources.","Recently, parameter-efficient fine-tuning (PEFT) methods have been widely studied due to its cost-effectiveness.","LoRA is one of the most widely used methods, which assumes that the optimization process is essentially low-dimensional.","Although LoRA fine-tuning is effective, there is still a performance gap compared to full fine-tuning, since its weight update is limited to low-rank matrices.","In order to break the low-rank bottleneck in LoRA Optimization, we propose PeriodicLoRA (PLoRA), which accumulates low-rank update matrices multiple times to achieve a higher update rank.","PLoRA has multiple training stages.","During each stage, we still update only the LoRA weights.","However, at the end of each stage, we unload the LoRA weights into the backbone parameters and then reinitialize the LoRA states.","Experimental results show that PLoRA has stronger learning ability, approximately 1.8 times that of LoRA's learning ability at most, but it does not increase memory usage.","Further, we introduce a momentum-based unloading strategy for PLoRA to mitigate the training instability."],"url":"http://arxiv.org/abs/2402.16141v1","category":"cs.CL"}
{"created":"2024-02-25 11:05:21","title":"Fractal Gripper: Adaptive manipulator with mode switching","abstract":"Although the multi-jointed underactuated manipulator is highly dexterous, its grasping capacity does not match that of the parallel jaw gripper. This work introduces a fractal gripper to enhance the grasping capacity of multi-joint underactuated manipulators, preserving their passive clamping features. We describe in detail the working principle and manufacturing process of the fractal gripper. This work, inspired by the 'Fractal Vise' structure, resulted in the invention of a fractal gripper with mode switching capabilities. The fractal gripper inherits the inherent adaptive properties of the fractal structure and realizes the self-resetting function by integrating spring into the original design, thereby enhancing the efficiency of object grasping tasks. The fractal gripper prevents object damage by distributing pressure evenly and applying it at multiple points through its fractal structure during closure. Objects of various shapes are effectively grasped by the fractal gripper, which ensures a safe and secure grasp. The superior performance was provided by the force distribution characteristics of the fractal gripper. By applying the flexible polymer PDMS, which possesses superior elasticity, to the fractal structure's wrapping surface, potential scratching during grasping is effectively prevented, thus protecting the object's geometric surface. Grab experiments with objects of diverse shapes and sizes confirm fractal gripper multi-scale adaptability and superior grasping stability.","sentences":["Although the multi-jointed underactuated manipulator is highly dexterous, its grasping capacity does not match that of the parallel jaw gripper.","This work introduces a fractal gripper to enhance the grasping capacity of multi-joint underactuated manipulators, preserving their passive clamping features.","We describe in detail the working principle and manufacturing process of the fractal gripper.","This work, inspired by the 'Fractal Vise' structure, resulted in the invention of a fractal gripper with mode switching capabilities.","The fractal gripper inherits the inherent adaptive properties of the fractal structure and realizes the self-resetting function by integrating spring into the original design, thereby enhancing the efficiency of object grasping tasks.","The fractal gripper prevents object damage by distributing pressure evenly and applying it at multiple points through its fractal structure during closure.","Objects of various shapes are effectively grasped by the fractal gripper, which ensures a safe and secure grasp.","The superior performance was provided by the force distribution characteristics of the fractal gripper.","By applying the flexible polymer PDMS, which possesses superior elasticity, to the fractal structure's wrapping surface, potential scratching during grasping is effectively prevented, thus protecting the object's geometric surface.","Grab experiments with objects of diverse shapes and sizes confirm fractal gripper multi-scale adaptability and superior grasping stability."],"url":"http://arxiv.org/abs/2402.16057v1","category":"cs.RO"}
{"created":"2024-02-25 08:35:21","title":"FedFDP: Federated Learning with Fairness and Differential Privacy","abstract":"Federated learning (FL) is a new machine learning paradigm to overcome the challenge of data silos and has garnered significant attention. However, through our observations, a globally effective trained model may performance disparities in different clients. This implies that the jointly trained models by clients may lead to unfair outcomes. On the other hand, relevant studies indicate that the transmission of gradients or models in federated learning can also give rise to privacy leakage issues, such as membership inference attacks.   To address the first issue mentioned above, we propose a federated algorithm with fairness, termed FedFair. Building upon FedFair, we introduce privacy protection to form the FedFDP algorithm to address the second issue mentioned above. In FedFDP, we devise a fairness-aware clipping strategy to achieve differential privacy while adjusting fairness. Additionally, for the extra uploaded loss values, we present an adaptive clipping approach to maximize utility. Furthermore, we theoretically prove that our algorithm converges and ensures differential privacy. Lastly, Extensive experimental results demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art solutions in terms of model performance and fairness. The code is accessible at https://anonymous.4open.science/r/FedFDP-E754.","sentences":["Federated learning (FL) is a new machine learning paradigm to overcome the challenge of data silos and has garnered significant attention.","However, through our observations, a globally effective trained model may performance disparities in different clients.","This implies that the jointly trained models by clients may lead to unfair outcomes.","On the other hand, relevant studies indicate that the transmission of gradients or models in federated learning can also give rise to privacy leakage issues, such as membership inference attacks.   ","To address the first issue mentioned above, we propose a federated algorithm with fairness, termed FedFair.","Building upon FedFair, we introduce privacy protection to form the FedFDP algorithm to address the second issue mentioned above.","In FedFDP, we devise a fairness-aware clipping strategy to achieve differential privacy while adjusting fairness.","Additionally, for the extra uploaded loss values, we present an adaptive clipping approach to maximize utility.","Furthermore, we theoretically prove that our algorithm converges and ensures differential privacy.","Lastly, Extensive experimental results demonstrate that FedFair and FedFDP significantly outperforms state-of-the-art solutions in terms of model performance and fairness.","The code is accessible at https://anonymous.4open.science/r/FedFDP-E754."],"url":"http://arxiv.org/abs/2402.16028v1","category":"cs.CR"}
{"created":"2024-02-25 06:32:21","title":"Exploring the Power of Pure Attention Mechanisms in Blind Room Parameter Estimation","abstract":"Dynamic parameterization of acoustic environments has drawn widespread attention in the field of audio processing. Precise representation of local room acoustic characteristics is crucial when designing audio filters for various audio rendering applications. Key parameters in this context include reverberation time (RT60) and geometric room volume. In recent years, neural networks have been extensively applied in the task of blind room parameter estimation. However, there remains a question of whether pure attention mechanisms can achieve superior performance in this task. To address this issue, this study employs blind room parameter estimation based on monaural noisy speech signals. Various model architectures are investigated, including a proposed attention-based model. This model is a convolution-free Audio Spectrogram Transformer, utilizing patch splitting, attention mechanisms, and cross-modality transfer learning from a pretrained Vision Transformer. Experimental results suggest that the proposed attention mechanism-based model, relying purely on attention mechanisms without using convolution, exhibits significantly improved performance across various room parameter estimation tasks, especially with the help of dedicated pretraining and data augmentation schemes. Additionally, the model demonstrates more advantageous adaptability and robustness when handling variable-length audio inputs compared to existing methods.","sentences":["Dynamic parameterization of acoustic environments has drawn widespread attention in the field of audio processing.","Precise representation of local room acoustic characteristics is crucial when designing audio filters for various audio rendering applications.","Key parameters in this context include reverberation time (RT60) and geometric room volume.","In recent years, neural networks have been extensively applied in the task of blind room parameter estimation.","However, there remains a question of whether pure attention mechanisms can achieve superior performance in this task.","To address this issue, this study employs blind room parameter estimation based on monaural noisy speech signals.","Various model architectures are investigated, including a proposed attention-based model.","This model is a convolution-free Audio Spectrogram Transformer, utilizing patch splitting, attention mechanisms, and cross-modality transfer learning from a pretrained Vision Transformer.","Experimental results suggest that the proposed attention mechanism-based model, relying purely on attention mechanisms without using convolution, exhibits significantly improved performance across various room parameter estimation tasks, especially with the help of dedicated pretraining and data augmentation schemes.","Additionally, the model demonstrates more advantageous adaptability and robustness when handling variable-length audio inputs compared to existing methods."],"url":"http://arxiv.org/abs/2402.16003v1","category":"eess.AS"}
{"created":"2024-02-25 03:43:07","title":"An Image Enhancement Method for Improving Small Intestinal Villi Clarity","abstract":"This paper presents, for the first time, an image enhancement methodology designed to enhance the clarity of small intestinal villi in Wireless Capsule Endoscopy (WCE) images. This method first separates the low-frequency and high-frequency components of small intestinal villi images using guided filtering. Subsequently, an adaptive light gain factor is generated based on the low-frequency component, and an adaptive gradient gain factor is derived from the convolution results of the Laplacian operator in different regions of small intestinal villi images. The obtained light gain factor and gradient gain factor are then combined to enhance the high-frequency components. Finally, the enhanced high-frequency component is fused with the original image to achieve adaptive sharpening of the edges of WCE small intestinal villi images. The experiments affirm that, compared to established WCE image enhancement methods, our approach not only accentuates the edge details of WCE small intestine villi images but also skillfully suppresses noise amplification, thereby preventing the occurrence of edge overshooting.","sentences":["This paper presents, for the first time, an image enhancement methodology designed to enhance the clarity of small intestinal villi in Wireless Capsule Endoscopy (WCE) images.","This method first separates the low-frequency and high-frequency components of small intestinal villi images using guided filtering.","Subsequently, an adaptive light gain factor is generated based on the low-frequency component, and an adaptive gradient gain factor is derived from the convolution results of the Laplacian operator in different regions of small intestinal villi images.","The obtained light gain factor and gradient gain factor are then combined to enhance the high-frequency components.","Finally, the enhanced high-frequency component is fused with the original image to achieve adaptive sharpening of the edges of WCE small intestinal villi images.","The experiments affirm that, compared to established WCE image enhancement methods, our approach not only accentuates the edge details of WCE small intestine villi images but also skillfully suppresses noise amplification, thereby preventing the occurrence of edge overshooting."],"url":"http://arxiv.org/abs/2402.15977v1","category":"cs.CV"}
{"created":"2024-02-25 03:15:12","title":"Efficient Online Learning for Networks of Two-Compartment Spiking Neurons","abstract":"The brain-inspired Spiking Neural Networks (SNNs) have garnered considerable research interest due to their superior performance and energy efficiency in processing temporal signals. Recently, a novel multi-compartment spiking neuron model, namely the Two-Compartment LIF (TC-LIF) model, has been proposed and exhibited a remarkable capacity for sequential modelling. However, training the TC-LIF model presents challenges stemming from the large memory consumption and the issue of gradient vanishing associated with the Backpropagation Through Time (BPTT) algorithm. To address these challenges, online learning methodologies emerge as a promising solution. Yet, to date, the application of online learning methods in SNNs has been predominantly confined to simplified Leaky Integrate-and-Fire (LIF) neuron models. In this paper, we present a novel online learning method specifically tailored for networks of TC-LIF neurons. Additionally, we propose a refined TC-LIF neuron model called Adaptive TC-LIF, which is carefully designed to enhance temporal information integration in online learning scenarios. Extensive experiments, conducted on various sequential benchmarks, demonstrate that our approach successfully preserves the superior sequential modeling capabilities of the TC-LIF neuron while incorporating the training efficiency and hardware friendliness of online learning. As a result, it offers a multitude of opportunities to leverage neuromorphic solutions for processing temporal signals.","sentences":["The brain-inspired Spiking Neural Networks (SNNs) have garnered considerable research interest due to their superior performance and energy efficiency in processing temporal signals.","Recently, a novel multi-compartment spiking neuron model, namely the Two-Compartment LIF (TC-LIF) model, has been proposed and exhibited a remarkable capacity for sequential modelling.","However, training the TC-LIF model presents challenges stemming from the large memory consumption and the issue of gradient vanishing associated with the Backpropagation Through Time (BPTT) algorithm.","To address these challenges, online learning methodologies emerge as a promising solution.","Yet, to date, the application of online learning methods in SNNs has been predominantly confined to simplified Leaky Integrate-and-Fire (LIF) neuron models.","In this paper, we present a novel online learning method specifically tailored for networks of TC-LIF neurons.","Additionally, we propose a refined TC-LIF neuron model called Adaptive TC-LIF, which is carefully designed to enhance temporal information integration in online learning scenarios.","Extensive experiments, conducted on various sequential benchmarks, demonstrate that our approach successfully preserves the superior sequential modeling capabilities of the TC-LIF neuron while incorporating the training efficiency and hardware friendliness of online learning.","As a result, it offers a multitude of opportunities to leverage neuromorphic solutions for processing temporal signals."],"url":"http://arxiv.org/abs/2402.15969v1","category":"cs.NE"}
{"created":"2024-02-25 02:36:33","title":"Towards Robust Image Stitching: An Adaptive Resistance Learning against Compatible Attacks","abstract":"Image stitching seamlessly integrates images captured from varying perspectives into a single wide field-of-view image. Such integration not only broadens the captured scene but also augments holistic perception in computer vision applications. Given a pair of captured images, subtle perturbations and distortions which go unnoticed by the human visual system tend to attack the correspondence matching, impairing the performance of image stitching algorithms. In light of this challenge, this paper presents the first attempt to improve the robustness of image stitching against adversarial attacks. Specifically, we introduce a stitching-oriented attack~(SoA), tailored to amplify the alignment loss within overlapping regions, thereby targeting the feature matching procedure. To establish an attack resistant model, we delve into the robustness of stitching architecture and develop an adaptive adversarial training~(AAT) to balance attack resistance with stitching precision. In this way, we relieve the gap between the routine adversarial training and benign models, ensuring resilience without quality compromise. Comprehensive evaluation across real-world and synthetic datasets validate the deterioration of SoA on stitching performance. Furthermore, AAT emerges as a more robust solution against adversarial perturbations, delivering superior stitching results. Code is available at:https://github.com/Jzy2017/TRIS.","sentences":["Image stitching seamlessly integrates images captured from varying perspectives into a single wide field-of-view image.","Such integration not only broadens the captured scene but also augments holistic perception in computer vision applications.","Given a pair of captured images, subtle perturbations and distortions which go unnoticed by the human visual system tend to attack the correspondence matching, impairing the performance of image stitching algorithms.","In light of this challenge, this paper presents the first attempt to improve the robustness of image stitching against adversarial attacks.","Specifically, we introduce a stitching-oriented attack~(SoA), tailored to amplify the alignment loss within overlapping regions, thereby targeting the feature matching procedure.","To establish an attack resistant model, we delve into the robustness of stitching architecture and develop an adaptive adversarial training~(AAT) to balance attack resistance with stitching precision.","In this way, we relieve the gap between the routine adversarial training and benign models, ensuring resilience without quality compromise.","Comprehensive evaluation across real-world and synthetic datasets validate the deterioration of SoA on stitching performance.","Furthermore, AAT emerges as a more robust solution against adversarial perturbations, delivering superior stitching results.","Code is available at:https://github.com/Jzy2017/TRIS."],"url":"http://arxiv.org/abs/2402.15959v1","category":"cs.CV"}
{"created":"2024-02-24 23:56:15","title":"Deep Separable Spatiotemporal Learning for Fast Dynamic Cardiac MRI","abstract":"Dynamic magnetic resonance imaging (MRI) plays an indispensable role in cardiac diagnosis. To enable fast imaging, the k-space data can be undersampled but the image reconstruction poses a great challenge of high-dimensional processing. This challenge leads to necessitate extensive training data in many deep learning reconstruction methods. This work proposes a novel and efficient approach, leveraging a dimension-reduced separable learning scheme that excels even with highly limited training data. We further integrate it with spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an iteration process of a reconstruction model with both temporal low-rankness and spatial sparsity. Intermediate outputs are visualized to provide insights into the network's behavior and enhance its interpretability. Extensive results on cardiac cine datasets show that the proposed DeepSSL is superior to the state-of-the-art methods visually and quantitatively, while reducing the demand for training cases by up to 75%. And its preliminary adaptability to cardiac patients has been verified through experienced radiologists' and cardiologists' blind reader study. Additionally, DeepSSL also benefits for achieving the downstream task of cardiac segmentation with higher accuracy and shows robustness in prospective real-time cardiac MRI.","sentences":["Dynamic magnetic resonance imaging (MRI) plays an indispensable role in cardiac diagnosis.","To enable fast imaging, the k-space data can be undersampled but the image reconstruction poses a great challenge of high-dimensional processing.","This challenge leads to necessitate extensive training data in many deep learning reconstruction methods.","This work proposes a novel and efficient approach, leveraging a dimension-reduced separable learning scheme that excels even with highly limited training data.","We further integrate it with spatiotemporal priors to develop a Deep Separable Spatiotemporal Learning network (DeepSSL), which unrolls an iteration process of a reconstruction model with both temporal low-rankness and spatial sparsity.","Intermediate outputs are visualized to provide insights into the network's behavior and enhance its interpretability.","Extensive results on cardiac cine datasets show that the proposed DeepSSL is superior to the state-of-the-art methods visually and quantitatively, while reducing the demand for training cases by up to 75%.","And its preliminary adaptability to cardiac patients has been verified through experienced radiologists' and cardiologists' blind reader study.","Additionally, DeepSSL also benefits for achieving the downstream task of cardiac segmentation with higher accuracy and shows robustness in prospective real-time cardiac MRI."],"url":"http://arxiv.org/abs/2402.15939v1","category":"eess.IV"}
{"created":"2024-02-24 23:53:39","title":"Interpolation-based immersogeometric analysis methods for multi-material and multi-physics problems","abstract":"Immersed boundary methods are high-order accurate computational tools used to model geometrically complex problems in computational mechanics. While traditional finite element methods require the construction of high-quality boundary-fitted meshes, immersed boundary methods instead embed the computational domain in a background grid. Interpolation-based immersed boundary methods augment existing finite element software to non-invasively implement immersed boundary capabilities through extraction. Extraction interpolates the background basis as a linear combination of Lagrange polynomials defined on a foreground mesh, creating an interpolated basis that can be easily integrated by existing methods. This work extends the interpolation-based immersed boundary method to multi-material and multi-physics problems. Beginning from level-set descriptions of domain geometries, Heaviside enrichment is implemented to accommodate discontinuities in state variable fields across material interfaces. Adaptive refinement with truncated hierarchical B-splines is used to both improve interface geometry representations and resolve large solution gradients near interfaces. Multi-physics problems typically involve coupled fields where each field has unique discretization requirements. This work presents a novel discretization method for coupled problems through the application of extraction, using a single foreground mesh for all fields. Numerical examples illustrate optimal convergence rates for this method in both 2D and 3D, for heat conduction, linear elasticity, and a coupled thermo-mechanical problem. The utility of this method is demonstrated through image-based analysis of a composite sample, where in addition to circumventing typical meshing difficulties, this method reduces the required degrees of freedom compared to classical boundary-fitted finite element methods.","sentences":["Immersed boundary methods are high-order accurate computational tools used to model geometrically complex problems in computational mechanics.","While traditional finite element methods require the construction of high-quality boundary-fitted meshes, immersed boundary methods instead embed the computational domain in a background grid.","Interpolation-based immersed boundary methods augment existing finite element software to non-invasively implement immersed boundary capabilities through extraction.","Extraction interpolates the background basis as a linear combination of Lagrange polynomials defined on a foreground mesh, creating an interpolated basis that can be easily integrated by existing methods.","This work extends the interpolation-based immersed boundary method to multi-material and multi-physics problems.","Beginning from level-set descriptions of domain geometries, Heaviside enrichment is implemented to accommodate discontinuities in state variable fields across material interfaces.","Adaptive refinement with truncated hierarchical B-splines is used to both improve interface geometry representations and resolve large solution gradients near interfaces.","Multi-physics problems typically involve coupled fields where each field has unique discretization requirements.","This work presents a novel discretization method for coupled problems through the application of extraction, using a single foreground mesh for all fields.","Numerical examples illustrate optimal convergence rates for this method in both 2D and 3D, for heat conduction, linear elasticity, and a coupled thermo-mechanical problem.","The utility of this method is demonstrated through image-based analysis of a composite sample, where in addition to circumventing typical meshing difficulties, this method reduces the required degrees of freedom compared to classical boundary-fitted finite element methods."],"url":"http://arxiv.org/abs/2402.15937v1","category":"math.NA"}
{"created":"2024-02-24 23:25:35","title":"Scalable Volt-VAR Optimization using RLlib-IMPALA Framework: A Reinforcement Learning Approach","abstract":"In the rapidly evolving domain of electrical power systems, the Volt-VAR optimization (VVO) is increasingly critical, especially with the burgeoning integration of renewable energy sources. Traditional approaches to learning-based VVO in expansive and dynamically changing power systems are often hindered by computational complexities. To address this challenge, our research presents a novel framework that harnesses the potential of Deep Reinforcement Learning (DRL), specifically utilizing the Importance Weighted Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform. This framework, built upon RLlib-an industry-standard in Reinforcement Learning-ingeniously capitalizes on the distributed computing capabilities and advanced hyperparameter tuning offered by RAY. This design significantly expedites the exploration and exploitation phases in the VVO solution space. Our empirical results demonstrate that our approach not only surpasses existing DRL methods in achieving superior reward outcomes but also manifests a remarkable tenfold reduction in computational requirements. The integration of our DRL agent with the RAY platform facilitates the creation of RLlib-IMPALA, a novel framework that efficiently uses RAY's resources to improve system adaptability and control. RLlib-IMPALA leverages RAY's toolkit to enhance analytical capabilities and significantly speeds up training to become more than 10 times faster than other state-of-the-art DRL methods.","sentences":["In the rapidly evolving domain of electrical power systems, the Volt-VAR optimization (VVO) is increasingly critical, especially with the burgeoning integration of renewable energy sources.","Traditional approaches to learning-based VVO in expansive and dynamically changing power systems are often hindered by computational complexities.","To address this challenge, our research presents a novel framework that harnesses the potential of Deep Reinforcement Learning (DRL), specifically utilizing the Importance Weighted Actor-Learner Architecture (IMPALA) algorithm, executed on the RAY platform.","This framework, built upon RLlib-an industry-standard in Reinforcement Learning-ingeniously capitalizes on the distributed computing capabilities and advanced hyperparameter tuning offered by RAY.","This design significantly expedites the exploration and exploitation phases in the VVO solution space.","Our empirical results demonstrate that our approach not only surpasses existing DRL methods in achieving superior reward outcomes but also manifests a remarkable tenfold reduction in computational requirements.","The integration of our DRL agent with the RAY platform facilitates the creation of RLlib-IMPALA, a novel framework that efficiently uses RAY's resources to improve system adaptability and control.","RLlib-IMPALA leverages RAY's toolkit to enhance analytical capabilities and significantly speeds up training to become more than 10 times faster than other state-of-the-art DRL methods."],"url":"http://arxiv.org/abs/2402.15932v1","category":"cs.LG"}
{"created":"2024-02-24 20:15:31","title":"Multimodal Instruction Tuning with Conditional Mixture of LoRA","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in diverse tasks across different domains, with an increasing focus on improving their zero-shot generalization capabilities for unseen multimodal tasks. Multimodal instruction tuning has emerged as a successful strategy for achieving zero-shot generalization by fine-tuning pre-trained models on diverse multimodal tasks through instructions. As MLLMs grow in complexity and size, the need for parameter-efficient fine-tuning methods like Low-Rank Adaption (LoRA), which fine-tunes with a minimal set of parameters, becomes essential. However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks. To address this, this paper introduces a novel approach that integrates multimodal instruction tuning with Conditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically constructing low-rank adaptation matrices tailored to the unique demands of each input instance, aiming to mitigate task interference. Experimental results on various multimodal evaluation datasets indicate that MixLoRA not only outperforms the conventional LoRA with the same or even higher ranks, demonstrating its efficacy and adaptability in diverse multimodal tasks.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in diverse tasks across different domains, with an increasing focus on improving their zero-shot generalization capabilities for unseen multimodal tasks.","Multimodal instruction tuning has emerged as a successful strategy for achieving zero-shot generalization by fine-tuning pre-trained models on diverse multimodal tasks through instructions.","As MLLMs grow in complexity and size, the need for parameter-efficient fine-tuning methods like Low-Rank Adaption (LoRA), which fine-tunes with a minimal set of parameters, becomes essential.","However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks.","To address this, this paper introduces a novel approach that integrates multimodal instruction tuning with Conditional Mixture-of-LoRA (MixLoRA).","It innovates upon LoRA by dynamically constructing low-rank adaptation matrices tailored to the unique demands of each input instance, aiming to mitigate task interference.","Experimental results on various multimodal evaluation datasets indicate that MixLoRA not only outperforms the conventional LoRA with the same or even higher ranks, demonstrating its efficacy and adaptability in diverse multimodal tasks."],"url":"http://arxiv.org/abs/2402.15896v1","category":"cs.CV"}
{"created":"2024-02-26 12:15:56","title":"Remarks on regularization by noise, convex integration and spontaneous stochasticity","abstract":"This note is devoted to a discussion of the potential links and differences between three topics: regularization by noise, convex integration, spontaneous stochasticity. All of them deal with the effect on large scales of a small-scale perturbation of fluid dynamic equations. The effects sometimes have something in common, like convex integration and spontaneous stochasticity, sometimes they look the opposite, as in regularization by noise. We are not aware of rigorous links or precise explanations of the differences, and hope to drive new research with this comparative examination.","sentences":["This note is devoted to a discussion of the potential links and differences between three topics: regularization by noise, convex integration, spontaneous stochasticity.","All of them deal with the effect on large scales of a small-scale perturbation of fluid dynamic equations.","The effects sometimes have something in common, like convex integration and spontaneous stochasticity, sometimes they look the opposite, as in regularization by noise.","We are not aware of rigorous links or precise explanations of the differences, and hope to drive new research with this comparative examination."],"url":"http://arxiv.org/abs/2402.16525v1","category":"math.PR"}
{"created":"2024-02-26 11:23:42","title":"Cluster Counting Algorithm for Drift Chamber using LSTM and DGCNN","abstract":"Particle identification (PID) of hadrons plays a crucial role in particle physics experiments, especially for flavor physics and jet tagging. The cluster counting method, which measures the number of primary ionizations in gaseous detectors, represents a promising breakthrough in PID. However, developing an effective reconstruction algorithm for cluster counting remains a major challenge. In this study, we address this challenge by proposing a cluster counting algorithm based on long short-term memory and dynamic graph convolutional neural networks. Leveraging Monte Carlo simulated samples, our machine learning-based algorithm surpasses traditional methods. Specifically, it achieves a remarkable 10% improvement in $K/\\pi$ separation for PID performance.","sentences":["Particle identification (PID) of hadrons plays a crucial role in particle physics experiments, especially for flavor physics and jet tagging.","The cluster counting method, which measures the number of primary ionizations in gaseous detectors, represents a promising breakthrough in PID.","However, developing an effective reconstruction algorithm for cluster counting remains a major challenge.","In this study, we address this challenge by proposing a cluster counting algorithm based on long short-term memory and dynamic graph convolutional neural networks.","Leveraging Monte Carlo simulated samples, our machine learning-based algorithm surpasses traditional methods.","Specifically, it achieves a remarkable 10% improvement in $K/\\pi$ separation for PID performance."],"url":"http://arxiv.org/abs/2402.16493v1","category":"hep-ex"}
{"created":"2024-02-26 09:57:25","title":"Autonomous Integration of TSN-unaware Applications with QoS Requirements in TSN Networks","abstract":"Modern industrial networks transport both best-effort and real-time traffic. Time-Sensitive Networking (TSN) was introduced by the IEEE TSN Task Group as an enhancement to Ethernet to provide high quality of service (QoS) for real-time traffic. In a TSN network, applications signal their QoS requirements to the network before transmitting data. The network then allocates resources to meet these requirements. However, TSN-unaware applications can neither perform this registration process nor profit from TSN's QoS benefits. The contributions of this paper are twofold. First, we introduce a novel network architecture in which an additional device autonomously signals the QoS requirements of TSN-unaware applications to the network. Second, we propose a processing method to detect real-time streams in a network and extract the necessary information for the TSN stream signaling. It leverages a Deep Recurrent Neural Network (DRNN) to detect periodic traffic, extracts an accurate traffic description, and uses traffic classification to determine the source application. As a result, our proposal allows TSN-unaware applications to benefit from TSNs QoS guarantees. Our evaluations underline the effectiveness of the proposed architecture and processing method.","sentences":["Modern industrial networks transport both best-effort and real-time traffic.","Time-Sensitive Networking (TSN) was introduced by the IEEE TSN Task Group as an enhancement to Ethernet to provide high quality of service (QoS) for real-time traffic.","In a TSN network, applications signal their QoS requirements to the network before transmitting data.","The network then allocates resources to meet these requirements.","However, TSN-unaware applications can neither perform this registration process nor profit from TSN's QoS benefits.","The contributions of this paper are twofold.","First, we introduce a novel network architecture in which an additional device autonomously signals the QoS requirements of TSN-unaware applications to the network.","Second, we propose a processing method to detect real-time streams in a network and extract the necessary information for the TSN stream signaling.","It leverages a Deep Recurrent Neural Network (DRNN) to detect periodic traffic, extracts an accurate traffic description, and uses traffic classification to determine the source application.","As a result, our proposal allows TSN-unaware applications to benefit from TSNs QoS guarantees.","Our evaluations underline the effectiveness of the proposed architecture and processing method."],"url":"http://arxiv.org/abs/2402.16454v1","category":"cs.NI"}
{"created":"2024-02-26 09:22:42","title":"Sharp blow-up stability for self-similar solutions of the modified Korteweg-de Vries equation","abstract":"We consider the modified Korteweg-de Vries equation. Given a self-similar solution, and a subcritical perturbation of any size, we prove that there exists a unique solution to the equation which behaves at blow-up time as the self-similar solution plus the perturbation. To this end, we develop the first robust analysis in spaces of functions with bounded Fourier transforms. To begin, we prove the local well-posedness in subcritical spaces through an appropriate restriction norm method. As this method is not sufficient to capture the critical self-similar dynamics, we develop an infinite normal form reduction (INFR) to derive time-dependent a priori $L^\\infty$ bounds in frequency variables. Both approaches rely on frequency-restricted estimates, which are specific positive multiplier estimates capable of capturing the oscillatory nature of the equation. As a consequence of our analysis, we also prove local well-posedness for small subcritical perturbations of self-similar solutions at positive time.","sentences":["We consider the modified Korteweg-de Vries equation.","Given a self-similar solution, and a subcritical perturbation of any size, we prove that there exists a unique solution to the equation which behaves at blow-up time as the self-similar solution plus the perturbation.","To this end, we develop the first robust analysis in spaces of functions with bounded Fourier transforms.","To begin, we prove the local well-posedness in subcritical spaces through an appropriate restriction norm method.","As this method is not sufficient to capture the critical self-similar dynamics, we develop an infinite normal form reduction (INFR) to derive time-dependent a priori $L^\\infty$ bounds in frequency variables.","Both approaches rely on frequency-restricted estimates, which are specific positive multiplier estimates capable of capturing the oscillatory nature of the equation.","As a consequence of our analysis, we also prove local well-posedness for small subcritical perturbations of self-similar solutions at positive time."],"url":"http://arxiv.org/abs/2402.16423v1","category":"math.AP"}
{"created":"2024-02-26 09:06:37","title":"First-principles construction of symmetry-informed quantum metrologies","abstract":"Combining quantum and Bayesian principles leads to optimality in metrology, but exact solutions can be hard to find. This work mitigates this problem with a novel class of exactly solvable optimisation equations. For any quantity isomorphic to a location parameter, rules to devise optimal measurements are given in closed form. These are valid for any parameter range, prior information, or state, and the associated estimators apply to finite samples. This framework unifies the metrology of locations, scales, and other parameter types such as relative weights, for which hyperbolic errors are required. But the central advantage lies on its simplifying power: searching for good strategies amounts to identifying which symmetry leaves a state of maximum ignorance invariant, irrespective of error bounds. This reduces the number of calculations needed in practice and enables the rigorous application of quantum metrology to fundamental physics, where symmetries play a key role.","sentences":["Combining quantum and Bayesian principles leads to optimality in metrology, but exact solutions can be hard to find.","This work mitigates this problem with a novel class of exactly solvable optimisation equations.","For any quantity isomorphic to a location parameter, rules to devise optimal measurements are given in closed form.","These are valid for any parameter range, prior information, or state, and the associated estimators apply to finite samples.","This framework unifies the metrology of locations, scales, and other parameter types such as relative weights, for which hyperbolic errors are required.","But the central advantage lies on its simplifying power: searching for good strategies amounts to identifying which symmetry leaves a state of maximum ignorance invariant, irrespective of error bounds.","This reduces the number of calculations needed in practice and enables the rigorous application of quantum metrology to fundamental physics, where symmetries play a key role."],"url":"http://arxiv.org/abs/2402.16410v1","category":"quant-ph"}
{"created":"2024-02-26 08:10:14","title":"Scalable Superconductor Neuron with Ternary Synaptic Connections for Ultra-Fast SNN Hardware","abstract":"A novel high-fan-in differential superconductor neuron structure designed for ultra-high-performance Spiking Neural Network (SNN) accelerators is presented. Utilizing a high-fan-in neuron structure allows us to design SNN accelerators with more synaptic connections, enhancing the overall network capabilities. The proposed neuron design is based on superconductor electronics fabric, incorporating multiple superconducting loops, each with two Josephson Junctions. This arrangement enables each input data branch to have positive and negative inductive coupling, supporting excitatory and inhibitory synaptic data. Compatibility with synaptic devices and thresholding operation is achieved using a single flux quantum (SFQ) pulse-based logic style. The neuron design, along with ternary synaptic connections, forms the foundation for a superconductor-based SNN inference. To demonstrate the capabilities of our design, we train the SNN using snnTorch, augmenting the PyTorch framework. After pruning, the demonstrated SNN inference achieves an impressive 96.1% accuracy on MNIST images. Notably, the network exhibits a remarkable throughput of 8.92 GHz while consuming only 1.5 nJ per inference, including the energy consumption associated with cooling to 4K. These results underscore the potential of superconductor electronics in developing high-performance and ultra-energy-efficient neural network accelerator architectures.","sentences":["A novel high-fan-in differential superconductor neuron structure designed for ultra-high-performance Spiking Neural Network (SNN) accelerators is presented.","Utilizing a high-fan-in neuron structure allows us to design SNN accelerators with more synaptic connections, enhancing the overall network capabilities.","The proposed neuron design is based on superconductor electronics fabric, incorporating multiple superconducting loops, each with two Josephson Junctions.","This arrangement enables each input data branch to have positive and negative inductive coupling, supporting excitatory and inhibitory synaptic data.","Compatibility with synaptic devices and thresholding operation is achieved using a single flux quantum (SFQ) pulse-based logic style.","The neuron design, along with ternary synaptic connections, forms the foundation for a superconductor-based SNN inference.","To demonstrate the capabilities of our design, we train the SNN using snnTorch, augmenting the PyTorch framework.","After pruning, the demonstrated SNN inference achieves an impressive 96.1% accuracy on MNIST images.","Notably, the network exhibits a remarkable throughput of 8.92 GHz while consuming only 1.5 nJ per inference, including the energy consumption associated with cooling to 4K. These results underscore the potential of superconductor electronics in developing high-performance and ultra-energy-efficient neural network accelerator architectures."],"url":"http://arxiv.org/abs/2402.16384v1","category":"cond-mat.supr-con"}
{"created":"2024-02-26 08:03:06","title":"Left-invariant Codazzi tensors and harmonic curvature on Lie groups endowed with a left invariant Lorentzian metric","abstract":"A Lorentzian Lie group is a Lie group endowed with a left invariant Lorentzian metric. We study left-invariant Codazzi tensors on Lorentzian Lie groups. We obtain new results on left-invariant Lorentzian metrics with harmonic curvature and non-parallel Ricci operator. In contrast to the Riemannian case, the Ricci operator of a let-invariant Lorentzian metric can be of four types: diagonal, of type $\\{n-2,z\\bar{z}\\}$, of type $\\{n,a2\\}$ and of type $\\{n,a3\\}$. We first describe Lorentzian Lie algebras with a non-diagonal Codazzi operator and with these descriptions in mind, we study three classes of Lorentzian Lie groups with harmonic curvature. Namely, we give a complete description of the Lie algebra of Lorentzian Lie groups having harmonic curvature and where the Ricci operator is non-diagonal and its diagonal part consists of one real eigenvalue $\\alpha$.","sentences":["A Lorentzian Lie group is a Lie group endowed with a left invariant Lorentzian metric.","We study left-invariant Codazzi tensors on Lorentzian Lie groups.","We obtain new results on left-invariant Lorentzian metrics with harmonic curvature and non-parallel Ricci operator.","In contrast to the Riemannian case, the Ricci operator of a let-invariant Lorentzian metric can be of four types: diagonal, of type $\\{n-2,z\\bar{z}\\}$, of type $\\{n,a2\\}$ and of type $\\{n,","a3\\}$. We first describe Lorentzian Lie algebras with a non-diagonal Codazzi operator and with these descriptions in mind, we study three classes of Lorentzian Lie groups with harmonic curvature.","Namely, we give a complete description of the Lie algebra of Lorentzian Lie groups having harmonic curvature and where the Ricci operator is non-diagonal and its diagonal part consists of one real eigenvalue $\\alpha$."],"url":"http://arxiv.org/abs/2402.16381v1","category":"math.DG"}
{"created":"2024-02-26 07:40:45","title":"SPC-NeRF: Spatial Predictive Compression for Voxel Based Radiance Field","abstract":"Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs. However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost. Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels. Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression. The proposed framework can remove spatial redundancy efficiently for better compression performance.Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency. Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time.","sentences":["Representing the Neural Radiance Field (NeRF) with the explicit voxel grid (EVG) is a promising direction for improving NeRFs.","However, the EVG representation is not efficient for storage and transmission because of the terrific memory cost.","Current methods for compressing EVG mainly inherit the methods designed for neural network compression, such as pruning and quantization, which do not take full advantage of the spatial correlation of voxels.","Inspired by prosperous digital image compression techniques, this paper proposes SPC-NeRF, a novel framework applying spatial predictive coding in EVG compression.","The proposed framework can remove spatial redundancy efficiently for better compression performance.","Moreover, we model the bitrate and design a novel form of the loss function, where we can jointly optimize compression ratio and distortion to achieve higher coding efficiency.","Extensive experiments demonstrate that our method can achieve 32% bit saving compared to the state-of-the-art method VQRF on multiple representative test datasets, with comparable training time."],"url":"http://arxiv.org/abs/2402.16366v1","category":"cs.CV"}
{"created":"2024-02-26 07:24:34","title":"Remark on Estimates in Modulation Spaces for Schr\u00f6dinger Evolution Operators with Sub-quadratic Potentials","abstract":"In this paper we give an estimate for the solution to the Schr\\\"odinger equation with sub-quadratic potentials in modulation spaces by the norm of the initial functions in Wiener-Amalgum spaces.","sentences":["In this paper we give an estimate for the solution to the Schr\\\"odinger equation with sub-quadratic potentials in modulation spaces by the norm of the initial functions in Wiener-Amalgum spaces."],"url":"http://arxiv.org/abs/2402.16360v1","category":"math.AP"}
{"created":"2024-02-26 07:19:36","title":"Development of a Generalizable Data-driven Turbulence Model: Conditioned Field Inversion and Symbolic Regression","abstract":"This paper addresses the issue of predicting separated flows with Reynolds-averaged Navier-Stokes (RANS) turbulence models, which are essential for many engineering tasks. Traditional RANS models usually struggle with this task, so recent efforts have focused on data-driven methods such as field inversion and machine learning (FIML) to correct this issue by adjusting the baseline equations. However, these FIML methods often reduce accuracy in attached boundary layers. To address this issue, we developed a \"conditioned field inversion\" technique. This method adjusts the corrective factor \\b{eta} (used by FIML) in the shear-stress transport (SST) model. It multiplies \\b{eta} with a shield function f_d that is off in the boundary layer and on elsewhere. This maintains the accuracy of the baseline model for the attached flows. We applied both conditioned and classic field inversion to the NASA hump and a curved backward-facing step (CBFS), creating two datasets. These datasets were used to train two models: SR-CND (from our new method) and SR-CLS (from the traditional method). The SR-CND model matches the SR-CLS model in predicting separated flows in various scenarios, such as periodic hills, the NLR7301 airfoil, the 3D SAE car model, and the 3D Ahmed body, and significantly outperforms the baseline SST model. Importantly, the SR-CND model maintains accuracy in the attached boundary layers, whereas the SR-CLS model does not. Therefore, the proposed method improves separated flow predictions while maintaining the accuracy of the original model for attached flows, offering a better way to create data-driven turbulence models.","sentences":["This paper addresses the issue of predicting separated flows with Reynolds-averaged Navier-Stokes (RANS) turbulence models, which are essential for many engineering tasks.","Traditional RANS models usually struggle with this task, so recent efforts have focused on data-driven methods such as field inversion and machine learning (FIML) to correct this issue by adjusting the baseline equations.","However, these FIML methods often reduce accuracy in attached boundary layers.","To address this issue, we developed a \"conditioned field inversion\" technique.","This method adjusts the corrective factor \\b{eta} (used by FIML) in the shear-stress transport (SST) model.","It multiplies \\b{eta} with a shield function f_d","that is off in the boundary layer and on elsewhere.","This maintains the accuracy of the baseline model for the attached flows.","We applied both conditioned and classic field inversion to the NASA hump and a curved backward-facing step (CBFS), creating two datasets.","These datasets were used to train two models: SR-CND (from our new method) and SR-CLS (from the traditional method).","The SR-CND model matches the SR-CLS model in predicting separated flows in various scenarios, such as periodic hills, the NLR7301 airfoil, the 3D SAE car model, and the 3D Ahmed body, and significantly outperforms the baseline SST model.","Importantly, the SR-CND model maintains accuracy in the attached boundary layers, whereas the SR-CLS model does not.","Therefore, the proposed method improves separated flow predictions while maintaining the accuracy of the original model for attached flows, offering a better way to create data-driven turbulence models."],"url":"http://arxiv.org/abs/2402.16355v1","category":"physics.flu-dyn"}
{"created":"2024-02-26 06:28:54","title":"Unveiling the Truth and Facilitating Change: Towards Agent-based Large-scale Social Movement Simulation","abstract":"Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change. Simulating the response of the public and forecasting the potential impact has become increasingly important. However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants. In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types. Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models. We further construct a Twitter-like environment to replicate their response dynamics following trigger events. Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets. Experimental results demonstrate the effectiveness and flexibility of our method.","sentences":["Social media has emerged as a cornerstone of social movements, wielding significant influence in driving societal change.","Simulating the response of the public and forecasting the potential impact has become increasingly important.","However, existing methods for simulating such phenomena encounter challenges concerning their efficacy and efficiency in capturing the behaviors of social movement participants.","In this paper, we introduce a hybrid framework for social media user simulation, wherein users are categorized into two types.","Core users are driven by Large Language Models, while numerous ordinary users are modeled by deductive agent-based models.","We further construct a Twitter-like environment to replicate their response dynamics following trigger events.","Subsequently, we develop a multi-faceted benchmark SoMoSiMu-Bench for evaluation and conduct comprehensive experiments across real-world datasets.","Experimental results demonstrate the effectiveness and flexibility of our method."],"url":"http://arxiv.org/abs/2402.16333v1","category":"cs.CY"}
{"created":"2024-02-26 04:58:53","title":"A Note on Explicit Convergence Rates of Nonlocal Peridynamic Operators in $L^q$-Norm","abstract":"This note investigates the explicit convergence rates of nonlocal peridynamic operators to their classical (local) counterparts in $L^q$-norm. Previous results used Fourier series and hence were restricted to showing convergence in $L^2$. Moreover, convergence rates were not explicit due to the use of the Lebesgue Dominated Convergence Theorem. Some previous results have also used the Taylor Remainder Theorem in differential form, but this often required an assumption of bounded fifth-order derivatives. We do not use these tools, but instead use the Hardy-Littlewood Maximal function, and combine it with the integral form of the Taylor Remainder Theorem. This approach allows us to establish convergence in the $L^q$-norm ($1 \\leq q \\leq \\infty$) for nonlocal peridynamic partial derivatives, which immediately yields convergence rates for the corresponding nonlocal peridynamic divergence, gradient, and curl operators to their local counterparts as the radius (a.k.a., ``horizon'') of the nonlocal interaction $\\delta \\to 0$. Moreover, we obtain an explicit rate of order $\\mathcal{O}(\\delta^2)$. This result contributes to the understanding of the relationship between nonlocal and local models, which is essential for applications in multiscale modeling and simulations.","sentences":["This note investigates the explicit convergence rates of nonlocal peridynamic operators to their classical (local) counterparts in $L^q$-norm.","Previous results used Fourier series and hence were restricted to showing convergence in $L^2$. Moreover, convergence rates were not explicit due to the use of the Lebesgue Dominated Convergence Theorem.","Some previous results have also used the Taylor Remainder Theorem in differential form, but this often required an assumption of bounded fifth-order derivatives.","We do not use these tools, but instead use the Hardy-Littlewood Maximal function, and combine it with the integral form of the Taylor Remainder Theorem.","This approach allows us to establish convergence in the $L^q$-norm ($1 \\leq q \\leq \\infty$) for nonlocal peridynamic partial derivatives, which immediately yields convergence rates for the corresponding nonlocal peridynamic divergence, gradient, and curl operators to their local counterparts as the radius (a.k.a., ``horizon'') of the nonlocal interaction $\\delta \\to 0$.","Moreover, we obtain an explicit rate of order $\\mathcal{O}(\\delta^2)$. This result contributes to the understanding of the relationship between nonlocal and local models, which is essential for applications in multiscale modeling and simulations."],"url":"http://arxiv.org/abs/2402.16303v1","category":"math.AP"}
{"created":"2024-02-26 04:07:53","title":"Characterization and enumeration on Lam\u00e9 equations with finite monodromy","abstract":"We give a complete characterization of the classical Lam\\'e equations $y'' = (n(n + 1)\\wp(z) + B)y$, $n \\in \\Bbb R$, $B \\in \\Bbb C$ on flat tori $E_\\tau = \\Bbb C/(\\Bbb Z + \\Bbb Z\\,\\tau)$ with finite monodromy groups $M$.   Beuker--Waall had shown that such $n$ must lie in a finite number of arithmetic progressions $n_i + \\Bbb N \\subset \\Bbb Q$ and they determined all corresponding $M$. By combining the theory of dessin d'enfants with the geometry of spherical tori, we prove the existence of $(B, \\tau)$ for each such $n$ and provide a description of all such $(n, B, \\tau, M)$.   In particular, for a given $(n, M)$ with $n \\not\\in \\tfrac{1}{2} + \\Bbb Z$, we prove the finiteness of $(B, \\tau)$ and derive an explicit counting formula of them. (The case $n \\in \\tfrac{1}{2} + \\Bbb Z$ is a classical result due to Brioschi--Halphen--Crawford.)   The main ingredients in this work are (1) the definition and classification of basic spherical triangles with finite monodromy and (2) the process of attaching cells corresponding to $n \\mapsto n + 1$ which reduces the problem to the basic case.","sentences":["We give a complete characterization of the classical Lam\\'e equations $y'' = (n(n + 1)\\wp(z)","+","B)y$, $n \\in \\Bbb R$, $B \\in \\Bbb C$ on flat tori $E_\\tau = \\Bbb C/(\\Bbb Z + \\Bbb Z\\,\\tau)$ with finite monodromy groups $M$.   Beuker--Waall had shown that such $n$ must lie in a finite number of arithmetic progressions $n_i + \\Bbb N \\subset \\Bbb Q$","and they determined all corresponding $M$. By combining the theory of dessin d'enfants with the geometry of spherical tori, we prove the existence of $(B, \\tau)$ for each such $n$ and provide a description of all such $(n, B, \\tau, M)$.   In particular, for a given $(n, M)$ with $n \\not\\in \\tfrac{1}{2} + \\Bbb Z$, we prove the finiteness of $(B, \\tau)$ and derive an explicit counting formula of them.","(The case $n \\in \\tfrac{1}{2} + \\Bbb Z$ is a classical result due to Brioschi--Halphen--Crawford.)   ","The main ingredients in this work are (1) the definition and classification of basic spherical triangles with finite monodromy and (2) the process of attaching cells corresponding to $n \\mapsto n + 1$ which reduces the problem to the basic case."],"url":"http://arxiv.org/abs/2402.16286v1","category":"math.DG"}
{"created":"2024-02-26 03:45:51","title":"On the positivity of the Q-curvatures of the conformal metrics","abstract":"We mainly show that for a conformal metric $g=u^{\\frac{4}{n-2m}}|dx|^2$ on $\\mathbb{R}^n$ with $n\\geq 2m+1$, if the higher order Q-curvature $Q^{(2m)}_g$ is positive and has slow decay near infinity, the lower order Q-curvature $Q^{(2)}_g$ and $Q^{(4)}_g$ are both positive if $m$ is at least two.","sentences":["We mainly show that for a conformal metric $g=u^{\\frac{4}{n-2m}}|dx|^2$ on $\\mathbb{R}^n$ with $n\\geq 2m+1$, if the higher order Q-curvature $Q^{(2m)}_g$ is positive and has slow decay near infinity, the lower order Q-curvature $Q^{(2)}_g$ and $Q^{(4)}_g$ are both positive if $m$ is at least two."],"url":"http://arxiv.org/abs/2402.16277v1","category":"math.DG"}
{"created":"2024-02-26 02:33:57","title":"Discrete spectra in phase mixing","abstract":"We study solutions of the collisionless Boltzmann equation (CBE) in a functional Koopman representation. This facilitates the use of linear spectral techniques characteristic of the analysis of Schr{\\\"o}dinger-type equations. For illustrative purposes, we consider the classical phase mixing of a non-interacting distribution function in a quartic potential. Solutions are determined perturbatively relative to a harmonic oscillator. We impose a form of coarse-graining by choosing a finite dimensional basis to represent the distribution function and time evolution operators, which sets a minimum length scale on phase space structure. We observe a relationship between the dimension of the representation and the multiplicity of the harmonic oscillator eigenvalues. The quartic potential splits the degenerate eigenvalues, which drives mixing in the CBE solution.","sentences":["We study solutions of the collisionless Boltzmann equation (CBE) in a functional Koopman representation.","This facilitates the use of linear spectral techniques characteristic of the analysis of Schr{\\\"o}dinger-type equations.","For illustrative purposes, we consider the classical phase mixing of a non-interacting distribution function in a quartic potential.","Solutions are determined perturbatively relative to a harmonic oscillator.","We impose a form of coarse-graining by choosing a finite dimensional basis to represent the distribution function and time evolution operators, which sets a minimum length scale on phase space structure.","We observe a relationship between the dimension of the representation and the multiplicity of the harmonic oscillator eigenvalues.","The quartic potential splits the degenerate eigenvalues, which drives mixing in the CBE solution."],"url":"http://arxiv.org/abs/2402.16252v1","category":"astro-ph.GA"}
{"created":"2024-02-25 22:01:27","title":"New approach method for solving nonlinear differential equations of blood flow with nanoparticle in presence of magnetic field","abstract":"In this paper, effect of physical parameters in presence of magnetic field on heat transfer and flow of third grade non-Newtonian Nanofluid in a porous medium with annular cross sectional analytically has been investigated. The viscosity of Nanofluid categorized in 3 model include constant model and variable models with temperature that in variable category Reynolds Model and Vogel's Model has been used to determine the effect of viscosity in flow filed. analytically solution for velocity, temperature, and nanoparticle concentration are developed by Akbari-Ganji's Method (AGM) that has high proximity with numerical solution (Runge-Kutta 4th-order). Physical parameters that used for extract result for non dimensional variables of nonlinear equations are pressure gradient, Brownian motion parameter, thermophoresis parameter, magnetic field intensity and Grashof number. The results show that the increase in the pressure gradient and Thermophoresis parameter and decrease in the Brownian motion parameter cause the rise in the velocity profile. Also the increase in the Grashof number and decrease in MHD parameter cause the rise in the velocity profile. Furthermore, either increase in Thermophoresis or decrease in Brownian motion parameters results in enhancement in nanoparticle concentration. The highest value of velocity is observed when the Vogel's Model is used for viscosity.","sentences":["In this paper, effect of physical parameters in presence of magnetic field on heat transfer and flow of third grade non-Newtonian Nanofluid in a porous medium with annular cross sectional analytically has been investigated.","The viscosity of Nanofluid categorized in 3 model include constant model and variable models with temperature that in variable category Reynolds Model and Vogel's Model has been used to determine the effect of viscosity in flow filed.","analytically solution for velocity, temperature, and nanoparticle concentration are developed by Akbari-Ganji's Method (AGM) that has high proximity with numerical solution (Runge-Kutta 4th-order).","Physical parameters that used for extract result for non dimensional variables of nonlinear equations are pressure gradient, Brownian motion parameter, thermophoresis parameter, magnetic field intensity and Grashof number.","The results show that the increase in the pressure gradient and Thermophoresis parameter and decrease in the Brownian motion parameter cause the rise in the velocity profile.","Also the increase in the Grashof number and decrease in MHD parameter cause the rise in the velocity profile.","Furthermore, either increase in Thermophoresis or decrease in Brownian motion parameters results in enhancement in nanoparticle concentration.","The highest value of velocity is observed when the Vogel's Model is used for viscosity."],"url":"http://arxiv.org/abs/2402.16208v1","category":"physics.flu-dyn"}
{"created":"2024-02-25 21:44:06","title":"Transferable Water Potentials Using Equivariant Neural Networks","abstract":"Machine learning interatomic potentials (MLIPs) are an emerging modeling technique that promises to provide electronic structure theory accuracy for a fraction of its cost, however, the transferability of MLIPs is a largely unknown factor. Recently, it has been proposed (J. Chem. Phys., 2023, 158, 084111) that MLIPs trained on solely liquid water data cannot describe vapor-liquid equilibrium while recovering the many-body decomposition analysis of gas-phase water clusters, as MLIPs do not directly learn the physically correct interactions of water molecules, limiting transferability. In this work, we show that MLIPs based on an equivariant neural network architecture trained on only 3,200 bulk liquid water structures reproduces liquid-phase water properties (e.g., density within 0.003 g/cm3 between 230 and 365 K), vapor-liquid equilibrium properties up to 550 K, the many-body decomposition analysis of gas-phase water cluster up to six-body interactions, and the relative energy and the vibrational density of states of ice phases. This study highlights that state-of-the-art MLIPs have the potential to develop transferable models for arbitrary phases of water that remain stable in nanosecond-long simulations.","sentences":["Machine learning interatomic potentials (MLIPs) are an emerging modeling technique that promises to provide electronic structure theory accuracy for a fraction of its cost, however, the transferability of MLIPs is a largely unknown factor.","Recently, it has been proposed (J. Chem.","Phys., 2023, 158, 084111) that MLIPs trained on solely liquid water data cannot describe vapor-liquid equilibrium while recovering the many-body decomposition analysis of gas-phase water clusters, as MLIPs do not directly learn the physically correct interactions of water molecules, limiting transferability.","In this work, we show that MLIPs based on an equivariant neural network architecture trained on only 3,200 bulk liquid water structures reproduces liquid-phase water properties (e.g., density within 0.003 g/cm3 between 230 and 365 K), vapor-liquid equilibrium properties up to 550 K, the many-body decomposition analysis of gas-phase water cluster up to six-body interactions, and the relative energy and the vibrational density of states of ice phases.","This study highlights that state-of-the-art MLIPs have the potential to develop transferable models for arbitrary phases of water that remain stable in nanosecond-long simulations."],"url":"http://arxiv.org/abs/2402.16204v1","category":"physics.chem-ph"}
{"created":"2024-02-25 20:38:00","title":"Tubed embeddings","abstract":"We consider the following question: When does a Riemannian manifold admit an embedding with a uniformly thick tubular neighborhood in another Riemannian manifold of large dimension?","sentences":["We consider the following question: When does a Riemannian manifold admit an embedding with a uniformly thick tubular neighborhood in another Riemannian manifold of large dimension?"],"url":"http://arxiv.org/abs/2402.16195v1","category":"math.DG"}
{"created":"2024-02-25 20:11:40","title":"Deep Neural Network Initialization with Sparsity Inducing Activations","abstract":"Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread. Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs. A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\\phi(x)=\\max(0, x-\\tau)$ for $\\tau\\ge 0$) and soft thresholding ($\\phi(x)=0$ for $|x|\\le\\tau$ and $x-\\text{sign}(x)\\tau$ for $|x|>\\tau$). We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map. Numerical experiments verify the theory and show that the proposed magnitude clipped sparsifying activations can be trained with training and test fractional sparsity as high as 85\\% while retaining close to full accuracy.","sentences":["Inducing and leveraging sparse activations during training and inference is a promising avenue for improving the computational efficiency of deep networks, which is increasingly important as network sizes continue to grow and their application becomes more widespread.","Here we use the large width Gaussian process limit to analyze the behaviour, at random initialization, of nonlinear activations that induce sparsity in the hidden outputs.","A previously unreported form of training instability is proven for arguably two of the most natural candidates for hidden layer sparsification; those being a shifted ReLU ($\\phi(x)=\\max(0, x-\\tau)$ for $\\tau\\ge 0$) and soft thresholding ($\\phi(x)=0$ for $|x|\\le\\tau$ and $x-\\text{sign}(x)\\tau$ for $|x|>\\tau$).","We show that this instability is overcome by clipping the nonlinear activation magnitude, at a level prescribed by the shape of the associated Gaussian process variance map.","Numerical experiments verify the theory and show that the proposed magnitude clipped sparsifying activations can be trained with training and test fractional sparsity as high as 85\\% while retaining close to full accuracy."],"url":"http://arxiv.org/abs/2402.16184v1","category":"cs.LG"}
{"created":"2024-02-25 19:47:43","title":"Symmetries of one-loop deformed q-map spaces","abstract":"Q-map spaces form an important class of quaternionic K\\\"ahler manifolds of negative scalar curvature. Their one-loop deformations are always inhomogeneous and have been used to construct cohomogeneity one quaternionic K\\\"ahler manifolds as deformations of homogeneous spaces. Here we study the group of isometries in the deformed case. Our main result is the statement that it always contains a semidirect product of a group of affine transformations of $\\mathbb{R}^{n-1}$ with a Heisenberg group of dimension $2n+1$ for a q-map space of dimension $4n$. The affine group and its action on the normal Heisenberg factor in the semidirect product depend on the cubic affine hypersurface which encodes the q-map space.","sentences":["Q-map spaces form an important class of quaternionic K\\\"ahler manifolds of negative scalar curvature.","Their one-loop deformations are always inhomogeneous and have been used to construct cohomogeneity one quaternionic K\\\"ahler manifolds as deformations of homogeneous spaces.","Here we study the group of isometries in the deformed case.","Our main result is the statement that it always contains a semidirect product of a group of affine transformations of $\\mathbb{R}^{n-1}$ with a Heisenberg group of dimension $2n+1$ for a q-map space of dimension $4n$. The affine group and its action on the normal Heisenberg factor in the semidirect product depend on the cubic affine hypersurface which encodes the q-map space."],"url":"http://arxiv.org/abs/2402.16178v1","category":"math.DG"}
{"created":"2024-02-25 15:08:37","title":"Informed Meta-Learning","abstract":"In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational noise, task distribution shifts, and heterogeneity.","sentences":["In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness.","Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline.","While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge.","This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines.","We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process.","Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational noise, task distribution shifts, and heterogeneity."],"url":"http://arxiv.org/abs/2402.16105v1","category":"cs.LG"}
{"created":"2024-02-25 14:55:11","title":"Notes on the Exact RG equation and the Wheeler-DeWitt equation","abstract":"In this note, in the context of the AdS/CFT correspondence, the holographic derivation of the Wilsonian effective action is proposed. Then, the exact RG equation in the boundary theory is derived from the Wheeler-DeWitt equation of the bulk, following the suggestion of arXiv:hep-th/9912012,arXiv:hep-th/9912018, and arXiv:1010.1264. The relationship between the exact RG and Stochastic Quantization is briefly discussed.","sentences":["In this note, in the context of the AdS/CFT correspondence, the holographic derivation of the Wilsonian effective action is proposed.","Then, the exact RG equation in the boundary theory is derived from the Wheeler-DeWitt equation of the bulk, following the suggestion of arXiv:hep-th/9912012,arXiv:hep-th/9912018, and arXiv:1010.1264.","The relationship between the exact RG and Stochastic Quantization is briefly discussed."],"url":"http://arxiv.org/abs/2402.16100v1","category":"hep-th"}
{"created":"2024-02-25 14:13:20","title":"Differential Galois Groups of Differential Central Simple Algebras and their Projective Representations","abstract":"Let $F$ be a $\\delta-$field (differential field) of characteristic zero with an algebraically closed field of constants $F^\\delta$, $A$ be a $\\delta-F-$central simple algebra, $K$ be a Picard-Vessiot extension for the $\\delta-F-$module $A$ and $\\mathscr G(K|F)$ be the $\\delta-$Galois group of $K$ over $F.$ We prove that a $\\delta-$field extension $L$ of $F,$ having $F^\\delta$ as its field of constants, splits the $\\delta-F-$central simple algebra $A$ if and only if the $\\delta-$field $K$ embeds in $L.$   We then extend the theory of $\\delta-F-$matrix algebras over a $\\delta-$field $F,$ put forward by Magid & Juan (2008), to arbitrary $\\delta-F-$central simple algebras. In particular, we establish a natural bijective correspondence between the isomorphism classes of $\\delta-F-$central simple algebras of dimension $n^2$ over $F$ that are split by the $\\delta-$field $K$ and the classes of inequivalent representations of the algebraic group $\\mathscr G(K|F)$ in $\\mathrm{PGL}_n(F^\\delta).$ We show that $\\mathscr G(K|F)$ is a reductive or a solvable algebraic group if and only if $A$ has certain kinds of $\\delta-$right ideals.","sentences":["Let $F$ be a $\\delta-$field (differential field) of characteristic zero with an algebraically closed field of constants $F^\\delta$, $A$ be a $\\delta-F-$central simple algebra, $K$ be a Picard-Vessiot extension for the $\\delta-F-$module $A$ and $\\mathscr G(K|F)$ be the $\\delta-$Galois group of $K$ over $F.$ We prove that a $\\delta-$field extension $L$ of $F,$ having $F^\\delta$ as its field of constants, splits the $\\delta-F-$central simple algebra $A$ if and only if the $\\delta-$field $K$ embeds in $L.$   We then extend the theory of $\\delta-F-$matrix algebras over a $\\delta-$field $F,$ put forward by Magid & Juan (2008), to arbitrary $\\delta-F-$central simple algebras.","In particular, we establish a natural bijective correspondence between the isomorphism classes of $\\delta-F-$central simple algebras of dimension $n^2$ over $F$ that are split by the $\\delta-$field $K$ and the classes of inequivalent representations of the algebraic group $\\mathscr G(K|F)$ in $\\mathrm{PGL}_n(F^\\delta).$ We show that $\\mathscr G(K|F)$ is a reductive or a solvable algebraic group if and only if $A$ has certain kinds of $\\delta-$right ideals."],"url":"http://arxiv.org/abs/2402.16093v1","category":"math.RA"}
{"created":"2024-02-25 11:44:05","title":"Four-gluon vertex in collinear kinematics","abstract":"To date, the four-gluon vertex is the least explored component of the QCD Lagrangian, mainly due to the vast proliferation of Lorentz and color structures required for its description. In this work we present a nonperturbative study of this vertex, based on the one-loop dressed Schwinger-Dyson equation obtained from the 4PI effective action. A vast simplification is brought about by resorting to ``collinear'' kinematics, where all momenta are parallel to each other, and by appealing to the charge conjugation symmetry in order to eliminate certain color structures. Out of the fifteen form factors that comprise the transversely-projected version of this vertex, two are singled out and studied in detail; the one associated with the classical tensorial structure is moderately suppressed in the infrared regime, while the other diverges logarithmically at the origin. Quite interestingly, both form factors display the property known as ``planar degeneracy'' at a rather high level of accuracy. With these results we construct an effective charge that quantifies the strength of the four-gluon interaction, and compare it with other vertex-derived charges from the gauge sector of QCD.","sentences":["To date, the four-gluon vertex is the least explored component of the QCD Lagrangian, mainly due to the vast proliferation of Lorentz and color structures required for its description.","In this work we present a nonperturbative study of this vertex, based on the one-loop dressed Schwinger-Dyson equation obtained from the 4PI effective action.","A vast simplification is brought about by resorting to ``collinear'' kinematics, where all momenta are parallel to each other, and by appealing to the charge conjugation symmetry in order to eliminate certain color structures.","Out of the fifteen form factors that comprise the transversely-projected version of this vertex, two are singled out and studied in detail; the one associated with the classical tensorial structure is moderately suppressed in the infrared regime, while the other diverges logarithmically at the origin.","Quite interestingly, both form factors display the property known as ``planar degeneracy'' at a rather high level of accuracy.","With these results we construct an effective charge that quantifies the strength of the four-gluon interaction, and compare it with other vertex-derived charges from the gauge sector of QCD."],"url":"http://arxiv.org/abs/2402.16071v1","category":"hep-ph"}
{"created":"2024-02-25 11:25:32","title":"$Q$-voter model with independence on signed random graphs: approximate master equations","abstract":"Approximate master equations are derived for the two-state $q$-voter model with independence on signed random graphs, with negative and positive weights of links corresponding to antagonistic and reinforcing interactions, respectively. Depending on the mean degree of nodes, the size of the $q$-neighborhood, and the fraction of the antagonistic links, with decreasing independence of agents, this model shows a first- or second-order ferromagnetic-like transition to an ordered state with one dominant opinion. Predictions of the approximate master equations concerning this transition exhibit quantitative agreement with results of Monte Carlo simulations in the whole range of parameters of the model, even if predictions of the widely used pair and mean field approximations are inaccurate. Heterogeneous pair approximation derived from the approximate master equations yields results indistinguishable from homogeneous pair approximation studied before and fails in the case of the model on networks with a small and comparable mean degree of nodes and size of the $q$-neighborhood.","sentences":["Approximate master equations are derived for the two-state $q$-voter model with independence on signed random graphs, with negative and positive weights of links corresponding to antagonistic and reinforcing interactions, respectively.","Depending on the mean degree of nodes, the size of the $q$-neighborhood, and the fraction of the antagonistic links, with decreasing independence of agents, this model shows a first- or second-order ferromagnetic-like transition to an ordered state with one dominant opinion.","Predictions of the approximate master equations concerning this transition exhibit quantitative agreement with results of Monte Carlo simulations in the whole range of parameters of the model, even if predictions of the widely used pair and mean field approximations are inaccurate.","Heterogeneous pair approximation derived from the approximate master equations yields results indistinguishable from homogeneous pair approximation studied before and fails in the case of the model on networks with a small and comparable mean degree of nodes and size of the $q$-neighborhood."],"url":"http://arxiv.org/abs/2402.16064v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-25 11:08:19","title":"Gradient-enhanced deep Gaussian processes for multifidelity modelling","abstract":"Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process. Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples. Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities. Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions. Principally, this work extends deep GPs to incorporate gradient data. We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aerodynamic coefficients of a hypersonic flight vehicle over a range of flight conditions and geometries. In both examples, the gradient-enhanced deep GP outperforms a gradient-enhanced linear GP model and their non-gradient-enhanced counterparts.","sentences":["Multifidelity models integrate data from multiple sources to produce a single approximator for the underlying process.","Dense low-fidelity samples are used to reduce interpolation error, while sparse high-fidelity samples are used to compensate for bias or noise in the low-fidelity samples.","Deep Gaussian processes (GPs) are attractive for multifidelity modelling as they are non-parametric, robust to overfitting, perform well for small datasets, and, critically, can capture nonlinear and input-dependent relationships between data of different fidelities.","Many datasets naturally contain gradient data, especially when they are generated by computational models that are compatible with automatic differentiation or have adjoint solutions.","Principally, this work extends deep GPs to incorporate gradient data.","We demonstrate this method on an analytical test problem and a realistic partial differential equation problem, where we predict the aerodynamic coefficients of a hypersonic flight vehicle over a range of flight conditions and geometries.","In both examples, the gradient-enhanced deep GP outperforms a gradient-enhanced linear GP model and their non-gradient-enhanced counterparts."],"url":"http://arxiv.org/abs/2402.16059v1","category":"stat.ML"}
{"created":"2024-02-25 07:28:28","title":"Spectrum Extraction and Clipping for Implicitly Linear Layers","abstract":"We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers. We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work. We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition. By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness. We provide the code for using our methods at https://github.com/Ali-E/FastClip.","sentences":["We show the effectiveness of automatic differentiation in efficiently and correctly computing and controlling the spectrum of implicitly linear operators, a rich family of layer types including all standard convolutional and dense layers.","We provide the first clipping method which is correct for general convolution layers, and illuminate the representational limitation that caused correctness issues in prior work.","We study the effect of the batch normalization layers when concatenated with convolutional layers and show how our clipping method can be applied to their composition.","By comparing the accuracy and performance of our algorithms to the state-of-the-art methods, using various experiments, we show they are more precise and efficient and lead to better generalization and adversarial robustness.","We provide the code for using our methods at https://github.com/Ali-E/FastClip."],"url":"http://arxiv.org/abs/2402.16017v1","category":"cs.LG"}
{"created":"2024-02-25 05:57:31","title":"Bayesian D-Optimal Experimental Designs via Column Subset Selection: The Power of Reweighted Sensors","abstract":"This paper tackles optimal sensor placement for Bayesian linear inverse problems, a popular version of the more general Optimal Experiment Design (OED) problem, using the D-optimality criterion. This is done by establishing connections between sensor placement and Column Subset Selection Problem (CSSP), which is a well-studied problem in Numerical Linear Algebra (NLA). In particular, we use the Golub-Klema-Stewart (GKS) approach which involves computing the truncated Singular Value Decomposition (SVD) followed by a pivoted QR factorization on the right singular vectors. The algorithms are further accelerated by using randomization to compute the low-rank approximation as well as for sampling the indices. The resulting algorithms are robust, computationally efficient, require virtually no parameter tuning, and come with strong theoretical guarantees. We also propose a new approach for OED, called reweighted sensors, that selects $k$ sensors but judiciously recombines sensor information to dramatically improve the D-optimality criterion. Additionally, we develop a method for data completion without solving the inverse problem. Numerical experiments on model inverse problems involving the heat equation and seismic tomography in two spatial dimensions demonstrate the performance of our approaches.","sentences":["This paper tackles optimal sensor placement for Bayesian linear inverse problems, a popular version of the more general Optimal Experiment Design (OED) problem, using the D-optimality criterion.","This is done by establishing connections between sensor placement and Column Subset Selection Problem (CSSP), which is a well-studied problem in Numerical Linear Algebra (NLA).","In particular, we use the Golub-Klema-Stewart (GKS) approach which involves computing the truncated Singular Value Decomposition (SVD) followed by a pivoted QR factorization on the right singular vectors.","The algorithms are further accelerated by using randomization to compute the low-rank approximation as well as for sampling the indices.","The resulting algorithms are robust, computationally efficient, require virtually no parameter tuning, and come with strong theoretical guarantees.","We also propose a new approach for OED, called reweighted sensors, that selects $k$ sensors but judiciously recombines sensor information to dramatically improve the D-optimality criterion.","Additionally, we develop a method for data completion without solving the inverse problem.","Numerical experiments on model inverse problems involving the heat equation and seismic tomography in two spatial dimensions demonstrate the performance of our approaches."],"url":"http://arxiv.org/abs/2402.16000v1","category":"math.NA"}
{"created":"2024-02-25 04:30:04","title":"A unified Fourier slice method to derive ridgelet transform for a variety of depth-2 neural networks","abstract":"To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron. The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\\gamma$ so that a network $\\mathtt{NN}[\\gamma]$ reproduces $f$, i.e. $\\mathtt{NN}[\\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed. However, for a variety of modern neural network architectures, the closed-form expression has not been known. In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\\mathbb{F}_p$, group convolutional networks on abstract Hilbert space $\\mathcal{H}$, fully-connected networks on noncompact symmetric spaces $G/K$, and pooling layers, or the $d$-plane ridgelet transform.","sentences":["To investigate neural network parameters, it is easier to study the distribution of parameters than to study the parameters in each neuron.","The ridgelet transform is a pseudo-inverse operator that maps a given function $f$ to the parameter distribution $\\gamma$ so that a network $\\mathtt{NN}[\\gamma]$ reproduces $f$, i.e. $\\mathtt{NN}[\\gamma]=f$. For depth-2 fully-connected networks on a Euclidean space, the ridgelet transform has been discovered up to the closed-form expression, thus we could describe how the parameters are distributed.","However, for a variety of modern neural network architectures, the closed-form expression has not been known.","In this paper, we explain a systematic method using Fourier expressions to derive ridgelet transforms for a variety of modern networks such as networks on finite fields $\\mathbb{F}_p$, group convolutional networks on abstract Hilbert space $\\mathcal{H}$, fully-connected networks on noncompact symmetric spaces $G/K$, and pooling layers, or the $d$-plane ridgelet transform."],"url":"http://arxiv.org/abs/2402.15984v1","category":"cs.LG"}
{"created":"2024-02-25 03:40:51","title":"Increasing stability in the n-dimensional inverse source problem with multi-frequencies","abstract":"In this paper, we show for the first time the increasing stability of the inverse source problem for the n-dimensional Helmholtz equation at multiple wave numbers, which is different from the two-or three-dimensional Helmholtz equation. In addition, we develop a new, unified approach to study increasing stability in any dimension. The method is based on the Fourier transform and explicit bounds for analytic continuation.","sentences":["In this paper, we show for the first time the increasing stability of the inverse source problem for the n-dimensional Helmholtz equation at multiple wave numbers, which is different from the two-or three-dimensional Helmholtz equation.","In addition, we develop a new, unified approach to study increasing stability in any dimension.","The method is based on the Fourier transform and explicit bounds for analytic continuation."],"url":"http://arxiv.org/abs/2402.15976v1","category":"math.AP"}
{"created":"2024-02-25 03:32:03","title":"Increasing stability for the inverse source problems in electrodynamics","abstract":"We are concerned with increasing stability in the inverse source problems for the time-dependent Maxwell equations in R^3 , where the source term is compactly supported in both time and spatial variables. By using the Fourier transform, sharp bounds of the analytic continuation and the Huygens principle, increasing stability estimates of the L^2 -norm of the source function are obtained. The main goal of this paper is to understand increasing stability for the Maxwell equations in the time domain.","sentences":["We are concerned with increasing stability in the inverse source problems for the time-dependent Maxwell equations in R^3 , where the source term is compactly supported in both time and spatial variables.","By using the Fourier transform, sharp bounds of the analytic continuation and the Huygens principle, increasing stability estimates of the L^2 -norm of the source function are obtained.","The main goal of this paper is to understand increasing stability for the Maxwell equations in the time domain."],"url":"http://arxiv.org/abs/2402.15973v1","category":"math.AP"}
{"created":"2024-02-25 02:54:14","title":"Hierarchical energy signatures using machine learning for operational visibility and diagnostics in automotive manufacturing","abstract":"Manufacturing energy consumption data contains important process signatures required for operational visibility and diagnostics. These signatures may be of different temporal scales, ranging from monthly to sub-second resolutions. We introduce a hierarchical machine learning approach to identify automotive process signatures from paint shop electricity consumption data at varying temporal scales (weekly and daily). A Multi-Layer Perceptron (MLP), a Convolutional Neural Network (CNN), and Principal Component Analysis (PCA) combined with Logistic Regression (LR) are used for the analysis. We validate the utility of the developed algorithms with subject matter experts for (i) better operational visibility, and (ii) identifying energy saving opportunities.","sentences":["Manufacturing energy consumption data contains important process signatures required for operational visibility and diagnostics.","These signatures may be of different temporal scales, ranging from monthly to sub-second resolutions.","We introduce a hierarchical machine learning approach to identify automotive process signatures from paint shop electricity consumption data at varying temporal scales (weekly and daily).","A Multi-Layer Perceptron (MLP), a Convolutional Neural Network (CNN), and Principal Component Analysis (PCA) combined with Logistic Regression (LR) are used for the analysis.","We validate the utility of the developed algorithms with subject matter experts for (i) better operational visibility, and (ii) identifying energy saving opportunities."],"url":"http://arxiv.org/abs/2402.15962v1","category":"cs.LG"}
{"created":"2024-02-25 02:36:14","title":"On the dynamics of three-layer neural networks: initial condensation","abstract":"Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations. This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process. In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks. Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results. Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization.","sentences":["Empirical and theoretical works show that the input weights of two-layer neural networks, when initialized with small values, converge towards isolated orientations.","This phenomenon, referred to as condensation, indicates that the gradient descent methods tend to spontaneously reduce the complexity of neural networks during the training process.","In this work, we elucidate the mechanisms behind the condensation phenomena occurring in the training of three-layer neural networks and distinguish it from the training of two-layer neural networks.","Through rigorous theoretical analysis, we establish the blow-up property of effective dynamics and present a sufficient condition for the occurrence of condensation, findings that are substantiated by experimental results.","Additionally, we explore the association between condensation and the low-rank bias observed in deep matrix factorization."],"url":"http://arxiv.org/abs/2402.15958v1","category":"cs.LG"}
{"created":"2024-02-25 02:33:54","title":"Anti-Instability of Complex Ghost","abstract":"We argue that Lee-Wick's complex ghost appearing in any higher derivative theory is stable and its asymptotic field exists. It may be more appropriate to call it ``anti-unstable\" in the sense that, the more the ghost `decays' into lighter ordinary particles, the larger the probability the ghost remains as itself becomes. This is explicitly shown by analyzing the two-point functions of the ghost Heisenberg field which is obtained as an exact result in the $N\\rightarrow\\infty$ limit in a massive scalar ghost theory with light $O(N)$-vector scalar matter. The anti-instability is a consequence of the fact that the poles of the complex ghost propagator are located on the physical sheet in the complex plane of four-momentum squared. This should be contrasted to the case of the ordinary unstable particle, whose propagator has no pole on the physical sheet.","sentences":["We argue that Lee-Wick's complex ghost appearing in any higher derivative theory is stable and its asymptotic field exists.","It may be more appropriate to call it ``anti-unstable\" in the sense that, the more the ghost `decays' into lighter ordinary particles, the larger the probability the ghost remains as itself becomes.","This is explicitly shown by analyzing the two-point functions of the ghost Heisenberg field which is obtained as an exact result in the $N\\rightarrow\\infty$ limit in a massive scalar ghost theory with light $O(N)$-vector scalar matter.","The anti-instability is a consequence of the fact that the poles of the complex ghost propagator are located on the physical sheet in the complex plane of four-momentum squared.","This should be contrasted to the case of the ordinary unstable particle, whose propagator has no pole on the physical sheet."],"url":"http://arxiv.org/abs/2402.15956v1","category":"hep-th"}
{"created":"2024-02-25 02:29:41","title":"Direct and Inverse scattering in a three-dimensional planar waveguide","abstract":"In this paper, we study the direct and inverse scattering of the Schr\\\"odinger equation in a three-dimensional planar waveguide. For the direct problem, we derive a resonance-free region and resolvent estimates for the resolvent of the Schr\\\"odinger operator in such a geometry. Based on the analysis of the resolvent, several inverse problems are investigated. First, given the potential function, we prove the uniqueness of the inverse source problem with multi-frequency data. We also develop a Fourier-based method to reconstruct the source function. The capability of this method is numerically illustrated by examples. Second, the uniqueness and increased stability of an inverse potential problem from data generated by incident waves are achieved in the absence of the source function. To derive the stability estimate, we use an argument of quantitative analytic continuation in complex theory. Third, we prove the uniqueness of simultaneously determining the source and potential by active boundary data generated by incident waves. In these inverse problems, we only use the limited lateral Dirichlet boundary data at multiple wavenumbers within a finite interval.","sentences":["In this paper, we study the direct and inverse scattering of the Schr\\\"odinger equation in a three-dimensional planar waveguide.","For the direct problem, we derive a resonance-free region and resolvent estimates for the resolvent of the Schr\\\"odinger operator in such a geometry.","Based on the analysis of the resolvent, several inverse problems are investigated.","First, given the potential function, we prove the uniqueness of the inverse source problem with multi-frequency data.","We also develop a Fourier-based method to reconstruct the source function.","The capability of this method is numerically illustrated by examples.","Second, the uniqueness and increased stability of an inverse potential problem from data generated by incident waves are achieved in the absence of the source function.","To derive the stability estimate, we use an argument of quantitative analytic continuation in complex theory.","Third, we prove the uniqueness of simultaneously determining the source and potential by active boundary data generated by incident waves.","In these inverse problems, we only use the limited lateral Dirichlet boundary data at multiple wavenumbers within a finite interval."],"url":"http://arxiv.org/abs/2402.15955v1","category":"math.AP"}
{"created":"2024-02-25 02:16:39","title":"Forward and inverse modeling of depth-of-field effects in background-oriented schlieren","abstract":"We report a novel \"cone-ray\" model of background-oriented schlieren (BOS) imaging that accounts for depth-of-field effects. Reconstructions of the density field performed with this model are far more robust to the blur associated with a finite aperture than conventional reconstructions, which presume a \"thin-ray\" pinhole camera. Our model is characterized and validated using forward evaluations based on simulated and experimental BOS measurements of buoyancy-driven flow and hypersonic flow over a sphere. Moreover, we embed the model in a neural reconstruction algorithm, which is demonstrated with a total variation penalty as well as the compressible Euler equations. Our cone-ray technique dramatically improves the accuracy of BOS reconstructions: the shock interface is well-resolved in all our tests, irrespective of the camera's aperture setting, which spans f-numbers from 22 down to 4.","sentences":["We report a novel \"cone-ray\" model of background-oriented schlieren (BOS) imaging that accounts for depth-of-field effects.","Reconstructions of the density field performed with this model are far more robust to the blur associated with a finite aperture than conventional reconstructions, which presume a \"thin-ray\" pinhole camera.","Our model is characterized and validated using forward evaluations based on simulated and experimental BOS measurements of buoyancy-driven flow and hypersonic flow over a sphere.","Moreover, we embed the model in a neural reconstruction algorithm, which is demonstrated with a total variation penalty as well as the compressible Euler equations.","Our cone-ray technique dramatically improves the accuracy of BOS reconstructions: the shock interface is well-resolved in all our tests, irrespective of the camera's aperture setting, which spans f-numbers from 22 down to 4."],"url":"http://arxiv.org/abs/2402.15954v1","category":"physics.flu-dyn"}
{"created":"2024-02-24 22:32:34","title":"Pretraining Strategy for Neural Potentials","abstract":"We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems. GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields. Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks. From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising. On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs. This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields.","sentences":["We propose a mask pretraining method for Graph Neural Networks (GNNs) to improve their performance on fitting potential energy surfaces, particularly in water systems.","GNNs are pretrained by recovering spatial information related to masked-out atoms from molecules, then transferred and finetuned on atomic forcefields.","Through such pretraining, GNNs learn meaningful prior about structural and underlying physical information of molecule systems that are useful for downstream tasks.","From comprehensive experiments and ablation studies, we show that the proposed method improves the accuracy and convergence speed compared to GNNs trained from scratch or using other pretraining techniques such as denoising.","On the other hand, our pretraining method is suitable for both energy-centric and force-centric GNNs.","This approach showcases its potential to enhance the performance and data efficiency of GNNs in fitting molecular force fields."],"url":"http://arxiv.org/abs/2402.15921v1","category":"cs.LG"}
{"created":"2024-02-24 22:22:02","title":"Sandwich GAN: Image Reconstruction from Phase Mask based Anti-dazzle Imaging","abstract":"Conventional camera systems are susceptible to the adverse effects of laser dazzle, which may over-saturate an image or cause permanent damage to pixels. To address this problem, we developed an approach combining point spread function engineering whereby a wavefront-coded mask in the pupil plane blurs both the laser and scene, together with a deep neural sandwich network. In addition to protecting the sensor, our approach jointly removes the laser from the scene and reconstructs a satisfactory deblurred image. Image recovery is achieved by wrapping two generative adversarial networks (GANs) around a learnable non-blind image deconvolution module. We trained the Sandwich GAN (SGAN) to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold - the point at which the bare system without the phase mask may exhibit damage. The end-to-end training includes physics-based modeling of the imaging system whereby a laser having an arbitrary angle of incidence is superimposed on images from a large publicly available library. The trained system was validated in the laboratory for laser strengths up to $10^4$ times the saturation value. The proposed image restoration model quantitatively and qualitatively outperforms other methods for a wide range of scene contents, illumination conditions, laser strengths, and noise characteristics.","sentences":["Conventional camera systems are susceptible to the adverse effects of laser dazzle, which may over-saturate an image or cause permanent damage to pixels.","To address this problem, we developed an approach combining point spread function engineering whereby a wavefront-coded mask in the pupil plane blurs both the laser and scene, together with a deep neural sandwich network.","In addition to protecting the sensor, our approach jointly removes the laser from the scene and reconstructs a satisfactory deblurred image.","Image recovery is achieved by wrapping two generative adversarial networks (GANs) around a learnable non-blind image deconvolution module.","We trained the Sandwich GAN (SGAN) to suppress the peak laser irradiance as high as $10^6$ times the sensor saturation threshold - the point at which the bare system without the phase mask may exhibit damage.","The end-to-end training includes physics-based modeling of the imaging system whereby a laser having an arbitrary angle of incidence is superimposed on images from a large publicly available library.","The trained system was validated in the laboratory for laser strengths up to $10^4$ times the saturation value.","The proposed image restoration model quantitatively and qualitatively outperforms other methods for a wide range of scene contents, illumination conditions, laser strengths, and noise characteristics."],"url":"http://arxiv.org/abs/2402.15919v1","category":"eess.IV"}
{"created":"2024-02-24 22:21:02","title":"A Finite Element Model for Hydro-thermal Convective Flow in a Porous Medium: Effects of Hydraulic Resistivity and Thermal Diffusivity","abstract":"In this article, a finite element model is implemented to analyze hydro-thermal convective flow in a porous medium. The mathematical model encompasses Darcy's law for incompressible fluid behavior, which is coupled with a convection-diffusion-type energy equation to characterize the temperature in the porous medium. The current investigation presents an efficient, stable, and accurate finite element discretization for the hydro-thermal convective flow model. The well-posedness of the proposed discrete Galerkin finite element formulation is guaranteed due to the decoupling property and the linearity of the numerical method. Computational experiments confirm the optimal convergence rates for a manufactured solution. Several numerical results are obtained for the variations of the hydraulic resistivity and thermal diffusivity. In the present study, the bottom wall is maintained at a constant higher hot temperature while side vertical walls are thermally insulated and the top wall is maintained at a constant cold temperature. Heat transfer rates at the heated bottom wall are presented in terms of local Nusselt number. A linear variation in hydraulic resistivity and a quadratic variation in thermal diffusivity show an increase in the heat transfer rate.","sentences":["In this article, a finite element model is implemented to analyze hydro-thermal convective flow in a porous medium.","The mathematical model encompasses Darcy's law for incompressible fluid behavior, which is coupled with a convection-diffusion-type energy equation to characterize the temperature in the porous medium.","The current investigation presents an efficient, stable, and accurate finite element discretization for the hydro-thermal convective flow model.","The well-posedness of the proposed discrete Galerkin finite element formulation is guaranteed due to the decoupling property and the linearity of the numerical method.","Computational experiments confirm the optimal convergence rates for a manufactured solution.","Several numerical results are obtained for the variations of the hydraulic resistivity and thermal diffusivity.","In the present study, the bottom wall is maintained at a constant higher hot temperature while side vertical walls are thermally insulated and the top wall is maintained at a constant cold temperature.","Heat transfer rates at the heated bottom wall are presented in terms of local Nusselt number.","A linear variation in hydraulic resistivity and a quadratic variation in thermal diffusivity show an increase in the heat transfer rate."],"url":"http://arxiv.org/abs/2402.15917v1","category":"math.NA"}
{"created":"2024-02-24 22:00:50","title":"Black holes thermodynamics with CFT re-scaling","abstract":"In this paper, we study the thermodynamic behavior of charged AdS black holes in a conformal holographic extended thermodynamic. Our setup is constructed using a new dictionary that relates AdS black hole quantities to the corresponding dual conformal field theory (CFT) one, with the conformal factor being treated as a variable thermodynamic. In this thermodynamic study, we investigate the critical phenomena of charged AdS black holes and their relationship to the central charge value, \\(C\\). Additionally, we examine the phase transitions and black hole stability using the free energy and the heat capacity, respectively. Furthermore, by examining the chemical potential, we establish criteria that differentiate between quantum and classical black hole behaviors. Our setup highlights one of the key findings, namely traditional black hole thermodynamics acts as a boundary between quantum and classical regimes.","sentences":["In this paper, we study the thermodynamic behavior of charged AdS black holes in a conformal holographic extended thermodynamic.","Our setup is constructed using a new dictionary that relates AdS black hole quantities to the corresponding dual conformal field theory (CFT) one, with the conformal factor being treated as a variable thermodynamic.","In this thermodynamic study, we investigate the critical phenomena of charged AdS black holes and their relationship to the central charge value, \\(C\\).","Additionally, we examine the phase transitions and black hole stability using the free energy and the heat capacity, respectively.","Furthermore, by examining the chemical potential, we establish criteria that differentiate between quantum and classical black hole behaviors.","Our setup highlights one of the key findings, namely traditional black hole thermodynamics acts as a boundary between quantum and classical regimes."],"url":"http://arxiv.org/abs/2402.15913v1","category":"hep-th"}
{"created":"2024-02-24 21:10:41","title":"Investigating the basis set convergence of diagrammatically decomposed coupled-cluster correlation energy contributions for the uniform electron gas","abstract":"We investigate the convergence of coupled-cluster correlation energies and related quantities with respect to the employed basis set size for the uniform electron gas to gain a better understanding of the basis set incompleteness error. To this end, coupled-cluster doubles (CCD) theory is applied to the three dimensional uniform electron gas for a range of densities, basis set sizes and electron numbers. We present a detailed analysis of individual, diagrammatically decomposed contributions to the amplitudes at the level of CCD theory. In particular, we show that only two terms from the amplitude equations contribute to the asymptotic large-momentum behavior of the transition structure factor, corresponding to the cusp region at short interelectronic distances. However, due to the coupling present in the amplitude equations, all decomposed correlation energy contributions show the same asymptotic convergence behavior to the complete basis set limit. These findings provide an additional rationale for the success of a recently proposed correction to the basis set incompleteness error (BSIE) of coupled-cluster theory. Lastly, we examine the BSIE in the coupled-cluster doubles plus perturbative triples [CCD(T)] method, as well as in the newly proposed coupled-cluster doubles plus complete perturbative triples [CCD(cT)] method.","sentences":["We investigate the convergence of coupled-cluster correlation energies and related quantities with respect to the employed basis set size for the uniform electron gas to gain a better understanding of the basis set incompleteness error.","To this end, coupled-cluster doubles (CCD) theory is applied to the three dimensional uniform electron gas for a range of densities, basis set sizes and electron numbers.","We present a detailed analysis of individual, diagrammatically decomposed contributions to the amplitudes at the level of CCD theory.","In particular, we show that only two terms from the amplitude equations contribute to the asymptotic large-momentum behavior of the transition structure factor, corresponding to the cusp region at short interelectronic distances.","However, due to the coupling present in the amplitude equations, all decomposed correlation energy contributions show the same asymptotic convergence behavior to the complete basis set limit.","These findings provide an additional rationale for the success of a recently proposed correction to the basis set incompleteness error (BSIE) of coupled-cluster theory.","Lastly, we examine the BSIE in the coupled-cluster doubles plus perturbative triples [CCD(T)] method, as well as in the newly proposed coupled-cluster doubles plus complete perturbative triples","[CCD(cT)] method."],"url":"http://arxiv.org/abs/2402.15907v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-24 19:35:47","title":"On real and imaginary roots of generalised Okamoto polynomials","abstract":"Recently, B. Yang and J. Yang derived a family of rational solutions to the Sasa-Satsuma equation, and showed that any of its members constitutes a partial-rogue wave provided that an associated generalised Okamoto polynomial has no real roots or no imaginary roots. In this paper, we derive exact formulas for the number of real and the number of imaginary roots of the generalised Okamoto polynomials. On the one hand, this yields a list of partial-rogue waves that satisfy the Sasa-Satsuma equation. On the other hand, it gives families of rational solutions of the fourth Painlev\\'e equation that are pole-free on either the real line or the imaginary line. To obtain these formulas, we develop an algorithmic procedure to derive the qualitative distribution of singularities on the real line for real solutions of Painlev\\'e equations, starting from the known distribution for a seed solution, through the action of B\\\"acklund transformations on the rational surfaces forming their spaces of initial conditions.","sentences":["Recently, B. Yang and J. Yang derived a family of rational solutions to the Sasa-Satsuma equation, and showed that any of its members constitutes a partial-rogue wave provided that an associated generalised Okamoto polynomial has no real roots or no imaginary roots.","In this paper, we derive exact formulas for the number of real and the number of imaginary roots of the generalised Okamoto polynomials.","On the one hand, this yields a list of partial-rogue waves that satisfy the Sasa-Satsuma equation.","On the other hand, it gives families of rational solutions of the fourth Painlev\\'e equation that are pole-free on either the real line or the imaginary line.","To obtain these formulas, we develop an algorithmic procedure to derive the qualitative distribution of singularities on the real line for real solutions of Painlev\\'e equations, starting from the known distribution for a seed solution, through the action of B\\\"acklund transformations on the rational surfaces forming their spaces of initial conditions."],"url":"http://arxiv.org/abs/2402.15887v1","category":"nlin.SI"}
{"created":"2024-02-24 19:29:54","title":"Symbolic Listings as Computation","abstract":"We propose an algebraic model of computation which formally relates symbolic listings, complexity of Boolean functions, and low depth arithmetic circuit complexity. In this model algorithms are arithmetic formula expressing symbolic listings of YES instances of Boolean functions, and computation is executed via partial differential operators. We consider the Chow rank of an arithmetic formula as a measure of complexity and establish the Chow rank of multilinear polynomials with totally non-overlapping monomial support. We also provide Chow rank non-decreasing transformations from sets of graphs to sets of functional graphs.","sentences":["We propose an algebraic model of computation which formally relates symbolic listings, complexity of Boolean functions, and low depth arithmetic circuit complexity.","In this model algorithms are arithmetic formula expressing symbolic listings of YES instances of Boolean functions, and computation is executed via partial differential operators.","We consider the Chow rank of an arithmetic formula as a measure of complexity and establish the Chow rank of multilinear polynomials with totally non-overlapping monomial support.","We also provide Chow rank non-decreasing transformations from sets of graphs to sets of functional graphs."],"url":"http://arxiv.org/abs/2402.15885v1","category":"cs.DM"}
{"created":"2024-02-24 19:06:41","title":"Fusion Encoder Networks","abstract":"In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.","sentences":["In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs.","The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors).","The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel.","The fact that these networks are constant depth means that backpropagation works well.","We note that currently the performance of FENs is only conjectured as we are yet to implement them."],"url":"http://arxiv.org/abs/2402.15883v1","category":"cs.LG"}
{"created":"2024-02-24 17:04:24","title":"Path constrained unbalanced optimal transport","abstract":"Dynamical formulations of optimal transport (OT) frame the task of comparing distributions as a variational problem which searches for a path between distributions minimizing a kinetic energy functional. In applications, it is frequently natural to require paths of distributions to satisfy additional conditions. Inspired by this, we introduce a model for dynamical OT which incorporates constraints on the space of admissible paths into the framework of unbalanced OT, where the source and target measures are allowed to have a different total mass. Our main results establish, for several general families of constraints, the existence of solutions to the variational problem which defines this path constrained unbalanced optimal transport framework. These results are primarily concerned with distributions defined on a Euclidean space, but we extend them to distributions defined over parallelizable Riemannian manifolds as well. We also consider metric properties of our framework, showing that, for certain types of constraints, our model defines a metric on the relevant space of distributions. This metric is shown to arise as a geodesic distance of a Riemannian metric, obtained through an analogue of Otto's submersion in the classical OT setting.","sentences":["Dynamical formulations of optimal transport (OT) frame the task of comparing distributions as a variational problem which searches for a path between distributions minimizing a kinetic energy functional.","In applications, it is frequently natural to require paths of distributions to satisfy additional conditions.","Inspired by this, we introduce a model for dynamical OT which incorporates constraints on the space of admissible paths into the framework of unbalanced OT, where the source and target measures are allowed to have a different total mass.","Our main results establish, for several general families of constraints, the existence of solutions to the variational problem which defines this path constrained unbalanced optimal transport framework.","These results are primarily concerned with distributions defined on a Euclidean space, but we extend them to distributions defined over parallelizable Riemannian manifolds as well.","We also consider metric properties of our framework, showing that, for certain types of constraints, our model defines a metric on the relevant space of distributions.","This metric is shown to arise as a geodesic distance of a Riemannian metric, obtained through an analogue of Otto's submersion in the classical OT setting."],"url":"http://arxiv.org/abs/2402.15860v1","category":"math.OC"}
{"created":"2024-02-24 16:53:06","title":"Radiative $E1$ transitions between $^3P_1$ and $^3S_1$ quarkonium states","abstract":"In this work we study the E1 decay processes, $^3P_1$ $\\rightarrow$ $^3S_1\\gamma$, and $^3S_1$ $\\rightarrow$ $^3P_1\\gamma$ in the framework of Bethe-Salpeter equation and calculate their decay widths. We have used algebraic forms of Salpeter wave functions obtained through analytic solutions of mass spectral equations for ground and excited states of $^3S_1$, and $^3P_1$ equal mass quarkonia in approximate harmonic oscillator basis to do analytic calculations of their decay widths. These decay widths have been compared with data and other models.","sentences":["In this work we study the E1 decay processes, $^3P_1$ $\\rightarrow$ $^3S_1\\gamma$, and $^3S_1$ $\\rightarrow$ $^3P_1\\gamma$ in the framework of Bethe-Salpeter equation and calculate their decay widths.","We have used algebraic forms of Salpeter wave functions obtained through analytic solutions of mass spectral equations for ground and excited states of $^3S_1$, and $^3P_1$ equal mass quarkonia in approximate harmonic oscillator basis to do analytic calculations of their decay widths.","These decay widths have been compared with data and other models."],"url":"http://arxiv.org/abs/2402.15856v1","category":"hep-ph"}
{"created":"2024-02-24 16:50:51","title":"Impact of projective curvature tensor in $f\\left(R,G\\right)$, $f\\left(R,T\\right)$ and $f\\left(R,L_{m}\\right)$-gravity","abstract":"This article concerns with the characterization of a spacetime and modified gravity, such as $f\\left(R,G\\right)$, $f\\left(R,T\\right)$ and $f\\left(R,L_{m}\\right)$-gravity equipped with the projective curvature tensor. We establish that a projectively flat perfect fluid spacetime represents dark energy era. Also, we prove that a projectively flat perfect fluid spacetime is either locally isometric to Minkowski spacetime or a de-Sitter spacetime. Furthermore, it is shown that a perfect fluid spacetime permitting harmonic projective curvature tensor becomes a generalized Robertson-Walker spacetime and is of Petrov type $I$, $D$ or $O$. Lastly, we investigate the effect of projectively flat perfect fluid spacetime solutions in $f\\left(R,G\\right)$, $f\\left(R,T\\right)$ and $f\\left(R,L_{m}\\right)$-gravity, respectively. We also investigate the spacetime as a $f\\left(R,G\\right)$-gravity solution of and use the flat Friedmann-Robertson-Walker metric to establish a relation among jerk, snap, and deceleration parameters. Numerous energy conditions are studied in terms of Ricci scalar with the model $f\\left(R,G\\right)=\\exp(R)+\\alpha \\left(6G\\right)^{\\beta}$. For this model, the strong energy condition is violated but the weak, dominant and null energy conditions are fulfilled, which is in excellent accordance with current observational investigations that show the universe is now accelerating.","sentences":["This article concerns with the characterization of a spacetime and modified gravity, such as $f\\left(R,G\\right)$, $f\\left(R,T\\right)$ and $f\\left(R,L_{m}\\right)$-gravity equipped with the projective curvature tensor.","We establish that a projectively flat perfect fluid spacetime represents dark energy era.","Also, we prove that a projectively flat perfect fluid spacetime is either locally isometric to Minkowski spacetime or a de-Sitter spacetime.","Furthermore, it is shown that a perfect fluid spacetime permitting harmonic projective curvature tensor becomes a generalized Robertson-Walker spacetime and is of Petrov type $I$, $D$ or $O$. Lastly, we investigate the effect of projectively flat perfect fluid spacetime solutions in $f\\left(R,G\\right)$, $f\\left(R,T\\right)$ and $f\\left(R,L_{m}\\right)$-gravity, respectively.","We also investigate the spacetime as a $f\\left(R,G\\right)$-gravity solution of and use the flat Friedmann-Robertson-Walker metric to establish a relation among jerk, snap, and deceleration parameters.","Numerous energy conditions are studied in terms of Ricci scalar with the model $f\\left(R,G\\right)=\\exp(R)+\\alpha \\left(6G\\right)^{\\beta}$.","For this model, the strong energy condition is violated but the weak, dominant and null energy conditions are fulfilled, which is in excellent accordance with current observational investigations that show the universe is now accelerating."],"url":"http://arxiv.org/abs/2402.15854v1","category":"gr-qc"}
{"created":"2024-02-24 16:50:10","title":"RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation","abstract":"Adversarial camouflage is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance. One promising approach involves using differentiable neural renderers to facilitate adversarial camouflage optimization through gradient back-propagation. However, existing methods often struggle to capture environmental characteristics during the rendering process or produce adversarial textures that can precisely map to the target vehicle, resulting in suboptimal attack performance. Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios. To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA. The core of RAUCA is a novel neural rendering component, Neural Renderer Plus (NRP), which can accurately project vehicle textures and render images with environmental characteristics such as lighting and weather. In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the NRP to enhance the attack robustness. Experimental results on six popular object detectors show that RAUCA consistently outperforms existing methods in both simulation and real-world settings.","sentences":["Adversarial camouflage is a widely used physical attack against vehicle detectors for its superiority in multi-view attack performance.","One promising approach involves using differentiable neural renderers to facilitate adversarial camouflage optimization through gradient back-propagation.","However, existing methods often struggle to capture environmental characteristics during the rendering process or produce adversarial textures that can precisely map to the target vehicle, resulting in suboptimal attack performance.","Moreover, these approaches neglect diverse weather conditions, reducing the efficacy of generated camouflage across varying weather scenarios.","To tackle these challenges, we propose a robust and accurate camouflage generation method, namely RAUCA.","The core of RAUCA is a novel neural rendering component, Neural Renderer Plus (NRP), which can accurately project vehicle textures and render images with environmental characteristics such as lighting and weather.","In addition, we integrate a multi-weather dataset for camouflage generation, leveraging the NRP to enhance the attack robustness.","Experimental results on six popular object detectors show that RAUCA consistently outperforms existing methods in both simulation and real-world settings."],"url":"http://arxiv.org/abs/2402.15853v1","category":"cs.CV"}
{"created":"2024-02-24 16:31:27","title":"A family of maps and a vector field on plane polygons","abstract":"We study, theoretically and experimentally, a 1-parameter family of transformations and their limiting vector field on the space of plane polygons. These transformations are discrete analogs of completely integrable transformation on closed plane curves, known as the bicycle correspondence, that is a geometric realization of the B\\\"acklund transformation of the planar filament equation. For odd-gons, we construct a symplectic form on the quotient space by parallel translations and show that the transformations are symplectic, and the vector field is Hamiltonian. In the case of triangles, we prove complete integrability of the respective vector field and provide evidence for the conjecture that the transformations are integrable as well.","sentences":["We study, theoretically and experimentally, a 1-parameter family of transformations and their limiting vector field on the space of plane polygons.","These transformations are discrete analogs of completely integrable transformation on closed plane curves, known as the bicycle correspondence, that is a geometric realization of the B\\\"acklund transformation of the planar filament equation.","For odd-gons, we construct a symplectic form on the quotient space by parallel translations and show that the transformations are symplectic, and the vector field is Hamiltonian.","In the case of triangles, we prove complete integrability of the respective vector field and provide evidence for the conjecture that the transformations are integrable as well."],"url":"http://arxiv.org/abs/2402.15848v1","category":"math.DS"}
{"created":"2024-02-24 16:29:09","title":"Investigations on a Riemannian manifold with a semi-symmetric non-metric connection and gradient solitons","abstract":"This article carries out the investigation of a three-dimensional Riemannian manifold $N^3$ endowed with a semi-symmetric type non-metric connection. Firstly, we construct a non-trivial example to prove the existence of a semi-symmetric type non-metric connection on $N^{3}$. It is established that a $N^3$ with the semi-symmetric type non-metric connection, whose metric is a gradient Ricci soliton, is a manifold of constant sectional curvature with respect to the semi-symmetric type non-metric connection. Moreover, we prove that if the Riemannian metric of $N^3$ with the semi-symmetric type non-metric connection is a gradient Yamabe soliton, then either $N^{3}$ is a manifold of constant scalar curvature or the gradient Yamabe soliton is trivial with respect to the semi-symmetric type non-metric connection. We also characterize the manifold $N^3$ with a semi-symmetric type non-metric connection whose metrics are Einstein solitons and $m$-quasi Einstein solitons of gradient type, respectively.","sentences":["This article carries out the investigation of a three-dimensional Riemannian manifold $N^3$ endowed with a semi-symmetric type non-metric connection.","Firstly, we construct a non-trivial example to prove the existence of a semi-symmetric type non-metric connection on $N^{3}$. It is established that a $N^3$ with the semi-symmetric type non-metric connection, whose metric is a gradient Ricci soliton, is a manifold of constant sectional curvature with respect to the semi-symmetric type non-metric connection.","Moreover, we prove that if the Riemannian metric of $N^3$ with the semi-symmetric type non-metric connection is a gradient Yamabe soliton, then either $N^{3}$ is a manifold of constant scalar curvature or the gradient Yamabe soliton is trivial with respect to the semi-symmetric type non-metric connection.","We also characterize the manifold $N^3$ with a semi-symmetric type non-metric connection whose metrics are Einstein solitons and $m$-quasi Einstein solitons of gradient type, respectively."],"url":"http://arxiv.org/abs/2402.15846v1","category":"math.DG"}
{"created":"2024-02-26 12:43:26","title":"PDRs4All VII. The 3.3 $\u03bc$m aromatic infrared band as a tracer of physical properties of the ISM in galaxies","abstract":"Aromatic infrared bands (AIBs) are a set of broad emission bands at 3.3, 6.2, 7.7, 8.6, 11.2, and 12.7 $\\mu$m, seen in the infrared spectra of most galaxies. With JWST, the 3.3 $\\mu$m AIB can in principle be detected up to a redshift of $\\sim$ 7. Relating the evolution of the 3.3 $\\mu$m AIB to local physical properties of the ISM is thus of paramount importance. By applying a dedicated machine learning algorithm to JWST NIRSpec observations of the Orion Bar photodissociation region obtained as part of the PDRs4All Early Release Science (ERS) program, we extracted two template spectra capturing the evolution of the AIB-related emission in the 3.2-3.6 $\\mu$m range, which includes the AIB at 3.3 $\\mu$m and its main satellite band at 3.4 $\\mu$m. In the Orion Bar, we analyze the spatial distribution of the templates and their relationship with the fluorescent emission of H$_2$ in the near infrared. We find that one template (\"AIB$_{\\rm Irrad}$\") traces regions of neutral atomic gas with strong far-UV fields, while the other template (\"AIB$_{\\rm Shielded}$\") corresponds to shielded regions with lower FUV fields and a higher molecular gas fraction. We then show that these two templates can be used to fit the NIRSpec AIB-related spectra of nearby galaxies. The relative weight of the two templates (AIB$_{\\rm Irrad/Shielded}$) is a tracer of the radiative feedback from massive stars on the ISM. We derive an estimate of AIB$_{\\rm Irrad/Shielded}$ in a $z$ = 4.22 lensed galaxy, and find that it has a lower value than for local galaxies. This pilot study illustrates how a detailed analysis of AIB emission in nearby regions can be used to probe the physical conditions of the extragalactic ISM.","sentences":["Aromatic infrared bands (AIBs) are a set of broad emission bands at 3.3, 6.2, 7.7, 8.6, 11.2, and 12.7 $\\mu$m, seen in the infrared spectra of most galaxies.","With JWST, the 3.3 $\\mu$m AIB can in principle be detected up to a redshift of $\\sim$ 7.","Relating the evolution of the 3.3 $\\mu$m AIB to local physical properties of the ISM is thus of paramount importance.","By applying a dedicated machine learning algorithm to JWST NIRSpec observations of the Orion Bar photodissociation region obtained as part of the PDRs4All Early Release Science (ERS) program, we extracted two template spectra capturing the evolution of the AIB-related emission in the 3.2-3.6 $\\mu$m range, which includes the AIB at 3.3 $\\mu$m and its main satellite band at 3.4 $\\mu$m.","In the Orion Bar, we analyze the spatial distribution of the templates and their relationship with the fluorescent emission of H$_2$ in the near infrared.","We find that one template (\"AIB$_{\\rm Irrad}$\") traces regions of neutral atomic gas with strong far-UV fields, while the other template (\"AIB$_{\\rm Shielded}$\") corresponds to shielded regions with lower FUV fields and a higher molecular gas fraction.","We then show that these two templates can be used to fit the NIRSpec AIB-related spectra of nearby galaxies.","The relative weight of the two templates (AIB$_{\\rm Irrad/Shielded}$) is a tracer of the radiative feedback from massive stars on the ISM.","We derive an estimate of AIB$_{\\rm Irrad/Shielded}$ in a $z$ = 4.22 lensed galaxy, and find that it has a lower value than for local galaxies.","This pilot study illustrates how a detailed analysis of AIB emission in nearby regions can be used to probe the physical conditions of the extragalactic ISM."],"url":"http://arxiv.org/abs/2402.16535v1","category":"astro-ph.GA"}
{"created":"2024-02-26 11:00:09","title":"A kernel-based analysis of Laplacian Eigenmaps","abstract":"Given i.i.d. observations uniformly distributed on a closed manifold $\\mathcal{M}\\subseteq \\mathbb{R}^p$, we study the spectral properties of the associated empirical graph Laplacian based on a Gaussian kernel. Our main results are non-asymptotic error bounds, showing that the eigenvalues and eigenspaces of the empirical graph Laplacian are close to the eigenvalues and eigenspaces of the Laplace-Beltrami operator of $\\mathcal{M}$. In our analysis, we connect the empirical graph Laplacian to kernel principal component analysis, and consider the heat kernel of $\\mathcal{M}$ as reproducing kernel feature map. This leads to novel points of view and allows to leverage results for empirical covariance operators in infinite dimensions.","sentences":["Given i.i.d. observations uniformly distributed on a closed manifold $\\mathcal{M}\\subseteq \\mathbb{R}^p$, we study the spectral properties of the associated empirical graph Laplacian based on a Gaussian kernel.","Our main results are non-asymptotic error bounds, showing that the eigenvalues and eigenspaces of the empirical graph Laplacian are close to the eigenvalues and eigenspaces of the Laplace-Beltrami operator of $\\mathcal{M}$. In our analysis, we connect the empirical graph Laplacian to kernel principal component analysis, and consider the heat kernel of $\\mathcal{M}$ as reproducing kernel feature map.","This leads to novel points of view and allows to leverage results for empirical covariance operators in infinite dimensions."],"url":"http://arxiv.org/abs/2402.16481v1","category":"math.ST"}
{"created":"2024-02-26 09:04:07","title":"Stable Training of Normalizing Flows for High-dimensional Variational Inference","abstract":"Variational inference with normalizing flows (NFs) is an increasingly popular alternative to MCMC methods. In particular, NFs based on coupling layers (Real NVPs) are frequently used due to their good empirical performance. In theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations. However, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the stochastic gradients. In this work, we show that previous methods for stabilizing the variance of stochastic gradient descent can be insufficient to achieve stable training of Real NVPs. As the source of the problem, we identify that, during training, samples often exhibit unusual high values. As a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log transformation of the samples. We evaluate these and other previously proposed modification on several challenging target distributions, including a high-dimensional horseshoe logistic regression model. Our experiments show that with our modifications, stable training of Real NVPs for posteriors with several thousand dimensions is possible, allowing for more accurate marginal likelihood estimation via importance sampling. Moreover, we evaluate several common training techniques and architecture choices and provide practical advise for training NFs for high-dimensional variational inference.","sentences":["Variational inference with normalizing flows (NFs) is an increasingly popular alternative to MCMC methods.","In particular, NFs based on coupling layers (Real NVPs) are frequently used due to their good empirical performance.","In theory, increasing the depth of normalizing flows should lead to more accurate posterior approximations.","However, in practice, training deep normalizing flows for approximating high-dimensional posterior distributions is often infeasible due to the high variance of the stochastic gradients.","In this work, we show that previous methods for stabilizing the variance of stochastic gradient descent can be insufficient to achieve stable training of Real NVPs.","As the source of the problem, we identify that, during training, samples often exhibit unusual high values.","As a remedy, we propose a combination of two methods: (1) soft-thresholding of the scale in Real NVPs, and (2) a bijective soft log transformation of the samples.","We evaluate these and other previously proposed modification on several challenging target distributions, including a high-dimensional horseshoe logistic regression model.","Our experiments show that with our modifications, stable training of Real NVPs for posteriors with several thousand dimensions is possible, allowing for more accurate marginal likelihood estimation via importance sampling.","Moreover, we evaluate several common training techniques and architecture choices and provide practical advise for training NFs for high-dimensional variational inference."],"url":"http://arxiv.org/abs/2402.16408v1","category":"stat.ML"}
{"created":"2024-02-26 08:36:11","title":"Communication Optimal Unbalanced Private Set Union","abstract":"We consider the private set union (PSU) problem, where two parties each hold a private set of elements, and they want one of the parties (the receiver) to learn the union of the two sets and nothing else. Our protocols are targeted for the unbalanced case where the receiver's set size is larger than the sender's set size, with the goal of minimizing the costs for the sender both in terms of communication volume and local computation time. This setting is motivated by applications where the receiver has significantly more data (input set size) and computational resources than the sender which might be realized on a small, low-power device. Asymptotically, we achieve communication cost linear in the sender's (smaller) set size, and computation costs for sender and receiver which are nearly-linear in their respective set sizes. To our knowledge, ours is the first algorithm to achieve nearly-linear communication and computation for PSU in this unbalanced setting. Our protocols utilize fully homomorphic encryption (FHE) and, optionally, linearly homomorphic encryption (LHE) to perform the necessary computations while preserving privacy. The underlying computations are based on univariate polynomial arithmetic realized within homomorphic encryption, namely fast multiplication, modular reduction, and multi-point evaluation. These asymptotically fast HE polynomial arithmetic algorithms may be of independent interest.","sentences":["We consider the private set union (PSU) problem, where two parties each hold a private set of elements, and they want one of the parties (the receiver) to learn the union of the two sets and nothing else.","Our protocols are targeted for the unbalanced case where the receiver's set size is larger than the sender's set size, with the goal of minimizing the costs for the sender both in terms of communication volume and local computation time.","This setting is motivated by applications where the receiver has significantly more data (input set size) and computational resources than the sender which might be realized on a small, low-power device.","Asymptotically, we achieve communication cost linear in the sender's (smaller) set size, and computation costs for sender and receiver which are nearly-linear in their respective set sizes.","To our knowledge, ours is the first algorithm to achieve nearly-linear communication and computation for PSU in this unbalanced setting.","Our protocols utilize fully homomorphic encryption (FHE) and, optionally, linearly homomorphic encryption (LHE) to perform the necessary computations while preserving privacy.","The underlying computations are based on univariate polynomial arithmetic realized within homomorphic encryption, namely fast multiplication, modular reduction, and multi-point evaluation.","These asymptotically fast HE polynomial arithmetic algorithms may be of independent interest."],"url":"http://arxiv.org/abs/2402.16393v1","category":"cs.CR"}
{"created":"2024-02-26 07:33:28","title":"Where Do We Go from Here? Multi-scale Allocentric Relational Inference from Natural Spatial Descriptions","abstract":"When communicating routes in natural language, the concept of {\\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research. However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions. Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception. These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right and then you will see...'). In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure. These instructions (e.g., `it is south of Central Park and a block north of a police station') are typically non-sequential, contain allocentric relations, with multiple spatial relations and implicit actions, without any explicit verification. This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge. Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks.","sentences":["When communicating routes in natural language, the concept of {\\em acquired spatial knowledge} is crucial for geographic information retrieval (GIR) and in spatial cognitive research.","However, NLP navigation studies often overlook the impact of such acquired knowledge on textual descriptions.","Current navigation studies concentrate on egocentric local descriptions (e.g., `it will be on your right') that require reasoning over the agent's local perception.","These instructions are typically given as a sequence of steps, with each action-step explicitly mentioning and being followed by a landmark that the agent can use to verify they are on the right path (e.g., `turn right","and then you will see...').","In contrast, descriptions based on knowledge acquired through a map provide a complete view of the environment and capture its overall structure.","These instructions (e.g., `it is south of Central Park and a block north of a police station') are typically non-sequential, contain allocentric relations, with multiple spatial relations and implicit actions, without any explicit verification.","This paper introduces the Rendezvous (RVS) task and dataset, which includes 10,404 examples of English geospatial instructions for reaching a target location using map-knowledge.","Our analysis reveals that RVS exhibits a richer use of spatial allocentric relations, and requires resolving more spatial relations simultaneously compared to previous text-based navigation benchmarks."],"url":"http://arxiv.org/abs/2402.16364v1","category":"cs.CL"}
{"created":"2024-02-26 07:22:51","title":"An Integrated Data Processing Framework for Pretraining Foundation Models","abstract":"The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data. In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository. Lacking a unified data processing framework, this process is repetitive and cumbersome. To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data. The proposed framework is easy to use and highly flexible. In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model. The code and demonstration videos are accessible on GitHub.","sentences":["The ability of the foundation models heavily relies on large-scale, diverse, and high-quality pretraining data.","In order to improve data quality, researchers and practitioners often have to manually curate datasets from difference sources and develop dedicated data cleansing pipeline for each data repository.","Lacking a unified data processing framework, this process is repetitive and cumbersome.","To mitigate this issue, we propose a data processing framework that integrates a Processing Module which consists of a series of operators at different granularity levels, and an Analyzing Module which supports probing and evaluation of the refined data.","The proposed framework is easy to use and highly flexible.","In this demo paper, we first introduce how to use this framework with some example use cases and then demonstrate its effectiveness in improving the data quality with an automated evaluation with ChatGPT and an end-to-end evaluation in pretraining the GPT-2 model.","The code and demonstration videos are accessible on GitHub."],"url":"http://arxiv.org/abs/2402.16358v1","category":"cs.LG"}
{"created":"2024-02-26 07:07:18","title":"Impression-CLIP: Contrastive Shape-Impression Embedding for Fonts","abstract":"Fonts convey different impressions to readers. These impressions often come from the font shapes. However, the correlation between fonts and their impression is weak and unstable because impressions are subjective. To capture such weak and unstable cross-modal correlation between font shapes and their impressions, we propose Impression-CLIP, which is a novel machine-learning model based on CLIP (Contrastive Language-Image Pre-training). By using the CLIP-based model, font image features and their impression features are pulled closer, and font image features and unrelated impression features are pushed apart. This procedure realizes co-embedding between font image and their impressions. In our experiment, we perform cross-modal retrieval between fonts and impressions through co-embedding. The results indicate that Impression-CLIP achieves better retrieval accuracy than the state-of-the-art method. Additionally, our model shows the robustness to noise and missing tags.","sentences":["Fonts convey different impressions to readers.","These impressions often come from the font shapes.","However, the correlation between fonts and their impression is weak and unstable because impressions are subjective.","To capture such weak and unstable cross-modal correlation between font shapes and their impressions, we propose Impression-CLIP, which is a novel machine-learning model based on CLIP (Contrastive Language-Image Pre-training).","By using the CLIP-based model, font image features and their impression features are pulled closer, and font image features and unrelated impression features are pushed apart.","This procedure realizes co-embedding between font image and their impressions.","In our experiment, we perform cross-modal retrieval between fonts and impressions through co-embedding.","The results indicate that Impression-CLIP achieves better retrieval accuracy than the state-of-the-art method.","Additionally, our model shows the robustness to noise and missing tags."],"url":"http://arxiv.org/abs/2402.16350v1","category":"cs.CV"}
{"created":"2024-02-26 06:20:28","title":"A Provably Accurate Randomized Sampling Algorithm for Logistic Regression","abstract":"In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks. When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model. Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra. We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations. To further validate our theoretical findings, we conduct comprehensive empirical evaluations. Overall, our work sheds light on the potential of using randomized sampling approaches to efficiently approximate the estimated probabilities in logistic regression, offering a practical and computationally efficient solution for large-scale datasets.","sentences":["In statistics and machine learning, logistic regression is a widely-used supervised learning technique primarily employed for binary classification tasks.","When the number of observations greatly exceeds the number of predictor variables, we present a simple, randomized sampling-based algorithm for logistic regression problem that guarantees high-quality approximations to both the estimated probabilities and the overall discrepancy of the model.","Our analysis builds upon two simple structural conditions that boil down to randomized matrix multiplication, a fundamental and well-understood primitive of randomized numerical linear algebra.","We analyze the properties of estimated probabilities of logistic regression when leverage scores are used to sample observations, and prove that accurate approximations can be achieved with a sample whose size is much smaller than the total number of observations.","To further validate our theoretical findings, we conduct comprehensive empirical evaluations.","Overall, our work sheds light on the potential of using randomized sampling approaches to efficiently approximate the estimated probabilities in logistic regression, offering a practical and computationally efficient solution for large-scale datasets."],"url":"http://arxiv.org/abs/2402.16326v1","category":"stat.ML"}
{"created":"2024-02-26 02:43:30","title":"Reinforcement of superconductivity by quantum critical fluctuations of metamagnetism in UTe$_2$","abstract":"The normal-conducting state of the superconductor UTe$_2$ is studied by entropy analysis for magnetic fields along the $b$-axis, obtained from magnetization using the relation $(\\partial M/\\partial T)_B=(\\partial S/\\partial B)_T$. We observe a strong increase in entropy with magnetic field due to metamagnetic fluctuations (spatially uniform, $Q=0$). The field dependence is well described by the Hertz-Millis-Moriya theory for quantum criticality of itinerant metamagnetism. Notably, the lower bound of the quantum-critical region coincides well with the position of the minimum in the superconducting transition temperature $T_c(B)$. Hence, our results suggest that $Q=0$ fluctuations reinforce the superconductivity.","sentences":["The normal-conducting state of the superconductor UTe$_2$ is studied by entropy analysis for magnetic fields along the $b$-axis, obtained from magnetization using the relation $(\\partial M/\\partial T)_B=(\\partial S/\\partial","B)_T$. We observe a strong increase in entropy with magnetic field due to metamagnetic fluctuations (spatially uniform, $Q=0$).","The field dependence is well described by the Hertz-Millis-Moriya theory for quantum criticality of itinerant metamagnetism.","Notably, the lower bound of the quantum-critical region coincides well with the position of the minimum in the superconducting transition temperature $T_c(B)$. Hence, our results suggest that $Q=0$ fluctuations reinforce the superconductivity."],"url":"http://arxiv.org/abs/2402.16258v1","category":"cond-mat.supr-con"}
{"created":"2024-02-25 22:30:09","title":"Photon-counting CT using a Conditional Diffusion Model for Super-resolution and Texture-preservation","abstract":"Ultra-high resolution images are desirable in photon counting CT (PCCT), but resolution is physically limited by interactions such as charge sharing. Deep learning is a possible method for super-resolution (SR), but sourcing paired training data that adequately models the target task is difficult. Additionally, SR algorithms can distort noise texture, which is an important in many clinical diagnostic scenarios. Here, we train conditional denoising diffusion probabilistic models (DDPMs) for PCCT super-resolution, with the objective to retain textural characteristics of local noise. PCCT simulation methods are used to synthesize realistic resolution degradation. To preserve noise texture, we explore decoupling the noise and signal image inputs and outputs via deep denoisers, explicitly mapping to each during the SR process. Our experimental results indicate that our DDPM trained on simulated data can improve sharpness in real PCCT images. Additionally, the disentanglement of noise from the original image allows our model more faithfully preserve noise texture.","sentences":["Ultra-high resolution images are desirable in photon counting CT (PCCT), but resolution is physically limited by interactions such as charge sharing.","Deep learning is a possible method for super-resolution (SR), but sourcing paired training data that adequately models the target task is difficult.","Additionally, SR algorithms can distort noise texture, which is an important in many clinical diagnostic scenarios.","Here, we train conditional denoising diffusion probabilistic models (DDPMs) for PCCT super-resolution, with the objective to retain textural characteristics of local noise.","PCCT simulation methods are used to synthesize realistic resolution degradation.","To preserve noise texture, we explore decoupling the noise and signal image inputs and outputs via deep denoisers, explicitly mapping to each during the SR process.","Our experimental results indicate that our DDPM trained on simulated data can improve sharpness in real PCCT images.","Additionally, the disentanglement of noise from the original image allows our model more faithfully preserve noise texture."],"url":"http://arxiv.org/abs/2402.16212v1","category":"eess.IV"}
{"created":"2024-02-25 20:30:18","title":"Accurate predictions of keyhole depths using machine learning-aided simulations","abstract":"The keyhole phenomenon is widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing. Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies. The formation of these pores is typically associated with the dynamic behavior of the keyhole. So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time has been a challenging task. In situ characterization of keyhole dynamic behavior using a synchrotron X-ray is complicated and expensive. Current simulations are hindered by their poor accuracies in predicting keyhole depths due to the lack of real-time laser absorptance data. Here, we develop a machine learning-aided simulation method that allows us to accurately predict keyhole depth over a wide range of processing parameters. Based on titanium and aluminum alloys, two commonly used engineering materials as examples, we achieve an accuracy with an error margin of 10 %, surpassing those simulated using other existing models (with an error margin in a range of 50-200 %). Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques.","sentences":["The keyhole phenomenon is widely observed in laser materials processing, including laser welding, remelting, cladding, drilling, and additive manufacturing.","Keyhole-induced defects, primarily pores, dramatically affect the performance of final products, impeding the broad use of these laser-based technologies.","The formation of these pores is typically associated with the dynamic behavior of the keyhole.","So far, the accurate characterization and prediction of keyhole features, particularly keyhole depth, as a function of time has been a challenging task.","In situ characterization of keyhole dynamic behavior using a synchrotron X-ray is complicated and expensive.","Current simulations are hindered by their poor accuracies in predicting keyhole depths due to the lack of real-time laser absorptance data.","Here, we develop a machine learning-aided simulation method that allows us to accurately predict keyhole depth over a wide range of processing parameters.","Based on titanium and aluminum alloys, two commonly used engineering materials as examples, we achieve an accuracy with an error margin of 10 %, surpassing those simulated using other existing models (with an error margin in a range of 50-200 %).","Our machine learning-aided simulation method is affordable and readily deployable for a large variety of materials, opening new doors to eliminate or reduce defects for a wide range of laser materials processing techniques."],"url":"http://arxiv.org/abs/2402.16190v1","category":"cs.CE"}
{"created":"2024-02-25 15:55:24","title":"A statistical method for crack detection in 3D concrete images","abstract":"In practical applications, effectively segmenting cracks in large-scale computed tomography (CT) images holds significant importance for understanding the structural integrity of materials. However, classical methods and Machine Learning algorithms often incur high computational costs when dealing with the substantial size of input images. Hence, a robust algorithm is needed to pre-detect crack regions, enabling focused analysis and reducing computational overhead. The proposed approach addresses this challenge by offering a streamlined method for identifying crack regions in CT images with high probability. By efficiently identifying areas of interest, our algorithm allows for a more focused examination of potential anomalies within the material structure. Through comprehensive testing on both semi-synthetic and real 3D CT images, we validate the efficiency of our approach in enhancing crack segmentation while reducing computational resource requirements.","sentences":["In practical applications, effectively segmenting cracks in large-scale computed tomography (CT) images holds significant importance for understanding the structural integrity of materials.","However, classical methods and Machine Learning algorithms often incur high computational costs when dealing with the substantial size of input images.","Hence, a robust algorithm is needed to pre-detect crack regions, enabling focused analysis and reducing computational overhead.","The proposed approach addresses this challenge by offering a streamlined method for identifying crack regions in CT images with high probability.","By efficiently identifying areas of interest, our algorithm allows for a more focused examination of potential anomalies within the material structure.","Through comprehensive testing on both semi-synthetic and real 3D CT images, we validate the efficiency of our approach in enhancing crack segmentation while reducing computational resource requirements."],"url":"http://arxiv.org/abs/2402.16126v1","category":"cs.CV"}
{"created":"2024-02-25 15:00:06","title":"Optimizing Base Placement of Surgical Robot: Kinematics Data-Driven Approach by Analyzing Working Pattern","abstract":"In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot's base is crucial for successful surgery. Improper placement can hinder performance due to manipulator limitations and inaccessible workspaces. Traditionally, trained medical staff rely on experience for base placement, but this approach lacks objectivity. This paper proposes a novel method to determine the optimal base pose based on the individual surgeon's working pattern. The proposed method analyzes recorded end-effector poses using machine-learning based clustering technique to identify key positions and orientations preferred by the surgeon. To address joint limits and singularities problems, we introduce two scoring metrics: joint margin score and manipulability score. We then train a multi-layer perceptron (MLP) regressor to predict the optimal base pose based on these scores. Evaluation in a simulated environment using the da Vinci Research Kit (dVRK) showed unique base pose-score maps for four volunteers, highlighting the individuality of working patterns. After conducting tests on the base poses identified using the proposed method, we confirmed that they have a score approximately 28.2\\% higher than when the robots were placed randomly, with respect to the score we defined. This emphasizes the need for operator-specific optimization in RAMIS base placement.","sentences":["In robot-assisted minimally invasive surgery (RAMIS), optimal placement of the surgical robot's base is crucial for successful surgery.","Improper placement can hinder performance due to manipulator limitations and inaccessible workspaces.","Traditionally, trained medical staff rely on experience for base placement, but this approach lacks objectivity.","This paper proposes a novel method to determine the optimal base pose based on the individual surgeon's working pattern.","The proposed method analyzes recorded end-effector poses using machine-learning based clustering technique to identify key positions and orientations preferred by the surgeon.","To address joint limits and singularities problems, we introduce two scoring metrics: joint margin score and manipulability score.","We then train a multi-layer perceptron (MLP) regressor to predict the optimal base pose based on these scores.","Evaluation in a simulated environment using the da Vinci Research Kit (dVRK) showed unique base pose-score maps for four volunteers, highlighting the individuality of working patterns.","After conducting tests on the base poses identified using the proposed method, we confirmed that they have a score approximately 28.2\\% higher than when the robots were placed randomly, with respect to the score we defined.","This emphasizes the need for operator-specific optimization in RAMIS base placement."],"url":"http://arxiv.org/abs/2402.16101v1","category":"cs.RO"}
{"created":"2024-02-25 13:25:51","title":"How to Privately Tune Hyperparameters in Federated Learning? Insights from a Benchmark Study","abstract":"In this paper, we address the problem of privacy-preserving hyperparameter (HP) tuning for cross-silo federated learning (FL). We first perform a comprehensive measurement study that benchmarks various HP strategies suitable for FL. Our benchmarks show that the optimal parameters of the FL server, e.g., the learning rate, can be accurately and efficiently tuned based on the HPs found by each client on its local data. We demonstrate that HP averaging is suitable for iid settings, while density-based clustering can uncover the optimal set of parameters in non-iid ones. Then, to prevent information leakage from the exchange of the clients' local HPs, we design and implement PrivTuna, a novel framework for privacy-preserving HP tuning using multiparty homomorphic encryption. We use PrivTuna to implement privacy-preserving federated averaging and density-based clustering, and we experimentally evaluate its performance demonstrating its computation/communication efficiency and its precision in tuning hyperparameters.","sentences":["In this paper, we address the problem of privacy-preserving hyperparameter (HP) tuning for cross-silo federated learning (FL).","We first perform a comprehensive measurement study that benchmarks various HP strategies suitable for FL.","Our benchmarks show that the optimal parameters of the FL server, e.g., the learning rate, can be accurately and efficiently tuned based on the HPs found by each client on its local data.","We demonstrate that HP averaging is suitable for iid settings, while density-based clustering can uncover the optimal set of parameters in non-iid ones.","Then, to prevent information leakage from the exchange of the clients' local HPs, we design and implement PrivTuna, a novel framework for privacy-preserving HP tuning using multiparty homomorphic encryption.","We use PrivTuna to implement privacy-preserving federated averaging and density-based clustering, and we experimentally evaluate its performance demonstrating its computation/communication efficiency and its precision in tuning hyperparameters."],"url":"http://arxiv.org/abs/2402.16087v1","category":"cs.CR"}
{"created":"2024-02-25 11:26:39","title":"Training a Bilingual Language Model by Mapping Tokens onto a Shared Character Space","abstract":"We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script. Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on machine translation which requires cross-lingual knowledge. The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step. Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in machine translation across both translation directions.","sentences":["We train a bilingual Arabic-Hebrew language model using a transliterated version of Arabic texts in Hebrew, to ensure both languages are represented in the same script.","Given the morphological, structural similarities, and the extensive number of cognates shared among Arabic and Hebrew, we assess the performance of a language model that employs a unified script for both languages, on machine translation which requires cross-lingual knowledge.","The results are promising: our model outperforms a contrasting model which keeps the Arabic texts in the Arabic script, demonstrating the efficacy of the transliteration step.","Despite being trained on a dataset approximately 60% smaller than that of other existing language models, our model appears to deliver comparable performance in machine translation across both translation directions."],"url":"http://arxiv.org/abs/2402.16065v1","category":"cs.CL"}
{"created":"2024-02-25 11:07:08","title":"Say More with Less: Understanding Prompt Learning Behaviors through Gist Compression","abstract":"Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference. In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering. Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens. It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model. By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates. Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both passage and instruction compression tasks. Further analysis on gist verbalization results suggests that our gist prompts serve different functions in aiding language models. They may directly provide potential answers, generate the chain-of-thought, or simply repeat the inputs. All data and codes are available at https://github.com/OpenMatch/Gist-COCO .","sentences":["Large language models (LLMs) require lengthy prompts as the input context to produce output aligned with user intentions, a process that incurs extra costs during inference.","In this paper, we propose the Gist COnditioned deCOding (Gist-COCO) model, introducing a novel method for compressing prompts which also can assist the prompt interpretation and engineering.","Gist-COCO employs an encoder-decoder based language model and then incorporates an additional encoder as a plugin module to compress prompts with inputs using gist tokens.","It finetunes the compression plugin module and uses the representations of gist tokens to emulate the raw prompts in the vanilla language model.","By verbalizing the representations of gist tokens into gist prompts, the compression ability of Gist-COCO can be generalized to different LLMs with high compression rates.","Our experiments demonstrate that Gist-COCO outperforms previous prompt compression models in both passage and instruction compression tasks.","Further analysis on gist verbalization results suggests that our gist prompts serve different functions in aiding language models.","They may directly provide potential answers, generate the chain-of-thought, or simply repeat the inputs.","All data and codes are available at https://github.com/OpenMatch/Gist-COCO ."],"url":"http://arxiv.org/abs/2402.16058v1","category":"cs.CL"}
{"created":"2024-02-25 09:57:51","title":"Harnessing the Synergy between Pushing, Grasping, and Throwing to Enhance Object Manipulation in Cluttered Scenarios","abstract":"In this work, we delve into the intricate synergy among non-prehensile actions like pushing, and prehensile actions such as grasping and throwing, within the domain of robotic manipulation. We introduce an innovative approach to learning these synergies by leveraging model-free deep reinforcement learning. The robot's workflow involves detecting the pose of the target object and the basket at each time step, predicting the optimal push configuration to isolate the target object, determining the appropriate grasp configuration, and inferring the necessary parameters for an accurate throw into the basket. This empowers robots to skillfully reconfigure cluttered scenarios through pushing, creating space for collision-free grasping actions. Simultaneously, we integrate throwing behavior, showcasing how this action significantly extends the robot's operational reach. Ensuring safety, we developed a simulation environment in Gazebo for robot training, applying the learned policy directly to our real robot. Notably, this work represents a pioneering effort to learn the synergy between pushing, grasping, and throwing actions. Extensive experimentation in both simulated and real-robot scenarios substantiates the effectiveness of our approach across diverse settings. Our approach achieves a success rate exceeding 80\\% in both simulated and real-world scenarios. A video showcasing our experiments is available online at: https://youtu.be/q1l4BJVDbRw","sentences":["In this work, we delve into the intricate synergy among non-prehensile actions like pushing, and prehensile actions such as grasping and throwing, within the domain of robotic manipulation.","We introduce an innovative approach to learning these synergies by leveraging model-free deep reinforcement learning.","The robot's workflow involves detecting the pose of the target object and the basket at each time step, predicting the optimal push configuration to isolate the target object, determining the appropriate grasp configuration, and inferring the necessary parameters for an accurate throw into the basket.","This empowers robots to skillfully reconfigure cluttered scenarios through pushing, creating space for collision-free grasping actions.","Simultaneously, we integrate throwing behavior, showcasing how this action significantly extends the robot's operational reach.","Ensuring safety, we developed a simulation environment in Gazebo for robot training, applying the learned policy directly to our real robot.","Notably, this work represents a pioneering effort to learn the synergy between pushing, grasping, and throwing actions.","Extensive experimentation in both simulated and real-robot scenarios substantiates the effectiveness of our approach across diverse settings.","Our approach achieves a success rate exceeding 80\\% in both simulated and real-world scenarios.","A video showcasing our experiments is available online at: https://youtu.be/q1l4BJVDbRw"],"url":"http://arxiv.org/abs/2402.16045v1","category":"cs.RO"}
{"created":"2024-02-25 09:44:56","title":"Detecting Machine-Generated Texts by Multi-Population Aware Optimization for Maximum Mean Discrepancy","abstract":"Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts. However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues. Therefore, it is very urgent and important to detect MGTs in many situations. Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs. In this paper, we seek to exploit \\textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies. However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \\textit{multiple text populations} due to various LLMs. This will severely impair MMD's ability to measure the difference between two samples. To tackle this, we propose a novel \\textit{multi-population} aware optimization method for MMD called MMD-MP, which can \\textit{avoid variance increases} and thus improve the stability to measure the distributional discrepancy. Relying on MMD-MP, we develop two methods for paragraph-based and sentence-based detection, respectively. Extensive experiments on various LLMs, \\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP. The source code is available at \\url{https://github.com/ZSHsh98/MMD-MP}.","sentences":["Large language models (LLMs) such as ChatGPT have exhibited remarkable performance in generating human-like texts.","However, machine-generated texts (MGTs) may carry critical risks, such as plagiarism issues, misleading information, or hallucination issues.","Therefore, it is very urgent and important to detect MGTs in many situations.","Unfortunately, it is challenging to distinguish MGTs and human-written texts because the distributional discrepancy between them is often very subtle due to the remarkable performance of LLMs.","In this paper, we seek to exploit \\textit{maximum mean discrepancy} (MMD) to address this issue in the sense that MMD can well identify distributional discrepancies.","However, directly training a detector with MMD using diverse MGTs will incur a significantly increased variance of MMD since MGTs may contain \\textit{multiple text populations} due to various LLMs.","This will severely impair MMD's ability to measure the difference between two samples.","To tackle this, we propose a novel \\textit{multi-population} aware optimization method for MMD called MMD-MP, which can \\textit{avoid variance increases} and thus improve the stability to measure the distributional discrepancy.","Relying on MMD-MP, we develop two methods for paragraph-based and sentence-based detection, respectively.","Extensive experiments on various LLMs, \\eg, GPT2 and ChatGPT, show superior detection performance of our MMD-MP.","The source code is available at \\url{https://github.com/ZSHsh98/MMD-MP}."],"url":"http://arxiv.org/abs/2402.16041v1","category":"cs.CL"}
{"created":"2024-02-25 09:28:20","title":"Machine Learning-Based Vehicle Intention Trajectory Recognition and Prediction for Autonomous Driving","abstract":"In recent years, the expansion of internet technology and advancements in automation have brought significant attention to autonomous driving technology. Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have progressively introduced products ranging from assisted-driving vehicles to semi-autonomous vehicles. However, this period has also witnessed several traffic safety incidents involving self-driving vehicles. For instance, in March 2016, a Google self-driving car was involved in a minor collision with a bus. At the time of the accident, the autonomous vehicle was attempting to merge into the right lane but failed to dynamically respond to the real-time environmental information during the lane change. It incorrectly assumed that the approaching bus would slow down to avoid it, leading to a low-speed collision with the bus. This incident highlights the current technological shortcomings and safety concerns associated with autonomous lane-changing behavior, despite the rapid advancements in autonomous driving technology. Lane-changing is among the most common and hazardous behaviors in highway driving, significantly impacting traffic safety and flow. Therefore, lane-changing is crucial for traffic safety, and accurately predicting drivers' lane change intentions can markedly enhance driving safety. This paper introduces a deep learning-based prediction method for autonomous driving lane change behavior, aiming to facilitate safe lane changes and thereby improve road safety.","sentences":["In recent years, the expansion of internet technology and advancements in automation have brought significant attention to autonomous driving technology.","Major automobile manufacturers, including Volvo, Mercedes-Benz, and Tesla, have progressively introduced products ranging from assisted-driving vehicles to semi-autonomous vehicles.","However, this period has also witnessed several traffic safety incidents involving self-driving vehicles.","For instance, in March 2016, a Google self-driving car was involved in a minor collision with a bus.","At the time of the accident, the autonomous vehicle was attempting to merge into the right lane but failed to dynamically respond to the real-time environmental information during the lane change.","It incorrectly assumed that the approaching bus would slow down to avoid it, leading to a low-speed collision with the bus.","This incident highlights the current technological shortcomings and safety concerns associated with autonomous lane-changing behavior, despite the rapid advancements in autonomous driving technology.","Lane-changing is among the most common and hazardous behaviors in highway driving, significantly impacting traffic safety and flow.","Therefore, lane-changing is crucial for traffic safety, and accurately predicting drivers' lane change intentions can markedly enhance driving safety.","This paper introduces a deep learning-based prediction method for autonomous driving lane change behavior, aiming to facilitate safe lane changes and thereby improve road safety."],"url":"http://arxiv.org/abs/2402.16036v1","category":"cs.RO"}
{"created":"2024-02-25 08:20:05","title":"Feature Selection Based on Orthogonal Constraints and Polygon Area","abstract":"The goal of feature selection is to choose the optimal subset of features for a recognition task by evaluating the importance of each feature, thereby achieving effective dimensionality reduction. Currently, proposed feature selection methods often overlook the discriminative dependencies between features and labels. To address this problem, this paper introduces a novel orthogonal regression model incorporating the area of a polygon. The model can intuitively capture the discriminative dependencies between features and labels. Additionally, this paper employs a hybrid non-monotone linear search method to efficiently tackle the non-convex optimization challenge posed by orthogonal constraints. Experimental results demonstrate that our approach not only effectively captures discriminative dependency information but also surpasses traditional methods in reducing feature dimensions and enhancing classification performance.","sentences":["The goal of feature selection is to choose the optimal subset of features for a recognition task by evaluating the importance of each feature, thereby achieving effective dimensionality reduction.","Currently, proposed feature selection methods often overlook the discriminative dependencies between features and labels.","To address this problem, this paper introduces a novel orthogonal regression model incorporating the area of a polygon.","The model can intuitively capture the discriminative dependencies between features and labels.","Additionally, this paper employs a hybrid non-monotone linear search method to efficiently tackle the non-convex optimization challenge posed by orthogonal constraints.","Experimental results demonstrate that our approach not only effectively captures discriminative dependency information but also surpasses traditional methods in reducing feature dimensions and enhancing classification performance."],"url":"http://arxiv.org/abs/2402.16026v1","category":"cs.LG"}
{"created":"2024-02-25 07:35:02","title":"Flares hunting in hot subdwarf and white dwarf stars from Cycles 1-5 of TESS photometry","abstract":"Stellar flares are critical phenomena on stellar surfaces, which are closely tied to stellar magnetism. While extensively studied in main-sequence (MS) stars, their occurrence in evolved compact stars, specifically hot subdwarfs and white dwarfs (WDs), remains scarcely explored. Based on Cycles 1-5 of TESS photometry, we conducted a pioneering survey of flare events in $\\sim12,000$ compact stars, corresponding to $\\sim38,000$ light curves with 2-minute cadence. Through dedicated techniques for detrending light curves, identifying preliminary flare candidates, and validating them via machine learning, we established a catalog of 1016 flares from 193 compact stars, including 182 from 58 sdB/sdO stars and 834 from 135 WDs, respectively. However, all flaring compact stars showed signs of contamination from nearby objects or companion stars, preventing sole attribution of the detected flares. For WDs, it is highly probable that the flares originated from their cool MS companions. In contrast, the higher luminosities of sdB/sdO stars diminish companion contributions, suggesting that detected flares originated from sdB/sdO stars themselves or through close magnetic interactions with companions. Focusing on a refined sample of 23 flares from 13 sdB/sdO stars, we found their flare frequency distributions were slightly divergent from those of cool MS stars; instead, they resemble those of hot B/A-type MS stars having radiative envelopes. This similarity implies the flares on sdB/sdO stars, if these flares did originate from them, may share underlying mechanisms with hot MS stars, which warrants further investigation.","sentences":["Stellar flares are critical phenomena on stellar surfaces, which are closely tied to stellar magnetism.","While extensively studied in main-sequence (MS) stars, their occurrence in evolved compact stars, specifically hot subdwarfs and white dwarfs (WDs), remains scarcely explored.","Based on Cycles 1-5 of TESS photometry, we conducted a pioneering survey of flare events in $\\sim12,000$ compact stars, corresponding to $\\sim38,000$ light curves with 2-minute cadence.","Through dedicated techniques for detrending light curves, identifying preliminary flare candidates, and validating them via machine learning, we established a catalog of 1016 flares from 193 compact stars, including 182 from 58 sdB/sdO stars and 834 from 135 WDs, respectively.","However, all flaring compact stars showed signs of contamination from nearby objects or companion stars, preventing sole attribution of the detected flares.","For WDs, it is highly probable that the flares originated from their cool MS companions.","In contrast, the higher luminosities of sdB/sdO stars diminish companion contributions, suggesting that detected flares originated from sdB/sdO stars themselves or through close magnetic interactions with companions.","Focusing on a refined sample of 23 flares from 13 sdB/sdO stars, we found their flare frequency distributions were slightly divergent from those of cool MS stars; instead, they resemble those of hot B/A-type MS stars having radiative envelopes.","This similarity implies the flares on sdB/sdO stars, if these flares did originate from them, may share underlying mechanisms with hot MS stars, which warrants further investigation."],"url":"http://arxiv.org/abs/2402.16018v1","category":"astro-ph.SR"}
{"created":"2024-02-25 05:16:43","title":"A Machine Learning Approach to Detect Customer Satisfaction From Multiple Tweet Parameters","abstract":"Since internet technologies have advanced, one of the primary factors in company development is customer happiness. Online platforms have become prominent places for sharing reviews. Twitter is one of these platforms where customers frequently post their thoughts. Reviews of flights on these platforms have become a concern for the airline business. A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation. So it's vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive. But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task. This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels. Some work has already been done on this strategy to automate the procedure using machine learning and deep learning techniques. However, they are all purely concerned with assessing the text's sentiment. In addition to the text, the tweet also includes the time, location, username, airline name, and so on. This additional information can be crucial for improving the model's outcome. To provide a machine learning based solution, this work has broadened its perspective to include these qualities. And it has come as no surprise that the additional features beyond text sentiment analysis produce better outcomes in machine learning based models.","sentences":["Since internet technologies have advanced, one of the primary factors in company development is customer happiness.","Online platforms have become prominent places for sharing reviews.","Twitter is one of these platforms where customers frequently post their thoughts.","Reviews of flights on these platforms have become a concern for the airline business.","A positive review can help the company grow, while a negative one can quickly ruin its revenue and reputation.","So it's vital for airline businesses to examine the feedback and experiences of their customers and enhance their services to remain competitive.","But studying thousands of tweets and analyzing them to find the satisfaction of the customer is quite a difficult task.","This tedious process can be made easier by using a machine learning approach to analyze tweets to determine client satisfaction levels.","Some work has already been done on this strategy to automate the procedure using machine learning and deep learning techniques.","However, they are all purely concerned with assessing the text's sentiment.","In addition to the text, the tweet also includes the time, location, username, airline name, and so on.","This additional information can be crucial for improving the model's outcome.","To provide a machine learning based solution, this work has broadened its perspective to include these qualities.","And it has come as no surprise that the additional features beyond text sentiment analysis produce better outcomes in machine learning based models."],"url":"http://arxiv.org/abs/2402.15992v1","category":"cs.LG"}
{"created":"2024-02-25 02:51:44","title":"VOLoc: Visual Place Recognition by Querying Compressed Lidar Map","abstract":"The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras. However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps. This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence. In the offline phase, VOLoc compresses the Lidar maps using a \\emph{Geometry-Preserving Compressor} (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation. In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the \\emph{Querying Point Cloud} (QPC). Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attention-based aggregation module, to query the compressed Lidar map in the vector space. A transfer learning mechanism is also proposed to improve the accuracy and the generality of the aggregation network. Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-to-Lidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras. The code are publicly available at https://github.com/Master-cai/VOLoc.","sentences":["The availability of city-scale Lidar maps enables the potential of city-scale place recognition using mobile cameras.","However, the city-scale Lidar maps generally need to be compressed for storage efficiency, which increases the difficulty of direct visual place recognition in compressed Lidar maps.","This paper proposes VOLoc, an accurate and efficient visual place recognition method that exploits geometric similarity to directly query the compressed Lidar map via the real-time captured image sequence.","In the offline phase, VOLoc compresses the Lidar maps using a \\emph{Geometry-Preserving Compressor} (GPC), in which the compression is reversible, a crucial requirement for the downstream 6DoF pose estimation.","In the online phase, VOLoc proposes an online Geometric Recovery Module (GRM), which is composed of online Visual Odometry (VO) and a point cloud optimization module, such that the local scene structure around the camera is online recovered to build the \\emph{Querying Point Cloud} (QPC).","Then the QPC is compressed by the same GPC, and is aggregated into a global descriptor by an attention-based aggregation module, to query the compressed Lidar map in the vector space.","A transfer learning mechanism is also proposed to improve the accuracy and the generality of the aggregation network.","Extensive evaluations show that VOLoc provides localization accuracy even better than the Lidar-to-Lidar place recognition, setting up a new record for utilizing the compressed Lidar map by low-end mobile cameras.","The code are publicly available at https://github.com/Master-cai/VOLoc."],"url":"http://arxiv.org/abs/2402.15961v1","category":"cs.CV"}
{"created":"2024-02-25 02:36:03","title":"DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning","abstract":"We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns.","sentences":["We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates.","We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning.","We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns."],"url":"http://arxiv.org/abs/2402.15957v1","category":"cs.LG"}
{"created":"2024-02-25 01:56:47","title":"GreenLLaMA: A Framework for Detoxification with Explanations","abstract":"Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario. Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored. Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning. We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations. We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT. We then train a suite of detoxification models with our cross-platform corpus. We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus. We further introduce explanation to promote transparency and trustworthiness. GreenLLaMA additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases. Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of GreenLLaMA against adversarial toxicity.","sentences":["Prior works on detoxification are scattered in the sense that they do not cover all aspects of detoxification needed in a real-world scenario.","Notably, prior works restrict the task of developing detoxification models to only a seen subset of platforms, leaving the question of how the models would perform on unseen platforms unexplored.","Additionally, these works do not address non-detoxifiability, a phenomenon whereby the toxic text cannot be detoxified without altering the meaning.","We propose GreenLLaMA, the first comprehensive end-to-end detoxification framework, which attempts to alleviate the aforementioned limitations.","We first introduce a cross-platform pseudo-parallel corpus applying multi-step data processing and generation strategies leveraging ChatGPT.","We then train a suite of detoxification models with our cross-platform corpus.","We show that our detoxification models outperform the SoTA model trained with human-annotated parallel corpus.","We further introduce explanation to promote transparency and trustworthiness.","GreenLLaMA additionally offers a unique paraphrase detector especially dedicated for the detoxification task to tackle the non-detoxifiable cases.","Through experimental analysis, we demonstrate the effectiveness of our cross-platform corpus and the robustness of GreenLLaMA against adversarial toxicity."],"url":"http://arxiv.org/abs/2402.15951v1","category":"cs.LG"}
{"created":"2024-02-25 00:15:30","title":"Implementing Recycling Methods for Linear Systems in Python with an Application to Multiple Objective Optimization","abstract":"Sequences of linear systems arise in the predictor-corrector method when computing the Pareto front for multi-objective optimization. Rather than discarding information generated when solving one system, it may be advantageous to recycle information for subsequent systems. To accomplish this, we seek to reduce the overall cost of computation when solving linear systems using common recycling methods. In this work, we assessed the performance of recycling minimum residual (RMINRES) method along with a map between coefficient matrices. For these methods to be fully integrated into the software used in Enouen et al. (2022), there must be working version of each in both Python and PyTorch. Herein, we discuss the challenges we encountered and solutions undertaken (and some ongoing) when computing efficient Python implementations of these recycling strategies. The goal of this project was to implement RMINRES in Python and PyTorch and add it to the established Pareto front code to reduce computational cost. Additionally, we wanted to implement the sparse approximate maps code in Python and PyTorch, so that it can be parallelized in future work.","sentences":["Sequences of linear systems arise in the predictor-corrector method when computing the Pareto front for multi-objective optimization.","Rather than discarding information generated when solving one system, it may be advantageous to recycle information for subsequent systems.","To accomplish this, we seek to reduce the overall cost of computation when solving linear systems using common recycling methods.","In this work, we assessed the performance of recycling minimum residual (RMINRES) method along with a map between coefficient matrices.","For these methods to be fully integrated into the software used in Enouen et al.","(2022), there must be working version of each in both Python and PyTorch.","Herein, we discuss the challenges we encountered and solutions undertaken (and some ongoing) when computing efficient Python implementations of these recycling strategies.","The goal of this project was to implement RMINRES in Python and PyTorch and add it to the established Pareto front code to reduce computational cost.","Additionally, we wanted to implement the sparse approximate maps code in Python and PyTorch, so that it can be parallelized in future work."],"url":"http://arxiv.org/abs/2402.15941v1","category":"math.NA"}
{"created":"2024-02-24 20:02:00","title":"Multi-graph Graph Matching for Coronary Artery Semantic Labeling","abstract":"Coronary artery disease (CAD) stands as the leading cause of death worldwide, and invasive coronary angiography (ICA) remains the gold standard for assessing vascular anatomical information. However, deep learning-based methods encounter challenges in generating semantic labels for arterial segments, primarily due to the morphological similarity between arterial branches. To address this challenge, we model the vascular tree as a graph and propose a multi-graph graph matching (MGM) algorithm for coronary artery semantic labeling. The MGM algorithm assesses the similarity between arterials in multiple vascular tree graphs, taking into account the cycle consistency between each pair of graphs. This ensures that unannotated arterial segments are appropriately labeled by matching them with annotated segments. Through the incorporation of anatomical graph structure, radiomics features, and semantic mapping, the proposed MGM model achieves an impressive accuracy of 0.9471 for coronary artery semantic labeling. This approach presents a novel tool for coronary artery analysis using ICA videos, offering valuable insights into vascular health and pathology.","sentences":["Coronary artery disease (CAD) stands as the leading cause of death worldwide, and invasive coronary angiography (ICA) remains the gold standard for assessing vascular anatomical information.","However, deep learning-based methods encounter challenges in generating semantic labels for arterial segments, primarily due to the morphological similarity between arterial branches.","To address this challenge, we model the vascular tree as a graph and propose a multi-graph graph matching (MGM) algorithm for coronary artery semantic labeling.","The MGM algorithm assesses the similarity between arterials in multiple vascular tree graphs, taking into account the cycle consistency between each pair of graphs.","This ensures that unannotated arterial segments are appropriately labeled by matching them with annotated segments.","Through the incorporation of anatomical graph structure, radiomics features, and semantic mapping, the proposed MGM model achieves an impressive accuracy of 0.9471 for coronary artery semantic labeling.","This approach presents a novel tool for coronary artery analysis using ICA videos, offering valuable insights into vascular health and pathology."],"url":"http://arxiv.org/abs/2402.15894v1","category":"cs.CV"}
{"created":"2024-02-24 17:22:15","title":"Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splatting","abstract":"The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces.","sentences":["The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality.","Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components.","This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information.","To overcome this challenge, we introduce Spec-Gaussian, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian.","Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes.","Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality.","Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians.","This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces."],"url":"http://arxiv.org/abs/2402.15870v1","category":"cs.CV"}
{"created":"2024-02-24 17:15:05","title":"HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models","abstract":"Hyperspectral image (HSI) restoration aims at recovering clean images from degraded observations and plays a vital role in downstream tasks. Existing model-based methods have limitations in accurately modeling the complex image characteristics with handcraft priors, and deep learning-based methods suffer from poor generalization ability. To alleviate these issues, this paper proposes an unsupervised HSI restoration framework with pre-trained diffusion model (HIR-Diff), which restores the clean HSIs from the product of two low-rank components, i.e., the reduced image and the coefficient matrix. Specifically, the reduced image, which has a low spectral dimension, lies in the image field and can be inferred from our improved diffusion model where a new guidance function with total variation (TV) prior is designed to ensure that the reduced image can be well sampled. The coefficient matrix can be effectively pre-estimated based on singular value decomposition (SVD) and rank-revealing QR (RRQR) factorization. Furthermore, a novel exponential noise schedule is proposed to accelerate the restoration process (about 5$\\times$ acceleration for denoising) with little performance decrease. Extensive experimental results validate the superiority of our method in both performance and speed on a variety of HSI restoration tasks, including HSI denoising, noisy HSI super-resolution, and noisy HSI inpainting. The code is available at https://github.com/LiPang/HIRDiff.","sentences":["Hyperspectral image (HSI) restoration aims at recovering clean images from degraded observations and plays a vital role in downstream tasks.","Existing model-based methods have limitations in accurately modeling the complex image characteristics with handcraft priors, and deep learning-based methods suffer from poor generalization ability.","To alleviate these issues, this paper proposes an unsupervised HSI restoration framework with pre-trained diffusion model (HIR-Diff), which restores the clean HSIs from the product of two low-rank components, i.e., the reduced image and the coefficient matrix.","Specifically, the reduced image, which has a low spectral dimension, lies in the image field and can be inferred from our improved diffusion model where a new guidance function with total variation (TV) prior is designed to ensure that the reduced image can be well sampled.","The coefficient matrix can be effectively pre-estimated based on singular value decomposition (SVD) and rank-revealing QR (RRQR) factorization.","Furthermore, a novel exponential noise schedule is proposed to accelerate the restoration process (about 5$\\times$ acceleration for denoising) with little performance decrease.","Extensive experimental results validate the superiority of our method in both performance and speed on a variety of HSI restoration tasks, including HSI denoising, noisy HSI super-resolution, and noisy HSI inpainting.","The code is available at https://github.com/LiPang/HIRDiff."],"url":"http://arxiv.org/abs/2402.15865v1","category":"cs.CV"}
{"created":"2024-02-24 17:13:58","title":"Field-based Molecule Generation","abstract":"This work introduces FMG, a field-based model for drug-like molecule generation. We show how the flexibility of this method provides crucial advantages over the prevalent, point-cloud based methods, and achieves competitive molecular stability generation. We tackle optical isomerism (enantiomers), a previously omitted molecular property that is crucial for drug safety and effectiveness, and thus account for all molecular geometry aspects. We demonstrate how previous methods are invariant to a group of transformations that includes enantiomer pairs, leading them invariant to the molecular R and S configurations, while our field-based generative model captures this property.","sentences":["This work introduces FMG, a field-based model for drug-like molecule generation.","We show how the flexibility of this method provides crucial advantages over the prevalent, point-cloud based methods, and achieves competitive molecular stability generation.","We tackle optical isomerism (enantiomers), a previously omitted molecular property that is crucial for drug safety and effectiveness, and thus account for all molecular geometry aspects.","We demonstrate how previous methods are invariant to a group of transformations that includes enantiomer pairs, leading them invariant to the molecular R and S configurations, while our field-based generative model captures this property."],"url":"http://arxiv.org/abs/2402.15864v1","category":"cs.LG"}
{"created":"2024-02-24 17:12:10","title":"SportQA: A Benchmark for Sports Understanding in Large Language Models","abstract":"A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP). This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks. To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding. SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks. We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting. Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise. The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs.","sentences":["A deep understanding of sports, a field rich in strategic and dynamic content, is crucial for advancing Natural Language Processing (NLP).","This holds particular significance in the context of evaluating and advancing Large Language Models (LLMs), given the existing gap in specialized benchmarks.","To bridge this gap, we introduce SportQA, a novel benchmark specifically designed for evaluating LLMs in the context of sports understanding.","SportQA encompasses over 70,000 multiple-choice questions across three distinct difficulty levels, each targeting different aspects of sports knowledge from basic historical facts to intricate, scenario-based reasoning tasks.","We conducted a thorough evaluation of prevalent LLMs, mainly utilizing few-shot learning paradigms supplemented by chain-of-thought (CoT) prompting.","Our results reveal that while LLMs exhibit competent performance in basic sports knowledge, they struggle with more complex, scenario-based sports reasoning, lagging behind human expertise.","The introduction of SportQA marks a significant step forward in NLP, offering a tool for assessing and enhancing sports understanding in LLMs."],"url":"http://arxiv.org/abs/2402.15862v1","category":"cs.CL"}
{"created":"2024-02-26 07:48:19","title":"DEYO: DETR with YOLO for End-to-End Object Detection","abstract":"The training paradigm of DETRs is heavily contingent upon pre-training their backbone on the ImageNet dataset. However, the limited supervisory signals provided by the image classification task and one-to-one matching strategy result in an inadequately pre-trained neck for DETRs. Additionally, the instability of matching in the early stages of training engenders inconsistencies in the optimization objectives of DETRs. To address these issues, we have devised an innovative training methodology termed step-by-step training. Specifically, in the first stage of training, we employ a classic detector, pre-trained with a one-to-many matching strategy, to initialize the backbone and neck of the end-to-end detector. In the second stage of training, we froze the backbone and neck of the end-to-end detector, necessitating the training of the decoder from scratch. Through the application of step-by-step training, we have introduced the first real-time end-to-end object detection model that utilizes a purely convolutional structure encoder, DETR with YOLO (DEYO). Without reliance on any supplementary training data, DEYO surpasses all existing real-time object detectors in both speed and accuracy. Moreover, the comprehensive DEYO series can complete its second-phase training on the COCO dataset using a single 8GB RTX 4060 GPU, significantly reducing the training expenditure. Source code and pre-trained models are available at https://github.com/ouyanghaodong/DEYO.","sentences":["The training paradigm of DETRs is heavily contingent upon pre-training their backbone on the ImageNet dataset.","However, the limited supervisory signals provided by the image classification task and one-to-one matching strategy result in an inadequately pre-trained neck for DETRs.","Additionally, the instability of matching in the early stages of training engenders inconsistencies in the optimization objectives of DETRs.","To address these issues, we have devised an innovative training methodology termed step-by-step training.","Specifically, in the first stage of training, we employ a classic detector, pre-trained with a one-to-many matching strategy, to initialize the backbone and neck of the end-to-end detector.","In the second stage of training, we froze the backbone and neck of the end-to-end detector, necessitating the training of the decoder from scratch.","Through the application of step-by-step training, we have introduced the first real-time end-to-end object detection model that utilizes a purely convolutional structure encoder, DETR with YOLO (DEYO).","Without reliance on any supplementary training data, DEYO surpasses all existing real-time object detectors in both speed and accuracy.","Moreover, the comprehensive DEYO series can complete its second-phase training on the COCO dataset using a single 8GB RTX 4060 GPU, significantly reducing the training expenditure.","Source code and pre-trained models are available at https://github.com/ouyanghaodong/DEYO."],"url":"http://arxiv.org/abs/2402.16370v1","category":"cs.CV"}
{"created":"2024-02-26 06:27:26","title":"Lower bound for large midpoint transversal fluctuations in the corner growth model","abstract":"The study of transversal fluctuation of the optimal path has been a crucial aspect of the KPZ universality class. In this paper, we establish the first probability lower bound, with optimal exponential order, for the rare event in which the midpoint (or any fixed level) of the geodesic has a large transversal fluctuation. We present our result in the setting of the corner growth model, which is also known as the exponential last-passage percolation. The previously known lower bounds hold for the maximum transversal fluctuation along the entire geodesic (Hammond-Sarkar'20) or the starting portion of the geodesic at a local scale (Agarwal'23). Our result improves upon these as now the rare event can demand where the large fluctuation occurs exactly along the geodesic, on both local and global scales. This fills the final missing piece in the literature on the tail bounds for the transversal fluctuation of geodesics. Our proof utilizes the coupling method: we first obtain a version of the estimate for the semi-infinite geodesic using duality and then transfer the result to finite geodesics using path monotonicity. Our method differs from the previous works (Agarwal'23, Hammond-Sarkar'20), and in fact, we do not require fine information about the left tail moderate deviation of the last-passage time, which played a crucial role in (Agarwal'23, Hammond-Sarkar'20).","sentences":["The study of transversal fluctuation of the optimal path has been a crucial aspect of the KPZ universality class.","In this paper, we establish the first probability lower bound, with optimal exponential order, for the rare event in which the midpoint (or any fixed level) of the geodesic has a large transversal fluctuation.","We present our result in the setting of the corner growth model, which is also known as the exponential last-passage percolation.","The previously known lower bounds hold for the maximum transversal fluctuation along the entire geodesic (Hammond-Sarkar'20) or the starting portion of the geodesic at a local scale (Agarwal'23).","Our result improves upon these as now the rare event can demand where the large fluctuation occurs exactly along the geodesic, on both local and global scales.","This fills the final missing piece in the literature on the tail bounds for the transversal fluctuation of geodesics.","Our proof utilizes the coupling method: we first obtain a version of the estimate for the semi-infinite geodesic using duality and then transfer the result to finite geodesics using path monotonicity.","Our method differs from the previous works (Agarwal'23, Hammond-Sarkar'20), and in fact, we do not require fine information about the left tail moderate deviation of the last-passage time, which played a crucial role in (Agarwal'23, Hammond-Sarkar'20)."],"url":"http://arxiv.org/abs/2402.16332v1","category":"math.PR"}
{"created":"2024-02-26 06:22:41","title":"A Joint Communication and Computation Design for Probabilistic Semantic Communications","abstract":"In this paper, the problem of joint transmission and computation resource allocation for a multi-user probabilistic semantic communication (PSC) network is investigated. In the considered model, users employ semantic information extraction techniques to compress their large-sized data before transmitting them to a multi-antenna base station (BS). Our model represents large-sized data through substantial knowledge graphs, utilizing shared probability graphs between the users and the BS for efficient semantic compression. The resource allocation problem is formulated as an optimization problem with the objective of maximizing the sum of equivalent rate of all users, considering total power budget and semantic resource limit constraints. The computation load considered in the PSC network is formulated as a non-smooth piecewise function with respect to the semantic compression ratio. To tackle this non-convex non-smooth optimization challenge, a three-stage algorithm is proposed where the solutions for the receive beamforming matrix of the BS, transmit power of each user, and semantic compression ratio of each user are obtained stage by stage. Numerical results validate the effectiveness of our proposed scheme.","sentences":["In this paper, the problem of joint transmission and computation resource allocation for a multi-user probabilistic semantic communication (PSC) network is investigated.","In the considered model, users employ semantic information extraction techniques to compress their large-sized data before transmitting them to a multi-antenna base station (BS).","Our model represents large-sized data through substantial knowledge graphs, utilizing shared probability graphs between the users and the BS for efficient semantic compression.","The resource allocation problem is formulated as an optimization problem with the objective of maximizing the sum of equivalent rate of all users, considering total power budget and semantic resource limit constraints.","The computation load considered in the PSC network is formulated as a non-smooth piecewise function with respect to the semantic compression ratio.","To tackle this non-convex non-smooth optimization challenge, a three-stage algorithm is proposed where the solutions for the receive beamforming matrix of the BS, transmit power of each user, and semantic compression ratio of each user are obtained stage by stage.","Numerical results validate the effectiveness of our proposed scheme."],"url":"http://arxiv.org/abs/2402.16328v1","category":"cs.IT"}
{"created":"2024-02-26 06:08:11","title":"Algorithms for Halfplane Coverage and Related Problems","abstract":"Given in the plane a set of points and a set of halfplanes, we consider the problem of computing a smallest subset of halfplanes whose union covers all points. In this paper, we present an $O(n^{4/3}\\log^{5/3}n\\log^{O(1)}\\log n)$-time algorithm for the problem, where $n$ is the total number of all points and halfplanes. This improves the previously best algorithm of $n^{10/3}2^{O(\\log^*n)}$ time by roughly a quadratic factor. For the special case where all halfplanes are lower ones, our algorithm runs in $O(n\\log n)$ time, which improves the previously best algorithm of $n^{4/3}2^{O(\\log^*n)}$ time and matches an $\\Omega(n\\log n)$ lower bound. Further, our techniques can be extended to solve a star-shaped polygon coverage problem in $O(n\\log n)$ time, which in turn leads to an $O(n\\log n)$-time algorithm for computing an instance-optimal $\\epsilon$-kernel of a set of $n$ points in the plane. Agarwal and Har-Peled presented an $O(nk\\log n)$-time algorithm for this problem in SoCG 2023, where $k$ is the size of the $\\epsilon$-kernel; they also raised an open question whether the problem can be solved in $O(n\\log n)$ time. Our result thus answers the open question affirmatively.","sentences":["Given in the plane a set of points and a set of halfplanes, we consider the problem of computing a smallest subset of halfplanes whose union covers all points.","In this paper, we present an $O(n^{4/3}\\log^{5/3}n\\log^{O(1)}\\log n)$-time algorithm for the problem, where $n$ is the total number of all points and halfplanes.","This improves the previously best algorithm of $n^{10/3}2^{O(\\log^*n)}$ time by roughly a quadratic factor.","For the special case where all halfplanes are lower ones, our algorithm runs in $O(n\\log n)$ time, which improves the previously best algorithm of $n^{4/3}2^{O(\\log^*n)}$ time and matches an $\\Omega(n\\log n)$ lower bound.","Further, our techniques can be extended to solve a star-shaped polygon coverage problem in $O(n\\log n)$ time, which in turn leads to an $O(n\\log n)$-time algorithm for computing an instance-optimal $\\epsilon$-kernel of a set of $n$ points in the plane.","Agarwal and Har-Peled presented an $O(nk\\log n)$-time algorithm for this problem in SoCG 2023, where $k$ is the size of the $\\epsilon$-kernel; they also raised an open question whether the problem can be solved in $O(n\\log n)$ time.","Our result thus answers the open question affirmatively."],"url":"http://arxiv.org/abs/2402.16323v1","category":"cs.CG"}
{"created":"2024-02-25 15:32:25","title":"Finding Near-Optimal Portfolios With Quality-Diversity","abstract":"The majority of standard approaches to financial portfolio optimization (PO) are based on the mean-variance (MV) framework. Given a risk aversion coefficient, the MV procedure yields a single portfolio that represents the optimal trade-off between risk and return. However, the resulting optimal portfolio is known to be highly sensitive to the input parameters, i.e., the estimates of the return covariance matrix and the mean return vector. It has been shown that a more robust and flexible alternative lies in determining the entire region of near-optimal portfolios. In this paper, we present a novel approach for finding a diverse set of such portfolios based on quality-diversity (QD) optimization. More specifically, we employ the CVT-MAP-Elites algorithm, which is scalable to high-dimensional settings with potentially hundreds of behavioral descriptors and/or assets. The results highlight the promising features of QD as a novel tool in PO.","sentences":["The majority of standard approaches to financial portfolio optimization (PO) are based on the mean-variance (MV) framework.","Given a risk aversion coefficient, the MV procedure yields a single portfolio that represents the optimal trade-off between risk and return.","However, the resulting optimal portfolio is known to be highly sensitive to the input parameters, i.e., the estimates of the return covariance matrix and the mean return vector.","It has been shown that a more robust and flexible alternative lies in determining the entire region of near-optimal portfolios.","In this paper, we present a novel approach for finding a diverse set of such portfolios based on quality-diversity (QD) optimization.","More specifically, we employ the CVT-MAP-Elites algorithm, which is scalable to high-dimensional settings with potentially hundreds of behavioral descriptors and/or assets.","The results highlight the promising features of QD as a novel tool in PO."],"url":"http://arxiv.org/abs/2402.16118v1","category":"q-fin.PM"}
{"created":"2024-02-25 08:46:48","title":"Optimizing single-photon quantum radar detection through partially postselected filtering","abstract":"In this study, we explore an approach aimed at enhancing the transmission or reflection coefficients of absorbing materials through the utilization of joint measurements of entangled photon states. On one hand, through the implementation of photon catalysis on the reflected channel, we can effectively modify the state of the transmission channel, leading to a notable improvement in the transmission ratio. Similarly, this approach holds potential for significantly amplifying the reflection ratio of absorbing materials, which is useful for detecting cooperative targets. On the other hand, employing statistical counting methods based on the technique of heralding on zero photons, we evaluate the influence of our reflection enhancement protocol for detecting noncooperative targets, which is validated through Monte Carlo simulations of a quantum-radar setup affected by Gaussian white noise. Our results demonstrate a remarkable enhancement in the signal-to-noise ratio of imaging, albeit with an increase in mean squared error. These findings highlight the potential practical applications of our approach in the implementation of quantum radar.","sentences":["In this study, we explore an approach aimed at enhancing the transmission or reflection coefficients of absorbing materials through the utilization of joint measurements of entangled photon states.","On one hand, through the implementation of photon catalysis on the reflected channel, we can effectively modify the state of the transmission channel, leading to a notable improvement in the transmission ratio.","Similarly, this approach holds potential for significantly amplifying the reflection ratio of absorbing materials, which is useful for detecting cooperative targets.","On the other hand, employing statistical counting methods based on the technique of heralding on zero photons, we evaluate the influence of our reflection enhancement protocol for detecting noncooperative targets, which is validated through Monte Carlo simulations of a quantum-radar setup affected by Gaussian white noise.","Our results demonstrate a remarkable enhancement in the signal-to-noise ratio of imaging, albeit with an increase in mean squared error.","These findings highlight the potential practical applications of our approach in the implementation of quantum radar."],"url":"http://arxiv.org/abs/2402.16031v1","category":"quant-ph"}
{"created":"2024-02-25 02:04:56","title":"ViSTec: Video Modeling for Sports Technique Recognition and Tactical Analysis","abstract":"The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos. However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques. State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos. To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights. Our approach integrates a graph to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias. A two-stage action perception model is jointly trained to align with the contextual knowledge in the graph. Experiments demonstrate that our method outperforms existing models by a significant margin. Case studies with experts from the Chinese national table tennis team validate our model's capacity to automate analysis for technical actions and tactical strategies. More details are available at: https://ViSTec2024.github.io/.","sentences":["The immense popularity of racket sports has fueled substantial demand in tactical analysis with broadcast videos.","However, existing manual methods require laborious annotation, and recent attempts leveraging video perception models are limited to low-level annotations like ball trajectories, overlooking tactics that necessitate an understanding of stroke techniques.","State-of-the-art action segmentation models also struggle with technique recognition due to frequent occlusions and motion-induced blurring in racket sports videos.","To address these challenges, We propose ViSTec, a Video-based Sports Technique recognition model inspired by human cognition that synergizes sparse visual data with rich contextual insights.","Our approach integrates a graph to explicitly model strategic knowledge in stroke sequences and enhance technique recognition with contextual inductive bias.","A two-stage action perception model is jointly trained to align with the contextual knowledge in the graph.","Experiments demonstrate that our method outperforms existing models by a significant margin.","Case studies with experts from the Chinese national table tennis team validate our model's capacity to automate analysis for technical actions and tactical strategies.","More details are available at: https://ViSTec2024.github.io/."],"url":"http://arxiv.org/abs/2402.15952v1","category":"cs.CV"}
{"created":"2024-02-25 00:37:02","title":"Minimum energy density steering of linear systems with Gromov-Wasserstein terminal cost","abstract":"In this study, we address optimal control problems focused on steering the probabilistic distribution of state variables in linear dynamical systems. Specifically, we address the problem of controlling the structural properties of Gaussian state distributions to predefined targets at terminal times. This task is not yet explored in existing works that primarily aim to exactly match state distributions. By employing the Gromov-Wasserstein (GW) distance as the terminal cost, we formulate a problem that seeks to align the structure of the state density with that of a desired distribution. This approach allows us to extend the control objectives to capture the distribution's shape. We demonstrate that this complex problem can be reduced to a Difference of Convex (DC) programming, which is efficiently solvable through the DC algorithm. Through numerical experiments, we confirm that the terminal distribution indeed gets closer to the desired structural properties of the target distribution.","sentences":["In this study, we address optimal control problems focused on steering the probabilistic distribution of state variables in linear dynamical systems.","Specifically, we address the problem of controlling the structural properties of Gaussian state distributions to predefined targets at terminal times.","This task is not yet explored in existing works that primarily aim to exactly match state distributions.","By employing the Gromov-Wasserstein (GW) distance as the terminal cost, we formulate a problem that seeks to align the structure of the state density with that of a desired distribution.","This approach allows us to extend the control objectives to capture the distribution's shape.","We demonstrate that this complex problem can be reduced to a Difference of Convex (DC) programming, which is efficiently solvable through the DC algorithm.","Through numerical experiments, we confirm that the terminal distribution indeed gets closer to the desired structural properties of the target distribution."],"url":"http://arxiv.org/abs/2402.15942v1","category":"math.OC"}
{"created":"2024-02-24 20:42:09","title":"Dedicated Restricted Target Wake Time for Real-Time Applications in Wi-Fi 7","abstract":"Real-time applications (RTA) tend to play a crucial role in people's everyday life. Such applications are among the key use cases for the next generations of wireless technologies. RTA applications are characterized by strict guaranteed delay requirements (in the order of a few milliseconds). One of the pillars of enabling RTA in next-generation Wi-Fi standards is Restricted Target Wake Time (R-TWT), which provides Wi-Fi stations exclusive channel access within negotiated service periods (SPs). If each RTA data flow uses dedicated SPs for data transmission, they are completely isolated from each other and do not experience any contention. To ensure the satisfaction of RTA QoS requirements while minimizing the channel airtime consumption, it is important to properly select the R-TWT parameters, namely the duration of SPs and the period between SPs. In this paper, we develop a mathematical model that estimates the delay probability distribution and packet loss probability for a given set of network, traffic and R-TWT parameters. Using this model, the access point can select the optimal R-TWT parameters for the given QoS requirements. The high accuracy of the model is proven by means of simulation.","sentences":["Real-time applications (RTA) tend to play a crucial role in people's everyday life.","Such applications are among the key use cases for the next generations of wireless technologies.","RTA applications are characterized by strict guaranteed delay requirements (in the order of a few milliseconds).","One of the pillars of enabling RTA in next-generation Wi-Fi standards is Restricted Target Wake Time (R-TWT), which provides Wi-Fi stations exclusive channel access within negotiated service periods (SPs).","If each RTA data flow uses dedicated SPs for data transmission, they are completely isolated from each other and do not experience any contention.","To ensure the satisfaction of RTA QoS requirements while minimizing the channel airtime consumption, it is important to properly select the R-TWT parameters, namely the duration of SPs and the period between SPs.","In this paper, we develop a mathematical model that estimates the delay probability distribution and packet loss probability for a given set of network, traffic and R-TWT parameters.","Using this model, the access point can select the optimal R-TWT parameters for the given QoS requirements.","The high accuracy of the model is proven by means of simulation."],"url":"http://arxiv.org/abs/2402.15900v1","category":"cs.NI"}
{"created":"2024-02-24 19:55:29","title":"Optimality of weighted contracts for multi-agent contract design with a budget","abstract":"We study a contract design problem between a principal and multiple agents. Each agent participates in an independent task with binary outcomes (success or failure), in which it may exert costly effort towards improving its probability of success, and the principal has a fixed budget which it can use to provide outcome-dependent rewards to the agents. Crucially, we assume the principal cares only about maximizing the agents' probabilities of success, not how much of the budget it expends. We first show that a contract is optimal for some objective if and only if it is a successful-get-everything contract. An immediate consequence of this result is that piece-rate contracts and bonus-pool contracts are never optimal in this setting. We then show that for any objective, there is an optimal priority-based weighted contract, which assigns positive weights and priority levels to the agents, and splits the budget among the highest-priority successful agents, with each such agent receiving a fraction of the budget proportional to her weight. This result provides a significant reduction in the dimensionality of the principal's optimal contract design problem and gives an interpretable and easily implementable optimal contract. Finally, we discuss an application of our results to the design of optimal contracts with two agents and quadratic costs. In this context, we find that the optimal contract assigns a higher weight to the agent whose success it values more, irrespective of the heterogeneity in the agents' cost parameters. This suggests that the structure of the optimal contract depends primarily on the bias in the principal's objective and is, to some extent, robust to the heterogeneity in the agents' cost functions.","sentences":["We study a contract design problem between a principal and multiple agents.","Each agent participates in an independent task with binary outcomes (success or failure), in which it may exert costly effort towards improving its probability of success, and the principal has a fixed budget which it can use to provide outcome-dependent rewards to the agents.","Crucially, we assume the principal cares only about maximizing the agents' probabilities of success, not how much of the budget it expends.","We first show that a contract is optimal for some objective if and only if it is a successful-get-everything contract.","An immediate consequence of this result is that piece-rate contracts and bonus-pool contracts are never optimal in this setting.","We then show that for any objective, there is an optimal priority-based weighted contract, which assigns positive weights and priority levels to the agents, and splits the budget among the highest-priority successful agents, with each such agent receiving a fraction of the budget proportional to her weight.","This result provides a significant reduction in the dimensionality of the principal's optimal contract design problem and gives an interpretable and easily implementable optimal contract.","Finally, we discuss an application of our results to the design of optimal contracts with two agents and quadratic costs.","In this context, we find that the optimal contract assigns a higher weight to the agent whose success it values more, irrespective of the heterogeneity in the agents' cost parameters.","This suggests that the structure of the optimal contract depends primarily on the bias in the principal's objective and is, to some extent, robust to the heterogeneity in the agents' cost functions."],"url":"http://arxiv.org/abs/2402.15890v1","category":"econ.TH"}
