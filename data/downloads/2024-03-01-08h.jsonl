{"created":"2024-02-29 18:59:58","title":"DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models","abstract":"Diffusion models have achieved great success in synthesizing high-quality images. However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications. In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs. Our method splits the model input into multiple patches and assigns each patch to a GPU. However, na\\\"{\\i}vely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead. To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step. Therefore, our method supports asynchronous communication, which can be pipelined by computation. Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1$\\times$ speedup on eight NVIDIA A100s compared to one. Our code is publicly available at https://github.com/mit-han-lab/distrifuser.","sentences":["Diffusion models have achieved great success in synthesizing high-quality images.","However, generating high-resolution images with diffusion models is still challenging due to the enormous computational costs, resulting in a prohibitive latency for interactive applications.","In this paper, we propose DistriFusion to tackle this problem by leveraging parallelism across multiple GPUs.","Our method splits the model input into multiple patches and assigns each patch to a GPU.","However, na\\\"{\\i}vely implementing such an algorithm breaks the interaction between patches and loses fidelity, while incorporating such an interaction will incur tremendous communication overhead.","To overcome this dilemma, we observe the high similarity between the input from adjacent diffusion steps and propose displaced patch parallelism, which takes advantage of the sequential nature of the diffusion process by reusing the pre-computed feature maps from the previous timestep to provide context for the current step.","Therefore, our method supports asynchronous communication, which can be pipelined by computation.","Extensive experiments show that our method can be applied to recent Stable Diffusion XL with no quality degradation and achieve up to a 6.1$\\times$ speedup on eight NVIDIA A100s compared to one.","Our code is publicly available at https://github.com/mit-han-lab/distrifuser."],"url":"http://arxiv.org/abs/2402.19481v1","category":"cs.CV"}
{"created":"2024-02-29 18:59:50","title":"Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers","abstract":"The quality of the data and annotation upper-bounds the quality of a downstream model. While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect. First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video. Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions. Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames. Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset. We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video. Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation. In this way, we get 70M videos paired with high-quality text captions. We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation. The models trained on the proposed data score substantially better on the majority of metrics across all the tasks.","sentences":["The quality of the data and annotation upper-bounds the quality of a downstream model.","While there exist large text corpora and image-text pairs, high-quality video-text data is much harder to collect.","First of all, manual labeling is more time-consuming, as it requires an annotator to watch an entire video.","Second, videos have a temporal dimension, consisting of several scenes stacked together, and showing multiple actions.","Accordingly, to establish a video dataset with high-quality captions, we propose an automatic approach leveraging multimodal inputs, such as textual video description, subtitles, and individual video frames.","Specifically, we curate 3.8M high-resolution videos from the publicly available HD-VILA-100M dataset.","We then split them into semantically consistent video clips, and apply multiple cross-modality teacher models to obtain captions for each video.","Next, we finetune a retrieval model on a small subset where the best caption of each video is manually selected and then employ the model in the whole dataset to select the best caption as the annotation.","In this way, we get 70M videos paired with high-quality text captions.","We dub the dataset as Panda-70M. We show the value of the proposed dataset on three downstream tasks: video captioning, video and text retrieval, and text-driven video generation.","The models trained on the proposed data score substantially better on the majority of metrics across all the tasks."],"url":"http://arxiv.org/abs/2402.19479v1","category":"cs.CV"}
{"created":"2024-02-29 18:59:31","title":"Learning a Generalized Physical Face Model From Data","abstract":"Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset in a simulation-free manner. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.","sentences":["Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits.","Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry.","Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training.","In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset in a simulation-free manner.","Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically.","Fitting is as easy as providing a single 3D face scan, or even a single face image.","After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters.","All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more."],"url":"http://arxiv.org/abs/2402.19477v1","category":"cs.CV"}
{"created":"2024-02-29 18:59:25","title":"The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?","abstract":"While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs. Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile. In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks. Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes. First, models mistakenly classify them as correct. Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct. Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of sampling a correct program from scratch. Counterfeits also have very unexpected properties: first, counterfeit programs for problems that are easier for a model to solve are not necessarily easier to detect and only slightly easier to execute and repair. Second, counterfeits from a given model are just as confusing to the model itself as they are to other models. Finally, both strong and weak models are able to generate counterfeit samples that equally challenge all models. In light of our findings, we recommend that care and caution be taken when relying on models to understand their own samples, especially when no external feedback is incorporated.","sentences":["While language models are increasingly more proficient at code generation, they still frequently generate incorrect programs.","Many of these programs are obviously wrong, but others are more subtle and pass weaker correctness checks such as being able to compile.","In this work, we focus on these counterfeit samples: programs sampled from a language model that 1) have a high enough log-probability to be generated at a moderate temperature and 2) pass weak correctness checks.","Overall, we discover that most models have a very shallow understanding of counterfeits through three clear failure modes.","First, models mistakenly classify them as correct.","Second, models are worse at reasoning about the execution behaviour of counterfeits and often predict their execution results as if they were correct.","Third, when asking models to fix counterfeits, the likelihood of a model successfully repairing a counterfeit is often even lower than that of sampling a correct program from scratch.","Counterfeits also have very unexpected properties: first, counterfeit programs for problems that are easier for a model to solve are not necessarily easier to detect and only slightly easier to execute and repair.","Second, counterfeits from a given model are just as confusing to the model itself as they are to other models.","Finally, both strong and weak models are able to generate counterfeit samples that equally challenge all models.","In light of our findings, we recommend that care and caution be taken when relying on models to understand their own samples, especially when no external feedback is incorporated."],"url":"http://arxiv.org/abs/2402.19475v1","category":"cs.SE"}
{"created":"2024-02-29 18:59:17","title":"The All-Seeing Project V2: Towards General Relation Comprehension of the Open World","abstract":"We present the All-Seeing Project V2: a new model and dataset designed for understanding object relations in images. Specifically, we propose the All-Seeing Model V2 (ASMv2) that integrates the formulation of text generation, object localization, and relation comprehension into a relation conversation (ReC) task. Leveraging this unified task, our model excels not only in perceiving and recognizing all objects within the image but also in grasping the intricate relation graph between them, diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs). To facilitate training and evaluation of MLLMs in relation understanding, we created the first high-quality ReC dataset ({AS-V2) which is aligned with the format of standard instruction tuning data. In addition, we design a new benchmark, termed Circular-based Relation Probing Evaluation (CRPE) for comprehensively evaluating the relation comprehension capabilities of MLLMs. Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware benchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that our work can inspire more future research and contribute to the evolution towards artificial general intelligence. Our project is released at https://github.com/OpenGVLab/all-seeing.","sentences":["We present the All-Seeing Project V2: a new model and dataset designed for understanding object relations in images.","Specifically, we propose the All-Seeing Model V2 (ASMv2) that integrates the formulation of text generation, object localization, and relation comprehension into a relation conversation (ReC) task.","Leveraging this unified task, our model excels not only in perceiving and recognizing all objects within the image but also in grasping the intricate relation graph between them, diminishing the relation hallucination often encountered by Multi-modal Large Language Models (MLLMs).","To facilitate training and evaluation of MLLMs in relation understanding, we created the first high-quality ReC dataset ({AS-V2) which is aligned with the format of standard instruction tuning data.","In addition, we design a new benchmark, termed Circular-based Relation Probing Evaluation (CRPE) for comprehensively evaluating the relation comprehension capabilities of MLLMs.","Notably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware benchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin.","We hope that our work can inspire more future research and contribute to the evolution towards artificial general intelligence.","Our project is released at https://github.com/OpenGVLab/all-seeing."],"url":"http://arxiv.org/abs/2402.19474v1","category":"cs.CV"}
{"created":"2024-02-29 18:59:01","title":"Retrieval-Augmented Generation for AI-Generated Content: A Survey","abstract":"The development of Artificial Intelligence Generated Content (AIGC) has been facilitated by advancements in model algorithms, scalable foundation model architectures, and the availability of ample high-quality datasets. While AIGC has achieved remarkable performance, it still faces challenges, such as the difficulty of maintaining up-to-date and long-tail knowledge, the risk of data leakage, and the high costs associated with training and inference. Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges. In particular, RAG introduces the information retrieval process, which enhances AIGC results by retrieving relevant objects from available data stores, leading to greater accuracy and robustness. In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios. We first classify RAG foundations according to how the retriever augments the generator. We distill the fundamental abstractions of the augmentation methodologies for various retrievers and generators. This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress. We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems. Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners. Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research. Project: https://github.com/hymie122/RAG-Survey","sentences":["The development of Artificial Intelligence Generated Content (AIGC) has been facilitated by advancements in model algorithms, scalable foundation model architectures, and the availability of ample high-quality datasets.","While AIGC has achieved remarkable performance, it still faces challenges, such as the difficulty of maintaining up-to-date and long-tail knowledge, the risk of data leakage, and the high costs associated with training and inference.","Retrieval-Augmented Generation (RAG) has recently emerged as a paradigm to address such challenges.","In particular, RAG introduces the information retrieval process, which enhances AIGC results by retrieving relevant objects from available data stores, leading to greater accuracy and robustness.","In this paper, we comprehensively review existing efforts that integrate RAG technique into AIGC scenarios.","We first classify RAG foundations according to how the retriever augments the generator.","We distill the fundamental abstractions of the augmentation methodologies for various retrievers and generators.","This unified perspective encompasses all RAG scenarios, illuminating advancements and pivotal technologies that help with potential future progress.","We also summarize additional enhancements methods for RAG, facilitating effective engineering and implementation of RAG systems.","Then from another view, we survey on practical applications of RAG across different modalities and tasks, offering valuable references for researchers and practitioners.","Furthermore, we introduce the benchmarks for RAG, discuss the limitations of current RAG systems, and suggest potential directions for future research.","Project: https://github.com/hymie122/RAG-Survey"],"url":"http://arxiv.org/abs/2402.19473v1","category":"cs.CV"}
{"created":"2024-02-29 18:58:15","title":"Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling","abstract":"Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty. How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources? We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship. Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain. We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios. In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines. Our results illustrate how Bayesian models of question-asking can leverage the statistics of language to capture human priors, while highlighting some shortcomings of pure LLMs as grounded reasoners.","sentences":["Questions combine our mastery of language with our remarkable facility for reasoning about uncertainty.","How do people navigate vast hypothesis spaces to pose informative questions given limited cognitive resources?","We study these tradeoffs in a classic grounded question-asking task based on the board game Battleship.","Our language-informed program sampling (LIPS) model uses large language models (LLMs) to generate natural language questions, translate them into symbolic programs, and evaluate their expected information gain.","We find that with a surprisingly modest resource budget, this simple Monte Carlo optimization strategy yields informative questions that mirror human performance across varied Battleship board scenarios.","In contrast, LLM-only baselines struggle to ground questions in the board state; notably, GPT-4V provides no improvement over non-visual baselines.","Our results illustrate how Bayesian models of question-asking can leverage the statistics of language to capture human priors, while highlighting some shortcomings of pure LLMs as grounded reasoners."],"url":"http://arxiv.org/abs/2402.19471v1","category":"cs.CL"}
{"created":"2024-02-29 18:57:39","title":"Towards Generalizable Tumor Synthesis","abstract":"Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation. However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals). This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys. We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ. Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities.","sentences":["Tumor synthesis enables the creation of artificial tumors in medical images, facilitating the training of AI models for tumor detection and segmentation.","However, success in tumor synthesis hinges on creating visually realistic tumors that are generalizable across multiple organs and, furthermore, the resulting AI models being capable of detecting real tumors in images sourced from different domains (e.g., hospitals).","This paper made a progressive stride toward generalizable tumor synthesis by leveraging a critical observation: early-stage tumors (< 2cm) tend to have similar imaging characteristics in computed tomography (CT), whether they originate in the liver, pancreas, or kidneys.","We have ascertained that generative AI models, e.g., Diffusion Models, can create realistic tumors generalized to a range of organs even when trained on a limited number of tumor examples from only one organ.","Moreover, we have shown that AI models trained on these synthetic tumors can be generalized to detect and segment real tumors from CT volumes, encompassing a broad spectrum of patient demographics, imaging protocols, and healthcare facilities."],"url":"http://arxiv.org/abs/2402.19470v1","category":"eess.IV"}
{"created":"2024-02-29 18:57:37","title":"Humanoid Locomotion as Next Token Prediction","abstract":"We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language. Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories. To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality. This general formulation enables us to leverage data with missing modalities, like video trajectories without actions. We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans. We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot. Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward. These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories.","sentences":["We cast real-world humanoid control as a next token prediction problem, akin to predicting the next word in language.","Our model is a causal transformer trained via autoregressive prediction of sensorimotor trajectories.","To account for the multi-modal nature of the data, we perform prediction in a modality-aligned way, and for each input token predict the next token from the same modality.","This general formulation enables us to leverage data with missing modalities, like video trajectories without actions.","We train our model on a collection of simulated trajectories coming from prior neural network policies, model-based controllers, motion capture data, and YouTube videos of humans.","We show that our model enables a full-sized humanoid to walk in San Francisco zero-shot.","Our model can transfer to the real world even when trained on only 27 hours of walking data, and can generalize to commands not seen during training like walking backward.","These findings suggest a promising path toward learning challenging real-world control tasks by generative modeling of sensorimotor trajectories."],"url":"http://arxiv.org/abs/2402.19469v1","category":"cs.RO"}
{"created":"2024-02-29 18:57:33","title":"Gravitational wave constraints on planetary-mass primordial black holes using LIGO O3a data","abstract":"Gravitational waves from sub-solar mass inspiraling compact objects would provide smoking-gun evidence for primordial black holes (PBHs). We perform the first search for inspiraling planetary-mass PBHs, both in equal-mass or asymmetric mass ratio binaries, using data from the first half of the LIGO-Virgo-KAGRA third observing run. We do not find any significant candidates, but determine the maximum luminosity distance reachable with our search to be of $O(0.1-100)$ kpc, and corresponding model-independent upper limits on the merger rate densities to be $O(10^{3}-10^{-7})$ kpc$^{-3}$yr$^{-1}$ for systems with chirp masses $O(10^{-4}-10^{-2})M_\\odot$, respectively. Furthermore, we interpret these rate densities as arising from PBH binaries, and thereby constrain the fraction of dark matter that these objects could compose. For equal-mass PBH binaries, we find $f_\\text{PBH}<$ [1, 0.04] for $m_\\text{PBH}\\in [2\\times 10^{-3},10^{-2}]M_\\odot$, respectively. For asymmetric mass-ratio binaries, where $m_1=2.5M_\\odot$ and $m_2\\ll m_1$, we constrain the mass function $f(m_2)<1$ for $m_2\\in [1.5\\times10^{-5},2\\times10^{-4}]M_\\odot$, assuming $f_\\text{PBH}=0.1$ and $f(m_1)\\sim 1$. Our results constitute the first gravitational-wave constraints on planetary-mass PBHs in both equal-mass and highly asymmetric mass-ratio systems, provide a computationally efficient alternative to matched filtering in this mass regime, and complement microlensing experiments to probe the existence of these objects. The data necessary to produce the upper limit plots has also been released on Zenodo.","sentences":["Gravitational waves from sub-solar mass inspiraling compact objects would provide smoking-gun evidence for primordial black holes (PBHs).","We perform the first search for inspiraling planetary-mass PBHs, both in equal-mass or asymmetric mass ratio binaries, using data from the first half of the LIGO-Virgo-KAGRA third observing run.","We do not find any significant candidates, but determine the maximum luminosity distance reachable with our search to be of $O(0.1-100)$ kpc, and corresponding model-independent upper limits on the merger rate densities to be $O(10^{3}-10^{-7})$ kpc$^{-3}$yr$^{-1}$ for systems with chirp masses $O(10^{-4}-10^{-2})M_\\odot$, respectively.","Furthermore, we interpret these rate densities as arising from PBH binaries, and thereby constrain the fraction of dark matter that these objects could compose.","For equal-mass PBH binaries, we find $f_\\text{PBH}<$","[1, 0.04] for $m_\\text{PBH}\\in [2\\times 10^{-3},10^{-2}]M_\\odot$, respectively.","For asymmetric mass-ratio binaries, where $m_1=2.5M_\\odot$ and $m_2\\ll m_1$, we constrain the mass function $f(m_2)<1$ for $m_2\\in [1.5\\times10^{-5},2\\times10^{-4}]M_\\odot$, assuming $f_\\text{PBH}=0.1$ and $f(m_1)\\sim 1$.","Our results constitute the first gravitational-wave constraints on planetary-mass PBHs in both equal-mass and highly asymmetric mass-ratio systems, provide a computationally efficient alternative to matched filtering in this mass regime, and complement microlensing experiments to probe the existence of these objects.","The data necessary to produce the upper limit plots has also been released on Zenodo."],"url":"http://arxiv.org/abs/2402.19468v1","category":"gr-qc"}
{"created":"2024-02-29 18:57:01","title":"TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning","abstract":"It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods.","sentences":["It is challenging to perform question-answering over complex, multimodal content such as television clips.","This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability.","We propose TV-TREES, the first multimodal entailment tree generator.","TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions.","We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods.","Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best of both worlds contrast to black-box methods."],"url":"http://arxiv.org/abs/2402.19467v1","category":"cs.CL"}
{"created":"2024-02-29 18:55:06","title":"Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models","abstract":"Ensuring the trustworthiness of large language models (LLMs) is crucial. Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness. In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness. To begin with, we apply linear probing to LLMs. The high probing accuracy suggests that \\textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}. Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training. We are the first to observe a similar two-phase phenomenon: fitting and compression~\\citep{shwartz2017opening}. This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field. We will make our code publicly accessible at \\url{https://github.com/ChnQ/TracingLLM}.","sentences":["Ensuring the trustworthiness of large language models (LLMs) is crucial.","Most studies concentrate on fully pre-trained LLMs to better understand and improve LLMs' trustworthiness.","In this paper, to reveal the untapped potential of pre-training, we pioneer the exploration of LLMs' trustworthiness during this period, focusing on five key dimensions: reliability, privacy, toxicity, fairness, and robustness.","To begin with, we apply linear probing to LLMs.","The high probing accuracy suggests that \\textit{LLMs in early pre-training can already distinguish concepts in each trustworthiness dimension}.","Therefore, to further uncover the hidden possibilities of pre-training, we extract steering vectors from a LLM's pre-training checkpoints to enhance the LLM's trustworthiness.","Finally, inspired by~\\citet{choi2023understanding} that mutual information estimation is bounded by linear probing accuracy, we also probe LLMs with mutual information to investigate the dynamics of trustworthiness during pre-training.","We are the first to observe a similar two-phase phenomenon: fitting and compression~\\citep{shwartz2017opening}.","This research provides an initial exploration of trustworthiness modeling during LLM pre-training, seeking to unveil new insights and spur further developments in the field.","We will make our code publicly accessible at \\url{https://github.com/ChnQ/TracingLLM}."],"url":"http://arxiv.org/abs/2402.19465v1","category":"cs.CL"}
{"created":"2024-02-29 18:55:03","title":"Curiosity-driven Red-teaming for Large Language Models","abstract":"Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content. To probe when an LLM generates unwanted content, the current paradigm is to recruit a \\textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs. However, relying solely on human testers is expensive and time-consuming. Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM. However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM. To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty. Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods. Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs. Code is available at \\url{https://github.com/Improbable-AI/curiosity_redteam}","sentences":["Large language models (LLMs) hold great potential for many natural language applications but risk generating incorrect or toxic content.","To probe when an LLM generates unwanted content, the current paradigm is to recruit a \\textit{red team} of human testers to design input prompts (i.e., test cases) that elicit undesirable responses from LLMs.","However, relying solely on human testers is expensive and time-consuming.","Recent works automate red teaming by training a separate red team LLM with reinforcement learning (RL) to generate test cases that maximize the chance of eliciting undesirable responses from the target LLM.","However, current RL methods are only able to generate a small number of effective test cases resulting in a low coverage of the span of prompts that elicit undesirable responses from the target LLM.","To overcome this limitation, we draw a connection between the problem of increasing the coverage of generated test cases and the well-studied approach of curiosity-driven exploration that optimizes for novelty.","Our method of curiosity-driven red teaming (CRT) achieves greater coverage of test cases while mantaining or increasing their effectiveness compared to existing methods.","Our method, CRT successfully provokes toxic responses from LLaMA2 model that has been heavily fine-tuned using human preferences to avoid toxic outputs.","Code is available at \\url{https://github.com/Improbable-AI/curiosity_redteam}"],"url":"http://arxiv.org/abs/2402.19464v1","category":"cs.LG"}
{"created":"2024-02-29 18:52:40","title":"Anomalous contribution to galactic rotation curves due to stochastic spacetime","abstract":"We consider a proposed alternative to quantum gravity, in which the spacetime metric is treated as classical, even while matter fields remain quantum. Consistency of the theory necessarily requires that the metric evolve stochastically. Here, we show that this stochastic behaviour leads to a modification of general relativity at low accelerations.   In the low acceleration regime, the variance in the acceleration produced by the gravitational field is high in comparison to that produced by the Newtonian potential, and acts as an entropic force, causing a deviation from Einstein's theory of general relativity. We show that in this \"diffusion regime\", the entropic force acts from a gravitational point of view, as if it were a contribution to the matter distribution.   We compute how this modifies the expectation value of the metric via the path integral formalism, and find that an entropic force driven by a stochastic cosmological constant can explain galactic rotation curves without needing to evoke dark matter. We caution that a greater understanding of this effect is needed before conclusions can be drawn, most likely through numerical simulations, and provide a template for computing the deviation from general relativity which serves as an experimental signature of the Brownian motion of spacetime.","sentences":["We consider a proposed alternative to quantum gravity, in which the spacetime metric is treated as classical, even while matter fields remain quantum.","Consistency of the theory necessarily requires that the metric evolve stochastically.","Here, we show that this stochastic behaviour leads to a modification of general relativity at low accelerations.   ","In the low acceleration regime, the variance in the acceleration produced by the gravitational field is high in comparison to that produced by the Newtonian potential, and acts as an entropic force, causing a deviation from Einstein's theory of general relativity.","We show that in this \"diffusion regime\", the entropic force acts from a gravitational point of view, as if it were a contribution to the matter distribution.   ","We compute how this modifies the expectation value of the metric via the path integral formalism, and find that an entropic force driven by a stochastic cosmological constant can explain galactic rotation curves without needing to evoke dark matter.","We caution that a greater understanding of this effect is needed before conclusions can be drawn, most likely through numerical simulations, and provide a template for computing the deviation from general relativity which serves as an experimental signature of the Brownian motion of spacetime."],"url":"http://arxiv.org/abs/2402.19459v1","category":"gr-qc"}
{"created":"2024-02-29 18:51:23","title":"$\\texttt{COSMIC}$: Mutual Information for Task-Agnostic Summarization Evaluation","abstract":"Assessing the quality of summarizers poses significant challenges. In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes. We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries. We introduce $\\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance. Comparative analyses against established metrics like $\\texttt{BERTScore}$ and $\\texttt{ROUGE}$ highlight the competitive performance of $\\texttt{COSMIC}$.","sentences":["Assessing the quality of summarizers poses significant challenges.","In response, we propose a novel task-oriented evaluation approach that assesses summarizers based on their capacity to produce summaries that are useful for downstream tasks, while preserving task outcomes.","We theoretically establish a direct relationship between the resulting error probability of these tasks and the mutual information between source texts and generated summaries.","We introduce $\\texttt{COSMIC}$ as a practical implementation of this metric, demonstrating its strong correlation with human judgment-based metrics and its effectiveness in predicting downstream task performance.","Comparative analyses against established metrics like $\\texttt{BERTScore}$ and $\\texttt{ROUGE}$ highlight the competitive performance of $\\texttt{COSMIC}$."],"url":"http://arxiv.org/abs/2402.19457v1","category":"cs.CL"}
{"created":"2024-02-29 18:50:28","title":"Statistical Estimation in the Spiked Tensor Model via the Quantum Approximate Optimization Algorithm","abstract":"The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization. In this paper, we analyze the performance of the QAOA on a statistical estimation problem, namely, the spiked tensor model, which exhibits a statistical-computational gap classically. We prove that the weak recovery threshold of $1$-step QAOA matches that of $1$-step tensor power iteration. Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant. This further implies that multi-step QAOA with tensor unfolding could achieve, but not surpass, the classical computation threshold $\\Theta(n^{(q-2)/4})$ for spiked $q$-tensors.   Meanwhile, we characterize the asymptotic overlap distribution for $p$-step QAOA, finding an intriguing sine-Gaussian law verified through simulations. For some $p$ and $q$, the QAOA attains an overlap that is larger by a constant factor than the tensor power iteration overlap. Of independent interest, our proof techniques employ the Fourier transform to handle difficult combinatorial sums, a novel approach differing from prior QAOA analyses on spin-glass models without planted structure.","sentences":["The quantum approximate optimization algorithm (QAOA) is a general-purpose algorithm for combinatorial optimization.","In this paper, we analyze the performance of the QAOA on a statistical estimation problem, namely, the spiked tensor model, which exhibits a statistical-computational gap classically.","We prove that the weak recovery threshold of $1$-step QAOA matches that of $1$-step tensor power iteration.","Additional heuristic calculations suggest that the weak recovery threshold of $p$-step QAOA matches that of $p$-step tensor power iteration when $p$ is a fixed constant.","This further implies that multi-step QAOA with tensor unfolding could achieve, but not surpass, the classical computation threshold $\\Theta(n^{(q-2)/4})$ for spiked $q$-tensors.   ","Meanwhile, we characterize the asymptotic overlap distribution for $p$-step QAOA, finding an intriguing sine-Gaussian law verified through simulations.","For some $p$ and $q$, the QAOA attains an overlap that is larger by a constant factor than the tensor power iteration overlap.","Of independent interest, our proof techniques employ the Fourier transform to handle difficult combinatorial sums, a novel approach differing from prior QAOA analyses on spin-glass models without planted structure."],"url":"http://arxiv.org/abs/2402.19456v1","category":"quant-ph"}
{"created":"2024-02-29 18:50:11","title":"Listening to the Noise: Blind Denoising with Gibbs Diffusion","abstract":"In recent years, denoising problems have become intertwined with the development of deep generative models. In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture. However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising. We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters. Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters. Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs stationary distribution caused by the diffusion model. We showcase our method for 1) blind denoising of natural images involving colored noises with unknown amplitude and spectral index, and 2) a cosmology problem, namely the analysis of cosmic microwave background data, where Bayesian inference of \"noise\" parameters means constraining models of the evolution of the Universe.","sentences":["In recent years, denoising problems have become intertwined with the development of deep generative models.","In particular, diffusion models are trained like denoisers, and the distribution they model coincide with denoising priors in the Bayesian picture.","However, denoising through diffusion-based posterior sampling requires the noise level and covariance to be known, preventing blind denoising.","We overcome this limitation by introducing Gibbs Diffusion (GDiff), a general methodology addressing posterior sampling of both the signal and the noise parameters.","Assuming arbitrary parametric Gaussian noise, we develop a Gibbs algorithm that alternates sampling steps from a conditional diffusion model trained to map the signal prior to the family of noise distributions, and a Monte Carlo sampler to infer the noise parameters.","Our theoretical analysis highlights potential pitfalls, guides diagnostic usage, and quantifies errors in the Gibbs stationary distribution caused by the diffusion model.","We showcase our method for 1) blind denoising of natural images involving colored noises with unknown amplitude and spectral index, and 2) a cosmology problem, namely the analysis of cosmic microwave background data, where Bayesian inference of \"noise\" parameters means constraining models of the evolution of the Universe."],"url":"http://arxiv.org/abs/2402.19455v1","category":"stat.ML"}
{"created":"2024-02-29 18:48:18","title":"Functional Benchmarks for Robust Evaluation of Reasoning Performance, and the Reasoning Gap","abstract":"We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks. Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant. We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow. When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies. We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies. Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building \"gap 0\" models. Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/.","sentences":["We propose a framework for robust evaluation of reasoning capabilities of language models, using functional variants of benchmarks.","Models that solve a reasoning test should exhibit no difference in performance over the static version of a problem compared to a snapshot of the functional variant.","We have rewritten the relevant fragment of the MATH benchmark into its functional variant MATH(), with functionalization of other benchmarks to follow.","When evaluating current state-of-the-art models over snapshots of MATH(), we find a reasoning gap -- the percentage difference between the static and functional accuracies.","We find reasoning gaps from 58.35% to 80.31% among the state-of-the-art closed and open weights models that perform well on static benchmarks, with the caveat that the gaps are likely to be smaller with more sophisticated prompting strategies.","Here we show that models which anecdotally have good reasoning performance over real-world tasks, have quantifiable lower gaps, motivating the open problem of building \"gap 0\" models.","Code for evaluation and new evaluation datasets, three MATH() snapshots, are publicly available at https://github.com/consequentai/fneval/."],"url":"http://arxiv.org/abs/2402.19450v1","category":"cs.AI"}
{"created":"2024-02-29 18:47:47","title":"The Structure of Quantum Questions","abstract":"In classical physics, a single measurement can in principle reveal the state of a system. However, quantum theory permits numerous non-equivalent measurements on a physical system, each providing only limited information about the state. This set of various measurements on a quantum system indicates a rich internal structure. We illuminate this structure for both individual and composite systems by conceptualizing measurements as questions with a finite number of outcomes. We create a mathematical question structure to explore the underlying properties, employing the concept of information as a key tool representing our knowledge gained from asking these questions. We subsequently propose informational assumptions based on properties observed from measurements on qubits, generalizing these to higher dimensional systems.   Our informational assumptions shape the correlations between subsystems, which are symbolized as classical logical gates. Interestingly, systems with prime number dimensions exhibit unique property: the logical gate can be expressed simply as a linear equation under modular arithmetic. We also identify structures in quantum theory that correspond to those in the structure of quantum questions. For instance, the questions determining the system correspond to generalized Pauli matrices, and the logical gate connecting questions in subsystems is directly related to the tensor product combining operators. Based on these correspondences, we present two equivalent scenarios regarding the evolution of systems and the change of information within both quantum questions and quantum mechanics.","sentences":["In classical physics, a single measurement can in principle reveal the state of a system.","However, quantum theory permits numerous non-equivalent measurements on a physical system, each providing only limited information about the state.","This set of various measurements on a quantum system indicates a rich internal structure.","We illuminate this structure for both individual and composite systems by conceptualizing measurements as questions with a finite number of outcomes.","We create a mathematical question structure to explore the underlying properties, employing the concept of information as a key tool representing our knowledge gained from asking these questions.","We subsequently propose informational assumptions based on properties observed from measurements on qubits, generalizing these to higher dimensional systems.   ","Our informational assumptions shape the correlations between subsystems, which are symbolized as classical logical gates.","Interestingly, systems with prime number dimensions exhibit unique property: the logical gate can be expressed simply as a linear equation under modular arithmetic.","We also identify structures in quantum theory that correspond to those in the structure of quantum questions.","For instance, the questions determining the system correspond to generalized Pauli matrices, and the logical gate connecting questions in subsystems is directly related to the tensor product combining operators.","Based on these correspondences, we present two equivalent scenarios regarding the evolution of systems and the change of information within both quantum questions and quantum mechanics."],"url":"http://arxiv.org/abs/2402.19448v1","category":"quant-ph"}
{"created":"2024-02-29 18:45:56","title":"ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL","abstract":"A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support). Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards. By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks. This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs? In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively. To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn. Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods. Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on).","sentences":["A broad use case of large language models (LLMs) is in goal-directed decision-making tasks (or \"agent\" tasks), where an LLM needs to not just generate completions for a given prompt, but rather make intelligent decisions over a multi-turn interaction to accomplish a task (e.g., when interacting with the web, using tools, or providing customer support).","Reinforcement learning (RL) provides a general paradigm to address such agent tasks, but current RL methods for LLMs largely focus on optimizing single-turn rewards.","By construction, most single-turn RL methods cannot endow LLMs with the ability to intelligently seek information over multiple turns, perform credit assignment, or reason about their past actions -- all of which are critical in agent tasks.","This raises the question: how can we design effective and efficient multi-turn RL algorithms for LLMs?","In this paper, we develop a framework for building multi-turn RL algorithms for fine-tuning LLMs, that preserves the flexibility of existing single-turn RL methods for LLMs (e.g., proximal policy optimization), while accommodating multiple turns, long horizons, and delayed rewards effectively.","To do this, our framework adopts a hierarchical RL approach and runs two RL algorithms in parallel: a high-level off-policy value-based RL algorithm to aggregate reward over utterances, and a low-level RL algorithm that utilizes this high-level value function to train a token policy within each utterance or turn.","Our hierarchical framework, Actor-Critic Framework with a Hierarchical Structure (ArCHer), can also give rise to other RL methods.","Empirically, we find that ArCHer significantly improves efficiency and performance on agent tasks, attaining a sample efficiency of about 100x over existing methods, while also improving with larger model capacity (upto the 7 billion scale that we tested on)."],"url":"http://arxiv.org/abs/2402.19446v1","category":"cs.LG"}
{"created":"2024-02-29 18:44:10","title":"The \"spread\" of Thompson's group $F$","abstract":"Recall that a group $G$ is said to be $\\frac{3}{2}$-generated if every non-trivial element $g\\in G$ has a co-generator in $G$ (i.e., an element which together with $g$ generates $G$). Thompson's group $V$ was proved to be $\\frac{3}{2}$-generated by Donoven and Harper in 2019. It was the first example of an infinite finitely presented non-cyclic $\\frac{3}{2}$-generated group. In 2022, Bleak, Harper and Skipper proved that Thompson's group $T$ is also $\\frac{3}{2}$-generated. Since the abelianization of Thompson's group $F$ is $\\mathbb{Z}$, it cannot be $\\frac{3}{2}$-generated. However, we recently proved that Thompson's group $F$ is \"almost\" $\\frac{3}{2}$-generated in the sense that every element of $F$ whose image in the abelianization forms part of a generating pair of $\\mathbb{Z}^2$ is part of a generating pair of $F$.   A natural generalization of $\\frac{3}{2}$-generation is the notion of spread. Recall that the spread of a group $G$ is the supremum over all integers $k$ such that every $k$ non-trivial elements of $G$ have a common co-generator in $G$. The uniform spread of a group $G$ is the supremum over all integers $k$ for which there exists a conjugacy class $C\\subseteq G$ such that every $k$ non-trivial elements of $G$ have a common co-generator which belongs to $C$. In this paper we study modified versions of these notions for Thompson's group $F$.","sentences":["Recall that a group $G$ is said to be $\\frac{3}{2}$-generated if every non-trivial element $g\\in G$ has a co-generator in $G$ (i.e., an element which together with $g$ generates $G$).","Thompson's group $V$ was proved to be $\\frac{3}{2}$-generated by Donoven and Harper in 2019.","It was the first example of an infinite finitely presented non-cyclic $\\frac{3}{2}$-generated group.","In 2022, Bleak, Harper and Skipper proved that Thompson's group $T$ is also $\\frac{3}{2}$-generated.","Since the abelianization of Thompson's group $F$ is $\\mathbb{Z}$, it cannot be $\\frac{3}{2}$-generated.","However, we recently proved that Thompson's group $F$ is \"almost\" $\\frac{3}{2}$-generated in the sense that every element of $F$ whose image in the abelianization forms part of a generating pair of $\\mathbb{Z}^2$ is part of a generating pair of $F$.   A natural generalization of $\\frac{3}{2}$-generation is the notion of spread.","Recall that the spread of a group $G$ is the supremum over all integers $k$ such that every $k$ non-trivial elements of $G$ have a common co-generator in $G$. The uniform spread of a group $G$ is the supremum over all integers $k$ for which there exists a conjugacy class $C\\subseteq G$ such that every $k$ non-trivial elements of $G$ have a common co-generator which belongs to $C$.","In this paper we study modified versions of these notions for Thompson's group $F$."],"url":"http://arxiv.org/abs/2402.19444v1","category":"math.GR"}
{"created":"2024-02-29 18:43:53","title":"Probing the Information Encoded in Neural-based Acoustic Models of Automatic Speech Recognition Systems","abstract":"Deep learning architectures have made significant progress in terms of performance in many research areas. The automatic speech recognition (ASR) field has thus benefited from these scientific and technological advances, particularly for acoustic modeling, now integrating deep neural network architectures. However, these performance gains have translated into increased complexity regarding the information learned and conveyed through these black-box architectures. Following many researches in neural networks interpretability, we propose in this article a protocol that aims to determine which and where information is located in an ASR acoustic model (AM). To do so, we propose to evaluate AM performance on a determined set of tasks using intermediate representations (here, at different layer levels). Regarding the performance variation and targeted tasks, we can emit hypothesis about which information is enhanced or perturbed at different architecture steps. Experiments are performed on both speaker verification, acoustic environment classification, gender classification, tempo-distortion detection systems and speech sentiment/emotion identification. Analysis showed that neural-based AMs hold heterogeneous information that seems surprisingly uncorrelated with phoneme recognition, such as emotion, sentiment or speaker identity. The low-level hidden layers globally appears useful for the structuring of information while the upper ones would tend to delete useless information for phoneme recognition.","sentences":["Deep learning architectures have made significant progress in terms of performance in many research areas.","The automatic speech recognition (ASR) field has thus benefited from these scientific and technological advances, particularly for acoustic modeling, now integrating deep neural network architectures.","However, these performance gains have translated into increased complexity regarding the information learned and conveyed through these black-box architectures.","Following many researches in neural networks interpretability, we propose in this article a protocol that aims to determine which and where information is located in an ASR acoustic model (AM).","To do so, we propose to evaluate AM performance on a determined set of tasks using intermediate representations (here, at different layer levels).","Regarding the performance variation and targeted tasks, we can emit hypothesis about which information is enhanced or perturbed at different architecture steps.","Experiments are performed on both speaker verification, acoustic environment classification, gender classification, tempo-distortion detection systems and speech sentiment/emotion identification.","Analysis showed that neural-based AMs hold heterogeneous information that seems surprisingly uncorrelated with phoneme recognition, such as emotion, sentiment or speaker identity.","The low-level hidden layers globally appears useful for the structuring of information while the upper ones would tend to delete useless information for phoneme recognition."],"url":"http://arxiv.org/abs/2402.19443v1","category":"cs.SD"}
{"created":"2024-02-29 18:43:52","title":"Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality","abstract":"We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression. We establish the global convergence of gradient flow under suitable choices of initialization. In addition, we prove that an interesting \"task allocation\" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model. Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit. Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor. Our analysis also delineates a strict separation in terms of the prediction accuracy of ICL between single-head and multi-head attention models. The key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation. To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model.","sentences":["We study the dynamics of gradient flow for training a multi-head softmax attention model for in-context learning of multi-task linear regression.","We establish the global convergence of gradient flow under suitable choices of initialization.","In addition, we prove that an interesting \"task allocation\" phenomenon emerges during the gradient flow dynamics, where each attention head focuses on solving a single task of the multi-task model.","Specifically, we prove that the gradient flow dynamics can be split into three phases -- a warm-up phase where the loss decreases rather slowly and the attention heads gradually build up their inclination towards individual tasks, an emergence phase where each head selects a single task and the loss rapidly decreases, and a convergence phase where the attention parameters converge to a limit.","Furthermore, we prove the optimality of gradient flow in the sense that the limiting model learned by gradient flow is on par with the best possible multi-head softmax attention model up to a constant factor.","Our analysis also delineates a strict separation in terms of the prediction accuracy of ICL between single-head and multi-head attention models.","The key technique for our convergence analysis is to map the gradient flow dynamics in the parameter space to a set of ordinary differential equations in the spectral domain, where the relative magnitudes of the semi-singular values of the attention weights determines task allocation.","To our best knowledge, our work provides the first convergence result for the multi-head softmax attention model."],"url":"http://arxiv.org/abs/2402.19442v1","category":"cs.LG"}
{"created":"2024-02-29 18:38:20","title":"Differentially Private Worst-group Risk Minimization","abstract":"We initiate a systematic study of worst-group risk minimization under $(\\epsilon, \\delta)$-differential privacy (DP). The goal is to privately find a model that approximately minimizes the maximal risk across $p$ sub-populations (groups) with different distributions, where each group distribution is accessed via a sample oracle. We first present a new algorithm that achieves excess worst-group population risk of $\\tilde{O}(\\frac{p\\sqrt{d}}{K\\epsilon} + \\sqrt{\\frac{p}{K}})$, where $K$ is the total number of samples drawn from all groups and $d$ is the problem dimension. Our rate is nearly optimal when each distribution is observed via a fixed-size dataset of size $K/p$. Our result is based on a new stability-based analysis for the generalization error. In particular, we show that $\\Delta$-uniform argument stability implies $\\tilde{O}(\\Delta + \\frac{1}{\\sqrt{n}})$ generalization error w.r.t. the worst-group risk, where $n$ is the number of samples drawn from each sample oracle. Next, we propose an algorithmic framework for worst-group population risk minimization using any DP online convex optimization algorithm as a subroutine. Hence, we give another excess risk bound of $\\tilde{O}\\left( \\sqrt{\\frac{d^{1/2}}{\\epsilon K}} +\\sqrt{\\frac{p}{K\\epsilon^2}} \\right)$. Assuming the typical setting of $\\epsilon=\\Theta(1)$, this bound is more favorable than our first bound in a certain range of $p$ as a function of $K$ and $d$. Finally, we study differentially private worst-group empirical risk minimization in the offline setting, where each group distribution is observed by a fixed-size dataset. We present a new algorithm with nearly optimal excess risk of $\\tilde{O}(\\frac{p\\sqrt{d}}{K\\epsilon})$.","sentences":["We initiate a systematic study of worst-group risk minimization under $(\\epsilon, \\delta)$-differential privacy (DP).","The goal is to privately find a model that approximately minimizes the maximal risk across $p$ sub-populations (groups) with different distributions, where each group distribution is accessed via a sample oracle.","We first present a new algorithm that achieves excess worst-group population risk of $\\tilde{O}(\\frac{p\\sqrt{d}}{K\\epsilon} + \\sqrt{\\frac{p}{K}})$, where $K$ is the total number of samples drawn from all groups and $d$ is the problem dimension.","Our rate is nearly optimal when each distribution is observed via a fixed-size dataset of size $K/p$. Our result is based on a new stability-based analysis for the generalization error.","In particular, we show that $\\Delta$-uniform argument stability implies $\\tilde{O}(\\Delta + \\frac{1}{\\sqrt{n}})$ generalization error w.r.t.","the worst-group risk, where $n$ is the number of samples drawn from each sample oracle.","Next, we propose an algorithmic framework for worst-group population risk minimization using any DP online convex optimization algorithm as a subroutine.","Hence, we give another excess risk bound of $\\tilde{O}\\left( \\sqrt{\\frac{d^{1/2}}{\\epsilon K}} +\\sqrt{\\frac{p}{K\\epsilon^2}} \\right)$. Assuming the typical setting of $\\epsilon=\\Theta(1)$, this bound is more favorable than our first bound in a certain range of $p$ as a function of $K$ and $d$. Finally, we study differentially private worst-group empirical risk minimization in the offline setting, where each group distribution is observed by a fixed-size dataset.","We present a new algorithm with nearly optimal excess risk of $\\tilde{O}(\\frac{p\\sqrt{d}}{K\\epsilon})$."],"url":"http://arxiv.org/abs/2402.19437v1","category":"cs.LG"}
{"created":"2024-02-29 18:35:12","title":"Digital Twin Aided Massive MIMO: CSI Compression and Feedback","abstract":"Deep learning (DL) approaches have demonstrated high performance in compressing and reconstructing the channel state information (CSI) and reducing the CSI feedback overhead in massive MIMO systems. One key challenge, however, with the DL approaches is the demand for extensive training data. Collecting this real-world CSI data incurs significant overhead that hinders the DL approaches from scaling to a large number of communication sites. To address this challenge, we propose a novel direction that utilizes site-specific \\textit{digital twins} to aid the training of DL models. The proposed digital twin approach generates site-specific synthetic CSI data from the EM 3D model and ray tracing, which can then be used to train the DL model without real-world data collection. To further improve the performance, we adopt online data selection to refine the DL model training with a small real-world CSI dataset. Results show that a DL model trained solely on the digital twin data can achieve high performance when tested in a real-world deployment. Further, leveraging domain adaptation techniques, the proposed approach requires orders of magnitude less real-world data to approach the same performance of the model trained completely on a real-world CSI dataset.","sentences":["Deep learning (DL) approaches have demonstrated high performance in compressing and reconstructing the channel state information (CSI) and reducing the CSI feedback overhead in massive MIMO systems.","One key challenge, however, with the DL approaches is the demand for extensive training data.","Collecting this real-world CSI data incurs significant overhead that hinders the DL approaches from scaling to a large number of communication sites.","To address this challenge, we propose a novel direction that utilizes site-specific \\textit{digital twins} to aid the training of DL models.","The proposed digital twin approach generates site-specific synthetic CSI data from the EM 3D model and ray tracing, which can then be used to train the DL model without real-world data collection.","To further improve the performance, we adopt online data selection to refine the DL model training with a small real-world CSI dataset.","Results show that a DL model trained solely on the digital twin data can achieve high performance when tested in a real-world deployment.","Further, leveraging domain adaptation techniques, the proposed approach requires orders of magnitude less real-world data to approach the same performance of the model trained completely on a real-world CSI dataset."],"url":"http://arxiv.org/abs/2402.19434v1","category":"cs.IT"}
{"created":"2024-02-29 18:32:44","title":"Magnon spectrum of altermagnets: Time-dependent matrix product states vs. linearized Holstein-Primakoff calculations unravelling spontaneous magnon decay","abstract":"The energy-momentum dispersion of magnons, viewed as noninteracting and infinitely long-lived quasiparticles describing collective low-energy excitations of magnetic materials, is often presented as sharp bands obtained from the effective quantum spin Hamiltonian, after being simplified via linearized Holstein-Primakoff (HP) transformations. However, magnons are prone to many-body interactions with other quasiparticles which can lead to their spontaneous decay. The magnon-magnon interactions could affect newly classified altermagnets. On the other hand, sharp bands of noninteracting chiral magnons in RuO2, as the canonical example of altermagnets, have been very recently predicted. Here, we employ nonperturbative numerically (quasi)exact quantum many-body calculations, via time-dependent matrix product states (TDMPS), to obtain magnon spectral function of RuO2. These calculations produce a broadened magnon dispersion, which overlaps with linearized HP theory sharp bands only at edges/center of the Brillouin zone. Substantially deviating otherwise. Artificially making exchange interaction within two sublattices of RuO2 closer in value forces these two spectra to overlap, thereby explaining the origin of the failure of linearized HP theory. Such features translate into the difference between their respective density of states, which we also compute and which could be tested by Raman scattering experiments. Finally, we employ popular Landau-Lifshitz-Gilbert (LLG) equation-based classical atomistic spin dynamics (ASD) simulations to obtain dynamical structure factor and extract magnon spectrum from it at finite temperature. Despite including magnon-magnon interactions via nonlinearity of LLG equation, ASD simulations cannot fully match the TDMPS-computed magnon spectrum due to nonclassical effects harbored by altermagnets.","sentences":["The energy-momentum dispersion of magnons, viewed as noninteracting and infinitely long-lived quasiparticles describing collective low-energy excitations of magnetic materials, is often presented as sharp bands obtained from the effective quantum spin Hamiltonian, after being simplified via linearized Holstein-Primakoff (HP) transformations.","However, magnons are prone to many-body interactions with other quasiparticles which can lead to their spontaneous decay.","The magnon-magnon interactions could affect newly classified altermagnets.","On the other hand, sharp bands of noninteracting chiral magnons in RuO2, as the canonical example of altermagnets, have been very recently predicted.","Here, we employ nonperturbative numerically (quasi)exact quantum many-body calculations, via time-dependent matrix product states (TDMPS), to obtain magnon spectral function of RuO2.","These calculations produce a broadened magnon dispersion, which overlaps with linearized HP theory sharp bands only at edges/center of the Brillouin zone.","Substantially deviating otherwise.","Artificially making exchange interaction within two sublattices of RuO2 closer in value forces these two spectra to overlap, thereby explaining the origin of the failure of linearized HP theory.","Such features translate into the difference between their respective density of states, which we also compute and which could be tested by Raman scattering experiments.","Finally, we employ popular Landau-Lifshitz-Gilbert (LLG) equation-based classical atomistic spin dynamics (ASD) simulations to obtain dynamical structure factor and extract magnon spectrum from it at finite temperature.","Despite including magnon-magnon interactions via nonlinearity of LLG equation, ASD simulations cannot fully match the TDMPS-computed magnon spectrum due to nonclassical effects harbored by altermagnets."],"url":"http://arxiv.org/abs/2402.19433v1","category":"cond-mat.str-el"}
{"created":"2024-02-29 18:27:27","title":"Compositional API Recommendation for Library-Oriented Code Generation","abstract":"Large language models (LLMs) have achieved exceptional performance in code generation. However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs. Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs. However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs. This granularity inconsistency makes API recommendation a challenging task. To address this, we propose CAPIR (Compositional API Recommendation), which adopts a \"divide-and-conquer\" strategy to recommend APIs for coarse-grained requirements. Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks. Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask. Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation. To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation). Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines. Specifically, on RAPID's Torchdata-AR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%. On LOCG's Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%.","sentences":["Large language models (LLMs) have achieved exceptional performance in code generation.","However, the performance remains unsatisfactory in generating library-oriented code, especially for the libraries not present in the training data of LLMs.","Previous work utilizes API recommendation technology to help LLMs use libraries: it retrieves APIs related to the user requirements, then leverages them as context to prompt LLMs.","However, developmental requirements can be coarse-grained, requiring a combination of multiple fine-grained APIs.","This granularity inconsistency makes API recommendation a challenging task.","To address this, we propose CAPIR (Compositional API Recommendation), which adopts a \"divide-and-conquer\" strategy to recommend APIs for coarse-grained requirements.","Specifically, CAPIR employs an LLM-based Decomposer to break down a coarse-grained task description into several detailed subtasks.","Then, CAPIR applies an embedding-based Retriever to identify relevant APIs corresponding to each subtask.","Moreover, CAPIR leverages an LLM-based Reranker to filter out redundant APIs and provides the final recommendation.","To facilitate the evaluation of API recommendation methods on coarse-grained requirements, we present two challenging benchmarks, RAPID (Recommend APIs based on Documentation) and LOCG (Library-Oriented Code Generation).","Experimental results on these benchmarks, demonstrate the effectiveness of CAPIR in comparison to existing baselines.","Specifically, on RAPID's Torchdata-AR dataset, compared to the state-of-the-art API recommendation approach, CAPIR improves recall@5 from 18.7% to 43.2% and precision@5 from 15.5% to 37.1%.","On LOCG's Torchdata-Code dataset, compared to code generation without API recommendation, CAPIR improves pass@100 from 16.0% to 28.0%."],"url":"http://arxiv.org/abs/2402.19431v1","category":"cs.SE"}
{"created":"2024-02-29 18:26:13","title":"Hamiltonian Engineering of collective XYZ spin models in an optical cavity: From one-axis twisting to two-axis counter twisting models","abstract":"Quantum simulation using synthetic quantum systems offers unique opportunities to explore open questions in many-body physics and a path for the generation of useful entangled states. Nevertheless, so far many quantum simulators have been fundamentally limited in the models they can mimic. Here, we are able to realize an all-to-all interaction with arbitrary quadratic Hamiltonian or effectively an infinite range tunable Heisenberg XYZ model. This is accomplished by engineering cavity-mediated four-photon interactions between 700 rubidium atoms in which we harness a pair of momentum states as the effective pseudo spin or qubit degree of freedom. Using this capability we realize for the first time the so-called two-axis counter-twisting model, an iconic XYZ collective spin model that can generate spin-squeezed states that saturate the Heisenberg limit bound. The versatility of our platform to include more than two relevant momentum states, combined with the flexibility of the simulated Hamiltonians by adding cavity tones opens rich opportunities for quantum simulation and quantum sensing in matter-wave interferometers and other quantum sensors such as optical clocks and magnetometers.","sentences":["Quantum simulation using synthetic quantum systems offers unique opportunities to explore open questions in many-body physics and a path for the generation of useful entangled states.","Nevertheless, so far many quantum simulators have been fundamentally limited in the models they can mimic.","Here, we are able to realize an all-to-all interaction with arbitrary quadratic Hamiltonian or effectively an infinite range tunable Heisenberg XYZ model.","This is accomplished by engineering cavity-mediated four-photon interactions between 700 rubidium atoms in which we harness a pair of momentum states as the effective pseudo spin or qubit degree of freedom.","Using this capability we realize for the first time the so-called two-axis counter-twisting model, an iconic XYZ collective spin model that can generate spin-squeezed states that saturate the Heisenberg limit bound.","The versatility of our platform to include more than two relevant momentum states, combined with the flexibility of the simulated Hamiltonians by adding cavity tones opens rich opportunities for quantum simulation and quantum sensing in matter-wave interferometers and other quantum sensors such as optical clocks and magnetometers."],"url":"http://arxiv.org/abs/2402.19429v1","category":"quant-ph"}
{"created":"2024-02-29 18:26:13","title":"Effective interactions of the open bosonic string via field theory","abstract":"We describe a method to extract an effective Lagrangian description for open bosonic strings, at zero transcendentality. The method relies on a particular formulation of its scattering amplitudes derived from color-kinematics duality. More precisely, starting from a $(DF)^2 + \\text{YM}$ quantum field theory, we integrate out all the massive degrees of freedom to generate an expansion in the inverse string tension $\\alpha^\\prime$. We explicitly compute the Lagrangian terms through $\\mathcal{O}(\\alpha^{\\prime 4})$, and target the sector of operators proportional to $F^4$ to all orders in $\\alpha^\\prime$.","sentences":["We describe a method to extract an effective Lagrangian description for open bosonic strings, at zero transcendentality.","The method relies on a particular formulation of its scattering amplitudes derived from color-kinematics duality.","More precisely, starting from a $(DF)^2 + \\text{YM}$ quantum field theory, we integrate out all the massive degrees of freedom to generate an expansion in the inverse string tension $\\alpha^\\prime$. We explicitly compute the Lagrangian terms through $\\mathcal{O}(\\alpha^{\\prime 4})$, and target the sector of operators proportional to $F^4$ to all orders in $\\alpha^\\prime$."],"url":"http://arxiv.org/abs/2402.19430v1","category":"hep-th"}
{"created":"2024-02-29 18:22:12","title":"Leveraging AI Predicted and Expert Revised Annotations in Interactive Segmentation: Continual Tuning or Full Training?","abstract":"Interactive segmentation, an integration of AI algorithms and human expertise, premises to improve the accuracy and efficiency of curating large-scale, detailed-annotated datasets in healthcare. Human experts revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from these revised annotations. This interactive process continues to enhance the quality of annotations until no major revision is needed from experts. The key challenge is how to leverage AI predicted and expert revised annotations to iteratively improve the AI. Two problems arise: (1) The risk of catastrophic forgetting--the AI tends to forget the previously learned classes if it is only retrained using the expert revised classes. (2) Computational inefficiency when retraining the AI using both AI predicted and expert revised annotations; moreover, given the dominant AI predicted annotations in the dataset, the contribution of newly revised annotations--often account for a very small fraction--to the AI training remains marginal. This paper proposes Continual Tuning to address the problems from two perspectives: network design and data reuse. Firstly, we design a shared network for all classes followed by class-specific networks dedicated to individual classes. To mitigate forgetting, we freeze the shared network for previously learned classes and only update the class-specific network for revised classes. Secondly, we reuse a small fraction of data with previous annotations to avoid over-computing. The selection of such data relies on the importance estimate of each data. The importance score is computed by combining the uncertainty and consistency of AI predictions. Our experiments demonstrate that Continual Tuning achieves a speed 16x greater than repeatedly training AI from scratch without compromising the performance.","sentences":["Interactive segmentation, an integration of AI algorithms and human expertise, premises to improve the accuracy and efficiency of curating large-scale, detailed-annotated datasets in healthcare.","Human experts revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from these revised annotations.","This interactive process continues to enhance the quality of annotations until no major revision is needed from experts.","The key challenge is how to leverage AI predicted and expert revised annotations to iteratively improve the AI.","Two problems arise: (1) The risk of catastrophic forgetting--the AI tends to forget the previously learned classes if it is only retrained using the expert revised classes.","(2) Computational inefficiency when retraining the AI using both AI predicted and expert revised annotations; moreover, given the dominant AI predicted annotations in the dataset, the contribution of newly revised annotations--often account for a very small fraction--to the AI training remains marginal.","This paper proposes Continual Tuning to address the problems from two perspectives: network design and data reuse.","Firstly, we design a shared network for all classes followed by class-specific networks dedicated to individual classes.","To mitigate forgetting, we freeze the shared network for previously learned classes and only update the class-specific network for revised classes.","Secondly, we reuse a small fraction of data with previous annotations to avoid over-computing.","The selection of such data relies on the importance estimate of each data.","The importance score is computed by combining the uncertainty and consistency of AI predictions.","Our experiments demonstrate that Continual Tuning achieves a speed 16x greater than repeatedly training AI from scratch without compromising the performance."],"url":"http://arxiv.org/abs/2402.19423v1","category":"cs.CV"}
{"created":"2024-02-29 18:21:54","title":"PEM: Prototype-based Efficient MaskFormer for Image Segmentation","abstract":"Recent transformer-based architectures have shown impressive results in the field of image segmentation. Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework. To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices. To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks. PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance. In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high semantic content in an efficient way, thanks to the combination of deformable convolutions and context-based self-modulation. We benchmark the proposed PEM architecture on two tasks, semantic and panoptic segmentation, evaluated on two different datasets, Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset, outperforming task-specific architectures while being comparable and even better than computationally-expensive baselines.","sentences":["Recent transformer-based architectures have shown impressive results in the field of image segmentation.","Thanks to their flexibility, they obtain outstanding performance in multiple segmentation tasks, such as semantic and panoptic, under a single unified framework.","To achieve such impressive performance, these architectures employ intensive operations and require substantial computational resources, which are often not available, especially on edge devices.","To fill this gap, we propose Prototype-based Efficient MaskFormer (PEM), an efficient transformer-based architecture that can operate in multiple segmentation tasks.","PEM proposes a novel prototype-based cross-attention which leverages the redundancy of visual features to restrict the computation and improve the efficiency without harming the performance.","In addition, PEM introduces an efficient multi-scale feature pyramid network, capable of extracting features that have high semantic content in an efficient way, thanks to the combination of deformable convolutions and context-based self-modulation.","We benchmark the proposed PEM architecture on two tasks, semantic and panoptic segmentation, evaluated on two different datasets, Cityscapes and ADE20K. PEM demonstrates outstanding performance on every task and dataset, outperforming task-specific architectures while being comparable and even better than computationally-expensive baselines."],"url":"http://arxiv.org/abs/2402.19422v1","category":"cs.CV"}
{"created":"2024-02-29 18:20:37","title":"Crafting Knowledge: Exploring the Creative Mechanisms of Chat-Based Search Engines","abstract":"In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers. The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem. They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity. Nonetheless, the intricate nature of LLMs renders their \"cognitive\" processes opaque, challenging even their designers' understanding. This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses. To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine. Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM. Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat. This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat's developers. Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines.","sentences":["In the domain of digital information dissemination, search engines act as pivotal conduits linking information seekers with providers.","The advent of chat-based search engines utilizing Large Language Models (LLMs) and Retrieval Augmented Generation (RAG), exemplified by Bing Chat, marks an evolutionary leap in the search ecosystem.","They demonstrate metacognitive abilities in interpreting web information and crafting responses with human-like understanding and creativity.","Nonetheless, the intricate nature of LLMs renders their \"cognitive\" processes opaque, challenging even their designers' understanding.","This research aims to dissect the mechanisms through which an LLM-powered chat-based search engine, specifically Bing Chat, selects information sources for its responses.","To this end, an extensive dataset has been compiled through engagements with New Bing, documenting the websites it cites alongside those listed by the conventional search engine.","Employing natural language processing (NLP) techniques, the research reveals that Bing Chat exhibits a preference for content that is not only readable and formally structured, but also demonstrates lower perplexity levels, indicating a unique inclination towards text that is predictable by the underlying LLM.","Further enriching our analysis, we procure an additional dataset through interactions with the GPT-4 based knowledge retrieval API, unveiling a congruent text preference between the RAG API and Bing Chat.","This consensus suggests that these text preferences intrinsically emerge from the underlying language models, rather than being explicitly crafted by Bing Chat's developers.","Moreover, our investigation documents a greater similarity among websites cited by RAG technologies compared to those ranked highest by conventional search engines."],"url":"http://arxiv.org/abs/2402.19421v1","category":"cs.IR"}
{"created":"2024-02-29 18:16:13","title":"Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning","abstract":"Iterative combinatorial auctions are widely used in high stakes settings such as spectrum auctions. Such auctions can be hard to understand analytically, making it difficult for bidders to determine how to behave and for designers to optimize auction rules to ensure desirable outcomes such as high revenue or welfare. In this paper, we investigate whether multi-agent reinforcement learning (MARL) algorithms can be used to understand iterative combinatorial auctions, given that these algorithms have recently shown empirical success in several other domains. We find that MARL can indeed benefit auction analysis, but that deploying it effectively is nontrivial. We begin by describing modelling decisions that keep the resulting game tractable without sacrificing important features such as imperfect information or asymmetry between bidders. We also discuss how to navigate pitfalls of various MARL algorithms, how to overcome challenges in verifying convergence, and how to generate and interpret multiple equilibria. We illustrate the promise of our resulting approach by using it to evaluate a specific rule change to a clock auction, finding substantially different auction outcomes due to complex changes in bidders' behavior.","sentences":["Iterative combinatorial auctions are widely used in high stakes settings such as spectrum auctions.","Such auctions can be hard to understand analytically, making it difficult for bidders to determine how to behave and for designers to optimize auction rules to ensure desirable outcomes such as high revenue or welfare.","In this paper, we investigate whether multi-agent reinforcement learning (MARL) algorithms can be used to understand iterative combinatorial auctions, given that these algorithms have recently shown empirical success in several other domains.","We find that MARL can indeed benefit auction analysis, but that deploying it effectively is nontrivial.","We begin by describing modelling decisions that keep the resulting game tractable without sacrificing important features such as imperfect information or asymmetry between bidders.","We also discuss how to navigate pitfalls of various MARL algorithms, how to overcome challenges in verifying convergence, and how to generate and interpret multiple equilibria.","We illustrate the promise of our resulting approach by using it to evaluate a specific rule change to a clock auction, finding substantially different auction outcomes due to complex changes in bidders' behavior."],"url":"http://arxiv.org/abs/2402.19420v1","category":"cs.GT"}
{"created":"2024-02-29 18:16:00","title":"Ab initio elasticity at finite temperature and stress in ferroelectrics","abstract":"Computing the temperature and stress dependence of the full elastic constant tensor from first-principles in non-cubic materials remains a challenging problem. Here we circumvent the aforementioned challenge via the generalized quasiharmonic approximation in conjunction with the irreducible derivative approach for computing strain dependent phonons using finite difference, explicitly including dipole-quadrupole contributions. We showcase this approach in ferroelectric PbTiO$_3$ using density functional theory, computing all independent elastic constants and piezoelectric strain coefficients at finite temperature and stress. There is good agreement between the quasiharmonic approximation and the experimental lattice parameters close to 0 K. However, the quasiharmonic approximation overestimates the temperature dependence of the lattice parameters and elastic constant tensor, demonstrating that a higher level of strain dependent anharmonic vibrational theory is needed.","sentences":["Computing the temperature and stress dependence of the full elastic constant tensor from first-principles in non-cubic materials remains a challenging problem.","Here we circumvent the aforementioned challenge via the generalized quasiharmonic approximation in conjunction with the irreducible derivative approach for computing strain dependent phonons using finite difference, explicitly including dipole-quadrupole contributions.","We showcase this approach in ferroelectric PbTiO$_3$ using density functional theory, computing all independent elastic constants and piezoelectric strain coefficients at finite temperature and stress.","There is good agreement between the quasiharmonic approximation and the experimental lattice parameters close to 0 K. However, the quasiharmonic approximation overestimates the temperature dependence of the lattice parameters and elastic constant tensor, demonstrating that a higher level of strain dependent anharmonic vibrational theory is needed."],"url":"http://arxiv.org/abs/2402.19419v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 18:13:31","title":"Cutting-Edge Tools for Cutting Edges","abstract":"We review different notions of cuts appearing throughout the literature on scattering amplitudes. Despite similar names, such as unitarity cuts or generalized cuts, they often represent distinct computations and distinct physics. We consolidate this knowledge, summarize how cuts are used in various computational strategies, and explain their relations to other quantities including imaginary parts, discontinuities, and monodromies. Differences and nuances are illustrated on explicit examples.","sentences":["We review different notions of cuts appearing throughout the literature on scattering amplitudes.","Despite similar names, such as unitarity cuts or generalized cuts, they often represent distinct computations and distinct physics.","We consolidate this knowledge, summarize how cuts are used in various computational strategies, and explain their relations to other quantities including imaginary parts, discontinuities, and monodromies.","Differences and nuances are illustrated on explicit examples."],"url":"http://arxiv.org/abs/2402.19415v1","category":"hep-th"}
{"created":"2024-02-29 18:09:03","title":"PaECTER: Patent-level Representation Learning using Citation-informed Transformers","abstract":"PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and patent examiners. PaECTER is available on Hugging Face.","sentences":["PaECTER is a publicly available, open-source document-level encoder specific for patents.","We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents.","PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain.","More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics.","PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents.","Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search.","Semantic similarity search is especially relevant in the context of prior art search for both inventors and patent examiners.","PaECTER is available on Hugging Face."],"url":"http://arxiv.org/abs/2402.19411v1","category":"cs.IR"}
{"created":"2024-02-29 18:07:58","title":"Genie: Smart ROS-based Caching for Connected Autonomous Robots","abstract":"Despite the promising future of autonomous robots, several key issues currently remain that can lead to compromised performance and safety. One such issue is latency, where we find that even the latest embedded platforms from NVIDIA fail to execute intelligence tasks (e.g., object detection) of autonomous vehicles in a real-time fashion. One remedy to this problem is the promising paradigm of edge computing. Through collaboration with our industry partner, we identify key prohibitive limitations of the current edge mindset: (1) servers are not distributed enough and thus, are not close enough to vehicles, (2) current proposed edge solutions do not provide substantially better performance and extra information specific to autonomous vehicles to warrant their cost to the user, and (3) the state-of-the-art solutions are not compatible with popular frameworks used in autonomous systems, particularly the Robot Operating System (ROS).   To remedy these issues, we provide Genie, an encapsulation technique that can enable transparent caching in ROS in a non-intrusive way (i.e., without modifying the source code), can build the cache in a distributed manner (in contrast to traditional central caching methods), and can construct a collective three-dimensional object map to provide substantially better latency (even on low-power edge servers) and higher quality data to all vehicles in a certain locality. We fully implement our design on state-of-the-art industry-adopted embedded and edge platforms, using the prominent autonomous driving software Autoware, and find that Genie can enhance the latency of Autoware Vision Detector by 82% on average, enable object reusability 31% of the time on average and as much as 67% for the incoming requests, and boost the confidence in its object map considerably over time.","sentences":["Despite the promising future of autonomous robots, several key issues currently remain that can lead to compromised performance and safety.","One such issue is latency, where we find that even the latest embedded platforms from NVIDIA fail to execute intelligence tasks (e.g., object detection) of autonomous vehicles in a real-time fashion.","One remedy to this problem is the promising paradigm of edge computing.","Through collaboration with our industry partner, we identify key prohibitive limitations of the current edge mindset: (1) servers are not distributed enough and thus, are not close enough to vehicles, (2) current proposed edge solutions do not provide substantially better performance and extra information specific to autonomous vehicles to warrant their cost to the user, and (3) the state-of-the-art solutions are not compatible with popular frameworks used in autonomous systems, particularly the Robot Operating System (ROS).   ","To remedy these issues, we provide Genie, an encapsulation technique that can enable transparent caching in ROS in a non-intrusive way (i.e., without modifying the source code), can build the cache in a distributed manner (in contrast to traditional central caching methods), and can construct a collective three-dimensional object map to provide substantially better latency (even on low-power edge servers) and higher quality data to all vehicles in a certain locality.","We fully implement our design on state-of-the-art industry-adopted embedded and edge platforms, using the prominent autonomous driving software Autoware, and find that Genie can enhance the latency of Autoware Vision Detector by 82% on average, enable object reusability 31% of the time on average and as much as 67% for the incoming requests, and boost the confidence in its object map considerably over time."],"url":"http://arxiv.org/abs/2402.19410v1","category":"cs.RO"}
{"created":"2024-02-29 18:06:02","title":"MENTOR: Multi-level Self-supervised Learning for Multimodal Recommendation","abstract":"With the increasing multimedia information, multimodal recommendation has received extensive attention. It utilizes multimodal information to alleviate the data sparsity problem in recommendation systems, thus improving recommendation accuracy. However, the reliance on labeled data severely limits the performance of multimodal recommendation models. Recently, self-supervised learning has been used in multimodal recommendations to mitigate the label sparsity problem. Nevertheless, the state-of-the-art methods cannot avoid the modality noise when aligning multimodal information due to the large differences in the distributions of different modalities. To this end, we propose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation (MENTOR) method to address the label sparsity problem and the modality alignment problem. Specifically, MENTOR first enhances the specific features of each modality using the graph convolutional network (GCN) and fuses the visual and textual modalities. It then enhances the item representation via the item semantic graph for all modalities, including the fused modality. Then, it introduces two multilevel self-supervised tasks: the multilevel cross-modal alignment task and the general feature enhancement task. The multilevel cross-modal alignment task aligns each modality under the guidance of the ID embedding from multiple levels while maintaining the historical interaction information. The general feature enhancement task enhances the general feature from both the graph and feature perspectives to improve the robustness of our model. Extensive experiments on three publicly available datasets demonstrate the effectiveness of our method. Our code is publicly available at https://github.com/Jinfeng-Xu/MENTOR.","sentences":["With the increasing multimedia information, multimodal recommendation has received extensive attention.","It utilizes multimodal information to alleviate the data sparsity problem in recommendation systems, thus improving recommendation accuracy.","However, the reliance on labeled data severely limits the performance of multimodal recommendation models.","Recently, self-supervised learning has been used in multimodal recommendations to mitigate the label sparsity problem.","Nevertheless, the state-of-the-art methods cannot avoid the modality noise when aligning multimodal information due to the large differences in the distributions of different modalities.","To this end, we propose a Multi-level sElf-supervised learNing for mulTimOdal Recommendation (MENTOR) method to address the label sparsity problem and the modality alignment problem.","Specifically, MENTOR first enhances the specific features of each modality using the graph convolutional network (GCN) and fuses the visual and textual modalities.","It then enhances the item representation via the item semantic graph for all modalities, including the fused modality.","Then, it introduces two multilevel self-supervised tasks: the multilevel cross-modal alignment task and the general feature enhancement task.","The multilevel cross-modal alignment task aligns each modality under the guidance of the ID embedding from multiple levels while maintaining the historical interaction information.","The general feature enhancement task enhances the general feature from both the graph and feature perspectives to improve the robustness of our model.","Extensive experiments on three publicly available datasets demonstrate the effectiveness of our method.","Our code is publicly available at https://github.com/Jinfeng-Xu/MENTOR."],"url":"http://arxiv.org/abs/2402.19407v1","category":"cs.IR"}
{"created":"2024-02-29 18:04:11","title":"On the Scaling Laws of Geographical Representation in Language Models","abstract":"Language models have long been shown to embed geographical information in their hidden representations. This line of work has recently been revisited by extending this result to Large Language Models (LLMs). In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models. We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size. Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data.","sentences":["Language models have long been shown to embed geographical information in their hidden representations.","This line of work has recently been revisited by extending this result to Large Language Models (LLMs).","In this paper, we propose to fill the gap between well-established and recent literature by observing how geographical knowledge evolves when scaling language models.","We show that geographical knowledge is observable even for tiny models, and that it scales consistently as we increase the model size.","Notably, we observe that larger language models cannot mitigate the geographical bias that is inherent to the training data."],"url":"http://arxiv.org/abs/2402.19406v1","category":"cs.CL"}
{"created":"2024-02-29 18:03:00","title":"Entity-Aware Multimodal Alignment Framework for News Image Captioning","abstract":"News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article. Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task. However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting. Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset. To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions. Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on NYTimes800k dataset.","sentences":["News image captioning task is a variant of image captioning task which requires model to generate a more informative caption with news image and the associated news article.","Multimodal Large Language models have developed rapidly in recent years and is promising in news image captioning task.","However, according to our experiments, common MLLMs are not good at generating the entities in zero-shot setting.","Their abilities to deal with the entities information are still limited after simply fine-tuned on news image captioning dataset.","To obtain a more powerful model to handle the multimodal entity information, we design two multimodal entity-aware alignment tasks and an alignment framework to align the model and generate the news image captions.","Our method achieves better results than previous state-of-the-art models in CIDEr score (72.33 -> 86.29) on GoodNews dataset and (70.83 -> 85.61) on NYTimes800k dataset."],"url":"http://arxiv.org/abs/2402.19404v1","category":"cs.CV"}
{"created":"2024-02-29 18:01:07","title":"A Scalable and Transferable Time Series Prediction Framework for Demand Forecasting","abstract":"Time series forecasting is one of the most essential and ubiquitous tasks in many business problems, including demand forecasting and logistics optimization. Traditional time series forecasting methods, however, have resulted in small models with limited expressive power because they have difficulty in scaling their model size up while maintaining high accuracy. In this paper, we propose Forecasting orchestra (Forchestra), a simple but powerful framework capable of accurately predicting future demand for a diverse range of items. We empirically demonstrate that the model size is scalable to up to 0.8 billion parameters. The proposed method not only outperforms existing forecasting models with a significant margin, but it could generalize well to unseen data points when evaluated in a zero-shot fashion on downstream datasets. Last but not least, we present extensive qualitative and quantitative studies to analyze how the proposed model outperforms baseline models and differs from conventional approaches. The original paper was presented as a full paper at ICDM 2022 and is available at: https://ieeexplore.ieee.org/document/10027662.","sentences":["Time series forecasting is one of the most essential and ubiquitous tasks in many business problems, including demand forecasting and logistics optimization.","Traditional time series forecasting methods, however, have resulted in small models with limited expressive power because they have difficulty in scaling their model size up while maintaining high accuracy.","In this paper, we propose Forecasting orchestra (Forchestra), a simple but powerful framework capable of accurately predicting future demand for a diverse range of items.","We empirically demonstrate that the model size is scalable to up to 0.8 billion parameters.","The proposed method not only outperforms existing forecasting models with a significant margin, but it could generalize well to unseen data points when evaluated in a zero-shot fashion on downstream datasets.","Last but not least, we present extensive qualitative and quantitative studies to analyze how the proposed model outperforms baseline models and differs from conventional approaches.","The original paper was presented as a full paper at ICDM 2022 and is available at: https://ieeexplore.ieee.org/document/10027662."],"url":"http://arxiv.org/abs/2402.19402v1","category":"cs.LG"}
{"created":"2024-02-29 17:58:39","title":"Refraction, the speed of light and minimal action: From Descartes to Maupertuis through many more","abstract":"In the 17th and 18th centuries, several natural philosophers studied the phenomenon of refraction and attempted to obtain the Snell law from various assumptions. Lacking experimental data, it was generally believed that light travels faster in a refracting medium than in air. In the present article, I review the contributions to the problem of light refraction by Descartes, Fermat, Huygens, Leibniz, Newton, Clairaut, and finally Maupertuis who established a principle of least action based on his own approach to the problem.","sentences":["In the 17th and 18th centuries, several natural philosophers studied the phenomenon of refraction and attempted to obtain the Snell law from various assumptions.","Lacking experimental data, it was generally believed that light travels faster in a refracting medium than in air.","In the present article, I review the contributions to the problem of light refraction by Descartes, Fermat, Huygens, Leibniz, Newton, Clairaut, and finally Maupertuis who established a principle of least action based on his own approach to the problem."],"url":"http://arxiv.org/abs/2402.19400v1","category":"physics.hist-ph"}
{"created":"2024-02-29 17:57:05","title":"An Empirical Analysis of Scam Token on Ethereum Blockchain: Counterfeit tokens on Uniswap","abstract":"This article presents an empirical investigation into the determinants of total revenue generated by counterfeit tokens on Uniswap. It offers a detailed overview of the counterfeit token fraud process, along with a systematic summary of characteristics associated with such fraudulent activities observed in Uniswap. The study primarily examines the relationship between revenue from counterfeit token scams and their defining characteristics, and analyzes the influence of market economic factors such as return on market capitalization and price return on Ethereum. Key findings include a significant increase in overall transactions of counterfeit tokens on their first day of fraud, and a rise in upfront fraud costs leading to corresponding increases in revenue. Furthermore, a negative correlation is identified between the total revenue of counterfeit tokens and the volatility of Ethereum market capitalization return, while price return volatility on Ethereum is found to have a positive impact on counterfeit token revenue, albeit requiring further investigation for a comprehensive understanding. Additionally, the number of subscribers for the real token correlates positively with the realized volume of scam tokens, indicating that a larger community following the legitimate token may inadvertently contribute to the visibility and success of counterfeit tokens. Conversely, the number of Telegram subscribers exhibits a negative impact on the realized volume of scam tokens, suggesting that a higher level of scrutiny or awareness within Telegram communities may act as a deterrent to fraudulent activities. Finally, the timing of when the scam token is introduced on the Ethereum blockchain may have a negative impact on its success. Notably, the cumulative amount scammed by only 42 counterfeit tokens amounted to almost 11214 Ether.","sentences":["This article presents an empirical investigation into the determinants of total revenue generated by counterfeit tokens on Uniswap.","It offers a detailed overview of the counterfeit token fraud process, along with a systematic summary of characteristics associated with such fraudulent activities observed in Uniswap.","The study primarily examines the relationship between revenue from counterfeit token scams and their defining characteristics, and analyzes the influence of market economic factors such as return on market capitalization and price return on Ethereum.","Key findings include a significant increase in overall transactions of counterfeit tokens on their first day of fraud, and a rise in upfront fraud costs leading to corresponding increases in revenue.","Furthermore, a negative correlation is identified between the total revenue of counterfeit tokens and the volatility of Ethereum market capitalization return, while price return volatility on Ethereum is found to have a positive impact on counterfeit token revenue, albeit requiring further investigation for a comprehensive understanding.","Additionally, the number of subscribers for the real token correlates positively with the realized volume of scam tokens, indicating that a larger community following the legitimate token may inadvertently contribute to the visibility and success of counterfeit tokens.","Conversely, the number of Telegram subscribers exhibits a negative impact on the realized volume of scam tokens, suggesting that a higher level of scrutiny or awareness within Telegram communities may act as a deterrent to fraudulent activities.","Finally, the timing of when the scam token is introduced on the Ethereum blockchain may have a negative impact on its success.","Notably, the cumulative amount scammed by only 42 counterfeit tokens amounted to almost 11214 Ether."],"url":"http://arxiv.org/abs/2402.19399v1","category":"q-fin.TR"}
{"created":"2024-02-29 17:55:26","title":"Testing gravitational waveforms in full General Relativity","abstract":"We perform a comprehensive analysis of state-of-the-art waveform models, focusing on their predictions concerning kick velocity and inferred gravitational wave memory. In our investigation we assess the accuracy of waveform models using energy-momentum balance laws, which were derived in the framework of full, non-linear General Relativity. The numerical accuracy assessment is performed for precessing as well as non-precessing scenarios for models belonging to the \\textit{EOB}, \\textit{Phenom}, and \\textit{Surrogate} families. We analyze the deviations of these models from each other and from Numerical Relativity waveforms. Our analysis reveals statistically significant deviations, which we trace back to inaccuracies in modelling subdominant modes and inherent systematic errors in the chosen models. We corroborate our findings through analytical considerations regarding the mixing of harmonic modes in the computed kick velocities and inferred memories.","sentences":["We perform a comprehensive analysis of state-of-the-art waveform models, focusing on their predictions concerning kick velocity and inferred gravitational wave memory.","In our investigation we assess the accuracy of waveform models using energy-momentum balance laws, which were derived in the framework of full, non-linear General Relativity.","The numerical accuracy assessment is performed for precessing as well as non-precessing scenarios for models belonging to the \\textit{EOB}, \\textit{Phenom}, and \\textit{Surrogate} families.","We analyze the deviations of these models from each other and from Numerical Relativity waveforms.","Our analysis reveals statistically significant deviations, which we trace back to inaccuracies in modelling subdominant modes and inherent systematic errors in the chosen models.","We corroborate our findings through analytical considerations regarding the mixing of harmonic modes in the computed kick velocities and inferred memories."],"url":"http://arxiv.org/abs/2402.19397v1","category":"gr-qc"}
{"created":"2024-02-29 17:46:12","title":"Periodic dynamics in viscous fingering","abstract":"The displacement of a viscous liquid by air in the narrow gap between two parallel plates - a Hele-Shaw channel - is an exemplar of complex pattern formation. Typically, bubbles or fingers of air propagate steadily at low values of the driving parameter. However, as the driving parameter increases, they can exhibit disordered pattern-forming dynamics. In this paper, we demonstrate experimentally that a remote perturbation of the bubble's tip can drive time-periodic bubble propagation: a fundamental building block of complex unsteady dynamics. We exploit the propensity of a group of bubbles to self-organise into a fixed spatial arrangement in a Hele-Shaw channel with a centralised depth-reduction in order to apply a sustained perturbation to a bubble's shape as it propagates. We find that the bubble with a perturbed shape begins to oscillate after the system undergoes a supercritical Hopf bifurcation upon variation of the tip perturbation and dimensionless flow rate. The oscillation cycle features the splitting of the bubble's tip and advection of the resulting finger-like protrusion along the bubble's length until it is absorbed by the bubble's advancing rear. The restoral of the bubble's tip follows naturally because the system is driven by a fixed flow rate and the perturbed bubble is attracted to the weakly unstable, steadily propagating state that is set by the ratio of imposed viscous and capillary forces. Our results suggest a generic mechanism for time-periodic dynamics of propagating curved fronts subject to a steady shape perturbation.","sentences":["The displacement of a viscous liquid by air in the narrow gap between two parallel plates - a Hele-Shaw channel - is an exemplar of complex pattern formation.","Typically, bubbles or fingers of air propagate steadily at low values of the driving parameter.","However, as the driving parameter increases, they can exhibit disordered pattern-forming dynamics.","In this paper, we demonstrate experimentally that a remote perturbation of the bubble's tip can drive time-periodic bubble propagation: a fundamental building block of complex unsteady dynamics.","We exploit the propensity of a group of bubbles to self-organise into a fixed spatial arrangement in a Hele-Shaw channel with a centralised depth-reduction in order to apply a sustained perturbation to a bubble's shape as it propagates.","We find that the bubble with a perturbed shape begins to oscillate after the system undergoes a supercritical Hopf bifurcation upon variation of the tip perturbation and dimensionless flow rate.","The oscillation cycle features the splitting of the bubble's tip and advection of the resulting finger-like protrusion along the bubble's length until it is absorbed by the bubble's advancing rear.","The restoral of the bubble's tip follows naturally because the system is driven by a fixed flow rate and the perturbed bubble is attracted to the weakly unstable, steadily propagating state that is set by the ratio of imposed viscous and capillary forces.","Our results suggest a generic mechanism for time-periodic dynamics of propagating curved fronts subject to a steady shape perturbation."],"url":"http://arxiv.org/abs/2402.19391v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 17:38:54","title":"SeD: Semantic-Aware Discriminator for Image Super-Resolution","abstract":"Generative Adversarial Networks (GANs) have been widely used to recover vivid textures in image super-resolution (SR) tasks. In particular, one discriminator is utilized to enable the SR network to learn the distribution of real-world high-quality images in an adversarial training manner. However, the distribution learning is overly coarse-grained, which is susceptible to virtual textures and causes counter-intuitive generation results. To mitigate this, we propose the simple and effective Semantic-aware Discriminator (denoted as SeD), which encourages the SR network to learn the fine-grained distributions by introducing the semantics of images as a condition. Concretely, we aim to excavate the semantics of images from a well-trained semantic extractor. Under different semantics, the discriminator is able to distinguish the real-fake images individually and adaptively, which guides the SR network to learn the more fine-grained semantic-aware textures. To obtain accurate and abundant semantics, we take full advantage of recently popular pretrained vision models (PVMs) with extensive datasets, and then incorporate its semantic features into the discriminator through a well-designed spatial cross-attention module. In this way, our proposed semantic-aware discriminator empowered the SR network to produce more photo-realistic and pleasing images. Extensive experiments on two typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our proposed methods.","sentences":["Generative Adversarial Networks (GANs) have been widely used to recover vivid textures in image super-resolution (SR) tasks.","In particular, one discriminator is utilized to enable the SR network to learn the distribution of real-world high-quality images in an adversarial training manner.","However, the distribution learning is overly coarse-grained, which is susceptible to virtual textures and causes counter-intuitive generation results.","To mitigate this, we propose the simple and effective Semantic-aware Discriminator (denoted as SeD), which encourages the SR network to learn the fine-grained distributions by introducing the semantics of images as a condition.","Concretely, we aim to excavate the semantics of images from a well-trained semantic extractor.","Under different semantics, the discriminator is able to distinguish the real-fake images individually and adaptively, which guides the SR network to learn the more fine-grained semantic-aware textures.","To obtain accurate and abundant semantics, we take full advantage of recently popular pretrained vision models (PVMs) with extensive datasets, and then incorporate its semantic features into the discriminator through a well-designed spatial cross-attention module.","In this way, our proposed semantic-aware discriminator empowered the SR network to produce more photo-realistic and pleasing images.","Extensive experiments on two typical tasks, i.e., SR and Real SR have demonstrated the effectiveness of our proposed methods."],"url":"http://arxiv.org/abs/2402.19387v1","category":"eess.IV"}
{"created":"2024-02-29 17:36:39","title":"Towards Safe and Reliable Autonomous Driving: Dynamic Occupancy Set Prediction","abstract":"In the rapidly evolving field of autonomous driving, accurate trajectory prediction is pivotal for vehicular safety. However, trajectory predictions often deviate from actual paths, particularly in complex and challenging environments, leading to significant errors. To address this issue, our study introduces a novel method for Dynamic Occupancy Set (DOS) prediction, enhancing trajectory prediction capabilities. This method effectively combines advanced trajectory prediction networks with a DOS prediction module, overcoming the shortcomings of existing models. It provides a comprehensive and adaptable framework for predicting the potential occupancy sets of traffic participants. The main contributions of this research include: 1) A novel DOS prediction model tailored for complex scenarios, augmenting traditional trajectory prediction; 2) The development of unique DOS representations and evaluation metrics; 3) Extensive validation through experiments, demonstrating enhanced performance and adaptability. This research contributes to the advancement of safer and more efficient intelligent vehicle and transportation systems.","sentences":["In the rapidly evolving field of autonomous driving, accurate trajectory prediction is pivotal for vehicular safety.","However, trajectory predictions often deviate from actual paths, particularly in complex and challenging environments, leading to significant errors.","To address this issue, our study introduces a novel method for Dynamic Occupancy Set (DOS) prediction, enhancing trajectory prediction capabilities.","This method effectively combines advanced trajectory prediction networks with a DOS prediction module, overcoming the shortcomings of existing models.","It provides a comprehensive and adaptable framework for predicting the potential occupancy sets of traffic participants.","The main contributions of this research include: 1) A novel DOS prediction model tailored for complex scenarios, augmenting traditional trajectory prediction; 2)","The development of unique DOS representations and evaluation metrics; 3) Extensive validation through experiments, demonstrating enhanced performance and adaptability.","This research contributes to the advancement of safer and more efficient intelligent vehicle and transportation systems."],"url":"http://arxiv.org/abs/2402.19385v1","category":"cs.RO"}
{"created":"2024-02-29 17:29:07","title":"Not flexible enough? Impacts of electric carsharing on a power sector with variable renewables","abstract":"Electrifying the car fleet is a major strategy for mitigating greenhouse gas emissions in the transport sector. However, electrification alone will not solve all the negative externalities associated with cars. In light of other problems such as street space as well as concerns about the use of mineral resources for battery electric cars, reducing the car fleet size would be beneficial, particularly in cities. Carsharing could offer a way to reconcile current car usage habits with a reduction in the car fleet size. However, it could also reduce the potential of electric cars to align their grid interactions with variable renewable electricity generation. We investigate how electric carsharing may impact the power sector in the future. We combine three open-source quantitative methods, including sequence clustering of car travel diaries, a probabilistic tool to generate synthetic electric vehicle time series, and an optimization model of the power sector. For 2030 scenarios of Germany with a renewable share of at least 80%, we show that switching to electric carsharing only moderately increases power sector costs. In our main setting, carsharing increases yearly power sector costs by less than 100 euros per substituted private electric car. This cost effect is largest under the assumption of bidirectional charging. It is mitigated when other sources of flexibility for the power sector are considered. Carsharing further causes a shift from wind power to solar PV in the optimal capacity mix, and may also trigger additional investments in stationary electricity storage. Overall, we find that shared electric cars still have the potential to be operated largely in line with variable renewable electricity generation. We conclude that electric carsharing is unlikely to cause much damage to the power sector, but could bring various other benefits, which may outweigh power sector cost increases.","sentences":["Electrifying the car fleet is a major strategy for mitigating greenhouse gas emissions in the transport sector.","However, electrification alone will not solve all the negative externalities associated with cars.","In light of other problems such as street space as well as concerns about the use of mineral resources for battery electric cars, reducing the car fleet size would be beneficial, particularly in cities.","Carsharing could offer a way to reconcile current car usage habits with a reduction in the car fleet size.","However, it could also reduce the potential of electric cars to align their grid interactions with variable renewable electricity generation.","We investigate how electric carsharing may impact the power sector in the future.","We combine three open-source quantitative methods, including sequence clustering of car travel diaries, a probabilistic tool to generate synthetic electric vehicle time series, and an optimization model of the power sector.","For 2030 scenarios of Germany with a renewable share of at least 80%, we show that switching to electric carsharing only moderately increases power sector costs.","In our main setting, carsharing increases yearly power sector costs by less than 100 euros per substituted private electric car.","This cost effect is largest under the assumption of bidirectional charging.","It is mitigated when other sources of flexibility for the power sector are considered.","Carsharing further causes a shift from wind power to solar PV in the optimal capacity mix, and may also trigger additional investments in stationary electricity storage.","Overall, we find that shared electric cars still have the potential to be operated largely in line with variable renewable electricity generation.","We conclude that electric carsharing is unlikely to cause much damage to the power sector, but could bring various other benefits, which may outweigh power sector cost increases."],"url":"http://arxiv.org/abs/2402.19380v1","category":"econ.GN"}
{"created":"2024-02-29 17:27:59","title":"Wisdom of the Silicon Crowd: LLM Ensemble Prediction Capabilities Match Human Crowd Accuracy","abstract":"Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd. We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety applications throughout society.","sentences":["Human forecasting accuracy in practice relies on the 'wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters.","Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate.","In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs.","We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament.","Our main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is statistically equivalent to the human crowd.","We also observe an acquiescence effect, with mean model predictions being significantly above 50%, despite an almost even split of positive and negative resolutions.","Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output.","We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17% and 28%: though this leads to less accurate predictions than simply averaging human and machine forecasts.","Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation.","This replicates the 'wisdom of the crowd' effect for LLMs, and opens up their use for a variety applications throughout society."],"url":"http://arxiv.org/abs/2402.19379v1","category":"cs.CY"}
{"created":"2024-02-29 17:27:05","title":"From design to device: challenges and opportunities in computational discovery of p-type transparent conductors","abstract":"A high-performance p-type transparent conductor (TC) does not yet exist, but could lead to advances in a wide range of optoelectronic applications and enable new architectures for, e.g., next-generation photovoltaic (PV) devices. High-throughput computational material screenings have been a promising approach to filter databases and identify new p-type TC candidates, and some of these predictions have been experimentally validated. However, most of these predicted candidates do not have experimentally-achieved properties on par with n-type TCs used in solar cells, and therefore have not yet been used in commercial devices. Thus, there is still a significant divide between transforming predictions into results that are actually achievable in the lab, and an even greater lag in scaling predicted materials into functional devices. In this perspective, we outline some of the major disconnects in this materials discovery process -- from scaling computational predictions into synthesizable crystals and thin films in the laboratory, to scaling lab-grown films into real-world solar devices -- and share insights to inform future strategies for TC discovery and design.","sentences":["A high-performance p-type transparent conductor (TC) does not yet exist, but could lead to advances in a wide range of optoelectronic applications and enable new architectures for, e.g., next-generation photovoltaic (PV) devices.","High-throughput computational material screenings have been a promising approach to filter databases and identify new p-type TC candidates, and some of these predictions have been experimentally validated.","However, most of these predicted candidates do not have experimentally-achieved properties on par with n-type TCs used in solar cells, and therefore have not yet been used in commercial devices.","Thus, there is still a significant divide between transforming predictions into results that are actually achievable in the lab, and an even greater lag in scaling predicted materials into functional devices.","In this perspective, we outline some of the major disconnects in this materials discovery process -- from scaling computational predictions into synthesizable crystals and thin films in the laboratory, to scaling lab-grown films into real-world solar devices -- and share insights to inform future strategies for TC discovery and design."],"url":"http://arxiv.org/abs/2402.19378v1","category":"physics.app-ph"}
{"created":"2024-02-29 17:24:22","title":"OzMAC: An Energy-Efficient Sparsity-Exploiting Multiply-Accumulate-Unit Design for DL Inference","abstract":"General Matrix Multiply (GEMM) hardware, employing large arrays of multiply-accumulate (MAC) units, perform bulk of the computation in deep learning (DL). Recent trends have established 8-bit integer (INT8) as the most widely used precision for DL inference. This paper proposes a novel MAC design capable of dynamically exploiting bit sparsity (i.e., number of `0' bits within a binary value) in input data to achieve significant improvements on area, power and energy. The proposed architecture, called OzMAC (Omit-zero-MAC), skips over zeros within a binary input value and performs simple shift-and-add-based compute in place of expensive multipliers. We implement OzMAC in SystemVerilog and present post-synthesis performance-power-area (PPA) results using commercial TSMC N5 (5nm) process node. Using eight pretrained INT8 deep neural networks (DNNs) as benchmarks, we demonstrate the existence of high bit sparsity in real DNN workloads and show that 8-bit OzMAC improves all three metrics of area, power, and energy significantly by 21%, 70%, and 28%, respectively. Similar improvements are achieved when scaling data precisions (4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz). For the 8-bit OzMAC, scaling its frequency to normalize the throughput relative to conventional MAC, it still achieves 30% improvement on both power and energy.","sentences":["General Matrix Multiply (GEMM) hardware, employing large arrays of multiply-accumulate (MAC) units, perform bulk of the computation in deep learning (DL).","Recent trends have established 8-bit integer (INT8) as the most widely used precision for DL inference.","This paper proposes a novel MAC design capable of dynamically exploiting bit sparsity (i.e., number of `0' bits within a binary value) in input data to achieve significant improvements on area, power and energy.","The proposed architecture, called OzMAC (Omit-zero-MAC), skips over zeros within a binary input value and performs simple shift-and-add-based compute in place of expensive multipliers.","We implement OzMAC in SystemVerilog and present post-synthesis performance-power-area (PPA) results using commercial TSMC N5 (5nm) process node.","Using eight pretrained INT8 deep neural networks (DNNs) as benchmarks, we demonstrate the existence of high bit sparsity in real DNN workloads and show that 8-bit OzMAC improves all three metrics of area, power, and energy significantly by 21%, 70%, and 28%, respectively.","Similar improvements are achieved when scaling data precisions (4, 8, 16 bits) and clock frequencies (0.5 GHz, 1 GHz, 1.5 GHz).","For the 8-bit OzMAC, scaling its frequency to normalize the throughput relative to conventional MAC, it still achieves 30% improvement on both power and energy."],"url":"http://arxiv.org/abs/2402.19376v1","category":"cs.AR"}
{"created":"2024-02-29 17:20:39","title":"Revisiting string-inspired running-vacuum models under the lens of light primordial black holes","abstract":"Light primordial black holes (PBHs) with masses $M_\\mathrm{PBH}<10^9\\mathrm{g}$ can interestingly dominate the Universe's energy budget and give rise to early matter-dominated (eMD) eras before Big Bang Nucleosyntesis (BBN). During this eMD era, one is met with an abundant production of induced gravitational waves (GWs) serving as a portal to constrain the underlying theory of gravity. In this work, we study this type of induced GWs within the context of string-inspired running-vaccuum models (StRVMs), which, when expanded around de Sitter backgrounds, include logarithmic corrections of the space-time curvature. In particular, we discuss in detail the effects of StRVMs on the source as well as on the propagation of these PBH-induced GWs. Remarkably, under the assumption that the logarithmic terms represent quantum gravity corrections in the PBH era, we show that GW overproduction can be avoided if one assumes a coefficient of these logarithmic corrections that is much larger than the square of the reduced Planck mass. The latter cannot characterise quantum gravity corrections, though, prompting the need for revision of the quantisation of StRVMs in different than de Sitter backgrounds, such as those characterising PBH-driven eMD eras. This non trivial result suggests the importance of light PBHs as probes of new physics.","sentences":["Light primordial black holes (PBHs) with masses $M_\\mathrm{PBH}<10^9\\mathrm{g}$ can interestingly dominate the Universe's energy budget and give rise to early matter-dominated (eMD) eras before Big Bang Nucleosyntesis (BBN).","During this eMD era, one is met with an abundant production of induced gravitational waves (GWs) serving as a portal to constrain the underlying theory of gravity.","In this work, we study this type of induced GWs within the context of string-inspired running-vaccuum models (StRVMs), which, when expanded around de Sitter backgrounds, include logarithmic corrections of the space-time curvature.","In particular, we discuss in detail the effects of StRVMs on the source as well as on the propagation of these PBH-induced GWs.","Remarkably, under the assumption that the logarithmic terms represent quantum gravity corrections in the PBH era, we show that GW overproduction can be avoided if one assumes a coefficient of these logarithmic corrections that is much larger than the square of the reduced Planck mass.","The latter cannot characterise quantum gravity corrections, though, prompting the need for revision of the quantisation of StRVMs in different than de Sitter backgrounds, such as those characterising PBH-driven eMD eras.","This non trivial result suggests the importance of light PBHs as probes of new physics."],"url":"http://arxiv.org/abs/2402.19373v1","category":"gr-qc"}
{"created":"2024-02-29 17:19:39","title":"OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models","abstract":"LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-shot, few-shot, chain-of-thought (random selection and kNN selection), and ensemble/self-consistency voting. We found that OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks, surpassing the previous best performing OS models that leveraged computationally costly extensive fine-tuning. The model delivers a 72.6% accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and achieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the first OS LLM to surpass 80% accuracy on this benchmark. Our results highlight medical-specific emergent properties in OS LLMs which have not yet been documented to date elsewhere, and showcase the benefits of further leveraging prompt engineering to improve the performance of accessible LLMs for medical applications.","sentences":["LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge.","Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power.","Many of the top performing LLMs are proprietary and their access is limited to very few research groups.","However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare.","We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks.","We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset).","We employed a series of prompting strategies, including zero-shot, few-shot, chain-of-thought (random selection and kNN selection), and ensemble/self-consistency voting.","We found that OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks, surpassing the previous best performing OS models that leveraged computationally costly extensive fine-tuning.","The model delivers a 72.6% accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and achieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the first OS LLM to surpass 80% accuracy on this benchmark.","Our results highlight medical-specific emergent properties in OS LLMs which have not yet been documented to date elsewhere, and showcase the benefits of further leveraging prompt engineering to improve the performance of accessible LLMs for medical applications."],"url":"http://arxiv.org/abs/2402.19371v1","category":"cs.CL"}
{"created":"2024-02-29 17:13:44","title":"SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency","abstract":"The growing number of cases requiring digital forensic analysis raises concerns about law enforcement's ability to conduct investigations promptly. Consequently, this systemisation of knowledge paper delves into the potential and effectiveness of integrating Large Language Models (LLMs) into digital forensic investigation to address these challenges. A thorough literature review is undertaken, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the utilisation of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs. In conclusion, the study asserts that the adoption of LLMs in digital forensics, with appropriate constraints, holds the potential to enhance investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement entities.","sentences":["The growing number of cases requiring digital forensic analysis raises concerns about law enforcement's ability to conduct investigations promptly.","Consequently, this systemisation of knowledge paper delves into the potential and effectiveness of integrating Large Language Models (LLMs) into digital forensic investigation to address these challenges.","A thorough literature review is undertaken, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the utilisation of LLMs in investigations.","The review identifies current challenges within existing digital forensic processes and explores both the obstacles and possibilities of incorporating LLMs.","In conclusion, the study asserts that the adoption of LLMs in digital forensics, with appropriate constraints, holds the potential to enhance investigation efficiency, improve traceability, and alleviate technical and judicial barriers faced by law enforcement entities."],"url":"http://arxiv.org/abs/2402.19366v1","category":"cs.CR"}
{"created":"2024-02-29 17:13:03","title":"Dark energy and dark matter configurations for wormholes and solitionic hierarchies of nonmetric Ricci flows and $F(R,T,Q,T_{m})$ gravity","abstract":"We extend the anholonomic frame and connection deformation method, AFCDM, for constructing exact and parametric solutions in general relativity, GR, to geometric flow models and modified gravity theories, MGTs, with nontrivial torsion and nonmetricity fields. Following abstract geometric or variational methods, we can derive corresponding systems of nonmetric gravitational and matter field equations which consist of very sophisticated systems of coupled nonlinear PDEs. Using nonholonomic frames with dyadic spacetime splitting and applying the AFCDM, we prove that such systems of PDEs can be decoupled and integrated in general forms for generic off-diagonal metric structures and generalized affine connections. We generate new classes of quasi-stationary solutions (which do not depend on time like coordinates) and study the physical properties of some physically important examples. Such exact or parametric solutions are determined by nonmetric solitonic distributions and/or ellipsoidal deformations of wormhole hole configurations. It is not possible to describe the thermodynamic properties of such solutions in the framework of the Bekenstein-Hawking paradigm because such metrics do not involve, in general, certain horizons, duality, or holographic configurations. Nevertheless, we can always elaborate on associated Grigori Perelman thermodynamic models elaborated for nonmetric geometric flows. In explicit form, applying the AFCDM, we construct and study the physical implications of new classes of traversable wormhole solutions describing solitonic deformation and dissipation of non-Riemannian geometric objects. Such models with nontrivial gravitational off-diagonal vacuum are important for elaborating models of dark energy and dark matter involving wormhole configurations and solitonic-type structure formation.","sentences":["We extend the anholonomic frame and connection deformation method, AFCDM, for constructing exact and parametric solutions in general relativity, GR, to geometric flow models and modified gravity theories, MGTs, with nontrivial torsion and nonmetricity fields.","Following abstract geometric or variational methods, we can derive corresponding systems of nonmetric gravitational and matter field equations which consist of very sophisticated systems of coupled nonlinear PDEs.","Using nonholonomic frames with dyadic spacetime splitting and applying the AFCDM, we prove that such systems of PDEs can be decoupled and integrated in general forms for generic off-diagonal metric structures and generalized affine connections.","We generate new classes of quasi-stationary solutions (which do not depend on time like coordinates) and study the physical properties of some physically important examples.","Such exact or parametric solutions are determined by nonmetric solitonic distributions and/or ellipsoidal deformations of wormhole hole configurations.","It is not possible to describe the thermodynamic properties of such solutions in the framework of the Bekenstein-Hawking paradigm because such metrics do not involve, in general, certain horizons, duality, or holographic configurations.","Nevertheless, we can always elaborate on associated Grigori Perelman thermodynamic models elaborated for nonmetric geometric flows.","In explicit form, applying the AFCDM, we construct and study the physical implications of new classes of traversable wormhole solutions describing solitonic deformation and dissipation of non-Riemannian geometric objects.","Such models with nontrivial gravitational off-diagonal vacuum are important for elaborating models of dark energy and dark matter involving wormhole configurations and solitonic-type structure formation."],"url":"http://arxiv.org/abs/2402.19362v1","category":"gr-qc"}
{"created":"2024-02-29 17:12:39","title":"Watermark Stealing in Large Language Models","abstract":"LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.","sentences":["LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment.","In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes.","We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as suggested in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed.","We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings.","We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%.","Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes.","We make all our code and additional examples available at https://watermark-stealing.org."],"url":"http://arxiv.org/abs/2402.19361v1","category":"cs.CR"}
{"created":"2024-02-29 17:08:45","title":"A Rigorous Holographic Bound on AdS Scale Separation","abstract":"We give an elementary proof of the following property of unitary, interacting four-dimensional $\\mathcal{N}=2$ superconformal field theories: at large central charge $c$, there exist at least $\\sqrt{c}$ single-trace, scalar superconformal primary operators with dimensions $\\Delta \\lesssim \\sqrt{c}$ (suppressing multiplicative logarithmic corrections). This follows from a stronger, more refined bound on the spectral density in terms of the asymptotic growth rate of the central charge. The proof employs known results on the structure of Coulomb branch operators. Interpreted holographically, this bounds the possible degree of scale separation in semiclassical AdS$_5$ half-maximal supergravity. In particular, the bulk must contain an infinite tower of charged scalar states of energies parametrically below the large black hole threshold, $E_{\\rm BH} \\sim c$. We address the extreme case of AdS$_5$ pure supergravity, ruling it out as the asymptotic limit of certain sequences in theory space, though the general question remains open.","sentences":["We give an elementary proof of the following property of unitary, interacting four-dimensional $\\mathcal{N}=2$ superconformal field theories: at large central charge $c$, there exist at least $\\sqrt{c}$ single-trace, scalar superconformal primary operators with dimensions $\\Delta \\lesssim \\sqrt{c}$ (suppressing multiplicative logarithmic corrections).","This follows from a stronger, more refined bound on the spectral density in terms of the asymptotic growth rate of the central charge.","The proof employs known results on the structure of Coulomb branch operators.","Interpreted holographically, this bounds the possible degree of scale separation in semiclassical AdS$_5$ half-maximal supergravity.","In particular, the bulk must contain an infinite tower of charged scalar states of energies parametrically below the large black hole threshold, $E_{\\rm BH} \\sim c$.","We address the extreme case of AdS$_5$ pure supergravity, ruling it out as the asymptotic limit of certain sequences in theory space, though the general question remains open."],"url":"http://arxiv.org/abs/2402.19358v1","category":"hep-th"}
{"created":"2024-02-29 17:06:52","title":"Unraveling Adversarial Examples against Speaker Identification -- Techniques for Attack Detection and Victim Model Classification","abstract":"Adversarial examples have proven to threaten speaker identification systems, and several countermeasures against them have been proposed. In this paper, we propose a method to detect the presence of adversarial examples, i.e., a binary classifier distinguishing between benign and adversarial examples. We build upon and extend previous work on attack type classification by exploring new architectures. Additionally, we introduce a method for identifying the victim model on which the adversarial attack is carried out. To achieve this, we generate a new dataset containing multiple attacks performed against various victim models. We achieve an AUC of 0.982 for attack detection, with no more than a 0.03 drop in performance for unknown attacks. Our attack classification accuracy (excluding benign) reaches 86.48% across eight attack types using our LightResNet34 architecture, while our victim model classification accuracy reaches 72.28% across four victim models.","sentences":["Adversarial examples have proven to threaten speaker identification systems, and several countermeasures against them have been proposed.","In this paper, we propose a method to detect the presence of adversarial examples, i.e., a binary classifier distinguishing between benign and adversarial examples.","We build upon and extend previous work on attack type classification by exploring new architectures.","Additionally, we introduce a method for identifying the victim model on which the adversarial attack is carried out.","To achieve this, we generate a new dataset containing multiple attacks performed against various victim models.","We achieve an AUC of 0.982 for attack detection, with no more than a 0.03 drop in performance for unknown attacks.","Our attack classification accuracy (excluding benign) reaches 86.48% across eight attack types using our LightResNet34 architecture, while our victim model classification accuracy reaches 72.28% across four victim models."],"url":"http://arxiv.org/abs/2402.19355v1","category":"cs.SD"}
{"created":"2024-02-29 16:58:59","title":"Searching for NLTE effects in the high-resolution transmission spectrum of WASP-121 b with Cloudy for Exoplanets","abstract":"Ultra-hot Jupiters (UHJs) undergo intense irradiation by their host stars and are expected to experience non-local thermodynamic equilibrium (NLTE) effects in their atmospheres. Such effects are computationally intensive to model but, at the low pressures probed by high-resolution cross-correlation spectroscopy (HRCCS), can significantly impact the formation of spectral lines. The UHJ WASP-121 b exhibits a highly inflated atmosphere, making it ideal for investigating the impact of NLTE effects on its transmission spectrum. Here, we formally introduce Cloudy for Exoplanets, a Cloudy-based modelling code, and use it to generate 1-D LTE and NLTE atmospheric models and spectra to analyse archival HARPS WASP-121 b transmission spectra. We assessed the models using two HRCCS methods: i) Pearson cross-correlation, and ii) a method that aims to match the average observed line depth for given atmospheric species. All models result in strong detections of Fe I ($7.5<S/N<10.5$). However, the highest S/N model (LTE) does not agree with the best-matching model of the average line depth (NLTE). We also find degeneracy, such that increasing the isothermal temperature and metallicity of the LTE models can produce average line depths similar to cooler, less metal rich NLTE models. Thus, we are unable to conclusively remark on the presence of NLTE effects in the atmosphere of WASP-121 b. We instead highlight the need for standardised metrics in HRCCS that enable robust statistical assessment of complex physical models, e.g. NLTE or 3-D effects, that are currently too computationally intensive to include in HRCCS atmospheric retrievals.","sentences":["Ultra-hot Jupiters (UHJs) undergo intense irradiation by their host stars and are expected to experience non-local thermodynamic equilibrium (NLTE) effects in their atmospheres.","Such effects are computationally intensive to model but, at the low pressures probed by high-resolution cross-correlation spectroscopy (HRCCS), can significantly impact the formation of spectral lines.","The UHJ WASP-121 b exhibits a highly inflated atmosphere, making it ideal for investigating the impact of NLTE effects on its transmission spectrum.","Here, we formally introduce Cloudy for Exoplanets, a Cloudy-based modelling code, and use it to generate 1-D LTE and NLTE atmospheric models and spectra to analyse archival HARPS WASP-121 b transmission spectra.","We assessed the models using two HRCCS methods: i) Pearson cross-correlation, and ii) a method that aims to match the average observed line depth for given atmospheric species.","All models result in strong detections of Fe I ($7.5<S/N<10.5$).","However, the highest S/N model (LTE) does not agree with the best-matching model of the average line depth (NLTE).","We also find degeneracy, such that increasing the isothermal temperature and metallicity of the LTE models can produce average line depths similar to cooler, less metal rich NLTE models.","Thus, we are unable to conclusively remark on the presence of NLTE effects in the atmosphere of WASP-121 b.","We instead highlight the need for standardised metrics in HRCCS that enable robust statistical assessment of complex physical models, e.g. NLTE or 3-D effects, that are currently too computationally intensive to include in HRCCS atmospheric retrievals."],"url":"http://arxiv.org/abs/2402.19352v1","category":"astro-ph.EP"}
{"created":"2024-02-29 16:56:23","title":"Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook","abstract":"As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.","sentences":["As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities).","Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities.","To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing.","Specifically, we first delve into data perspective to comprehend the role of each modality and data source.","Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods.","Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy.","Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications.","Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field.","We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community.","The summary of the comprehensive and up-to-date paper list can be found at https://github.com/yoshall/Awesome-Multimodal-Urban-Computing."],"url":"http://arxiv.org/abs/2402.19348v1","category":"cs.LG"}
{"created":"2024-02-29 16:49:10","title":"Infinite Order Hydrodynamics: an Analytical Example","abstract":"We construct a kinetic model for matter-radiation interactions where the hydrodynamic gradient expansion can be computed analytically up to infinite order in derivatives, in the fully non-linear regime, and for arbitrary flows. The frequency dependence of the opacity of matter is chosen to mimic the relaxation time of a self-interacting scalar field. In this way, the transient sector simulates that of a realistic quantum field theory. As expected, the gradient series is divergent for most flows. We identify the mechanism at the origin of the divergence, and we provide a successful regularization scheme. Additionally, we propose a universal qualitative framework for predicting the breakdown of the gradient expansion of an arbitrary microscopic system undergoing a given flow. This framework correctly predicts the factorial divergence of the gradient expansion in most non-linear flows and its breakdown due to stochastic fluctuations. It also predicts that jets may induce an ultraviolet divergence in the gradient expansion of quark matter hydrodynamics.","sentences":["We construct a kinetic model for matter-radiation interactions where the hydrodynamic gradient expansion can be computed analytically up to infinite order in derivatives, in the fully non-linear regime, and for arbitrary flows.","The frequency dependence of the opacity of matter is chosen to mimic the relaxation time of a self-interacting scalar field.","In this way, the transient sector simulates that of a realistic quantum field theory.","As expected, the gradient series is divergent for most flows.","We identify the mechanism at the origin of the divergence, and we provide a successful regularization scheme.","Additionally, we propose a universal qualitative framework for predicting the breakdown of the gradient expansion of an arbitrary microscopic system undergoing a given flow.","This framework correctly predicts the factorial divergence of the gradient expansion in most non-linear flows and its breakdown due to stochastic fluctuations.","It also predicts that jets may induce an ultraviolet divergence in the gradient expansion of quark matter hydrodynamics."],"url":"http://arxiv.org/abs/2402.19343v1","category":"nucl-th"}
{"created":"2024-02-29 16:47:54","title":"RoadRunner -- Learning Traversability Estimation for Autonomous Off-road Driving","abstract":"Autonomous navigation at high speeds in off-road environments necessitates robots to comprehensively understand their surroundings using onboard sensing only. The extreme conditions posed by the off-road setting can cause degraded camera image quality due to poor lighting and motion blur, as well as limited sparse geometric information available from LiDAR sensing when driving at high speeds. In this work, we present RoadRunner, a novel framework capable of predicting terrain traversability and an elevation map directly from camera and LiDAR sensor inputs. RoadRunner enables reliable autonomous navigation, by fusing sensory information, handling of uncertainty, and generation of contextually informed predictions about the geometry and traversability of the terrain while operating at low latency. In contrast to existing methods relying on classifying handcrafted semantic classes and using heuristics to predict traversability costs, our method is trained end-to-end in a self-supervised fashion. The RoadRunner network architecture builds upon popular sensor fusion network architectures from the autonomous driving domain, which embed LiDAR and camera information into a common Bird's Eye View perspective. Training is enabled by utilizing an existing traversability estimation stack to generate training data in hindsight in a scalable manner from real-world off-road driving datasets. Furthermore, RoadRunner improves the system latency by a factor of roughly 4, from 500 ms to 140 ms, while improving the accuracy for traversability costs and elevation map predictions. We demonstrate the effectiveness of RoadRunner in enabling safe and reliable off-road navigation at high speeds in multiple real-world driving scenarios through unstructured desert environments.","sentences":["Autonomous navigation at high speeds in off-road environments necessitates robots to comprehensively understand their surroundings using onboard sensing only.","The extreme conditions posed by the off-road setting can cause degraded camera image quality due to poor lighting and motion blur, as well as limited sparse geometric information available from LiDAR sensing when driving at high speeds.","In this work, we present RoadRunner, a novel framework capable of predicting terrain traversability and an elevation map directly from camera and LiDAR sensor inputs.","RoadRunner enables reliable autonomous navigation, by fusing sensory information, handling of uncertainty, and generation of contextually informed predictions about the geometry and traversability of the terrain while operating at low latency.","In contrast to existing methods relying on classifying handcrafted semantic classes and using heuristics to predict traversability costs, our method is trained end-to-end in a self-supervised fashion.","The RoadRunner network architecture builds upon popular sensor fusion network architectures from the autonomous driving domain, which embed LiDAR and camera information into a common Bird's Eye View perspective.","Training is enabled by utilizing an existing traversability estimation stack to generate training data in hindsight in a scalable manner from real-world off-road driving datasets.","Furthermore, RoadRunner improves the system latency by a factor of roughly 4, from 500 ms to 140 ms, while improving the accuracy for traversability costs and elevation map predictions.","We demonstrate the effectiveness of RoadRunner in enabling safe and reliable off-road navigation at high speeds in multiple real-world driving scenarios through unstructured desert environments."],"url":"http://arxiv.org/abs/2402.19341v1","category":"cs.RO"}
{"created":"2024-02-29 16:46:49","title":"One model to use them all: Training a segmentation model with complementary datasets","abstract":"Understanding a surgical scene is crucial for computer-assisted surgery systems to provide any intelligent assistance functionality. One way of achieving this scene understanding is via scene segmentation, where every pixel of a frame is classified and therefore identifies the visible structures and tissues. Progress on fully segmenting surgical scenes has been made using machine learning. However, such models require large amounts of annotated training data, containing examples of all relevant object classes. Such fully annotated datasets are hard to create, as every pixel in a frame needs to be annotated by medical experts and, therefore, are rarely available. In this work, we propose a method to combine multiple partially annotated datasets, which provide complementary annotations, into one model, enabling better scene segmentation and the use of multiple readily available datasets. Our method aims to combine available data with complementary labels by leveraging mutual exclusive properties to maximize information. Specifically, we propose to use positive annotations of other classes as negative samples and to exclude background pixels of binary annotations, as we cannot tell if they contain a class not annotated but predicted by the model. We evaluate our method by training a DeepLabV3 on the publicly available Dresden Surgical Anatomy Dataset, which provides multiple subsets of binary segmented anatomical structures. Our approach successfully combines 6 classes into one model, increasing the overall Dice Score by 4.4% compared to an ensemble of models trained on the classes individually. By including information on multiple classes, we were able to reduce confusion between stomach and colon by 24%. Our results demonstrate the feasibility of training a model on multiple datasets. This paves the way for future work further alleviating the need for one large, fully segmented datasets.","sentences":["Understanding a surgical scene is crucial for computer-assisted surgery systems to provide any intelligent assistance functionality.","One way of achieving this scene understanding is via scene segmentation, where every pixel of a frame is classified and therefore identifies the visible structures and tissues.","Progress on fully segmenting surgical scenes has been made using machine learning.","However, such models require large amounts of annotated training data, containing examples of all relevant object classes.","Such fully annotated datasets are hard to create, as every pixel in a frame needs to be annotated by medical experts and, therefore, are rarely available.","In this work, we propose a method to combine multiple partially annotated datasets, which provide complementary annotations, into one model, enabling better scene segmentation and the use of multiple readily available datasets.","Our method aims to combine available data with complementary labels by leveraging mutual exclusive properties to maximize information.","Specifically, we propose to use positive annotations of other classes as negative samples and to exclude background pixels of binary annotations, as we cannot tell if they contain a class not annotated but predicted by the model.","We evaluate our method by training a DeepLabV3 on the publicly available Dresden Surgical Anatomy Dataset, which provides multiple subsets of binary segmented anatomical structures.","Our approach successfully combines 6 classes into one model, increasing the overall Dice Score by 4.4% compared to an ensemble of models trained on the classes individually.","By including information on multiple classes, we were able to reduce confusion between stomach and colon by 24%.","Our results demonstrate the feasibility of training a model on multiple datasets.","This paves the way for future work further alleviating the need for one large, fully segmented datasets."],"url":"http://arxiv.org/abs/2402.19340v1","category":"cs.CV"}
{"created":"2024-02-29 16:46:48","title":"Stitching Gaps: Fusing Situated Perceptual Knowledge with Vision Transformers for High-Level Image Classification","abstract":"The increasing demand for automatic high-level image understanding, particularly in detecting abstract concepts (AC) within images, underscores the necessity for innovative and more interpretable approaches. These approaches need to harmonize traditional deep vision methods with the nuanced, context-dependent knowledge humans employ to interpret images at intricate semantic levels. In this work, we leverage situated perceptual knowledge of cultural images to enhance performance and interpretability in AC image classification. We automatically extract perceptual semantic units from images, which we then model and integrate into the ARTstract Knowledge Graph (AKG). This resource captures situated perceptual semantics gleaned from over 14,000 cultural images labeled with ACs. Additionally, we enhance the AKG with high-level linguistic frames. We compute KG embeddings and experiment with relative representations and hybrid approaches that fuse these embeddings with visual transformer embeddings. Finally, for interpretability, we conduct posthoc qualitative analyses by examining model similarities with training instances. Our results show that our hybrid KGE-ViT methods outperform existing techniques in AC image classification. The posthoc interpretability analyses reveal the visual transformer's proficiency in capturing pixel-level visual attributes, contrasting with our method's efficacy in representing more abstract and semantic scene elements. We demonstrate the synergy and complementarity between KGE embeddings' situated perceptual knowledge and deep visual model's sensory-perceptual understanding for AC image classification. This work suggests a strong potential of neuro-symbolic methods for knowledge integration and robust image representation for use in downstream intricate visual comprehension tasks. All the materials and code are available online.","sentences":["The increasing demand for automatic high-level image understanding, particularly in detecting abstract concepts (AC) within images, underscores the necessity for innovative and more interpretable approaches.","These approaches need to harmonize traditional deep vision methods with the nuanced, context-dependent knowledge humans employ to interpret images at intricate semantic levels.","In this work, we leverage situated perceptual knowledge of cultural images to enhance performance and interpretability in AC image classification.","We automatically extract perceptual semantic units from images, which we then model and integrate into the ARTstract Knowledge Graph (AKG).","This resource captures situated perceptual semantics gleaned from over 14,000 cultural images labeled with ACs.","Additionally, we enhance the AKG with high-level linguistic frames.","We compute KG embeddings and experiment with relative representations and hybrid approaches that fuse these embeddings with visual transformer embeddings.","Finally, for interpretability, we conduct posthoc qualitative analyses by examining model similarities with training instances.","Our results show that our hybrid KGE-ViT methods outperform existing techniques in AC image classification.","The posthoc interpretability analyses reveal the visual transformer's proficiency in capturing pixel-level visual attributes, contrasting with our method's efficacy in representing more abstract and semantic scene elements.","We demonstrate the synergy and complementarity between KGE embeddings' situated perceptual knowledge and deep visual model's sensory-perceptual understanding for AC image classification.","This work suggests a strong potential of neuro-symbolic methods for knowledge integration and robust image representation for use in downstream intricate visual comprehension tasks.","All the materials and code are available online."],"url":"http://arxiv.org/abs/2402.19339v1","category":"cs.CV"}
{"created":"2024-02-29 16:41:53","title":"Integral field spectroscopy supports atmospheric optics to reveal the finite outer scale of the turbulence","abstract":"The spatial coherence wavefront outer scale (L_0) characterizes the size of the largest turbulence eddies in Earth's atmosphere, determining low spatial frequency perturbations in the wavefront of the light captured by ground-based telescopes. The motivation of this work is to introduce a novel technique for estimating L_0 from seeing-limited integral field spectroscopic (IFS) data. This approach is based on the impact of a finite L_0 on the light collected by the pupil entrance of a ground-based telescope. We take advantage of the homogeneity of IFS to generate band filter images spanning a wide wavelength range, enabling the assessment of image quality (IQ) at the telescope's focal plane. Comparing the measured wavelength-dependent IQ variation with predictions from Tokovinin (2002) analytical approach offers valuable insights into the prevailing L_0 parameter during the observations. We applied the proposed technique to observations from MUSE in the Wide Field Mode obtained at the Paranal Observatory. Our analysis successfully validates Tokovinin's analytical expression, which combines the seeing (E_0) and the L_0 parameters, to predict the IQ variations with the wavelength in ground-based astronomical data. However, we observed some discrepancies between the measured and predictions of the IQ that are analyzed in terms of uncertainties in the estimated E_0 and dome-induced turbulence contributions. This work constitutes the empirical validation of the analytical expression for estimating IQ at the focal plane of ground-based telescopes under specific E_0 and finite L_0 conditions. Additionally, we provide a simple methodology to characterize the L_0 and dome-seeing (E_d) as by-products of IFS observations routinely conducted at major ground-based astronomical observatories.","sentences":["The spatial coherence wavefront outer scale (L_0) characterizes the size of the largest turbulence eddies in Earth's atmosphere, determining low spatial frequency perturbations in the wavefront of the light captured by ground-based telescopes.","The motivation of this work is to introduce a novel technique for estimating L_0 from seeing-limited integral field spectroscopic (IFS) data.","This approach is based on the impact of a finite L_0 on the light collected by the pupil entrance of a ground-based telescope.","We take advantage of the homogeneity of IFS to generate band filter images spanning a wide wavelength range, enabling the assessment of image quality (IQ) at the telescope's focal plane.","Comparing the measured wavelength-dependent IQ variation with predictions from Tokovinin (2002) analytical approach offers valuable insights into the prevailing L_0 parameter during the observations.","We applied the proposed technique to observations from MUSE in the Wide Field Mode obtained at the Paranal Observatory.","Our analysis successfully validates Tokovinin's analytical expression, which combines the seeing (E_0) and the L_0 parameters, to predict the IQ variations with the wavelength in ground-based astronomical data.","However, we observed some discrepancies between the measured and predictions of the IQ that are analyzed in terms of uncertainties in the estimated E_0 and dome-induced turbulence contributions.","This work constitutes the empirical validation of the analytical expression for estimating IQ at the focal plane of ground-based telescopes under specific E_0 and finite L_0 conditions.","Additionally, we provide a simple methodology to characterize the L_0 and dome-seeing (E_d) as by-products of IFS observations routinely conducted at major ground-based astronomical observatories."],"url":"http://arxiv.org/abs/2402.19337v1","category":"astro-ph.IM"}
{"created":"2024-02-29 16:38:44","title":"Measurements of CNT Forest Self-Assembly from In-situ ESEM Synthesis","abstract":"Understanding the dynamic self-assembly mechanisms of carbon nanotube (CNT) forests is necessary to advance their technological promise. Here, in-situ environmental scanning electron microscope (ESEM) chemical vapor deposition (CVD) synthesis observes the real-time nucleation, assembly, delamination, and self-termination of dense ($>$ 10$^9$ CNT/cm$^2$), tall ($>$ 100 $\\mu$m) CNT forests in real time. Forest synthesis is continuously observed from nucleation to self-termination. Assembly forces generated near the substrate detach CNTs from the substrate, which simulation suggests requires approximately 10 nN of tensile force. Delamination initiates at both the CNT-catalyst and the catalyst-substrate interfaces, indicating multiple delamination mechanism. Digital image correlation applied to SEM image sequences measures time-invariant strain within growing forests, indicating that forests grow as rigid bodies after liftoff. The Meta CoTracker algorithm measured CNT growth rates reduce from 50 nm/sec to full termination over 150 seconds. This work provides a robust strategy to observe and measure CVD material synthesis in-situ using ESEM. The method is uniquely suited to observe population-based phenomena at both nanometer spatial resolution and at a highly scalable field of view.","sentences":["Understanding the dynamic self-assembly mechanisms of carbon nanotube (CNT) forests is necessary to advance their technological promise.","Here, in-situ environmental scanning electron microscope (ESEM) chemical vapor deposition (CVD) synthesis observes the real-time nucleation, assembly, delamination, and self-termination of dense ($>$ 10$^9$ CNT/cm$^2$), tall ($>$ 100 $\\mu$m) CNT forests in real time.","Forest synthesis is continuously observed from nucleation to self-termination.","Assembly forces generated near the substrate detach CNTs from the substrate, which simulation suggests requires approximately 10 nN of tensile force.","Delamination initiates at both the CNT-catalyst and the catalyst-substrate interfaces, indicating multiple delamination mechanism.","Digital image correlation applied to SEM image sequences measures time-invariant strain within growing forests, indicating that forests grow as rigid bodies after liftoff.","The Meta CoTracker algorithm measured CNT growth rates reduce from 50 nm/sec to full termination over 150 seconds.","This work provides a robust strategy to observe and measure CVD material synthesis in-situ using ESEM.","The method is uniquely suited to observe population-based phenomena at both nanometer spatial resolution and at a highly scalable field of view."],"url":"http://arxiv.org/abs/2402.19336v1","category":"physics.app-ph"}
{"created":"2024-02-29 16:33:12","title":"A Novel Approach to Industrial Defect Generation through Blended Latent Diffusion Model with Online Adaptation","abstract":"Effectively addressing the challenge of industrial Anomaly Detection (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts. This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance. The proposed method tailors the blended latent diffusion model for defect sample generation, employing a diffusion model to generate defective samples in the latent space. A feature editing process, controlled by a \"trimap\" mask and text prompts, refines the generated samples. The image generation inference process is structured into three stages: a free diffusion stage, an editing diffusion stage, and an online decoder adaptation stage. This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set. Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively. The implementation code of this work can be found at the GitHub repository https://github.com/GrandpaXun242/AdaBLDM.git","sentences":["Effectively addressing the challenge of industrial Anomaly Detection (AD) necessitates an ample supply of defective samples, a constraint often hindered by their scarcity in industrial contexts.","This paper introduces a novel algorithm designed to augment defective samples, thereby enhancing AD performance.","The proposed method tailors the blended latent diffusion model for defect sample generation, employing a diffusion model to generate defective samples in the latent space.","A feature editing process, controlled by a \"trimap\" mask and text prompts, refines the generated samples.","The image generation inference process is structured into three stages: a free diffusion stage, an editing diffusion stage, and an online decoder adaptation stage.","This sophisticated inference strategy yields high-quality synthetic defective samples with diverse pattern variations, leading to significantly improved AD accuracies based on the augmented training set.","Specifically, on the widely recognized MVTec AD dataset, the proposed method elevates the state-of-the-art (SOTA) performance of AD with augmented data by 1.5%, 1.9%, and 3.1% for AD metrics AP, IAP, and IAP90, respectively.","The implementation code of this work can be found at the GitHub repository https://github.com/GrandpaXun242/AdaBLDM.git"],"url":"http://arxiv.org/abs/2402.19330v1","category":"cs.CV"}
{"created":"2024-02-29 16:31:54","title":"Social Links vs. Language Barriers: Decoding the Global Spread of Streaming Content","abstract":"The development of the internet has allowed for the global distribution of content, redefining media communication and property structures through various streaming platforms. Previous studies successfully clarified the factors contributing to trends in each streaming service, yet the similarities and differences between platforms are commonly unexplored; moreover, the influence of social connections and cultural similarity is usually overlooked. We hereby examine the social aspects of three significant streaming services--Netflix, Spotify, and YouTube--with an emphasis on the dissemination of content across countries. Using two-year-long trending chart datasets, we find that streaming content can be divided into two types: video-oriented (Netflix) and audio-oriented (Spotify). This characteristic is differentiated by accounting for the significance of social connectedness and linguistic similarity: audio-oriented content travels via social links, but video-oriented content tends to spread throughout linguistically akin countries. Interestingly, user-generated contents, YouTube, exhibits a dual characteristic by integrating both visual and auditory characteristics, indicating the platform is evolving into unique medium rather than simply residing a midpoint between video and audio media.","sentences":["The development of the internet has allowed for the global distribution of content, redefining media communication and property structures through various streaming platforms.","Previous studies successfully clarified the factors contributing to trends in each streaming service, yet the similarities and differences between platforms are commonly unexplored; moreover, the influence of social connections and cultural similarity is usually overlooked.","We hereby examine the social aspects of three significant streaming services--Netflix, Spotify, and YouTube--with an emphasis on the dissemination of content across countries.","Using two-year-long trending chart datasets, we find that streaming content can be divided into two types: video-oriented (Netflix) and audio-oriented (Spotify).","This characteristic is differentiated by accounting for the significance of social connectedness and linguistic similarity: audio-oriented content travels via social links, but video-oriented content tends to spread throughout linguistically akin countries.","Interestingly, user-generated contents, YouTube, exhibits a dual characteristic by integrating both visual and auditory characteristics, indicating the platform is evolving into unique medium rather than simply residing a midpoint between video and audio media."],"url":"http://arxiv.org/abs/2402.19329v1","category":"physics.soc-ph"}
{"created":"2024-02-29 16:29:53","title":"Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction","abstract":"Whole Slide Image (WSI) classification is often formulated as a Multiple Instance Learning (MIL) problem. Recently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification. However, existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks. Additionally, processing high-resolution WSIs can be computationally expensive. In this paper, we propose a novel \"Fine-grained Visual-Semantic Interaction\" (FiVE) framework for WSI classification. It is designed to enhance the model's generalizability by leveraging the interplay between localized visual patterns and fine-grained pathological semantics. Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports. The output descriptions are then reconstructed into fine-grained labels used for training. By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and augments generalization capabilities significantly. Furthermore, given that pathological visual patterns are redundantly distributed across tissue slices, we sample a subset of visual instances during training. Our method demonstrates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accuracy in few-shot experiments.","sentences":["Whole Slide Image (WSI) classification is often formulated as a Multiple Instance Learning (MIL) problem.","Recently, Vision-Language Models (VLMs) have demonstrated remarkable performance in WSI classification.","However, existing methods leverage coarse-grained pathogenetic descriptions for visual representation supervision, which are insufficient to capture the complex visual appearance of pathogenetic images, hindering the generalizability of models on diverse downstream tasks.","Additionally, processing high-resolution WSIs can be computationally expensive.","In this paper, we propose a novel \"Fine-grained Visual-Semantic Interaction\" (FiVE) framework for WSI classification.","It is designed to enhance the model's generalizability by leveraging the interplay between localized visual patterns and fine-grained pathological semantics.","Specifically, with meticulously designed queries, we start by utilizing a large language model to extract fine-grained pathological descriptions from various non-standardized raw reports.","The output descriptions are then reconstructed into fine-grained labels used for training.","By introducing a Task-specific Fine-grained Semantics (TFS) module, we enable prompts to capture crucial visual information in WSIs, which enhances representation learning and augments generalization capabilities significantly.","Furthermore, given that pathological visual patterns are redundantly distributed across tissue slices, we sample a subset of visual instances during training.","Our method demonstrates robust generalizability and strong transferability, dominantly outperforming the counterparts on the TCGA Lung Cancer dataset with at least 9.19% higher accuracy in few-shot experiments."],"url":"http://arxiv.org/abs/2402.19326v1","category":"cs.CV"}
{"created":"2024-02-29 16:27:59","title":"Verification of Neural Networks' Global Robustness","abstract":"Neural networks are successful in various applications but are also susceptible to adversarial attacks. To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation. While successful, local robustness cannot generalize to unseen inputs. Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification. In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers. We introduce VHAGaR, an anytime verifier for computing this bound. VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the perturbation or network computation and generalizing adversarial attacks to unknown inputs. We evaluate VHAGaR on several datasets and classifiers and show that, given a three hour timeout, the average gap between the lower and upper bound on the minimal globally robust bound computed by VHAGaR is 1.9, while the gap of an existing global robustness verifier is 154.7. Moreover, VHAGaR is 130.6x faster than this verifier. Our results further indicate that leveraging dependencies and adversarial attacks makes VHAGaR 78.6x faster.","sentences":["Neural networks are successful in various applications but are also susceptible to adversarial attacks.","To show the safety of network classifiers, many verifiers have been introduced to reason about the local robustness of a given input to a given perturbation.","While successful, local robustness cannot generalize to unseen inputs.","Several works analyze global robustness properties, however, neither can provide a precise guarantee about the cases where a network classifier does not change its classification.","In this work, we propose a new global robustness property for classifiers aiming at finding the minimal globally robust bound, which naturally extends the popular local robustness property for classifiers.","We introduce VHAGaR, an anytime verifier for computing this bound.","VHAGaR relies on three main ideas: encoding the problem as a mixed-integer programming and pruning the search space by identifying dependencies stemming from the perturbation or network computation and generalizing adversarial attacks to unknown inputs.","We evaluate VHAGaR on several datasets and classifiers and show that, given a three hour timeout, the average gap between the lower and upper bound on the minimal globally robust bound computed by VHAGaR is 1.9, while the gap of an existing global robustness verifier is 154.7.","Moreover, VHAGaR is 130.6x faster than this verifier.","Our results further indicate that leveraging dependencies and adversarial attacks makes VHAGaR 78.6x faster."],"url":"http://arxiv.org/abs/2402.19322v1","category":"cs.LG"}
{"created":"2024-02-29 16:24:19","title":"Attacks Against Mobility Prediction in 5G Networks","abstract":"The $5^{th}$ generation of mobile networks introduces a new Network Function (NF) that was not present in previous generations, namely the Network Data Analytics Function (NWDAF). Its primary objective is to provide advanced analytics services to various entities within the network and also towards external application services in the 5G ecosystem. One of the key use cases of NWDAF is mobility trajectory prediction, which aims to accurately support efficient mobility management of User Equipment (UE) in the network by allocating ``just in time'' necessary network resources. In this paper, we show that there are potential mobility attacks that can compromise the accuracy of these predictions. In a semi-realistic scenario with 10,000 subscribers, we demonstrate that an adversary equipped with the ability to hijack cellular mobile devices and clone them can significantly reduce the prediction accuracy from 75\\% to 40\\% using just 100 adversarial UEs. While a defense mechanism largely depends on the attack and the mobility types in a particular area, we prove that a basic KMeans clustering is effective in distinguishing legitimate and adversarial UEs.","sentences":["The $5^{th}$ generation of mobile networks introduces a new Network Function (NF) that was not present in previous generations, namely the Network Data Analytics Function (NWDAF).","Its primary objective is to provide advanced analytics services to various entities within the network and also towards external application services in the 5G ecosystem.","One of the key use cases of NWDAF is mobility trajectory prediction, which aims to accurately support efficient mobility management of User Equipment (UE) in the network by allocating ``just in time'' necessary network resources.","In this paper, we show that there are potential mobility attacks that can compromise the accuracy of these predictions.","In a semi-realistic scenario with 10,000 subscribers, we demonstrate that an adversary equipped with the ability to hijack cellular mobile devices and clone them can significantly reduce the prediction accuracy from 75\\% to 40\\% using just 100 adversarial UEs.","While a defense mechanism largely depends on the attack and the mobility types in a particular area, we prove that a basic KMeans clustering is effective in distinguishing legitimate and adversarial UEs."],"url":"http://arxiv.org/abs/2402.19319v1","category":"cs.CR"}
{"created":"2024-02-29 16:18:53","title":"Calibrating gravitational-wave search algorithms with conformal prediction","abstract":"In astronomy, we frequently face the decision problem: does this data contain a signal? Typically, a statistical approach is used, which requires a threshold. The choice of threshold presents a common challenge in settings where signals and noise must be delineated, but their distributions overlap. Gravitational-wave astronomy, which has gone from the first discovery to catalogues of hundreds of events in less than a decade, presents a fascinating case study. For signals from colliding compact objects, the field has evolved from a frequentist to a Bayesian methodology. However, the issue of choosing a threshold and validating noise contamination in a catalogue persists. Confusion and debate often arise due to the misapplication of statistical concepts, the complicated nature of the detection statistics, and the inclusion of astrophysical background models. We introduce Conformal Prediction (CP), a framework developed in Machine Learning to provide distribution-free uncertainty quantification to point predictors. We show that CP can be viewed as an extension of the traditional statistical frameworks whereby thresholds are calibrated such that the uncertainty intervals are statistically rigorous and the error rate can be validated. Moreover, we discuss how CP offers a framework to optimally build a meta-pipeline combining the outputs from multiple independent searches. We introduce CP with a toy cosmic-ray detector, which captures the salient features of most astrophysical search problems and allows us to demonstrate the features of CP in a simple context. We then apply the approach to a recent gravitational-wave Mock Data Challenge using multiple search algorithms for compact binary coalescence signals in interferometric gravitational-wave data. Finally, we conclude with a discussion on the future potential of the method for gravitational-wave astronomy.","sentences":["In astronomy, we frequently face the decision problem: does this data contain a signal?","Typically, a statistical approach is used, which requires a threshold.","The choice of threshold presents a common challenge in settings where signals and noise must be delineated, but their distributions overlap.","Gravitational-wave astronomy, which has gone from the first discovery to catalogues of hundreds of events in less than a decade, presents a fascinating case study.","For signals from colliding compact objects, the field has evolved from a frequentist to a Bayesian methodology.","However, the issue of choosing a threshold and validating noise contamination in a catalogue persists.","Confusion and debate often arise due to the misapplication of statistical concepts, the complicated nature of the detection statistics, and the inclusion of astrophysical background models.","We introduce Conformal Prediction (CP), a framework developed in Machine Learning to provide distribution-free uncertainty quantification to point predictors.","We show that CP can be viewed as an extension of the traditional statistical frameworks whereby thresholds are calibrated such that the uncertainty intervals are statistically rigorous and the error rate can be validated.","Moreover, we discuss how CP offers a framework to optimally build a meta-pipeline combining the outputs from multiple independent searches.","We introduce CP with a toy cosmic-ray detector, which captures the salient features of most astrophysical search problems and allows us to demonstrate the features of CP in a simple context.","We then apply the approach to a recent gravitational-wave Mock Data Challenge using multiple search algorithms for compact binary coalescence signals in interferometric gravitational-wave data.","Finally, we conclude with a discussion on the future potential of the method for gravitational-wave astronomy."],"url":"http://arxiv.org/abs/2402.19313v1","category":"gr-qc"}
{"created":"2024-02-29 16:16:56","title":"Basolateral mechanics prevents rigidity transition in epithelial monolayers","abstract":"The mechanics of epithelial tissues, which is governed by forces generated in various cell domains, is often investigated using two-dimensional models that account for the apically-positioned actomyosin structures but neglect basolateral mechanics. We employ a more detailed three-dimensional model to study how lateral surface tensions affect the structure and rigidity of such tissues. We find that cells are apicobasally asymmetric, with one side appearing more ordered than the other depending on cell target perimeter. In contrast to the 2D model, which predicts a rigidity transition at large target perimeters, tissues in the 3D model remain solid-like across all parameter space.","sentences":["The mechanics of epithelial tissues, which is governed by forces generated in various cell domains, is often investigated using two-dimensional models that account for the apically-positioned actomyosin structures but neglect basolateral mechanics.","We employ a more detailed three-dimensional model to study how lateral surface tensions affect the structure and rigidity of such tissues.","We find that cells are apicobasally asymmetric, with one side appearing more ordered than the other depending on cell target perimeter.","In contrast to the 2D model, which predicts a rigidity transition at large target perimeters, tissues in the 3D model remain solid-like across all parameter space."],"url":"http://arxiv.org/abs/2402.19312v1","category":"cond-mat.soft"}
{"created":"2024-02-29 16:15:43","title":"Anisotropy-Induced Spin Parity Effects in an Antiferromagnetic Spin Chain","abstract":"Spin parity effects refer to those special situations where a dichotomy in the physical behavior of a system arises, solely depending on whether the relevant spin quantum number is integral or half-odd integral. As is the case with the Haldane conjecture in antiferromagnetic spin chains, their pursuit often provides deep insights and invokes new developments in quantum condensed matter physics. Here we put forth a simple and general scheme for generating such effects in any spatial dimension through the use of anisotropic interactions, a setup within reasonable reach of state-of-the-art cold-atom implementations. We demonstrate its utility through a detailed analysis of the magnetization behavior of a specific one-dimensional spin chain model -- an anisotropic antiferromagnet in a transverse magnetic field, unraveling along the way the quantum origin of finite-size effects observed in the magnetization curve that had previously been noted but not clearly understood.","sentences":["Spin parity effects refer to those special situations where a dichotomy in the physical behavior of a system arises, solely depending on whether the relevant spin quantum number is integral or half-odd integral.","As is the case with the Haldane conjecture in antiferromagnetic spin chains, their pursuit often provides deep insights and invokes new developments in quantum condensed matter physics.","Here we put forth a simple and general scheme for generating such effects in any spatial dimension through the use of anisotropic interactions, a setup within reasonable reach of state-of-the-art cold-atom implementations.","We demonstrate its utility through a detailed analysis of the magnetization behavior of a specific one-dimensional spin chain model -- an anisotropic antiferromagnet in a transverse magnetic field, unraveling along the way the quantum origin of finite-size effects observed in the magnetization curve that had previously been noted but not clearly understood."],"url":"http://arxiv.org/abs/2402.19311v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-29 16:15:41","title":"Some Remarks on Wang-Yau Quasi-Local Mass","abstract":"We review Wang-Yau quasi-local definitions along the line of gravitational Hamiltonian. This makes clear the connection and difference between Wang-Yau definition and Brown-York or even global ADM definition. We make a brief comment on admissibility condition in Wang-Yau quasi-lcoal mass. We extend the positivity proof for Wang-Yau quasi-local energy to allow possible presence of strictly stable apparent horizons through establishing solvability of Dirac equation in certain 3-manifolds that possess cylindrical ends, as in the case of Jang's graph blowing up at marginally outer trapped surfaces.","sentences":["We review Wang-Yau quasi-local definitions along the line of gravitational Hamiltonian.","This makes clear the connection and difference between Wang-Yau definition and Brown-York or even global ADM definition.","We make a brief comment on admissibility condition in Wang-Yau quasi-lcoal mass.","We extend the positivity proof for Wang-Yau quasi-local energy to allow possible presence of strictly stable apparent horizons through establishing solvability of Dirac equation in certain 3-manifolds that possess cylindrical ends, as in the case of Jang's graph blowing up at marginally outer trapped surfaces."],"url":"http://arxiv.org/abs/2402.19310v1","category":"gr-qc"}
{"created":"2024-02-29 16:15:04","title":"Solucion exacta para un modelo simplificado de un sistema cuantico abierto","abstract":"A simplified model of an initially excited oscillator as a quantum system interacting with a large number of oscillators acting as a reservoir has been developed in this work. All these oscillators are in their ground state uncoupled each other and at the limit of the weak coupling between the system and the reservoir. This system could be an oscillator excited in a microcavity that interacts with the vacuum's electromagnetic field at zero temperature. This work's primary goal is to obtain the system's density matrix's exact solution in these conditions. The general approach calculates all oscillators' evolution as a single isolated entity using the evolution operator. Starting from a total initial state that can be factored between the system and the reservoir, the evolution is unitary, and the partial trace is taken in all the degrees of freedom of the environment to obtain the density matrix of the system at any instant of time; this procedure requires diagonalizing Hamiltonian. The results are evaluated for a reservoir of N=1000 oscillators, particular values of the coupling force, and ohmic order of the spectral density, contrasted with the corresponding Markovian solution described in section [2.3.1].","sentences":["A simplified model of an initially excited oscillator as a quantum system interacting with a large number of oscillators acting as a reservoir has been developed in this work.","All these oscillators are in their ground state uncoupled each other and at the limit of the weak coupling between the system and the reservoir.","This system could be an oscillator excited in a microcavity that interacts with the vacuum's electromagnetic field at zero temperature.","This work's primary goal is to obtain the system's density matrix's exact solution in these conditions.","The general approach calculates all oscillators' evolution as a single isolated entity using the evolution operator.","Starting from a total initial state that can be factored between the system and the reservoir, the evolution is unitary, and the partial trace is taken in all the degrees of freedom of the environment to obtain the density matrix of the system at any instant of time; this procedure requires diagonalizing Hamiltonian.","The results are evaluated for a reservoir of N=1000 oscillators, particular values of the coupling force, and ohmic order of the spectral density, contrasted with the corresponding Markovian solution described in section [2.3.1]."],"url":"http://arxiv.org/abs/2402.19307v1","category":"quant-ph"}
{"created":"2024-02-29 16:09:12","title":"DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly","abstract":"Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems. In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.). We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation. Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph. Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose. DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation. Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving. Code available at https://github.com/IIT-PAVIS/DiffAssemble","sentences":["Reassembly tasks play a fundamental role in many fields and multiple approaches exist to solve specific reassembly problems.","In this context, we posit that a general unified model can effectively address them all, irrespective of the input data type (images, 3D, etc.).","We introduce DiffAssemble, a Graph Neural Network (GNN)-based architecture that learns to solve reassembly tasks using a diffusion model formulation.","Our method treats the elements of a set, whether pieces of 2D patch or 3D object fragments, as nodes of a spatial graph.","Training is performed by introducing noise into the position and rotation of the elements and iteratively denoising them to reconstruct the coherent initial pose.","DiffAssemble achieves state-of-the-art (SOTA) results in most 2D and 3D reassembly tasks and is the first learning-based approach that solves 2D puzzles for both rotation and translation.","Furthermore, we highlight its remarkable reduction in run-time, performing 11 times faster than the quickest optimization-based method for puzzle solving.","Code available at https://github.com/IIT-PAVIS/DiffAssemble"],"url":"http://arxiv.org/abs/2402.19302v1","category":"cs.CV"}
{"created":"2024-02-29 16:07:35","title":"$\\mathrm{SL}_2$-like Properties of Matrices Over Noncommutative Rings and Generalizations of Markov Numbers","abstract":"We study $2\\times 2$ matrices over noncommutative rings with anti-involution, with a special focus on the symplectic group $\\mathrm{Sp}_2(\\mathcal{A},\\sigma)$. We define traces and determinants of such matrices and use them to prove a Cayley Hamilton identity and trace relations which generalize well known relations for elements of $\\mathrm{SL}_2(R)$ over a commutative ring. We compare the structure of elements of $\\mathrm{Sp}_2(\\mathcal{A},\\sigma)$ with Manin matrices over general noncommutative rings; this naturally leads to a quantization $\\mathrm{Sp}_2(\\mathcal{A},\\sigma)_q$. In contrast to the usual definition of the quantum group as a deformation of the ring of matrix functions on $\\mathrm{SL}_2(R)$, this quantization produces a group of matrices over a new noncommutative ring with involution. We finish the comparison by constructing a generalization of a Hopf algebra structure on the noncommutative ring of matrix functions of our quantum group. Finally, we use the noncommutative surface-type cluster algebras of Berenstein and Retakh to give a geometric interpretation of our Hopf algebra structure and to produce noncommutative generalizations of Markov numbers over many rings with involution including the complex numbers, dual numbers, matrix rings, and group rings.","sentences":["We study $2\\times 2$ matrices over noncommutative rings with anti-involution, with a special focus on the symplectic group $\\mathrm{Sp}_2(\\mathcal{A},\\sigma)$. We define traces and determinants of such matrices and use them to prove a Cayley Hamilton identity and trace relations which generalize well known relations for elements of $\\mathrm{SL}_2(R)$ over a commutative ring.","We compare the structure of elements of $\\mathrm{Sp}_2(\\mathcal{A},\\sigma)$ with Manin matrices over general noncommutative rings; this naturally leads to a quantization $\\mathrm{Sp}_2(\\mathcal{A},\\sigma)_q$. In contrast to the usual definition of the quantum group as a deformation of the ring of matrix functions on $\\mathrm{SL}_2(R)$, this quantization produces a group of matrices over a new noncommutative ring with involution.","We finish the comparison by constructing a generalization of a Hopf algebra structure on the noncommutative ring of matrix functions of our quantum group.","Finally, we use the noncommutative surface-type cluster algebras of Berenstein and Retakh to give a geometric interpretation of our Hopf algebra structure and to produce noncommutative generalizations of Markov numbers over many rings with involution including the complex numbers, dual numbers, matrix rings, and group rings."],"url":"http://arxiv.org/abs/2402.19300v1","category":"math.RA"}
{"created":"2024-02-29 16:07:22","title":"RL-GPT: Integrating Reinforcement Learning and Code-as-policy","abstract":"Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control. In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL). To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent. The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks. This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline. Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency. In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090. Additionally, it achieves SOTA performance across all designated MineDojo tasks.","sentences":["Large Language Models (LLMs) have demonstrated proficiency in utilizing various tools by coding, yet they face limitations in handling intricate logic and precise control.","In embodied tasks, high-level planning is amenable to direct coding, while low-level actions often necessitate task-specific refinement, such as Reinforcement Learning (RL).","To seamlessly integrate both modalities, we introduce a two-level hierarchical framework, RL-GPT, comprising a slow agent and a fast agent.","The slow agent analyzes actions suitable for coding, while the fast agent executes coding tasks.","This decomposition effectively focuses each agent on specific tasks, proving highly efficient within our pipeline.","Our approach outperforms traditional RL methods and existing GPT agents, demonstrating superior efficiency.","In the Minecraft game, it rapidly obtains diamonds within a single day on an RTX3090.","Additionally, it achieves SOTA performance across all designated MineDojo tasks."],"url":"http://arxiv.org/abs/2402.19299v1","category":"cs.AI"}
{"created":"2024-02-29 16:06:36","title":"Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing","abstract":"Face Anti-Spoofing (FAS) is crucial for securing face recognition systems against presentation attacks. With advancements in sensor manufacture and multi-modal learning techniques, many multi-modal FAS approaches have emerged. However, they face challenges in generalizing to unseen attacks and deployment conditions. These challenges arise from (1) modality unreliability, where some modality sensors like depth and infrared undergo significant domain shifts in varying environments, leading to the spread of unreliable information during cross-modal feature fusion, and (2) modality imbalance, where training overly relies on a dominant modality hinders the convergence of others, reducing effectiveness against attack types that are indistinguishable sorely using the dominant modality. To address modality unreliability, we propose the Uncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detected regions within each modality and suppress the impact of unreliable regions on other modalities. For modality imbalance, we propose a Rebalanced Modality Gradient Modulation (ReGrad) strategy to rebalance the convergence speed of all modalities by adaptively adjusting their gradients. Besides, we provide the first large-scale benchmark for evaluating multi-modal FAS performance under domain generalization scenarios. Extensive experiments demonstrate that our method outperforms state-of-the-art methods. Source code and protocols will be released on https://github.com/OMGGGGG/mmdg.","sentences":["Face Anti-Spoofing (FAS) is crucial for securing face recognition systems against presentation attacks.","With advancements in sensor manufacture and multi-modal learning techniques, many multi-modal FAS approaches have emerged.","However, they face challenges in generalizing to unseen attacks and deployment conditions.","These challenges arise from (1) modality unreliability, where some modality sensors like depth and infrared undergo significant domain shifts in varying environments, leading to the spread of unreliable information during cross-modal feature fusion, and (2) modality imbalance, where training overly relies on a dominant modality hinders the convergence of others, reducing effectiveness against attack types that are indistinguishable sorely using the dominant modality.","To address modality unreliability, we propose the Uncertainty-Guided Cross-Adapter (U-Adapter) to recognize unreliably detected regions within each modality and suppress the impact of unreliable regions on other modalities.","For modality imbalance, we propose a Rebalanced Modality Gradient Modulation (ReGrad) strategy to rebalance the convergence speed of all modalities by adaptively adjusting their gradients.","Besides, we provide the first large-scale benchmark for evaluating multi-modal FAS performance under domain generalization scenarios.","Extensive experiments demonstrate that our method outperforms state-of-the-art methods.","Source code and protocols will be released on https://github.com/OMGGGGG/mmdg."],"url":"http://arxiv.org/abs/2402.19298v1","category":"cs.CV"}
{"created":"2024-02-29 15:59:42","title":"An AI based Digital Score of Tumour-Immune Microenvironment Predicts Benefit to Maintenance Immunotherapy in Advanced Oesophagogastric Adenocarcinoma","abstract":"Gastric and oesophageal (OG) cancers are the leading causes of cancer mortality worldwide. In OG cancers, recent studies have showed that PDL1 immune checkpoint inhibitors (ICI) in combination with chemotherapy improves patient survival. However, our understanding of the tumour immune microenvironment in OG cancers remains limited. In this study, we interrogate multiplex immunofluorescence (mIF) images taken from patients with advanced Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict the efficacy of the treatment and to explore the biological basis of patients responding to maintenance durvalumab (PDL1 inhibitor). Our proposed Artificial Intelligence (AI) based marker successfully identified responder from non-responder (p < 0.05) as well as those who could potentially benefit from ICI with statistical significance (p < 0.05) for both progression free and overall survival. Our findings suggest that T cells that express FOXP3 seem to heavily influence the patient treatment response and survival outcome. We also observed that higher levels of CD8+PD1+ cells are consistently linked to poor prognosis for both OS and PFS, regardless of ICI.","sentences":["Gastric and oesophageal (OG) cancers are the leading causes of cancer mortality worldwide.","In OG cancers, recent studies have showed that PDL1 immune checkpoint inhibitors (ICI) in combination with chemotherapy improves patient survival.","However, our understanding of the tumour immune microenvironment in OG cancers remains limited.","In this study, we interrogate multiplex immunofluorescence (mIF) images taken from patients with advanced Oesophagogastric Adenocarcinoma (OGA) who received first-line fluoropyrimidine and platinum-based chemotherapy in the PLATFORM trial (NCT02678182) to predict the efficacy of the treatment and to explore the biological basis of patients responding to maintenance durvalumab (PDL1 inhibitor).","Our proposed Artificial Intelligence (AI) based marker successfully identified responder from non-responder (p < 0.05) as well as those who could potentially benefit from ICI with statistical significance (p < 0.05) for both progression free and overall survival.","Our findings suggest that T cells that express FOXP3 seem to heavily influence the patient treatment response and survival outcome.","We also observed that higher levels of CD8+PD1+ cells are consistently linked to poor prognosis for both OS and PFS, regardless of ICI."],"url":"http://arxiv.org/abs/2402.19296v1","category":"cs.CV"}
{"created":"2024-02-29 15:58:16","title":"Anomaly Detection in Offshore Wind Turbine Structures using Hierarchical Bayesian Modelling","abstract":"Population-based structural health monitoring (PBSHM), aims to share information between members of a population. An offshore wind (OW) farm could be considered as a population of nominally-identical wind-turbine structures. However, benign variations exist among members, such as geometry, sea-bed conditions and temperature differences. These factors could influence structural properties and therefore the dynamic response, making it more difficult to detect structural problems via traditional SHM techniques. This paper explores the use of a hierarchical Bayesian model to infer expected soil stiffness distributions at both population and local levels, as a basis to perform anomaly detection, in the form of scour, for new and existing turbines. To do this, observations of natural frequency will be generated as though they are from a small population of wind turbines. Differences between individual observations will be introduced by postulating distributions over the soil stiffness and measurement noise, as well as reducing soil depth (to represent scour), in the case of anomaly detection.","sentences":["Population-based structural health monitoring (PBSHM), aims to share information between members of a population.","An offshore wind (OW) farm could be considered as a population of nominally-identical wind-turbine structures.","However, benign variations exist among members, such as geometry, sea-bed conditions and temperature differences.","These factors could influence structural properties and therefore the dynamic response, making it more difficult to detect structural problems via traditional SHM techniques.","This paper explores the use of a hierarchical Bayesian model to infer expected soil stiffness distributions at both population and local levels, as a basis to perform anomaly detection, in the form of scour, for new and existing turbines.","To do this, observations of natural frequency will be generated as though they are from a small population of wind turbines.","Differences between individual observations will be introduced by postulating distributions over the soil stiffness and measurement noise, as well as reducing soil depth (to represent scour), in the case of anomaly detection."],"url":"http://arxiv.org/abs/2402.19295v1","category":"cs.LG"}
{"created":"2024-02-29 15:57:09","title":"Degradation Modeling and Prognostic Analysis Under Unknown Failure Modes","abstract":"Operating units often experience various failure modes in complex systems, leading to distinct degradation paths. Relying on a prognostic model trained on a single failure mode may lead to poor generalization performance across multiple failure modes. Therefore, accurately identifying the failure mode is of critical importance. Current prognostic approaches either ignore failure modes during degradation or assume known failure mode labels, which can be challenging to acquire in practice. Moreover, the high dimensionality and complex relations of sensor signals make it challenging to identify the failure modes accurately. To address these issues, we propose a novel failure mode diagnosis method that leverages a dimension reduction technique called UMAP (Uniform Manifold Approximation and Projection) to project and visualize each unit's degradation trajectory into a lower dimension. Then, using these degradation trajectories, we develop a time series-based clustering method to identify the training units' failure modes. Finally, we introduce a monotonically constrained prognostic model to predict the failure mode labels and RUL of the test units simultaneously using the obtained failure modes of the training units. The proposed prognostic model provides failure mode-specific RUL predictions while preserving the monotonic property of the RUL predictions across consecutive time steps. We evaluate the proposed model using a case study with the aircraft gas turbine engine dataset.","sentences":["Operating units often experience various failure modes in complex systems, leading to distinct degradation paths.","Relying on a prognostic model trained on a single failure mode may lead to poor generalization performance across multiple failure modes.","Therefore, accurately identifying the failure mode is of critical importance.","Current prognostic approaches either ignore failure modes during degradation or assume known failure mode labels, which can be challenging to acquire in practice.","Moreover, the high dimensionality and complex relations of sensor signals make it challenging to identify the failure modes accurately.","To address these issues, we propose a novel failure mode diagnosis method that leverages a dimension reduction technique called UMAP (Uniform Manifold Approximation and Projection) to project and visualize each unit's degradation trajectory into a lower dimension.","Then, using these degradation trajectories, we develop a time series-based clustering method to identify the training units' failure modes.","Finally, we introduce a monotonically constrained prognostic model to predict the failure mode labels and RUL of the test units simultaneously using the obtained failure modes of the training units.","The proposed prognostic model provides failure mode-specific RUL predictions while preserving the monotonic property of the RUL predictions across consecutive time steps.","We evaluate the proposed model using a case study with the aircraft gas turbine engine dataset."],"url":"http://arxiv.org/abs/2402.19294v1","category":"cs.LG"}
{"created":"2024-02-29 15:55:29","title":"Thermodynamic Trade-Off Relations in Trace-Preserving Completely Positive Maps and Its Observation on a Quantum Computer","abstract":"Thermodynamic trade-off relations reveal the costs inherent in quantum information processing. Using the framework of trace-preserving completely positive maps, we derive a generalized quantum thermodynamic uncertainty relation applicable to arbitrary observables. Exploiting this relation, we establish multiple trade-offs that connect thermodynamic costs with the precision, evolution of observables, and quantum time correlations. We experimentally demonstrate the trade-off relations using superconducting qubits on a quantum computer to verify the theory. The empirical results not only show remarkable agreement with the theoretical predictions but also reveal that the precision of an observable and the quantum time correlator are tightly constrained by the thermodynamic cost. Our findings highlight the relevance of the thermodynamic trade-off relations in current quantum technologies.","sentences":["Thermodynamic trade-off relations reveal the costs inherent in quantum information processing.","Using the framework of trace-preserving completely positive maps, we derive a generalized quantum thermodynamic uncertainty relation applicable to arbitrary observables.","Exploiting this relation, we establish multiple trade-offs that connect thermodynamic costs with the precision, evolution of observables, and quantum time correlations.","We experimentally demonstrate the trade-off relations using superconducting qubits on a quantum computer to verify the theory.","The empirical results not only show remarkable agreement with the theoretical predictions but also reveal that the precision of an observable and the quantum time correlator are tightly constrained by the thermodynamic cost.","Our findings highlight the relevance of the thermodynamic trade-off relations in current quantum technologies."],"url":"http://arxiv.org/abs/2402.19293v1","category":"quant-ph"}
{"created":"2024-02-29 15:55:10","title":"Fundamental Limits of Throughput and Availability: Applications to prophet inequalities & transaction fee mechanism design","abstract":"This paper studies the fundamental limits of availability and throughput for independent and heterogeneous demands of a limited resource. Availability is the probability that the demands are below the capacity of the resource. Throughput is the expected fraction of the resource that is utilized by the demands. We offer a concentration inequality generator that gives lower bounds on feasible availability and throughput pairs with a given capacity and independent but not necessarily identical distributions of up-to-unit demands. We show that availability and throughput cannot both be poor. These bounds are analogous to tail inequalities on sums of independent random variables, but hold throughout the support of the demand distribution. This analysis gives analytically tractable bounds supporting the unit-demand characterization of Chawla, Devanur, and Lykouris (2023) and generalizes to up-to-unit demands. Our bounds also provide an approach towards improved multi-unit prophet inequalities (Hajiaghayi, Kleinberg, and Sandholm, 2007). They have applications to transaction fee mechanism design (for blockchains) where high availability limits the probability of profitable user-miner coalitions (Chung and Shi, 2023).","sentences":["This paper studies the fundamental limits of availability and throughput for independent and heterogeneous demands of a limited resource.","Availability is the probability that the demands are below the capacity of the resource.","Throughput is the expected fraction of the resource that is utilized by the demands.","We offer a concentration inequality generator that gives lower bounds on feasible availability and throughput pairs with a given capacity and independent but not necessarily identical distributions of up-to-unit demands.","We show that availability and throughput cannot both be poor.","These bounds are analogous to tail inequalities on sums of independent random variables, but hold throughout the support of the demand distribution.","This analysis gives analytically tractable bounds supporting the unit-demand characterization of Chawla, Devanur, and Lykouris (2023) and generalizes to up-to-unit demands.","Our bounds also provide an approach towards improved multi-unit prophet inequalities (Hajiaghayi, Kleinberg, and Sandholm, 2007).","They have applications to transaction fee mechanism design (for blockchains) where high availability limits the probability of profitable user-miner coalitions (Chung and Shi, 2023)."],"url":"http://arxiv.org/abs/2402.19292v1","category":"cs.GT"}
{"created":"2024-02-29 15:52:59","title":"CAMixerSR: Only Details Need More \"Attention\"","abstract":"To satisfy the rapidly increasing demands on the large image (2K-8K) super-resolution (SR), prevailing methods follow two independent tracks: 1) accelerate existing networks by content-aware routing, and 2) design better super-resolution networks via token mixer refining. Despite directness, they encounter unavoidable defects (e.g., inflexible route or non-discriminative processing) limiting further improvements of quality-complexity trade-off. To erase the drawbacks, we integrate these schemes by proposing a content-aware mixer (CAMixer), which assigns convolution for simple contexts and additional deformable window-attention for sparse textures. Specifically, the CAMixer uses a learnable predictor to generate multiple bootstraps, including offsets for windows warping, a mask for classifying windows, and convolutional attentions for endowing convolution with the dynamic property, which modulates attention to include more useful textures self-adaptively and improves the representation capability of convolution. We further introduce a global classification loss to improve the accuracy of predictors. By simply stacking CAMixers, we obtain CAMixerSR which achieves superior performance on large-image SR, lightweight SR, and omnidirectional-image SR.","sentences":["To satisfy the rapidly increasing demands on the large image (2K-8K) super-resolution (SR), prevailing methods follow two independent tracks: 1) accelerate existing networks by content-aware routing, and 2) design better super-resolution networks via token mixer refining.","Despite directness, they encounter unavoidable defects (e.g., inflexible route or non-discriminative processing) limiting further improvements of quality-complexity trade-off.","To erase the drawbacks, we integrate these schemes by proposing a content-aware mixer (CAMixer), which assigns convolution for simple contexts and additional deformable window-attention for sparse textures.","Specifically, the CAMixer uses a learnable predictor to generate multiple bootstraps, including offsets for windows warping, a mask for classifying windows, and convolutional attentions for endowing convolution with the dynamic property, which modulates attention to include more useful textures self-adaptively and improves the representation capability of convolution.","We further introduce a global classification loss to improve the accuracy of predictors.","By simply stacking CAMixers, we obtain CAMixerSR which achieves superior performance on large-image SR, lightweight SR, and omnidirectional-image SR."],"url":"http://arxiv.org/abs/2402.19289v1","category":"eess.IV"}
{"created":"2024-02-29 15:52:21","title":"StiefelGen: A Simple, Model Agnostic Approach for Time Series Data Augmentation over Riemannian Manifolds","abstract":"Data augmentation is an area of research which has seen active development in many machine learning fields, such as in image-based learning models, reinforcement learning for self driving vehicles, and general noise injection for point cloud data. However, convincing methods for general time series data augmentation still leaves much to be desired, especially since the methods developed for these models do not readily cross-over. Three common approaches for time series data augmentation include: (i) Constructing a physics-based model and then imbuing uncertainty over the coefficient space (for example), (ii) Adding noise to the observed data set(s), and, (iii) Having access to ample amounts of time series data sets from which a robust generative neural network model can be trained. However, for many practical problems that work with time series data in the industry: (i) One usually does not have access to a robust physical model, (ii) The addition of noise can in of itself require large or difficult assumptions (for example, what probability distribution should be used? Or, how large should the noise variance be?), and, (iii) In practice, it can be difficult to source a large representative time series data base with which to train the neural network model for the underlying problem. In this paper, we propose a methodology which attempts to simultaneously tackle all three of these previous limitations to a large extent. The method relies upon the well-studied matrix differential geometry of the Stiefel manifold, as it proposes a simple way in which time series signals can placed on, and then smoothly perturbed over the manifold. We attempt to clarify how this method works by showcasing several potential use cases which in particular work to take advantage of the unique properties of this underlying manifold.","sentences":["Data augmentation is an area of research which has seen active development in many machine learning fields, such as in image-based learning models, reinforcement learning for self driving vehicles, and general noise injection for point cloud data.","However, convincing methods for general time series data augmentation still leaves much to be desired, especially since the methods developed for these models do not readily cross-over.","Three common approaches for time series data augmentation include: (i) Constructing a physics-based model and then imbuing uncertainty over the coefficient space (for example), (ii) Adding noise to the observed data set(s), and, (iii) Having access to ample amounts of time series data sets from which a robust generative neural network model can be trained.","However, for many practical problems that work with time series data in the industry: (i) One usually does not have access to a robust physical model, (ii) The addition of noise can in of itself require large or difficult assumptions (for example, what probability distribution should be used?","Or, how large should the noise variance be?), and, (iii) In practice, it can be difficult to source a large representative time series data base with which to train the neural network model for the underlying problem.","In this paper, we propose a methodology which attempts to simultaneously tackle all three of these previous limitations to a large extent.","The method relies upon the well-studied matrix differential geometry of the Stiefel manifold, as it proposes a simple way in which time series signals can placed on, and then smoothly perturbed over the manifold.","We attempt to clarify how this method works by showcasing several potential use cases which in particular work to take advantage of the unique properties of this underlying manifold."],"url":"http://arxiv.org/abs/2402.19287v1","category":"cs.LG"}
{"created":"2024-02-29 15:49:02","title":"First simultaneous measurement of differential muon-neutrino charged-current cross sections on argon for final states with and without protons using MicroBooNE data","abstract":"We report the first double-differential neutrino-argon cross section measurement made simultaneously for final states with and without protons for the inclusive muon neutrino charged-current interaction channel. The proton kinematics of this channel are further explored with a differential cross section measurement as a function of the leading proton's kinetic energy that extends across the detection threshold. These measurements utilize data collected using the MicroBooNE detector from 6.4$\\times10^{20}$ protons on target from the Fermilab Booster Neutrino Beam with a mean neutrino energy of $\\sim$0.8 GeV. Extensive data-driven model validation utilizing the conditional constraint formalism is employed. This motivates enlarging the uncertainties with an empirical reweighting approach to minimize the possibility of extracting biased cross section results. The extracted nominal flux-averaged cross sections are compared to widely used event generator predictions revealing severe mismodeling of final states without protons for muon neutrino charged-current interactions, possibly from insufficient treatment of final state interactions. These measurements provide a wealth of new information useful for improving event generators which will enhance the sensitivity of precision measurements in neutrino experiments.","sentences":["We report the first double-differential neutrino-argon cross section measurement made simultaneously for final states with and without protons for the inclusive muon neutrino charged-current interaction channel.","The proton kinematics of this channel are further explored with a differential cross section measurement as a function of the leading proton's kinetic energy that extends across the detection threshold.","These measurements utilize data collected using the MicroBooNE detector from 6.4$\\times10^{20}$ protons on target from the Fermilab Booster Neutrino Beam with a mean neutrino energy of $\\sim$0.8 GeV. Extensive data-driven model validation utilizing the conditional constraint formalism is employed.","This motivates enlarging the uncertainties with an empirical reweighting approach to minimize the possibility of extracting biased cross section results.","The extracted nominal flux-averaged cross sections are compared to widely used event generator predictions revealing severe mismodeling of final states without protons for muon neutrino charged-current interactions, possibly from insufficient treatment of final state interactions.","These measurements provide a wealth of new information useful for improving event generators which will enhance the sensitivity of precision measurements in neutrino experiments."],"url":"http://arxiv.org/abs/2402.19281v1","category":"hep-ex"}
{"created":"2024-02-29 15:45:22","title":"Electrically defined quantum dots for bosonic excitons","abstract":"Quantum dots are semiconductor nano-structures where particle motion is confined in all three spatial dimensions. Since their first experimental realization, nanocrystals confining the quanta of polarization waves, termed excitons, have found numerous applications in fields ranging from single photon sources for quantum information processing to commercial displays. A major limitation to further extending the range of potential applications has been the large inhomogeneity in, and lack-of tunability of, exciton energy that is generic to quantum dot materials. Here, we address this challenge by demonstrating electrically-defined quantum dots for excitons in monolayer semiconductors where the discrete exciton energies can be tuned using applied gate voltages. Resonance fluorescence measurements show strong spectral jumps and blinking of these resonances, verifying their zero-dimensional nature. Our work paves the way for realizing quantum confined bosonic modes where nonlinear response would arise exclusively from exciton--exciton interactions.","sentences":["Quantum dots are semiconductor nano-structures where particle motion is confined in all three spatial dimensions.","Since their first experimental realization, nanocrystals confining the quanta of polarization waves, termed excitons, have found numerous applications in fields ranging from single photon sources for quantum information processing to commercial displays.","A major limitation to further extending the range of potential applications has been the large inhomogeneity in, and lack-of tunability of, exciton energy that is generic to quantum dot materials.","Here, we address this challenge by demonstrating electrically-defined quantum dots for excitons in monolayer semiconductors where the discrete exciton energies can be tuned using applied gate voltages.","Resonance fluorescence measurements show strong spectral jumps and blinking of these resonances, verifying their zero-dimensional nature.","Our work paves the way for realizing quantum confined bosonic modes where nonlinear response would arise exclusively from exciton--exciton interactions."],"url":"http://arxiv.org/abs/2402.19278v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 15:45:08","title":"The impact of the explicit representation of convection on the climate of a tidally locked planet in global stretched-mesh simulations","abstract":"Convective processes are crucial in shaping exoplanetary atmospheres but are computationally expensive to simulate directly. A novel technique of simulating moist convection on tidally locked exoplanets is to use a global 3D model with a stretched mesh. This allows us to locally refine the model resolution to 4.7 km and resolve fine-scale convective processes without relying on parameterizations. We explore the impact of mesh stretching on the climate of a slowly rotating TRAPPIST-1e-like planet, assuming it is 1:1 tidally locked. In the stretched-mesh simulation with explicit convection, the climate is 5 K colder and 25% drier than that in the simulations with parameterized convection. This is due to the increased cloud reflectivity and exacerbated by the diminished greenhouse effect due to less water vapor. At the same time, our stretched-mesh simulations reproduce the key characteristics of the global climate of tidally locked rocky exoplanets, without any noticeable numerical artifacts. Our methodology opens an exciting and computationally feasible avenue for improving our understanding of 3D mixing in exoplanetary atmospheres. Our study also demonstrates the feasibility of a global stretched mesh configuration for LFRic-Atmosphere, the next-generation Met Office climate and weather model.","sentences":["Convective processes are crucial in shaping exoplanetary atmospheres but are computationally expensive to simulate directly.","A novel technique of simulating moist convection on tidally locked exoplanets is to use a global 3D model with a stretched mesh.","This allows us to locally refine the model resolution to 4.7 km and resolve fine-scale convective processes without relying on parameterizations.","We explore the impact of mesh stretching on the climate of a slowly rotating TRAPPIST-1e-like planet, assuming it is 1:1 tidally locked.","In the stretched-mesh simulation with explicit convection, the climate is 5 K colder and 25% drier than that in the simulations with parameterized convection.","This is due to the increased cloud reflectivity and exacerbated by the diminished greenhouse effect due to less water vapor.","At the same time, our stretched-mesh simulations reproduce the key characteristics of the global climate of tidally locked rocky exoplanets, without any noticeable numerical artifacts.","Our methodology opens an exciting and computationally feasible avenue for improving our understanding of 3D mixing in exoplanetary atmospheres.","Our study also demonstrates the feasibility of a global stretched mesh configuration for LFRic-Atmosphere, the next-generation Met Office climate and weather model."],"url":"http://arxiv.org/abs/2402.19277v1","category":"astro-ph.EP"}
{"created":"2024-02-29 15:44:00","title":"Modular Blind Video Quality Assessment","abstract":"Blind video quality assessment (BVQA) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services. Contemporary deep learning-based models primarily analyze the video content in its aggressively downsampled format, while being blind to the impact of actual spatial resolution and frame rate on video quality. In this paper, we propose a modular BVQA model, and a method of training it to improve its modularity. Specifically, our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively. During training, spatial and temporal rectifiers are dropped out with some probabilities so as to make the base quality predictor a standalone BVQA model, which should work better with the rectifiers. Extensive experiments on both professionally-generated content and user generated content video databases show that our quality model achieves superior or comparable performance to current methods. Furthermore, the modularity of our model offers a great opportunity to analyze existing video quality databases in terms of their spatial and temporal complexities. Last, our BVQA model is cost-effective to add other quality-relevant video attributes such as dynamic range and color gamut as additional rectifiers.","sentences":["Blind video quality assessment (BVQA) plays a pivotal role in evaluating and improving the viewing experience of end-users across a wide range of video-based platforms and services.","Contemporary deep learning-based models primarily analyze the video content in its aggressively downsampled format, while being blind to the impact of actual spatial resolution and frame rate on video quality.","In this paper, we propose a modular BVQA model, and a method of training it to improve its modularity.","Specifically, our model comprises a base quality predictor, a spatial rectifier, and a temporal rectifier, responding to the visual content and distortion, spatial resolution, and frame rate changes on video quality, respectively.","During training, spatial and temporal rectifiers are dropped out with some probabilities so as to make the base quality predictor a standalone BVQA model, which should work better with the rectifiers.","Extensive experiments on both professionally-generated content and user generated content video databases show that our quality model achieves superior or comparable performance to current methods.","Furthermore, the modularity of our model offers a great opportunity to analyze existing video quality databases in terms of their spatial and temporal complexities.","Last, our BVQA model is cost-effective to add other quality-relevant video attributes such as dynamic range and color gamut as additional rectifiers."],"url":"http://arxiv.org/abs/2402.19276v1","category":"eess.IV"}
{"created":"2024-02-29 15:42:33","title":"Adaptive Testing Environment Generation for Connected and Automated Vehicles with Dense Reinforcement Learning","abstract":"The assessment of safety performance plays a pivotal role in the development and deployment of connected and automated vehicles (CAVs). A common approach involves designing testing scenarios based on prior knowledge of CAVs (e.g., surrogate models), conducting tests in these scenarios, and subsequently evaluating CAVs' safety performances. However, substantial differences between CAVs and the prior knowledge can significantly diminish the evaluation efficiency. In response to this issue, existing studies predominantly concentrate on the adaptive design of testing scenarios during the CAV testing process. Yet, these methods have limitations in their applicability to high-dimensional scenarios. To overcome this challenge, we develop an adaptive testing environment that bolsters evaluation robustness by incorporating multiple surrogate models and optimizing the combination coefficients of these surrogate models to enhance evaluation efficiency. We formulate the optimization problem as a regression task utilizing quadratic programming. To efficiently obtain the regression target via reinforcement learning, we propose the dense reinforcement learning method and devise a new adaptive policy with high sample efficiency. Essentially, our approach centers on learning the values of critical scenes displaying substantial surrogate-to-real gaps. The effectiveness of our method is validated in high-dimensional overtaking scenarios, demonstrating that our approach achieves notable evaluation efficiency.","sentences":["The assessment of safety performance plays a pivotal role in the development and deployment of connected and automated vehicles (CAVs).","A common approach involves designing testing scenarios based on prior knowledge of CAVs (e.g., surrogate models), conducting tests in these scenarios, and subsequently evaluating CAVs' safety performances.","However, substantial differences between CAVs and the prior knowledge can significantly diminish the evaluation efficiency.","In response to this issue, existing studies predominantly concentrate on the adaptive design of testing scenarios during the CAV testing process.","Yet, these methods have limitations in their applicability to high-dimensional scenarios.","To overcome this challenge, we develop an adaptive testing environment that bolsters evaluation robustness by incorporating multiple surrogate models and optimizing the combination coefficients of these surrogate models to enhance evaluation efficiency.","We formulate the optimization problem as a regression task utilizing quadratic programming.","To efficiently obtain the regression target via reinforcement learning, we propose the dense reinforcement learning method and devise a new adaptive policy with high sample efficiency.","Essentially, our approach centers on learning the values of critical scenes displaying substantial surrogate-to-real gaps.","The effectiveness of our method is validated in high-dimensional overtaking scenarios, demonstrating that our approach achieves notable evaluation efficiency."],"url":"http://arxiv.org/abs/2402.19275v1","category":"eess.SY"}
{"created":"2024-02-29 15:41:20","title":"PlanGPT: Enhancing Urban Planning with Tailored Language Model and Efficient Retrieval","abstract":"In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners. Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges. To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning. Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities. Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning.","sentences":["In the field of urban planning, general-purpose large language models often struggle to meet the specific needs of planners.","Tasks like generating urban planning texts, retrieving related information, and evaluating planning documents pose unique challenges.","To enhance the efficiency of urban professionals and overcome these obstacles, we introduce PlanGPT, the first specialized Large Language Model tailored for urban and spatial planning.","Developed through collaborative efforts with institutions like the Chinese Academy of Urban Planning, PlanGPT leverages a customized local database retrieval framework, domain-specific fine-tuning of base models, and advanced tooling capabilities.","Empirical tests demonstrate that PlanGPT has achieved advanced performance, delivering responses of superior quality precisely tailored to the intricacies of urban planning."],"url":"http://arxiv.org/abs/2402.19273v1","category":"cs.CL"}
{"created":"2024-02-29 15:41:15","title":"Response to David Steigmann's discussion of our paper","abstract":"We respond to David Steigmann's discussion of our paper \"A general theory for anisotropic Kirchhoff-Love shells with in-plane bending of embedded fibers, Math. Mech. Solids, 28(5):1274-1317\" (arXiv:2101.03122). His discussion allows us to clarify two misleading statements in our original paper, and confirm that its formulation is fully consistent with the formulation of Steigmann. We also demonstrate that some of our original statements criticized by Steigmann are not wrong.","sentences":["We respond to David Steigmann's discussion of our paper \"A general theory for anisotropic Kirchhoff-Love shells with in-plane bending of embedded fibers, Math.","Mech.","Solids, 28(5):1274-1317\" (arXiv:2101.03122).","His discussion allows us to clarify two misleading statements in our original paper, and confirm that its formulation is fully consistent with the formulation of Steigmann.","We also demonstrate that some of our original statements criticized by Steigmann are not wrong."],"url":"http://arxiv.org/abs/2402.19272v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 15:41:07","title":"Coloring locally sparse graphs","abstract":"A graph $G$ is $k$-locally sparse if for each vertex $v \\in V(G)$, the subgraph induced by its neighborhood contains at most $k$ edges. Alon, Krivelevich, and Sudakov showed that for $f > 0$ if a graph $G$ of maximum degree $\\Delta$ is $\\Delta^2/f$-locally-sparse, then $\\chi(G) = O\\left(\\Delta/\\log f\\right)$. We introduce a more general notion of local sparsity by defining graphs $G$ to be $(k, F)$-locally-sparse for some graph $F$ if for each vertex $v \\in V(G)$ the subgraph induced by the neighborhood of $v$ contains at most $k$ copies of $F$. Employing the R\\\"{o}dl nibble method, we prove the following generalization of the above result: for every bipartite graph $F$, if $G$ is $(k, F)$-locally-sparse, then $\\chi(G) = O\\left( \\Delta /\\log\\left(\\Delta k^{-1/|V(F)|}\\right)\\right)$. This improves upon results of Davies, Kang, Pirot, and Sereni who consider the case when $F$ is a path. Our results also recover the best known bound on $\\chi(G)$ when $G$ is $K_{1, t, t}$-free for $t \\geq 4$, and hold for list and correspondence coloring in the more general so-called ''color-degree'' setting.","sentences":["A graph $G$ is $k$-locally sparse if for each vertex $v \\in V(G)$, the subgraph induced by its neighborhood contains at most $k$ edges.","Alon, Krivelevich, and Sudakov showed that for $f > 0$ if a graph $G$ of maximum degree $\\Delta$ is $\\Delta^2/f$-locally-sparse, then $\\chi(G) =","O\\left(\\Delta/\\log f\\right)$. We introduce a more general notion of local sparsity by defining graphs $G$ to be $(k, F)$-locally-sparse for some graph $F$ if for each vertex $v \\in V(G)$ the subgraph induced by the neighborhood of $v$ contains at most $k$ copies of $F$. Employing the R\\\"{o}dl nibble method, we prove the following generalization of the above result: for every bipartite graph $F$, if $G$ is $(k, F)$-locally-sparse, then $\\chi(G) = O\\left( \\Delta /\\log\\left(\\Delta","k^{-1/|V(F)|}\\right)\\right)$.","This improves upon results of Davies, Kang, Pirot, and Sereni who consider the case when $F$ is a path.","Our results also recover the best known bound on $\\chi(G)$ when $G$ is $K_{1, t, t}$-free for $t \\geq 4$, and hold for list and correspondence coloring in the more general so-called ''color-degree'' setting."],"url":"http://arxiv.org/abs/2402.19271v1","category":"math.CO"}
{"created":"2024-02-29 15:38:58","title":"Extremal quantiles of intermediate orders under two-way clustering","abstract":"This paper investigates extremal quantiles under two-way cluster dependence. We demonstrate that the limiting distribution of the unconditional intermediate order quantiles in the tails converges to a Gaussian distribution. This is remarkable as two-way cluster dependence entails potential non-Gaussianity in general, but extremal quantiles do not suffer from this issue. Building upon this result, we extend our analysis to extremal quantile regressions of intermediate order.","sentences":["This paper investigates extremal quantiles under two-way cluster dependence.","We demonstrate that the limiting distribution of the unconditional intermediate order quantiles in the tails converges to a Gaussian distribution.","This is remarkable as two-way cluster dependence entails potential non-Gaussianity in general, but extremal quantiles do not suffer from this issue.","Building upon this result, we extend our analysis to extremal quantile regressions of intermediate order."],"url":"http://arxiv.org/abs/2402.19268v1","category":"math.ST"}
{"created":"2024-02-29 15:38:28","title":"Robust Guidance for Unsupervised Data Selection: Capturing Perplexing Named Entities for Domain-Specific Machine Translation","abstract":"Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains. Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations. Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs. Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume. This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited. However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on. We introduce a novel unsupervised data selection method, 'Capturing Perplexing Named Entities', which adopts the maximum inference entropy in translated named entities as a selection measure. The motivation was that named entities in domain-specific data are considered the most complex portion of the data and should be predicted with high confidence. When verified with the 'Korean-English Parallel Corpus of Specialized Domains,' our method served as a robust guidance for unsupervised data selection, in contrast to existing methods.","sentences":["Employing extensive datasets enables the training of multilingual machine translation models; however, these models often fail to accurately translate sentences within specialized domains.","Although obtaining and translating domain-specific data incurs high costs, it is inevitable for high-quality translations.","Hence, finding the most 'effective' data with an unsupervised setting becomes a practical strategy for reducing labeling costs.","Recent research indicates that this effective data could be found by selecting 'properly difficult data' based on its volume.","This means the data should not be excessively challenging or overly simplistic, especially if the amount of data is limited.","However, we found that establishing a criterion for unsupervised data selection remains challenging, as the 'proper difficulty' might vary based on the data domain being trained on.","We introduce a novel unsupervised data selection method, 'Capturing Perplexing Named Entities', which adopts the maximum inference entropy in translated named entities as a selection measure.","The motivation was that named entities in domain-specific data are considered the most complex portion of the data and should be predicted with high confidence.","When verified with the 'Korean-English Parallel Corpus of Specialized Domains,' our method served as a robust guidance for unsupervised data selection, in contrast to existing methods."],"url":"http://arxiv.org/abs/2402.19267v1","category":"cs.CL"}
{"created":"2024-02-29 15:37:31","title":"Cauchy-completions and the rule of unique choice in relational doctrines","abstract":"Lawvere's generalised the notion of complete metric space to the field of enriched categories: an enriched category is said to be Cauchy-complete if every left adjoint bimodule into it is represented by an enriched functor. Looking at this definition from a logical standpoint, regarding bimodules as an abstraction of relations and functors as an abstraction of functions, Cauchy-completeness resembles a formulation of the rule of unique choice. In this paper, we make this analogy precise, using the language of relational doctrines, a categorical tool that provides a functorial description of the calculus of relations, in the same way Lawvere's hyperdoctrines give a functorial description of predicate logic. Given a relational doctrine, we define Cauchy-complete objects as those objects of the domain category satisfying the rule of unique choice. Then, we present a universal construction that completes a relational doctrine with the rule of unique choice, that is, producing a new relational doctrine where all objects are Cauchy-complete. We also introduce relational doctrines with singleton objects and show that these have the minimal structure needed to build the reflector of the full subcategory of its domain on Cauchy-complete objects. The main result is that this reflector exists if and only if the relational doctrine has singleton objects and this happens if and only if its restriction to Cauchy-complete objects is equivalent to its completion with the rule of unique choice. We support our results with many examples, also falling outside the scope of standard doctrines, such as complete metric spaces, Banach spaces and compact Hausdorff spaces in the general context of monoidal topology, which are all shown to be Cauchy-complete objects for appropriate relational doctrines.","sentences":["Lawvere's generalised the notion of complete metric space to the field of enriched categories: an enriched category is said to be Cauchy-complete if every left adjoint bimodule into it is represented by an enriched functor.","Looking at this definition from a logical standpoint, regarding bimodules as an abstraction of relations and functors as an abstraction of functions, Cauchy-completeness resembles a formulation of the rule of unique choice.","In this paper, we make this analogy precise, using the language of relational doctrines, a categorical tool that provides a functorial description of the calculus of relations, in the same way Lawvere's hyperdoctrines give a functorial description of predicate logic.","Given a relational doctrine, we define Cauchy-complete objects as those objects of the domain category satisfying the rule of unique choice.","Then, we present a universal construction that completes a relational doctrine with the rule of unique choice, that is, producing a new relational doctrine where all objects are Cauchy-complete.","We also introduce relational doctrines with singleton objects and show that these have the minimal structure needed to build the reflector of the full subcategory of its domain on Cauchy-complete objects.","The main result is that this reflector exists if and only if the relational doctrine has singleton objects and this happens if and only if its restriction to Cauchy-complete objects is equivalent to its completion with the rule of unique choice.","We support our results with many examples, also falling outside the scope of standard doctrines, such as complete metric spaces, Banach spaces and compact Hausdorff spaces in the general context of monoidal topology, which are all shown to be Cauchy-complete objects for appropriate relational doctrines."],"url":"http://arxiv.org/abs/2402.19266v1","category":"math.CT"}
{"created":"2024-02-29 15:36:01","title":"Learning Logic Specifications for Policy Guidance in POMDPs: an Inductive Logic Programming Approach","abstract":"Partially Observable Markov Decision Processes (POMDPs) are a powerful framework for planning under uncertainty. They allow to model state uncertainty as a belief probability distribution. Approximate solvers based on Monte Carlo sampling show great success to relax the computational demand and perform online planning. However, scaling to complex realistic domains with many actions and long planning horizons is still a major challenge, and a key point to achieve good performance is guiding the action-selection process with domain-dependent policy heuristics which are tailored for the specific application domain. We propose to learn high-quality heuristics from POMDP traces of executions generated by any solver. We convert the belief-action pairs to a logical semantics, and exploit data- and time-efficient Inductive Logic Programming (ILP) to generate interpretable belief-based policy specifications, which are then used as online heuristics. We evaluate thoroughly our methodology on two notoriously challenging POMDP problems, involving large action spaces and long planning horizons, namely, rocksample and pocman. Considering different state-of-the-art online POMDP solvers, including POMCP, DESPOT and AdaOPS, we show that learned heuristics expressed in Answer Set Programming (ASP) yield performance superior to neural networks and similar to optimal handcrafted task-specific heuristics within lower computational time. Moreover, they well generalize to more challenging scenarios not experienced in the training phase (e.g., increasing rocks and grid size in rocksample, incrementing the size of the map and the aggressivity of ghosts in pocman).","sentences":["Partially Observable Markov Decision Processes (POMDPs) are a powerful framework for planning under uncertainty.","They allow to model state uncertainty as a belief probability distribution.","Approximate solvers based on Monte Carlo sampling show great success to relax the computational demand and perform online planning.","However, scaling to complex realistic domains with many actions and long planning horizons is still a major challenge, and a key point to achieve good performance is guiding the action-selection process with domain-dependent policy heuristics which are tailored for the specific application domain.","We propose to learn high-quality heuristics from POMDP traces of executions generated by any solver.","We convert the belief-action pairs to a logical semantics, and exploit data-","and time-efficient Inductive Logic Programming (ILP) to generate interpretable belief-based policy specifications, which are then used as online heuristics.","We evaluate thoroughly our methodology on two notoriously challenging POMDP problems, involving large action spaces and long planning horizons, namely, rocksample and pocman.","Considering different state-of-the-art online POMDP solvers, including POMCP, DESPOT and AdaOPS, we show that learned heuristics expressed in Answer Set Programming (ASP) yield performance superior to neural networks and similar to optimal handcrafted task-specific heuristics within lower computational time.","Moreover, they well generalize to more challenging scenarios not experienced in the training phase (e.g., increasing rocks and grid size in rocksample, incrementing the size of the map and the aggressivity of ghosts in pocman)."],"url":"http://arxiv.org/abs/2402.19265v1","category":"cs.AI"}
{"created":"2024-02-29 15:32:25","title":"Spinal Osteophyte Detection via Robust Patch Extraction on minimally annotated X-rays","abstract":"The development and progression of arthritis is strongly associated with osteophytes, which are small and elusive bone growths. This paper presents one of the first efforts towards automated spinal osteophyte detection in spinal X-rays. A novel automated patch extraction process, called SegPatch, has been proposed based on deep learning-driven vertebrae segmentation and the enlargement of mask contours. A final patch classification accuracy of 84.5\\% is secured, surpassing a baseline tiling-based patch generation technique by 9.5%. This demonstrates that even with limited annotations, SegPatch can deliver superior performance for detection of tiny structures such as osteophytes. The proposed approach has potential to assist clinicians in expediting the process of manually identifying osteophytes in spinal X-ray.","sentences":["The development and progression of arthritis is strongly associated with osteophytes, which are small and elusive bone growths.","This paper presents one of the first efforts towards automated spinal osteophyte detection in spinal X-rays.","A novel automated patch extraction process, called SegPatch, has been proposed based on deep learning-driven vertebrae segmentation and the enlargement of mask contours.","A final patch classification accuracy of 84.5\\% is secured, surpassing a baseline tiling-based patch generation technique by 9.5%.","This demonstrates that even with limited annotations, SegPatch can deliver superior performance for detection of tiny structures such as osteophytes.","The proposed approach has potential to assist clinicians in expediting the process of manually identifying osteophytes in spinal X-ray."],"url":"http://arxiv.org/abs/2402.19263v1","category":"cs.CV"}
{"created":"2024-02-29 15:31:15","title":"Total Completion Time Scheduling Under Scenarios","abstract":"Scheduling jobs with given processing times on identical parallel machines so as to minimize their total completion time is one of the most basic scheduling problems. We study interesting generalizations of this classical problem involving scenarios. In our model, a scenario is defined as a subset of a predefined and fully specified set of jobs. The aim is to find an assignment of the whole set of jobs to identical parallel machines such that the schedule, obtained for the given scenarios by simply skipping the jobs not in the scenario, optimizes a function of the total completion times over all scenarios.   While the underlying scheduling problem without scenarios can be solved efficiently by a simple greedy procedure (SPT rule), scenarios, in general, make the problem NP-hard. We paint an almost complete picture of the evolving complexity landscape, drawing the line between easy and hard. One of our main algorithmic contributions relies on a deep structural result on the maximum imbalance of an optimal schedule, based on a subtle connection to Hilbert bases of a related convex cone.","sentences":["Scheduling jobs with given processing times on identical parallel machines so as to minimize their total completion time is one of the most basic scheduling problems.","We study interesting generalizations of this classical problem involving scenarios.","In our model, a scenario is defined as a subset of a predefined and fully specified set of jobs.","The aim is to find an assignment of the whole set of jobs to identical parallel machines such that the schedule, obtained for the given scenarios by simply skipping the jobs not in the scenario, optimizes a function of the total completion times over all scenarios.   ","While the underlying scheduling problem without scenarios can be solved efficiently by a simple greedy procedure (SPT rule), scenarios, in general, make the problem NP-hard.","We paint an almost complete picture of the evolving complexity landscape, drawing the line between easy and hard.","One of our main algorithmic contributions relies on a deep structural result on the maximum imbalance of an optimal schedule, based on a subtle connection to Hilbert bases of a related convex cone."],"url":"http://arxiv.org/abs/2402.19259v1","category":"cs.DS"}
{"created":"2024-02-29 15:26:14","title":"GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers","abstract":"Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (\\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result. Code and data are available at \\url{https://github.com/qtli/GSM-Plus}.","sentences":["Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks.","However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning.","One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly.","This motivates us to evaluate the robustness of LLMs' math reasoning capability by testing a wide range of question variations.","We introduce the adversarial grade school math (\\datasetname) dataset, an extension of GSM8K augmented with various mathematical perturbations.","Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust.","In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered.","We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.","Code and data are available at \\url{https://github.com/qtli/GSM-Plus}."],"url":"http://arxiv.org/abs/2402.19255v1","category":"cs.CL"}
{"created":"2024-02-29 15:23:06","title":"Fast neutrino flavor conversions in a supernova: emergence, evolution, and effects","abstract":"Fast flavor conversions (FFCs) of neutrinos, which can occur in core-collapse supernovae (CCSNe), are multiangle effects. They depend on the angular distribution of the neutrino's electron lepton number (ELN). In this work, we present a comprehensive study of the FFCs by solving the multienergy and multiangle quantum kinetic equations with an extended set of collisional weak processes based on a static and spherically symmetric CCSN matter background profile. We investigate the emergence and evolution of FFCs in models featuring different ELN angular distributions, considering scenarios with two and three neutrino flavors. The spectrogram method is utilized to illustrate the small-scale spatial structure, and we show that this structure of neutrino flavor coherence and number densities in the nonlinear regime is qualitatively consistent with the dispersion relation analysis. On the coarse-grained level, we find that different asymptotic states can be achieved following the FFCs depending on the locations and shapes of the ELN distributions, despite sharing a common feature of the elimination of the ELN angular crossing. While equilibration among different neutrino flavors may be achieved immediately after the prompt FFCs, it is not a general outcome of the asymptotic state, as subsequent feedback effects from collisional neutrino-matter interactions come into play, particularly for cases where FFCs occur inside the neutrinosphere. The impacts of FFCs and the feedback effect on the net neutrino heating rates, the equilibrium electron fraction of CCSN matter, and the free-streaming neutrino energy spectra are quantitatively assessed. Other aspects including the impact of the vacuum term and the coexistence with other type of flavor instabilities are also discussed.","sentences":["Fast flavor conversions (FFCs) of neutrinos, which can occur in core-collapse supernovae (CCSNe), are multiangle effects.","They depend on the angular distribution of the neutrino's electron lepton number (ELN).","In this work, we present a comprehensive study of the FFCs by solving the multienergy and multiangle quantum kinetic equations with an extended set of collisional weak processes based on a static and spherically symmetric CCSN matter background profile.","We investigate the emergence and evolution of FFCs in models featuring different ELN angular distributions, considering scenarios with two and three neutrino flavors.","The spectrogram method is utilized to illustrate the small-scale spatial structure, and we show that this structure of neutrino flavor coherence and number densities in the nonlinear regime is qualitatively consistent with the dispersion relation analysis.","On the coarse-grained level, we find that different asymptotic states can be achieved following the FFCs depending on the locations and shapes of the ELN distributions, despite sharing a common feature of the elimination of the ELN angular crossing.","While equilibration among different neutrino flavors may be achieved immediately after the prompt FFCs, it is not a general outcome of the asymptotic state, as subsequent feedback effects from collisional neutrino-matter interactions come into play, particularly for cases where FFCs occur inside the neutrinosphere.","The impacts of FFCs and the feedback effect on the net neutrino heating rates, the equilibrium electron fraction of CCSN matter, and the free-streaming neutrino energy spectra are quantitatively assessed.","Other aspects including the impact of the vacuum term and the coexistence with other type of flavor instabilities are also discussed."],"url":"http://arxiv.org/abs/2402.19252v1","category":"astro-ph.HE"}
{"created":"2024-02-29 15:22:26","title":"A Cognitive-Based Trajectory Prediction Approach for Autonomous Driving","abstract":"In autonomous vehicle (AV) technology, the ability to accurately predict the movements of surrounding vehicles is paramount for ensuring safety and operational efficiency. Incorporating human decision-making insights enables AVs to more effectively anticipate the potential actions of other vehicles, significantly improving prediction accuracy and responsiveness in dynamic environments. This paper introduces the Human-Like Trajectory Prediction (HLTP) model, which adopts a teacher-student knowledge distillation framework inspired by human cognitive processes. The HLTP model incorporates a sophisticated teacher-student knowledge distillation framework. The \"teacher\" model, equipped with an adaptive visual sector, mimics the visual processing of the human brain, particularly the functions of the occipital and temporal lobes. The \"student\" model focuses on real-time interaction and decision-making, drawing parallels to prefrontal and parietal cortex functions. This approach allows for dynamic adaptation to changing driving scenarios, capturing essential perceptual cues for accurate prediction. Evaluated using the Macao Connected and Autonomous Driving (MoCAD) dataset, along with the NGSIM and HighD benchmarks, HLTP demonstrates superior performance compared to existing models, particularly in challenging environments with incomplete data. The project page is available at Github.","sentences":["In autonomous vehicle (AV) technology, the ability to accurately predict the movements of surrounding vehicles is paramount for ensuring safety and operational efficiency.","Incorporating human decision-making insights enables AVs to more effectively anticipate the potential actions of other vehicles, significantly improving prediction accuracy and responsiveness in dynamic environments.","This paper introduces the Human-Like Trajectory Prediction (HLTP) model, which adopts a teacher-student knowledge distillation framework inspired by human cognitive processes.","The HLTP model incorporates a sophisticated teacher-student knowledge distillation framework.","The \"teacher\" model, equipped with an adaptive visual sector, mimics the visual processing of the human brain, particularly the functions of the occipital and temporal lobes.","The \"student\" model focuses on real-time interaction and decision-making, drawing parallels to prefrontal and parietal cortex functions.","This approach allows for dynamic adaptation to changing driving scenarios, capturing essential perceptual cues for accurate prediction.","Evaluated using the Macao Connected and Autonomous Driving (MoCAD) dataset, along with the NGSIM and HighD benchmarks, HLTP demonstrates superior performance compared to existing models, particularly in challenging environments with incomplete data.","The project page is available at Github."],"url":"http://arxiv.org/abs/2402.19251v1","category":"cs.AI"}
{"created":"2024-02-29 15:22:21","title":"Feature boosting with efficient attention for scene parsing","abstract":"The complexity of scene parsing grows with the number of object and scene classes, which is higher in unrestricted open scenes. The biggest challenge is to model the spatial relation between scene elements while succeeding in identifying objects at smaller scales. This paper presents a novel feature-boosting network that gathers spatial context from multiple levels of feature extraction and computes the attention weights for each level of representation to generate the final class labels. A novel `channel attention module' is designed to compute the attention weights, ensuring that features from the relevant extraction stages are boosted while the others are attenuated. The model also learns spatial context information at low resolution to preserve the abstract spatial relationships among scene elements and reduce computation cost. Spatial attention is subsequently concatenated into a final feature set before applying feature boosting. Low-resolution spatial attention features are trained using an auxiliary task that helps learning a coarse global scene structure. The proposed model outperforms all state-of-the-art models on both the ADE20K and the Cityscapes datasets.","sentences":["The complexity of scene parsing grows with the number of object and scene classes, which is higher in unrestricted open scenes.","The biggest challenge is to model the spatial relation between scene elements while succeeding in identifying objects at smaller scales.","This paper presents a novel feature-boosting network that gathers spatial context from multiple levels of feature extraction and computes the attention weights for each level of representation to generate the final class labels.","A novel `channel attention module' is designed to compute the attention weights, ensuring that features from the relevant extraction stages are boosted while the others are attenuated.","The model also learns spatial context information at low resolution to preserve the abstract spatial relationships among scene elements and reduce computation cost.","Spatial attention is subsequently concatenated into a final feature set before applying feature boosting.","Low-resolution spatial attention features are trained using an auxiliary task that helps learning a coarse global scene structure.","The proposed model outperforms all state-of-the-art models on both the ADE20K and the Cityscapes datasets."],"url":"http://arxiv.org/abs/2402.19250v1","category":"cs.CV"}
{"created":"2024-02-29 15:22:17","title":"Mirage: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting","abstract":"The ability to reuse collected data and transfer trained policies between robots could alleviate the burden of additional data collection and training. While existing approaches such as pretraining plus finetuning and co-training show promise, they do not generalize to robots unseen in training. Focusing on common robot arms with similar workspaces and 2-jaw grippers, we investigate the feasibility of zero-shot transfer. Through simulation studies on 8 manipulation tasks, we find that state-based Cartesian control policies can successfully zero-shot transfer to a target robot after accounting for forward dynamics. To address robot visual disparities for vision-based policies, we introduce Mirage, which uses \"cross-painting\"--masking out the unseen target robot and inpainting the seen source robot--during execution in real time so that it appears to the policy as if the trained source robot were performing the task. Despite its simplicity, our extensive simulation and physical experiments provide strong evidence that Mirage can successfully zero-shot transfer between different robot arms and grippers with only minimal performance degradation on a variety of manipulation tasks such as picking, stacking, and assembly, significantly outperforming a generalist policy. Project website: https://robot-mirage.github.io/","sentences":["The ability to reuse collected data and transfer trained policies between robots could alleviate the burden of additional data collection and training.","While existing approaches such as pretraining plus finetuning and co-training show promise, they do not generalize to robots unseen in training.","Focusing on common robot arms with similar workspaces and 2-jaw grippers, we investigate the feasibility of zero-shot transfer.","Through simulation studies on 8 manipulation tasks, we find that state-based Cartesian control policies can successfully zero-shot transfer to a target robot after accounting for forward dynamics.","To address robot visual disparities for vision-based policies, we introduce Mirage, which uses \"cross-painting\"--masking out the unseen target robot and inpainting the seen source robot--during execution in real time so that it appears to the policy as if the trained source robot were performing the task.","Despite its simplicity, our extensive simulation and physical experiments provide strong evidence that Mirage can successfully zero-shot transfer between different robot arms and grippers with only minimal performance degradation on a variety of manipulation tasks such as picking, stacking, and assembly, significantly outperforming a generalist policy.","Project website: https://robot-mirage.github.io/"],"url":"http://arxiv.org/abs/2402.19249v1","category":"cs.RO"}
{"created":"2024-02-29 15:19:15","title":"Electron conductance of a cavity-embedded topological 1D chain","abstract":"We investigate many-body topological and transport properties of a one-dimensional Su-Schrieffer-Heeger (SSH) topological chain coupled to the quantum field of a cavity mode. The quantum conductance is determined via Green's function formalism in terms of light-matter eigenstates calculated via exact diagonalization for a finite number of electrons. We show that the topology of the cavity-embedded many-electron system is described by a generalized electron-photon Zak marker. We reveal how the quantization of transport is modified by the cavity vacuum fields for a finite-size chain and how it is impacted by electronic disorder. Moreover, we show that electron-photon entanglement produces dramatic differences with respect to the prediction of mean-field theory, which strongly underestimates cavity-modified effects.","sentences":["We investigate many-body topological and transport properties of a one-dimensional Su-Schrieffer-Heeger (SSH) topological chain coupled to the quantum field of a cavity mode.","The quantum conductance is determined via Green's function formalism in terms of light-matter eigenstates calculated via exact diagonalization for a finite number of electrons.","We show that the topology of the cavity-embedded many-electron system is described by a generalized electron-photon Zak marker.","We reveal how the quantization of transport is modified by the cavity vacuum fields for a finite-size chain and how it is impacted by electronic disorder.","Moreover, we show that electron-photon entanglement produces dramatic differences with respect to the prediction of mean-field theory, which strongly underestimates cavity-modified effects."],"url":"http://arxiv.org/abs/2402.19244v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 15:19:03","title":"Revisiting $O(N)$ $\u03c3$ model at unphysical pion masses and high temperatures","abstract":"Roy-equation analyses on lattice data of $\\pi\\pi$ scattering phase shifts at $m_\\pi=391$MeV reveals that the lowest $f_0$ meson becomes a bound state under this condition. In addition, there is a pair of complex poles below threshold generated by crossing symmetry (PRD 108, 034009). We use the $N/D$ method to partially recover crossing symmetry of the $O(N)$ $\\sigma$ model amplitude at leading order of $1/N$ expansion, and qualitatively reproduce the pole structure and pole trajectories with varying pion masses as revealed by Roy-equation analyses. The $\\sigma$ pole trajectory with varying temperature is also discussed and found to be similar to its properties when varying $m_\\pi$. As the temperature increases, the complex $\\sigma$ poles firstly move from the second Riemann sheet to the real axis becoming two virtual state poles, and then one virtual state pole moves to the first sheet turning into a bound state pole and finally tends to the pion pole position at high temperature which is as expected from the chiral symmetry restoration. Our results provide further evidences that the lowest $f_0$ state extracted from experiments and lattice data plays the role of $\\sigma$ meson in the spontaneous breaking of chiral symmetry. Finally, we also briefly discuss the problems of the effective potential in the situation when $m_\\pi$ and temperature get large.","sentences":["Roy-equation analyses on lattice data of $\\pi\\pi$ scattering phase shifts at $m_\\pi=391$MeV reveals that the lowest $f_0$ meson becomes a bound state under this condition.","In addition, there is a pair of complex poles below threshold generated by crossing symmetry (PRD 108, 034009).","We use the $N/D$ method to partially recover crossing symmetry of the $O(N)$ $\\sigma$ model amplitude at leading order of $1/N$ expansion, and qualitatively reproduce the pole structure and pole trajectories with varying pion masses as revealed by Roy-equation analyses.","The $\\sigma$ pole trajectory with varying temperature is also discussed and found to be similar to its properties when varying $m_\\pi$. As the temperature increases, the complex $\\sigma$ poles firstly move from the second Riemann sheet to the real axis becoming two virtual state poles, and then one virtual state pole moves to the first sheet turning into a bound state pole and finally tends to the pion pole position at high temperature which is as expected from the chiral symmetry restoration.","Our results provide further evidences that the lowest $f_0$ state extracted from experiments and lattice data plays the role of $\\sigma$ meson in the spontaneous breaking of chiral symmetry.","Finally, we also briefly discuss the problems of the effective potential in the situation when $m_\\pi$ and temperature get large."],"url":"http://arxiv.org/abs/2402.19243v1","category":"hep-ph"}
{"created":"2024-02-29 15:12:06","title":"Measurement Schemes in AQFT, Contextuality and the Wigner's Friend Gedankenexperiment","abstract":"Measurements have historically presented a problem for the consistent description of quantum theories, be it in non-relativistic quantum mechanics or in quantum field theory. Drawing on a recent surge of interest in the description of measurements in Algebraic Quantum Field theory, it was decided that this dissertation would be focused on trying to close the gap between the description of measurements proposed by K. Hepp in the 70's, considering decoherence of states in quasilocal algebras and the new framework of generally covariant measurement schemes proposed recently by C. Fewster and R. Verch. Another recent result that we shall also consider is the Frauchinger-Renner Gedankenexperiment, that has taken inspiration on Hepp's article about decoherence based measurements to arrive at a no-go result about the consistency of quantum descriptions of systems containing rational agents, we shall seek to provide a closure for the interpretation of this result. In doing so we naturally arrive at the study of the contextual properties of measurement setups.","sentences":["Measurements have historically presented a problem for the consistent description of quantum theories, be it in non-relativistic quantum mechanics or in quantum field theory.","Drawing on a recent surge of interest in the description of measurements in Algebraic Quantum Field theory, it was decided that this dissertation would be focused on trying to close the gap between the description of measurements proposed by K. Hepp in the 70's, considering decoherence of states in quasilocal algebras and the new framework of generally covariant measurement schemes proposed recently by C. Fewster and R. Verch.","Another recent result that we shall also consider is the Frauchinger-Renner Gedankenexperiment, that has taken inspiration on Hepp's article about decoherence based measurements to arrive at a no-go result about the consistency of quantum descriptions of systems containing rational agents, we shall seek to provide a closure for the interpretation of this result.","In doing so we naturally arrive at the study of the contextual properties of measurement setups."],"url":"http://arxiv.org/abs/2402.19235v1","category":"math-ph"}
{"created":"2024-02-29 14:57:04","title":"Deterministic Molecular Assembly with a Finite Set of Building Blocks: Universal Assembly Kits for Backbone-Assisted and Sequence-Directed Abstract Tile Assembly Models","abstract":"Backbone-assisted assembly processes -- such as protein folding -- allow the assembly of a large number of structures with high accuracy from only a small handful of fundamental building blocks. We aim to explore general principles underlying this phenomenon by studying variants of the temperature-1 abstract tile assembly model (aTAM). We consider the existence of finite sets of tile types that can deterministically assemble any shape producible by a given assembly model; we call such tile type sets universal assembly kits. Our first model, which we call the ``backboned aTAM\", generates backbone-assisted assembly by forcing tiles to be be added to lattice positions neighbouring the immediately preceding tile, using a predetermined sequence of tile types. We demonstrate the existence of universal assembly kit for the backboned aTAM, and show that the existence of this set is maintained even under stringent restrictions to the rules of assembly. We hypothesise that the advantage of the backboned aTAM relative to the conventional aTAM -- which does not possess such a set -- is in part due to the specification of a sequence, and in part due to the geometric restrictions imposed by the backbone. To explore this intuition, we develop a second model call the ``sequenced aTAM\". The sequenced aTAM uses a predetermined sequence of tiles, but does not constrain a tile to neighbour the immediately preceding tiles. We prove that this model has no universal assembly kit. The lack of such a kit is surprising, given that the number of tile sequences of length $N$ scales faster than the number of producible shapes of size $N$ for a sufficiently large -- but finite -- set of tiles.","sentences":["Backbone-assisted assembly processes -- such as protein folding -- allow the assembly of a large number of structures with high accuracy from only a small handful of fundamental building blocks.","We aim to explore general principles underlying this phenomenon by studying variants of the temperature-1 abstract tile assembly model (aTAM).","We consider the existence of finite sets of tile types that can deterministically assemble any shape producible by a given assembly model; we call such tile type sets universal assembly kits.","Our first model, which we call the ``backboned aTAM\", generates backbone-assisted assembly by forcing tiles to be be added to lattice positions neighbouring the immediately preceding tile, using a predetermined sequence of tile types.","We demonstrate the existence of universal assembly kit for the backboned aTAM, and show that the existence of this set is maintained even under stringent restrictions to the rules of assembly.","We hypothesise that the advantage of the backboned aTAM relative to the conventional aTAM -- which does not possess such a set -- is in part due to the specification of a sequence, and in part due to the geometric restrictions imposed by the backbone.","To explore this intuition, we develop a second model call the ``sequenced aTAM\".","The sequenced aTAM uses a predetermined sequence of tiles, but does not constrain a tile to neighbour the immediately preceding tiles.","We prove that this model has no universal assembly kit.","The lack of such a kit is surprising, given that the number of tile sequences of length $N$ scales faster than the number of producible shapes of size $N$ for a sufficiently large -- but finite -- set of tiles."],"url":"http://arxiv.org/abs/2402.19225v1","category":"physics.bio-ph"}
{"created":"2024-02-29 14:53:46","title":"FKS subtraction for quarkonium production at NLO","abstract":"We extend the local infrared-divergence subtraction formalism, originally proposed by Frixione, Kunszt and Signer (FKS), to calculate short-distance (differential) cross section for any inclusive process involving a quarkonium particle in non-relativistic QCD (NRQCD) factorisation at next-to-leading order (NLO) accuracy in the strong coupling constant $\\alpha_s$. The new formulas are generally applicable to the production of an S- or P-wave quarkonium state in association with any number of elementary particles. The main new ingredients derived in this paper are the local and integrated soft counterterms for the colour-singlet and colour-octet P-wave bound states. It, therefore, paves the way to the automation of the NLO calculations for heavy quarkonium inclusive and associated production processes.","sentences":["We extend the local infrared-divergence subtraction formalism, originally proposed by Frixione, Kunszt and Signer (FKS), to calculate short-distance (differential) cross section for any inclusive process involving a quarkonium particle in non-relativistic QCD (NRQCD) factorisation at next-to-leading order (NLO) accuracy in the strong coupling constant $\\alpha_s$. The new formulas are generally applicable to the production of an S- or P-wave quarkonium state in association with any number of elementary particles.","The main new ingredients derived in this paper are the local and integrated soft counterterms for the colour-singlet and colour-octet P-wave bound states.","It, therefore, paves the way to the automation of the NLO calculations for heavy quarkonium inclusive and associated production processes."],"url":"http://arxiv.org/abs/2402.19221v1","category":"hep-ph"}
{"created":"2024-02-29 14:49:53","title":"Transition of the semiclassical resonance widths across a tangential crossing energy-level","abstract":"We consider a 1D $2\\times 2$ matrix-valued operator \\eqref{System0} with two semiclassical Schr\\\"odinger operators on the diagonal entries and small interactions on the off-diagonal ones. When the two potentials cross at a turning point with contact order $n$, the corresponding two classical trajectories at the crossing level intersect at one point in the phase space with contact order $2n$. We compute the transfer matrix at this point between the incoming and outgoing microlocal solutions and apply it to the semiclassical distribution of resonances at the energy crossing level. It is described in terms of a generalized Airy function. This result generalizes \\cite{FMW1} to the tangential crossing and \\cite{AFH1} to the crossing at a turning point.","sentences":["We consider a 1D $2\\times 2$ matrix-valued operator \\eqref{System0} with two semiclassical Schr\\\"odinger operators on the diagonal entries and small interactions on the off-diagonal ones.","When the two potentials cross at a turning point with contact order $n$, the corresponding two classical trajectories at the crossing level intersect at one point in the phase space with contact order $2n$. We compute the transfer matrix at this point between the incoming and outgoing microlocal solutions and apply it to the semiclassical distribution of resonances at the energy crossing level.","It is described in terms of a generalized Airy function.","This result generalizes \\cite{FMW1} to the tangential crossing and \\cite{AFH1} to the crossing at a turning point."],"url":"http://arxiv.org/abs/2402.19219v1","category":"math-ph"}
{"created":"2024-02-29 14:47:24","title":"Memory-Augmented Generative Adversarial Transformers","abstract":"Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate. Vanilla Transformer architectures are not designed for answering factual questions with high accuracy. This paper investigates a possible route for addressing this problem. We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory. We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture. This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer. We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues. Secondly, we demonstrate that our approach can be useful for applications like {\\it style adaptation} as well: the adaptation of utterances according to certain stylistic (external) constraints, like social properties of human interlocutors in dialogues.","sentences":["Conversational AI systems that rely on Large Language Models, like Transformers, have difficulty interweaving external data (like facts) with the language they generate.","Vanilla Transformer architectures are not designed for answering factual questions with high accuracy.","This paper investigates a possible route for addressing this problem.","We propose to extend the standard Transformer architecture with an additional memory bank holding extra information (such as facts drawn from a knowledge base), and an extra attention layer for addressing this memory.","We add this augmented memory to a Generative Adversarial Network-inspired Transformer architecture.","This setup allows for implementing arbitrary felicity conditions on the generated language of the Transformer.","We first demonstrate how this machinery can be deployed for handling factual questions in goal-oriented dialogues.","Secondly, we demonstrate that our approach can be useful for applications like {\\it style adaptation} as well: the adaptation of utterances according to certain stylistic (external) constraints, like social properties of human interlocutors in dialogues."],"url":"http://arxiv.org/abs/2402.19218v1","category":"cs.CL"}
{"created":"2024-02-29 14:45:57","title":"Inclusive cross section measurements in final states with and without protons for charged-current $\u03bd_\u03bc$-Ar scattering in MicroBooNE","abstract":"A detailed understanding of inclusive muon neutrino charged-current interactions on argon is crucial to the study of neutrino oscillations in current and future experiments using liquid argon time projection chambers. To that end, we report a comprehensive set of differential cross section measurements for this channel that simultaneously probe the leptonic and hadronic systems by dividing the channel into final states with and without protons. Measurements of the proton kinematics and proton multiplicity of the final state are also presented. For these measurements, we utilize data collected with the MicroBooNE detector from 6.4$\\times10^{20}$ protons on target from the Fermilab Booster Neutrino Beam at a mean neutrino energy of approximately 0.8 GeV. We present in detail the cross section extraction procedure, including the unfolding, and model validation that uses data to model comparisons and the conditional constraint formalism to detect mismodeling that may introduce biases to extracted cross sections that are larger than their uncertainties. The validation exposes insufficiencies in the overall model, motivating the inclusion of an additional data-driven reweighting systematic to ensure the accuracy of the unfolding. The extracted results are compared to a number of event generators and their performance is discussed with a focus on the regions of phase-space that indicate the greatest need for modeling improvements.","sentences":["A detailed understanding of inclusive muon neutrino charged-current interactions on argon is crucial to the study of neutrino oscillations in current and future experiments using liquid argon time projection chambers.","To that end, we report a comprehensive set of differential cross section measurements for this channel that simultaneously probe the leptonic and hadronic systems by dividing the channel into final states with and without protons.","Measurements of the proton kinematics and proton multiplicity of the final state are also presented.","For these measurements, we utilize data collected with the MicroBooNE detector from 6.4$\\times10^{20}$ protons on target from the Fermilab Booster Neutrino Beam at a mean neutrino energy of approximately 0.8 GeV. We present in detail the cross section extraction procedure, including the unfolding, and model validation that uses data to model comparisons and the conditional constraint formalism to detect mismodeling that may introduce biases to extracted cross sections that are larger than their uncertainties.","The validation exposes insufficiencies in the overall model, motivating the inclusion of an additional data-driven reweighting systematic to ensure the accuracy of the unfolding.","The extracted results are compared to a number of event generators and their performance is discussed with a focus on the regions of phase-space that indicate the greatest need for modeling improvements."],"url":"http://arxiv.org/abs/2402.19216v1","category":"hep-ex"}
{"created":"2024-02-29 14:45:00","title":"Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts","abstract":"Super-resolution (SR) is an ill-posed inverse problem, where the size of the set of feasible solutions that are consistent with a given low-resolution image is very large. Many algorithms have been proposed to find a \"good\" solution among the feasible solutions that strike a balance between fidelity and perceptual quality. Unfortunately, all known methods generate artifacts and hallucinations while trying to reconstruct high-frequency (HF) image details. A fundamental question is: Can a model learn to distinguish genuine image details from artifacts? Although some recent works focused on the differentiation of details and artifacts, this is a very challenging problem and a satisfactory solution is yet to be found. This paper shows that the characterization of genuine HF details versus artifacts can be better learned by training GAN-based SR models using wavelet-domain loss functions compared to RGB-domain or Fourier-space losses. Although wavelet-domain losses have been used in the literature before, they have not been used in the context of the SR task. More specifically, we train the discriminator only on the HF wavelet sub-bands instead of on RGB images and the generator is trained by a fidelity loss over wavelet subbands to make it sensitive to the scale and orientation of structures. Extensive experimental results demonstrate that our model achieves better perception-distortion trade-off according to multiple objective measures and visual evaluations.","sentences":["Super-resolution (SR) is an ill-posed inverse problem, where the size of the set of feasible solutions that are consistent with a given low-resolution image is very large.","Many algorithms have been proposed to find a \"good\" solution among the feasible solutions that strike a balance between fidelity and perceptual quality.","Unfortunately, all known methods generate artifacts and hallucinations while trying to reconstruct high-frequency (HF) image details.","A fundamental question is: Can a model learn to distinguish genuine image details from artifacts?","Although some recent works focused on the differentiation of details and artifacts, this is a very challenging problem and a satisfactory solution is yet to be found.","This paper shows that the characterization of genuine HF details versus artifacts can be better learned by training GAN-based SR models using wavelet-domain loss functions compared to RGB-domain or Fourier-space losses.","Although wavelet-domain losses have been used in the literature before, they have not been used in the context of the SR task.","More specifically, we train the discriminator only on the HF wavelet sub-bands instead of on RGB images and the generator is trained by a fidelity loss over wavelet subbands to make it sensitive to the scale and orientation of structures.","Extensive experimental results demonstrate that our model achieves better perception-distortion trade-off according to multiple objective measures and visual evaluations."],"url":"http://arxiv.org/abs/2402.19215v1","category":"eess.IV"}
{"created":"2024-02-29 14:40:11","title":"Classifying pseudo-ovals, translation generalized quadrangles, and elation Laguerre planes of small order","abstract":"We provide classification results for translation generalized quadrangles of order less or equal to $64$, and hence, for all incidence geometries related to them. The results consist of the classification of all pseudo-ovals in $PG(3n-1,2)$, for $n=3,4$, and that of the pseudo-ovals in $PG(3n-1,q)$, for $n=5,6$, such that one of the associated projective planes is Desarguesian.","sentences":["We provide classification results for translation generalized quadrangles of order less or equal to $64$, and hence, for all incidence geometries related to them.","The results consist of the classification of all pseudo-ovals in $PG(3n-1,2)$, for $n=3,4$, and that of the pseudo-ovals in $PG(3n-1,q)$, for $n=5,6$, such that one of the associated projective planes is Desarguesian."],"url":"http://arxiv.org/abs/2402.19211v1","category":"math.CO"}
{"created":"2024-02-29 14:37:57","title":"Deciphering the Belle II data on $B\\to K \u03bd\\bar\u03bd$ decay in the (dark) SMEFT with minimal flavour violation","abstract":"Recently, the Belle II collaboration announced the first measurement of $\\mathcal B(B^+\\to K^+\\nu\\bar\\nu)$, which is found to be about $2.7\\sigma$ higher than the SM prediction. We decipher the data with two new physics scenarios: the underlying $b\\to s \\nu\\bar\\nu$ transition is, besides the SM contribution, further affected by heavy new mediators that are much heavier than the electroweak scale, or amended by an additional decay channel with undetected light final states like dark matter or axion-like particles. These two scenarios can be most conveniently analyzed in the SMEFT and the dark SMEFT (DSMEFT) framework, respectively. We consider the flavour structures of the resulting effective operators to be either generic or satisfy the minimal flavour violation (MFV) hypothesis, both for the quark and lepton sectors. In the first scenario, once the MFV assumption is made, only one SM-like low-energy effective operator induced by the SMEFT dim-6 operators can account for the Belle II excess, the parameter space of which is, however, excluded by the Belle upper bound on $\\mathcal B(B^0\\to K^{* 0}\\nu\\bar\\nu)$. In the second scenario, it is found that the Belle II excess can be accommodated by 22 of the DSMEFT operators involving one or two scalar, fermionic, or vector dark matters as well as axion-like particles. These operators also receive dominant constraints from the $B^0\\to K^{*0}+\\rm inv$ and $B_s\\to\\rm inv$ decays. Once the MFV hypothesis is assumed, the number of viable operators is reduced to 14, and the $B^+\\to\\pi^+ +\\rm inv$ and $K^+\\to\\pi^+ +\\rm inv$ decays start to put further constraints. Within the parameter space allowed by all the current experimental data, the $q^2$ distributions of the $B\\to K^{(*)}+\\rm inv$ decays are studied for each viable operator. In addition, the future prospects at Belle II, CEPC and FCC-ee are also discussed for some of these FCNC processes.","sentences":["Recently, the Belle II collaboration announced the first measurement of $\\mathcal B(B^+\\to K^+\\nu\\bar\\nu)$, which is found to be about $2.7\\sigma$ higher than the SM prediction.","We decipher the data with two new physics scenarios: the underlying $b\\to s \\nu\\bar\\nu$ transition is, besides the SM contribution, further affected by heavy new mediators that are much heavier than the electroweak scale, or amended by an additional decay channel with undetected light final states like dark matter or axion-like particles.","These two scenarios can be most conveniently analyzed in the SMEFT and the dark SMEFT (DSMEFT) framework, respectively.","We consider the flavour structures of the resulting effective operators to be either generic or satisfy the minimal flavour violation (MFV) hypothesis, both for the quark and lepton sectors.","In the first scenario, once the MFV assumption is made, only one SM-like low-energy effective operator induced by the SMEFT dim-6 operators can account for the Belle II excess, the parameter space of which is, however, excluded by the Belle upper bound on $\\mathcal B(B^0\\to K^{* 0}\\nu\\bar\\nu)$.","In the second scenario, it is found that the Belle II excess can be accommodated by 22 of the DSMEFT operators involving one or two scalar, fermionic, or vector dark matters as well as axion-like particles.","These operators also receive dominant constraints from the $B^0\\to K^{*0}+\\rm inv$ and $B_s\\to\\rm inv$ decays.","Once the MFV hypothesis is assumed, the number of viable operators is reduced to 14, and the $B^+\\to\\pi^+ +\\rm inv$ and $K^+\\to\\pi^+ +\\rm inv$ decays start to put further constraints.","Within the parameter space allowed by all the current experimental data, the $q^2$ distributions of the $B\\to K^{(*)}+\\rm inv$ decays are studied for each viable operator.","In addition, the future prospects at Belle II, CEPC and FCC-ee are also discussed for some of these FCNC processes."],"url":"http://arxiv.org/abs/2402.19208v1","category":"hep-ph"}
{"created":"2024-02-29 14:36:02","title":"Distribution Properties of the 6.7 GHz Methanol Masers and Their Surrounding Gases in the Milky Way","abstract":"An updated catalog consisting of 1092 6.7-GHz methanol maser sources was reported in this work. Additionally, the NH3 (1, 1), NH3 (2, 2), and NH3 (3, 3) transitions were observed towards 214 star forming regions using the Shanghai Tianma radio telescope (TMRT) in order to examine the differences in physical environments, such as excitation temperature and column density of molecular clouds associated with methanol masers on the Galactic scale. Statistical results reveal that the number of 6.7 GHz methanol masers in the Perseus arm is significantly lower than that in the other three main spiral arms. In addition, the Perseus arm also has the lowest gas column density among the main spiral arms traced by the NH3 observations. Both of these findings suggest that the Perseus arm has the lowest rate of high-mass star formation compared to the other three main spiral arms. We also observed a trend in which both the luminosity of 6.7 GHz methanol masers and the ammonia gas column density decreased as the galactocentric distances. This finding indicates that the density of material in the inner Milky Way is generally higher than that in the outer Milky Way. It further suggests that high-mass stars are more easily formed at the head of spiral arms. Furthermore, we found that the column density of ammonia gas is higher in the regions on the arms than that in the inter-arm regions, supporting that the former is more likely to be the birthplace of high-mass stars.","sentences":["An updated catalog consisting of 1092 6.7-GHz methanol maser sources was reported in this work.","Additionally, the NH3 (1, 1), NH3 (2, 2), and NH3 (3, 3) transitions were observed towards 214 star forming regions using the Shanghai Tianma radio telescope (TMRT) in order to examine the differences in physical environments, such as excitation temperature and column density of molecular clouds associated with methanol masers on the Galactic scale.","Statistical results reveal that the number of 6.7 GHz methanol masers in the Perseus arm is significantly lower than that in the other three main spiral arms.","In addition, the Perseus arm also has the lowest gas column density among the main spiral arms traced by the NH3 observations.","Both of these findings suggest that the Perseus arm has the lowest rate of high-mass star formation compared to the other three main spiral arms.","We also observed a trend in which both the luminosity of 6.7 GHz methanol masers and the ammonia gas column density decreased as the galactocentric distances.","This finding indicates that the density of material in the inner Milky Way is generally higher than that in the outer Milky Way.","It further suggests that high-mass stars are more easily formed at the head of spiral arms.","Furthermore, we found that the column density of ammonia gas is higher in the regions on the arms than that in the inter-arm regions, supporting that the former is more likely to be the birthplace of high-mass stars."],"url":"http://arxiv.org/abs/2402.19206v1","category":"astro-ph.GA"}
{"created":"2024-02-29 14:32:06","title":"VLBI observations of the high-redshift X-ray bright blazar SRGE J170245.3+130104","abstract":"Aims. The X-ray luminous and radio-loud AGN SRGE J170245.3+130104 discovered at z $\\sim$ 5.5 provides unique chances to probe the SMBH growth and evolution with powerful jets in the early Universe. Methods. We present 1.35 - 5.1 GHz Very Long Baseline Array (VLBA) results on the radio continuum emission and spectrum analysis for this quasar in a low flux density state. Results. This source is unresolved at three frequencies with the total flux densities of 8.35$\\pm$0.09 mJy beam-1, 7.47$\\pm$0.08 mJy beam-1, and 6.57$\\pm$0.02 mJy beam-1 at 1.73 GHz, 2.26 GHz, and 4.87 GHz, respectively. Meanwhile, the brightness temperature is higher than 109 K. Conclusions. Compared with previous radio observations with arcsec-scale resolution, nearly all the radio emission from this source concentrates in the very central milli-arcsecond (mas) scale area. We confirm this source is a bright blazar at z > 5. This young AGN provide us the great chances to understand the first generation of strong jets in the early Universe.","sentences":["Aims.","The X-ray luminous and radio-loud AGN SRGE","J170245.3+130104 discovered at z $\\sim$ 5.5 provides unique chances to probe the SMBH growth and evolution with powerful jets in the early Universe.","Methods.","We present 1.35 - 5.1 GHz Very Long Baseline Array (VLBA) results on the radio continuum emission and spectrum analysis for this quasar in a low flux density state.","Results.","This source is unresolved at three frequencies with the total flux densities of 8.35$\\pm$0.09 mJy beam-1, 7.47$\\pm$0.08 mJy beam-1, and 6.57$\\pm$0.02 mJy beam-1 at 1.73 GHz, 2.26 GHz, and 4.87 GHz, respectively.","Meanwhile, the brightness temperature is higher than 109 K. Conclusions.","Compared with previous radio observations with arcsec-scale resolution, nearly all the radio emission from this source concentrates in the very central milli-arcsecond (mas) scale area.","We confirm this source is a bright blazar at z > 5.","This young AGN provide us the great chances to understand the first generation of strong jets in the early Universe."],"url":"http://arxiv.org/abs/2402.19202v1","category":"astro-ph.GA"}
{"created":"2024-02-29 14:30:28","title":"Rewriting and Inductive Reasoning","abstract":"Rewriting techniques based on reduction orderings generate \"just enough\" consequences to retain first-order completeness. This is ideal for superposition-based first-order theorem proving, but for at least one approach to inductive reasoning we show that we are missing crucial consequences. We therefore extend the superposition calculus with rewriting-based techniques to generate sufficient consequences for automating induction in saturation. When applying our work within the unit-equational fragment, our experiments with the theorem prover Vampire show significant improvements for inductive reasoning.","sentences":["Rewriting techniques based on reduction orderings generate \"just enough\" consequences to retain first-order completeness.","This is ideal for superposition-based first-order theorem proving, but for at least one approach to inductive reasoning we show that we are missing crucial consequences.","We therefore extend the superposition calculus with rewriting-based techniques to generate sufficient consequences for automating induction in saturation.","When applying our work within the unit-equational fragment, our experiments with the theorem prover Vampire show significant improvements for inductive reasoning."],"url":"http://arxiv.org/abs/2402.19199v1","category":"cs.LO"}
{"created":"2024-02-29 14:30:28","title":"PRSA: Prompt Reverse Stealing Attacks against Large Language Models","abstract":"Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance. With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users. However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers? To our knowledge, this problem still has not been comprehensively explored yet. To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA. The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and gradually infer (steal) the target prompts. In detail, PRSA mainly consists of two key phases: prompt mutation and prompt pruning. In the mutation phase, we propose a prompt attention algorithm based on differential feedback to capture these critical features for effectively inferring the target prompts. In the prompt pruning phase, we identify and mask the words dependent on specific inputs, enabling the prompts to accommodate diverse inputs for generalization. Through extensive evaluation, we verify that PRSA poses a severe threat in real world scenarios. We have reported these findings to prompt service providers and actively collaborate with them to take protective measures for prompt copyright.","sentences":["Prompt, recognized as crucial intellectual property, enables large language models (LLMs) to perform specific tasks without the need of fine-tuning, underscoring their escalating importance.","With the rise of prompt-based services, such as prompt marketplaces and LLM applications, providers often display prompts' capabilities through input-output examples to attract users.","However, this paradigm raises a pivotal security concern: does the exposure of input-output pairs pose the risk of potential prompt leakage, infringing on the intellectual property rights of the developers?","To our knowledge, this problem still has not been comprehensively explored yet.","To remedy this gap, in this paper, we perform the first in depth exploration and propose a novel attack framework for reverse-stealing prompts against commercial LLMs, namely PRSA.","The main idea of PRSA is that by analyzing the critical features of the input-output pairs, we mimic and gradually infer (steal) the target prompts.","In detail, PRSA mainly consists of two key phases: prompt mutation and prompt pruning.","In the mutation phase, we propose a prompt attention algorithm based on differential feedback to capture these critical features for effectively inferring the target prompts.","In the prompt pruning phase, we identify and mask the words dependent on specific inputs, enabling the prompts to accommodate diverse inputs for generalization.","Through extensive evaluation, we verify that PRSA poses a severe threat in real world scenarios.","We have reported these findings to prompt service providers and actively collaborate with them to take protective measures for prompt copyright."],"url":"http://arxiv.org/abs/2402.19200v1","category":"cs.CR"}
{"created":"2024-02-29 14:26:46","title":"Fine Structure-Aware Sampling: A New Sampling Training Scheme for Pixel-Aligned Implicit Models in Single-View Human Reconstruction","abstract":"Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction. These models need to be trained using a sampling training scheme. Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes. To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction. FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces. In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results. Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models. It becomes computationally feasible to introduce this loss once a slight reworking of the pixel-aligned implicit function framework is carried out. Our results show that our methods significantly outperform SOTA methods qualitatively and quantitatively. Our code is publicly available at https://github.com/kcyt/FSS.","sentences":["Pixel-aligned implicit models, such as PIFu, PIFuHD, and ICON, are used for single-view clothed human reconstruction.","These models need to be trained using a sampling training scheme.","Existing sampling training schemes either fail to capture thin surfaces (e.g. ears, fingers) or cause noisy artefacts in reconstructed meshes.","To address these problems, we introduce Fine Structured-Aware Sampling (FSS), a new sampling training scheme to train pixel-aligned implicit models for single-view human reconstruction.","FSS resolves the aforementioned problems by proactively adapting to the thickness and complexity of surfaces.","In addition, unlike existing sampling training schemes, FSS shows how normals of sample points can be capitalized in the training process to improve results.","Lastly, to further improve the training process, FSS proposes a mesh thickness loss signal for pixel-aligned implicit models.","It becomes computationally feasible to introduce this loss once a slight reworking of the pixel-aligned implicit function framework is carried out.","Our results show that our methods significantly outperform SOTA methods qualitatively and quantitatively.","Our code is publicly available at https://github.com/kcyt/FSS."],"url":"http://arxiv.org/abs/2402.19197v1","category":"cs.CV"}
{"created":"2024-02-29 14:26:41","title":"Generative models struggle with kirigami metamaterials","abstract":"Generative machine learning models have shown notable success in identifying architectures for metamaterials - materials whose behavior is determined primarily by their internal organization - that match specific target properties. By examining kirigami metamaterials, in which dependencies between cuts yield complex design restrictions, we demonstrate that this perceived success in the employment of generative models for metamaterials might be akin to survivorship bias. We assess the performance of the four most popular generative models - the Variational Autoencoder (VAE), the Generative Adversarial Network (GAN), the Wasserstein GAN (WGAN), and the Denoising Diffusion Probabilistic Model (DDPM) - in generating kirigami structures. Prohibiting cut intersections can prevent the identification of an appropriate similarity measure for kirigami metamaterials, significantly impacting the effectiveness of VAE and WGAN, which rely on the Euclidean distance - a metric shown to be unsuitable for considered geometries. This imposes significant limitations on employing modern generative models for the creation of diverse metamaterials.","sentences":["Generative machine learning models have shown notable success in identifying architectures for metamaterials - materials whose behavior is determined primarily by their internal organization - that match specific target properties.","By examining kirigami metamaterials, in which dependencies between cuts yield complex design restrictions, we demonstrate that this perceived success in the employment of generative models for metamaterials might be akin to survivorship bias.","We assess the performance of the four most popular generative models - the Variational Autoencoder (VAE), the Generative Adversarial Network (GAN), the Wasserstein GAN (WGAN), and the Denoising Diffusion Probabilistic Model (DDPM) - in generating kirigami structures.","Prohibiting cut intersections can prevent the identification of an appropriate similarity measure for kirigami metamaterials, significantly impacting the effectiveness of VAE and WGAN, which rely on the Euclidean distance - a metric shown to be unsuitable for considered geometries.","This imposes significant limitations on employing modern generative models for the creation of diverse metamaterials."],"url":"http://arxiv.org/abs/2402.19196v1","category":"cs.CE"}
{"created":"2024-02-29 14:26:20","title":"Negative Sampling in Knowledge Graph Representation Learning: A Review","abstract":"Knowledge graph representation learning (KGRL) or knowledge graph embedding (KGE) plays a crucial role in AI applications for knowledge construction and information exploration. These models aim to encode entities and relations present in a knowledge graph into a lower-dimensional vector space. During the training process of KGE models, using positive and negative samples becomes essential for discrimination purposes. However, obtaining negative samples directly from existing knowledge graphs poses a challenge, emphasizing the need for effective generation techniques. The quality of these negative samples greatly impacts the accuracy of the learned embeddings, making their generation a critical aspect of KGRL. This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL. Their respective advantages and disadvantages are outlined by categorizing existing NS methods into five distinct categories. Moreover, this survey identifies open research questions that serve as potential directions for future investigations. By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field.","sentences":["Knowledge graph representation learning (KGRL) or knowledge graph embedding (KGE) plays a crucial role in AI applications for knowledge construction and information exploration.","These models aim to encode entities and relations present in a knowledge graph into a lower-dimensional vector space.","During the training process of KGE models, using positive and negative samples becomes essential for discrimination purposes.","However, obtaining negative samples directly from existing knowledge graphs poses a challenge, emphasizing the need for effective generation techniques.","The quality of these negative samples greatly impacts the accuracy of the learned embeddings, making their generation a critical aspect of KGRL.","This comprehensive survey paper systematically reviews various negative sampling (NS) methods and their contributions to the success of KGRL.","Their respective advantages and disadvantages are outlined by categorizing existing NS methods into five distinct categories.","Moreover, this survey identifies open research questions that serve as potential directions for future investigations.","By offering a generalization and alignment of fundamental NS concepts, this survey provides valuable insights for designing effective NS methods in the context of KGRL and serves as a motivating force for further advancements in the field."],"url":"http://arxiv.org/abs/2402.19195v1","category":"cs.AI"}
{"created":"2024-02-29 14:26:05","title":"High Expectations: An Observational Study of Programming and Cannabis Intoxication","abstract":"Anecdotal evidence of cannabis use by professional programmers abounds. Recent studies have found that some professionals regularly use cannabis while programming even for work-related tasks. However, accounts of the impacts of cannabis on programming vary widely and are often contradictory. For example, some programmers claim that it impairs their ability to generate correct solutions while others claim it enhances creativity and focus. There remains a need for an empirical understanding of the true impacts of cannabis on programming. This paper presents the first controlled observational study of the effects of cannabis on programming ability. Based on a within-subjects design with over 70 participants, we find that at ecologically valid dosages, cannabis significantly impairs programming performance. Programs implemented while high contain more bugs and take longer to write (p < 0.05), a small to medium effect (0.22 <= d <= 0.44). We also did not find any evidence that high programmers generate more divergent solutions. However, programmers can accurately assess differences in their programming performance (r = 0.59), even when under the influence of cannabis. We hope that this research will facilitate evidence-based policies and help developers make informed decisions regarding cannabis use while programming.","sentences":["Anecdotal evidence of cannabis use by professional programmers abounds.","Recent studies have found that some professionals regularly use cannabis while programming even for work-related tasks.","However, accounts of the impacts of cannabis on programming vary widely and are often contradictory.","For example, some programmers claim that it impairs their ability to generate correct solutions while others claim it enhances creativity and focus.","There remains a need for an empirical understanding of the true impacts of cannabis on programming.","This paper presents the first controlled observational study of the effects of cannabis on programming ability.","Based on a within-subjects design with over 70 participants, we find that at ecologically valid dosages, cannabis significantly impairs programming performance.","Programs implemented while high contain more bugs and take longer to write (p < 0.05), a small to medium effect (0.22 <= d <= 0.44).","We also did not find any evidence that high programmers generate more divergent solutions.","However, programmers can accurately assess differences in their programming performance (r = 0.59), even when under the influence of cannabis.","We hope that this research will facilitate evidence-based policies and help developers make informed decisions regarding cannabis use while programming."],"url":"http://arxiv.org/abs/2402.19194v1","category":"cs.SE"}
{"created":"2024-02-29 14:22:24","title":"An asymptotic-preserving method for the three-temperature radiative transfer model","abstract":"We present an asymptotic-preserving (AP) numerical method for solving the three-temperature radiative transfer model, which holds significant importance in inertial confinement fusion. A carefully designedsplitting method is developed that can provide a general framework of extending AP schemes for the gray radiative transport equation to the more complex three-temperature radiative transfer model. The proposed scheme captures two important limiting models: the three-temperature radiation diffusion equation (3TRDE) when opacity approaches infinity and the two-temperature limit when the ion-electron coupling coefficient goes to infinity. We have rigorously demonstrated the AP property and energy conservation characteristics of the proposed scheme and its efficiency has been validated through a series of benchmark tests in the numerical part.","sentences":["We present an asymptotic-preserving (AP) numerical method for solving the three-temperature radiative transfer model, which holds significant importance in inertial confinement fusion.","A carefully designedsplitting method is developed that can provide a general framework of extending AP schemes for the gray radiative transport equation to the more complex three-temperature radiative transfer model.","The proposed scheme captures two important limiting models: the three-temperature radiation diffusion equation (3TRDE) when opacity approaches infinity and the two-temperature limit when the ion-electron coupling coefficient goes to infinity.","We have rigorously demonstrated the AP property and energy conservation characteristics of the proposed scheme and its efficiency has been validated through a series of benchmark tests in the numerical part."],"url":"http://arxiv.org/abs/2402.19191v1","category":"math.NA"}
{"created":"2024-02-29 14:13:46","title":"KGAMC: A Novel Knowledge Graph Driven Automatic Modulation Classification Scheme","abstract":"Automatic modulation classification (AMC) is a promising technology to realize intelligent wireless communications in the sixth generation (6G) wireless communication networks. Recently, many data-and-knowledge dual-driven AMC schemes have achieved high accuracy. However, most of these schemes focus on generating additional prior knowledge or features of blind signals, which consumes longer computation time and ignores the interpretability of the model learning process. To solve these problems, we propose a novel knowledge graph (KG) driven AMC (KGAMC) scheme by training the networks under the guidance of domain knowledge. A modulation knowledge graph (MKG) with the knowledge of modulation technical characteristics and application scenarios is constructed and a relation-graph convolution network (RGCN) is designed to extract knowledge of the MKG. This knowledge is utilized to facilitate the signal features separation of the data-oriented model by implementing a specialized feature aggregation method. Simulation results demonstrate that KGAMC achieves superior classification performance compared to other benchmark schemes, especially in the low signal-to-noise ratio (SNR) range. Furthermore, the signal features of the high-order modulation are more discriminative, thus reducing the confusion between similar signals.","sentences":["Automatic modulation classification (AMC) is a promising technology to realize intelligent wireless communications in the sixth generation (6G) wireless communication networks.","Recently, many data-and-knowledge dual-driven AMC schemes have achieved high accuracy.","However, most of these schemes focus on generating additional prior knowledge or features of blind signals, which consumes longer computation time and ignores the interpretability of the model learning process.","To solve these problems, we propose a novel knowledge graph (KG) driven AMC (KGAMC) scheme by training the networks under the guidance of domain knowledge.","A modulation knowledge graph (MKG) with the knowledge of modulation technical characteristics and application scenarios is constructed and a relation-graph convolution network (RGCN) is designed to extract knowledge of the MKG.","This knowledge is utilized to facilitate the signal features separation of the data-oriented model by implementing a specialized feature aggregation method.","Simulation results demonstrate that KGAMC achieves superior classification performance compared to other benchmark schemes, especially in the low signal-to-noise ratio (SNR) range.","Furthermore, the signal features of the high-order modulation are more discriminative, thus reducing the confusion between similar signals."],"url":"http://arxiv.org/abs/2402.19188v1","category":"eess.SP"}
{"created":"2024-02-29 14:11:08","title":"Disentangling representations of retinal images with generative models","abstract":"Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demonstrate the effectiveness of this novel loss function in disentangling the learned subspaces. Our results show that our model provides a new perspective on the complex relationship between patient attributes and technical confounders in retinal fundus image generation.","sentences":["Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders.","However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology.","For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process.","Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation.","To achieve this, we propose a novel disentanglement loss based on distance correlation.","Through qualitative and quantitative analyses, we demonstrate the effectiveness of this novel loss function in disentangling the learned subspaces.","Our results show that our model provides a new perspective on the complex relationship between patient attributes and technical confounders in retinal fundus image generation."],"url":"http://arxiv.org/abs/2402.19186v1","category":"cs.CV"}
{"created":"2024-02-29 14:09:40","title":"Properties of Hagen-Poiseuille flows in channel networks","abstract":"We derive the main properties of adaptive Hagen-Poiseuille flows in elastic microchannel networks akin to biological veins in organisms. We show that adaptive Hagen-Poiseuille flows successfully simulate key features of \\textit{Physarum polycephalum} networks, replicating physiological out-of-equilibrium phenomena like peristalsis and shuttle streaming, associated with the mechanism of nutrient transport in \\textit{Physarum}. A new topological steady state has been identified for asynchronous adaptation, supporting out-of-equilibrium laminar fluxes. Adaptive Hagen-Poiseuille flows show saturation effects on the fluxes in contractile veins, as observed in animal and artificial contractile veins.","sentences":["We derive the main properties of adaptive Hagen-Poiseuille flows in elastic microchannel networks akin to biological veins in organisms.","We show that adaptive Hagen-Poiseuille flows successfully simulate key features of \\textit{Physarum polycephalum} networks, replicating physiological out-of-equilibrium phenomena like peristalsis and shuttle streaming, associated with the mechanism of nutrient transport in \\textit{Physarum}.","A new topological steady state has been identified for asynchronous adaptation, supporting out-of-equilibrium laminar fluxes.","Adaptive Hagen-Poiseuille flows show saturation effects on the fluxes in contractile veins, as observed in animal and artificial contractile veins."],"url":"http://arxiv.org/abs/2402.19185v1","category":"q-bio.CB"}
{"created":"2024-02-29 14:05:20","title":"Data Transfer Optimizations for Host-CPU and Accelerators in AXI4MLIR","abstract":"As custom hardware accelerators become more prevalent, it becomes increasingly important to automatically generate efficient host-driver code that can fully leverage the capabilities of these accelerators. This approach saves time and reduces the likelihood of errors that can occur during manual implementation. AXI4MLIR extends the MLIR compiler framework to generate host-driver code for custom accelerators for linear algebra problems. By leveraging specific compiler optimizations, we can further increase accelerator utilization.   In this work we offer two key observations through a MatMul accelerator case study. First, the accelerator's compute core utilization is less than 10%, and second, the critical latency bottleneck is caused by copying data between the heap and memory-mapped DMA buffers. We identify a set of missing host code optimizations to improve the under-utilization and the latency bottleneck. Therefore, we propose three key host-code data-movement-related optimizations, extending AXI4MLIR. The optimizations provide DMA-based data allocation, coalescing of DMA transfers, and pipelining of the accelerator's load, compute, and store stages.","sentences":["As custom hardware accelerators become more prevalent, it becomes increasingly important to automatically generate efficient host-driver code that can fully leverage the capabilities of these accelerators.","This approach saves time and reduces the likelihood of errors that can occur during manual implementation.","AXI4MLIR extends the MLIR compiler framework to generate host-driver code for custom accelerators for linear algebra problems.","By leveraging specific compiler optimizations, we can further increase accelerator utilization.   ","In this work we offer two key observations through a MatMul accelerator case study.","First, the accelerator's compute core utilization is less than 10%, and second, the critical latency bottleneck is caused by copying data between the heap and memory-mapped DMA buffers.","We identify a set of missing host code optimizations to improve the under-utilization and the latency bottleneck.","Therefore, we propose three key host-code data-movement-related optimizations, extending AXI4MLIR.","The optimizations provide DMA-based data allocation, coalescing of DMA transfers, and pipelining of the accelerator's load, compute, and store stages."],"url":"http://arxiv.org/abs/2402.19184v1","category":"cs.PL"}
{"created":"2024-02-29 13:54:28","title":"On an $n$-ary generalization of the Lie representation","abstract":"We continue our study, initiated in our prior work with Richard Stanley, of the representation of the symmetric group on the multilinear component of an $n$-ary generalization of the free Lie algebra known as the free Fillipov $n$-algebra with $k$ brackets. Our ultimate aim is to determine the multiplicities of the irreducible representations in this representation. This had been done for the ordinary Lie representation ($n=2$ case) by Kraskiewicz and Weyman. The $k=2$ case was handled in our prior work, where the representation was shown to be isomorphic to $S^{2^{n-1}1}$. In this paper, for general $n$ and $k$, we obtain decomposition results that enable us to prove that in the $k=3$ case, the representation is isomorphic to $S^{3^{n-1}1} \\oplus S^{3^{n-2}21^2}$.","sentences":["We continue our study, initiated in our prior work with Richard Stanley, of the representation of the symmetric group on the multilinear component of an $n$-ary generalization of the free Lie algebra known as the free Fillipov $n$-algebra with $k$ brackets.","Our ultimate aim is to determine the multiplicities of the irreducible representations in this representation.","This had been done for the ordinary Lie representation ($n=2$ case) by Kraskiewicz and Weyman.","The $k=2$ case was handled in our prior work, where the representation was shown to be isomorphic to $S^{2^{n-1}1}$. In this paper, for general $n$ and $k$, we obtain decomposition results that enable us to prove that in the $k=3$ case, the representation is isomorphic to $S^{3^{n-1}1} \\oplus S^{3^{n-2}21^2}$."],"url":"http://arxiv.org/abs/2402.19174v1","category":"math.CO"}
{"created":"2024-02-29 13:53:35","title":"StarCoder 2 and The Stack v2: The Next Generation","abstract":"The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.","sentences":["The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2.","In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive.","Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation.","This results in a training set that is 4x larger than the first StarCoder dataset.","We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks.","We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B.","Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size.","In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size.","Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages.","We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data."],"url":"http://arxiv.org/abs/2402.19173v1","category":"cs.SE"}
{"created":"2024-02-29 13:53:21","title":"Point Processes and spatial statistics in time-frequency analysis","abstract":"A finite-energy signal is represented by a square-integrable, complex-valued function $t\\mapsto s(t)$ of a real variable $t$, interpreted as time. Similarly, a noisy signal is represented by a random process. Time-frequency analysis, a subfield of signal processing, amounts to describing the temporal evolution of the frequency content of a signal. Loosely speaking, if $s$ is the audio recording of a musical piece, time-frequency analysis somehow consists in writing the musical score of the piece. Mathematically, the operation is performed through a transform $\\mathcal{V}$, mapping $s \\in L^2(\\mathbb{R})$ onto a complex-valued function $\\mathcal{V}s \\in L^2(\\mathbb{R}^2)$ of time $t$ and angular frequency $\\omega$. The squared modulus $(t, \\omega) \\mapsto \\vert\\mathcal{V}s(t,\\omega)\\vert^2$ of the time-frequency representation is known as the spectrogram of $s$; in the musical score analogy, a peaked spectrogram at $(t_0,\\omega_0)$ corresponds to a musical note at angular frequency $\\omega_0$ localized at time $t_0$. More generally, the intuition is that upper level sets of the spectrogram contain relevant information about in the original signal. Hence, many signal processing algorithms revolve around identifying maxima of the spectrogram. In contrast, zeros of the spectrogram indicate perfect silence, that is, a time at which a particular frequency is absent. Assimilating $\\mathbb{R}^2$ to $\\mathbb{C}$ through $z = \\omega + \\mathrm{i}t$, this chapter focuses on time-frequency transforms $\\mathcal{V}$ that map signals to analytic functions. The zeros of the spectrogram of a noisy signal are then the zeros of a random analytic function, hence forming a Point Process in $\\mathbb{C}$. This chapter is devoted to the study of these Point Processes, to their links with zeros of Gaussian Analytic Functions, and to designing signal detection and denoising algorithms using spatial statistics.","sentences":["A finite-energy signal is represented by a square-integrable, complex-valued function $t\\mapsto s(t)$ of a real variable $t$, interpreted as time.","Similarly, a noisy signal is represented by a random process.","Time-frequency analysis, a subfield of signal processing, amounts to describing the temporal evolution of the frequency content of a signal.","Loosely speaking, if $s$ is the audio recording of a musical piece, time-frequency analysis somehow consists in writing the musical score of the piece.","Mathematically, the operation is performed through a transform $\\mathcal{V}$, mapping $s \\in L^2(\\mathbb{R})$ onto a complex-valued function $\\mathcal{V}s \\in L^2(\\mathbb{R}^2)$ of time $t$ and angular frequency $\\omega$. The squared modulus $(t, \\omega) \\mapsto \\vert\\mathcal{V}s(t,\\omega)\\vert^2$ of the time-frequency representation is known as the spectrogram of $s$; in the musical score analogy, a peaked spectrogram at $(t_0,\\omega_0)$ corresponds to a musical note at angular frequency $\\omega_0$ localized at time $t_0$. More generally, the intuition is that upper level sets of the spectrogram contain relevant information about in the original signal.","Hence, many signal processing algorithms revolve around identifying maxima of the spectrogram.","In contrast, zeros of the spectrogram indicate perfect silence, that is, a time at which a particular frequency is absent.","Assimilating $\\mathbb{R}^2$ to $\\mathbb{C}$ through $z = \\omega + \\mathrm{i}t$, this chapter focuses on time-frequency transforms $\\mathcal{V}$ that map signals to analytic functions.","The zeros of the spectrogram of a noisy signal are then the zeros of a random analytic function, hence forming a Point Process in $\\mathbb{C}$. This chapter is devoted to the study of these Point Processes, to their links with zeros of Gaussian Analytic Functions, and to designing signal detection and denoising algorithms using spatial statistics."],"url":"http://arxiv.org/abs/2402.19172v1","category":"eess.SP"}
{"created":"2024-02-29 13:52:39","title":"Towards Assessing Spread in Sets of Software Architecture Designs","abstract":"Several approaches have recently used automated techniques to generate architecture design alternatives by means of optimization techniques. These approaches aim at improving an initial architecture with respect to quality aspects, such as performance, reliability, or maintainability. In this context, each optimization experiment usually produces a different set of architecture alternatives that is characterized by specific settings. As a consequence, the designer is left with the task of comparing such sets to identify the settings that lead to better solution sets for the problem. To assess the quality of solution sets, multi-objective optimization commonly relies on quality indicators. Among these, the quality indicator for the maximum spread estimates the diversity of the generated alternatives, providing a measure of how much of the solution space has been explored. However, the maximum spread indicator is computed only on the objective space and does not consider architectural information (e.g., components structure, design decisions) from the architectural space. In this paper, we propose a quality indicator for the spread that assesses the diversity of alternatives by taking into account architectural features. To compute the spread, we rely on a notion of distance between alternatives according to the way they were generated during the optimization. We demonstrate how our architectural quality indicator can be applied to a dataset from the literature.","sentences":["Several approaches have recently used automated techniques to generate architecture design alternatives by means of optimization techniques.","These approaches aim at improving an initial architecture with respect to quality aspects, such as performance, reliability, or maintainability.","In this context, each optimization experiment usually produces a different set of architecture alternatives that is characterized by specific settings.","As a consequence, the designer is left with the task of comparing such sets to identify the settings that lead to better solution sets for the problem.","To assess the quality of solution sets, multi-objective optimization commonly relies on quality indicators.","Among these, the quality indicator for the maximum spread estimates the diversity of the generated alternatives, providing a measure of how much of the solution space has been explored.","However, the maximum spread indicator is computed only on the objective space and does not consider architectural information (e.g., components structure, design decisions) from the architectural space.","In this paper, we propose a quality indicator for the spread that assesses the diversity of alternatives by taking into account architectural features.","To compute the spread, we rely on a notion of distance between alternatives according to the way they were generated during the optimization.","We demonstrate how our architectural quality indicator can be applied to a dataset from the literature."],"url":"http://arxiv.org/abs/2402.19171v1","category":"cs.SE"}
{"created":"2024-02-29 13:52:33","title":"Improving Legal Judgement Prediction in Romanian with Long Text Encoders","abstract":"In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks. Legal NLP domain has also been part of this process, as it has seen an impressive growth. However, general-purpose models are not readily applicable for legal domain. Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP. In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP). We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora. Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized models and handling long texts are critical for a good performance.","sentences":["In recent years,the entire field of Natural Language Processing (NLP) has enjoyed amazing novel results achieving almost human-like performance on a variety of tasks.","Legal NLP domain has also been part of this process, as it has seen an impressive growth.","However, general-purpose models are not readily applicable for legal domain.","Due to the nature of the domain (e.g. specialized vocabulary, long documents) specific models and methods are often needed for Legal NLP.","In this work we investigate both specialized and general models for predicting the final ruling of a legal case, task known as Legal Judgment Prediction (LJP).","We particularly focus on methods to extend to sequence length of Transformer-based models to better understand the long documents present in legal corpora.","Extensive experiments on 4 LJP datasets in Romanian, originating from 2 sources with significantly different sizes and document lengths, show that specialized models and handling long texts are critical for a good performance."],"url":"http://arxiv.org/abs/2402.19170v1","category":"cs.CL"}
{"created":"2024-02-29 13:50:28","title":"Conversational Language Models for Human-in-the-Loop Multi-Robot Coordination","abstract":"With the increasing prevalence and diversity of robots interacting in the real world, there is need for flexible, on-the-fly planning and cooperation. Large Language Models are starting to be explored in a multimodal setup for communication, coordination, and planning in robotics. Existing approaches generally use a single agent building a plan, or have multiple homogeneous agents coordinating for a simple task. We present a decentralised, dialogical approach in which a team of agents with different abilities plans solutions through peer-to-peer and human-robot discussion. We suggest that argument-style dialogues are an effective way to facilitate adaptive use of each agent's abilities within a cooperative team. Two robots discuss how to solve a cleaning problem set by a human, define roles, and agree on paths they each take. Each step can be interrupted by a human advisor and agents check their plans with the human. Agents then execute this plan in the real world, collecting rubbish from people in each room. Our implementation uses text at every step, maintaining transparency and effective human-multi-robot interaction.","sentences":["With the increasing prevalence and diversity of robots interacting in the real world, there is need for flexible, on-the-fly planning and cooperation.","Large Language Models are starting to be explored in a multimodal setup for communication, coordination, and planning in robotics.","Existing approaches generally use a single agent building a plan, or have multiple homogeneous agents coordinating for a simple task.","We present a decentralised, dialogical approach in which a team of agents with different abilities plans solutions through peer-to-peer and human-robot discussion.","We suggest that argument-style dialogues are an effective way to facilitate adaptive use of each agent's abilities within a cooperative team.","Two robots discuss how to solve a cleaning problem set by a human, define roles, and agree on paths they each take.","Each step can be interrupted by a human advisor and agents check their plans with the human.","Agents then execute this plan in the real world, collecting rubbish from people in each room.","Our implementation uses text at every step, maintaining transparency and effective human-multi-robot interaction."],"url":"http://arxiv.org/abs/2402.19166v1","category":"cs.RO"}
{"created":"2024-02-29 13:47:23","title":"FedStruct: Federated Decoupled Learning over Interconnected Graphs","abstract":"We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.","sentences":["We address the challenge of federated learning on graph-structured data distributed across multiple clients.","Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role.","We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies.","To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients.","Instead, it leverages explicit global graph structure information to capture inter-node dependencies.","We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients."],"url":"http://arxiv.org/abs/2402.19163v1","category":"cs.LG"}
{"created":"2024-02-29 13:45:13","title":"MemoNav: Working Memory Model for Visual Navigation","abstract":"Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments. Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction. To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance. Specifically, we employ three types of navigation memory. The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated. A forgetting module then retains the informative STM fraction to increase efficiency. We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features. Subsequently, a graph attention module encodes the retained STM and the LTM to generate working memory (WM) which contains the scene features essential for efficient navigation. The synergy among these three memory types boosts navigation performance by enabling the agent to learn and leverage goal-relevant scene features within a topological map. Our evaluation on multi-goal tasks demonstrates that MemoNav significantly outperforms previous methods across all difficulty levels in both Gibson and Matterport3D scenes. Qualitative results further illustrate that MemoNav plans more efficient routes.","sentences":["Image-goal navigation is a challenging task that requires an agent to navigate to a goal indicated by an image in unfamiliar environments.","Existing methods utilizing diverse scene memories suffer from inefficient exploration since they use all historical observations for decision-making without considering the goal-relevant fraction.","To address this limitation, we present MemoNav, a novel memory model for image-goal navigation, which utilizes a working memory-inspired pipeline to improve navigation performance.","Specifically, we employ three types of navigation memory.","The node features on a map are stored in the short-term memory (STM), as these features are dynamically updated.","A forgetting module then retains the informative STM fraction to increase efficiency.","We also introduce long-term memory (LTM) to learn global scene representations by progressively aggregating STM features.","Subsequently, a graph attention module encodes the retained STM and the LTM to generate working memory (WM) which contains the scene features essential for efficient navigation.","The synergy among these three memory types boosts navigation performance by enabling the agent to learn and leverage goal-relevant scene features within a topological map.","Our evaluation on multi-goal tasks demonstrates that MemoNav significantly outperforms previous methods across all difficulty levels in both Gibson and Matterport3D scenes.","Qualitative results further illustrate that MemoNav plans more efficient routes."],"url":"http://arxiv.org/abs/2402.19161v1","category":"cs.CV"}
{"created":"2024-02-29 13:44:14","title":"Trajectory Consistency Distillation","abstract":"Latent Consistency Model (LCM) extends the Consistency Model to the latent space and leverages the guided consistency distillation technique to achieve impressive performance in accelerating text-to-image synthesis. However, we observed that LCM struggles to generate images with both clarity and detailed intricacy. To address this limitation, we initially delve into and elucidate the underlying causes. Our investigation identifies that the primary issue stems from errors in three distinct areas. Consequently, we introduce Trajectory Consistency Distillation (TCD), which encompasses trajectory consistency function and strategic stochastic sampling. The trajectory consistency function diminishes the distillation errors by broadening the scope of the self-consistency boundary condition and endowing the TCD with the ability to accurately trace the entire trajectory of the Probability Flow ODE. Additionally, strategic stochastic sampling is specifically designed to circumvent the accumulated errors inherent in multi-step consistency sampling, which is meticulously tailored to complement the TCD model. Experiments demonstrate that TCD not only significantly enhances image quality at low NFEs but also yields more detailed results compared to the teacher model at high NFEs.","sentences":["Latent Consistency Model (LCM) extends the Consistency Model to the latent space and leverages the guided consistency distillation technique to achieve impressive performance in accelerating text-to-image synthesis.","However, we observed that LCM struggles to generate images with both clarity and detailed intricacy.","To address this limitation, we initially delve into and elucidate the underlying causes.","Our investigation identifies that the primary issue stems from errors in three distinct areas.","Consequently, we introduce Trajectory Consistency Distillation (TCD), which encompasses trajectory consistency function and strategic stochastic sampling.","The trajectory consistency function diminishes the distillation errors by broadening the scope of the self-consistency boundary condition and endowing the TCD with the ability to accurately trace the entire trajectory of the Probability Flow ODE.","Additionally, strategic stochastic sampling is specifically designed to circumvent the accumulated errors inherent in multi-step consistency sampling, which is meticulously tailored to complement the TCD model.","Experiments demonstrate that TCD not only significantly enhances image quality at low NFEs but also yields more detailed results compared to the teacher model at high NFEs."],"url":"http://arxiv.org/abs/2402.19159v1","category":"cs.CV"}
{"created":"2024-02-29 13:33:42","title":"Banach lattices with upper $p$-estimates: free and injective objects","abstract":"We study the free Banach lattice $FBL^{(p,\\infty)}[E]$ with upper $p$-estimates generated by a Banach space $E$. Using a classical result of Pisier on factorization through $L^{p,\\infty}(\\mu)$ together with a finite dimensional reduction, it is shown that the spaces $\\ell^{p,\\infty}(n)$ witness the universal property of $FBL^{(p,\\infty)}[E]$ isomorphically. As a consequence, we obtain a functional representation for $FBL^{(p,\\infty)}[E]$. More generally, our proof allows us to identify the norm of any free Banach lattice over $E$ associated with a rearrangement invariant function space.   After obtaining the above functional representation, we take the first steps towards analyzing the fine structure of $FBL^{(p,\\infty)}[E]$. Notably, we prove that the norm for $FBL^{(p,\\infty)}[E]$ cannot be isometrically witnessed by $L^{p,\\infty}(\\mu)$ and settle the question of characterizing when an embedding between Banach spaces extends to a lattice embedding between the corresponding free Banach lattices with upper $p$-estimates. To prove this latter result, we introduce a novel push-out argument, which when combined with the injectivity of $\\ell^p$ allows us to give an alternative proof of the subspace problem for free $p$-convex Banach lattices. On the other hand, we prove that $\\ell^{p,\\infty}$ is not injective in the class of Banach lattices with upper $p$-estimates, elucidating one of many difficulties arising in the study of $FBL^{(p,\\infty)}[E]$.","sentences":["We study the free Banach lattice $FBL^{(p,\\infty)}[E]$ with upper $p$-estimates generated by a Banach space $E$. Using a classical result of Pisier on factorization through $L^{p,\\infty}(\\mu)$ together with a finite dimensional reduction, it is shown that the spaces $\\ell^{p,\\infty}(n)$ witness the universal property of $FBL^{(p,\\infty)}[E]$ isomorphically.","As a consequence, we obtain a functional representation for $FBL^{(p,\\infty)}[E]$. More generally, our proof allows us to identify the norm of any free Banach lattice over $E$ associated with a rearrangement invariant function space.   ","After obtaining the above functional representation, we take the first steps towards analyzing the fine structure of $FBL^{(p,\\infty)}[E]$. Notably, we prove that the norm for $FBL^{(p,\\infty)}[E]$ cannot be isometrically witnessed by $L^{p,\\infty}(\\mu)$ and settle the question of characterizing when an embedding between Banach spaces extends to a lattice embedding between the corresponding free Banach lattices with upper $p$-estimates.","To prove this latter result, we introduce a novel push-out argument, which when combined with the injectivity of $\\ell^p$ allows us to give an alternative proof of the subspace problem for free $p$-convex Banach lattices.","On the other hand, we prove that $\\ell^{p,\\infty}$ is not injective in the class of Banach lattices with upper $p$-estimates, elucidating one of many difficulties arising in the study of $FBL^{(p,\\infty)}[E]$."],"url":"http://arxiv.org/abs/2402.19152v1","category":"math.FA"}
{"created":"2024-02-29 13:31:18","title":"Couette flow turbulence reduction by the flow spanwise reflection symmetry breaking: On the universality of the control strategy","abstract":"A novel turbulence control strategy for wall-bounded shear flow is proposed by Chagelishvili et al, 2014. The essence of this strategy involves continuously imposition of specially designed seed velocity perturbations with spanwise asymmetry near the flow wall. The configuration of this imposed velocity field, enhanced due to the shear flow non-normality, breaks the flow spanwise reflection symmetry, specifically, resulting in the generation of a secondary nonuniform spanwise mean flow. Consequently, this secondary flow significantly reduces flow turbulence. In Chagelishvili et al, 2014, the first steps were taken towards developing this new turbulence control strategy and demonstrating its efficiency. The plane Couette flow was considered, as a representative example, and a theoretical and hypothetical weak near-wall volume forcing was designed, which though theoretical, provided valuable insights into the characteristics of the seed velocity field. Obviously, the practical significance of this control strategy should be confirmed by evaluating its effectiveness in flows at various Reynolds numbers, and with different parameters of the imposed seed velocity field. In this paper, we investigate the effectiveness and universality of the turbulence control strategy for Couette flow at various Reynolds numbers and different locations of the volume forcing. Through direct numerical simulations, we show universality of the discussed turbulence control method. The application of a specially designed, weak near-wall volume forcing with a fixed configuration and amplitude results in the same effective turbulence control, reducing turbulence kinetic energy production by 30-40\\% across a wider range Reynolds numbers, $Re_{\\tau} = 52,92,128,270$ and various localizations.","sentences":["A novel turbulence control strategy for wall-bounded shear flow is proposed by Chagelishvili et al, 2014.","The essence of this strategy involves continuously imposition of specially designed seed velocity perturbations with spanwise asymmetry near the flow wall.","The configuration of this imposed velocity field, enhanced due to the shear flow non-normality, breaks the flow spanwise reflection symmetry, specifically, resulting in the generation of a secondary nonuniform spanwise mean flow.","Consequently, this secondary flow significantly reduces flow turbulence.","In Chagelishvili et al, 2014, the first steps were taken towards developing this new turbulence control strategy and demonstrating its efficiency.","The plane Couette flow was considered, as a representative example, and a theoretical and hypothetical weak near-wall volume forcing was designed, which though theoretical, provided valuable insights into the characteristics of the seed velocity field.","Obviously, the practical significance of this control strategy should be confirmed by evaluating its effectiveness in flows at various Reynolds numbers, and with different parameters of the imposed seed velocity field.","In this paper, we investigate the effectiveness and universality of the turbulence control strategy for Couette flow at various Reynolds numbers and different locations of the volume forcing.","Through direct numerical simulations, we show universality of the discussed turbulence control method.","The application of a specially designed, weak near-wall volume forcing with a fixed configuration and amplitude results in the same effective turbulence control, reducing turbulence kinetic energy production by 30-40\\% across a wider range Reynolds numbers, $Re_{\\tau} = 52,92,128,270$ and various localizations."],"url":"http://arxiv.org/abs/2402.19148v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 13:31:08","title":"Efficient quaternion CUR method for low-rank approximation to quaternion matrix","abstract":"The low-rank quaternion matrix approximation has been successfully applied in many applications involving signal processing and color image processing. However, the cost of quaternion models for generating low-rank quaternion matrix approximation is sometimes considerable due to the computation of the quaternion singular value decomposition (QSVD), which limits their application to real large-scale data. To address this deficiency, an efficient quaternion matrix CUR (QMCUR) method for low-rank approximation is suggested, which provides significant acceleration in color image processing. We first explore the QMCUR approximation method, which uses actual columns and rows of the given quaternion matrix, instead of the costly QSVD. Additionally, two different sampling strategies are used to sample the above-selected columns and rows. Then, the perturbation analysis is performed on the QMCUR approximation of noisy versions of low-rank quaternion matrices. Extensive experiments on both synthetic and real data further reveal the superiority of the proposed algorithm compared with other algorithms for getting low-rank approximation, in terms of both efficiency and accuracy.","sentences":["The low-rank quaternion matrix approximation has been successfully applied in many applications involving signal processing and color image processing.","However, the cost of quaternion models for generating low-rank quaternion matrix approximation is sometimes considerable due to the computation of the quaternion singular value decomposition (QSVD), which limits their application to real large-scale data.","To address this deficiency, an efficient quaternion matrix CUR (QMCUR) method for low-rank approximation is suggested, which provides significant acceleration in color image processing.","We first explore the QMCUR approximation method, which uses actual columns and rows of the given quaternion matrix, instead of the costly QSVD.","Additionally, two different sampling strategies are used to sample the above-selected columns and rows.","Then, the perturbation analysis is performed on the QMCUR approximation of noisy versions of low-rank quaternion matrices.","Extensive experiments on both synthetic and real data further reveal the superiority of the proposed algorithm compared with other algorithms for getting low-rank approximation, in terms of both efficiency and accuracy."],"url":"http://arxiv.org/abs/2402.19147v1","category":"math.NA"}
{"created":"2024-02-29 13:29:53","title":"Computing Longest Common Subsequence under Cartesian-Tree Matching Model","abstract":"Two strings of the same length are said to Cartesian-tree match (CT-match) if their Cartesian-trees are isomorphic [Park et al., TCS 2020]. Cartesian-tree matching is a natural model that allows for capturing similarities of numerical sequences. Oizumi et al. [CPM 2022] showed that subsequence pattern matching under CT-matching model can be solved in polynomial time. This current article follows and extends this line of research: We present the first polynomial-time algorithm that finds the longest common subsequence under CT-matching of two given strings $S$ and $T$ of length $n$, in $O(n^6)$ time and $O(n^4)$ space for general ordered alphabets. We then show that the problem has a faster solution in the binary case, by presenting an $O(n^2 / \\log n)$-time and space algorithm.","sentences":["Two strings of the same length are said to Cartesian-tree match (CT-match) if their Cartesian-trees are isomorphic","[Park et al., TCS 2020].","Cartesian-tree matching is a natural model that allows for capturing similarities of numerical sequences.","Oizumi et al.","[CPM 2022] showed that subsequence pattern matching under CT-matching model can be solved in polynomial time.","This current article follows and extends this line of research: We present the first polynomial-time algorithm that finds the longest common subsequence under CT-matching of two given strings $S$ and $T$ of length $n$, in $O(n^6)$ time and $O(n^4)$ space for general ordered alphabets.","We then show that the problem has a faster solution in the binary case, by presenting an $O(n^2 / \\log n)$-time and space algorithm."],"url":"http://arxiv.org/abs/2402.19146v1","category":"cs.DS"}
{"created":"2024-02-29 13:29:10","title":"A SAM-guided Two-stream Lightweight Model for Anomaly Detection","abstract":"In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications. Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns. In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM. We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM's knowledge. To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions. Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps. Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO. We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM.","sentences":["In industrial anomaly detection, model efficiency and mobile-friendliness become the primary concerns in real-world applications.","Simultaneously, the impressive generalization capabilities of Segment Anything (SAM) have garnered broad academic attention, making it an ideal choice for localizing unseen anomalies and diverse real-world patterns.","In this paper, considering these two critical factors, we propose a SAM-guided Two-stream Lightweight Model for unsupervised anomaly detection (STLM) that not only aligns with the two practical application requirements but also harnesses the robust generalization capabilities of SAM.","We employ two lightweight image encoders, i.e., our two-stream lightweight module, guided by SAM's knowledge.","To be specific, one stream is trained to generate discriminative and general feature representations in both normal and anomalous regions, while the other stream reconstructs the same images without anomalies, which effectively enhances the differentiation of two-stream representations when facing anomalous regions.","Furthermore, we employ a shared mask decoder and a feature aggregation module to generate anomaly maps.","Our experiments conducted on MVTec AD benchmark show that STLM, with about 16M parameters and achieving an inference time in 20ms, competes effectively with state-of-the-art methods in terms of performance, 98.26% on pixel-level AUC and 94.92% on PRO.","We further experiment on more difficult datasets, e.g., VisA and DAGM, to demonstrate the effectiveness and generalizability of STLM."],"url":"http://arxiv.org/abs/2402.19145v1","category":"cs.CV"}
{"created":"2024-02-29 13:26:13","title":"Recurrence Theorem for Open Quantum Systems","abstract":"Quantum (Poincar\\'e) recurrence theorem are known for closed quantum (classical) systems. Can recurrence happen in open systems? We provide the recurrence theorem for open quantum systems via non-Hermitian (NH) description. We find that PT symmetry and pseudo-Hermitian symmetry protect recurrence for NH open quantum systems and the recurrence fails with the symmetry breaking.   Applying our theorem to PT-symmetric systems, we reveal why quantum recurrence happens in PT-unbroken phase but fails in PT-broken phase, which was misunderstood before.   A contradiction emerges when we apply our theorem to anti-PT symmetric systems and we settle it, revealing that distinguishability and von Neumann entropy are generally not effective to describe the information dynamics in NH systems.   A new approach is developed to investigate the information dynamics of NH systems. For anti-PT symmetric systems in PT-broken phase, we find there are three information-dynamics patterns: oscillations with an overall decrease (increase) , and periodic oscillations. The periodic oscillations (information complete retrieval) happen only if the spectrum of NH Hamiltonian is real. The three patterns degenerate to the periodic oscillation using distinguishability or von Neumann entropy because normalization of non-unitary evolved states leads to loss of information. We conclude with a discussion of the physical meaning behind the recurrence in open systems and give the direction of recurrence theorem not limited to conservative systems in classical mechanics.","sentences":["Quantum (Poincar\\'e) recurrence theorem are known for closed quantum (classical) systems.","Can recurrence happen in open systems?","We provide the recurrence theorem for open quantum systems via non-Hermitian (NH) description.","We find that PT symmetry and pseudo-Hermitian symmetry protect recurrence for NH open quantum systems and the recurrence fails with the symmetry breaking.   ","Applying our theorem to PT-symmetric systems, we reveal why quantum recurrence happens in PT-unbroken phase but fails in PT-broken phase, which was misunderstood before.   ","A contradiction emerges when we apply our theorem to anti-PT symmetric systems and we settle it, revealing that distinguishability and von Neumann entropy are generally not effective to describe the information dynamics in NH systems.   ","A new approach is developed to investigate the information dynamics of NH systems.","For anti-PT symmetric systems in PT-broken phase, we find there are three information-dynamics patterns: oscillations with an overall decrease (increase) , and periodic oscillations.","The periodic oscillations (information complete retrieval) happen only if the spectrum of NH Hamiltonian is real.","The three patterns degenerate to the periodic oscillation using distinguishability or von Neumann entropy because normalization of non-unitary evolved states leads to loss of information.","We conclude with a discussion of the physical meaning behind the recurrence in open systems and give the direction of recurrence theorem not limited to conservative systems in classical mechanics."],"url":"http://arxiv.org/abs/2402.19143v1","category":"quant-ph"}
{"created":"2024-02-29 13:24:29","title":"Inertial spin waves in spin spirals","abstract":"Inertial effects in spin dynamics emerge on picosecond time scales, giving rise to nutational excitations at THz frequencies. Here, we describe a general framework for investigating the precessional and nutational excitations in any type of spin structure within linear spin-wave theory. We consider the particular cases of planar and conical spin spirals in detail. We observe a change in the sign of the curvature of the high-frequency nutational spin-wave band as the spiral period is decreased when passing from the ferromagnetic to the antiferromagnetic limit. We identify conditions for the interaction parameters where the curvature changes sign and asymptotical flat bands are formed.","sentences":["Inertial effects in spin dynamics emerge on picosecond time scales, giving rise to nutational excitations at THz frequencies.","Here, we describe a general framework for investigating the precessional and nutational excitations in any type of spin structure within linear spin-wave theory.","We consider the particular cases of planar and conical spin spirals in detail.","We observe a change in the sign of the curvature of the high-frequency nutational spin-wave band as the spiral period is decreased when passing from the ferromagnetic to the antiferromagnetic limit.","We identify conditions for the interaction parameters where the curvature changes sign and asymptotical flat bands are formed."],"url":"http://arxiv.org/abs/2402.19141v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 13:24:11","title":"Globally hyperbolic evaporating black hole and the information loss issue","abstract":"We discuss the loss of information in black holes in the context of a globally hyperbolic spacetime that maintains unchanged the whole semiclassical picture except for the ``last evaporation breath,'' which actually pertains to full quantum gravity. Although observers outside the black hole will not have access to information that enters the horizon, no information is lost in the sense it is carried over from one Cauchy surface to the next one (provided the evolution is unitary). In this token, the loss of information in black holes becomes as benign (or distressing, at the reader's discretion) as in the usual classical stellar collapse into a non-evaporating black hole.","sentences":["We discuss the loss of information in black holes in the context of a globally hyperbolic spacetime that maintains unchanged the whole semiclassical picture except for the ``last evaporation breath,'' which actually pertains to full quantum gravity.","Although observers outside the black hole will not have access to information that enters the horizon, no information is lost in the sense it is carried over from one Cauchy surface to the next one (provided the evolution is unitary).","In this token, the loss of information in black holes becomes as benign (or distressing, at the reader's discretion) as in the usual classical stellar collapse into a non-evaporating black hole."],"url":"http://arxiv.org/abs/2402.19140v1","category":"gr-qc"}
{"created":"2024-02-29 13:21:21","title":"Generalized Pentagon Equations","abstract":"Drinfeld defined the Knizhinik--Zamolodchikov (KZ) associator $\\Phi_{\\rm KZ}$ by considering the regularized holonomy of the KZ connection along the {\\em droit chemin} $[0,1]$. The KZ associator is a group-like element of the free associative algebra with two generators, and it satisfies the pentagon equation.   In this paper, we consider paths on $\\mathbb{C}\\backslash \\{ z_1, \\dots, z_n\\}$ which start and end at tangential base points. These paths are not necessarily straight, and they may have a finite number of transversal self-intersections. We show that the regularized holonomy $H$ of the KZ connection associated to such a path satisfies a generalization of Drinfeld's pentagon equation. In this equation, we encounter $H$, $\\Phi_{\\rm KZ}$, and new factors associated to self-intersections, to tangential base points, and to the rotation number of the path.","sentences":["Drinfeld defined the Knizhinik--Zamolodchikov (KZ) associator $\\Phi_{\\rm KZ}$ by considering the regularized holonomy of the KZ connection along the {\\em droit chemin} $[0,1]$. The KZ associator is a group-like element of the free associative algebra with two generators, and it satisfies the pentagon equation.   ","In this paper, we consider paths on $\\mathbb{C}\\backslash \\{ z_1, \\dots, z_n\\}$ which start and end at tangential base points.","These paths are not necessarily straight, and they may have a finite number of transversal self-intersections.","We show that the regularized holonomy $H$ of the KZ connection associated to such a path satisfies a generalization of Drinfeld's pentagon equation.","In this equation, we encounter $H$, $\\Phi_{\\rm KZ}$, and new factors associated to self-intersections, to tangential base points, and to the rotation number of the path."],"url":"http://arxiv.org/abs/2402.19138v1","category":"math.QA"}
{"created":"2024-02-29 13:20:07","title":"Global well-posedness for 2D generalized Parabolic Anderson Model via paracontrolled calculus","abstract":"This article revisits the problem of global well-posedness for the generalized parabolic Anderson model on $\\mathbb{R}^+\\times \\mathbb{T}^2$ within the framework of paracontrolled calculus \\cite{GIP15}. The model is given by the equation:   \\begin{equation*}   (\\partial_t-\\Delta) u=F(u)\\eta   \\end{equation*}   where $\\eta\\in C^{-1-\\kappa}$ with $1/6>\\kappa>0$, and $F\\in C_b^2(\\mathbb{R})$. Assume that $\\eta\\in C^{-1-\\kappa}$ and can be lifted to enhanced noise, we derive new a priori bounds. The key idea follows from the recent work   \\cite{CFW24} by A.Chandra, G.L. Feltes and H.Weber to represent the leading error term as a transport type term, and our techniques encompass the paracontrolled calculus, the maximum principle, and the localization approach (i.e. high-low frequency argument).","sentences":["This article revisits the problem of global well-posedness for the generalized parabolic Anderson model on $\\mathbb{R}^+\\times \\mathbb{T}^2$ within the framework of paracontrolled calculus \\cite{GIP15}.","The model is given by the equation:   \\begin{equation*}   (\\partial_t-\\Delta) u=F(u)\\eta   \\end{equation*}   where $\\eta\\in C^{-1-\\kappa}$ with $1/6>\\kappa>0$, and $F\\in C_b^2(\\mathbb{R})$. Assume that $\\eta\\in C^{-1-\\kappa}$ and can be lifted to enhanced noise, we derive new a priori bounds.","The key idea follows from the recent work   \\cite{CFW24} by A.Chandra, G.L. Feltes and H.Weber to represent the leading error term as a transport type term, and our techniques encompass the paracontrolled calculus, the maximum principle, and the localization approach (i.e. high-low frequency argument)."],"url":"http://arxiv.org/abs/2402.19137v1","category":"math.AP"}
{"created":"2024-02-29 13:12:31","title":"Think Fast, Think Slow, Think Critical: Designing an Automated Propaganda Detection Tool","abstract":"In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies. This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition. Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking. Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking. The study thus offers both a practical tool and useful design knowledge for mitigating propaganda in digital news.","sentences":["In today's digital age, characterized by rapid news consumption and increasing vulnerability to propaganda, fostering citizens' critical thinking is crucial for stable democracies.","This paper introduces the design of ClarifAI, a novel automated propaganda detection tool designed to nudge readers towards more critical news consumption by activating the analytical mode of thinking, following Kahneman's dual-system theory of cognition.","Using Large Language Models, ClarifAI detects propaganda in news articles and provides context-rich explanations, enhancing users' understanding and critical thinking.","Our contribution is threefold: first, we propose the design of ClarifAI; second, in an online experiment, we demonstrate that this design effectively encourages news readers to engage in more critical reading; and third, we emphasize the value of explanations for fostering critical thinking.","The study thus offers both a practical tool and useful design knowledge for mitigating propaganda in digital news."],"url":"http://arxiv.org/abs/2402.19135v1","category":"cs.HC"}
{"created":"2024-02-29 13:09:00","title":"Weighted least $\\ell_p$ approximation on compact Riemannian manifolds","abstract":"Given a sequence of Marcinkiewicz-Zygmund inequalities in $L_2$ on a compact space, Gr\\\"ochenig in \\cite{G} discussed weighted least squares approximation and least squares quadrature. Inspired by this work, for all $1\\le p\\le\\infty$, we develop weighted least $\\ell_p$ approximation induced by a sequence of Marcinkiewicz-Zygmund inequalities in $L_p$ on a compact smooth Riemannian manifold $\\Bbb M$ with normalized Riemannian measure (typical examples are the torus and the sphere). In this paper we derive corresponding approximation theorems with the error measured in $L_q,\\,1\\le q\\le\\infty$, and least quadrature errors for both Sobolev spaces $H_p^r(\\Bbb M), \\, r>d/p$ generated by eigenfunctions associated with the Laplace-Beltrami operator and Besov spaces $B_{p,\\tau}^r(\\Bbb M),\\, 0<\\tau\\le \\infty, r>d/p $ defined by best polynomial approximation. Finally, we discuss the optimality of the obtained results by giving sharp estimates of sampling numbers and optimal quadrature errors for the aforementioned spaces.","sentences":["Given a sequence of Marcinkiewicz-Zygmund inequalities in $L_2$ on a compact space, Gr\\\"ochenig in \\cite{G} discussed weighted least squares approximation and least squares quadrature.","Inspired by this work, for all $1\\le p\\le\\infty$, we develop weighted least $\\ell_p$ approximation induced by a sequence of Marcinkiewicz-Zygmund inequalities in $L_p$ on a compact smooth Riemannian manifold $\\Bbb M$ with normalized Riemannian measure (typical examples are the torus and the sphere).","In this paper we derive corresponding approximation theorems with the error measured in $L_q,\\,1\\le q\\le\\infty$, and least quadrature errors for both Sobolev spaces $H_p^r(\\Bbb M), \\, r>d/p$ generated by eigenfunctions associated with the Laplace-Beltrami operator and Besov spaces $B_{p,\\tau}^r(\\Bbb M),\\, 0<\\tau\\le \\infty, r>d/p $ defined by best polynomial approximation.","Finally, we discuss the optimality of the obtained results by giving sharp estimates of sampling numbers and optimal quadrature errors for the aforementioned spaces."],"url":"http://arxiv.org/abs/2402.19132v1","category":"math.NA"}
{"created":"2024-02-29 13:06:26","title":"Stacking faults enabled second harmonic generation in centrosymmetric van der Waals RhI3","abstract":"Second harmonic generation (SHG) in van der Waals (vdWs) materials has garnered significant attention due to its potential for integrated nonlinear optical and optoelectronic applications. Stacking faults in vdWs materials, a typical kind of planar defect, can introduce a new degree of freedom to modulate the crystal symmetry and resultant SHG response, however, the physical origin and tunability of stacking-fault-governed SHG in vdWs materials remain unclear. Here, taking the intrinsically centrosymmetric vdWs RhI3 as an example, we theoretically reveal the origin of stacking-fault-governed SHG response, where the SHG response comes from the energetically favorable AC- Cstacking fault of which the electrical transitions along the high symmetry paths Gamma-M and Gamma-K in the Brillion zone play the dominant role at 810 nm. Such stacking-fault-governed SHG response is further confirmed via structural characterizations and SHG measurements. Furthermore, by applying hydrostatic pressure on RhI3, the correlation between structural evolution and SHG response is revealed with SHG enhancement up to 6.9 times, where the decreased electronic transition energies and huger momentum matrix elements due to the stronger interlayer interactions upon compression magnify the SHG susceptibility. This study develops a promising foundation based on strategically designed stacking faults for pioneering new avenues in nonlinear nano-optics.","sentences":["Second harmonic generation (SHG) in van der Waals (vdWs) materials has garnered significant attention due to its potential for integrated nonlinear optical and optoelectronic applications.","Stacking faults in vdWs materials, a typical kind of planar defect, can introduce a new degree of freedom to modulate the crystal symmetry and resultant SHG response, however, the physical origin and tunability of stacking-fault-governed SHG in vdWs materials remain unclear.","Here, taking the intrinsically centrosymmetric vdWs RhI3 as an example, we theoretically reveal the origin of stacking-fault-governed SHG response, where the SHG response comes from the energetically favorable AC- Cstacking fault of which the electrical transitions along the high symmetry paths Gamma-M and Gamma-K in the Brillion zone play the dominant role at 810 nm.","Such stacking-fault-governed SHG response is further confirmed via structural characterizations and SHG measurements.","Furthermore, by applying hydrostatic pressure on RhI3, the correlation between structural evolution and SHG response is revealed with SHG enhancement up to 6.9 times, where the decreased electronic transition energies and huger momentum matrix elements due to the stronger interlayer interactions upon compression magnify the SHG susceptibility.","This study develops a promising foundation based on strategically designed stacking faults for pioneering new avenues in nonlinear nano-optics."],"url":"http://arxiv.org/abs/2402.19129v1","category":"physics.optics"}
{"created":"2024-02-29 13:06:05","title":"Hankel Determinants of convoluted Catalan numbers and nonintersecting lattice paths: A bijective proof of Cigler's Conjecture","abstract":"In recent preprints, Cigler considered certain Hankel determinants of convoluted Catalan numbers and conjectured identities for these determinants. In this note, we shall give a bijective proof of Cigler's Conjecture by interpreting determinants as generating functions of nonintersecting lattice paths: This proof employs the reflection principle, the Lindstr\\\"om-Gessel-Viennot-method and a certain construction involving reflections and overlays of nonintersecting lattice paths.","sentences":["In recent preprints, Cigler considered certain Hankel determinants of convoluted Catalan numbers and conjectured identities for these determinants.","In this note, we shall give a bijective proof of Cigler's Conjecture by interpreting determinants as generating functions of nonintersecting lattice paths: This proof employs the reflection principle, the Lindstr\\\"om-Gessel-Viennot-method and a certain construction involving reflections and overlays of nonintersecting lattice paths."],"url":"http://arxiv.org/abs/2402.19127v1","category":"math.CO"}
{"created":"2024-02-29 13:01:47","title":"Highly efficient Gauss's law-preserving spectral algorithms for Maxwell's double-curl source and eigenvalue problems based on eigen-decomposition","abstract":"In this paper, we present Gauss's law-preserving spectral methods and their efficient solution algorithms for curl-curl source and eigenvalue problems in two and three dimensions arising from Maxwell's equations. Arbitrary order $H(curl)$-conforming spectral basis functions in two and three dimensions are firstly proposed using compact combination of Legendre polynomials. A mixed formulation involving a Lagrange multiplier is then adopted to preserve the Gauss's law in the weak sense. To overcome the bottleneck of computational efficiency caused by the saddle-point nature of the mixed scheme, we present highly efficient solution algorithms based on reordering and decoupling of the resultant linear algebraic system and numerical eigen-decomposition of one dimensional mass matrix. The proposed solution algorithms are direct methods requiring only several matrix-matrix or matrix-tensor products of $N$-by-$N$ matrices, where $N$ is the highest polynomial order in each direction. Compared with other direct methods, the computational complexities are reduced from $O(N^6)$ and $O(N^9)$ to $O(N^3)$ and $O(N^4)$ with small and constant pre-factors for 2D and 3D cases, respectively, and can further be accelerated to $O(N^{2.807})$ and $O(N^{3.807})$, when boosted with the Strassen's matrix multiplication algorithm. Moreover, these algorithms strictly obey the Helmholtz-Hodge decomposition, thus totally eliminate the spurious eigen-modes of non-physical zero eigenvalues. Extensions of the proposed methods and algorithms to problems in complex geometries with variable coefficients and inhomogeneous boundary conditions are discussed to deal with more general situations. Ample numerical examples for solving Maxwell's source and eigenvalue problems are presented to demonstrate the accuracy and efficiency of the proposed methods.","sentences":["In this paper, we present Gauss's law-preserving spectral methods and their efficient solution algorithms for curl-curl source and eigenvalue problems in two and three dimensions arising from Maxwell's equations.","Arbitrary order $H(curl)$-conforming spectral basis functions in two and three dimensions are firstly proposed using compact combination of Legendre polynomials.","A mixed formulation involving a Lagrange multiplier is then adopted to preserve the Gauss's law in the weak sense.","To overcome the bottleneck of computational efficiency caused by the saddle-point nature of the mixed scheme, we present highly efficient solution algorithms based on reordering and decoupling of the resultant linear algebraic system and numerical eigen-decomposition of one dimensional mass matrix.","The proposed solution algorithms are direct methods requiring only several matrix-matrix or matrix-tensor products of $N$-by-$N$ matrices, where $N$ is the highest polynomial order in each direction.","Compared with other direct methods, the computational complexities are reduced from $O(N^6)$ and $O(N^9)$ to $O(N^3)$ and $O(N^4)$ with small and constant pre-factors for 2D and 3D cases, respectively, and can further be accelerated to $O(N^{2.807})$ and $O(N^{3.807})$, when boosted with the Strassen's matrix multiplication algorithm.","Moreover, these algorithms strictly obey the Helmholtz-Hodge decomposition, thus totally eliminate the spurious eigen-modes of non-physical zero eigenvalues.","Extensions of the proposed methods and algorithms to problems in complex geometries with variable coefficients and inhomogeneous boundary conditions are discussed to deal with more general situations.","Ample numerical examples for solving Maxwell's source and eigenvalue problems are presented to demonstrate the accuracy and efficiency of the proposed methods."],"url":"http://arxiv.org/abs/2402.19125v1","category":"math.NA"}
{"created":"2024-02-29 13:01:22","title":"Analysis of Processing Pipelines for Indoor Human Tracking using FMCW radar","abstract":"In this paper, the problem of formulating effective processing pipelines for indoor human tracking is investigated, with the usage of a Multiple Input Multiple Output (MIMO) Frequency Modulated Continuous Wave (FMCW) radar. Specifically, two processing pipelines starting with detections on the Range-Azimuth (RA) maps and the Range-Doppler (RD) maps are formulated and compared, together with subsequent clustering and tracking algorithms and their relevant parameters. Experimental results are presented to validate and assess both pipelines, using a 24 GHz commercial radar platform with 250 MHz bandwidth and 15 virtual channels. Scenarios where 1 and 2 people move in an indoor environment are considered, and the influence of the number of virtual channels and detectors' parameters is discussed. The characteristics and limitations of both pipelines are presented, with the approach based on detections on RA maps showing in general more robust results.","sentences":["In this paper, the problem of formulating effective processing pipelines for indoor human tracking is investigated, with the usage of a Multiple Input Multiple Output (MIMO) Frequency Modulated Continuous Wave (FMCW) radar.","Specifically, two processing pipelines starting with detections on the Range-Azimuth (RA) maps and the Range-Doppler (RD) maps are formulated and compared, together with subsequent clustering and tracking algorithms and their relevant parameters.","Experimental results are presented to validate and assess both pipelines, using a 24 GHz commercial radar platform with 250 MHz bandwidth and 15 virtual channels.","Scenarios where 1 and 2 people move in an indoor environment are considered, and the influence of the number of virtual channels and detectors' parameters is discussed.","The characteristics and limitations of both pipelines are presented, with the approach based on detections on RA maps showing in general more robust results."],"url":"http://arxiv.org/abs/2402.19124v1","category":"eess.SP"}
{"created":"2024-02-29 13:00:22","title":"BigGait: Learning Gait Representation You Want by Large Vision Models","abstract":"Gait recognition stands as one of the most pivotal remote identification technologies and progressively expands across research and industrial communities. However, existing gait recognition methods heavily rely on task-specific upstream driven by supervised learning to provide explicit gait representations, which inevitably introduce expensive annotation costs and potentially cause cumulative errors. Escaping from this trend, this work explores effective gait representations based on the all-purpose knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a simple yet efficient gait framework, termed BigGait. Specifically, the Gait Representation Extractor (GRE) in BigGait effectively transforms all-purpose knowledge into implicit gait features in an unsupervised manner, drawing from design principles of established gait representation construction approaches. Experimental results on CCPG, CAISA-B* and SUSTech1K indicate that BigGait significantly outperforms the previous methods in both self-domain and cross-domain tasks in most cases, and provides a more practical paradigm for learning the next-generation gait representation. Eventually, we delve into prospective challenges and promising directions in LVMs-based gait recognition, aiming to inspire future work in this emerging topic. The source code will be available at https://github.com/ShiqiYu/OpenGait.","sentences":["Gait recognition stands as one of the most pivotal remote identification technologies and progressively expands across research and industrial communities.","However, existing gait recognition methods heavily rely on task-specific upstream driven by supervised learning to provide explicit gait representations, which inevitably introduce expensive annotation costs and potentially cause cumulative errors.","Escaping from this trend, this work explores effective gait representations based on the all-purpose knowledge produced by task-agnostic Large Vision Models (LVMs) and proposes a simple yet efficient gait framework, termed BigGait.","Specifically, the Gait Representation Extractor (GRE) in BigGait effectively transforms all-purpose knowledge into implicit gait features in an unsupervised manner, drawing from design principles of established gait representation construction approaches.","Experimental results on CCPG, CAISA-B* and SUSTech1K indicate that BigGait significantly outperforms the previous methods in both self-domain and cross-domain tasks in most cases, and provides a more practical paradigm for learning the next-generation gait representation.","Eventually, we delve into prospective challenges and promising directions in LVMs-based gait recognition, aiming to inspire future work in this emerging topic.","The source code will be available at https://github.com/ShiqiYu/OpenGait."],"url":"http://arxiv.org/abs/2402.19122v1","category":"cs.CV"}
{"created":"2024-02-29 12:56:18","title":"VIXEN: Visual Text Comparison Network for Image Difference Captioning","abstract":"We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present. Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model. We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework. We augment this dataset with change summaries produced via GPT-3. We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content. Code and data are available at http://github.com/alexblck/vixen","sentences":["We present VIXEN - a technique that succinctly summarizes in text the visual differences between a pair of images in order to highlight any content manipulation present.","Our proposed network linearly maps image features in a pairwise manner, constructing a soft prompt for a pretrained large language model.","We address the challenge of low volume of training data and lack of manipulation variety in existing image difference captioning (IDC) datasets by training on synthetically manipulated images from the recent InstructPix2Pix dataset generated via prompt-to-prompt editing framework.","We augment this dataset with change summaries produced via GPT-3.","We show that VIXEN produces state-of-the-art, comprehensible difference captions for diverse image contents and edit types, offering a potential mitigation against misinformation disseminated via manipulated image content.","Code and data are available at http://github.com/alexblck/vixen"],"url":"http://arxiv.org/abs/2402.19119v1","category":"cs.CV"}
{"created":"2024-02-29 12:49:48","title":"How to Understand \"Support\"? An Implicit-enhanced Causal Inference Approach for Weakly-supervised Phrase Grounding","abstract":"Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training. However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics. To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit. Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively. Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines. Particularly, we observe an interesting finding that IECI outperforms the advanced multimodal LLMs by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the multimodal LLMs in this direction.","sentences":["Weakly-supervised Phrase Grounding (WPG) is an emerging task of inferring the fine-grained phrase-region matching, while merely leveraging the coarse-grained sentence-image pairs for training.","However, existing studies on WPG largely ignore the implicit phrase-region matching relations, which are crucial for evaluating the capability of models in understanding the deep multimodal semantics.","To this end, this paper proposes an Implicit-Enhanced Causal Inference (IECI) approach to address the challenges of modeling the implicit relations and highlighting them beyond the explicit.","Specifically, this approach leverages both the intervention and counterfactual techniques to tackle the above two challenges respectively.","Furthermore, a high-quality implicit-enhanced dataset is annotated to evaluate IECI and detailed evaluations show the great advantages of IECI over the state-of-the-art baselines.","Particularly, we observe an interesting finding that IECI outperforms the advanced multimodal LLMs by a large margin on this implicit-enhanced dataset, which may facilitate more research to evaluate the multimodal LLMs in this direction."],"url":"http://arxiv.org/abs/2402.19116v1","category":"cs.CL"}
{"created":"2024-02-29 12:43:28","title":"Deep Network for Image Compressed Sensing Coding Using Local Structural Sampling","abstract":"Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: 1) The widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency. 2) The optimization-based reconstruction methods generally maintain a much higher computational complexity. In this paper, we propose a new CNN based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding and Laplacian pyramid reconstruction. In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy. Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during training process. After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec. At last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain. Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods, while maintaining fast computational speed.","sentences":["Existing image compressed sensing (CS) coding frameworks usually solve an inverse problem based on measurement coding and optimization-based image reconstruction, which still exist the following two challenges: 1) The widely used random sampling matrix, such as the Gaussian Random Matrix (GRM), usually leads to low measurement coding efficiency.","2)","The optimization-based reconstruction methods generally maintain a much higher computational complexity.","In this paper, we propose a new CNN based image CS coding framework using local structural sampling (dubbed CSCNet) that includes three functional modules: local structural sampling, measurement coding and Laplacian pyramid reconstruction.","In the proposed framework, instead of GRM, a new local structural sampling matrix is first developed, which is able to enhance the correlation between the measurements through a local perceptual sampling strategy.","Besides, the designed local structural sampling matrix can be jointly optimized with the other functional modules during training process.","After sampling, the measurements with high correlations are produced, which are then coded into final bitstreams by the third-party image codec.","At last, a Laplacian pyramid reconstruction network is proposed to efficiently recover the target image from the measurement domain to the image domain.","Extensive experimental results demonstrate that the proposed scheme outperforms the existing state-of-the-art CS coding methods, while maintaining fast computational speed."],"url":"http://arxiv.org/abs/2402.19111v1","category":"eess.IV"}
{"created":"2024-02-29 12:39:04","title":"DeepEraser: Deep Iterative Context Mining for Generic Text Eraser","abstract":"In this work, we present DeepEraser, an effective deep network for generic text removal. DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations. Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure. Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text. Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status. Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image. Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner. To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, and Oxford Synthetic text dataset. The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal. The codes and pre-trained models are available at https://github.com/fh2019ustc/DeepEraser","sentences":["In this work, we present DeepEraser, an effective deep network for generic text removal.","DeepEraser utilizes a recurrent architecture that erases the text in an image via iterative operations.","Our idea comes from the process of erasing pencil script, where the text area designated for removal is subject to continuous monitoring and the text is attenuated progressively, ensuring a thorough and clean erasure.","Technically, at each iteration, an innovative erasing module is deployed, which not only explicitly aggregates the previous erasing progress but also mines additional semantic context to erase the target text.","Through iterative refinements, the text regions are progressively replaced with more appropriate content and finally converge to a relatively accurate status.","Furthermore, a custom mask generation strategy is introduced to improve the capability of DeepEraser for adaptive text removal, as opposed to indiscriminately removing all the text in an image.","Our DeepEraser is notably compact with only 1.4M parameters and trained in an end-to-end manner.","To verify its effectiveness, extensive experiments are conducted on several prevalent benchmarks, including SCUT-Syn, SCUT-EnsText, and Oxford Synthetic text dataset.","The quantitative and qualitative results demonstrate the effectiveness of our DeepEraser over the state-of-the-art methods, as well as its strong generalization ability in custom mask text removal.","The codes and pre-trained models are available at https://github.com/fh2019ustc/DeepEraser"],"url":"http://arxiv.org/abs/2402.19108v1","category":"cs.CV"}
{"created":"2024-02-29 12:38:43","title":"A SOUND APPROACH: Using Large Language Models to generate audio descriptions for egocentric text-audio retrieval","abstract":"Video databases from the internet are a valuable source of text-audio retrieval datasets. However, given that sound and vision streams represent different \"views\" of the data, treating visual descriptions as audio descriptions is far from optimal. Even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval. To exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using Large Language Models (LLMs). In this work, we consider the egocentric video setting and propose three new text-audio retrieval benchmarks based on the EpicMIR and EgoMCQ tasks, and on the EpicSounds dataset. Our approach for obtaining audio-centric descriptions gives significantly higher zero-shot performance than using the original visual-centric descriptions. Furthermore, we show that using the same prompts, we can successfully employ LLMs to improve the retrieval on EpicSounds, compared to using the original audio class labels of the dataset. Finally, we confirm that LLMs can be used to determine the difficulty of identifying the action associated with a sound.","sentences":["Video databases from the internet are a valuable source of text-audio retrieval datasets.","However, given that sound and vision streams represent different \"views\" of the data, treating visual descriptions as audio descriptions is far from optimal.","Even if audio class labels are present, they commonly are not very detailed, making them unsuited for text-audio retrieval.","To exploit relevant audio information from video-text datasets, we introduce a methodology for generating audio-centric descriptions using Large Language Models (LLMs).","In this work, we consider the egocentric video setting and propose three new text-audio retrieval benchmarks based on the EpicMIR and EgoMCQ tasks, and on the EpicSounds dataset.","Our approach for obtaining audio-centric descriptions gives significantly higher zero-shot performance than using the original visual-centric descriptions.","Furthermore, we show that using the same prompts, we can successfully employ LLMs to improve the retrieval on EpicSounds, compared to using the original audio class labels of the dataset.","Finally, we confirm that LLMs can be used to determine the difficulty of identifying the action associated with a sound."],"url":"http://arxiv.org/abs/2402.19106v1","category":"eess.AS"}
{"created":"2024-02-29 12:36:10","title":"CollaFuse: Navigating Limited Resources and Privacy in Collaborative Generative AI","abstract":"In the landscape of generative artificial intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy. Traditional approaches like federated learning distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices). In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning. Tailored for efficient and collaborative use of denoising diffusion probabilistic models, CollaFuse enables shared server training and inference, alleviating client computational burdens. This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server. Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing. These capabilities hold the potential to impact various application areas, such as the design of edge computing solutions, healthcare research, or autonomous driving. In essence, our work advances distributed machine learning, shaping the future of collaborative GenAI networks.","sentences":["In the landscape of generative artificial intelligence, diffusion-based models present challenges for socio-technical systems in data requirements and privacy.","Traditional approaches like federated learning distribute the learning process but strain individual clients, especially with constrained resources (e.g., edge devices).","In response to these challenges, we introduce CollaFuse, a novel framework inspired by split learning.","Tailored for efficient and collaborative use of denoising diffusion probabilistic models, CollaFuse enables shared server training and inference, alleviating client computational burdens.","This is achieved by retaining data and computationally inexpensive GPU processes locally at each client while outsourcing the computationally expensive processes to the shared server.","Demonstrated in a healthcare context, CollaFuse enhances privacy by highly reducing the need for sensitive information sharing.","These capabilities hold the potential to impact various application areas, such as the design of edge computing solutions, healthcare research, or autonomous driving.","In essence, our work advances distributed machine learning, shaping the future of collaborative GenAI networks."],"url":"http://arxiv.org/abs/2402.19105v1","category":"cs.LG"}
{"created":"2024-02-29 12:35:45","title":"Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models","abstract":"Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations. A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions. In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination. Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse premise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating \\textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations. It constrains the false premise attention heads during the model inference process. Impressively, extensive experiments demonstrate that constraining only approximately $1\\%$ of the attention heads in the model yields a notable increase of nearly $20\\%$ of model performance.","sentences":["Large Language Models (LLMs) have shown impressive capabilities but still suffer from the issue of hallucinations.","A significant type of this issue is the false premise hallucination, which we define as the phenomenon when LLMs generate hallucinated text when confronted with false premise questions.","In this paper, we perform a comprehensive analysis of the false premise hallucination and elucidate its internal working mechanism: a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination.","Based on our analysis, we propose \\textbf{FAITH} (\\textbf{F}alse premise \\textbf{A}ttention head constra\\textbf{I}ining for mi\\textbf{T}igating \\textbf{H}allucinations), a novel and effective method to mitigate false premise hallucinations.","It constrains the false premise attention heads during the model inference process.","Impressively, extensive experiments demonstrate that constraining only approximately $1\\%$ of the attention heads in the model yields a notable increase of nearly $20\\%$ of model performance."],"url":"http://arxiv.org/abs/2402.19103v1","category":"cs.CL"}
{"created":"2024-02-29 12:33:14","title":"FlatNAS: optimizing Flatness in Neural Architecture Search for Out-of-Distribution Robustness","abstract":"Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios. This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM). FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their out-of-distribution (OOD) robustness, and constraining the number of parameters in their architecture. Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect in real-world applications of machine and deep learning. FlatNAS achieves a good trade-off between performance, OOD generalization, and the number of parameters, by using only in-distribution data in the NAS exploration. The OOD robustness of the NAS-designed models is evaluated by focusing on robustness to input data corruptions, using popular benchmark datasets in the literature.","sentences":["Neural Architecture Search (NAS) paves the way for the automatic definition of Neural Network (NN) architectures, attracting increasing research attention and offering solutions in various scenarios.","This study introduces a novel NAS solution, called Flat Neural Architecture Search (FlatNAS), which explores the interplay between a novel figure of merit based on robustness to weight perturbations and single NN optimization with Sharpness-Aware Minimization (SAM).","FlatNAS is the first work in the literature to systematically explore flat regions in the loss landscape of NNs in a NAS procedure, while jointly optimizing their performance on in-distribution data, their out-of-distribution (OOD) robustness, and constraining the number of parameters in their architecture.","Differently from current studies primarily concentrating on OOD algorithms, FlatNAS successfully evaluates the impact of NN architectures on OOD robustness, a crucial aspect in real-world applications of machine and deep learning.","FlatNAS achieves a good trade-off between performance, OOD generalization, and the number of parameters, by using only in-distribution data in the NAS exploration.","The OOD robustness of the NAS-designed models is evaluated by focusing on robustness to input data corruptions, using popular benchmark datasets in the literature."],"url":"http://arxiv.org/abs/2402.19102v1","category":"cs.LG"}
{"created":"2024-02-29 12:29:34","title":"Bayesian distances for quantifying tensions in cosmological inference and the surprise statistic","abstract":"Tensions between cosmological parameters derived through different channels can be a genuine signature of new physics that $\\Lambda$CDM as the standard model is not able to reproduce, in particular in the missing consistency between parameter estimates from measurements the early and late Universe. Or, they could be caused by yet to be understood systematics in the measurements as a more mundane explanation. Commonly, cosmological tensions are stated in terms of mismatches of the posterior parameter distributions, often assuming Gaussian statistics. More importantly, though, would be a quantification if two data sets are consistent to each other before combining them into a joint measurement, ideally isolating hints at individual data points that have a strong influence in generating the tension. For this purpose, we start with statistical divergences applied to posterior distributions following from different data sets and develop the theory of a Fisher metric between two data sets, in analogy to the Fisher metric for different parameter choices. As a topical example, we consider the tension in the Hubble-Lema\\^itre constant $H_0$ from supernova and measurements of the cosmic microwave background, derive a ranking of data points in order of their influence on the tension on $H_0$. For this particular example, we compute Bayesian distance measures and show that in the light of CMB data, supernovae are commonly too bright, whereas the low-$\\ell$ CMB spectrum is too high, in agreement with intuition about the parameter sensitivity.","sentences":["Tensions between cosmological parameters derived through different channels can be a genuine signature of new physics that $\\Lambda$CDM as the standard model is not able to reproduce, in particular in the missing consistency between parameter estimates from measurements the early and late Universe.","Or, they could be caused by yet to be understood systematics in the measurements as a more mundane explanation.","Commonly, cosmological tensions are stated in terms of mismatches of the posterior parameter distributions, often assuming Gaussian statistics.","More importantly, though, would be a quantification if two data sets are consistent to each other before combining them into a joint measurement, ideally isolating hints at individual data points that have a strong influence in generating the tension.","For this purpose, we start with statistical divergences applied to posterior distributions following from different data sets and develop the theory of a Fisher metric between two data sets, in analogy to the Fisher metric for different parameter choices.","As a topical example, we consider the tension in the Hubble-Lema\\^itre constant $H_0$ from supernova and measurements of the cosmic microwave background, derive a ranking of data points in order of their influence on the tension on $H_0$. For this particular example, we compute Bayesian distance measures and show that in the light of CMB data, supernovae are commonly too bright, whereas the low-$\\ell$ CMB spectrum is too high, in agreement with intuition about the parameter sensitivity."],"url":"http://arxiv.org/abs/2402.19100v1","category":"astro-ph.CO"}
{"created":"2024-02-29 12:25:45","title":"TEncDM: Understanding the Properties of Diffusion Model in the Space of Language Model Encodings","abstract":"Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data. Despite these efforts, none of them has managed to achieve the quality of the large language models. In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM). Instead of the commonly used token embedding space, we train our model in the space of the language model encodings. Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction. We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage. Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority over existing non-autoregressive models.","sentences":["Drawing inspiration from the success of diffusion models in various domains, numerous research papers proposed methods for adapting them to text data.","Despite these efforts, none of them has managed to achieve the quality of the large language models.","In this paper, we conduct a comprehensive analysis of key components of the text diffusion models and introduce a novel approach named Text Encoding Diffusion Model (TEncDM).","Instead of the commonly used token embedding space, we train our model in the space of the language model encodings.","Additionally, we propose to use a Transformer-based decoder that utilizes contextual information for text reconstruction.","We also analyse self-conditioning and find that it increases the magnitude of the model outputs, allowing the reduction of the number of denoising steps at the inference stage.","Evaluation of TEncDM on two downstream text generation tasks, QQP and XSum, demonstrates its superiority over existing non-autoregressive models."],"url":"http://arxiv.org/abs/2402.19097v1","category":"cs.CL"}
{"created":"2024-02-29 12:25:29","title":"Structural Stability Hypothesis of Dual Unitary Quantum Chaos","abstract":"Having spectral correlations that, over small enough energy scales, are described by random matrix theory is regarded as the most general defining feature of quantum chaotic systems as it applies in the many-body setting and away from any semiclassical limit. Although this property is extremely difficult to prove analytically for generic many-body systems, a rigorous proof has been achieved for dual-unitary circuits -- a special class of local quantum circuits that remain unitary upon swapping space and time. Here we consider the fate of this property when moving from dual-unitary to generic quantum circuits focussing on the \\emph{spectral form factor}, i.e., the Fourier transform of the two-point correlation. We begin with a numerical survey that, in agreement with previous studies, suggests that there exists a finite region in parameter space where dual-unitary physics is stable and spectral correlations are still described by random matrix theory, although up to a maximal quasienergy scale. To explain these findings, we develop a perturbative expansion: it recovers the random matrix theory predictions, provided the terms occurring in perturbation theory obey a relatively simple set of assumptions. We then provide numerical evidence and a heuristic analytical argument supporting these assumptions.","sentences":["Having spectral correlations that, over small enough energy scales, are described by random matrix theory is regarded as the most general defining feature of quantum chaotic systems as it applies in the many-body setting and away from any semiclassical limit.","Although this property is extremely difficult to prove analytically for generic many-body systems, a rigorous proof has been achieved for dual-unitary circuits -- a special class of local quantum circuits that remain unitary upon swapping space and time.","Here we consider the fate of this property when moving from dual-unitary to generic quantum circuits focussing on the \\emph{spectral form factor}, i.e., the Fourier transform of the two-point correlation.","We begin with a numerical survey that, in agreement with previous studies, suggests that there exists a finite region in parameter space where dual-unitary physics is stable and spectral correlations are still described by random matrix theory, although up to a maximal quasienergy scale.","To explain these findings, we develop a perturbative expansion: it recovers the random matrix theory predictions, provided the terms occurring in perturbation theory obey a relatively simple set of assumptions.","We then provide numerical evidence and a heuristic analytical argument supporting these assumptions."],"url":"http://arxiv.org/abs/2402.19096v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-29 12:21:42","title":"Mathematical and computational framework for moving and colliding rigid bodies in a Newtonian fluid","abstract":"We studied numerically the dynamics of colliding rigid bodies in a Newtonian fluid. The finite element method is used to solve the fluid-body interaction and the fluid motion is described in the Arbitrary-Lagrangian-Eulerian framework. To model the interactions between bodies, we consider a repulsive collision-avoidance model, defined by R. Glowinski. The main emphasis in this work is the generalization of this collision model to multiple rigid bodies of arbitrary shape. Our model first uses a narrow-band fast marching method to detect the set of colliding bodies. Then, collision forces and torques are computed for these bodies via a general expression, which does not depend on their shape. Numerical experiments examining the performance of the narrow-band fast marching method and the parallel execution of the collision algorithm are discussed. We validate our model with literature results and show various applications of colliding bodies in two and three dimensions. In these applications, the bodies move due to forces such as gravity, a fluid flow, or their own actuation. Finally, we present a tool to create arbitrarily shaped bodies in discretized fluid domains, enabling conforming body-fluid interface and allowing to perform simulations of fluid-body interactions with collision treatment in these realistic environments. All simulations are conducted with the Feel++ open source library.","sentences":["We studied numerically the dynamics of colliding rigid bodies in a Newtonian fluid.","The finite element method is used to solve the fluid-body interaction and the fluid motion is described in the Arbitrary-Lagrangian-Eulerian framework.","To model the interactions between bodies, we consider a repulsive collision-avoidance model, defined by R. Glowinski.","The main emphasis in this work is the generalization of this collision model to multiple rigid bodies of arbitrary shape.","Our model first uses a narrow-band fast marching method to detect the set of colliding bodies.","Then, collision forces and torques are computed for these bodies via a general expression, which does not depend on their shape.","Numerical experiments examining the performance of the narrow-band fast marching method and the parallel execution of the collision algorithm are discussed.","We validate our model with literature results and show various applications of colliding bodies in two and three dimensions.","In these applications, the bodies move due to forces such as gravity, a fluid flow, or their own actuation.","Finally, we present a tool to create arbitrarily shaped bodies in discretized fluid domains, enabling conforming body-fluid interface and allowing to perform simulations of fluid-body interactions with collision treatment in these realistic environments.","All simulations are conducted with the Feel++ open source library."],"url":"http://arxiv.org/abs/2402.19093v1","category":"math.AP"}
{"created":"2024-02-29 12:20:06","title":"A two spaces extension of Cauchy-Lipschitz Theorem","abstract":"We adapt the classical theory of local well-posedness of evolution problems to cases in which the nonlinearity can be accurately quantified by two different norms. For ordinary differential equations, we consider $\\dot{x} = f(x,x)$ for a function $f: V\\times E \\to E$ where $E$ is a Banach space and $V \\hookrightarrow E$ a normed vector space. This structure allows us to distinguish between the two dependencies of $f$ in $x$ and allows to generalize classical results. We also prove a similar results for partial differential equations.","sentences":["We adapt the classical theory of local well-posedness of evolution problems to cases in which the nonlinearity can be accurately quantified by two different norms.","For ordinary differential equations, we consider $\\dot{x} = f(x,x)$ for a function $f: V\\times E \\to E$ where $E$ is a Banach space and $V \\hookrightarrow E$ a normed vector space.","This structure allows us to distinguish between the two dependencies of $f$ in $x$ and allows to generalize classical results.","We also prove a similar results for partial differential equations."],"url":"http://arxiv.org/abs/2402.19092v1","category":"math.AP"}
{"created":"2024-02-29 12:18:43","title":"Leveraging Representations from Intermediate Encoder-blocks for Synthetic Image Detection","abstract":"The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information. State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from foundation models. However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which are more important for the SID task. On the contrary, shallow layers encode low-level visual information. In this work, we leverage the image representations extracted by intermediate Transformer blocks of CLIP's image-encoder via a lightweight network that maps them to a learnable forgery-aware vector space capable of generalizing exceptionally well. We also employ a trainable module to incorporate the importance of each Transformer block to the final prediction. Our method is compared against the state-of-the-art by evaluating it on 20 test datasets and exhibits an average +10.6% absolute performance improvement. Notably, the best performing models require just a single epoch for training (~8 minutes). Code available at https://github.com/mever-team/rine.","sentences":["The recently developed and publicly available synthetic image generation methods and services make it possible to create extremely realistic imagery on demand, raising great risks for the integrity and safety of online information.","State-of-the-art Synthetic Image Detection (SID) research has led to strong evidence on the advantages of feature extraction from foundation models.","However, such extracted features mostly encapsulate high-level visual semantics instead of fine-grained details, which are more important for the SID task.","On the contrary, shallow layers encode low-level visual information.","In this work, we leverage the image representations extracted by intermediate Transformer blocks of CLIP's image-encoder via a lightweight network that maps them to a learnable forgery-aware vector space capable of generalizing exceptionally well.","We also employ a trainable module to incorporate the importance of each Transformer block to the final prediction.","Our method is compared against the state-of-the-art by evaluating it on 20 test datasets and exhibits an average +10.6% absolute performance improvement.","Notably, the best performing models require just a single epoch for training (~8 minutes).","Code available at https://github.com/mever-team/rine."],"url":"http://arxiv.org/abs/2402.19091v1","category":"cs.CV"}
{"created":"2024-02-29 12:14:29","title":"Around Don's conjecture for binary completely reachable automata","abstract":"A word $w$ is called a reaching word of a subset $S$ of states in a deterministic finite automaton (DFA) if $S$ is the image of $Q$ under the action of $w$. A DFA is called completely reachable if every non-empty subset of the state set has a reaching word. A conjecture states that in every $n$-state completely reachable DFA, for every $k$-element subset of states, there exists a reaching word of length at most $n(n-k)$. We present infinitely many completely reachable DFAs with two letters that violate this conjecture. A subfamily of completely reachable DFAs with two letters, is called standardized DFAs, introduced by Casas and Volkov (2023). We prove that every $k$-element subset of states in an $n$-state standardized DFA has a reaching word of length $\\le n(n-k) + n - 1$. Finally, we confirm the conjecture for standardized DFAs with additional properties, thus generalizing a result of Casas and Volkov (2023).","sentences":["A word $w$ is called a reaching word of a subset $S$ of states in a deterministic finite automaton (DFA) if $S$ is the image of $Q$ under the action of $w$. A DFA is called completely reachable if every non-empty subset of the state set has a reaching word.","A conjecture states that in every $n$-state completely reachable DFA, for every $k$-element subset of states, there exists a reaching word of length at most $n(n-k)$.","We present infinitely many completely reachable DFAs with two letters that violate this conjecture.","A subfamily of completely reachable DFAs with two letters, is called standardized DFAs, introduced by Casas and Volkov (2023).","We prove that every $k$-element subset of states in an $n$-state standardized DFA has a reaching word of length $\\le n(n-k) + n - 1$.","Finally, we confirm the conjecture for standardized DFAs with additional properties, thus generalizing a result of Casas and Volkov (2023)."],"url":"http://arxiv.org/abs/2402.19089v1","category":"cs.FL"}
{"created":"2024-02-29 12:13:50","title":"Survey in Characterization of Semantic Change","abstract":"Live languages continuously evolve to integrate the cultural change of human societies. This evolution manifests through neologisms (new words) or \\textbf{semantic changes} of words (new meaning to existing words). Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods. In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc. Semantic changes can potentially impact the quality of the outcomes of these algorithms. Therefore, it is important to understand and characterize these changes formally. The study of this impact is a recent problem that has attracted the attention of the computational linguistics community. Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to characterize how the meaning of words changes and to reason about how to reduce the impact of semantic change. This survey provides an understandable overview of existing approaches to the \\textit{characterization of semantic changes} and also formally defines three classes of characterizations: if the meaning of a word becomes more general or narrow (change in dimension) if the word is used in a more pejorative or positive/ameliorated sense (change in orientation), and if there is a trend to use the word in a, for instance, metaphoric or metonymic context (change in relation). We summarized the main aspects of the selected publications in a table and discussed the needs and trends in the research activities on semantic change characterization.","sentences":["Live languages continuously evolve to integrate the cultural change of human societies.","This evolution manifests through neologisms (new words) or \\textbf{semantic changes} of words (new meaning to existing words).","Understanding the meaning of words is vital for interpreting texts coming from different cultures (regionalism or slang), domains (e.g., technical terms), or periods.","In computer science, these words are relevant to computational linguistics algorithms such as translation, information retrieval, question answering, etc.","Semantic changes can potentially impact the quality of the outcomes of these algorithms.","Therefore, it is important to understand and characterize these changes formally.","The study of this impact is a recent problem that has attracted the attention of the computational linguistics community.","Several approaches propose methods to detect semantic changes with good precision, but more effort is needed to characterize how the meaning of words changes and to reason about how to reduce the impact of semantic change.","This survey provides an understandable overview of existing approaches to the \\textit{characterization of semantic changes} and also formally defines three classes of characterizations: if the meaning of a word becomes more general or narrow (change in dimension) if the word is used in a more pejorative or positive/ameliorated sense (change in orientation), and if there is a trend to use the word in a, for instance, metaphoric or metonymic context (change in relation).","We summarized the main aspects of the selected publications in a table and discussed the needs and trends in the research activities on semantic change characterization."],"url":"http://arxiv.org/abs/2402.19088v1","category":"cs.CL"}
{"created":"2024-02-29 12:12:38","title":"Cosmic Inflation, Dark Energy and Gravitational Waves","abstract":"We briefly discuss cosmic inflation, which is the dominant paradigm for the generation of the large scale structure in the Universe and also for arranging for the initial conditions of the hot Big Bang. We then present quintessential inflation, which also accounts of the observed dark energy. We discuss how quintessential inflation can be successfully modelled in modified gravity in the Palatini formalism. Finally, we focus on the generation of primordial gravitational waves by inflation and how their spectrum can be enhanced when the early Universe goes through periods of stiff equation of state. This results in gravitational waves with a characteristic spectrum, which may well be observed in the near future, providing insights for the background theory.","sentences":["We briefly discuss cosmic inflation, which is the dominant paradigm for the generation of the large scale structure in the Universe and also for arranging for the initial conditions of the hot Big Bang.","We then present quintessential inflation, which also accounts of the observed dark energy.","We discuss how quintessential inflation can be successfully modelled in modified gravity in the Palatini formalism.","Finally, we focus on the generation of primordial gravitational waves by inflation and how their spectrum can be enhanced when the early Universe goes through periods of stiff equation of state.","This results in gravitational waves with a characteristic spectrum, which may well be observed in the near future, providing insights for the background theory."],"url":"http://arxiv.org/abs/2402.19086v1","category":"hep-ph"}
{"created":"2024-02-29 12:12:30","title":"Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment","abstract":"Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values. In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the \"alignment tax\" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness). However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives. To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences. We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements. Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the \"3H\" (helpfulness, honesty, harmlessness) desiderata. Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving Pareto improvements in multi-objective alignment.","sentences":["Alignment in artificial intelligence pursues the consistency between model responses and human preferences as well as values.","In practice, the multifaceted nature of human preferences inadvertently introduces what is known as the \"alignment tax\" -a compromise where enhancements in alignment within one objective (e.g.,harmlessness) can diminish performance in others (e.g.,helpfulness).","However, existing alignment techniques are mostly unidirectional, leading to suboptimal trade-offs and poor flexibility over various objectives.","To navigate this challenge, we argue the prominence of grounding LLMs with evident preferences.","We introduce controllable preference optimization (CPO), which explicitly specifies preference scores for different objectives, thereby guiding the model to generate responses that meet the requirements.","Our experimental analysis reveals that the aligned models can provide responses that match various preferences among the \"3H\" (helpfulness, honesty, harmlessness) desiderata.","Furthermore, by introducing diverse data and alignment goals, we surpass baseline methods in aligning with single objectives, hence mitigating the impact of the alignment tax and achieving Pareto improvements in multi-objective alignment."],"url":"http://arxiv.org/abs/2402.19085v1","category":"cs.AI"}
{"created":"2024-02-29 12:11:36","title":"High multiplicity of positive solutions in a superlinear problem of Moore-Nehari type","abstract":"In this paper we consider a superlinear one-dimensional elliptic boundary value problem that generalizes the one studied by Moore and Nehari in [43]. Specifically, we deal with piecewise-constant weight functions in front of the nonlinearity with an arbitrary number $\\kappa\\geq 1$ of vanishing regions. We study, from an analytic and numerical point of view, the number of positive solutions, depending on the value of a parameter $\\lambda$ and on $\\kappa$.   Our main results are twofold. On the one hand, we study analytically the behavior of the solutions, as $\\lambda\\downarrow-\\infty$, in the regions where the weight vanishes. Our result leads us to conjecture the existence of $2^{\\kappa+1}-1$ solutions for sufficiently negative $\\lambda$. On the other hand, we support such a conjecture with the results of numerical simulations which also shed light on the structure of the global bifurcation diagrams in $\\lambda$ and the profiles of positive solutions.   Finally, we give additional numerical results suggesting that the same high multiplicity result holds true for a much larger class of weights, also arbitrarily close to situations where there is uniqueness of positive solutions.","sentences":["In this paper we consider a superlinear one-dimensional elliptic boundary value problem that generalizes the one studied by Moore and Nehari in [43].","Specifically, we deal with piecewise-constant weight functions in front of the nonlinearity with an arbitrary number $\\kappa\\geq 1$ of vanishing regions.","We study, from an analytic and numerical point of view, the number of positive solutions, depending on the value of a parameter $\\lambda$ and on $\\kappa$.   Our main results are twofold.","On the one hand, we study analytically the behavior of the solutions, as $\\lambda\\downarrow-\\infty$, in the regions where the weight vanishes.","Our result leads us to conjecture the existence of $2^{\\kappa+1}-1$ solutions for sufficiently negative $\\lambda$. On the other hand, we support such a conjecture with the results of numerical simulations which also shed light on the structure of the global bifurcation diagrams in $\\lambda$ and the profiles of positive solutions.   ","Finally, we give additional numerical results suggesting that the same high multiplicity result holds true for a much larger class of weights, also arbitrarily close to situations where there is uniqueness of positive solutions."],"url":"http://arxiv.org/abs/2402.19084v1","category":"math.AP"}
{"created":"2024-02-29 12:08:57","title":"Finitely generated weakly monotone C*-algebra","abstract":"We consider the $C^*$-algebra generated by finitely many annihilation operators acting on the weakly monotone Fock space, and we call it weakly monotone $C^*$-algebra. We give an abstract representation for this algebra, showing that it is isomorphic to a suitable quotient of a Cuntz-Krieger $C^*$-algebra $\\mathcal{O}_A$ corresponding to a suitable matrix $A$. Furthermore, we show that the diagonal subalgebra of the weakly monotone $C^*$-algebra is a MASA and we give the detailed description of its Gelfand spectrum.","sentences":["We consider the $C^*$-algebra generated by finitely many annihilation operators acting on the weakly monotone Fock space, and we call it weakly monotone $C^*$-algebra.","We give an abstract representation for this algebra, showing that it is isomorphic to a suitable quotient of a Cuntz-Krieger $C^*$-algebra $\\mathcal{O}_A$ corresponding to a suitable matrix $A$.","Furthermore, we show that the diagonal subalgebra of the weakly monotone $C^*$-algebra is a MASA and we give the detailed description of its Gelfand spectrum."],"url":"http://arxiv.org/abs/2402.19081v1","category":"math.OA"}
{"created":"2024-02-29 12:04:36","title":"Experimental investigation of a multi-photon Heisenberg-limited interferometric scheme: the effect of imperfections","abstract":"Interferometric phase estimation is an essential tool for precise measurements of quantities such as displacement, velocity and material properties. The lower bound on measurement uncertainty achievable with classical resources is set by the shot-noise limit (SNL) that scales asymptotically as $1/\\sqrt{N}$, where $N$ is the number of resources used. The experiment of [S. Daryanoosh et al., Nat. Commun. ${\\bf 9}$, 4606 (2018)] showed how to achieve the ultimate precision limit, the exact Heisenberg limit (HL), in ab-initio phase estimation with $N=3$ photon-passes, using an entangled biphoton state in combination with particular measurement techniques. The advantage of the HL over the SNL increases with the number of resources used. Here we present, and implement experimentally, a scheme for generation of the optimal $N=7$ triphoton state. We study experimentally and theoretically the generated state quality and its potential for phase estimation. We show that the expected usefulness of the prepared triphoton state for HL phase estimation is significantly degraded by even quite small experimental imperfections, such as optical mode mismatch and unwanted higher-order multi-photon terms in the states produced in parametric down-conversion.","sentences":["Interferometric phase estimation is an essential tool for precise measurements of quantities such as displacement, velocity and material properties.","The lower bound on measurement uncertainty achievable with classical resources is set by the shot-noise limit (SNL) that scales asymptotically as $1/\\sqrt{N}$, where $N$ is the number of resources used.","The experiment of [S. Daryanoosh et al., Nat.","Commun.","${\\bf 9}$, 4606 (2018)] showed how to achieve the ultimate precision limit, the exact Heisenberg limit (HL), in ab-initio phase estimation with $N=3$ photon-passes, using an entangled biphoton state in combination with particular measurement techniques.","The advantage of the HL over the SNL increases with the number of resources used.","Here we present, and implement experimentally, a scheme for generation of the optimal $N=7$ triphoton state.","We study experimentally and theoretically the generated state quality and its potential for phase estimation.","We show that the expected usefulness of the prepared triphoton state for HL phase estimation is significantly degraded by even quite small experimental imperfections, such as optical mode mismatch and unwanted higher-order multi-photon terms in the states produced in parametric down-conversion."],"url":"http://arxiv.org/abs/2402.19079v1","category":"quant-ph"}
{"created":"2024-02-29 12:03:05","title":"Smooth Tchebycheff Scalarization for Multi-Objective Optimization","abstract":"Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution. In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem. However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem. In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization. It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods. Experimental results on various real-world application problems fully demonstrate the effectiveness of our proposed method.","sentences":["Multi-objective optimization problems can be found in many real-world applications, where the objectives often conflict each other and cannot be optimized by a single solution.","In the past few decades, numerous methods have been proposed to find Pareto solutions that represent different optimal trade-offs among the objectives for a given problem.","However, these existing methods could have high computational complexity or may not have good theoretical properties for solving a general differentiable multi-objective optimization problem.","In this work, by leveraging the smooth optimization technique, we propose a novel and lightweight smooth Tchebycheff scalarization approach for gradient-based multi-objective optimization.","It has good theoretical properties for finding all Pareto solutions with valid trade-off preferences, while enjoying significantly lower computational complexity compared to other methods.","Experimental results on various real-world application problems fully demonstrate the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2402.19078v1","category":"cs.LG"}
{"created":"2024-02-29 12:01:46","title":"Pointing out the Shortcomings of Relation Extraction Models with Semantically Motivated Adversarials","abstract":"In recent years, large language models have achieved state-of-the-art performance across various NLP tasks. However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples. For instance, in the context of relation extraction (RE), we would expect a model to identify the same relation independently of the entities involved in it. For example, consider the sentence \"Leonardo da Vinci painted the Mona Lisa\" expressing the created(Leonardo_da_Vinci, Mona_Lisa) relation. If we substiute \"Leonardo da Vinci\" with \"Barack Obama\", then the sentence still expresses the created relation. A robust model is supposed to detect the same relation in both cases. In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigate how state-of-the-art RE models perform under pressure. Our analyses show that the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent on shortcuts, such as surface forms (or patterns therein) of entities, without making full use of the information present in the sentences.","sentences":["In recent years, large language models have achieved state-of-the-art performance across various NLP tasks.","However, investigations have shown that these models tend to rely on shortcut features, leading to inaccurate predictions and causing the models to be unreliable at generalization to out-of-distribution (OOD) samples.","For instance, in the context of relation extraction (RE), we would expect a model to identify the same relation independently of the entities involved in it.","For example, consider the sentence \"Leonardo da Vinci painted the Mona Lisa\" expressing the created(Leonardo_da_Vinci, Mona_Lisa) relation.","If we substiute \"Leonardo da Vinci\" with \"Barack Obama\", then the sentence still expresses the created relation.","A robust model is supposed to detect the same relation in both cases.","In this work, we describe several semantically-motivated strategies to generate adversarial examples by replacing entity mentions and investigate how state-of-the-art RE models perform under pressure.","Our analyses show that the performance of these models significantly deteriorates on the modified datasets (avg. of -48.5% in F1), which indicates that these models rely to a great extent on shortcuts, such as surface forms (or patterns therein) of entities, without making full use of the information present in the sentences."],"url":"http://arxiv.org/abs/2402.19076v1","category":"cs.CL"}
{"created":"2024-02-29 11:54:35","title":"TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables","abstract":"Recent studies have demonstrated remarkable performance in time series forecasting. However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting. Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables. Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables. We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables. With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are employed. Moreover, a global endogenous variate token is adopted to effectively bridge the exogenous series into endogenous temporal patches. Experimentally, TimeXer significantly improves time series forecasting with exogenous variables and achieves consistent state-of-the-art performance in twelve real-world forecasting benchmarks.","sentences":["Recent studies have demonstrated remarkable performance in time series forecasting.","However, due to the partially-observed nature of real-world applications, solely focusing on the target of interest, so-called endogenous variables, is usually insufficient to guarantee accurate forecasting.","Notably, a system is often recorded into multiple variables, where the exogenous series can provide valuable external information for endogenous variables.","Thus, unlike prior well-established multivariate or univariate forecasting that either treats all the variables equally or overlooks exogenous information, this paper focuses on a practical setting, which is time series forecasting with exogenous variables.","We propose a novel framework, TimeXer, to utilize external information to enhance the forecasting of endogenous variables.","With a deftly designed embedding layer, TimeXer empowers the canonical Transformer architecture with the ability to reconcile endogenous and exogenous information, where patch-wise self-attention and variate-wise cross-attention are employed.","Moreover, a global endogenous variate token is adopted to effectively bridge the exogenous series into endogenous temporal patches.","Experimentally, TimeXer significantly improves time series forecasting with exogenous variables and achieves consistent state-of-the-art performance in twelve real-world forecasting benchmarks."],"url":"http://arxiv.org/abs/2402.19072v1","category":"cs.LG"}
{"created":"2024-02-29 11:52:06","title":"FATE in MMLA: A Student-Centred Exploration of Fairness, Accountability, Transparency, and Ethics in Multimodal Learning Analytics","abstract":"Multimodal Learning Analytics (MMLA) integrates novel sensing technologies and artificial intelligence algorithms, providing opportunities to enhance student reflection during complex, collaborative learning experiences. Although recent advancements in MMLA have shown its capability to generate insights into diverse learning behaviours across various learning settings, little research has been conducted to evaluate these systems in authentic learning contexts, particularly regarding students' perceived fairness, accountability, transparency, and ethics (FATE). Understanding these perceptions is essential to using MMLA effectively without introducing ethical complications or negatively affecting how students learn. This study aimed to address this gap by assessing the FATE of MMLA in an authentic, collaborative learning context. We conducted semi-structured interviews with 14 undergraduate students who used MMLA visualisations for post-activity reflection. The findings highlighted the significance of accurate and comprehensive data representation to ensure visualisation fairness, the need for different levels of data access to foster accountability, the imperative of measuring and cultivating transparency with students, and the necessity of transforming informed consent from dichotomous to continuous and measurable scales. While students value the benefits of MMLA, they also emphasise the importance of ethical considerations, highlighting a pressing need for the LA and MMLA community to investigate and address FATE issues actively.","sentences":["Multimodal Learning Analytics (MMLA) integrates novel sensing technologies and artificial intelligence algorithms, providing opportunities to enhance student reflection during complex, collaborative learning experiences.","Although recent advancements in MMLA have shown its capability to generate insights into diverse learning behaviours across various learning settings, little research has been conducted to evaluate these systems in authentic learning contexts, particularly regarding students' perceived fairness, accountability, transparency, and ethics (FATE).","Understanding these perceptions is essential to using MMLA effectively without introducing ethical complications or negatively affecting how students learn.","This study aimed to address this gap by assessing the FATE of MMLA in an authentic, collaborative learning context.","We conducted semi-structured interviews with 14 undergraduate students who used MMLA visualisations for post-activity reflection.","The findings highlighted the significance of accurate and comprehensive data representation to ensure visualisation fairness, the need for different levels of data access to foster accountability, the imperative of measuring and cultivating transparency with students, and the necessity of transforming informed consent from dichotomous to continuous and measurable scales.","While students value the benefits of MMLA, they also emphasise the importance of ethical considerations, highlighting a pressing need for the LA and MMLA community to investigate and address FATE issues actively."],"url":"http://arxiv.org/abs/2402.19071v1","category":"cs.CY"}
{"created":"2024-02-29 11:45:24","title":"Graph Convolutional Neural Networks for Automated Echocardiography View Recognition: A Holistic Approach","abstract":"To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired. Automatic view recognition involves grouping those images into classes of standard views. Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures. Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation. In this work, we explore learning 3D heart meshes via graph convolutions, using similar techniques to learn 3D meshes in natural images, such as human pose estimation. As the availability of fully annotated 3D images is limited, we generate synthetic US images from 3D meshes by training an adversarial denoising diffusion model. Experiments were conducted on synthetic and clinical cases for view recognition and structure detection. The approach yielded good performance on synthetic images and, despite being exclusively trained on synthetic data, it already showed potential when applied to clinical images. With this proof-of-concept, we aim to demonstrate the benefits of graphs to improve cardiac view recognition that can ultimately lead to better efficiency in cardiac diagnosis.","sentences":["To facilitate diagnosis on cardiac ultrasound (US), clinical practice has established several standard views of the heart, which serve as reference points for diagnostic measurements and define viewports from which images are acquired.","Automatic view recognition involves grouping those images into classes of standard views.","Although deep learning techniques have been successful in achieving this, they still struggle with fully verifying the suitability of an image for specific measurements due to factors like the correct location, pose, and potential occlusions of cardiac structures.","Our approach goes beyond view classification and incorporates a 3D mesh reconstruction of the heart that enables several more downstream tasks, like segmentation and pose estimation.","In this work, we explore learning 3D heart meshes via graph convolutions, using similar techniques to learn 3D meshes in natural images, such as human pose estimation.","As the availability of fully annotated 3D images is limited, we generate synthetic US images from 3D meshes by training an adversarial denoising diffusion model.","Experiments were conducted on synthetic and clinical cases for view recognition and structure detection.","The approach yielded good performance on synthetic images and, despite being exclusively trained on synthetic data, it already showed potential when applied to clinical images.","With this proof-of-concept, we aim to demonstrate the benefits of graphs to improve cardiac view recognition that can ultimately lead to better efficiency in cardiac diagnosis."],"url":"http://arxiv.org/abs/2402.19062v1","category":"eess.IV"}
{"created":"2024-02-29 11:41:12","title":"Optimal ANN-SNN Conversion with Group Neurons","abstract":"Spiking Neural Networks (SNNs) have emerged as a promising third generation of neural networks, offering unique characteristics such as binary outputs, high sparsity, and biological plausibility. However, the lack of effective learning algorithms remains a challenge for SNNs. For instance, while converting artificial neural networks (ANNs) to SNNs circumvents the need for direct training of SNNs, it encounters issues related to conversion errors and high inference time delays. In order to reduce or even eliminate conversion errors while decreasing inference time-steps, we have introduced a novel type of neuron called Group Neurons (GNs). One GN is composed of multiple Integrate-and-Fire (IF) neurons as members, and its neural dynamics are meticulously designed. Based on GNs, we have optimized the traditional ANN-SNN conversion framework. Specifically, we replace the IF neurons in the SNNs obtained by the traditional conversion framework with GNs. The resulting SNNs, which utilize GNs, are capable of achieving accuracy levels comparable to ANNs even within extremely short inference time-steps. The experiments on CIFAR10, CIFAR100, and ImageNet datasets demonstrate the superiority of the proposed methods in terms of both inference accuracy and latency. Code is available at https://github.com/Lyu6PosHao/ANN2SNN_GN.","sentences":["Spiking Neural Networks (SNNs) have emerged as a promising third generation of neural networks, offering unique characteristics such as binary outputs, high sparsity, and biological plausibility.","However, the lack of effective learning algorithms remains a challenge for SNNs.","For instance, while converting artificial neural networks (ANNs) to SNNs circumvents the need for direct training of SNNs, it encounters issues related to conversion errors and high inference time delays.","In order to reduce or even eliminate conversion errors while decreasing inference time-steps, we have introduced a novel type of neuron called Group Neurons (GNs).","One GN is composed of multiple Integrate-and-Fire (IF) neurons as members, and its neural dynamics are meticulously designed.","Based on GNs, we have optimized the traditional ANN-SNN conversion framework.","Specifically, we replace the IF neurons in the SNNs obtained by the traditional conversion framework with GNs.","The resulting SNNs, which utilize GNs, are capable of achieving accuracy levels comparable to ANNs even within extremely short inference time-steps.","The experiments on CIFAR10, CIFAR100, and ImageNet datasets demonstrate the superiority of the proposed methods in terms of both inference accuracy and latency.","Code is available at https://github.com/Lyu6PosHao/ANN2SNN_GN."],"url":"http://arxiv.org/abs/2402.19061v1","category":"cs.NE"}
{"created":"2024-02-29 11:33:46","title":"Locally approximable CR functions, a sharp maximum modulus principle and holomorphic extension","abstract":"We introduce a notion of locally approximable continuous CR functions on locally closed subsets of reduced complex spaces, generalizing both holomorphic functions and CR functions on CR submanifolds. Under additional assumptions of set-theoretical weak pseudoconcavity we prove optimal maximum modulus principles for these functions. Restricting to real submanifolds (possibly with CR singularities) of complexmanifolds, we generalize results on holomorphic extension known for CR submanifolds.","sentences":["We introduce a notion of locally approximable continuous CR functions on locally closed subsets of reduced complex spaces, generalizing both holomorphic functions and CR functions on CR submanifolds.","Under additional assumptions of set-theoretical weak pseudoconcavity we prove optimal maximum modulus principles for these functions.","Restricting to real submanifolds (possibly with CR singularities) of complexmanifolds, we generalize results on holomorphic extension known for CR submanifolds."],"url":"http://arxiv.org/abs/2402.19057v1","category":"math.CV"}
{"created":"2024-02-29 11:31:50","title":"RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection","abstract":"Embedding watermarks into models has been widely used to protect model ownership in federated learning (FL). However, existing methods are inadequate for protecting the ownership of personalized models acquired by clients in personalized FL (PFL). This is due to the aggregation of the global model in PFL, resulting in conflicts over clients' private watermarks. Moreover, malicious clients may tamper with embedded watermarks to facilitate model leakage and evade accountability. This paper presents a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in PFL. We first decouple the watermark embedding of personalized models into two parts: head layer embedding and representation layer embedding. The head layer belongs to clients' private part without participating in model aggregation, while the representation layer is the shared part for aggregation. For representation layer embedding, we employ a watermark slice embedding operation, which avoids watermark embedding conflicts. Furthermore, we design a malicious watermark detection scheme enabling the server to verify the correctness of watermarks before aggregating local models. We conduct an exhaustive experimental evaluation of RobWE. The results demonstrate that RobWE significantly outperforms the state-of-the-art watermark embedding schemes in FL in terms of fidelity, reliability, and robustness.","sentences":["Embedding watermarks into models has been widely used to protect model ownership in federated learning (FL).","However, existing methods are inadequate for protecting the ownership of personalized models acquired by clients in personalized FL (PFL).","This is due to the aggregation of the global model in PFL, resulting in conflicts over clients' private watermarks.","Moreover, malicious clients may tamper with embedded watermarks to facilitate model leakage and evade accountability.","This paper presents a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in PFL.","We first decouple the watermark embedding of personalized models into two parts: head layer embedding and representation layer embedding.","The head layer belongs to clients' private part without participating in model aggregation, while the representation layer is the shared part for aggregation.","For representation layer embedding, we employ a watermark slice embedding operation, which avoids watermark embedding conflicts.","Furthermore, we design a malicious watermark detection scheme enabling the server to verify the correctness of watermarks before aggregating local models.","We conduct an exhaustive experimental evaluation of RobWE.","The results demonstrate that RobWE significantly outperforms the state-of-the-art watermark embedding schemes in FL in terms of fidelity, reliability, and robustness."],"url":"http://arxiv.org/abs/2402.19054v1","category":"cs.CR"}
{"created":"2024-02-29 11:29:47","title":"Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: A Benchmark Study","abstract":"Comprehensive summaries of sessions enable an effective continuity in mental health counseling, facilitating informed therapy planning. Yet, manual summarization presents a significant challenge, diverting experts' attention from the core counseling process. This study evaluates the effectiveness of state-of-the-art Large Language Models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance. We introduce MentalCLOUDS, a counseling-component guided summarization dataset consisting of 191 counseling sessions with summaries focused on three distinct counseling components (aka counseling aspects). Additionally, we assess the capabilities of 11 state-of-the-art LLMs in addressing the task of component-guided summarization in counseling. The generated summaries are evaluated quantitatively using standard summarization metrics and verified qualitatively by mental health professionals. Our findings demonstrate the superior performance of task-specific LLMs such as MentalLlama, Mistral, and MentalBART in terms of standard quantitative metrics such as Rouge-1, Rouge-2, Rouge-L, and BERTScore across all aspects of counseling components. Further, expert evaluation reveals that Mistral supersedes both MentalLlama and MentalBART based on six parameters -- affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness. However, these models share the same weakness by demonstrating a potential for improvement in the opportunity costs and perceived effectiveness metrics.","sentences":["Comprehensive summaries of sessions enable an effective continuity in mental health counseling, facilitating informed therapy planning.","Yet, manual summarization presents a significant challenge, diverting experts' attention from the core counseling process.","This study evaluates the effectiveness of state-of-the-art Large Language Models (LLMs) in selectively summarizing various components of therapy sessions through aspect-based summarization, aiming to benchmark their performance.","We introduce MentalCLOUDS, a counseling-component guided summarization dataset consisting of 191 counseling sessions with summaries focused on three distinct counseling components (aka counseling aspects).","Additionally, we assess the capabilities of 11 state-of-the-art LLMs in addressing the task of component-guided summarization in counseling.","The generated summaries are evaluated quantitatively using standard summarization metrics and verified qualitatively by mental health professionals.","Our findings demonstrate the superior performance of task-specific LLMs such as MentalLlama, Mistral, and MentalBART in terms of standard quantitative metrics such as Rouge-1, Rouge-2, Rouge-L, and BERTScore across all aspects of counseling components.","Further, expert evaluation reveals that Mistral supersedes both MentalLlama and MentalBART based on six parameters -- affective attitude, burden, ethicality, coherence, opportunity costs, and perceived effectiveness.","However, these models share the same weakness by demonstrating a potential for improvement in the opportunity costs and perceived effectiveness metrics."],"url":"http://arxiv.org/abs/2402.19052v1","category":"cs.CL"}
{"created":"2024-02-29 11:28:28","title":"The de Sitter swampland conjectures in the context of Chaplygin-inspired inflation","abstract":"In this work, we discuss the de Sitter swampland conjectures in the context of the generalized Chaplygin-inspired inflationary model. We demonstrate that these conjectures can be satisfied, but only in the region of the parameter space far away from the General Relativity limit. The cosmic microwave background data had already been found to restrict the allowed inflationary potentials of this model. Our results impose a further limitation on the possible potentials.","sentences":["In this work, we discuss the de Sitter swampland conjectures in the context of the generalized Chaplygin-inspired inflationary model.","We demonstrate that these conjectures can be satisfied, but only in the region of the parameter space far away from the General Relativity limit.","The cosmic microwave background data had already been found to restrict the allowed inflationary potentials of this model.","Our results impose a further limitation on the possible potentials."],"url":"http://arxiv.org/abs/2402.19051v1","category":"gr-qc"}
{"created":"2024-02-29 11:11:05","title":"WDM: 3D Wavelet Diffusion Models for High-Resolution Medical Image Synthesis","abstract":"Due to the three-dimensional nature of CT- or MR-scans, generative modeling of medical images is a particularly challenging task. Existing approaches mostly apply patch-wise, slice-wise, or cascaded generation techniques to fit the high-dimensional data into the limited GPU memory. However, these approaches may introduce artifacts and potentially restrict the model's applicability for certain downstream tasks. This work presents WDM, a wavelet-based medical image synthesis framework that applies a diffusion model on wavelet decomposed images. The presented approach is a simple yet effective way of scaling diffusion models to high resolutions and can be trained on a single 40 GB GPU. Experimental results on BraTS and LIDC-IDRI unconditional image generation at a resolution of $128 \\times 128 \\times 128$ show state-of-the-art image fidelity (FID) and sample diversity (MS-SSIM) scores compared to GANs, Diffusion Models, and Latent Diffusion Models. Our proposed method is the only one capable of generating high-quality images at a resolution of $256 \\times 256 \\times 256$.","sentences":["Due to the three-dimensional nature of CT- or MR-scans, generative modeling of medical images is a particularly challenging task.","Existing approaches mostly apply patch-wise, slice-wise, or cascaded generation techniques to fit the high-dimensional data into the limited GPU memory.","However, these approaches may introduce artifacts and potentially restrict the model's applicability for certain downstream tasks.","This work presents WDM, a wavelet-based medical image synthesis framework that applies a diffusion model on wavelet decomposed images.","The presented approach is a simple yet effective way of scaling diffusion models to high resolutions and can be trained on a single 40 GB GPU.","Experimental results on BraTS and LIDC-IDRI unconditional image generation at a resolution of $128 \\times 128 \\times 128$ show state-of-the-art image fidelity (FID) and sample diversity (MS-SSIM) scores compared to GANs, Diffusion Models, and Latent Diffusion Models.","Our proposed method is the only one capable of generating high-quality images at a resolution of $256 \\times 256 \\times 256$."],"url":"http://arxiv.org/abs/2402.19043v1","category":"eess.IV"}
{"created":"2024-02-29 11:06:14","title":"Atmospheric Turbulence Removal with Video Sequence Deep Visual Priors","abstract":"Atmospheric turbulence poses a challenge for the interpretation and visual perception of visual imagery due to its distortion effects. Model-based approaches have been used to address this, but such methods often suffer from artefacts associated with moving content. Conversely, deep learning based methods are dependent on large and diverse datasets that may not effectively represent any specific content. In this paper, we address these problems with a self-supervised learning method that does not require ground truth. The proposed method is not dependent on any dataset outside of the single data sequence being processed but is also able to improve the quality of any input raw sequences or pre-processed sequences. Specifically, our method is based on an accelerated Deep Image Prior (DIP), but integrates temporal information using pixel shuffling and a temporal sliding window. This efficiently learns spatio-temporal priors leading to a system that effectively mitigates atmospheric turbulence distortions. The experiments show that our method improves visual quality results qualitatively and quantitatively.","sentences":["Atmospheric turbulence poses a challenge for the interpretation and visual perception of visual imagery due to its distortion effects.","Model-based approaches have been used to address this, but such methods often suffer from artefacts associated with moving content.","Conversely, deep learning based methods are dependent on large and diverse datasets that may not effectively represent any specific content.","In this paper, we address these problems with a self-supervised learning method that does not require ground truth.","The proposed method is not dependent on any dataset outside of the single data sequence being processed but is also able to improve the quality of any input raw sequences or pre-processed sequences.","Specifically, our method is based on an accelerated Deep Image Prior (DIP), but integrates temporal information using pixel shuffling and a temporal sliding window.","This efficiently learns spatio-temporal priors leading to a system that effectively mitigates atmospheric turbulence distortions.","The experiments show that our method improves visual quality results qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2402.19041v1","category":"cs.CV"}
{"created":"2024-02-29 11:01:25","title":"Empirical Bayes in Bayesian learning: understanding a common practice","abstract":"In applications of Bayesian procedures, even when the prior law is carefully specified, it may be delicate to elicit the prior hyperparameters so that it is often tempting to fix them from the data, usually by their maximum likelihood estimates (MMLE), obtaining a so-called empirical Bayes posterior distribution. Although questionable, this is a common practice; but theoretical properties seem mostly only available on a case-by-case basis. In this paper we provide general properties for parametric models. First, we study the limit behavior of the MMLE and prove results in quite general settings, while also conceptualizing the frequentist context as an unexplored case of maximum likelihood estimation under model misspecification. We cover both identifiable models, illustrating applications to sparse regression, and non-identifiable models - specifically, overfitted mixture models. Finally, we prove higher order merging results. In regular cases, the empirical Bayes posterior is shown to be a fast approximation to the Bayesian posterior distribution of the researcher who, within the given class of priors, has the most information about the true model's parameters. This is a faster approximation than classic Bernstein-von Mises results. Given the class of priors, our work provides formal contents to common beliefs on this popular practice.","sentences":["In applications of Bayesian procedures, even when the prior law is carefully specified, it may be delicate to elicit the prior hyperparameters so that it is often tempting to fix them from the data, usually by their maximum likelihood estimates (MMLE), obtaining a so-called empirical Bayes posterior distribution.","Although questionable, this is a common practice; but theoretical properties seem mostly only available on a case-by-case basis.","In this paper we provide general properties for parametric models.","First, we study the limit behavior of the MMLE and prove results in quite general settings, while also conceptualizing the frequentist context as an unexplored case of maximum likelihood estimation under model misspecification.","We cover both identifiable models, illustrating applications to sparse regression, and non-identifiable models - specifically, overfitted mixture models.","Finally, we prove higher order merging results.","In regular cases, the empirical Bayes posterior is shown to be a fast approximation to the Bayesian posterior distribution of the researcher who, within the given class of priors, has the most information about the true model's parameters.","This is a faster approximation than classic Bernstein-von Mises results.","Given the class of priors, our work provides formal contents to common beliefs on this popular practice."],"url":"http://arxiv.org/abs/2402.19036v1","category":"math.ST"}
{"created":"2024-02-29 11:00:19","title":"Lotka-Volterra Model with Mutations and Generative Adversarial Networks","abstract":"A model of population genetics of the Lotka-Volterra type with mutations on a statistical manifold is introduced. Mutations in the model are described by diffusion on a statistical manifold with a generator in the form of a Laplace-Beltrami operator with a Fisher-Rao metric, that is, the model combines population genetics and information geometry. This model describes a generalization of the model of machine learning theory, the model of generative adversarial network (GAN), to the case of populations of generative adversarial networks. The introduced model describes the control of overfitting for generative adversarial networks.","sentences":["A model of population genetics of the Lotka-Volterra type with mutations on a statistical manifold is introduced.","Mutations in the model are described by diffusion on a statistical manifold with a generator in the form of a Laplace-Beltrami operator with a Fisher-Rao metric, that is, the model combines population genetics and information geometry.","This model describes a generalization of the model of machine learning theory, the model of generative adversarial network (GAN), to the case of populations of generative adversarial networks.","The introduced model describes the control of overfitting for generative adversarial networks."],"url":"http://arxiv.org/abs/2402.19035v1","category":"q-bio.PE"}
{"created":"2024-02-29 10:56:44","title":"Geometry from Integrability: Multi-Leg Fishnet Integrals in Two Dimensions","abstract":"We generalise the geometric analysis of square fishnet integrals in two dimensions to the case of hexagonal fishnets with three-point vertices. Our results support the conjecture that fishnet Feynman integrals in two dimensions, together with their associated geometry, are completely fixed by their Yangian and permutation symmetries. As a new feature for the hexagonal fishnets, the star-triangle identity introduces an ambiguity in the graph representation of a given Feynman integral. This translates into a map between different geometric interpretations attached to a graph. We demonstrate explicitly how these fishnet integrals can be understood as Calabi-Yau varieties, whose Picard-Fuchs ideals are generated by the Yangian over the conformal algebra. In analogy to elliptic curves, which represent the simplest examples of fishnet integrals with four-point vertices, we find that the simplest examples of three-point fishnets correspond to Picard curves with natural generalisations at higher loop orders.","sentences":["We generalise the geometric analysis of square fishnet integrals in two dimensions to the case of hexagonal fishnets with three-point vertices.","Our results support the conjecture that fishnet Feynman integrals in two dimensions, together with their associated geometry, are completely fixed by their Yangian and permutation symmetries.","As a new feature for the hexagonal fishnets, the star-triangle identity introduces an ambiguity in the graph representation of a given Feynman integral.","This translates into a map between different geometric interpretations attached to a graph.","We demonstrate explicitly how these fishnet integrals can be understood as Calabi-Yau varieties, whose Picard-Fuchs ideals are generated by the Yangian over the conformal algebra.","In analogy to elliptic curves, which represent the simplest examples of fishnet integrals with four-point vertices, we find that the simplest examples of three-point fishnets correspond to Picard curves with natural generalisations at higher loop orders."],"url":"http://arxiv.org/abs/2402.19034v1","category":"hep-th"}
{"created":"2024-02-29 10:48:53","title":"High-Speed Motion Planning for Aerial Swarms in Unknown and Cluttered Environments","abstract":"Coordinated flight of multiple drones allows to achieve tasks faster such as search and rescue and infrastructure inspection. Thus, pushing the state-of-the-art of aerial swarms in navigation speed and robustness is of tremendous benefit. In particular, being able to account for unexplored/unknown environments when planning trajectories allows for safer flight. In this work, we propose the first high-speed, decentralized, and synchronous motion planning framework (HDSM) for an aerial swarm that explicitly takes into account the unknown/undiscovered parts of the environment. The proposed approach generates an optimized trajectory for each planning agent that avoids obstacles and other planning agents while moving and exploring the environment. The only global information that each agent has is the target location. The generated trajectory is high-speed, safe from unexplored spaces, and brings the agent closer to its goal. The proposed method outperforms four recent state-of-the-art methods in success rate (100% success in reaching the target location), flight speed (67% faster), and flight time (42% lower). Finally, the method is validated on a set of Crazyflie nano-drones as a proof of concept.","sentences":["Coordinated flight of multiple drones allows to achieve tasks faster such as search and rescue and infrastructure inspection.","Thus, pushing the state-of-the-art of aerial swarms in navigation speed and robustness is of tremendous benefit.","In particular, being able to account for unexplored/unknown environments when planning trajectories allows for safer flight.","In this work, we propose the first high-speed, decentralized, and synchronous motion planning framework (HDSM) for an aerial swarm that explicitly takes into account the unknown/undiscovered parts of the environment.","The proposed approach generates an optimized trajectory for each planning agent that avoids obstacles and other planning agents while moving and exploring the environment.","The only global information that each agent has is the target location.","The generated trajectory is high-speed, safe from unexplored spaces, and brings the agent closer to its goal.","The proposed method outperforms four recent state-of-the-art methods in success rate (100% success in reaching the target location), flight speed (67% faster), and flight time (42% lower).","Finally, the method is validated on a set of Crazyflie nano-drones as a proof of concept."],"url":"http://arxiv.org/abs/2402.19033v1","category":"cs.RO"}
{"created":"2024-02-29 10:40:55","title":"Invariant Checking for SMT-based Systems with Quantifiers","abstract":"This paper addresses the problem of checking invariant properties for a large class of symbolic transition systems, defined by a combination of SMT theories and quantifiers. State variables can be functions from an uninterpreted sort (finite, but unbounded) to an interpreted sort, such as the the integers under the theory of linear arithmetic. This formalism is very expressive and can be used for modeling parameterized systems, array-manipulating programs, and more. We propose two algorithms for finding universal inductive invariants for such systems. The first algorithm combines an IC3-style loop with a form of implicit predicate abstraction to construct an invariant in an incremental manner. The second algorithm constructs an under-approximation of the original problem, and searches for a formula which is an inductive invariant for this case; then, the invariant is generalized to the original case, and checked with a portfolio of techniques. We have implemented the two algorithms and conducted an extensive experimental evaluation, considering various benchmarks and different tools from the literature. As far as we know, our method is the first capable of handling in a large class of systems in a uniform way. The experiment shows that both algorithms are competitive with the state of the art.","sentences":["This paper addresses the problem of checking invariant properties for a large class of symbolic transition systems, defined by a combination of SMT theories and quantifiers.","State variables can be functions from an uninterpreted sort (finite, but unbounded) to an interpreted sort, such as the the integers under the theory of linear arithmetic.","This formalism is very expressive and can be used for modeling parameterized systems, array-manipulating programs, and more.","We propose two algorithms for finding universal inductive invariants for such systems.","The first algorithm combines an IC3-style loop with a form of implicit predicate abstraction to construct an invariant in an incremental manner.","The second algorithm constructs an under-approximation of the original problem, and searches for a formula which is an inductive invariant for this case; then, the invariant is generalized to the original case, and checked with a portfolio of techniques.","We have implemented the two algorithms and conducted an extensive experimental evaluation, considering various benchmarks and different tools from the literature.","As far as we know, our method is the first capable of handling in a large class of systems in a uniform way.","The experiment shows that both algorithms are competitive with the state of the art."],"url":"http://arxiv.org/abs/2402.19028v1","category":"cs.LO"}
{"created":"2024-02-29 10:38:56","title":"How to Train your Antivirus: RL-based Hardening through the Problem-Space","abstract":"ML-based malware detection on dynamic analysis reports is vulnerable to both evasion and spurious correlations. In this work, we investigate a specific ML architecture employed in the pipeline of a widely-known commercial antivirus company, with the goal to harden it against adversarial malware. Adversarial training, the sole defensive technique that can confer empirical robustness, is not applicable out of the box in this domain, for the principal reason that gradient-based perturbations rarely map back to feasible problem-space programs. We introduce a novel Reinforcement Learning approach for constructing adversarial examples, a constituent part of adversarially training a model against evasion. Our approach comes with multiple advantages. It performs modifications that are feasible in the problem-space, and only those; thus it circumvents the inverse mapping problem. It also makes possible to provide theoretical guarantees on the robustness of the model against a particular set of adversarial capabilities. Our empirical exploration validates our theoretical insights, where we can consistently reach 0\\% Attack Success Rate after a few adversarial retraining iterations.","sentences":["ML-based malware detection on dynamic analysis reports is vulnerable to both evasion and spurious correlations.","In this work, we investigate a specific ML architecture employed in the pipeline of a widely-known commercial antivirus company, with the goal to harden it against adversarial malware.","Adversarial training, the sole defensive technique that can confer empirical robustness, is not applicable out of the box in this domain, for the principal reason that gradient-based perturbations rarely map back to feasible problem-space programs.","We introduce a novel Reinforcement Learning approach for constructing adversarial examples, a constituent part of adversarially training a model against evasion.","Our approach comes with multiple advantages.","It performs modifications that are feasible in the problem-space, and only those; thus it circumvents the inverse mapping problem.","It also makes possible to provide theoretical guarantees on the robustness of the model against a particular set of adversarial capabilities.","Our empirical exploration validates our theoretical insights, where we can consistently reach 0\\% Attack Success Rate after a few adversarial retraining iterations."],"url":"http://arxiv.org/abs/2402.19027v1","category":"cs.CR"}
{"created":"2024-02-29 10:37:49","title":"Progressive Contrastive Learning with Multi-Prototype for Unsupervised Visible-Infrared Person Re-identification","abstract":"Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotation, and vice versa. USVI-ReID is a challenging yet under-explored task. Most existing methods address the USVI-ReID problem using cluster-based contrastive learning, which simply employs the cluster center as a representation of a person. However, the cluster center primarily focuses on shared information, overlooking disparity. To address the problem, we propose a Progressive Contrastive Learning with Multi-Prototype (PCLMP) method for USVI-ReID. In brief, we first generate the hard prototype by selecting the sample with the maximum distance from the cluster center. This hard prototype is used in the contrastive loss to emphasize disparity. Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster. This dynamic prototype is used to retain the natural variety of features while reducing instability in the simultaneous learning of both common and disparate information. Finally, we introduce a progressive learning strategy to gradually shift the model's attention towards hard samples, avoiding cluster deterioration. Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method. PCLMP outperforms the existing state-of-the-art method with an average mAP improvement of 3.9%. The source codes will be released.","sentences":["Unsupervised visible-infrared person re-identification (USVI-ReID) aims to match specified people in infrared images to visible images without annotation, and vice versa.","USVI-ReID is a challenging yet under-explored task.","Most existing methods address the USVI-ReID problem using cluster-based contrastive learning, which simply employs the cluster center as a representation of a person.","However, the cluster center primarily focuses on shared information, overlooking disparity.","To address the problem, we propose a Progressive Contrastive Learning with Multi-Prototype (PCLMP) method for USVI-ReID.","In brief, we first generate the hard prototype by selecting the sample with the maximum distance from the cluster center.","This hard prototype is used in the contrastive loss to emphasize disparity.","Additionally, instead of rigidly aligning query images to a specific prototype, we generate the dynamic prototype by randomly picking samples within a cluster.","This dynamic prototype is used to retain the natural variety of features while reducing instability in the simultaneous learning of both common and disparate information.","Finally, we introduce a progressive learning strategy to gradually shift the model's attention towards hard samples, avoiding cluster deterioration.","Extensive experiments conducted on the publicly available SYSU-MM01 and RegDB datasets validate the effectiveness of the proposed method.","PCLMP outperforms the existing state-of-the-art method with an average mAP improvement of 3.9%.","The source codes will be released."],"url":"http://arxiv.org/abs/2402.19026v1","category":"cs.CV"}
{"created":"2024-02-29 10:37:40","title":"Combination of Weak Learners eXplanations to Improve Random Forest eXplicability Robustness","abstract":"The notion of robustness in XAI refers to the observed variations in the explanation of the prediction of a learned model with respect to changes in the input leading to that prediction. Intuitively, if the input being explained is modified slightly subtly enough so as to not change the prediction of the model too much, then we would expect that the explanation provided for that new input does not change much either. We argue that a combination through discriminative averaging of ensembles weak learners explanations can improve the robustness of explanations in ensemble methods.This approach has been implemented and tested with post-hoc SHAP method and Random Forest ensemble with successful results. The improvements obtained have been measured quantitatively and some insights into the explicability robustness in ensemble methods are presented.","sentences":["The notion of robustness in XAI refers to the observed variations in the explanation of the prediction of a learned model with respect to changes in the input leading to that prediction.","Intuitively, if the input being explained is modified slightly subtly enough so as to not change the prediction of the model too much, then we would expect that the explanation provided for that new input does not change much either.","We argue that a combination through discriminative averaging of ensembles weak learners explanations can improve the robustness of explanations in ensemble methods.","This approach has been implemented and tested with post-hoc SHAP method and Random Forest ensemble with successful results.","The improvements obtained have been measured quantitatively and some insights into the explicability robustness in ensemble methods are presented."],"url":"http://arxiv.org/abs/2402.19025v1","category":"cs.LG"}
{"created":"2024-02-29 10:17:55","title":"Fractional material derivative: pointwise representation and a finite volume numerical scheme","abstract":"The fractional material derivative appears as the fractional operator that governs the dynamics of the scaling limits of L\\'evy walks - a stochastic process that originates from the famous continuous-time random walks. It is usually defined as the Fourier-Laplace multiplier, therefore, it can be thought of as a pseudo-differential operator. In this paper, we show that there exists a local representation in time and space, pointwise, of the fractional material derivative. This allows us to define it on a space of locally integrable functions which is larger than the original one in which Fourier and Laplace transform exist as functions.   We consider several typical differential equations involving the fractional material derivative and provide conditions for their solutions to exist. In some cases, the analytical solution can be found. For the general initial value problem, we devise a finite volume method and prove its stability, convergence, and conservation of probability. Numerical illustrations verify our analytical findings. Moreover, our numerical experiments show superiority in the computation time of the proposed numerical scheme over a Monte Carlo method applied to the problem of probability density function's derivation.","sentences":["The fractional material derivative appears as the fractional operator that governs the dynamics of the scaling limits of L\\'evy walks - a stochastic process that originates from the famous continuous-time random walks.","It is usually defined as the Fourier-Laplace multiplier, therefore, it can be thought of as a pseudo-differential operator.","In this paper, we show that there exists a local representation in time and space, pointwise, of the fractional material derivative.","This allows us to define it on a space of locally integrable functions which is larger than the original one in which Fourier and Laplace transform exist as functions.   ","We consider several typical differential equations involving the fractional material derivative and provide conditions for their solutions to exist.","In some cases, the analytical solution can be found.","For the general initial value problem, we devise a finite volume method and prove its stability, convergence, and conservation of probability.","Numerical illustrations verify our analytical findings.","Moreover, our numerical experiments show superiority in the computation time of the proposed numerical scheme over a Monte Carlo method applied to the problem of probability density function's derivation."],"url":"http://arxiv.org/abs/2402.19015v1","category":"math.NA"}
{"created":"2024-02-29 10:17:27","title":"Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models","abstract":"Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks.","sentences":["Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU).","Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements.","Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios.","In this paper, we abbreviate it as the fine-grained feature collapse issue.","With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU.","DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios.","It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs.","We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process.","Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks."],"url":"http://arxiv.org/abs/2402.19014v1","category":"cs.CV"}
{"created":"2024-02-29 10:11:05","title":"Reparametrization invariant action for Gravity with Dynamical Determinant of Metric","abstract":"We present manifestly reparametrization invariant action for theory of gravity with dynamical determinant of metric. We show that it is similar to a reparametrization invariant action for unimodular gravity. We determine canonical form of the action and study structure of constraints.","sentences":["We present manifestly reparametrization invariant action for theory of gravity with dynamical determinant of metric.","We show that it is similar to a reparametrization invariant action for unimodular gravity.","We determine canonical form of the action and study structure of constraints."],"url":"http://arxiv.org/abs/2402.19010v1","category":"gr-qc"}
{"created":"2024-02-29 18:30:32","title":"Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation","abstract":"Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments. The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer? In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model. We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases. We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task. We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera. We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner. These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments. Further information and robot videos can be found on our project website http://extreme-cross-embodiment.github.io.","sentences":["Recent years in robotics and imitation learning have shown remarkable progress in training large-scale foundation models by leveraging data across a multitude of embodiments.","The success of such policies might lead us to wonder: just how diverse can the robots in the training set be while still facilitating positive transfer?","In this work, we study this question in the context of heterogeneous embodiments, examining how even seemingly very different domains, such as robotic navigation and manipulation, can provide benefits when included in the training data for the same model.","We train a single goal-conditioned policy that is capable of controlling robotic arms, quadcopters, quadrupeds, and mobile bases.","We then investigate the extent to which transfer can occur across navigation and manipulation on these embodiments by framing them as a single goal-reaching task.","We find that co-training with navigation data can enhance robustness and performance in goal-conditioned manipulation with a wrist-mounted camera.","We then deploy our policy trained only from navigation-only and static manipulation-only data on a mobile manipulator, showing that it can control a novel embodiment in a zero-shot manner.","These results provide evidence that large-scale robotic policies can benefit from data collected across various embodiments.","Further information and robot videos can be found on our project website http://extreme-cross-embodiment.github.io."],"url":"http://arxiv.org/abs/2402.19432v1","category":"cs.RO"}
{"created":"2024-02-29 17:43:14","title":"A model of pan-immunity maintenance by horizontal gene transfer in the ecological dynamics of bacteria and phages","abstract":"Phages and their bacterial hosts are locked in an evolutionary competition which in small and closed systems typically results in the extinction of one or the other. To resist phages bacteria have evolved numerous defense systems, which nevertheless are still overcome by specific phage counter-defense mechanisms. These defense/counter-defense systems are a major element of microbial genetic diversity and have been demonstrated to propagate between strains by horizontal gene transfer (HGT). It has been proposed that the totality of defense systems found in microbial communities collectively form a distributed \"pan-immune\" system with individual elements moving between strains via ubiquitous HGT. Here, we formulate a Lotka-Volterra type model of a host/phage system interacting via a combinatorial variety of defense/counter-defense systems and show that HGT enables stable maintenance of diverse defense/counter-defense genes in the microbial pan-genome even when individual microbial strains inevitably undergo extinction. This stability requires the HGT rate to be sufficiently high to ensure that some descendant of a \"dying\" strain survives thanks to the immunity acquired through HGT from the community at large, thus establishing a new strain. This mechanism of persistence for the pan-immune gene pool is fundamentally similar to the \"island migration\" model of ecological diversity, with genes moving between genomes instead of species migrating between islands.","sentences":["Phages and their bacterial hosts are locked in an evolutionary competition which in small and closed systems typically results in the extinction of one or the other.","To resist phages bacteria have evolved numerous defense systems, which nevertheless are still overcome by specific phage counter-defense mechanisms.","These defense/counter-defense systems are a major element of microbial genetic diversity and have been demonstrated to propagate between strains by horizontal gene transfer (HGT).","It has been proposed that the totality of defense systems found in microbial communities collectively form a distributed \"pan-immune\" system with individual elements moving between strains via ubiquitous HGT.","Here, we formulate a Lotka-Volterra type model of a host/phage system interacting via a combinatorial variety of defense/counter-defense systems and show that HGT enables stable maintenance of diverse defense/counter-defense genes in the microbial pan-genome even when individual microbial strains inevitably undergo extinction.","This stability requires the HGT rate to be sufficiently high to ensure that some descendant of a \"dying\" strain survives thanks to the immunity acquired through HGT from the community at large, thus establishing a new strain.","This mechanism of persistence for the pan-immune gene pool is fundamentally similar to the \"island migration\" model of ecological diversity, with genes moving between genomes instead of species migrating between islands."],"url":"http://arxiv.org/abs/2402.19388v1","category":"q-bio.PE"}
{"created":"2024-02-29 17:21:59","title":"Unveiling Internet Censorship: Analysing the Impact of Nation States' Content Control Efforts on Internet Architecture and Routing Patterns","abstract":"Heightened interest from nation states to perform content censorship make it evermore critical to identify the impact of censorship efforts on the Internet. We undertake a study of Internet architecture, capturing the state of Internet topology with greater completeness than existing state-of-the-art. We describe our methodology for this, including the tooling we create to collect and process data from a wide range of sources. We analyse this data to find key patterns in nation states with higher censorship, discovering a funnelling effect wherein higher Internet censorship effort is reflected in a constraining effect on a state's Internet routing architecture. However, there are a small number of nation states that do not follow this trend, for which we provide an analysis and explanation, demonstrating a relationship between geographical factors in addition to geopolitics. In summary, our work provides a deeper understanding of how these censorship measures impact the overall functioning and dynamics of the Internet.","sentences":["Heightened interest from nation states to perform content censorship make it evermore critical to identify the impact of censorship efforts on the Internet.","We undertake a study of Internet architecture, capturing the state of Internet topology with greater completeness than existing state-of-the-art.","We describe our methodology for this, including the tooling we create to collect and process data from a wide range of sources.","We analyse this data to find key patterns in nation states with higher censorship, discovering a funnelling effect wherein higher Internet censorship effort is reflected in a constraining effect on a state's Internet routing architecture.","However, there are a small number of nation states that do not follow this trend, for which we provide an analysis and explanation, demonstrating a relationship between geographical factors in addition to geopolitics.","In summary, our work provides a deeper understanding of how these censorship measures impact the overall functioning and dynamics of the Internet."],"url":"http://arxiv.org/abs/2402.19375v1","category":"cs.NI"}
{"created":"2024-02-29 17:16:20","title":"Structure Preserving Diffusion Models","abstract":"Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise reduction without prior knowledge of the image orientation.","sentences":["Diffusion models have become the leading distribution-learning method in recent years.","Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry.","While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric.","Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality.","We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise reduction without prior knowledge of the image orientation."],"url":"http://arxiv.org/abs/2402.19369v1","category":"cs.LG"}
{"created":"2024-02-29 15:49:53","title":"The higher fixed point theorem for foliations. Applications to rigidity and integrality","abstract":"We give applications of the higher Lefschetz theorems for foliations of [BH10], primarily involving Haefliger cohomology. These results show that the transverse structures of foliations carry important topological and geometric information. This is in the spirit of the passage from the Atiyah-Singer index theorem for a single compact manifold to their families index theorem, involving a compact fiber bundle over a compact base. For foliations, Haefliger cohomology plays the role that the cohomology of the base space plays in the families index theorem.   We obtain highly useful numerical invariants by paring with closed holonomy invariant currents. In particular, we prove that the non-triviality of the higher A-hat genus of the foliation in Haefliger cohomology can be an obstruction to the existence of non-trivial leaf-preserving compact connected group actions. We then construct a large collection of examples for which no such actions exist. Finally, we relate our results to Connes' spectral triples, and prove useful integrality results.","sentences":["We give applications of the higher Lefschetz theorems for foliations of [BH10], primarily involving Haefliger cohomology.","These results show that the transverse structures of foliations carry important topological and geometric information.","This is in the spirit of the passage from the Atiyah-Singer index theorem for a single compact manifold to their families index theorem, involving a compact fiber bundle over a compact base.","For foliations, Haefliger cohomology plays the role that the cohomology of the base space plays in the families index theorem.   ","We obtain highly useful numerical invariants by paring with closed holonomy invariant currents.","In particular, we prove that the non-triviality of the higher A-hat genus of the foliation in Haefliger cohomology can be an obstruction to the existence of non-trivial leaf-preserving compact connected group actions.","We then construct a large collection of examples for which no such actions exist.","Finally, we relate our results to Connes' spectral triples, and prove useful integrality results."],"url":"http://arxiv.org/abs/2402.19283v1","category":"math.DG"}
{"created":"2024-02-29 15:27:55","title":"MaskFi: Unsupervised Learning of WiFi and Vision Representations for Multimodal Human Activity Recognition","abstract":"Human activity recognition (HAR) has been playing an increasingly important role in various domains such as healthcare, security monitoring, and metaverse gaming. Though numerous HAR methods based on computer vision have been developed to show prominent performance, they still suffer from poor robustness in adverse visual conditions in particular low illumination, which motivates WiFi-based HAR to serve as a good complementary modality. Existing solutions using WiFi and vision modalities rely on massive labeled data that are very cumbersome to collect. In this paper, we propose a novel unsupervised multimodal HAR solution, MaskFi, that leverages only unlabeled video and WiFi activity data for model training. We propose a new algorithm, masked WiFi-vision modeling (MI2M), that enables the model to learn cross-modal and single-modal features by predicting the masked sections in representation learning. Benefiting from our unsupervised learning procedure, the network requires only a small amount of annotated data for finetuning and can adapt to the new environment with better performance. We conduct extensive experiments on two WiFi-vision datasets collected in-house, and our method achieves human activity recognition and human identification in terms of both robustness and accuracy.","sentences":["Human activity recognition (HAR) has been playing an increasingly important role in various domains such as healthcare, security monitoring, and metaverse gaming.","Though numerous HAR methods based on computer vision have been developed to show prominent performance, they still suffer from poor robustness in adverse visual conditions in particular low illumination, which motivates WiFi-based HAR to serve as a good complementary modality.","Existing solutions using WiFi and vision modalities rely on massive labeled data that are very cumbersome to collect.","In this paper, we propose a novel unsupervised multimodal HAR solution, MaskFi, that leverages only unlabeled video and WiFi activity data for model training.","We propose a new algorithm, masked WiFi-vision modeling (MI2M), that enables the model to learn cross-modal and single-modal features by predicting the masked sections in representation learning.","Benefiting from our unsupervised learning procedure, the network requires only a small amount of annotated data for finetuning and can adapt to the new environment with better performance.","We conduct extensive experiments on two WiFi-vision datasets collected in-house, and our method achieves human activity recognition and human identification in terms of both robustness and accuracy."],"url":"http://arxiv.org/abs/2402.19258v1","category":"cs.CV"}
{"created":"2024-02-29 15:12:08","title":"Review: Spatial inhomogeneities, moir\u00e9 potential and moir\u00e9 excitons","abstract":"In this short review, we provide an overview of recent progress in deploying advanced characterization techniques to understand the effects of local inhomogeneities in moir\\'e heterostructures over multiple length scales. Particular emphasis is placed on correlating the impact of twist angle misalignment, nano-scale disorder, and atomic relaxation on the moir\\'e potential and its collective excitations, particularly moir\\'e excitons. Finally, we discuss future technological applications leveraging based on moi\\'e excitons.","sentences":["In this short review, we provide an overview of recent progress in deploying advanced characterization techniques to understand the effects of local inhomogeneities in moir\\'e heterostructures over multiple length scales.","Particular emphasis is placed on correlating the impact of twist angle misalignment, nano-scale disorder, and atomic relaxation on the moir\\'e potential and its collective excitations, particularly moir\\'e excitons.","Finally, we discuss future technological applications leveraging based on moi\\'e excitons."],"url":"http://arxiv.org/abs/2402.19236v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 15:04:34","title":"CAPTURE-24: A large dataset of wrist-worn activity tracker data collected in the wild for human activity recognition","abstract":"Existing activity tracker datasets for human activity recognition are typically obtained by having participants perform predefined activities in an enclosed environment under supervision. This results in small datasets with a limited number of activities and heterogeneity, lacking the mixed and nuanced movements normally found in free-living scenarios. As such, models trained on laboratory-style datasets may not generalise out of sample. To address this problem, we introduce a new dataset involving wrist-worn accelerometers, wearable cameras, and sleep diaries, enabling data collection for over 24 hours in a free-living setting. The result is CAPTURE-24, a large activity tracker dataset collected in the wild from 151 participants, amounting to 3883 hours of accelerometer data, of which 2562 hours are annotated. CAPTURE-24 is two to three orders of magnitude larger than existing publicly available datasets, which is critical to developing accurate human activity recognition models.","sentences":["Existing activity tracker datasets for human activity recognition are typically obtained by having participants perform predefined activities in an enclosed environment under supervision.","This results in small datasets with a limited number of activities and heterogeneity, lacking the mixed and nuanced movements normally found in free-living scenarios.","As such, models trained on laboratory-style datasets may not generalise out of sample.","To address this problem, we introduce a new dataset involving wrist-worn accelerometers, wearable cameras, and sleep diaries, enabling data collection for over 24 hours in a free-living setting.","The result is CAPTURE-24, a large activity tracker dataset collected in the wild from 151 participants, amounting to 3883 hours of accelerometer data, of which 2562 hours are annotated.","CAPTURE-24 is two to three orders of magnitude larger than existing publicly available datasets, which is critical to developing accurate human activity recognition models."],"url":"http://arxiv.org/abs/2402.19229v1","category":"cs.HC"}
{"created":"2024-02-29 13:50:47","title":"Teaching Large Language Models an Unseen Language on the Fly","abstract":"Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating. We thus investigate whether LLMs can learn a new language on the fly solely through prompting. To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently. We introduce \\textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning. Using a dictionary and only 5K parallel sentences, \\textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation. Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity.","sentences":["Existing large language models struggle to support numerous low-resource languages, particularly the extremely low-resource ones where there is minimal training data available for effective parameter updating.","We thus investigate whether LLMs can learn a new language on the fly solely through prompting.","To study this question, we collect a research suite for Zhuang, a language supported by no LLMs currently.","We introduce \\textsc{DiPMT++}, a framework for adapting LLMs to unseen languages by in-context learning.","Using a dictionary and only 5K parallel sentences, \\textsc{DiPMT++} significantly enhances the performance of GPT-4 from 0 to 16 BLEU for Chinese-to-Zhuang translation and achieves 32 BLEU for Zhuang-to-Chinese translation.","Furthermore, we demonstrate the practical utility of this framework in aiding humans to translate completely unseen languages, which could contribute to the preservation of linguistic diversity."],"url":"http://arxiv.org/abs/2402.19167v1","category":"cs.CL"}
{"created":"2024-02-29 13:45:36","title":"A Bayesian approach to uncover spatio-temporal determinants of heterogeneity in repeated cross-sectional health surveys","abstract":"In several countries, including Italy, a prominent approach to population health surveillance involves conducting repeated cross-sectional surveys at short intervals of time. These surveys gather information on the health status of individual respondents, including details on their behaviors, risk factors, and relevant socio-demographic information. While the collected data undoubtedly provides valuable information, modeling such data presents several challenges. For instance, in health risk models, it is essential to consider behavioral information, spatio-temporal dynamics, and disease co-occurrence. In response to these challenges, our work proposes a multivariate spatio-temporal logistic model for chronic disease diagnoses. Predictors are modeled using individual risk factor covariates and a latent individual propensity to the disease.   Leveraging a state space formulation of the model, we construct a framework in which spatio-temporal heterogeneity in regression parameters is informed by exogenous spatial information, corresponding to different spatial contextual risk factors that may affect health and the occurrence of chronic diseases in different ways. To explore the utility and the effectiveness of our method, we analyze behavioral and risk factor surveillance data collected in Italy (PASSI), which is well-known as a country characterized by high peculiar administrative, social and territorial diversities reflected on high variability in morbidity among population subgroups.","sentences":["In several countries, including Italy, a prominent approach to population health surveillance involves conducting repeated cross-sectional surveys at short intervals of time.","These surveys gather information on the health status of individual respondents, including details on their behaviors, risk factors, and relevant socio-demographic information.","While the collected data undoubtedly provides valuable information, modeling such data presents several challenges.","For instance, in health risk models, it is essential to consider behavioral information, spatio-temporal dynamics, and disease co-occurrence.","In response to these challenges, our work proposes a multivariate spatio-temporal logistic model for chronic disease diagnoses.","Predictors are modeled using individual risk factor covariates and a latent individual propensity to the disease.   ","Leveraging a state space formulation of the model, we construct a framework in which spatio-temporal heterogeneity in regression parameters is informed by exogenous spatial information, corresponding to different spatial contextual risk factors that may affect health and the occurrence of chronic diseases in different ways.","To explore the utility and the effectiveness of our method, we analyze behavioral and risk factor surveillance data collected in Italy (PASSI), which is well-known as a country characterized by high peculiar administrative, social and territorial diversities reflected on high variability in morbidity among population subgroups."],"url":"http://arxiv.org/abs/2402.19162v1","category":"stat.AP"}
{"created":"2024-02-29 13:40:04","title":"Broken detailed balance and entropy production in directed networks","abstract":"The structure of a complex network plays a crucial role in determining its dynamical properties. In this work, we show that the directed, hierarchical organisation of a network causes the system to break detailed balance and dictates the production of entropy through non-equilibrium dynamics. We consider a wide range of dynamical processes and show how different directed network features govern their thermodynamics. Next, we analyse a collection of 97 empirical networks and show that strong directedness and non-equilibrium dynamics are both ubiquitous in real-world systems. Finally, we present a simple method for inferring broken detailed balance and directed network structure from multivariate time-series and apply our method to identify non-equilibrium and hierarchical organisation in both human neuroimaging and financial time-series. Overall, our results shed light on the thermodynamic consequences of directed network structure and indicate the importance and ubiquity of hierarchical organisation and non-equilibrium dynamics in real-world systems.","sentences":["The structure of a complex network plays a crucial role in determining its dynamical properties.","In this work, we show that the directed, hierarchical organisation of a network causes the system to break detailed balance and dictates the production of entropy through non-equilibrium dynamics.","We consider a wide range of dynamical processes and show how different directed network features govern their thermodynamics.","Next, we analyse a collection of 97 empirical networks and show that strong directedness and non-equilibrium dynamics are both ubiquitous in real-world systems.","Finally, we present a simple method for inferring broken detailed balance and directed network structure from multivariate time-series and apply our method to identify non-equilibrium and hierarchical organisation in both human neuroimaging and financial time-series.","Overall, our results shed light on the thermodynamic consequences of directed network structure and indicate the importance and ubiquity of hierarchical organisation and non-equilibrium dynamics in real-world systems."],"url":"http://arxiv.org/abs/2402.19157v1","category":"physics.soc-ph"}
{"created":"2024-02-29 11:47:13","title":"The Influence of Color Stimuli on Adolescents' Emotion Playing Mobile Games","abstract":"Video games elicit emotions which can be influenced by color stimuli as shown by previous studies. However, little research has been conducted on whether this applies to mobile games played by adolescents. Therefore, we examined the influence of color stimuli hue and saturation on mobile game play. Adolescents (n=21) played a mobile platformer game with varying hue and saturation per level for about 25 minutes. We gathered data on emotional states after each level using the Self-Assessment Manikin questionnaire, recorded time spent in each level, and collected participant self-reports on their video game experience. We performed statistical tests, such as ANOVA, which depict no significant influence of hue and/or saturation on the emotional state of our players. We conclude that it is possible that color alone is not an effective measure for eliciting emotion in mobile games, and further research is needed to consider measures such as time spent in the game and screen size, as these are unique to mobile games. There was a noticeable variance in emotional response between male and female players, with a significant interaction of hue and saturation among male players for valence ratings. This may be an indication that color preference influences perceived pleasantness.","sentences":["Video games elicit emotions which can be influenced by color stimuli as shown by previous studies.","However, little research has been conducted on whether this applies to mobile games played by adolescents.","Therefore, we examined the influence of color stimuli hue and saturation on mobile game play.","Adolescents (n=21) played a mobile platformer game with varying hue and saturation per level for about 25 minutes.","We gathered data on emotional states after each level using the Self-Assessment Manikin questionnaire, recorded time spent in each level, and collected participant self-reports on their video game experience.","We performed statistical tests, such as ANOVA, which depict no significant influence of hue and/or saturation on the emotional state of our players.","We conclude that it is possible that color alone is not an effective measure for eliciting emotion in mobile games, and further research is needed to consider measures such as time spent in the game and screen size, as these are unique to mobile games.","There was a noticeable variance in emotional response between male and female players, with a significant interaction of hue and saturation among male players for valence ratings.","This may be an indication that color preference influences perceived pleasantness."],"url":"http://arxiv.org/abs/2402.19064v1","category":"cs.HC"}
{"created":"2024-02-29 11:38:44","title":"VEnvision3D: A Synthetic Perception Dataset for 3D Multi-Task Model Research","abstract":"Developing a unified multi-task foundation model has become a critical challenge in computer vision research. In the current field of 3D computer vision, most datasets solely focus on a relatively limited set of tasks, which complicates the concurrent training requirements of various downstream tasks. This makes the training of multi-objective networks difficult to proceed with, which further hinders the development of foundation models in the 3D vision field. In this paper, we introduce VEnvision3D, a large 3D synthetic perception dataset for multi-task learning, including depth completion, segmentation, upsampling, place recognition, and 3D reconstruction. Since the data for each task was collected in the same scenarios, tasks are inherently aligned in terms of the utilized data. Therefore, such a unique attribute can assist in exploring the potential for the multi-task model and even the foundation model without separate training methods. Several new benchmarks based on the characteristics of the proposed dataset were presented. Extensive studies were performed on end-to-end models, revealing new observations, challenges, and opportunities for future research. In addition, we designed a straightfoward multi-task network to uncover the ability that VEnvision3D can offer for the foundation model. Our dataset and code will be open-sourced upon acceptance.","sentences":["Developing a unified multi-task foundation model has become a critical challenge in computer vision research.","In the current field of 3D computer vision, most datasets solely focus on a relatively limited set of tasks, which complicates the concurrent training requirements of various downstream tasks.","This makes the training of multi-objective networks difficult to proceed with, which further hinders the development of foundation models in the 3D vision field.","In this paper, we introduce VEnvision3D, a large 3D synthetic perception dataset for multi-task learning, including depth completion, segmentation, upsampling, place recognition, and 3D reconstruction.","Since the data for each task was collected in the same scenarios, tasks are inherently aligned in terms of the utilized data.","Therefore, such a unique attribute can assist in exploring the potential for the multi-task model and even the foundation model without separate training methods.","Several new benchmarks based on the characteristics of the proposed dataset were presented.","Extensive studies were performed on end-to-end models, revealing new observations, challenges, and opportunities for future research.","In addition, we designed a straightfoward multi-task network to uncover the ability that VEnvision3D can offer for the foundation model.","Our dataset and code will be open-sourced upon acceptance."],"url":"http://arxiv.org/abs/2402.19059v1","category":"cs.CV"}
{"created":"2024-02-29 10:37:34","title":"Temporal segmentation of motion propagation in response to an external impulse","abstract":"In high-density crowds, local motion can propagate, amplify, and lead to macroscopic phenomena, including 'density waves'. These density waves only occur when individuals interact, and impulses are transferred to neighbours. How this impulse is passed on by the human body and which effects this has on individuals is still not fully understood. To further investigate this, experiments focusing on the propagation of a push were conducted. In the experiments the crowd is greatly simplified by five people lining up in a row. The rearmost person in the row was pushed forward in a controlled manner with a punching bag. The intensity of the push, the initial distance between participants and the initial arm posture were varied. Collected data included side view and top view video recordings, head trajectories, 3D motion using motion capturing (MoCap) suits as well as pressure measured at the punching bag. With a hybrid tracking algorithm, the MoCap data are combined with the head trajectories to allow an analysis of the motion of each limb in relation to other persons. The observed motion of the body in response to the push can be divided into three phases. These are (i) receiving an impulse, (ii) receiving and passing on an impulse, and (iii) passing on an impulse. Using the 3D MoCap data, we can identify the start and end times of each phase. To determine when a push is passed on, the forward motion of the person in front has to be considered. The projection of the center of mass relative to the initial position of the feet is a measure of the extent to which a person is displaced from the rest position. Specifying the timing of these phases is particularly important to distinguish between different types of physical interactions. Our results contribute to the development and validation of a pedestrian model for identifying risks due to motion propagation in dense crowds.","sentences":["In high-density crowds, local motion can propagate, amplify, and lead to macroscopic phenomena, including 'density waves'.","These density waves only occur when individuals interact, and impulses are transferred to neighbours.","How this impulse is passed on by the human body and which effects this has on individuals is still not fully understood.","To further investigate this, experiments focusing on the propagation of a push were conducted.","In the experiments the crowd is greatly simplified by five people lining up in a row.","The rearmost person in the row was pushed forward in a controlled manner with a punching bag.","The intensity of the push, the initial distance between participants and the initial arm posture were varied.","Collected data included side view and top view video recordings, head trajectories, 3D motion using motion capturing (MoCap) suits as well as pressure measured at the punching bag.","With a hybrid tracking algorithm, the MoCap data are combined with the head trajectories to allow an analysis of the motion of each limb in relation to other persons.","The observed motion of the body in response to the push can be divided into three phases.","These are (i) receiving an impulse, (ii) receiving and passing on an impulse, and (iii) passing on an impulse.","Using the 3D MoCap data, we can identify the start and end times of each phase.","To determine when a push is passed on, the forward motion of the person in front has to be considered.","The projection of the center of mass relative to the initial position of the feet is a measure of the extent to which a person is displaced from the rest position.","Specifying the timing of these phases is particularly important to distinguish between different types of physical interactions.","Our results contribute to the development and validation of a pedestrian model for identifying risks due to motion propagation in dense crowds."],"url":"http://arxiv.org/abs/2402.19024v1","category":"physics.soc-ph"}
{"created":"2024-02-29 10:08:57","title":"Generating, Reconstructing, and Representing Discrete and Continuous Data: Generalized Diffusion with Learnable Encoding-Decoding","abstract":"The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images. Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others. We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance. DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding. Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder parameters jointly with diffusion. By choosing appropriate encoder/decoder (e.g., large language models), DiLED naturally applies to different data types. Extensive experiments on text, proteins, and images demonstrate DiLED's flexibility to handle diverse data and tasks and its strong improvement over various existing models.","sentences":["The vast applications of deep generative models are anchored in three core capabilities -- generating new instances, reconstructing inputs, and learning compact representations -- across various data types, such as discrete text/protein sequences and continuous images.","Existing model families, like Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), autoregressive models, and diffusion models, generally excel in specific capabilities and data types but fall short in others.","We introduce generalized diffusion with learnable encoder-decoder (DiLED), that seamlessly integrates the core capabilities for broad applicability and enhanced performance.","DiLED generalizes the Gaussian noising-denoising in standard diffusion by introducing parameterized encoding-decoding.","Crucially, DiLED is compatible with the well-established diffusion model objective and training recipes, allowing effective learning of the encoder-decoder parameters jointly with diffusion.","By choosing appropriate encoder/decoder (e.g., large language models), DiLED naturally applies to different data types.","Extensive experiments on text, proteins, and images demonstrate DiLED's flexibility to handle diverse data and tasks and its strong improvement over various existing models."],"url":"http://arxiv.org/abs/2402.19009v1","category":"cs.LG"}
{"created":"2024-02-29 09:53:19","title":"GoalNet: Goal Areas Oriented Pedestrian Trajectory Prediction","abstract":"Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving. The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem. Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets. Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories. By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the \"goals\" of the pedestrians. In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian. Our network can predict both pedestrian's trajectories and bounding boxes. The overall model is efficient and modular, and its outputs can be changed according to the usage scenario. Experimental results show that GoalNet significantly improves the previous state-of-the-art performance by 48.7% on the JAAD and 40.8% on the PIE dataset.","sentences":["Predicting the future trajectories of pedestrians on the road is an important task for autonomous driving.","The pedestrian trajectory prediction is affected by scene paths, pedestrian's intentions and decision-making, which is a multi-modal problem.","Most recent studies use past trajectories to predict a variety of potential future trajectory distributions, which do not account for the scene context and pedestrian targets.","Instead of predicting the future trajectory directly, we propose to use scene context and observed trajectory to predict the goal points first, and then reuse the goal points to predict the future trajectories.","By leveraging the information from scene context and observed trajectory, the uncertainty can be limited to a few target areas, which represent the \"goals\" of the pedestrians.","In this paper, we propose GoalNet, a new trajectory prediction neural network based on the goal areas of a pedestrian.","Our network can predict both pedestrian's trajectories and bounding boxes.","The overall model is efficient and modular, and its outputs can be changed according to the usage scenario.","Experimental results show that GoalNet significantly improves the previous state-of-the-art performance by 48.7% on the JAAD and 40.8% on the PIE dataset."],"url":"http://arxiv.org/abs/2402.19002v1","category":"cs.CV"}
{"created":"2024-02-29 09:47:12","title":"Metasurface spectrometers beyond resolution-sensitivity constraints","abstract":"Optical spectroscopy plays an essential role across scientific research and industry for non-contact materials analysis1-3, increasingly through in-situ or portable platforms4-6. However, when considering low-light-level applications, conventional spectrometer designs necessitate a compromise between their resolution and sensitivity7,8, especially as device and detector dimensions are scaled down. Here, we report on a miniaturizable spectrometer platform where light throughput onto the detector is instead enhanced as the resolution is increased. This planar, CMOS-compatible platform is based around metasurface encoders designed to exhibit photonic bound states in the continuum9, where operational range can be altered or extended simply through adjusting geometric parameters. This system can enhance photon collection efficiency by up to two orders of magnitude versus conventional designs; we demonstrate this sensitivity advantage through ultra-low-intensity fluorescent and astrophotonic spectroscopy. This work represents a step forward for the practical utility of spectrometers, affording a route to integrated, chip-based devices that maintain high resolution and SNR without requiring prohibitively long integration times.","sentences":["Optical spectroscopy plays an essential role across scientific research and industry for non-contact materials analysis1-3, increasingly through in-situ or portable platforms4-6.","However, when considering low-light-level applications, conventional spectrometer designs necessitate a compromise between their resolution and sensitivity7,8, especially as device and detector dimensions are scaled down.","Here, we report on a miniaturizable spectrometer platform where light throughput onto the detector is instead enhanced as the resolution is increased.","This planar, CMOS-compatible platform is based around metasurface encoders designed to exhibit photonic bound states in the continuum9, where operational range can be altered or extended simply through adjusting geometric parameters.","This system can enhance photon collection efficiency by up to two orders of magnitude versus conventional designs; we demonstrate this sensitivity advantage through ultra-low-intensity fluorescent and astrophotonic spectroscopy.","This work represents a step forward for the practical utility of spectrometers, affording a route to integrated, chip-based devices that maintain high resolution and SNR without requiring prohibitively long integration times."],"url":"http://arxiv.org/abs/2402.18996v1","category":"physics.optics"}
{"created":"2024-02-29 09:46:47","title":"Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series","abstract":"Modeling count-valued time series has been receiving increasing attention since count time series naturally arise in physical and social domains. Poisson gamma dynamical systems (PGDSs) are newly-developed methods, which can well capture the expressive latent transition structure and bursty dynamics behind count sequences. In particular, PGDSs demonstrate superior performance in terms of data imputation and prediction, compared with canonical linear dynamical system (LDS) based methods. Despite these advantages, PGDS cannot capture the heterogeneous overdispersed behaviours of the underlying dynamic processes. To mitigate this defect, we propose a negative-binomial-randomized gamma Markov process, which not only significantly improves the predictive performance of the proposed dynamical system, but also facilitates the fast convergence of the inference algorithm. Moreover, we develop methods to estimate both factor-structured and graph-structured transition dynamics, which enable us to infer more explainable latent structure, compared with PGDSs. Finally, we demonstrate the explainable latent structure learned by the proposed method, and show its superior performance in imputing missing data and forecasting future observations, compared with the related models.","sentences":["Modeling count-valued time series has been receiving increasing attention since count time series naturally arise in physical and social domains.","Poisson gamma dynamical systems (PGDSs) are newly-developed methods, which can well capture the expressive latent transition structure and bursty dynamics behind count sequences.","In particular, PGDSs demonstrate superior performance in terms of data imputation and prediction, compared with canonical linear dynamical system (LDS) based methods.","Despite these advantages, PGDS cannot capture the heterogeneous overdispersed behaviours of the underlying dynamic processes.","To mitigate this defect, we propose a negative-binomial-randomized gamma Markov process, which not only significantly improves the predictive performance of the proposed dynamical system, but also facilitates the fast convergence of the inference algorithm.","Moreover, we develop methods to estimate both factor-structured and graph-structured transition dynamics, which enable us to infer more explainable latent structure, compared with PGDSs.","Finally, we demonstrate the explainable latent structure learned by the proposed method, and show its superior performance in imputing missing data and forecasting future observations, compared with the related models."],"url":"http://arxiv.org/abs/2402.18995v1","category":"cs.LG"}
{"created":"2024-02-29 09:46:44","title":"Spyx: A Library for Just-In-Time Compiled Optimization of Spiking Neural Networks","abstract":"As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus. Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models. Despite their effectiveness, these powerful networks often incur high execution costs in production environments. Neuromorphic computing, inspired by biological neural processes, offers a promising alternative. By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint. However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators. To facilitate the investigation of SNN architectures and dynamics researchers have sought to bridge Python-based deep learning frameworks such as PyTorch or TensorFlow with custom-implemented compute kernels. This paper introduces Spyx, a new and lightweight SNN simulation and optimization library designed in JAX. By pre-staging data in the expansive vRAM of contemporary accelerators and employing extensive JIT compilation, Spyx allows for SNN optimization to be executed as a unified, low-level program on NVIDIA GPUs or Google TPUs. This approach achieves optimal hardware utilization, surpassing the performance of many existing SNN training frameworks while maintaining considerable flexibility.","sentences":["As the role of artificial intelligence becomes increasingly pivotal in modern society, the efficient training and deployment of deep neural networks have emerged as critical areas of focus.","Recent advancements in attention-based large neural architectures have spurred the development of AI accelerators, facilitating the training of extensive, multi-billion parameter models.","Despite their effectiveness, these powerful networks often incur high execution costs in production environments.","Neuromorphic computing, inspired by biological neural processes, offers a promising alternative.","By utilizing temporally-sparse computations, Spiking Neural Networks (SNNs) offer to enhance energy efficiency through a reduced and low-power hardware footprint.","However, the training of SNNs can be challenging due to their recurrent nature which cannot as easily leverage the massive parallelism of modern AI accelerators.","To facilitate the investigation of SNN architectures and dynamics researchers have sought to bridge Python-based deep learning frameworks such as PyTorch or TensorFlow with custom-implemented compute kernels.","This paper introduces Spyx, a new and lightweight SNN simulation and optimization library designed in JAX.","By pre-staging data in the expansive vRAM of contemporary accelerators and employing extensive JIT compilation, Spyx allows for SNN optimization to be executed as a unified, low-level program on NVIDIA GPUs or Google TPUs.","This approach achieves optimal hardware utilization, surpassing the performance of many existing SNN training frameworks while maintaining considerable flexibility."],"url":"http://arxiv.org/abs/2402.18994v1","category":"cs.NE"}
{"created":"2024-02-29 09:29:29","title":"Domain growth kinetics of active model B with thermal fluctuations","abstract":"We perform a comprehensive study on the role of thermal noise on the ordering kinetics of a collection of active Brownian particles modeled using coarse-grained conserved active model B (AMB). The ordering kinetics of the system is studied for the critical mixture when quenched from high to a low temperature. The structure of the growing domains changes from isolated droplet type for AMB without noise to bi-continuous type for active model B with noise (AMBN). Unlike the passive counterpart of the AMB, the noise is relevant for the growth kinetics of the AMB. We use extensive numerical study, as well as dynamic scaling hypothesis to characterize the kinetics of the system. We find that the asymptotic growth law for AMBN is diffusive Lifshitz-Slyozov (LS) type, whereas it was reported previously that the asymptotic growth law for the AMB without noise is slower, with a growth exponent 4. Moreover, the kinetics of the growing domains show a strong time dependent growth for AMBN. The growth law shows a crossover from early time 1/3 value to intermediate time 1/4 value, and it again traverses from 1/4 to 1/3 asymptotically. The two different scaling functions are found for intermediate time and late time with growth law 1/4 and 1/3 respectively.","sentences":["We perform a comprehensive study on the role of thermal noise on the ordering kinetics of a collection of active Brownian particles modeled using coarse-grained conserved active model B (AMB).","The ordering kinetics of the system is studied for the critical mixture when quenched from high to a low temperature.","The structure of the growing domains changes from isolated droplet type for AMB without noise to bi-continuous type for active model B with noise (AMBN).","Unlike the passive counterpart of the AMB, the noise is relevant for the growth kinetics of the AMB.","We use extensive numerical study, as well as dynamic scaling hypothesis to characterize the kinetics of the system.","We find that the asymptotic growth law for AMBN is diffusive Lifshitz-Slyozov (LS) type, whereas it was reported previously that the asymptotic growth law for the AMB without noise is slower, with a growth exponent 4.","Moreover, the kinetics of the growing domains show a strong time dependent growth for AMBN.","The growth law shows a crossover from early time 1/3 value to intermediate time 1/4 value, and it again traverses from 1/4 to 1/3 asymptotically.","The two different scaling functions are found for intermediate time and late time with growth law 1/4 and 1/3 respectively."],"url":"http://arxiv.org/abs/2402.18977v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-29 09:27:40","title":"Theoretically Achieving Continuous Representation of Oriented Bounding Boxes","abstract":"Considerable efforts have been devoted to Oriented Object Detection (OOD). However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods. This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction. Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature. Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin. It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in literature for rectangle-based object representation. For fairness and transparency of experiments, we have developed a modularized benchmark based on the open-source deep learning framework Jittor's detection toolbox JDet for OOD evaluation. On the popular DOTA dataset, by integrating Faster-RCNN as the same baseline model, our new method outperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement 1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks.","sentences":["Considerable efforts have been devoted to Oriented Object Detection (OOD).","However, one lasting issue regarding the discontinuity in Oriented Bounding Box (OBB) representation remains unresolved, which is an inherent bottleneck for extant OOD methods.","This paper endeavors to completely solve this issue in a theoretically guaranteed manner and puts an end to the ad-hoc efforts in this direction.","Prior studies typically can only address one of the two cases of discontinuity: rotation and aspect ratio, and often inadvertently introduce decoding discontinuity, e.g. Decoding Incompleteness (DI) and Decoding Ambiguity (DA) as discussed in literature.","Specifically, we propose a novel representation method called Continuous OBB (COBB), which can be readily integrated into existing detectors e.g. Faster-RCNN as a plugin.","It can theoretically ensure continuity in bounding box regression which to our best knowledge, has not been achieved in literature for rectangle-based object representation.","For fairness and transparency of experiments, we have developed a modularized benchmark based on the open-source deep learning framework Jittor's detection toolbox JDet for OOD evaluation.","On the popular DOTA dataset, by integrating Faster-RCNN as the same baseline model, our new method outperforms the peer method Gliding Vertex by 1.13% mAP50 (relative improvement 1.54%), and 2.46% mAP75 (relative improvement 5.91%), without any tricks."],"url":"http://arxiv.org/abs/2402.18975v1","category":"cs.CV"}
{"created":"2024-02-29 09:19:06","title":"PrivatEyes: Appearance-based Gaze Estimation Using Federated Secure Multi-Party Computation","abstract":"Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks. We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC). PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators' updates. PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious. We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches. Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts.","sentences":["Latest gaze estimation methods require large-scale training data but their collection and exchange pose significant privacy risks.","We propose PrivatEyes - the first privacy-enhancing training approach for appearance-based gaze estimation based on federated learning (FL) and secure multi-party computation (MPC).","PrivatEyes enables training gaze estimators on multiple local datasets across different users and server-based secure aggregation of the individual estimators' updates.","PrivatEyes guarantees that individual gaze data remains private even if a majority of the aggregating servers is malicious.","We also introduce a new data leakage attack DualView that shows that PrivatEyes limits the leakage of private training data more effectively than previous approaches.","Evaluations on the MPIIGaze, MPIIFaceGaze, GazeCapture, and NVGaze datasets further show that the improved privacy does not lead to a lower gaze estimation accuracy or substantially higher computational costs - both of which are on par with its non-secure counterparts."],"url":"http://arxiv.org/abs/2402.18970v1","category":"cs.CV"}
{"created":"2024-02-29 08:59:51","title":"Towards Out-of-Distribution Detection for breast cancer classification in Point-of-Care Ultrasound Imaging","abstract":"Deep learning has shown to have great potential in medical applications. In critical domains as such, it is of high interest to have trustworthy algorithms which are able to tell when reliable assessments cannot be guaranteed. Detecting out-of-distribution (OOD) samples is a crucial step towards building a safe classifier. Following a previous study, showing that it is possible to classify breast cancer in point-of-care ultrasound images, this study investigates OOD detection using three different methods: softmax, energy score and deep ensembles. All methods are tested on three different OOD data sets. The results show that the energy score method outperforms the softmax method, performing well on two of the data sets. The ensemble method is the most robust, performing the best at detecting OOD samples for all three OOD data sets.","sentences":["Deep learning has shown to have great potential in medical applications.","In critical domains as such, it is of high interest to have trustworthy algorithms which are able to tell when reliable assessments cannot be guaranteed.","Detecting out-of-distribution (OOD) samples is a crucial step towards building a safe classifier.","Following a previous study, showing that it is possible to classify breast cancer in point-of-care ultrasound images, this study investigates OOD detection using three different methods: softmax, energy score and deep ensembles.","All methods are tested on three different OOD data sets.","The results show that the energy score method outperforms the softmax method, performing well on two of the data sets.","The ensemble method is the most robust, performing the best at detecting OOD samples for all three OOD data sets."],"url":"http://arxiv.org/abs/2402.18960v1","category":"cs.CV"}
{"created":"2024-02-29 08:20:49","title":"Syntactic Ghost: An Imperceptible General-purpose Backdoor Attacks on Pre-trained Language Models","abstract":"Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks. However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality. In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \\textbf{Syntactic Ghost} (synGhost for short). Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge. The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors. Additionally, in light of the unique properties of syntactic triggers, we introduce an auxiliary module to drive the PLMs to learn this knowledge in priority, which can alleviate the interference between different syntactic structures. Experiments show that our method outperforms the previous methods and achieves the predefined objectives. Not only do severe threats to various natural language understanding (NLU) tasks on two tuning paradigms but also to multiple PLMs. Meanwhile, the synGhost is imperceptible against three countermeasures based on perplexity, fine-pruning, and the proposed maxEntropy.","sentences":["Pre-trained language models (PLMs) have been found susceptible to backdoor attacks, which can transfer vulnerabilities to various downstream tasks.","However, existing PLM backdoors are conducted with explicit triggers under the manually aligned, thus failing to satisfy expectation goals simultaneously in terms of effectiveness, stealthiness, and universality.","In this paper, we propose a novel approach to achieve invisible and general backdoor implantation, called \\textbf{Syntactic Ghost} (synGhost for short).","Specifically, the method hostilely manipulates poisoned samples with different predefined syntactic structures as stealth triggers and then implants the backdoor to pre-trained representation space without disturbing the primitive knowledge.","The output representations of poisoned samples are distributed as uniformly as possible in the feature space via contrastive learning, forming a wide range of backdoors.","Additionally, in light of the unique properties of syntactic triggers, we introduce an auxiliary module to drive the PLMs to learn this knowledge in priority, which can alleviate the interference between different syntactic structures.","Experiments show that our method outperforms the previous methods and achieves the predefined objectives.","Not only do severe threats to various natural language understanding (NLU) tasks on two tuning paradigms but also to multiple PLMs.","Meanwhile, the synGhost is imperceptible against three countermeasures based on perplexity, fine-pruning, and the proposed maxEntropy."],"url":"http://arxiv.org/abs/2402.18945v1","category":"cs.CR"}
{"created":"2024-02-29 08:20:06","title":"SemEval 2024 -- Task 10: Emotion Discovery and Reasoning its Flip in Conversation (EDiReF)","abstract":"We present SemEval-2024 Task 10, a shared task centred on identifying emotions and finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues. This task comprises three distinct subtasks - emotion recognition in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip reasoning for English dialogues. Participating systems were tasked to automatically execute one or more of these subtasks. The datasets for these tasks comprise manually annotated conversations focusing on emotions and triggers for emotion shifts (The task data is available at https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git). A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks. This paper summarises the results and findings from 24 teams alongside their system descriptions.","sentences":["We present SemEval-2024 Task 10, a shared task centred on identifying emotions and finding the rationale behind their flips within monolingual English and Hindi-English code-mixed dialogues.","This task comprises three distinct subtasks - emotion recognition in conversation for code-mixed dialogues, emotion flip reasoning for code-mixed dialogues, and emotion flip reasoning for English dialogues.","Participating systems were tasked to automatically execute one or more of these subtasks.","The datasets for these tasks comprise manually annotated conversations focusing on emotions and triggers for emotion shifts (The task data is available at https://github.com/LCS2-IIITD/EDiReF-SemEval2024.git).","A total of 84 participants engaged in this task, with the most adept systems attaining F1-scores of 0.70, 0.79, and 0.76 for the respective subtasks.","This paper summarises the results and findings from 24 teams alongside their system descriptions."],"url":"http://arxiv.org/abs/2402.18944v1","category":"cs.CL"}
{"created":"2024-02-29 08:05:23","title":"Energy-Efficient UAV Swarm Assisted MEC with Dynamic Clustering and Scheduling","abstract":"In this paper, the energy-efficient unmanned aerial vehicle (UAV) swarm assisted mobile edge computing (MEC) with dynamic clustering and scheduling is studied. In the considered system model, UAVs are divided into multiple swarms, with each swarm consisting of a leader UAV and several follower UAVs to provide computing services to end-users. Unlike existing work, we allow UAVs to dynamically cluster into different swarms, i.e., each follower UAV can change its leader based on the time-varying spatial positions, updated application placement, etc. in a dynamic manner. Meanwhile, UAVs are required to dynamically schedule their energy replenishment, application placement, trajectory planning and task delegation. With the aim of maximizing the long-term energy efficiency of the UAV swarm assisted MEC system, a joint optimization problem of dynamic clustering and scheduling is formulated. Taking into account the underlying cooperation and competition among intelligent UAVs, we further reformulate this optimization problem as a combination of a series of strongly coupled multi-agent stochastic games, and then propose a novel reinforcement learning-based UAV swarm dynamic coordination (RLDC) algorithm for obtaining the equilibrium. Simulations are conducted to evaluate the performance of the RLDC algorithm and demonstrate its superiority over counterparts.","sentences":["In this paper, the energy-efficient unmanned aerial vehicle (UAV) swarm assisted mobile edge computing (MEC) with dynamic clustering and scheduling is studied.","In the considered system model, UAVs are divided into multiple swarms, with each swarm consisting of a leader UAV and several follower UAVs to provide computing services to end-users.","Unlike existing work, we allow UAVs to dynamically cluster into different swarms, i.e., each follower UAV can change its leader based on the time-varying spatial positions, updated application placement, etc. in a dynamic manner.","Meanwhile, UAVs are required to dynamically schedule their energy replenishment, application placement, trajectory planning and task delegation.","With the aim of maximizing the long-term energy efficiency of the UAV swarm assisted MEC system, a joint optimization problem of dynamic clustering and scheduling is formulated.","Taking into account the underlying cooperation and competition among intelligent UAVs, we further reformulate this optimization problem as a combination of a series of strongly coupled multi-agent stochastic games, and then propose a novel reinforcement learning-based UAV swarm dynamic coordination (RLDC) algorithm for obtaining the equilibrium.","Simulations are conducted to evaluate the performance of the RLDC algorithm and demonstrate its superiority over counterparts."],"url":"http://arxiv.org/abs/2402.18936v1","category":"cs.NI"}
{"created":"2024-02-29 07:49:10","title":"Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data","abstract":"Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems. This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision. The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning. Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth). With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages.","sentences":["Collecting high-quality studio recordings of audio is challenging, which limits the language coverage of text-to-speech (TTS) systems.","This paper proposes a framework for scaling a multilingual TTS model to 100+ languages using found data without supervision.","The proposed framework combines speech-text encoder pretraining with unsupervised training using untranscribed speech and unspoken text data sources, thereby leveraging massively multilingual joint speech and text representation learning.","Without any transcribed speech in a new language, this TTS model can generate intelligible speech in >30 unseen languages (CER difference of <10% to ground truth).","With just 15 minutes of transcribed, found data, we can reduce the intelligibility difference to 1% or less from the ground-truth, and achieve naturalness scores that match the ground-truth in several languages."],"url":"http://arxiv.org/abs/2402.18932v1","category":"eess.AS"}
{"created":"2024-02-29 07:44:31","title":"Navigating Beyond Dropout: An Intriguing Solution Towards Generalizable Image Super Resolution","abstract":"Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years. %Despite the substantial advancement% While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation. Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout. Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model's capacity to faithfully reconstruct fine details. We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-order features statistics. Experimental results have shown that our method could serve as a model-agnostic regularization and outperforms Dropout on seven benchmark datasets including both synthetic and real-world scenarios.","sentences":["Deep learning has led to a dramatic leap on Single Image Super-Resolution (SISR) performances in recent years.","%Despite the substantial advancement%","While most existing work assumes a simple and fixed degradation model (e.g., bicubic downsampling), the research of Blind SR seeks to improve model generalization ability with unknown degradation.","Recently, Kong et al pioneer the investigation of a more suitable training strategy for Blind SR using Dropout.","Although such method indeed brings substantial generalization improvements via mitigating overfitting, we argue that Dropout simultaneously introduces undesirable side-effect that compromises model's capacity to faithfully reconstruct fine details.","We show both the theoretical and experimental analyses in our paper, and furthermore, we present another easy yet effective training strategy that enhances the generalization ability of the model by simply modulating its first and second-order features statistics.","Experimental results have shown that our method could serve as a model-agnostic regularization and outperforms Dropout on seven benchmark datasets including both synthetic and real-world scenarios."],"url":"http://arxiv.org/abs/2402.18929v1","category":"cs.CV"}
{"created":"2024-02-29 07:42:03","title":"Edge Computing Enabled Real-Time Video Analysis via Adaptive Spatial-Temporal Semantic Filtering","abstract":"This paper proposes a novel edge computing enabled real-time video analysis system for intelligent visual devices. The proposed system consists of a tracking-assisted object detection module (TAODM) and a region of interesting module (ROIM). TAODM adaptively determines the offloading decision to process each video frame locally with a tracking algorithm or to offload it to the edge server inferred by an object detection model. ROIM determines each offloading frame's resolution and detection model configuration to ensure that the analysis results can return in time. TAODM and ROIM interact jointly to filter the repetitive spatial-temporal semantic information to maximize the processing rate while ensuring high video analysis accuracy. Unlike most existing works, this paper investigates the real-time video analysis systems where the intelligent visual device connects to the edge server through a wireless network with fluctuating network conditions. We decompose the real-time video analysis problem into the offloading decision and configurations selection sub-problems. To solve these two sub-problems, we introduce a double deep Q network (DDQN) based offloading approach and a contextual multi-armed bandit (CMAB) based adaptive configurations selection approach, respectively. A DDQN-CMAB reinforcement learning (DCRL) training framework is further developed to integrate these two approaches to improve the overall video analyzing performance. Extensive simulations are conducted to evaluate the performance of the proposed solution, and demonstrate its superiority over counterparts.","sentences":["This paper proposes a novel edge computing enabled real-time video analysis system for intelligent visual devices.","The proposed system consists of a tracking-assisted object detection module (TAODM) and a region of interesting module (ROIM).","TAODM adaptively determines the offloading decision to process each video frame locally with a tracking algorithm or to offload it to the edge server inferred by an object detection model.","ROIM determines each offloading frame's resolution and detection model configuration to ensure that the analysis results can return in time.","TAODM and ROIM interact jointly to filter the repetitive spatial-temporal semantic information to maximize the processing rate while ensuring high video analysis accuracy.","Unlike most existing works, this paper investigates the real-time video analysis systems where the intelligent visual device connects to the edge server through a wireless network with fluctuating network conditions.","We decompose the real-time video analysis problem into the offloading decision and configurations selection sub-problems.","To solve these two sub-problems, we introduce a double deep Q network (DDQN) based offloading approach and a contextual multi-armed bandit (CMAB) based adaptive configurations selection approach, respectively.","A DDQN-CMAB reinforcement learning (DCRL) training framework is further developed to integrate these two approaches to improve the overall video analyzing performance.","Extensive simulations are conducted to evaluate the performance of the proposed solution, and demonstrate its superiority over counterparts."],"url":"http://arxiv.org/abs/2402.18927v1","category":"cs.CV"}
{"created":"2024-02-29 07:29:42","title":"Inappropriate Pause Detection In Dysarthric Speech Using Large-Scale Speech Recognition","abstract":"Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility. Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy. We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech. To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer. First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags. According to the newly designed task, we label pause locations at the text level and their appropriateness. We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data. Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection. Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance. Our experiments show that the proposed method better detects inappropriate pauses in dysarthric speech than baselines. (Inappropriate Pause Error Rate: 14.47%)","sentences":["Dysarthria, a common issue among stroke patients, severely impacts speech intelligibility.","Inappropriate pauses are crucial indicators in severity assessment and speech-language therapy.","We propose to extend a large-scale speech recognition model for inappropriate pause detection in dysarthric speech.","To this end, we propose task design, labeling strategy, and a speech recognition model with an inappropriate pause prediction layer.","First, we treat pause detection as speech recognition, using an automatic speech recognition (ASR) model to convert speech into text with pause tags.","According to the newly designed task, we label pause locations at the text level and their appropriateness.","We collaborate with speech-language pathologists to establish labeling criteria, ensuring high-quality annotated data.","Finally, we extend the ASR model with an inappropriate pause prediction layer for end-to-end inappropriate pause detection.","Moreover, we propose a task-tailored metric for evaluating inappropriate pause detection independent of ASR performance.","Our experiments show that the proposed method better detects inappropriate pauses in dysarthric speech than baselines.","(Inappropriate Pause Error Rate: 14.47%)"],"url":"http://arxiv.org/abs/2402.18923v1","category":"cs.CL"}
{"created":"2024-02-29 07:26:23","title":"Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation","abstract":"Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both pose-dominant and shape-dominant deformations. Using different challenging datasets, we demonstrate that our method outperforms previous state-of-the-art methods for both shape matching and interpolation, even compared to supervised approaches.","sentences":["Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance.","In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes.","To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains.","On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching.","On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations.","Furthermore, we propose a novel test-time adaptation scheme to capture both pose-dominant and shape-dominant deformations.","Using different challenging datasets, we demonstrate that our method outperforms previous state-of-the-art methods for both shape matching and interpolation, even compared to supervised approaches."],"url":"http://arxiv.org/abs/2402.18920v1","category":"cs.CV"}
{"created":"2024-02-29 07:20:02","title":"SNE-RoadSegV2: Advancing Heterogeneous Feature Fusion and Fallibility Awareness for Freespace Detection","abstract":"Feature-fusion networks with duplex encoders have proven to be an effective technique to solve the freespace detection problem. However, despite the compelling results achieved by previous research efforts, the exploration of adequate and discriminative heterogeneous feature fusion, as well as the development of fallibility-aware loss functions remains relatively scarce. This paper makes several significant contributions to address these limitations: (1) It presents a novel heterogeneous feature fusion block, comprising a holistic attention module, a heterogeneous feature contrast descriptor, and an affinity-weighted feature recalibrator, enabling a more in-depth exploitation of the inherent characteristics of the extracted features, (2) it incorporates both inter-scale and intra-scale skip connections into the decoder architecture while eliminating redundant ones, leading to both improved accuracy and computational efficiency, and (3) it introduces two fallibility-aware loss functions that separately focus on semantic-transition and depth-inconsistent regions, collectively contributing to greater supervision during model training. Our proposed heterogeneous feature fusion network (SNE-RoadSegV2), which incorporates all these innovative components, demonstrates superior performance in comparison to all other freespace detection algorithms across multiple public datasets. Notably, it ranks the 1st on the official KITTI Road benchmark.","sentences":["Feature-fusion networks with duplex encoders have proven to be an effective technique to solve the freespace detection problem.","However, despite the compelling results achieved by previous research efforts, the exploration of adequate and discriminative heterogeneous feature fusion, as well as the development of fallibility-aware loss functions remains relatively scarce.","This paper makes several significant contributions to address these limitations: (1) It presents a novel heterogeneous feature fusion block, comprising a holistic attention module, a heterogeneous feature contrast descriptor, and an affinity-weighted feature recalibrator, enabling a more in-depth exploitation of the inherent characteristics of the extracted features, (2) it incorporates both inter-scale and intra-scale skip connections into the decoder architecture while eliminating redundant ones, leading to both improved accuracy and computational efficiency, and (3) it introduces two fallibility-aware loss functions that separately focus on semantic-transition and depth-inconsistent regions, collectively contributing to greater supervision during model training.","Our proposed heterogeneous feature fusion network (SNE-RoadSegV2), which incorporates all these innovative components, demonstrates superior performance in comparison to all other freespace detection algorithms across multiple public datasets.","Notably, it ranks the 1st on the official KITTI Road benchmark."],"url":"http://arxiv.org/abs/2402.18918v1","category":"cs.CV"}
{"created":"2024-02-29 07:11:24","title":"AdaMergeX: Cross-Lingual Transfer with Large Language Models via Adaptive Adapter Merging","abstract":"As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling ''task ability'' and ''language ability'' by fine-tuning on the target task in the source language and another selected task in the target language, respectively. However, they fail to fully separate the task ability from the source language or the language ability from the chosen task. In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks. As the gap removes the impact of tasks, we assume that it remains consistent across tasks. Based on this assumption, we propose a new cross-lingual transfer method called $\\texttt{AdaMergeX}$ that utilizes adaptive adapter merging. By introducing a reference task, we can determine that the divergence of adapters fine-tuned on the reference task in both languages follows the same distribution as the divergence of adapters fine-tuned on the target task in both languages. Hence, we can obtain target adapters by combining the other three adapters. Furthermore, we propose a structure-adaptive adapter merging method. Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings.","sentences":["As an effective alternative to the direct fine-tuning on target tasks in specific languages, cross-lingual transfer addresses the challenges of limited training data by decoupling ''task ability'' and ''language ability'' by fine-tuning on the target task in the source language and another selected task in the target language, respectively.","However, they fail to fully separate the task ability from the source language or the language ability from the chosen task.","In this paper, we acknowledge the mutual reliance between task ability and language ability and direct our attention toward the gap between the target language and the source language on tasks.","As the gap removes the impact of tasks, we assume that it remains consistent across tasks.","Based on this assumption, we propose a new cross-lingual transfer method called $\\texttt{AdaMergeX}$ that utilizes adaptive adapter merging.","By introducing a reference task, we can determine that the divergence of adapters fine-tuned on the reference task in both languages follows the same distribution as the divergence of adapters fine-tuned on the target task in both languages.","Hence, we can obtain target adapters by combining the other three adapters.","Furthermore, we propose a structure-adaptive adapter merging method.","Our empirical results demonstrate that our approach yields new and effective cross-lingual transfer, outperforming existing methods across all settings."],"url":"http://arxiv.org/abs/2402.18913v1","category":"cs.CL"}
{"created":"2024-02-29 07:09:01","title":"DIGIC: Domain Generalizable Imitation Learning by Causal Discovery","abstract":"Causality has been combined with machine learning to produce robust representations for domain generalization. Most existing methods of this type require massive data from multiple domains to identify causal features by cross-domain variations, which can be expensive or even infeasible and may lead to misidentification in some cases. In this work, we make a different attempt by leveraging the demonstration data distribution to discover the causal features for a domain generalizable policy. We design a novel framework, called DIGIC, to identify the causal features by finding the direct cause of the expert action from the demonstration data distribution via causal discovery. Our framework can achieve domain generalizable imitation learning with only single-domain data and serve as a complement for cross-domain variation-based methods under non-structural assumptions on the underlying causal models. Our empirical study in various control tasks shows that the proposed framework evidently improves the domain generalization performance and has comparable performance to the expert in the original domain simultaneously.","sentences":["Causality has been combined with machine learning to produce robust representations for domain generalization.","Most existing methods of this type require massive data from multiple domains to identify causal features by cross-domain variations, which can be expensive or even infeasible and may lead to misidentification in some cases.","In this work, we make a different attempt by leveraging the demonstration data distribution to discover the causal features for a domain generalizable policy.","We design a novel framework, called DIGIC, to identify the causal features by finding the direct cause of the expert action from the demonstration data distribution via causal discovery.","Our framework can achieve domain generalizable imitation learning with only single-domain data and serve as a complement for cross-domain variation-based methods under non-structural assumptions on the underlying causal models.","Our empirical study in various control tasks shows that the proposed framework evidently improves the domain generalization performance and has comparable performance to the expert in the original domain simultaneously."],"url":"http://arxiv.org/abs/2402.18910v1","category":"cs.LG"}
{"created":"2024-02-29 07:08:34","title":"Updating Language Models with Unstructured Facts: Towards Practical Knowledge Editing","abstract":"Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date. However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles. In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE). It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts. Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark. We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines. We further show that this challenge persists even if we extract triplets as structured facts. Our analysis discloses key insights to motivate future research in UKE for more practical knowledge editing.","sentences":["Knowledge editing aims to inject knowledge updates into language models to keep them correct and up-to-date.","However, its current evaluation strategies are notably impractical: they solely update with well-curated structured facts (triplets with subjects, relations, and objects), whereas real-world knowledge updates commonly emerge in unstructured texts like news articles.","In this paper, we propose a new benchmark, Unstructured Knowledge Editing (UKE).","It evaluates editing performance directly using unstructured texts as knowledge updates, termed unstructured facts.","Hence UKE avoids the laborious construction of structured facts and enables efficient and responsive knowledge editing, becoming a more practical benchmark.","We conduct extensive experiments on newly built datasets and demonstrate that UKE poses a significant challenge to state-of-the-art knowledge editing methods, resulting in their critical performance declines.","We further show that this challenge persists even if we extract triplets as structured facts.","Our analysis discloses key insights to motivate future research in UKE for more practical knowledge editing."],"url":"http://arxiv.org/abs/2402.18909v1","category":"cs.CL"}
{"created":"2024-02-29 07:08:18","title":"Facility Location Games with Scaling Effects","abstract":"We take the classic facility location problem and consider a variation, in which each agent's individual cost function is equal to their distance from the facility multiplied by a scaling factor which is determined by the facility placement. In addition to the general class of continuous scaling functions, we also provide results for piecewise linear scaling functions which can effectively approximate or model the scaling of many real world scenarios. We focus on the objectives of total and maximum cost, describing the computation of the optimal solution. We then move to the approximate mechanism design setting, observing that the agents' preferences may no longer be single-peaked. Consequently, we characterize the conditions on scaling functions which ensure that agents have single-peaked preferences. Under these conditions, we find results on the total and maximum cost approximation ratios achievable by strategyproof and anonymous mechanisms.","sentences":["We take the classic facility location problem and consider a variation, in which each agent's individual cost function is equal to their distance from the facility multiplied by a scaling factor which is determined by the facility placement.","In addition to the general class of continuous scaling functions, we also provide results for piecewise linear scaling functions which can effectively approximate or model the scaling of many real world scenarios.","We focus on the objectives of total and maximum cost, describing the computation of the optimal solution.","We then move to the approximate mechanism design setting, observing that the agents' preferences may no longer be single-peaked.","Consequently, we characterize the conditions on scaling functions which ensure that agents have single-peaked preferences.","Under these conditions, we find results on the total and maximum cost approximation ratios achievable by strategyproof and anonymous mechanisms."],"url":"http://arxiv.org/abs/2402.18908v1","category":"cs.GT"}
{"created":"2024-02-29 07:01:48","title":"On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?","abstract":"Differentially private (DP) machine learning pipelines typically involve a two-phase process: non-private pre-training on a public dataset, followed by fine-tuning on private data using DP optimization techniques. In the DP setting, it has been observed that full fine-tuning may not always yield the best test accuracy, even for in-distribution data. This paper (1) analyzes the training dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2) explores the phenomenon of sequential fine-tuning, starting with linear probing and transitioning to full fine-tuning (LP-FT), and its impact on test loss. We provide theoretical insights into the convergence of DP fine-tuning within an overparameterized neural network and establish a utility curve that determines the allocation of privacy budget between linear probing and full fine-tuning. The theoretical results are supported by empirical evaluations on various benchmarks and models. The findings reveal the complex nature of DP fine-tuning methods. These results contribute to a deeper understanding of DP machine learning and highlight the importance of considering the allocation of privacy budget in the fine-tuning process.","sentences":["Differentially private (DP) machine learning pipelines typically involve a two-phase process: non-private pre-training on a public dataset, followed by fine-tuning on private data using DP optimization techniques.","In the DP setting, it has been observed that full fine-tuning may not always yield the best test accuracy, even for in-distribution data.","This paper (1) analyzes the training dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2) explores the phenomenon of sequential fine-tuning, starting with linear probing and transitioning to full fine-tuning (LP-FT), and its impact on test loss.","We provide theoretical insights into the convergence of DP fine-tuning within an overparameterized neural network and establish a utility curve that determines the allocation of privacy budget between linear probing and full fine-tuning.","The theoretical results are supported by empirical evaluations on various benchmarks and models.","The findings reveal the complex nature of DP fine-tuning methods.","These results contribute to a deeper understanding of DP machine learning and highlight the importance of considering the allocation of privacy budget in the fine-tuning process."],"url":"http://arxiv.org/abs/2402.18905v1","category":"cs.LG"}
{"created":"2024-02-29 06:53:16","title":"Prognostic Covariate Adjustment for Logistic Regression in Randomized Controlled Trials","abstract":"Randomized controlled trials (RCTs) with binary primary endpoints introduce novel challenges for inferring the causal effects of treatments. The most significant challenge is non-collapsibility, in which the conditional odds ratio estimand under covariate adjustment differs from the unconditional estimand in the logistic regression analysis of RCT data. This issue gives rise to apparent paradoxes, such as the variance of the estimator for the conditional odds ratio from a covariate-adjusted model being greater than the variance of the estimator from the unadjusted model. We address this challenge in the context of adjustment based on predictions of control outcomes from generative artificial intelligence (AI) algorithms, which are referred to as prognostic scores. We demonstrate that prognostic score adjustment in logistic regression increases the power of the Wald test for the conditional odds ratio under a fixed sample size, or alternatively reduces the necessary sample size to achieve a desired power, compared to the unadjusted analysis. We derive formulae for prospective calculations of the power gain and sample size reduction that can result from adjustment for the prognostic score. Furthermore, we utilize g-computation to expand the scope of prognostic score adjustment to inferences on the marginal risk difference, relative risk, and odds ratio estimands. We demonstrate the validity of our formulae via extensive simulation studies that encompass different types of logistic regression model specifications. Our simulation studies also indicate how prognostic score adjustment can reduce the variance of g-computation estimators for the marginal estimands while maintaining frequentist properties such as asymptotic unbiasedness and Type I error rate control. Our methodology can ultimately enable more definitive and conclusive analyses for RCTs with binary primary endpoints.","sentences":["Randomized controlled trials (RCTs) with binary primary endpoints introduce novel challenges for inferring the causal effects of treatments.","The most significant challenge is non-collapsibility, in which the conditional odds ratio estimand under covariate adjustment differs from the unconditional estimand in the logistic regression analysis of RCT data.","This issue gives rise to apparent paradoxes, such as the variance of the estimator for the conditional odds ratio from a covariate-adjusted model being greater than the variance of the estimator from the unadjusted model.","We address this challenge in the context of adjustment based on predictions of control outcomes from generative artificial intelligence (AI) algorithms, which are referred to as prognostic scores.","We demonstrate that prognostic score adjustment in logistic regression increases the power of the Wald test for the conditional odds ratio under a fixed sample size, or alternatively reduces the necessary sample size to achieve a desired power, compared to the unadjusted analysis.","We derive formulae for prospective calculations of the power gain and sample size reduction that can result from adjustment for the prognostic score.","Furthermore, we utilize g-computation to expand the scope of prognostic score adjustment to inferences on the marginal risk difference, relative risk, and odds ratio estimands.","We demonstrate the validity of our formulae via extensive simulation studies that encompass different types of logistic regression model specifications.","Our simulation studies also indicate how prognostic score adjustment can reduce the variance of g-computation estimators for the marginal estimands while maintaining frequentist properties such as asymptotic unbiasedness and Type I error rate control.","Our methodology can ultimately enable more definitive and conclusive analyses for RCTs with binary primary endpoints."],"url":"http://arxiv.org/abs/2402.18900v1","category":"stat.ME"}
{"created":"2024-02-29 05:39:33","title":"B-mesons as essential probes of hot QCD matter","abstract":"This article elucidates the pivotal role of b-mesons and bottomonium states in exploring the existence and properties of hot QCD matter (commonly known as quark-gluon-plasma (QGP) produced within the crucible heavy-ion collision experiments). Owing to the complex and confounding nature of strong interaction force the direct detection of probing the hot QCD matter is not feasible. In light of this, investigating the dynamics of b-quarks and anti-quarks within the hot QCD medium emerges as an invaluable indirect probe. The impact of b-quarks and the mesons spans a spectrum of interesting domains regarding the physics of QCD at finite temperature, encompassing the QCD phase transition, color screening, quarkonia dissociation, heavy quark energy loss and collective flow, anisotropic aspects, and strongly coupled nature of hot QCD medium. These aspects underscore the indispensable nature of B-mesons in the quest to create and explore the complex nature of strong interaction force through the QGP/hot QCD matter. In this context, we mainly focus on works related to transport studies of b-mesons in hot QCD medium, lattice QCD, and effective field theory studies on bottomonium states, and finally, open quantum system frameworks to quarkonia to explore the properties of hot QCD medium in relativistic heavy-ion collision experiments.","sentences":["This article elucidates the pivotal role of b-mesons and bottomonium states in exploring the existence and properties of hot QCD matter (commonly known as quark-gluon-plasma (QGP) produced within the crucible heavy-ion collision experiments).","Owing to the complex and confounding nature of strong interaction force the direct detection of probing the hot QCD matter is not feasible.","In light of this, investigating the dynamics of b-quarks and anti-quarks within the hot QCD medium emerges as an invaluable indirect probe.","The impact of b-quarks and the mesons spans a spectrum of interesting domains regarding the physics of QCD at finite temperature, encompassing the QCD phase transition, color screening, quarkonia dissociation, heavy quark energy loss and collective flow, anisotropic aspects, and strongly coupled nature of hot QCD medium.","These aspects underscore the indispensable nature of B-mesons in the quest to create and explore the complex nature of strong interaction force through the QGP/hot QCD matter.","In this context, we mainly focus on works related to transport studies of b-mesons in hot QCD medium, lattice QCD, and effective field theory studies on bottomonium states, and finally, open quantum system frameworks to quarkonia to explore the properties of hot QCD medium in relativistic heavy-ion collision experiments."],"url":"http://arxiv.org/abs/2402.18870v1","category":"hep-ph"}
{"created":"2024-02-29 05:27:45","title":"Analyzing and Reducing Catastrophic Forgetting in Parameter Efficient Tuning","abstract":"Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation. However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem. A trade-off needs to be kept between learning plasticity and memory stability. Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios. In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley. Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability. Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations. Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem. Our code is available at \\url{https://github.com/which47/LLMCL}.","sentences":["Existing research has shown that large language models (LLMs) exhibit remarkable performance in language understanding and generation.","However, when LLMs are continuously fine-tuned on complex and diverse domain-specific downstream tasks, the inference performance on historical tasks decreases dramatically, which is known as a catastrophic forgetting problem.","A trade-off needs to be kept between learning plasticity and memory stability.","Plenty of existing works have explored strategies like memory replay, regularization and parameter isolation, but little is known about the geometric connection of various adjacent minima in the continual LLMs fine-tuning scenarios.","In this work, we investigate the geometric connections of different minima through the lens of mode connectivity, which means different minima can be connected by a low-loss valley.","Through extensive experiments, we uncover the mode connectivity phenomenon in the LLMs continual learning scenario and find that it can strike a balance between plasticity and stability.","Building upon these findings, we propose a simple yet effective method called Interpolation-based LoRA (I-LoRA), which constructs a dual-memory experience replay framework based on LoRA parameter interpolations.","Extensive experiments and analysis on eight domain-specific CL benchmarks demonstrate that I-LoRA consistently show significant improvement over the previous state-of-the-art approaches with up to $11\\%$ performance gains, providing a strong baseline and insights for future research on the large language model continual learning problem.","Our code is available at \\url{https://github.com/which47/LLMCL}."],"url":"http://arxiv.org/abs/2402.18865v1","category":"cs.LG"}
{"created":"2024-02-29 05:00:30","title":"Rethinking Multi-domain Generalization with A General Learning Objective","abstract":"Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping. However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions. In this paper, we propose to leverage a $Y$-mapping to relax the constraint. We rethink the learning objective for mDG and design a new \\textbf{general learning objective} to interpret and analyze most existing mDG wisdom. This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior. Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints. We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \\textbf{optimize partially the objective} and thus lead to limited performance. As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts. Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification.","sentences":["Multi-domain generalization (mDG) is universally aimed to minimize the discrepancy between training and testing distributions to enhance marginal-to-label distribution mapping.","However, existing mDG literature lacks a general learning objective paradigm and often imposes constraints on static target marginal distributions.","In this paper, we propose to leverage a $Y$-mapping to relax the constraint.","We rethink the learning objective for mDG and design a new \\textbf{general learning objective} to interpret and analyze most existing mDG wisdom.","This general objective is bifurcated into two synergistic amis: learning domain-independent conditional features and maximizing a posterior.","Explorations also extend to two effective regularization terms that incorporate prior information and suppress invalid causality, alleviating the issues that come with relaxed constraints.","We theoretically contribute an upper bound for the domain alignment of domain-independent conditional features, disclosing that many previous mDG endeavors actually \\textbf{optimize partially the objective} and thus lead to limited performance.","As such, our study distills a general learning objective into four practical components, providing a general, robust, and flexible mechanism to handle complex domain shifts.","Extensive empirical results indicate that the proposed objective with $Y$-mapping leads to substantially better mDG performance in various downstream tasks, including regression, segmentation, and classification."],"url":"http://arxiv.org/abs/2402.18853v1","category":"cs.LG"}
{"created":"2024-02-29 05:00:01","title":"Applications of 0-1 Neural Networks in Prescription and Prediction","abstract":"A key challenge in medical decision making is learning treatment policies for patients with limited observational data. This challenge is particularly evident in personalized healthcare decision-making, where models need to take into account the intricate relationships between patient characteristics, treatment options, and health outcomes. To address this, we introduce prescriptive networks (PNNs), shallow 0-1 neural networks trained with mixed integer programming that can be used with counterfactual estimation to optimize policies in medium data settings. These models offer greater interpretability than deep neural networks and can encode more complex policies than common models such as decision trees. We show that PNNs can outperform existing methods in both synthetic data experiments and in a case study of assigning treatments for postpartum hypertension. In particular, PNNs are shown to produce policies that could reduce peak blood pressure by 5.47 mm Hg (p=0.02) over existing clinical practice, and by 2 mm Hg (p=0.01) over the next best prescriptive modeling technique. Moreover PNNs were more likely than all other models to correctly identify clinically significant features while existing models relied on potentially dangerous features such as patient insurance information and race that could lead to bias in treatment.","sentences":["A key challenge in medical decision making is learning treatment policies for patients with limited observational data.","This challenge is particularly evident in personalized healthcare decision-making, where models need to take into account the intricate relationships between patient characteristics, treatment options, and health outcomes.","To address this, we introduce prescriptive networks (PNNs), shallow 0-1 neural networks trained with mixed integer programming that can be used with counterfactual estimation to optimize policies in medium data settings.","These models offer greater interpretability than deep neural networks and can encode more complex policies than common models such as decision trees.","We show that PNNs can outperform existing methods in both synthetic data experiments and in a case study of assigning treatments for postpartum hypertension.","In particular, PNNs are shown to produce policies that could reduce peak blood pressure by 5.47 mm","Hg (p=0.02) over existing clinical practice, and by 2 mm Hg (p=0.01) over the next best prescriptive modeling technique.","Moreover PNNs were more likely than all other models to correctly identify clinically significant features while existing models relied on potentially dangerous features such as patient insurance information and race that could lead to bias in treatment."],"url":"http://arxiv.org/abs/2402.18851v1","category":"cs.LG"}
{"created":"2024-02-29 04:56:07","title":"A simple model of global cascades on random hypergraphs","abstract":"This study investigates the dynamics of social contagion within hypergraphs, reflecting the modern intricacies of group influence on individual decision-making as magnified by social media platforms. We introduce a novel social contagion model that captures the nuanced interplay between individual caution in decision-making (\\(\\phi_k\\)) and the efficiency of information transmission within group interactions (\\(\\phi_m\\)). By integrating dual-threshold mechanisms, our model delineates the conditions under which individuals assimilate group behaviors, opinions, or attitudes. Our findings reveal a critical balance in the transmission of information, with higher individual thresholds (\\(\\phi_k\\)) necessitating greater group consensus (\\(\\phi_m\\)) to precipitate a global cascade. The model underscores the pivotal role of group interactions in social contagion processes, challenging traditional contagion models that focus on pairwise interactions. We demonstrate the utility of our model through extensive simulations, revealing that the collective dynamics of social contagion are significantly influenced by the structural parameters of the underlying hypergraph. The implications of our work extend to a variety of domains, from marketing to public health, offering a robust framework to better understand and leverage the phenomena of social contagion in an increasingly interconnected world.","sentences":["This study investigates the dynamics of social contagion within hypergraphs, reflecting the modern intricacies of group influence on individual decision-making as magnified by social media platforms.","We introduce a novel social contagion model that captures the nuanced interplay between individual caution in decision-making (\\(\\phi_k\\)) and the efficiency of information transmission within group interactions (\\(\\phi_m\\)).","By integrating dual-threshold mechanisms, our model delineates the conditions under which individuals assimilate group behaviors, opinions, or attitudes.","Our findings reveal a critical balance in the transmission of information, with higher individual thresholds (\\(\\phi_k\\)) necessitating greater group consensus (\\(\\phi_m\\)) to precipitate a global cascade.","The model underscores the pivotal role of group interactions in social contagion processes, challenging traditional contagion models that focus on pairwise interactions.","We demonstrate the utility of our model through extensive simulations, revealing that the collective dynamics of social contagion are significantly influenced by the structural parameters of the underlying hypergraph.","The implications of our work extend to a variety of domains, from marketing to public health, offering a robust framework to better understand and leverage the phenomena of social contagion in an increasingly interconnected world."],"url":"http://arxiv.org/abs/2402.18850v1","category":"physics.soc-ph"}
{"created":"2024-02-29 04:53:06","title":"Enhancing Steganographic Text Extraction: Evaluating the Impact of NLP Models on Accuracy and Semantic Coherence","abstract":"This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text. Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters. To address this issue, this study proposes an innovative LSB-NLP hybrid framework. This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction. Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters. The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding. The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems.","sentences":["This study discusses a new method combining image steganography technology with Natural Language Processing (NLP) large models, aimed at improving the accuracy and robustness of extracting steganographic text.","Traditional Least Significant Bit (LSB) steganography techniques face challenges in accuracy and robustness of information extraction when dealing with complex character encoding, such as Chinese characters.","To address this issue, this study proposes an innovative LSB-NLP hybrid framework.","This framework integrates the advanced capabilities of NLP large models, such as error detection, correction, and semantic consistency analysis, as well as information reconstruction techniques, thereby significantly enhancing the robustness of steganographic text extraction.","Experimental results show that the LSB-NLP hybrid framework excels in improving the extraction accuracy of steganographic text, especially in handling Chinese characters.","The findings of this study not only confirm the effectiveness of combining image steganography technology and NLP large models but also propose new ideas for research and application in the field of information hiding.","The successful implementation of this interdisciplinary approach demonstrates the great potential of integrating image steganography technology with natural language processing technology in solving complex information processing problems."],"url":"http://arxiv.org/abs/2402.18849v1","category":"cs.CV"}
{"created":"2024-02-29 03:20:53","title":"The Machine Can't Replace the Human Heart","abstract":"What is the true heart of mental healthcare -- innovation or humanity? Can virtual therapy ever replicate the profound human bonds where healing arises? As artificial intelligence and immersive technologies promise expanded access, safeguards must ensure technologies remain supplementary tools guided by providers' wisdom. Implementation requires nuance balancing efficiency and empathy. If conscious of ethical risks, perhaps AI could restore humanity by automating tasks, giving providers more time to listen. Yet no algorithm can replicate the seat of dignity within. We must ask ourselves: What future has people at its core? One where AI thoughtfully plays a collaborative role? Or where pursuit of progress leaves vulnerability behind? This commentary argues for a balanced approach thoughtfully integrating technology while retaining care's irreplaceable human essence, at the heart of this profoundly human profession. Ultimately, by nurturing innovation and humanity together, perhaps we reach new heights of empathy previously unimaginable.","sentences":["What is the true heart of mental healthcare -- innovation or humanity?","Can virtual therapy ever replicate the profound human bonds where healing arises?","As artificial intelligence and immersive technologies promise expanded access, safeguards must ensure technologies remain supplementary tools guided by providers' wisdom.","Implementation requires nuance balancing efficiency and empathy.","If conscious of ethical risks, perhaps AI could restore humanity by automating tasks, giving providers more time to listen.","Yet no algorithm can replicate the seat of dignity within.","We must ask ourselves: What future has people at its core?","One where AI thoughtfully plays a collaborative role?","Or where pursuit of progress leaves vulnerability behind?","This commentary argues for a balanced approach thoughtfully integrating technology while retaining care's irreplaceable human essence, at the heart of this profoundly human profession.","Ultimately, by nurturing innovation and humanity together, perhaps we reach new heights of empathy previously unimaginable."],"url":"http://arxiv.org/abs/2402.18826v1","category":"cs.CY"}
{"created":"2024-02-29 02:55:59","title":"Nano-Electromagnetic Super-dephasing in Collective Atom-Atom Interactions","abstract":"Pure dephasing and spontaneous emission are two non-unitary processes of atoms or spins interacting with fluctuating electromagnetic (EM) modes. Collective spontaneous emission (e.g., superradiance) originates from interactions with EM modes in resonance with atoms and has received considerable attention. Meanwhile, the analogous collective dephasing phenomena remain poorly understood. Here, we introduce the nano-EM super-dephasing phenomenon arising in the photonic environment near lossy material interfaces. We show that this effect is enhanced by over 10 orders of magnitude compared to free space or photonic cavities due to the presence of long-range correlations in low-frequency evanescent EM fluctuations. We unravel the universality of nano-EM super-dephasing behaviors near ferrimagnets, metals, and superconductors and their dependence on low-frequency material properties. We demonstrate that the scaling of nano-EM super-dephasing is independent of EM modes' wavelengths and differs from the conventional $N^2$ scaling of superradiance by analyzing the decoherence of entangled states, including GHZ states. Finally, we show how to experimentally isolate and control super-dephasing to open interesting frontiers for scalable quantum systems.","sentences":["Pure dephasing and spontaneous emission are two non-unitary processes of atoms or spins interacting with fluctuating electromagnetic (EM) modes.","Collective spontaneous emission (e.g., superradiance) originates from interactions with EM modes in resonance with atoms and has received considerable attention.","Meanwhile, the analogous collective dephasing phenomena remain poorly understood.","Here, we introduce the nano-EM super-dephasing phenomenon arising in the photonic environment near lossy material interfaces.","We show that this effect is enhanced by over 10 orders of magnitude compared to free space or photonic cavities due to the presence of long-range correlations in low-frequency evanescent EM fluctuations.","We unravel the universality of nano-EM super-dephasing behaviors near ferrimagnets, metals, and superconductors and their dependence on low-frequency material properties.","We demonstrate that the scaling of nano-EM super-dephasing is independent of EM modes' wavelengths and differs from the conventional $N^2$ scaling of superradiance by analyzing the decoherence of entangled states, including GHZ states.","Finally, we show how to experimentally isolate and control super-dephasing to open interesting frontiers for scalable quantum systems."],"url":"http://arxiv.org/abs/2402.18816v1","category":"quant-ph"}
{"created":"2024-02-29 02:55:26","title":"How do Large Language Models Handle Multilingualism?","abstract":"Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages. In this work, we delve into the question: How do LLMs handle multilingualism? We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase. In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively. In the last several layers, LLMs generate responses that align with the original language of the query. In addition, we investigate the existence of language-specific neurons when processing a certain language. To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs. By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose. Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort.","sentences":["Large language models (LLMs) demonstrate remarkable performance across a spectrum of languages.","In this work, we delve into the question: How do LLMs handle multilingualism?","We introduce a framework that depicts LLMs' processing of multilingual inputs: In the first several layers, LLMs understand the question, converting multilingual inputs into English to facilitate the task-solving phase.","In the intermediate layers, LLMs engage in problem-solving by thinking in English and incorporating multilingual knowledge to obtain factual content, leveraging the self-attention and feed-forward structures, respectively.","In the last several layers, LLMs generate responses that align with the original language of the query.","In addition, we investigate the existence of language-specific neurons when processing a certain language.","To detect neurons activated by the input language, even without labels, we innovatively design a Parallel Language specific Neuron Detection ($\\texttt{PLND}$) method that effectively measures the significance of neurons when handling multilingual inputs.","By comprehensive ablation analysis through deactivating neurons of different layers and structures, we verify the framework that we propose.","Additionally, we demonstrate that we can utilize such a framework to effectively enhance the multilingual ability with much less training effort."],"url":"http://arxiv.org/abs/2402.18815v1","category":"cs.CL"}
{"created":"2024-02-29 02:26:33","title":"Stimulation technology for brain and nerves, now and future","abstract":"In individuals afflicted with conditions such as paralysis, the implementation of Brain-Computer-Interface (BCI) has begun to significantly impact their quality of life. Furthermore, even in healthy individuals, the anticipated advantages of brain-to-brain communication and brain-to-computer interaction hold considerable promise for the future. This is attributed to the liberation from bodily constraints and the transcendence of existing limitations inherent in contemporary brain-to-brain communication methods. To actualize a comprehensive BCI, the establishment of bidirectional communication between the brain and the external environment is imperative. While neural input technology spans diverse disciplines and is currently advancing rapidly, a notable absence exists in the form of review papers summarizing the technology from the standpoint of the latest or potential input methods. The challenges encountered encompass the requisite for bidirectional communication to achieve a holistic BCI, as well as obstacles related to information volume, precision, and invasiveness. The review section comprehensively addresses both invasive and non-invasive techniques, incorporating nanotech/micro-device technology and the integration of Artificial Intelligence (AI) in brain stimulation.","sentences":["In individuals afflicted with conditions such as paralysis, the implementation of Brain-Computer-Interface (BCI) has begun to significantly impact their quality of life.","Furthermore, even in healthy individuals, the anticipated advantages of brain-to-brain communication and brain-to-computer interaction hold considerable promise for the future.","This is attributed to the liberation from bodily constraints and the transcendence of existing limitations inherent in contemporary brain-to-brain communication methods.","To actualize a comprehensive BCI, the establishment of bidirectional communication between the brain and the external environment is imperative.","While neural input technology spans diverse disciplines and is currently advancing rapidly, a notable absence exists in the form of review papers summarizing the technology from the standpoint of the latest or potential input methods.","The challenges encountered encompass the requisite for bidirectional communication to achieve a holistic BCI, as well as obstacles related to information volume, precision, and invasiveness.","The review section comprehensively addresses both invasive and non-invasive techniques, incorporating nanotech/micro-device technology and the integration of Artificial Intelligence (AI) in brain stimulation."],"url":"http://arxiv.org/abs/2402.18808v1","category":"q-bio.NC"}
{"created":"2024-02-29 02:22:23","title":"On the Decision-Making Abilities in Role-Playing using Large Language Models","abstract":"Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts. When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns. In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing. Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks. Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population. Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\\&$exploitation trade-off ability, reasoning ability, and safety. Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4. Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs. These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics.","sentences":["Large language models (LLMs) are now increasingly utilized for role-playing tasks, especially in impersonating domain-specific experts, primarily through role-playing prompts.","When interacting in real-world scenarios, the decision-making abilities of a role significantly shape its behavioral patterns.","In this paper, we concentrate on evaluating the decision-making abilities of LLMs post role-playing thereby validating the efficacy of role-playing.","Our goal is to provide metrics and guidance for enhancing the decision-making abilities of LLMs in role-playing tasks.","Specifically, we first use LLMs to generate virtual role descriptions corresponding to the 16 personality types of Myers-Briggs Type Indicator (abbreviated as MBTI) representing a segmentation of the population.","Then we design specific quantitative operations to evaluate the decision-making abilities of LLMs post role-playing from four aspects: adaptability, exploration$\\&$exploitation trade-off ability, reasoning ability, and safety.","Finally, we analyze the association between the performance of decision-making and the corresponding MBTI types through GPT-4.","Extensive experiments demonstrate stable differences in the four aspects of decision-making abilities across distinct roles, signifying a robust correlation between decision-making abilities and the roles emulated by LLMs.","These results underscore that LLMs can effectively impersonate varied roles while embodying their genuine sociological characteristics."],"url":"http://arxiv.org/abs/2402.18807v1","category":"cs.CL"}
{"created":"2024-02-29 01:15:17","title":"Brain-inspired and Self-based Artificial Intelligence","abstract":"The question \"Can machines think?\" and the Turing Test to assess whether machines could achieve human-level intelligence is one of the roots of AI. With the philosophical argument \"I think, therefore I am\", this paper challenge the idea of a \"thinking machine\" supported by current AIs since there is no sense of self in them. Current artificial intelligence is only seemingly intelligent information processing and does not truly understand or be subjectively aware of oneself and perceive the world with the self as human intelligence does. In this paper, we introduce a Brain-inspired and Self-based Artificial Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to coordinating various cognitive functions and learning strategies in a self-organized manner to build human-level AI models and robotic applications. Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the future AI, rooted with a practical hierarchical Self framework, including Perception and Learning, Bodily Self, Autonomous Self, Social Self, and Conceptual Self. The hierarchical framework of the Self highlights self-based environment perception, self-bodily modeling, autonomous interaction with the environment, social interaction and collaboration with others, and even more abstract understanding of the Self. Furthermore, the positive mutual promotion and support among multiple levels of Self, as well as between Self and learning, enhance the BriSe AI's conscious understanding of information and flexible adaptation to complex environments, serving as a driving force propelling BriSe AI towards real Artificial General Intelligence.","sentences":["The question \"Can machines think?\" and the Turing Test to assess whether machines could achieve human-level intelligence is one of the roots of AI.","With the philosophical argument \"I think, therefore I am\", this paper challenge the idea of a \"thinking machine\" supported by current AIs since there is no sense of self in them.","Current artificial intelligence is only seemingly intelligent information processing and does not truly understand or be subjectively aware of oneself and perceive the world with the self as human intelligence does.","In this paper, we introduce a Brain-inspired and Self-based Artificial Intelligence (BriSe AI) paradigm.","This BriSe AI paradigm is dedicated to coordinating various cognitive functions and learning strategies in a self-organized manner to build human-level AI models and robotic applications.","Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the future AI, rooted with a practical hierarchical Self framework, including Perception and Learning, Bodily Self, Autonomous Self, Social Self, and Conceptual Self.","The hierarchical framework of the Self highlights self-based environment perception, self-bodily modeling, autonomous interaction with the environment, social interaction and collaboration with others, and even more abstract understanding of the Self.","Furthermore, the positive mutual promotion and support among multiple levels of Self, as well as between Self and learning, enhance the BriSe AI's conscious understanding of information and flexible adaptation to complex environments, serving as a driving force propelling BriSe AI towards real Artificial General Intelligence."],"url":"http://arxiv.org/abs/2402.18784v1","category":"cs.AI"}
{"created":"2024-02-28 23:57:04","title":"Learning with Language-Guided State Abstractions","abstract":"We describe a framework for using natural language to design state abstractions for imitation learning. Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones. These state representations are typically manually specified, or derived from other labor-intensive labeling procedures. Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks. In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states. Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications. We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot.","sentences":["We describe a framework for using natural language to design state abstractions for imitation learning.","Generalizable policy learning in high-dimensional observation spaces is facilitated by well-designed state representations, which can surface important features of an environment and hide irrelevant ones.","These state representations are typically manually specified, or derived from other labor-intensive labeling procedures.","Our method, LGA (language-guided abstraction), uses a combination of natural language supervision and background knowledge from language models (LMs) to automatically build state representations tailored to unseen tasks.","In LGA, a user first provides a (possibly incomplete) description of a target task in natural language; next, a pre-trained LM translates this task description into a state abstraction function that masks out irrelevant features; finally, an imitation policy is trained using a small number of demonstrations and LGA-generated abstract states.","Experiments on simulated robotic tasks show that LGA yields state abstractions similar to those designed by humans, but in a fraction of the time, and that these abstractions improve generalization and robustness in the presence of spurious correlations and ambiguous specifications.","We illustrate the utility of the learned abstractions on mobile manipulation tasks with a Spot robot."],"url":"http://arxiv.org/abs/2402.18759v1","category":"cs.RO"}
{"created":"2024-02-28 23:01:24","title":"Fine-Tuned Machine Translation Metrics Struggle in Unseen Domains","abstract":"We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain. We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference. We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments.","sentences":["We introduce a new, extensive multidimensional quality metrics (MQM) annotated dataset covering 11 language pairs in the biomedical domain.","We use this dataset to investigate whether machine translation (MT) metrics which are fine-tuned on human-generated MT quality judgements are robust to domain shifts between training and inference.","We find that fine-tuned metrics exhibit a substantial performance drop in the unseen domain scenario relative to metrics that rely on the surface form, as well as pre-trained metrics which are not fine-tuned on MT quality judgments."],"url":"http://arxiv.org/abs/2402.18747v1","category":"cs.CL"}
{"created":"2024-02-28 22:54:08","title":"A revision on Multi-Criteria Decision Making methods for Multi-UAV Mission Planning Support","abstract":"Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively used in many commercial applications due to their manageability and risk avoidance. One of the main problems considered is the Mission Planning for multiple UAVs, where a solution plan must be found satisfying the different constraints of the problem. This problem has multiple variables that must be optimized simultaneously, such as the makespan, the cost of the mission or the risk. Therefore, the problem has a lot of possible optimal solutions, and the operator must select the final solution to be executed among them. In order to reduce the workload of the operator in this decision process, a Decision Support System (DSS) becomes necessary. In this work, a DSS consisting of ranking and filtering systems, which order and reduce the optimal solutions, has been designed. With regard to the ranking system, a wide range of Multi-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are compared on a multi-UAV mission planning scenario, in order to study which method could fit better in a multi-UAV decision support system. Expert operators have evaluated the solutions returned, and the results show, on the one hand, that fuzzy methods generally achieve better average scores, and on the other, that all of the tested methods perform better when the preferences of the operators are biased towards a specific variable, and worse when their preferences are balanced. For the filtering system, a similarity function based on the proximity of the solutions has been designed, and on top of that, a threshold is tuned empirically to decide how to filter solutions without losing much of the hypervolume of the space of solutions.","sentences":["Over the last decade, Unmanned Aerial Vehicles (UAVs) have been extensively used in many commercial applications due to their manageability and risk avoidance.","One of the main problems considered is the Mission Planning for multiple UAVs, where a solution plan must be found satisfying the different constraints of the problem.","This problem has multiple variables that must be optimized simultaneously, such as the makespan, the cost of the mission or the risk.","Therefore, the problem has a lot of possible optimal solutions, and the operator must select the final solution to be executed among them.","In order to reduce the workload of the operator in this decision process, a Decision Support System (DSS) becomes necessary.","In this work, a DSS consisting of ranking and filtering systems, which order and reduce the optimal solutions, has been designed.","With regard to the ranking system, a wide range of Multi-Criteria Decision Making (MCDM) methods, including some fuzzy MCDM, are compared on a multi-UAV mission planning scenario, in order to study which method could fit better in a multi-UAV decision support system.","Expert operators have evaluated the solutions returned, and the results show, on the one hand, that fuzzy methods generally achieve better average scores, and on the other, that all of the tested methods perform better when the preferences of the operators are biased towards a specific variable, and worse when their preferences are balanced.","For the filtering system, a similarity function based on the proximity of the solutions has been designed, and on top of that, a threshold is tuned empirically to decide how to filter solutions without losing much of the hypervolume of the space of solutions."],"url":"http://arxiv.org/abs/2402.18743v1","category":"cs.AI"}
{"created":"2024-02-28 22:25:02","title":"GAIA: Categorical Foundations of Generative AI","abstract":"In this paper, we propose GAIA, a generative AI architecture based on category theory. GAIA is based on a hierarchical model where modules are organized as a simplicial complex. Each simplicial complex updates its internal parameters biased on information it receives from its superior simplices and in turn relays updates to its subordinate sub-simplices. Parameter updates are formulated in terms of lifting diagrams over simplicial sets, where inner and outer horn extensions correspond to different types of learning problems. Backpropagation is modeled as an endofunctor over the category of parameters, leading to a coalgebraic formulation of deep learning.","sentences":["In this paper, we propose GAIA, a generative AI architecture based on category theory.","GAIA is based on a hierarchical model where modules are organized as a simplicial complex.","Each simplicial complex updates its internal parameters biased on information it receives from its superior simplices and in turn relays updates to its subordinate sub-simplices.","Parameter updates are formulated in terms of lifting diagrams over simplicial sets, where inner and outer horn extensions correspond to different types of learning problems.","Backpropagation is modeled as an endofunctor over the category of parameters, leading to a coalgebraic formulation of deep learning."],"url":"http://arxiv.org/abs/2402.18732v1","category":"cs.AI"}
{"created":"2024-02-28 22:20:18","title":"Searches for exclusive Higgs boson decays into $D^*\u03b3$ and $Z$ boson decays into $D^0\u03b3$ and $K^0_s\u03b3$ in $pp$ collisions at $\\sqrt{s} = 13$ TeV with the ATLAS detector","abstract":"Searches for the exclusive decays of the Higgs boson into $D^*\\gamma$ and of the $Z$ boson into $D^0\\gamma$ and $K^0_s\\gamma$ can probe flavour-violating Higgs and $Z$ boson couplings to light quarks. Searches for these decays are performed with a $pp$ collision data sample corresponding to an integrated luminosity of $136.3$ fb$^{-1}$ collected at $\\sqrt{s}=13$ TeV between 2016-2018 with the ATLAS detector at the CERN Large Hadron Collider. In the $D^*\\gamma$ and $D^0\\gamma$ channels, the observed (expected) 95$\\%$ confidence-level upper limits on the respective branching fractions are ${\\cal B}(H\\rightarrow D^*\\gamma)< 1.0 (1.2)\\times 10^{-3}$, ${\\cal B}(Z\\rightarrow D^0\\gamma)< 4.0 (3.4)\\times 10^{-6}$, while the corresponding results in the $K^0_s\\gamma$ channel are ${\\cal B}(Z\\rightarrow K^0_s\\gamma)< 3.1 (3.0)\\times 10^{-6}$.","sentences":["Searches for the exclusive decays of the Higgs boson into $D^*\\gamma$ and of the $Z$ boson into $D^0\\gamma$ and $K^0_s\\gamma$ can probe flavour-violating Higgs and $Z$ boson couplings to light quarks.","Searches for these decays are performed with a $pp$ collision data sample corresponding to an integrated luminosity of $136.3$ fb$^{-1}$ collected at $\\sqrt{s}=13$ TeV between 2016-2018 with the ATLAS detector at the CERN Large Hadron Collider.","In the $D^*\\gamma$ and $D^0\\gamma$ channels, the observed (expected) 95$\\%$ confidence-level upper limits on the respective branching fractions are ${\\cal B}(H\\rightarrow D^*\\gamma)<","1.0 (1.2)\\times 10^{-3}$, ${\\cal B}(Z\\rightarrow D^0\\gamma)< 4.0 (3.4)\\times 10^{-6}$, while the corresponding results in the $K^0_s\\gamma$ channel are ${\\cal B}(Z\\rightarrow K^0_s\\gamma)< 3.1 (3.0)\\times 10^{-6}$."],"url":"http://arxiv.org/abs/2402.18731v1","category":"hep-ex"}
{"created":"2024-02-28 22:02:10","title":"Unveiling Privacy, Memorization, and Input Curvature Links","abstract":"Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems. However, they tend to overfit to and memorize the training set. Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy. To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use. Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization. It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score. However, there is a lack of theoretical understanding linking memorization with input loss curvature. In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature. First, we derive an upper bound on memorization characterized by both differential privacy and input loss curvature. Second, we present a novel insight showing that input loss curvature is upper-bounded by the differential privacy parameter. Our theoretical findings are further empirically validated using deep models on CIFAR and ImageNet datasets, showing a strong correlation between our theoretical predictions and results observed in practice.","sentences":["Deep Neural Nets (DNNs) have become a pervasive tool for solving many emerging problems.","However, they tend to overfit to and memorize the training set.","Memorization is of keen interest since it is closely related to several concepts such as generalization, noisy learning, and privacy.","To study memorization, Feldman (2019) proposed a formal score, however its computational requirements limit its practical use.","Recent research has shown empirical evidence linking input loss curvature (measured by the trace of the loss Hessian w.r.t inputs) and memorization.","It was shown to be ~3 orders of magnitude more efficient than calculating the memorization score.","However, there is a lack of theoretical understanding linking memorization with input loss curvature.","In this paper, we not only investigate this connection but also extend our analysis to establish theoretical links between differential privacy, memorization, and input loss curvature.","First, we derive an upper bound on memorization characterized by both differential privacy and input loss curvature.","Second, we present a novel insight showing that input loss curvature is upper-bounded by the differential privacy parameter.","Our theoretical findings are further empirically validated using deep models on CIFAR and ImageNet datasets, showing a strong correlation between our theoretical predictions and results observed in practice."],"url":"http://arxiv.org/abs/2402.18726v1","category":"cs.LG"}
{"created":"2024-02-28 21:47:30","title":"Learning Associative Memories with Gradient Descent","abstract":"This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings. We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings. Through theory and experiments, we provide several insights. In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.'' Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes. The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence. In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes. Finally, we assess the validity of our findings on small Transformer models.","sentences":["This work focuses on the training dynamics of one associative memory module storing outer products of token embeddings.","We reduce this problem to the study of a system of particles, which interact according to properties of the data distribution and correlations between embeddings.","Through theory and experiments, we provide several insights.","In overparameterized regimes, we obtain logarithmic growth of the ``classification margins.''","Yet, we show that imbalance in token frequencies and memory interferences due to correlated embeddings lead to oscillatory transitory regimes.","The oscillations are more pronounced with large step sizes, which can create benign loss spikes, although these learning rates speed up the dynamics and accelerate the asymptotic convergence.","In underparameterized regimes, we illustrate how the cross-entropy loss can lead to suboptimal memorization schemes.","Finally, we assess the validity of our findings on small Transformer models."],"url":"http://arxiv.org/abs/2402.18724v1","category":"cs.LG"}
{"created":"2024-02-28 21:46:42","title":"Spectroscopy of N=50 isotones with the valence-space density matrix renormalization group","abstract":"The recently proposed combination of the valence-space in-medium similarity renormalization group (VS-IMSRG) with the density matrix renormalization group (DMRG) offers a scalable and flexible many-body approach for strongly correlated open-shell nuclei. We use the VS-DMRG to investigate the low-lying spectroscopy of N=50 isotones, which are characteristic for their transition between single-particle and collective excitations. We also study electromagnetic transitions and show the advantage of the VS-DMRG to capture the underlying physics more efficiently, with significantly improved convergence compared to state-of-the-art shell-model truncations. Combined with an analysis of quantum information measures, this further establishes the VS-DMRG as a valuable method for ab initio calculations of nuclei.","sentences":["The recently proposed combination of the valence-space in-medium similarity renormalization group (VS-IMSRG) with the density matrix renormalization group (DMRG) offers a scalable and flexible many-body approach for strongly correlated open-shell nuclei.","We use the VS-DMRG to investigate the low-lying spectroscopy of N=50 isotones, which are characteristic for their transition between single-particle and collective excitations.","We also study electromagnetic transitions and show the advantage of the VS-DMRG to capture the underlying physics more efficiently, with significantly improved convergence compared to state-of-the-art shell-model truncations.","Combined with an analysis of quantum information measures, this further establishes the VS-DMRG as a valuable method for ab initio calculations of nuclei."],"url":"http://arxiv.org/abs/2402.18723v1","category":"nucl-th"}
{"created":"2024-02-28 21:23:54","title":"Commonsense Ontology Micropatterns","abstract":"The previously introduced Modular Ontology Modeling methodology (MOMo) attempts to mimic the human analogical process by using modular patterns to assemble more complex concepts. To support this, MOMo organizes organizes ontology design patterns into design libraries, which are programmatically queryable, to support accelerated ontology development, for both human and automated processes. However, a major bottleneck to large-scale deployment of MOMo is the (to-date) limited availability of ready-to-use ontology design patterns. At the same time, Large Language Models have quickly become a source of common knowledge and, in some cases, replacing search engines for questions. In this paper, we thus present a collection of 104 ontology design patterns representing often occurring nouns, curated from the common-sense knowledge available in LLMs, organized into a fully-annotated modular ontology design library ready for use with MOMo.","sentences":["The previously introduced Modular Ontology Modeling methodology (MOMo) attempts to mimic the human analogical process by using modular patterns to assemble more complex concepts.","To support this, MOMo organizes organizes ontology design patterns into design libraries, which are programmatically queryable, to support accelerated ontology development, for both human and automated processes.","However, a major bottleneck to large-scale deployment of MOMo is the (to-date) limited availability of ready-to-use ontology design patterns.","At the same time, Large Language Models have quickly become a source of common knowledge and, in some cases, replacing search engines for questions.","In this paper, we thus present a collection of 104 ontology design patterns representing often occurring nouns, curated from the common-sense knowledge available in LLMs, organized into a fully-annotated modular ontology design library ready for use with MOMo."],"url":"http://arxiv.org/abs/2402.18715v1","category":"cs.AI"}
{"created":"2024-02-28 20:41:21","title":"Learning to Compress Prompt in Natural Language Formats","abstract":"Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results. Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently. Existing works rely on compressing long prompt contexts into soft prompts. However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs. To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability. This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints. In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framework compressing original prompts into NL formatted Capsule Prompt while maintaining the prompt utility and transferability. Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss. To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints. Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of the original length, decrease inference latency up to 4.5x, and save 80.1% of budget overheads while providing transferability across diverse LLMs and different datasets.","sentences":["Large language models (LLMs) are great at processing multiple natural language processing tasks, but their abilities are constrained by inferior performance with long context, slow inference speed, and the high cost of computing the results.","Deploying LLMs with precise and informative context helps users process large-scale datasets more effectively and cost-efficiently.","Existing works rely on compressing long prompt contexts into soft prompts.","However, soft prompt compression encounters limitations in transferability across different LLMs, especially API-based LLMs.","To this end, this work aims to compress lengthy prompts in the form of natural language with LLM transferability.","This poses two challenges: (i) Natural Language (NL) prompts are incompatible with back-propagation, and (ii) NL prompts lack flexibility in imposing length constraints.","In this work, we propose a Natural Language Prompt Encapsulation (Nano-Capsulator) framework compressing original prompts into NL formatted Capsule Prompt while maintaining the prompt utility and transferability.","Specifically, to tackle the first challenge, the Nano-Capsulator is optimized by a reward function that interacts with the proposed semantics preserving loss.","To address the second question, the Nano-Capsulator is optimized by a reward function featuring length constraints.","Experimental results demonstrate that the Capsule Prompt can reduce 81.4% of the original length, decrease inference latency up to 4.5x, and save 80.1% of budget overheads while providing transferability across diverse LLMs and different datasets."],"url":"http://arxiv.org/abs/2402.18700v1","category":"cs.CL"}
{"created":"2024-02-28 20:02:35","title":"Integrated Sensing and Communication Meets Smart Propagation Engineering: Opportunities and Challenges","abstract":"Both smart propagation engineering as well as integrated sensing and communication (ISAC) constitute promising candidates for next-generation (NG) mobile networks. We provide a synergistic view of these technologies, and explore their mutual benefits. First, moving beyond just intelligent surfaces, we provide a holistic view of the engineering aspects of smart propagation environments. By delving into the fundamental characteristics of intelligent surfaces, fluid antennas, and unmanned aerial vehicles, we reveal that more efficient control of the pathloss and fading can be achieved, thus facilitating intrinsic integration and mutual assistance between sensing and communication functionalities. In turn, with the exploitation of the sensing capabilities of ISAC to orchestrate the efficient configuration of radio environments, both the computational effort and signaling overheads can be reduced. We present indicative simulation results, which verify that cooperative smart propagation environment design significantly enhances the ISAC performance. Finally, some promising directions are outlined for combining ISAC with smart propagation engineering.","sentences":["Both smart propagation engineering as well as integrated sensing and communication (ISAC) constitute promising candidates for next-generation (NG) mobile networks.","We provide a synergistic view of these technologies, and explore their mutual benefits.","First, moving beyond just intelligent surfaces, we provide a holistic view of the engineering aspects of smart propagation environments.","By delving into the fundamental characteristics of intelligent surfaces, fluid antennas, and unmanned aerial vehicles, we reveal that more efficient control of the pathloss and fading can be achieved, thus facilitating intrinsic integration and mutual assistance between sensing and communication functionalities.","In turn, with the exploitation of the sensing capabilities of ISAC to orchestrate the efficient configuration of radio environments, both the computational effort and signaling overheads can be reduced.","We present indicative simulation results, which verify that cooperative smart propagation environment design significantly enhances the ISAC performance.","Finally, some promising directions are outlined for combining ISAC with smart propagation engineering."],"url":"http://arxiv.org/abs/2402.18683v1","category":"cs.IT"}
{"created":"2024-02-28 19:49:55","title":"Data Interpreter: An LLM Agent For Data Science","abstract":"Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness. However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning. In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording. We evaluate the Data Interpreter on various data science and real-world tasks. Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95. Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks. The solution will be released at https://github.com/geekan/MetaGPT.","sentences":["Large Language Model (LLM)-based agents have demonstrated remarkable effectiveness.","However, their performance can be compromised in data science scenarios that require real-time data adjustment, expertise in optimization due to complex dependencies among various tasks, and the ability to identify logical errors for precise reasoning.","In this study, we introduce the Data Interpreter, a solution designed to solve with code that emphasizes three pivotal techniques to augment problem-solving in data science: 1) dynamic planning with hierarchical graph structures for real-time data adaptability;2) tool integration dynamically to enhance code proficiency during execution, enriching the requisite expertise;3) logical inconsistency identification in feedback, and efficiency enhancement through experience recording.","We evaluate the Data Interpreter on various data science and real-world tasks.","Compared to open-source baselines, it demonstrated superior performance, exhibiting significant improvements in machine learning tasks, increasing from 0.86 to 0.95.","Additionally, it showed a 26% increase in the MATH dataset and a remarkable 112% improvement in open-ended tasks.","The solution will be released at https://github.com/geekan/MetaGPT."],"url":"http://arxiv.org/abs/2402.18679v1","category":"cs.AI"}
{"created":"2024-02-28 19:44:19","title":"Fault Tolerant Neural Control Barrier Functions for Robotic Systems under Sensor Faults and Attacks","abstract":"Safety is a fundamental requirement of many robotic systems. Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems. However, the effectiveness of these approaches highly relies on the choice of CBFs. Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs). Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks. In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks. Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF). We derive the necessary and sufficient conditions for FT-NCBFs to guarantee safety, and develop a data-driven method to learn FT-NCBFs by minimizing a loss function constructed using the derived conditions. Using the learned FT-NCBF, we synthesize a control input and formally prove the safety guarantee provided by our approach. We demonstrate our proposed approach using two case studies: obstacle avoidance problem for an autonomous mobile robot and spacecraft rendezvous problem, with code available via https://github.com/HongchaoZhang-HZ/FTNCBF.","sentences":["Safety is a fundamental requirement of many robotic systems.","Control barrier function (CBF)-based approaches have been proposed to guarantee the safety of robotic systems.","However, the effectiveness of these approaches highly relies on the choice of CBFs.","Inspired by the universal approximation power of neural networks, there is a growing trend toward representing CBFs using neural networks, leading to the notion of neural CBFs (NCBFs).","Current NCBFs, however, are trained and deployed in benign environments, making them ineffective for scenarios where robotic systems experience sensor faults and attacks.","In this paper, we study safety-critical control synthesis for robotic systems under sensor faults and attacks.","Our main contribution is the development and synthesis of a new class of CBFs that we term fault tolerant neural control barrier function (FT-NCBF).","We derive the necessary and sufficient conditions for FT-NCBFs to guarantee safety, and develop a data-driven method to learn FT-NCBFs by minimizing a loss function constructed using the derived conditions.","Using the learned FT-NCBF, we synthesize a control input and formally prove the safety guarantee provided by our approach.","We demonstrate our proposed approach using two case studies: obstacle avoidance problem for an autonomous mobile robot and spacecraft rendezvous problem, with code available via https://github.com/HongchaoZhang-HZ/FTNCBF."],"url":"http://arxiv.org/abs/2402.18677v1","category":"cs.RO"}
{"created":"2024-02-28 19:39:27","title":"Robot Body Schema Learning from Full-body Extero/Proprioception Sensors","abstract":"For a robot, its body structure is an a-prior knowledge when it is designed. However, when such information is not available, can a robot recognize it by itself? In this paper, we aim to grant a robot such ability to learn its body structure from exteroception and proprioception data collected from on-body sensors. By a novel machine learning method, the robot can learn a binary Heterogeneous Dependency Matrix from its sensor readings. We showed such matrix is equivalent to a Heterogeneous out-tree structure which can uniquely represent the robot body topology. We explored the properties of such matrix and the out-tree, and proposed a remedy to fix them when they are contaminated by partial observability or data noise. We ran our algorithm on 6 different robots with different body structures in simulation and 1 real robot. Our algorithm correctly recognized their body structures with only on-body sensor readings but no topology prior knowledge.","sentences":["For a robot, its body structure is an a-prior knowledge when it is designed.","However, when such information is not available, can a robot recognize it by itself?","In this paper, we aim to grant a robot such ability to learn its body structure from exteroception and proprioception data collected from on-body sensors.","By a novel machine learning method, the robot can learn a binary Heterogeneous Dependency Matrix from its sensor readings.","We showed such matrix is equivalent to a Heterogeneous out-tree structure which can uniquely represent the robot body topology.","We explored the properties of such matrix and the out-tree, and proposed a remedy to fix them when they are contaminated by partial observability or data noise.","We ran our algorithm on 6 different robots with different body structures in simulation and 1 real robot.","Our algorithm correctly recognized their body structures with only on-body sensor readings but no topology prior knowledge."],"url":"http://arxiv.org/abs/2402.18675v1","category":"cs.RO"}
{"created":"2024-02-28 19:38:15","title":"QRIS: A Quantitative Reflectance Imaging System for the Pristine Sample of Asteroid Bennu","abstract":"The Quantitative Reflectance Imaging System (QRIS) is a laboratory-based spectral imaging system constructed to image the sample of asteroid Bennu delivered to Earth by the Origins, Spectral Interpretation, Resource Identification, and Security-Regolith Explorer (OSIRIS-REx) spacecraft. The system was installed in the OSIRIS-REx cleanroom at NASA's Johnson Space Center to collect data during preliminary examination of the Bennu sample. QRIS uses a 12-bit machine vision camera to measure reflectance over wavelength bands spanning the near ultraviolet to the near infrared. Raw data are processed by a calibration pipeline that generates a series of monochromatic, high-dynamic-range reflectance images, as well as band ratio maps, band depth maps, and 3-channel color images. The purpose of these spectral reflectance data is to help characterize lithologies in the sample and compare them to lithologies observed on Bennu by the OSIRIS-REx spacecraft. This initial assessment of lithological diversity was intended to help select the subsamples that will be used to address mission science questions about the early solar system and the origins of life and to provide important context for the selection of representative subsamples for preservation and distribution to international partners. When QRIS imaged the Bennu sample, unexpected calibration issues arose that had not been evident at imaging rehearsals and negatively impacted the quality of QRIS data. These issues were caused by stray light within the lens and reflections off the glovebox window and interior, and were exacerbated by the sample's extremely low reflectance. QRIS data were useful for confirming conclusions drawn from other data, but reflectance and spectral data from QRIS alone unfortunately have limited utility.","sentences":["The Quantitative Reflectance Imaging System (QRIS) is a laboratory-based spectral imaging system constructed to image the sample of asteroid Bennu delivered to Earth by the Origins, Spectral Interpretation, Resource Identification, and Security-Regolith Explorer (OSIRIS-REx) spacecraft.","The system was installed in the OSIRIS-REx cleanroom at NASA's Johnson Space Center to collect data during preliminary examination of the Bennu sample.","QRIS uses a 12-bit machine vision camera to measure reflectance over wavelength bands spanning the near ultraviolet to the near infrared.","Raw data are processed by a calibration pipeline that generates a series of monochromatic, high-dynamic-range reflectance images, as well as band ratio maps, band depth maps, and 3-channel color images.","The purpose of these spectral reflectance data is to help characterize lithologies in the sample and compare them to lithologies observed on Bennu by the OSIRIS-REx spacecraft.","This initial assessment of lithological diversity was intended to help select the subsamples that will be used to address mission science questions about the early solar system and the origins of life and to provide important context for the selection of representative subsamples for preservation and distribution to international partners.","When QRIS imaged the Bennu sample, unexpected calibration issues arose that had not been evident at imaging rehearsals and negatively impacted the quality of QRIS data.","These issues were caused by stray light within the lens and reflections off the glovebox window and interior, and were exacerbated by the sample's extremely low reflectance.","QRIS data were useful for confirming conclusions drawn from other data, but reflectance and spectral data from QRIS alone unfortunately have limited utility."],"url":"http://arxiv.org/abs/2402.18674v1","category":"astro-ph.EP"}
{"created":"2024-02-28 19:35:30","title":"Trends, Applications, and Challenges in Human Attention Modelling","abstract":"Human attention modelling has proven, in recent years, to be particularly useful not only for understanding the cognitive processes underlying visual exploration, but also for providing support to artificial intelligence models that aim to solve problems in various domains, including image and video processing, vision-and-language applications, and language modelling. This survey offers a reasoned overview of recent efforts to integrate human attention mechanisms into contemporary deep learning models and discusses future research directions and challenges. For a comprehensive overview on the ongoing research refer to our dedicated repository available at https://github.com/aimagelab/awesome-human-visual-attention.","sentences":["Human attention modelling has proven, in recent years, to be particularly useful not only for understanding the cognitive processes underlying visual exploration, but also for providing support to artificial intelligence models that aim to solve problems in various domains, including image and video processing, vision-and-language applications, and language modelling.","This survey offers a reasoned overview of recent efforts to integrate human attention mechanisms into contemporary deep learning models and discusses future research directions and challenges.","For a comprehensive overview on the ongoing research refer to our dedicated repository available at https://github.com/aimagelab/awesome-human-visual-attention."],"url":"http://arxiv.org/abs/2402.18673v1","category":"cs.CV"}
{"created":"2024-02-28 19:09:08","title":"Large Language Models and Games: A Survey and Roadmap","abstract":"Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.","sentences":["Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic.","While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games.","This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game.","Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain.","As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field."],"url":"http://arxiv.org/abs/2402.18659v1","category":"cs.CL"}
{"created":"2024-02-28 19:00:36","title":"Quantifying Human Priors over Social and Navigation Networks","abstract":"Human knowledge is largely implicit and relational -- do we have a friend in common? can I walk from here to there? In this work, we leverage the combinatorial structure of graphs to quantify human priors over such relational data. Our experiments focus on two domains that have been continuously relevant over evolutionary timescales: social interaction and spatial navigation. We find that some features of the inferred priors are remarkably consistent, such as the tendency for sparsity as a function of graph size. Other features are domain-specific, such as the propensity for triadic closure in social interactions. More broadly, our work demonstrates how nonclassical statistical analysis of indirect behavioral experiments can be used to efficiently model latent biases in the data.","sentences":["Human knowledge is largely implicit and relational -- do we have a friend in common?","can I walk from here to there?","In this work, we leverage the combinatorial structure of graphs to quantify human priors over such relational data.","Our experiments focus on two domains that have been continuously relevant over evolutionary timescales: social interaction and spatial navigation.","We find that some features of the inferred priors are remarkably consistent, such as the tendency for sparsity as a function of graph size.","Other features are domain-specific, such as the propensity for triadic closure in social interactions.","More broadly, our work demonstrates how nonclassical statistical analysis of indirect behavioral experiments can be used to efficiently model latent biases in the data."],"url":"http://arxiv.org/abs/2402.18651v1","category":"cs.LG"}
{"created":"2024-02-28 19:00:17","title":"The Grasp Reset Mechanism: An Automated Apparatus for Conducting Grasping Trials","abstract":"Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps. Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials. The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation. It also collects data and swaps between multiple objects enabling robust dataset collection with no human intervention. We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort. In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps. The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM. The dataset includes ranges of grasps conducted across four objects and a variety of orientations. Manipulator states, object pose, video, and grasp success data are provided for every trial.","sentences":["Advancing robotic grasping and manipulation requires the ability to test algorithms and/or train learning models on large numbers of grasps.","Towards the goal of more advanced grasping, we present the Grasp Reset Mechanism (GRM), a fully automated apparatus for conducting large-scale grasping trials.","The GRM automates the process of resetting a grasping environment, repeatably placing an object in a fixed location and controllable 1-D orientation.","It also collects data and swaps between multiple objects enabling robust dataset collection with no human intervention.","We also present a standardized state machine interface for control, which allows for integration of most manipulators with minimal effort.","In addition to the physical design and corresponding software, we include a dataset of 1,020 grasps.","The grasps were created with a Kinova Gen3 robot arm and Robotiq 2F-85 Adaptive Gripper to enable training of learning models and to demonstrate the capabilities of the GRM.","The dataset includes ranges of grasps conducted across four objects and a variety of orientations.","Manipulator states, object pose, video, and grasp success data are provided for every trial."],"url":"http://arxiv.org/abs/2402.18650v1","category":"cs.RO"}
{"created":"2024-02-28 19:00:12","title":"A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems","abstract":"Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on. Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems. However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on). In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs. To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects. Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints. To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4. Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components. We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers. To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user's chat history, all without the need to manipulate the user's input or gain direct access to OpenAI GPT4. Our demo is in the link: https://fzwark.github.io/LLM-System-Attack-Demo/","sentences":["Large Language Model (LLM) systems are inherently compositional, with individual LLM serving as the core foundation with additional layers of objects such as plugins, sandbox, and so on.","Along with the great potential, there are also increasing concerns over the security of such probabilistic intelligent systems.","However, existing studies on LLM security often focus on individual LLM, but without examining the ecosystem through the lens of LLM systems with other objects (e.g., Frontend, Webtool, Sandbox, and so on).","In this paper, we systematically analyze the security of LLM systems, instead of focusing on the individual LLMs.","To do so, we build on top of the information flow and formulate the security of LLM systems as constraints on the alignment of the information flow within LLM and between LLM and other objects.","Based on this construction and the unique probabilistic nature of LLM, the attack surface of the LLM system can be decomposed into three key components: (1) multi-layer security analysis, (2) analysis of the existence of constraints, and (3) analysis of the robustness of these constraints.","To ground this new attack surface, we propose a multi-layer and multi-step approach and apply it to the state-of-art LLM system, OpenAI GPT4.","Our investigation exposes several security issues, not just within the LLM model itself but also in its integration with other components.","We found that although the OpenAI GPT4 has designed numerous safety constraints to improve its safety features, these safety constraints are still vulnerable to attackers.","To further demonstrate the real-world threats of our discovered vulnerabilities, we construct an end-to-end attack where an adversary can illicitly acquire the user's chat history, all without the need to manipulate the user's input or gain direct access to OpenAI GPT4.","Our demo is in the link: https://fzwark.github.io/LLM-System-Attack-Demo/"],"url":"http://arxiv.org/abs/2402.18649v1","category":"cs.CR"}
{"created":"2024-02-28 19:00:09","title":"Remarks on Geometric Engineering, Symmetry TFTs and Anomalies","abstract":"Geometric engineering is a collection of tools developed to establish dictionaries between local singularities in string theory and (supersymmetric) quantum fields. Extended operators and defects, as well as their higher quantum numbers captured by topological symmetries, can be encoded within geometric engineering dictionaries. In this paper we revisit and clarify aspects of these techniques, with special emphasis on 't Hooft anomalies, interpreted from the SymTFT perspective as obstructions to the existence of Neumann boundary conditions. These obstructions to gauging higher symmetries are captured via higher link correlators for the SymTFT on spheres. In this work, we give the geometric engineering counterpart of this construction in terms of higher links of topological membranes. We provide a consistency check in the context of 5D SCFTs with anomalous 1-form symmetries, where we give two independent derivations of the anomaly in terms of higher links, one purely field theoretical and the other purely geometrical. Along the way, we also recover the construction of non-invertible duality defects in 4D $\\mathcal N=4$ SYM from a geometric engineering perspective.","sentences":["Geometric engineering is a collection of tools developed to establish dictionaries between local singularities in string theory and (supersymmetric) quantum fields.","Extended operators and defects, as well as their higher quantum numbers captured by topological symmetries, can be encoded within geometric engineering dictionaries.","In this paper we revisit and clarify aspects of these techniques, with special emphasis on 't Hooft anomalies, interpreted from the SymTFT perspective as obstructions to the existence of Neumann boundary conditions.","These obstructions to gauging higher symmetries are captured via higher link correlators for the SymTFT on spheres.","In this work, we give the geometric engineering counterpart of this construction in terms of higher links of topological membranes.","We provide a consistency check in the context of 5D SCFTs with anomalous 1-form symmetries, where we give two independent derivations of the anomaly in terms of higher links, one purely field theoretical and the other purely geometrical.","Along the way, we also recover the construction of non-invertible duality defects in 4D $\\mathcal N=4$ SYM from a geometric engineering perspective."],"url":"http://arxiv.org/abs/2402.18646v1","category":"hep-th"}
{"created":"2024-02-28 19:00:01","title":"GNSS Positioning using Cost Function Regulated Multilateration and Graph Neural Networks","abstract":"In urban environments, where line-of-sight signals from GNSS satellites are frequently blocked by high-rise objects, GNSS receivers are subject to large errors in measuring satellite ranges. Heuristic methods are commonly used to estimate these errors and reduce the impact of noisy measurements on localization accuracy. In our work, we replace these error estimation heuristics with a deep learning model based on Graph Neural Networks. Additionally, by analyzing the cost function of the multilateration process, we derive an optimal method to utilize the estimated errors. Our approach guarantees that the multilateration converges to the receiver's location as the error estimation accuracy increases. We evaluate our solution on a real-world dataset containing more than 100k GNSS epochs, collected from multiple cities with diverse characteristics. The empirical results show improvements from 40% to 80% in the horizontal localization error against recent deep learning baselines as well as classical localization approaches.","sentences":["In urban environments, where line-of-sight signals from GNSS satellites are frequently blocked by high-rise objects, GNSS receivers are subject to large errors in measuring satellite ranges.","Heuristic methods are commonly used to estimate these errors and reduce the impact of noisy measurements on localization accuracy.","In our work, we replace these error estimation heuristics with a deep learning model based on Graph Neural Networks.","Additionally, by analyzing the cost function of the multilateration process, we derive an optimal method to utilize the estimated errors.","Our approach guarantees that the multilateration converges to the receiver's location as the error estimation accuracy increases.","We evaluate our solution on a real-world dataset containing more than 100k GNSS epochs, collected from multiple cities with diverse characteristics.","The empirical results show improvements from 40% to 80% in the horizontal localization error against recent deep learning baselines as well as classical localization approaches."],"url":"http://arxiv.org/abs/2402.18630v1","category":"cs.LG"}
{"created":"2024-02-28 19:00:01","title":"The Collective Coordinate Fix","abstract":"Collective coordinates are frequently employed in path integrals to manage divergences caused by fluctuations around saddle points that align with classical symmetries. These coordinates parameterize a manifold of zero modes and more broadly provide judicious coordinates on the space of fields. However, changing from local coordinates around a saddle point to more global collective coordinates is remarkably subtle. The main complication is that the mapping from local coordinates to collective coordinates is generically multi-valued. Consequently one is forced to either restrict the domain of path integral in a delicate way, or otherwise correct for the multi-valuedness by dividing the path integral by certain intersection numbers. We provide a careful treatment of how to fix collective coordinates while accounting for these intersection numbers, and then demonstrate the importance of the fix for free theories. We also provide a detailed study of the fix for interacting theories and show that the contributions of higher intersections to the path integral can be non-perturbatively suppressed. Using a variety of examples ranging from single-particle quantum mechanics to quantum field theory, we explain and resolve various pitfalls in the implementation of collective coordinates.","sentences":["Collective coordinates are frequently employed in path integrals to manage divergences caused by fluctuations around saddle points that align with classical symmetries.","These coordinates parameterize a manifold of zero modes and more broadly provide judicious coordinates on the space of fields.","However, changing from local coordinates around a saddle point to more global collective coordinates is remarkably subtle.","The main complication is that the mapping from local coordinates to collective coordinates is generically multi-valued.","Consequently one is forced to either restrict the domain of path integral in a delicate way, or otherwise correct for the multi-valuedness by dividing the path integral by certain intersection numbers.","We provide a careful treatment of how to fix collective coordinates while accounting for these intersection numbers, and then demonstrate the importance of the fix for free theories.","We also provide a detailed study of the fix for interacting theories and show that the contributions of higher intersections to the path integral can be non-perturbatively suppressed.","Using a variety of examples ranging from single-particle quantum mechanics to quantum field theory, we explain and resolve various pitfalls in the implementation of collective coordinates."],"url":"http://arxiv.org/abs/2402.18633v1","category":"hep-th"}
{"created":"2024-02-28 18:13:20","title":"Urban Green Index estimation based on data collected by remote sensing for Romanian cities","abstract":"The modernization of offi cial statistics involves the use of new data sources, such as data collected through remote sensing. The document contains a description of how an urban green index, derived from the SDG 11.7 objective, was obtained for Romania's 41 county seat cities based on free data sets collected by remote sensing from the European and North American space agencies. The main result is represented by an estimate of the areas of surfaces covered with vegetation for the 40 county seat towns and the municipality of Bucharest, relative to the total surface. To estimate the area covered with vegetation, we used two data sets obtained by remote sensing, namely data provided by the MODIS mission, the TERRA satellite, and data provided by the Sentinel 2 mission from the Copernicus space program. Based on the results obtained, namely the surface area covered with vegetation, estimated in square kilometers, and the percentage of the total surface area or urban green index, we have created a national top of the county seat cities","sentences":["The modernization of offi cial statistics involves the use of new data sources, such as data collected through remote sensing.","The document contains a description of how an urban green index, derived from the SDG 11.7 objective, was obtained for Romania's 41 county seat cities based on free data sets collected by remote sensing from the European and North American space agencies.","The main result is represented by an estimate of the areas of surfaces covered with vegetation for the 40 county seat towns and the municipality of Bucharest, relative to the total surface.","To estimate the area covered with vegetation, we used two data sets obtained by remote sensing, namely data provided by the MODIS mission, the TERRA satellite, and data provided by the Sentinel 2 mission from the Copernicus space program.","Based on the results obtained, namely the surface area covered with vegetation, estimated in square kilometers, and the percentage of the total surface area or urban green index, we have created a national top of the county seat cities"],"url":"http://arxiv.org/abs/2402.18618v1","category":"cs.OH"}
{"created":"2024-02-28 17:44:02","title":"ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games","abstract":"Offline learning has become widely used due to its ability to derive effective policies from offline datasets gathered by expert demonstrators without interacting with the environment directly. Recent research has explored various ways to enhance offline learning efficiency by considering the characteristics (e.g., expertise level or multiple demonstrators) of the dataset. However, a different approach is necessary in the context of zero-sum games, where outcomes vary significantly based on the strategy of the opponent. In this study, we introduce a novel approach that uses unsupervised learning techniques to estimate the exploited level of each trajectory from the offline dataset of zero-sum games made by diverse demonstrators. Subsequently, we incorporate the estimated exploited level into the offline learning to maximize the influence of the dominant strategy. Our method enables interpretable exploited level estimation in multiple zero-sum games and effectively identifies dominant strategy data. Also, our exploited level augmented offline learning significantly enhances the original offline learning algorithms including imitation learning and offline reinforcement learning for zero-sum games.","sentences":["Offline learning has become widely used due to its ability to derive effective policies from offline datasets gathered by expert demonstrators without interacting with the environment directly.","Recent research has explored various ways to enhance offline learning efficiency by considering the characteristics (e.g., expertise level or multiple demonstrators) of the dataset.","However, a different approach is necessary in the context of zero-sum games, where outcomes vary significantly based on the strategy of the opponent.","In this study, we introduce a novel approach that uses unsupervised learning techniques to estimate the exploited level of each trajectory from the offline dataset of zero-sum games made by diverse demonstrators.","Subsequently, we incorporate the estimated exploited level into the offline learning to maximize the influence of the dominant strategy.","Our method enables interpretable exploited level estimation in multiple zero-sum games and effectively identifies dominant strategy data.","Also, our exploited level augmented offline learning significantly enhances the original offline learning algorithms including imitation learning and offline reinforcement learning for zero-sum games."],"url":"http://arxiv.org/abs/2402.18617v1","category":"cs.GT"}
{"created":"2024-02-28 17:38:01","title":"JCLEC-MO: a Java suite for solving many-objective optimization engineering problems","abstract":"Although metaheuristics have been widely recognized as efficient techniques to solve real-world optimization problems, implementing them from scratch remains difficult for domain-specific experts without programming skills. In this scenario, metaheuristic optimization frameworks are a practical alternative as they provide a variety of algorithms composed of customized elements, as well as experimental support. Recently, many engineering problems require to optimize multiple or even many objectives, increasing the interest in appropriate metaheuristic algorithms and frameworks that might integrate new specific requirements while maintaining the generality and reusability principles they were conceived for. Based on this idea, this paper introduces JCLEC-MO, a Java framework for both multi- and many-objective optimization that enables engineers to apply, or adapt, a great number of multi-objective algorithms with little coding effort. A case study is developed and explained to show how JCLEC-MO can be used to address many-objective engineering problems, often requiring the inclusion of domain-specific elements, and to analyze experimental outcomes by means of conveniently connected R utilities.","sentences":["Although metaheuristics have been widely recognized as efficient techniques to solve real-world optimization problems, implementing them from scratch remains difficult for domain-specific experts without programming skills.","In this scenario, metaheuristic optimization frameworks are a practical alternative as they provide a variety of algorithms composed of customized elements, as well as experimental support.","Recently, many engineering problems require to optimize multiple or even many objectives, increasing the interest in appropriate metaheuristic algorithms and frameworks that might integrate new specific requirements while maintaining the generality and reusability principles they were conceived for.","Based on this idea, this paper introduces JCLEC-MO, a Java framework for both multi- and many-objective optimization that enables engineers to apply, or adapt, a great number of multi-objective algorithms with little coding effort.","A case study is developed and explained to show how JCLEC-MO can be used to address many-objective engineering problems, often requiring the inclusion of domain-specific elements, and to analyze experimental outcomes by means of conveniently connected R utilities."],"url":"http://arxiv.org/abs/2402.18616v1","category":"cs.NE"}
{"created":"2024-02-28 17:06:54","title":"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist","abstract":"Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets. While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks. To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading. FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market. Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes. The agent's emphasis on reasoning for actions fosters trust in its financial decisions. Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles. With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit. Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset. Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks.","sentences":["Financial trading is a crucial component of the markets, informed by a multimodal information landscape encompassing news, prices, and Kline charts, and encompasses diverse tasks such as quantitative trading and high-frequency trading with various assets.","While advanced AI techniques like deep learning and reinforcement learning are extensively utilized in finance, their application in financial trading tasks often faces challenges due to inadequate handling of multimodal data and limited generalizability across various tasks.","To address these challenges, we present FinAgent, a multimodal foundational agent with tool augmentation for financial trading.","FinAgent's market intelligence module processes a diverse range of data-numerical, textual, and visual-to accurately analyze the financial market.","Its unique dual-level reflection module not only enables rapid adaptation to market dynamics but also incorporates a diversified memory retrieval system, enhancing the agent's ability to learn from historical data and improve decision-making processes.","The agent's emphasis on reasoning for actions fosters trust in its financial decisions.","Moreover, FinAgent integrates established trading strategies and expert insights, ensuring that its trading approaches are both data-driven and rooted in sound financial principles.","With comprehensive experiments on 6 financial datasets, including stocks and Crypto, FinAgent significantly outperforms 9 state-of-the-art baselines in terms of 6 financial metrics with over 36% average improvement on profit.","Specifically, a 92.27% return (a 84.39% relative improvement) is achieved on one dataset.","Notably, FinAgent is the first advanced multimodal foundation agent designed for financial trading tasks."],"url":"http://arxiv.org/abs/2402.18485v2","category":"q-fin.TR"}
{"created":"2024-02-28 15:52:30","title":"Deep Neural Network Models Trained With A Fixed Random Classifier Transfer Better Across Domains","abstract":"The recently discovered Neural collapse (NC) phenomenon states that the last-layer weights of Deep Neural Networks (DNN), converge to the so-called Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training. This ETF geometry is equivalent to vanishing within-class variability of the last layer activations. Inspired by NC properties, we explore in this paper the transferability of DNN models trained with their last layer weight fixed according to ETF. This enforces class separation by eliminating class covariance information, effectively providing implicit regularization. We show that DNN models trained with such a fixed classifier significantly improve transfer performance, particularly on out-of-domain datasets. On a broad range of fine-grained image classification datasets, our approach outperforms i) baseline methods that do not perform any covariance regularization (up to 22%), as well as ii) methods that explicitly whiten covariance of activations throughout training (up to 19%). Our findings suggest that DNNs trained with fixed ETF classifiers offer a powerful mechanism for improving transfer learning across domains.","sentences":["The recently discovered Neural collapse (NC) phenomenon states that the last-layer weights of Deep Neural Networks (DNN), converge to the so-called Equiangular Tight Frame (ETF) simplex, at the terminal phase of their training.","This ETF geometry is equivalent to vanishing within-class variability of the last layer activations.","Inspired by NC properties, we explore in this paper the transferability of DNN models trained with their last layer weight fixed according to ETF.","This enforces class separation by eliminating class covariance information, effectively providing implicit regularization.","We show that DNN models trained with such a fixed classifier significantly improve transfer performance, particularly on out-of-domain datasets.","On a broad range of fine-grained image classification datasets, our approach outperforms i) baseline methods that do not perform any covariance regularization (up to 22%), as well as ii) methods that explicitly whiten covariance of activations throughout training (up to 19%).","Our findings suggest that DNNs trained with fixed ETF classifiers offer a powerful mechanism for improving transfer learning across domains."],"url":"http://arxiv.org/abs/2402.18614v1","category":"cs.LG"}
{"created":"2024-02-28 15:06:25","title":"ICE-SEARCH: A Language Model-Driven Feature Selection Approach","abstract":"This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate the efficacy of ICE-SEARCH in medical FS but also underscore the versatility, efficiency, and scalability of integrating LMs in FS tasks. The study emphasizes the critical role of incorporating domain-specific insights, illustrating ICE-SEARCH's robustness, generalizability, and swift convergence. This opens avenues for further research into comprehensive and intricate FS landscapes, marking a significant stride in the application of artificial intelligence in medical predictive analytics.","sentences":["This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications.","ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles.","Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications.","ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction.","Our results not only demonstrate the efficacy of ICE-SEARCH in medical FS but also underscore the versatility, efficiency, and scalability of integrating LMs in FS tasks.","The study emphasizes the critical role of incorporating domain-specific insights, illustrating ICE-SEARCH's robustness, generalizability, and swift convergence.","This opens avenues for further research into comprehensive and intricate FS landscapes, marking a significant stride in the application of artificial intelligence in medical predictive analytics."],"url":"http://arxiv.org/abs/2402.18609v1","category":"cs.LG"}
{"created":"2024-02-28 12:21:12","title":"Exploring Privacy and Fairness Risks in Sharing Diffusion Models: An Adversarial Perspective","abstract":"Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage. Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly. However, the potential risks associated with such an approach have not been comprehensively examined.   In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models. Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks. We demonstrate that the sharer can execute fairness poisoning attacks to undermine the receiver's downstream models by manipulating the training data distribution of the diffusion model. Meanwhile, the receiver can perform property inference attacks to reveal the distribution of sensitive features in the sharer's dataset. Our experiments conducted on real-world datasets demonstrate remarkable attack performance on different types of diffusion models, which highlights the critical importance of robust data auditing and privacy protection protocols in pertinent applications.","sentences":["Diffusion models have recently gained significant attention in both academia and industry due to their impressive generative performance in terms of both sampling quality and distribution coverage.","Accordingly, proposals are made for sharing pre-trained diffusion models across different organizations, as a way of improving data utilization while enhancing privacy protection by avoiding sharing private data directly.","However, the potential risks associated with such an approach have not been comprehensively examined.   ","In this paper, we take an adversarial perspective to investigate the potential privacy and fairness risks associated with the sharing of diffusion models.","Specifically, we investigate the circumstances in which one party (the sharer) trains a diffusion model using private data and provides another party (the receiver) black-box access to the pre-trained model for downstream tasks.","We demonstrate that the sharer can execute fairness poisoning attacks to undermine the receiver's downstream models by manipulating the training data distribution of the diffusion model.","Meanwhile, the receiver can perform property inference attacks to reveal the distribution of sensitive features in the sharer's dataset.","Our experiments conducted on real-world datasets demonstrate remarkable attack performance on different types of diffusion models, which highlights the critical importance of robust data auditing and privacy protection protocols in pertinent applications."],"url":"http://arxiv.org/abs/2402.18607v1","category":"cs.LG"}
{"created":"2024-02-28 11:13:53","title":"Impact of network topology on the performance of Decentralized Federated Learning","abstract":"Fully decentralized learning is gaining momentum for training AI models at the Internet's edge, addressing infrastructure challenges and privacy concerns. In a decentralized machine learning system, data is distributed across multiple nodes, with each node training a local model based on its respective dataset. The local models are then shared and combined to form a global model capable of making accurate predictions on new data. Our exploration focuses on how different types of network structures influence the spreading of knowledge - the process by which nodes incorporate insights gained from learning patterns in data available on other nodes across the network. Specifically, this study investigates the intricate interplay between network structure and learning performance using three network topologies and six data distribution methods. These methods consider different vertex properties, including degree centrality, betweenness centrality, and clustering coefficient, along with whether nodes exhibit high or low values of these metrics. Our findings underscore the significance of global centrality metrics (degree, betweenness) in correlating with learning performance, while local clustering proves less predictive. We highlight the challenges in transferring knowledge from peripheral to central nodes, attributed to a dilution effect during model aggregation. Additionally, we observe that central nodes exert a pull effect, facilitating the spread of knowledge. In examining degree distribution, hubs in Barabasi-Albert networks positively impact learning for central nodes but exacerbate dilution when knowledge originates from peripheral nodes. Finally, we demonstrate the formidable challenge of knowledge circulation outside of segregated communities.","sentences":["Fully decentralized learning is gaining momentum for training AI models at the Internet's edge, addressing infrastructure challenges and privacy concerns.","In a decentralized machine learning system, data is distributed across multiple nodes, with each node training a local model based on its respective dataset.","The local models are then shared and combined to form a global model capable of making accurate predictions on new data.","Our exploration focuses on how different types of network structures influence the spreading of knowledge - the process by which nodes incorporate insights gained from learning patterns in data available on other nodes across the network.","Specifically, this study investigates the intricate interplay between network structure and learning performance using three network topologies and six data distribution methods.","These methods consider different vertex properties, including degree centrality, betweenness centrality, and clustering coefficient, along with whether nodes exhibit high or low values of these metrics.","Our findings underscore the significance of global centrality metrics (degree, betweenness) in correlating with learning performance, while local clustering proves less predictive.","We highlight the challenges in transferring knowledge from peripheral to central nodes, attributed to a dilution effect during model aggregation.","Additionally, we observe that central nodes exert a pull effect, facilitating the spread of knowledge.","In examining degree distribution, hubs in Barabasi-Albert networks positively impact learning for central nodes but exacerbate dilution when knowledge originates from peripheral nodes.","Finally, we demonstrate the formidable challenge of knowledge circulation outside of segregated communities."],"url":"http://arxiv.org/abs/2402.18606v1","category":"cs.LG"}
{"created":"2024-02-29 18:59:57","title":"Single Electron Quantum Dot in Two-Dimensional Transition Metal Dichalcogenides","abstract":"Spin-valley properties in two-dimensional (2D) semiconducting transition metal dichalcogenides (TMDC) has attracted significant interest due to the possible applications in quantum computing. Spin-valley properties can be exploited in TMDC quantum dot (QD) with well-resolved energy levels. This requires smaller QDs, especially in material systems with heavy carrier effective mass e.g. TMDCs and silicon. Device architectures employed for TMDC QDs so far have difficulty achieving smaller QDs. Therefore, an alternative approach in the device architecture is needed. Here, we propose a multilayer device architecture to achieve a gate-defined QD in TMDC with a relatively large energy splitting on the QD. We provide a range of device dimensions and dielectric thicknesses and its correlation with the QD energy splitting. The device architecture is modeled realistically. Moreover, we show that all the device parameters used in modeling are experimentally achievable. These studies lay the foundation for future work toward spin-valley qubits in TMDCs. The successful implementation of these device architectures will drive the technological development of 2D materials-based quantum technologies.","sentences":["Spin-valley properties in two-dimensional (2D) semiconducting transition metal dichalcogenides (TMDC) has attracted significant interest due to the possible applications in quantum computing.","Spin-valley properties can be exploited in TMDC quantum dot (QD) with well-resolved energy levels.","This requires smaller QDs, especially in material systems with heavy carrier effective mass e.g. TMDCs and silicon.","Device architectures employed for TMDC QDs so far have difficulty achieving smaller QDs.","Therefore, an alternative approach in the device architecture is needed.","Here, we propose a multilayer device architecture to achieve a gate-defined QD in TMDC with a relatively large energy splitting on the QD.","We provide a range of device dimensions and dielectric thicknesses and its correlation with the QD energy splitting.","The device architecture is modeled realistically.","Moreover, we show that all the device parameters used in modeling are experimentally achievable.","These studies lay the foundation for future work toward spin-valley qubits in TMDCs.","The successful implementation of these device architectures will drive the technological development of 2D materials-based quantum technologies."],"url":"http://arxiv.org/abs/2402.19480v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 18:54:46","title":"Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing","abstract":"We present a natural language processing pipeline that was used to extract polymer solar cell property data from the literature and simulate various active learning strategies. While data-driven methods have been well established to discover novel materials faster than Edisonian trial-and-error approaches, their benefits have not been quantified. Our approach demonstrates a potential reduction in discovery time by approximately 75 %, equivalent to a 15 year acceleration in material innovation. Our pipeline enables us to extract data from more than 3300 papers which is ~5 times larger than similar data sets reported by others. We also trained machine learning models to predict the power conversion efficiency and used our model to identify promising donor-acceptor combinations that are as yet unreported. We thus demonstrate a workflow that goes from published literature to extracted material property data which in turn is used to obtain data-driven insights. Our insights include active learning strategies that can simultaneously optimize the material system and train strong predictive models of material properties. This work provides a valuable framework for research in material science.","sentences":["We present a natural language processing pipeline that was used to extract polymer solar cell property data from the literature and simulate various active learning strategies.","While data-driven methods have been well established to discover novel materials faster than Edisonian trial-and-error approaches, their benefits have not been quantified.","Our approach demonstrates a potential reduction in discovery time by approximately 75 %, equivalent to a 15 year acceleration in material innovation.","Our pipeline enables us to extract data from more than 3300 papers which is ~5 times larger than similar data sets reported by others.","We also trained machine learning models to predict the power conversion efficiency and used our model to identify promising donor-acceptor combinations that are as yet unreported.","We thus demonstrate a workflow that goes from published literature to extracted material property data which in turn is used to obtain data-driven insights.","Our insights include active learning strategies that can simultaneously optimize the material system and train strong predictive models of material properties.","This work provides a valuable framework for research in material science."],"url":"http://arxiv.org/abs/2402.19462v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 18:49:46","title":"Controllable suppression of the unconventional superconductivity in bulk and thin-film Sr$_{2}$RuO$_{4}$ via high-energy electron irradiation","abstract":"In bulk Sr$_{2}$RuO$_{4}$, the strong sensitivity of the superconducting transition temperature $T_{\\text{c}}$ to nonmagnetic impurities provides robust evidence for a superconducting order parameter that changes sign around the Fermi surface. In superconducting epitaxial thin-film Sr$_{2}$RuO$_{4}$, the relationship between $T_{\\text{c}}$ and the residual resistivity $\\rho_0$, which in bulk samples is taken to be a proxy for the low-temperature elastic scattering rate, is far less clear. Using high-energy electron irradiation to controllably introduce point disorder into bulk single-crystal and thin-film Sr$_{2}$RuO$_{4}$, we show that $T_{\\text{c}}$ is suppressed in both systems at nearly identical rates. This suggests that part of $\\rho_0$ in films comes from defects that do not contribute to superconducting pairbreaking, and establishes a quantitative link between the superconductivity of bulk and thin-film samples.","sentences":["In bulk Sr$_{2}$RuO$_{4}$, the strong sensitivity of the superconducting transition temperature $T_{\\text{c}}$ to nonmagnetic impurities provides robust evidence for a superconducting order parameter that changes sign around the Fermi surface.","In superconducting epitaxial thin-film Sr$_{2}$RuO$_{4}$, the relationship between $T_{\\text{c}}$ and the residual resistivity $\\rho_0$, which in bulk samples is taken to be a proxy for the low-temperature elastic scattering rate, is far less clear.","Using high-energy electron irradiation to controllably introduce point disorder into bulk single-crystal and thin-film Sr$_{2}$RuO$_{4}$, we show that $T_{\\text{c}}$ is suppressed in both systems at nearly identical rates.","This suggests that part of $\\rho_0$ in films comes from defects that do not contribute to superconducting pairbreaking, and establishes a quantitative link between the superconductivity of bulk and thin-film samples."],"url":"http://arxiv.org/abs/2402.19454v1","category":"cond-mat.supr-con"}
{"created":"2024-02-29 18:48:54","title":"Hyperuniformity in deterministic anti-aligning active matter","abstract":"We highlight the importance of long-range correlations in active matter systems of dilute self-propelling particles even in the absence of global order by demonstrating that long-range density fluctuations are suppressed. We show this analytically for a one-dimensional lattice process employing Poisson representation. Numerically, we corroborate the relevance of these findings for off-lattice Vicsek-type models with anti-aligning interactions.","sentences":["We highlight the importance of long-range correlations in active matter systems of dilute self-propelling particles even in the absence of global order by demonstrating that long-range density fluctuations are suppressed.","We show this analytically for a one-dimensional lattice process employing Poisson representation.","Numerically, we corroborate the relevance of these findings for off-lattice Vicsek-type models with anti-aligning interactions."],"url":"http://arxiv.org/abs/2402.19451v1","category":"cond-mat.soft"}
{"created":"2024-02-29 18:23:26","title":"L\u00e9vy noise-induced coherence resonance in the FitzHugh-Nagumo oscillator: numerical study versus experiment","abstract":"Using a model of the FitzHugh-Nagumo oscillator in the excitable regime, we investigate the influence of the L\\'evy noise's properties on the effect of coherence resonance. In particular, we demonstrate that the L\\'evy noise can be a constructive or destructive factor providing for enhancement or suppression of noise-induced coherence. We show that the positive or negative role of the L\\'evy noise impact is dictated by the noise's stability index and skewness parameter. The correlation time and the deviation of interspike intervals used in this analysis are shown to be maximized or minimized for an appropriate choice of the noise parameters. Numerical simulations are combined with experiments on an electronic circuit showing an excellent qualitative correspondence and proving thereby the robustness of the observed phenomena.","sentences":["Using a model of the FitzHugh-Nagumo oscillator in the excitable regime, we investigate the influence of the L\\'evy noise's properties on the effect of coherence resonance.","In particular, we demonstrate that the L\\'evy noise can be a constructive or destructive factor providing for enhancement or suppression of noise-induced coherence.","We show that the positive or negative role of the L\\'evy noise impact is dictated by the noise's stability index and skewness parameter.","The correlation time and the deviation of interspike intervals used in this analysis are shown to be maximized or minimized for an appropriate choice of the noise parameters.","Numerical simulations are combined with experiments on an electronic circuit showing an excellent qualitative correspondence and proving thereby the robustness of the observed phenomena."],"url":"http://arxiv.org/abs/2402.19426v1","category":"nlin.AO"}
{"created":"2024-02-29 18:11:25","title":"Higher-Order Networks Representation and Learning: A Survey","abstract":"Network data has become widespread, larger, and more complex over the years. Traditional network data is dyadic, capturing the relations among pairs of entities. With the need to model interactions among more than two entities, significant research has focused on higher-order networks and ways to represent, analyze, and learn from them. There are two main directions to studying higher-order networks. One direction has focused on capturing higher-order patterns in traditional (dyadic) graphs by changing the basic unit of study from nodes to small frequently observed subgraphs, called motifs. As most existing network data comes in the form of pairwise dyadic relationships, studying higher-order structures within such graphs may uncover new insights. The second direction aims to directly model higher-order interactions using new and more complex representations such as simplicial complexes or hypergraphs. Some of these models have long been proposed, but improvements in computational power and the advent of new computational techniques have increased their popularity. Our goal in this paper is to provide a succinct yet comprehensive summary of the advanced higher-order network analysis techniques. We provide a systematic review of its foundations and algorithms, along with use cases and applications of higher-order networks in various scientific domains.","sentences":["Network data has become widespread, larger, and more complex over the years.","Traditional network data is dyadic, capturing the relations among pairs of entities.","With the need to model interactions among more than two entities, significant research has focused on higher-order networks and ways to represent, analyze, and learn from them.","There are two main directions to studying higher-order networks.","One direction has focused on capturing higher-order patterns in traditional (dyadic) graphs by changing the basic unit of study from nodes to small frequently observed subgraphs, called motifs.","As most existing network data comes in the form of pairwise dyadic relationships, studying higher-order structures within such graphs may uncover new insights.","The second direction aims to directly model higher-order interactions using new and more complex representations such as simplicial complexes or hypergraphs.","Some of these models have long been proposed, but improvements in computational power and the advent of new computational techniques have increased their popularity.","Our goal in this paper is to provide a succinct yet comprehensive summary of the advanced higher-order network analysis techniques.","We provide a systematic review of its foundations and algorithms, along with use cases and applications of higher-order networks in various scientific domains."],"url":"http://arxiv.org/abs/2402.19414v1","category":"cs.SI"}
{"created":"2024-02-29 18:10:05","title":"Does the system entanglement care about the readout efficiency of quantum measurement?","abstract":"Monitored quantum systems evolve along stochastic trajectories correlated with the observer's knowledge of the system's state. Under such dynamics, certain quantum resources like entanglement may depend on the observer's state of knowledge. Here, we quantify the entanglement for a particle on a 1d quantum random walk under inefficient monitoring using a mixed state-entanglement measure -- the configuration coherence. We find that the system's maximal mean entanglement at the measurement-induced quantum-to-classical crossover is suppressed in different ways by the measurement strength and inefficiency. In principle, strong measurements can lower the amount of entanglement indefinitely. However, at a given measurement strength, efficient readout can crucially increase the system entanglement, making high-fidelity detectors essential for successful quantum computing.","sentences":["Monitored quantum systems evolve along stochastic trajectories correlated with the observer's knowledge of the system's state.","Under such dynamics, certain quantum resources like entanglement may depend on the observer's state of knowledge.","Here, we quantify the entanglement for a particle on a 1d quantum random walk under inefficient monitoring using a mixed state-entanglement measure -- the configuration coherence.","We find that the system's maximal mean entanglement at the measurement-induced quantum-to-classical crossover is suppressed in different ways by the measurement strength and inefficiency.","In principle, strong measurements can lower the amount of entanglement indefinitely.","However, at a given measurement strength, efficient readout can crucially increase the system entanglement, making high-fidelity detectors essential for successful quantum computing."],"url":"http://arxiv.org/abs/2402.19412v1","category":"quant-ph"}
{"created":"2024-02-29 17:54:42","title":"Exploring Eco-Friendly Gas Mixtures for Resistive Plate Chambers: A Comprehensive Study on Performance and Aging","abstract":"Resistive Plate Chambers (RPCs) are gaseous detectors widely used in high energy physics experiments, operating with a gas mixture primarily containing Tetrafluoroethane (C$_{2}$H$_{2}$F$_{4}$), commonly known as R-134a, which has a global warming potential (GWP) of 1430. To comply with European regulations and explore environmentally friendly alternatives, the RPC EcoGas@GIF++ collaboration, involving ALICE, ATLAS, CMS, LHCb/SHiP, and EP-DT communities, has undertaken intensive R\\&D efforts to explore new gas mixtures for RPC technology.   A leading alternative under investigation is HFO1234ze, boasting a low GWP of 6 and demonstrating reasonable performance compared to R-134a. Over the past few years, RPC detectors with slightly different characteristics and electronics have been studied using HFO and CO$_{2}$-based gas mixtures at the CERN Gamma Irradiation Facility. An aging test campaign was launched in August 2022, and during the latest test beam in July 2023, all detector systems underwent evaluation. This contribution will report the results of the aging studies and the performance evaluations of the detectors with and without irradiation.","sentences":["Resistive Plate Chambers (RPCs) are gaseous detectors widely used in high energy physics experiments, operating with a gas mixture primarily containing Tetrafluoroethane (C$_{2}$H$_{2}$F$_{4}$), commonly known as R-134a, which has a global warming potential (GWP) of 1430.","To comply with European regulations and explore environmentally friendly alternatives, the RPC EcoGas@GIF++ collaboration, involving ALICE, ATLAS, CMS, LHCb/SHiP, and EP-DT communities, has undertaken intensive R\\&D efforts to explore new gas mixtures for RPC technology.   ","A leading alternative under investigation is HFO1234ze, boasting a low GWP of 6 and demonstrating reasonable performance compared to R-134a.","Over the past few years, RPC detectors with slightly different characteristics and electronics have been studied using HFO and CO$_{2}$-based gas mixtures at the CERN Gamma Irradiation Facility.","An aging test campaign was launched in August 2022, and during the latest test beam in July 2023, all detector systems underwent evaluation.","This contribution will report the results of the aging studies and the performance evaluations of the detectors with and without irradiation."],"url":"http://arxiv.org/abs/2402.19395v1","category":"physics.ins-det"}
{"created":"2024-02-29 17:37:43","title":"The viscous variational wave equation with transport noise","abstract":"This article considers the variational wave equation with viscosity and transport noise as a system of three coupled nonlinear stochastic partial differential equations. We prove pathwise global existence, uniqueness, and temporal continuity of solutions to this system in $L^2_x$. Martingale solutions are extracted from a two-level Galerkin approximation via the Skorokhod--Jakubowski theorem. We use the apparatus of Dudley maps to streamline this stochastic compactness method, bypassing the usual martingale identification argument. Pathwise uniqueness for the system is established through a renormalisation procedure that involves double commutator estimates and a delicate handling of noise and nonlinear terms. New model-specific commutator estimates are proven.","sentences":["This article considers the variational wave equation with viscosity and transport noise as a system of three coupled nonlinear stochastic partial differential equations.","We prove pathwise global existence, uniqueness, and temporal continuity of solutions to this system in $L^2_x$. Martingale solutions are extracted from a two-level Galerkin approximation via the Skorokhod--Jakubowski theorem.","We use the apparatus of Dudley maps to streamline this stochastic compactness method, bypassing the usual martingale identification argument.","Pathwise uniqueness for the system is established through a renormalisation procedure that involves double commutator estimates and a delicate handling of noise and nonlinear terms.","New model-specific commutator estimates are proven."],"url":"http://arxiv.org/abs/2402.19386v1","category":"math.AP"}
{"created":"2024-02-29 17:13:30","title":"On Efficient Computation of DiRe Committees","abstract":"Consider a committee election consisting of (i) a set of candidates who are divided into arbitrary groups each of size \\emph{at most} two and a diversity constraint that stipulates the selection of \\emph{at least} one candidate from each group and (ii) a set of voters who are divided into arbitrary populations each approving \\emph{at most} two candidates and a representation constraint that stipulates the selection of \\emph{at least} one candidate from each population who has a non-null set of approved candidates.   The DiRe (Diverse + Representative) committee feasibility problem (a.k.a. the minimum vertex cover problem on unweighted undirected graphs) concerns the determination of the smallest size committee that satisfies the given constraints. Here, for this problem, we discover an unconditional deterministic polynomial-time algorithm that is an amalgamation of maximum matching, breadth-first search, maximal matching, and local minimization.","sentences":["Consider a committee election consisting of (i) a set of candidates who are divided into arbitrary groups each of size \\emph{at most} two and a diversity constraint that stipulates the selection of \\emph{at least} one candidate from each group and (ii) a set of voters who are divided into arbitrary populations each approving \\emph{at most} two candidates and a representation constraint that stipulates the selection of \\emph{at least} one candidate from each population who has a non-null set of approved candidates.   ","The DiRe (Diverse + Representative) committee feasibility problem (a.k.a.","the minimum vertex cover problem on unweighted undirected graphs) concerns the determination of the smallest size committee that satisfies the given constraints.","Here, for this problem, we discover an unconditional deterministic polynomial-time algorithm that is an amalgamation of maximum matching, breadth-first search, maximal matching, and local minimization."],"url":"http://arxiv.org/abs/2402.19365v1","category":"cs.CC"}
{"created":"2024-02-29 17:13:23","title":"Approximate controllability and Irreducibility of the transition semigroup associated with Convective Brinkman-Forchheimer extended Darcy Equations","abstract":"In this article, the following controlled convective Brinkman-Forchheimer extended Darcy (CBFeD) system is considered in a $d$-dimensional torus $\\mathbb{T}^d$:   \\begin{align*}   \\frac{\\partial\\boldsymbol{y}}{\\partial t}-\\mu \\Delta\\boldsymbol{y}+(\\boldsymbol{y}\\cdot\\nabla)\\boldsymbol{y}+\\alpha\\boldsymbol{y}+\\beta\\vert \\boldsymbol{y}\\vert^{r-1}\\boldsymbol{y}+\\gamma\\vert \\boldsymbol{y}\\vert ^{q-1}\\boldsymbol{y}+\\nabla p=\\boldsymbol{g}+\\boldsymbol{u},\\ \\nabla\\cdot\\boldsymbol{y}=0,   \\end{align*}   where $d\\in\\{2,3\\}$, $\\mu,\\alpha,\\beta>0$, $\\gamma\\in\\mathbb{R}$, $r,q\\in[1,\\infty)$ with $r>q\\geq 1$ and $\\boldsymbol{u}$ is the control. For the super critical ($r>3$) and critical ($r=3$ with $2\\beta\\mu>1$) cases, we first show the approximate controllability of the above system in the usual energy space (divergence-free $\\mathbb{L}^2(\\mathbb{T}^d)$ space). As an application of the approximate controllability result, we establish the irreducibility of the transition semigroup associated with stochastic CBFeD system perturbed by non-degenerate Gaussian noise in the usual energy space by exploiting the regularity of solutions, smooth approximation of the multi-valued map $\\mathrm{sgn}(\\cdot)$ a density argument and monotonicity properties of the linear and nonlinear operators.","sentences":["In this article, the following controlled convective Brinkman-Forchheimer extended Darcy (CBFeD) system is considered in a $d$-dimensional torus $\\mathbb{T}^d$:   \\begin{align*}   \\frac{\\partial\\boldsymbol{y}}{\\partial t}-\\mu \\Delta\\boldsymbol{y}+(\\boldsymbol{y}\\cdot\\nabla)\\boldsymbol{y}+\\alpha\\boldsymbol{y}+\\beta\\vert \\boldsymbol{y}\\vert^{r-1}\\boldsymbol{y}+\\gamma\\vert \\boldsymbol{y}\\vert ^{q-1}\\boldsymbol{y}+\\nabla p=\\boldsymbol{g}+\\boldsymbol{u},\\ \\nabla\\cdot\\boldsymbol{y}=0,   \\end{align*}   where $d\\in\\{2,3\\}$, $\\mu,\\alpha,\\beta>0$, $\\gamma\\in\\mathbb{R}$, $r,q\\in[1,\\infty)$ with $r>q\\geq 1$ and $\\boldsymbol{u}$ is the control.","For the super critical ($r>3$) and critical ($r=3$ with $2\\beta\\mu>1$) cases, we first show the approximate controllability of the above system in the usual energy space (divergence-free $\\mathbb{L}^2(\\mathbb{T}^d)$ space).","As an application of the approximate controllability result, we establish the irreducibility of the transition semigroup associated with stochastic CBFeD system perturbed by non-degenerate Gaussian noise in the usual energy space by exploiting the regularity of solutions, smooth approximation of the multi-valued map $\\mathrm{sgn}(\\cdot)$ a density argument and monotonicity properties of the linear and nonlinear operators."],"url":"http://arxiv.org/abs/2402.19363v1","category":"math.PR"}
{"created":"2024-02-29 17:12:02","title":"Joint Chance Constrained Optimal Control via Linear Programming","abstract":"We establish a linear programming formulation for the solution of joint chance constrained optimal control problems over finite time horizons. The joint chance constraint may represent an invariance, reachability or reach-avoid specification that the trajectory must satisfy with a predefined probability. Compared to the existing literature, the formulation is computationally tractable and the solution exact.","sentences":["We establish a linear programming formulation for the solution of joint chance constrained optimal control problems over finite time horizons.","The joint chance constraint may represent an invariance, reachability or reach-avoid specification that the trajectory must satisfy with a predefined probability.","Compared to the existing literature, the formulation is computationally tractable and the solution exact."],"url":"http://arxiv.org/abs/2402.19360v1","category":"math.OC"}
{"created":"2024-02-29 16:56:36","title":"Prompting Explicit and Implicit Knowledge for Multi-hop Question Answering Based on Human Reading Process","abstract":"Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA. However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems. Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading. Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies. In this study, we introduce a \\textbf{P}rompting \\textbf{E}xplicit and \\textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA. We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reasoning. Furthermore, our model incorporates type-specific reasoning via prompts, a form of implicit knowledge. Experimental results show that PEI performs comparably to the state-of-the-art on HotpotQA. Ablation studies confirm the efficacy of our model in bridging and integrating explicit and implicit knowledge.","sentences":["Pre-trained language models (PLMs) leverage chains-of-thought (CoT) to simulate human reasoning and inference processes, achieving proficient performance in multi-hop QA.","However, a gap persists between PLMs' reasoning abilities and those of humans when tackling complex problems.","Psychological studies suggest a vital connection between explicit information in passages and human prior knowledge during reading.","Nevertheless, current research has given insufficient attention to linking input passages and PLMs' pre-training-based knowledge from the perspective of human cognition studies.","In this study, we introduce a \\textbf{P}rompting \\textbf{E}xplicit and \\textbf{I}mplicit knowledge (PEI) framework, which uses prompts to connect explicit and implicit knowledge, aligning with human reading process for multi-hop QA.","We consider the input passages as explicit knowledge, employing them to elicit implicit knowledge through unified prompt reasoning.","Furthermore, our model incorporates type-specific reasoning via prompts, a form of implicit knowledge.","Experimental results show that PEI performs comparably to the state-of-the-art on HotpotQA.","Ablation studies confirm the efficacy of our model in bridging and integrating explicit and implicit knowledge."],"url":"http://arxiv.org/abs/2402.19350v1","category":"cs.CL"}
{"created":"2024-02-29 16:56:35","title":"Optimal Fermionic Joint Measurements for Estimating Non-Commuting Majorana Observables","abstract":"An important class of fermionic observables, relevant in tasks such as fermionic partial tomography and estimating energy levels of chemical Hamiltonians, are the binary measurements obtained from the product of anti-commuting Majorana operators. In this work, we investigate efficient estimation strategies of these observables based on a joint measurement which, after classical post-processing, yields all sufficiently unsharp (noisy) Majorana observables of even-degree. By exploiting the symmetry properties of the Majorana observables, as described by the braid group, we show that the incompatibility robustness, i.e., the minimal classical noise necessary for joint measurability, relates to the spectral properties of the Sachdev-Ye-Kitaev (SYK) model. In particular, we show that for an $n$ mode fermionic system, the incompatibility robustness of all degree--$2k$ Majorana observables satisfies $\\Theta(n^{-k/2})$ for $k\\leq 5$. Furthermore, we present a joint measurement scheme achieving the asymptotically optimal noise, implemented by a small number of fermionic Gaussian unitaries and sampling from the set of all Majorana monomials. Our joint measurement, which can be performed via a randomization over projective measurements, provides rigorous performance guarantees for estimating fermionic observables comparable with fermionic classical shadows.","sentences":["An important class of fermionic observables, relevant in tasks such as fermionic partial tomography and estimating energy levels of chemical Hamiltonians, are the binary measurements obtained from the product of anti-commuting Majorana operators.","In this work, we investigate efficient estimation strategies of these observables based on a joint measurement which, after classical post-processing, yields all sufficiently unsharp (noisy)","Majorana observables of even-degree.","By exploiting the symmetry properties of the Majorana observables, as described by the braid group, we show that the incompatibility robustness, i.e., the minimal classical noise necessary for joint measurability, relates to the spectral properties of the Sachdev-Ye-Kitaev (SYK) model.","In particular, we show that for an $n$ mode fermionic system, the incompatibility robustness of all degree--$2k$ Majorana observables satisfies $\\Theta(n^{-k/2})$ for $k\\leq 5$.","Furthermore, we present a joint measurement scheme achieving the asymptotically optimal noise, implemented by a small number of fermionic Gaussian unitaries and sampling from the set of all Majorana monomials.","Our joint measurement, which can be performed via a randomization over projective measurements, provides rigorous performance guarantees for estimating fermionic observables comparable with fermionic classical shadows."],"url":"http://arxiv.org/abs/2402.19349v1","category":"quant-ph"}
{"created":"2024-02-29 16:55:44","title":"#PoetsOfInstagram: Navigating The Practices And Challenges Of Novice Poets On Instagram","abstract":"Commencing as a photo-sharing platform, Instagram has since become multifaceted, accommodating diverse art forms, with poetry emerging as a prominent one. However, the academic understanding of Instagram's poetry community is limited, yet its significance emerges from its distinctive utilization of a primarily visual social media platform guided by recommendation algorithms for disseminating poetry, further characterized by a predominantly novice creative population. We employ qualitative analysis to explore motivations, experiences, and algorithmic influence within Instagram's poetry community. We demonstrate that participants prioritize conforming to algorithmic constraints for visibility, yet maintain their community's values of integrity and originality, illustrating the tension between algorithmic growth and participant authenticity. We introduce the concept of Algorithmically Mediated Creative Labor, a phenomenon specific to non-monetizing creative users who are impacted by the prioritization of professional creators and continually adapt their creative endeavors to align with platform logic, thereby affecting their motivation and creative outputs.","sentences":["Commencing as a photo-sharing platform, Instagram has since become multifaceted, accommodating diverse art forms, with poetry emerging as a prominent one.","However, the academic understanding of Instagram's poetry community is limited, yet its significance emerges from its distinctive utilization of a primarily visual social media platform guided by recommendation algorithms for disseminating poetry, further characterized by a predominantly novice creative population.","We employ qualitative analysis to explore motivations, experiences, and algorithmic influence within Instagram's poetry community.","We demonstrate that participants prioritize conforming to algorithmic constraints for visibility, yet maintain their community's values of integrity and originality, illustrating the tension between algorithmic growth and participant authenticity.","We introduce the concept of Algorithmically Mediated Creative Labor, a phenomenon specific to non-monetizing creative users who are impacted by the prioritization of professional creators and continually adapt their creative endeavors to align with platform logic, thereby affecting their motivation and creative outputs."],"url":"http://arxiv.org/abs/2402.19347v1","category":"cs.HC"}
{"created":"2024-02-29 16:49:38","title":"The 6th Affective Behavior Analysis in-the-wild (ABAW) Competition","abstract":"This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW) Competition, which is part of the respective Workshop held in conjunction with IEEE CVPR 2024. The 6th ABAW Competition addresses contemporary challenges in understanding human emotions and behaviors, crucial for the development of human-centered technologies. In more detail, the Competition focuses on affect related benchmarking tasks and comprises of five sub-challenges: i) Valence-Arousal Estimation (the target is to estimate two continuous affect dimensions, valence and arousal), ii) Expression Recognition (the target is to recognise between the mutually exclusive classes of the 7 basic expressions and 'other'), iii) Action Unit Detection (the target is to detect 12 action units), iv) Compound Expression Recognition (the target is to recognise between the 7 mutually exclusive compound expression classes), and v) Emotional Mimicry Intensity Estimation (the target is to estimate six continuous emotion dimensions). In the paper, we present these Challenges, describe their respective datasets and challenge protocols (we outline the evaluation metrics) and present the baseline systems as well as their obtained performance. More information for the Competition can be found in: \\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}.","sentences":["This paper describes the 6th Affective Behavior Analysis in-the-wild (ABAW) Competition, which is part of the respective Workshop held in conjunction with IEEE CVPR 2024.","The 6th ABAW Competition addresses contemporary challenges in understanding human emotions and behaviors, crucial for the development of human-centered technologies.","In more detail, the Competition focuses on affect related benchmarking tasks and comprises of five sub-challenges: i) Valence-Arousal Estimation (the target is to estimate two continuous affect dimensions, valence and arousal), ii) Expression Recognition (the target is to recognise between the mutually exclusive classes of the 7 basic expressions and 'other'), iii) Action Unit Detection (the target is to detect 12 action units), iv) Compound Expression Recognition (the target is to recognise between the 7 mutually exclusive compound expression classes), and v) Emotional Mimicry Intensity Estimation (the target is to estimate six continuous emotion dimensions).","In the paper, we present these Challenges, describe their respective datasets and challenge protocols (we outline the evaluation metrics) and present the baseline systems as well as their obtained performance.","More information for the Competition can be found in: \\url{https://affective-behavior-analysis-in-the-wild.github.io/6th}."],"url":"http://arxiv.org/abs/2402.19344v1","category":"cs.CV"}
{"created":"2024-02-29 16:37:08","title":"Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge","abstract":"The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies. However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability. This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure. In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI). Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge. Our approach consistently outperforms the other advanced baselines, leading to an average of 75% reduction in the attack success rate. Since model merging has been an established approach for improving model performance, the extra advantage it provides regarding defense can be seen as a cost-free bonus.","sentences":["The democratization of pre-trained language models through open-source initiatives has rapidly advanced innovation and expanded access to cutting-edge technologies.","However, this openness also brings significant security risks, including backdoor attacks, where hidden malicious behaviors are triggered by specific inputs, compromising natural language processing (NLP) system integrity and reliability.","This paper suggests that merging a backdoored model with other homogeneous models can remediate backdoor vulnerabilities even if such models are not entirely secure.","In our experiments, we explore various models (BERT-Base, RoBERTa-Large, Llama2-7B, and Mistral-7B) and datasets (SST-2, OLID, AG News, and QNLI).","Compared to multiple advanced defensive approaches, our method offers an effective and efficient inference-stage defense against backdoor attacks without additional resources or specific knowledge.","Our approach consistently outperforms the other advanced baselines, leading to an average of 75% reduction in the attack success rate.","Since model merging has been an established approach for improving model performance, the extra advantage it provides regarding defense can be seen as a cost-free bonus."],"url":"http://arxiv.org/abs/2402.19334v1","category":"cs.CL"}
{"created":"2024-02-29 16:30:07","title":"GPTFF: A high-accuracy out-of-the-box universal AI force field for arbitrary inorganic materials","abstract":"This study introduces a novel AI force field, namely graph-based pre-trained transformer force field (GPTFF), which can simulate arbitrary inorganic systems with good precision and generalizability. Harnessing a large trove of the data and the attention mechanism of transformer algorithms, the model can accurately predict energy, atomic forces, and stress with Mean Absolute Error (MAE) values of 32 meV/atom, 71 meV/{\\AA}, and 0.365 GPa, respectively. The dataset used to train the model includes 37.8 million single-point energies, 11.7 billion force pairs, and 340.2 million stresses. We also demonstrated that GPTFF can be universally used to simulate various physical systems, such as crystal structure optimization, phase transition simulations, and mass transport.","sentences":["This study introduces a novel AI force field, namely graph-based pre-trained transformer force field (GPTFF), which can simulate arbitrary inorganic systems with good precision and generalizability.","Harnessing a large trove of the data and the attention mechanism of transformer algorithms, the model can accurately predict energy, atomic forces, and stress with Mean Absolute Error (MAE) values of 32 meV/atom, 71 meV/{\\AA}, and 0.365 GPa, respectively.","The dataset used to train the model includes 37.8 million single-point energies, 11.7 billion force pairs, and 340.2 million stresses.","We also demonstrated that GPTFF can be universally used to simulate various physical systems, such as crystal structure optimization, phase transition simulations, and mass transport."],"url":"http://arxiv.org/abs/2402.19327v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 16:30:07","title":"Seeking Soulmate via Voice: Understanding Promises and Challenges of Online Synchronized Voice-Based Mobile Dating","abstract":"Online dating has become a popular way for individuals to connect with potential romantic partners. Many dating apps use personal profiles that include a headshot and self-description, allowing users to present themselves and search for compatible matches. However, this traditional model often has limitations. In this study, we explore a non-traditional voice-based dating app called \"Soul\". Unlike traditional platforms that rely heavily on profile information, Soul facilitates user interactions through voice-based communication. We conducted semi-structured interviews with 18 dedicated Soul users to investigate how they engage with the platform and perceive themselves and others in this unique dating environment. Our findings indicate that the role of voice as a moderator influences impression management and shapes perceptions between the sender and the receiver of the voice. Additionally, the synchronous voice-based and community-based dating model offers benefits to users in the Chinese cultural context. Our study contributes to understanding the affordances introduced by voice-based interactions in online dating in China.","sentences":["Online dating has become a popular way for individuals to connect with potential romantic partners.","Many dating apps use personal profiles that include a headshot and self-description, allowing users to present themselves and search for compatible matches.","However, this traditional model often has limitations.","In this study, we explore a non-traditional voice-based dating app called \"Soul\".","Unlike traditional platforms that rely heavily on profile information, Soul facilitates user interactions through voice-based communication.","We conducted semi-structured interviews with 18 dedicated Soul users to investigate how they engage with the platform and perceive themselves and others in this unique dating environment.","Our findings indicate that the role of voice as a moderator influences impression management and shapes perceptions between the sender and the receiver of the voice.","Additionally, the synchronous voice-based and community-based dating model offers benefits to users in the Chinese cultural context.","Our study contributes to understanding the affordances introduced by voice-based interactions in online dating in China."],"url":"http://arxiv.org/abs/2402.19328v1","category":"cs.HC"}
{"created":"2024-02-29 16:28:58","title":"Do End-to-End Neural Diarization Attractors Need to Encode Speaker Characteristic Information?","abstract":"In this paper, we apply the variational information bottleneck approach to end-to-end neural diarization with encoder-decoder attractors (EEND-EDA). This allows us to investigate what information is essential for the model. EEND-EDA utilizes vector representations of the speakers in a conversation - attractors. Our analysis shows that, attractors do not necessarily have to contain speaker characteristic information. On the other hand, giving the attractors more freedom allowing them to encode some extra (possibly speaker-specific) information leads to small but consistent diarization performance improvements. Despite architectural differences in EEND systems, the notion of attractors and frame embeddings is common to most of them and not specific to EEND-EDA. We believe that the main conclusions of this work can apply to other variants of EEND. Thus, we hope this paper will be a valuable contribution to guide the community to make more informed decisions when designing new systems.","sentences":["In this paper, we apply the variational information bottleneck approach to end-to-end neural diarization with encoder-decoder attractors (EEND-EDA).","This allows us to investigate what information is essential for the model.","EEND-EDA utilizes vector representations of the speakers in a conversation - attractors.","Our analysis shows that, attractors do not necessarily have to contain speaker characteristic information.","On the other hand, giving the attractors more freedom allowing them to encode some extra (possibly speaker-specific) information leads to small but consistent diarization performance improvements.","Despite architectural differences in EEND systems, the notion of attractors and frame embeddings is common to most of them and not specific to EEND-EDA.","We believe that the main conclusions of this work can apply to other variants of EEND.","Thus, we hope this paper will be a valuable contribution to guide the community to make more informed decisions when designing new systems."],"url":"http://arxiv.org/abs/2402.19325v1","category":"cs.SD"}
{"created":"2024-02-29 16:28:42","title":"Entropy of axial product of multiplicative subshifts","abstract":"We obtain the entropy and the surface entropy of the axial products on $\\mathbb{N}^d$ and the $d$-tree $T^d$ of two types of systems: the subshift and the multiplicative subshift.","sentences":["We obtain the entropy and the surface entropy of the axial products on $\\mathbb{N}^d$ and the $d$-tree $T^d$ of two types of systems: the subshift and the multiplicative subshift."],"url":"http://arxiv.org/abs/2402.19324v1","category":"math.DS"}
{"created":"2024-02-29 16:24:17","title":"DISCERN: Designing Decision Support Interfaces to Investigate the Complexities of Workplace Social Decision-Making With Line Managers","abstract":"Line managers form the first level of management in organizations, and must make complex decisions, while maintaining relationships with those impacted by their decisions. Amidst growing interest in technology-supported decision-making at work, their needs remain understudied. Further, most existing design knowledge for supporting social decision-making comes from domains where decision-makers are more socially detached from those they decide for. We conducted iterative design research with line managers within a technology organization, investigating decision-making practices, and opportunities for technological support. Through formative research, development of a decision-representation tool -- DISCERN -- and user enactments, we identify their communication and analysis needs that lack adequate support. We found they preferred tools for externalizing reasoning rather than tools that replace interpersonal interactions, and they wanted tools to support a range of intuitive and calculative decision-making. We discuss how design of social decision-making supports, especially in the workplace, can more explicitly support highly interactional social decision-making.","sentences":["Line managers form the first level of management in organizations, and must make complex decisions, while maintaining relationships with those impacted by their decisions.","Amidst growing interest in technology-supported decision-making at work, their needs remain understudied.","Further, most existing design knowledge for supporting social decision-making comes from domains where decision-makers are more socially detached from those they decide for.","We conducted iterative design research with line managers within a technology organization, investigating decision-making practices, and opportunities for technological support.","Through formative research, development of a decision-representation tool -- DISCERN -- and user enactments, we identify their communication and analysis needs that lack adequate support.","We found they preferred tools for externalizing reasoning rather than tools that replace interpersonal interactions, and they wanted tools to support a range of intuitive and calculative decision-making.","We discuss how design of social decision-making supports, especially in the workplace, can more explicitly support highly interactional social decision-making."],"url":"http://arxiv.org/abs/2402.19318v1","category":"cs.HC"}
{"created":"2024-02-29 16:22:13","title":"Simulation framework for integrated nonlinear quantum photonics","abstract":"Nonlinear quantum photonics serves as a cornerstone in photonic quantum technologies, such as universal quantum computing and quantum communications. The emergence of integrated photonics platform not only offers the advantage of large-scale manufacturing but also provides a variety of engineering methods. Given the complexity of integrated photonics engineering, a comprehensive simulation framework is essential to fully harness the potential of the platform. In this context, we introduce a nonlinear quantum photonics simulation framework which can accurately model a variety of features such as adiabatic waveguide, material anisotropy, linear optics components, photon losses, and detectors. Furthermore, utilizing the framework, we have developed a device scheme, chip-scale temporal walk-off compensation, that is useful for various quantum information processing tasks. Applying the simulation framework, we show that the proposed device scheme can enhance the squeezing parameter of photon-pair sources and the conversion efficiency of quantum frequency converters without relying on higher pump power.","sentences":["Nonlinear quantum photonics serves as a cornerstone in photonic quantum technologies, such as universal quantum computing and quantum communications.","The emergence of integrated photonics platform not only offers the advantage of large-scale manufacturing but also provides a variety of engineering methods.","Given the complexity of integrated photonics engineering, a comprehensive simulation framework is essential to fully harness the potential of the platform.","In this context, we introduce a nonlinear quantum photonics simulation framework which can accurately model a variety of features such as adiabatic waveguide, material anisotropy, linear optics components, photon losses, and detectors.","Furthermore, utilizing the framework, we have developed a device scheme, chip-scale temporal walk-off compensation, that is useful for various quantum information processing tasks.","Applying the simulation framework, we show that the proposed device scheme can enhance the squeezing parameter of photon-pair sources and the conversion efficiency of quantum frequency converters without relying on higher pump power."],"url":"http://arxiv.org/abs/2402.19317v1","category":"quant-ph"}
{"created":"2024-02-29 16:22:12","title":"Taut-Line Buzzers with Periodic Forcing","abstract":"The time-dependent forcing and work per cycle required to drive sinusoidal spinning of a taut-line buzzer is analytically derived, both on and off resonance, from the nonlinear equation of motion. To test predictions, a model experimental system is constructed and characterized in terms of contraction versus twist angle and damped oscillations. The predicted force profile is then approximately implemented by hand. Nearly sinusoidal motion is observed, and the energy injection per cycle needed to maintain steady state oscillations is found to agree with theory. Additional force profiles are implemented, one to maximize non-sinusoidal response and one to maximize the response per operator effort. With the latter, an Aluminum disk of radius 5~cm and height 0.95~cm was spun at a peak speed of over 11,000~RMP for fifteen minutes. The corresponding hand-powered centrifuge system would required $1.5\\times$ more force, twice the power, and triple the time in order to run $90\\times$ more samples than prior state-of-art.","sentences":["The time-dependent forcing and work per cycle required to drive sinusoidal spinning of a taut-line buzzer is analytically derived, both on and off resonance, from the nonlinear equation of motion.","To test predictions, a model experimental system is constructed and characterized in terms of contraction versus twist angle and damped oscillations.","The predicted force profile is then approximately implemented by hand.","Nearly sinusoidal motion is observed, and the energy injection per cycle needed to maintain steady state oscillations is found to agree with theory.","Additional force profiles are implemented, one to maximize non-sinusoidal response and one to maximize the response per operator effort.","With the latter, an Aluminum disk of radius 5~cm and height 0.95~cm was spun at a peak speed of over 11,000~RMP for fifteen minutes.","The corresponding hand-powered centrifuge system would required $1.5\\times$ more force, twice the power, and triple the time in order to run $90\\times$ more samples than prior state-of-art."],"url":"http://arxiv.org/abs/2402.19316v1","category":"cond-mat.soft"}
{"created":"2024-02-29 16:21:14","title":"On the Existence of Static Equilibria of a Cable-Suspended Load with Non-stopping Flying Carriers","abstract":"Aerial cooperative robotic manipulation of cable-suspended objects has been largely studied as it allows handling large and heavy objects, and cables offer multiple advantages, such as their low weight and cost efficiency. Multirotors have been typically considered, which, however, can be unsuitable for long-lasting manipulation tasks due to their scarce endurance. Hence, this work investigates whether non-stop flights are possible for maintaining constant the pose of cable-suspended objects. First, we show that one or two flying carriers alone cannot perform non-stop flights while maintaining a constant pose of the suspended object. Instead, we demonstrate that \\emph{three} flying carriers can achieve this task provided that the orientation of the load at the equilibrium is such that the components of the cable forces that balance the external force (typically gravity) do not belong to the plane of the cable anchoring points on the load. Numerical tests are presented in support of the analytical results.","sentences":["Aerial cooperative robotic manipulation of cable-suspended objects has been largely studied as it allows handling large and heavy objects, and cables offer multiple advantages, such as their low weight and cost efficiency.","Multirotors have been typically considered, which, however, can be unsuitable for long-lasting manipulation tasks due to their scarce endurance.","Hence, this work investigates whether non-stop flights are possible for maintaining constant the pose of cable-suspended objects.","First, we show that one or two flying carriers alone cannot perform non-stop flights while maintaining a constant pose of the suspended object.","Instead, we demonstrate that \\emph{three} flying carriers can achieve this task provided that the orientation of the load at the equilibrium is such that the components of the cable forces that balance the external force (typically gravity) do not belong to the plane of the cable anchoring points on the load.","Numerical tests are presented in support of the analytical results."],"url":"http://arxiv.org/abs/2402.19315v1","category":"cs.RO"}
{"created":"2024-02-29 16:15:37","title":"Closed-loop training of static output feedback neural network controllers for large systems: A distillation case study","abstract":"The online implementation of model predictive control for constrained multivariate systems has two main disadvantages: it requires an estimate of the entire model state and an optimisation problem must be solved online. These issues have typically been treated separately. This work proposes an integrated approach for the offline training of an output feedback neural network controller in closed loop. Online this neural network controller computers the plant inputs cheaply using noisy measurements. In addition, the controller can be trained to only make use of certain predefined measurements. Further, a heuristic approach is proposed to perform the automatic selection of important measurements. The proposed method is demonstrated by extensive simulations using a non-linear distillation column model of 50 states.","sentences":["The online implementation of model predictive control for constrained multivariate systems has two main disadvantages: it requires an estimate of the entire model state and an optimisation problem must be solved online.","These issues have typically been treated separately.","This work proposes an integrated approach for the offline training of an output feedback neural network controller in closed loop.","Online this neural network controller computers the plant inputs cheaply using noisy measurements.","In addition, the controller can be trained to only make use of certain predefined measurements.","Further, a heuristic approach is proposed to perform the automatic selection of important measurements.","The proposed method is demonstrated by extensive simulations using a non-linear distillation column model of 50 states."],"url":"http://arxiv.org/abs/2402.19309v1","category":"eess.SY"}
{"created":"2024-02-29 16:10:49","title":"HyenaPixel: Global Image Context with Convolutions","abstract":"In vision tasks, a larger effective receptive field (ERF) is associated with better performance. While attention natively supports global context, convolution requires multiple stacked layers and a hierarchical structure for large context. In this work, we extend Hyena, a convolution-based attention replacement, from causal sequences to the non-causal two-dimensional image space. We scale the Hyena convolution kernels beyond the feature map size up to 191$\\times$191 to maximize the ERF while maintaining sub-quadratic complexity in the number of pixels. We integrate our two-dimensional Hyena, HyenaPixel, and bidirectional Hyena into the MetaFormer framework. For image categorization, HyenaPixel and bidirectional Hyena achieve a competitive ImageNet-1k top-1 accuracy of 83.0% and 83.5%, respectively, while outperforming other large-kernel networks. Combining HyenaPixel with attention further increases accuracy to 83.6%. We attribute the success of attention to the lack of spatial bias in later stages and support this finding with bidirectional Hyena.","sentences":["In vision tasks, a larger effective receptive field (ERF) is associated with better performance.","While attention natively supports global context, convolution requires multiple stacked layers and a hierarchical structure for large context.","In this work, we extend Hyena, a convolution-based attention replacement, from causal sequences to the non-causal two-dimensional image space.","We scale the Hyena convolution kernels beyond the feature map size up to 191$\\times$191 to maximize the ERF while maintaining sub-quadratic complexity in the number of pixels.","We integrate our two-dimensional Hyena, HyenaPixel, and bidirectional Hyena into the MetaFormer framework.","For image categorization, HyenaPixel and bidirectional Hyena achieve a competitive ImageNet-1k top-1 accuracy of 83.0% and 83.5%, respectively, while outperforming other large-kernel networks.","Combining HyenaPixel with attention further increases accuracy to 83.6%.","We attribute the success of attention to the lack of spatial bias in later stages and support this finding with bidirectional Hyena."],"url":"http://arxiv.org/abs/2402.19305v1","category":"cs.CV"}
{"created":"2024-02-29 16:09:19","title":"Learnability Gaps of Strategic Classification","abstract":"In contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions. For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards to fool the classifier. The learning goal is to find a classifier robust against strategic manipulations. Various settings, based on what and when information is known, have been explored in strategic classification. In this work, we focus on addressing a fundamental question: the learnability gaps between strategic classification and standard learning.   We essentially show that any learnable class is also strategically learnable: we first consider a fully informative setting, where the manipulation structure (which is modeled by a manipulation graph $G^\\star$) is known and during training time the learner has access to both the pre-manipulation data and post-manipulation data. We provide nearly tight sample complexity and regret bounds, offering significant improvements over prior results. Then, we relax the fully informative setting by introducing two natural types of uncertainty. First, following Ahmadi et al. (2023), we consider the setting in which the learner only has access to the post-manipulation data. We improve the results of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower bound raised by them. Our second relaxation of the fully informative setting introduces uncertainty to the manipulation structure. That is, we assume that the manipulation graph is unknown but belongs to a known class of graphs. We provide nearly tight bounds on the learning complexity in various unknown manipulation graph settings. Notably, our algorithm in this setting is of independent interest and can be applied to other problems such as multi-label learning.","sentences":["In contrast with standard classification tasks, strategic classification involves agents strategically modifying their features in an effort to receive favorable predictions.","For instance, given a classifier determining loan approval based on credit scores, applicants may open or close their credit cards to fool the classifier.","The learning goal is to find a classifier robust against strategic manipulations.","Various settings, based on what and when information is known, have been explored in strategic classification.","In this work, we focus on addressing a fundamental question: the learnability gaps between strategic classification and standard learning.   ","We essentially show that any learnable class is also strategically learnable: we first consider a fully informative setting, where the manipulation structure (which is modeled by a manipulation graph $G^\\star$) is known and during training time the learner has access to both the pre-manipulation data and post-manipulation data.","We provide nearly tight sample complexity and regret bounds, offering significant improvements over prior results.","Then, we relax the fully informative setting by introducing two natural types of uncertainty.","First, following Ahmadi et al. (2023), we consider the setting in which the learner only has access to the post-manipulation data.","We improve the results of Ahmadi et al. (2023) and close the gap between mistake upper bound and lower bound raised by them.","Our second relaxation of the fully informative setting introduces uncertainty to the manipulation structure.","That is, we assume that the manipulation graph is unknown but belongs to a known class of graphs.","We provide nearly tight bounds on the learning complexity in various unknown manipulation graph settings.","Notably, our algorithm in this setting is of independent interest and can be applied to other problems such as multi-label learning."],"url":"http://arxiv.org/abs/2402.19303v1","category":"cs.LG"}
{"created":"2024-02-29 16:02:24","title":"Linear stability of cylindrical, multicomponent vesicles","abstract":"Vesicles are important surrogate structures made up of multiple phospholipids and cholesterol distributed in the form of a lipid bilayer. Tubular vesicles can undergo pearling i.e., formation of beads on the liquid thread akin to the Rayleigh-Plateau instability. Previous studies have inspected the effects of surface tension on the pearling instabilities of single-component vesicles. In this study, we perform a linear stability analysis on a multicomponent cylindrical vesicle. We solve the Stokes equations along with the Cahn-Hilliard equations to develop the linearized dynamic equations governing the vesicle shape and surface concentration fields. This helps us show that multicomponent vesicles can undergo pearling, buckling, and wrinkling even in the absence of surface tension, which is a significantly different result from studies on single-component vesicles. This behaviour arises due to the competition between the free energies of phase separation, line tension, and bending for this multi-phospholipid system. We determine the conditions under which axisymmetric and non-axisymmetric modes are dominant, and supplement our results with an energy analysis that shows the sources for these instabilities. We further show that these trends qualitatively match recent experiments.","sentences":["Vesicles are important surrogate structures made up of multiple phospholipids and cholesterol distributed in the form of a lipid bilayer.","Tubular vesicles can undergo pearling i.e., formation of beads on the liquid thread akin to the Rayleigh-Plateau instability.","Previous studies have inspected the effects of surface tension on the pearling instabilities of single-component vesicles.","In this study, we perform a linear stability analysis on a multicomponent cylindrical vesicle.","We solve the Stokes equations along with the Cahn-Hilliard equations to develop the linearized dynamic equations governing the vesicle shape and surface concentration fields.","This helps us show that multicomponent vesicles can undergo pearling, buckling, and wrinkling even in the absence of surface tension, which is a significantly different result from studies on single-component vesicles.","This behaviour arises due to the competition between the free energies of phase separation, line tension, and bending for this multi-phospholipid system.","We determine the conditions under which axisymmetric and non-axisymmetric modes are dominant, and supplement our results with an energy analysis that shows the sources for these instabilities.","We further show that these trends qualitatively match recent experiments."],"url":"http://arxiv.org/abs/2402.19297v1","category":"cond-mat.soft"}
{"created":"2024-02-29 15:53:47","title":"Estimation and Deconvolution of Second Order Cyclostationary Signals","abstract":"This method solves the dual problem of blind deconvolution and estimation of the time waveform of noisy second-order cyclo-stationary (CS2) signals that traverse a Transfer Function (TF) en route to a sensor. We have proven that the deconvolution filter exists and eliminates the TF effect from signals whose statistics vary over time. This method is blind, meaning it does not require prior knowledge about the signals or TF. Simulations demonstrate the algorithm high precision across various signal types, TFs, and Signal-to-Noise Ratios (SNRs). In this study, the CS2 signals family is restricted to the product of a deterministic periodic function and white noise. Furthermore, this method has the potential to improve the training of Machine Learning models where the aggregation of signals from identical systems but with different TFs is required.","sentences":["This method solves the dual problem of blind deconvolution and estimation of the time waveform of noisy second-order cyclo-stationary (CS2) signals that traverse a Transfer Function (TF) en route to a sensor.","We have proven that the deconvolution filter exists and eliminates the TF effect from signals whose statistics vary over time.","This method is blind, meaning it does not require prior knowledge about the signals or TF.","Simulations demonstrate the algorithm high precision across various signal types, TFs, and Signal-to-Noise Ratios (SNRs).","In this study, the CS2 signals family is restricted to the product of a deterministic periodic function and white noise.","Furthermore, this method has the potential to improve the training of Machine Learning models where the aggregation of signals from identical systems but with different TFs is required."],"url":"http://arxiv.org/abs/2402.19290v1","category":"cs.LG"}
{"created":"2024-02-29 15:51:14","title":"PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology Segmentation","abstract":"Understanding the anatomy of renal pathology is crucial for advancing disease diagnostics, treatment evaluation, and clinical research. The complex kidney system comprises various components across multiple levels, including regions (cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes, mesangial cells in glomerulus). Prior studies have predominantly overlooked the intricate spatial interrelations among objects from clinical knowledge. In this research, we introduce a novel universal proposition learning approach, called panoramic renal pathology segmentation (PrPSeg), designed to segment comprehensively panoramic structures within kidney by integrating extensive knowledge of kidney anatomy.   In this paper, we propose (1) the design of a comprehensive universal proposition matrix for renal pathology, facilitating the incorporation of classification and spatial relationships into the segmentation process; (2) a token-based dynamic head single network architecture, with the improvement of the partial label image segmentation and capability for future data enlargement; and (3) an anatomy loss function, quantifying the inter-object relationships across the kidney.","sentences":["Understanding the anatomy of renal pathology is crucial for advancing disease diagnostics, treatment evaluation, and clinical research.","The complex kidney system comprises various components across multiple levels, including regions (cortex, medulla), functional units (glomeruli, tubules), and cells (podocytes, mesangial cells in glomerulus).","Prior studies have predominantly overlooked the intricate spatial interrelations among objects from clinical knowledge.","In this research, we introduce a novel universal proposition learning approach, called panoramic renal pathology segmentation (PrPSeg), designed to segment comprehensively panoramic structures within kidney by integrating extensive knowledge of kidney anatomy.   ","In this paper, we propose (1) the design of a comprehensive universal proposition matrix for renal pathology, facilitating the incorporation of classification and spatial relationships into the segmentation process; (2) a token-based dynamic head single network architecture, with the improvement of the partial label image segmentation and capability for future data enlargement; and (3) an anatomy loss function, quantifying the inter-object relationships across the kidney."],"url":"http://arxiv.org/abs/2402.19286v1","category":"eess.IV"}
{"created":"2024-02-29 15:50:40","title":"Supercurrent through an Andreev trimer","abstract":"Detection and control of Andreev Bound States (ABSs) localized at semiconductor-superconductor interfaces are essential for their use in quantum applications. Here we investigate the impact of ABSs on the supercurrent through a Josephson junction containing a quantum dot (QD). Additional normal-metal tunneling probes on both sides of the junction unveil the ABSs residing at the semi-superconductor interfaces. Such knowledge provides an ingredient missing in previous studies, improving the connection between theory and experimental data. By varying the ABS energies using electrostatic gates, we show control of the switching current, with the ability to alter it by more than an order of magnitude. Finally, the large degree of ABS tunability allows us to realize a three-site ABS-QD-ABS molecule (Andreev trimer) in which the central QD is screened by both ABSs. This system is studied simultaneously using both supercurrent and spectroscopy.","sentences":["Detection and control of Andreev Bound States (ABSs) localized at semiconductor-superconductor interfaces are essential for their use in quantum applications.","Here we investigate the impact of ABSs on the supercurrent through a Josephson junction containing a quantum dot (QD).","Additional normal-metal tunneling probes on both sides of the junction unveil the ABSs residing at the semi-superconductor interfaces.","Such knowledge provides an ingredient missing in previous studies, improving the connection between theory and experimental data.","By varying the ABS energies using electrostatic gates, we show control of the switching current, with the ability to alter it by more than an order of magnitude.","Finally, the large degree of ABS tunability allows us to realize a three-site ABS-QD-ABS molecule (Andreev trimer) in which the central QD is screened by both ABSs.","This system is studied simultaneously using both supercurrent and spectroscopy."],"url":"http://arxiv.org/abs/2402.19284v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 15:41:38","title":"Mixed-halide perovskite alloys $\\text{CsPb}(\\text{I}_{1-x}^{}\\text{Br}_x^{})_3^{}$ and $\\text{CsPb}(\\text{Br}_{1-x}^{}\\text{Cl}_x^{})_3^{}$: New insight of configuration entropy effect from first principles and phase diagrams","abstract":"Stability is one of the key issues in mixed-halide perovskite alloys which are promising in emergent optoelectronics. Previous density-functional-theory (DFT) and machine learning studies indicate that the formation-energy convex hulls of these materials are very shallow, and stable alloy compositions are rare. In this work, we revisit this problem using DFT with special focus on the effects of configuration and vibration entropies. Allowed by the $20$-atomic models for the $\\text{CsPb}(\\text{I}_{1-x}^{}\\text{Br}_x^{})_3^{}$ and $\\text{CsPb}(\\text{Br}_{1-x}^{}\\text{Cl}_x^{})_3^{}$ series, the partition functions and therewith thermodynamic state functions are calculated by traversing all possible mixed-halide configurations. We can thus evaluate the temperature- and system-dependent configuration entropy, which largely corrects the conventional approach based on the ideal solution model. Finally, temperature-composition phase diagrams that include $\\alpha$, $\\beta$, $\\gamma$ and $\\delta$ phases of both alloys are constructed based on the free energy data, for which the contribution of phonon vibrations is included.","sentences":["Stability is one of the key issues in mixed-halide perovskite alloys which are promising in emergent optoelectronics.","Previous density-functional-theory (DFT) and machine learning studies indicate that the formation-energy convex hulls of these materials are very shallow, and stable alloy compositions are rare.","In this work, we revisit this problem using DFT with special focus on the effects of configuration and vibration entropies.","Allowed by the $20$-atomic models for the $\\text{CsPb}(\\text{I}_{1-x}^{}\\text{Br}_x^{})_3^{}$ and $\\text{CsPb}(\\text{Br}_{1-x}^{}\\text{Cl}_x^{})_3^{}$ series, the partition functions and therewith thermodynamic state functions are calculated by traversing all possible mixed-halide configurations.","We can thus evaluate the temperature- and system-dependent configuration entropy, which largely corrects the conventional approach based on the ideal solution model.","Finally, temperature-composition phase diagrams that include $\\alpha$, $\\beta$, $\\gamma$ and $\\delta$ phases of both alloys are constructed based on the free energy data, for which the contribution of phonon vibrations is included."],"url":"http://arxiv.org/abs/2402.19274v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 15:39:04","title":"Highly stable photoluminescence in vacuum-processed halide perovskite core-shell 1D nanostructures","abstract":"Hybrid organometal halide perovskites (HP) present exceptional optoelectronic properties, but their poor long-term stability is a major bottleneck for their commercialization. Herein, we present a solvent-free approach to growing single-crystal organic nanowires (ONW), nanoporous metal oxide scaffolds, and HP to form a core@multishell architecture. The synthetic procedure is carried out under mild vacuum conditions employing thermal evaporation for the metal-free phthalocyanine (H2Pc) nanowires, which will be the core, plasma-enhanced chemical vapor deposition (PECVD) for the TiO2 shell, and co-evaporation of lead iodide (PbI2) and methylammonium iodide (CH3NH3I / MAI) for the CH3NH3PbI3 (MAPbI3 / MAPI) perovskite shell. We present a detailed characterization of the nanostructures by (S)-TEM and XRD, revealing a different crystallization of the hybrid perovskite depending on the template: while the growth on H2Pc nanowires induces the typical tetragonal structure of the MAPI perovskite, a low-dimensional phase (LDP) was observed on the one-dimensional TiO2 nanotubes. Such a combination yields an unprecedentedly stable photoluminescence emission over 20 hours and over 300 hours after encapsulation in polymethyl methacrylate (PMMA) under different atmospheres including N2, air, and high moisture levels. In addition, the unique one-dimensional morphology of the system, together with the high refractive index HP, allows for a strong waveguiding effect along the nanowire length.","sentences":["Hybrid organometal halide perovskites (HP) present exceptional optoelectronic properties, but their poor long-term stability is a major bottleneck for their commercialization.","Herein, we present a solvent-free approach to growing single-crystal organic nanowires (ONW), nanoporous metal oxide scaffolds, and HP to form a core@multishell architecture.","The synthetic procedure is carried out under mild vacuum conditions employing thermal evaporation for the metal-free phthalocyanine (H2Pc) nanowires, which will be the core, plasma-enhanced chemical vapor deposition (PECVD) for the TiO2 shell, and co-evaporation of lead iodide (PbI2) and methylammonium iodide (CH3NH3I / MAI) for the CH3NH3PbI3 (MAPbI3 / MAPI) perovskite shell.","We present a detailed characterization of the nanostructures by (S)-TEM and XRD, revealing a different crystallization of the hybrid perovskite depending on the template: while the growth on H2Pc nanowires induces the typical tetragonal structure of the MAPI perovskite, a low-dimensional phase (LDP) was observed on the one-dimensional TiO2 nanotubes.","Such a combination yields an unprecedentedly stable photoluminescence emission over 20 hours and over 300 hours after encapsulation in polymethyl methacrylate (PMMA) under different atmospheres including N2, air, and high moisture levels.","In addition, the unique one-dimensional morphology of the system, together with the high refractive index HP, allows for a strong waveguiding effect along the nanowire length."],"url":"http://arxiv.org/abs/2402.19269v1","category":"physics.app-ph"}
{"created":"2024-02-29 15:26:22","title":"Collet-Eckmann maps in the unicritical family","abstract":"In this paper we study perturbations of complex unicritical polynomials satisfying the Collet-Eckmann condition. We show that Collet-Eckmann parameters are Lebesgue density points of the complement of the Mandelbrot set (i.e. the connectedness locus).","sentences":["In this paper we study perturbations of complex unicritical polynomials satisfying the Collet-Eckmann condition.","We show that Collet-Eckmann parameters are Lebesgue density points of the complement of the Mandelbrot set (i.e. the connectedness locus)."],"url":"http://arxiv.org/abs/2402.19256v1","category":"math.DS"}
{"created":"2024-02-29 15:19:58","title":"Feedback cooling a levitated nanoparticle's libration to below 100 phonons","abstract":"Macroscopic rotors are interesting model systems to test quantum theory and for quantum sensing. A promising approach for bringing these systems to the quantum regime is to combine sensitive detection with feedback cooling to reduce the thermal occupation of the mechanics. Here, we implement a backward-scattering scheme to efficiently detect all three libration modes of an optically levitated nanoparticle. We demonstrate parametric feedback cooling of all three libration degrees of freedom to below 16~mK, with one of the modes reaching the temperature of 1.3~mK, corresponding to a mean phonon number of 84. Finally, we characterize the backward-scattering scheme by determining its measurement efficiency to be 0.5\\%.","sentences":["Macroscopic rotors are interesting model systems to test quantum theory and for quantum sensing.","A promising approach for bringing these systems to the quantum regime is to combine sensitive detection with feedback cooling to reduce the thermal occupation of the mechanics.","Here, we implement a backward-scattering scheme to efficiently detect all three libration modes of an optically levitated nanoparticle.","We demonstrate parametric feedback cooling of all three libration degrees of freedom to below 16~mK, with one of the modes reaching the temperature of 1.3~mK, corresponding to a mean phonon number of 84.","Finally, we characterize the backward-scattering scheme by determining its measurement efficiency to be 0.5\\%."],"url":"http://arxiv.org/abs/2402.19245v1","category":"quant-ph"}
{"created":"2024-02-29 15:18:37","title":"Derivative-enhanced Deep Operator Network","abstract":"Deep operator networks (DeepONets), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs). In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages the derivative information to enhance the prediction accuracy, and provide a more accurate approximation of the derivatives, especially when the training data are limited. DE-DeepONet incorporates dimension reduction of input into DeepONet and includes two types of derivative labels in the loss function for training, that is, the directional derivatives of the output function with respect to the input function and the gradient of the output function with respect to the physical domain variables. We test DE-DeepONet on three different equations with increasing complexity to demonstrate its effectiveness compared to the vanilla DeepONet.","sentences":["Deep operator networks (DeepONets), a class of neural operators that learn mappings between function spaces, have recently been developed as surrogate models for parametric partial differential equations (PDEs).","In this work we propose a derivative-enhanced deep operator network (DE-DeepONet), which leverages the derivative information to enhance the prediction accuracy, and provide a more accurate approximation of the derivatives, especially when the training data are limited.","DE-DeepONet incorporates dimension reduction of input into DeepONet and includes two types of derivative labels in the loss function for training, that is, the directional derivatives of the output function with respect to the input function and the gradient of the output function with respect to the physical domain variables.","We test DE-DeepONet on three different equations with increasing complexity to demonstrate its effectiveness compared to the vanilla DeepONet."],"url":"http://arxiv.org/abs/2402.19242v1","category":"cs.LG"}
{"created":"2024-02-29 15:17:25","title":"Open Quantum System Approaches to Superconducting Qubits","abstract":"Random and uncontrollable noises from the environment during the design and measurement of superconducting qubits lead to limitations in qubit coherence time and gate fidelity, which is a major challenge in the current state of the art for superconducting quantum computing. To advance superconducting qubits technologies it is essential to understand and mitigate environmentally induced errors. This requires modeling superconducting qubits as open quantum systems coupled to their surroundings. The present study aims to provide useful open quantum system approaches to analyze and quantify the interaction between the superconducting qubits and their environment. We provide an accessible introduction to open quantum systems for newcomers to the field. For experts we discuss recently developed methods for analyzing qubit dynamics under realistic noises. We outline how these techniques provide quantitative insights into the decoherence mechanism and how they can guide design improvements to enhance qubits' coherence time. This self-contained review of open quantum system approaches can be used to model, understand, and improve superconducting qubit performance in the presence of unavoidable environmental noises.","sentences":["Random and uncontrollable noises from the environment during the design and measurement of superconducting qubits lead to limitations in qubit coherence time and gate fidelity, which is a major challenge in the current state of the art for superconducting quantum computing.","To advance superconducting qubits technologies it is essential to understand and mitigate environmentally induced errors.","This requires modeling superconducting qubits as open quantum systems coupled to their surroundings.","The present study aims to provide useful open quantum system approaches to analyze and quantify the interaction between the superconducting qubits and their environment.","We provide an accessible introduction to open quantum systems for newcomers to the field.","For experts we discuss recently developed methods for analyzing qubit dynamics under realistic noises.","We outline how these techniques provide quantitative insights into the decoherence mechanism and how they can guide design improvements to enhance qubits' coherence time.","This self-contained review of open quantum system approaches can be used to model, understand, and improve superconducting qubit performance in the presence of unavoidable environmental noises."],"url":"http://arxiv.org/abs/2402.19241v1","category":"quant-ph"}
{"created":"2024-02-29 15:04:34","title":"A Simple and Efficient Joint Measurement Strategy for Estimating Fermionic Observables and Hamiltonians","abstract":"We propose a simple scheme to estimate fermionic observables and Hamiltonians relevant in quantum chemistry and correlated fermionic systems. Our approach is based on implementing a measurement that jointly measures noisy versions of any product of two or four Majorana operators in an $N$ mode fermionic system. To realize our measurement we use: (i) a randomization over a set of unitaries that realize products of Majorana fermion operators; (ii) a unitary, sampled at random from a constant-size set of suitably chosen fermionic Gaussian unitaries; (iii) a measurement of fermionic occupation numbers; (iv) suitable post-processing. Our scheme can estimate expectation values of all quadratic and quartic Majorana monomials to $\\epsilon$ precision using $\\mathcal{O}(N \\log(N)/\\epsilon^2)$ and $\\mathcal{O}(N^2 \\log(N)/\\epsilon^2)$ measurement rounds respectively, matching the performance offered by fermionic shadow tomography. In certain settings, such as a rectangular lattice of qubits which encode an $N$ mode fermionic system via the Jordan-Wigner transformation, our scheme can be implemented in circuit depth $\\mathcal{O}(N^{1/2})$ with $\\mathcal{O}(N^{3/2})$ two-qubit gates, offering an improvement over fermionic and matchgate classical shadows that require depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ two-qubit gates. We also benchmark our method on molecular Hamiltonians and observe performances comparable to those offered by fermionic classical shadows.","sentences":["We propose a simple scheme to estimate fermionic observables and Hamiltonians relevant in quantum chemistry and correlated fermionic systems.","Our approach is based on implementing a measurement that jointly measures noisy versions of any product of two or four Majorana operators in an $N$ mode fermionic system.","To realize our measurement we use: (i) a randomization over a set of unitaries that realize products of Majorana fermion operators; (ii) a unitary, sampled at random from a constant-size set of suitably chosen fermionic Gaussian unitaries; (iii) a measurement of fermionic occupation numbers; (iv) suitable post-processing.","Our scheme can estimate expectation values of all quadratic and quartic Majorana monomials to $\\epsilon$ precision using $\\mathcal{O}(N \\log(N)/\\epsilon^2)$ and $\\mathcal{O}(N^2 \\log(N)/\\epsilon^2)$ measurement rounds respectively, matching the performance offered by fermionic shadow tomography.","In certain settings, such as a rectangular lattice of qubits which encode an $N$ mode fermionic system via the Jordan-Wigner transformation, our scheme can be implemented in circuit depth $\\mathcal{O}(N^{1/2})$ with $\\mathcal{O}(N^{3/2})$ two-qubit gates, offering an improvement over fermionic and matchgate classical shadows that require depth $\\mathcal{O}(N)$ and $\\mathcal{O}(N^2)$ two-qubit gates.","We also benchmark our method on molecular Hamiltonians and observe performances comparable to those offered by fermionic classical shadows."],"url":"http://arxiv.org/abs/2402.19230v1","category":"quant-ph"}
{"created":"2024-02-29 14:53:55","title":"Airport take-off and landing optimization through genetic algorithms","abstract":"This research addresses the crucial issue of pollution from aircraft operations, focusing on optimizing both gate allocation and runway scheduling simultaneously, a novel approach not previously explored. The study presents an innovative genetic algorithm-based method for minimizing pollution from fuel combustion during aircraft take-off and landing at airports. This algorithm uniquely integrates the optimization of both landing gates and take-off/landing runways, considering the correlation between engine operation time and pollutant levels. The approach employs advanced constraint handling techniques to manage the intricate time and resource limitations inherent in airport operations. Additionally, the study conducts a thorough sensitivity analysis of the model, with a particular emphasis on the mutation factor and the type of penalty function, to fine-tune the optimization process. This dual-focus optimization strategy represents a significant advancement in reducing environmental impact in the aviation sector, establishing a new standard for comprehensive and efficient airport operation management.","sentences":["This research addresses the crucial issue of pollution from aircraft operations, focusing on optimizing both gate allocation and runway scheduling simultaneously, a novel approach not previously explored.","The study presents an innovative genetic algorithm-based method for minimizing pollution from fuel combustion during aircraft take-off and landing at airports.","This algorithm uniquely integrates the optimization of both landing gates and take-off/landing runways, considering the correlation between engine operation time and pollutant levels.","The approach employs advanced constraint handling techniques to manage the intricate time and resource limitations inherent in airport operations.","Additionally, the study conducts a thorough sensitivity analysis of the model, with a particular emphasis on the mutation factor and the type of penalty function, to fine-tune the optimization process.","This dual-focus optimization strategy represents a significant advancement in reducing environmental impact in the aviation sector, establishing a new standard for comprehensive and efficient airport operation management."],"url":"http://arxiv.org/abs/2402.19222v1","category":"cs.NE"}
{"created":"2024-02-29 14:42:07","title":"Classification of permanence and impermanence for a Lotka-Volterra model of three competing species with seasonal succession","abstract":"In this paper, we are concerned with the permanence of a Lotka-Volterra model of three competing species with seasonal succession. Based on the existence of a carrying simplex, that is a globally attracting hypersurface of codimension one, we provide a complete classification of the permanence and impermanence in terms of inequalities on the parameters of this model. Moreover, we numerically show that invariant closed curves can occur in the permanent classes, which means that the positive fixed point of the associated Poincare map in the permanent classes is not always globally asymptotically stable.","sentences":["In this paper, we are concerned with the permanence of a Lotka-Volterra model of three competing species with seasonal succession.","Based on the existence of a carrying simplex, that is a globally attracting hypersurface of codimension one, we provide a complete classification of the permanence and impermanence in terms of inequalities on the parameters of this model.","Moreover, we numerically show that invariant closed curves can occur in the permanent classes, which means that the positive fixed point of the associated Poincare map in the permanent classes is not always globally asymptotically stable."],"url":"http://arxiv.org/abs/2402.19213v1","category":"math.DS"}
{"created":"2024-02-29 14:41:31","title":"Deep Reinforcement Learning: A Convex Optimization Approach","abstract":"In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces. We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function. The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode. For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters. In particular, if the regularization parameter is $\\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\\star$ is bounded by $\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to infinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le C\\cdot\\frac{\\rho}{T}.\\] In particular, our algorithm converges arbitrarily close to the optimal neural network parameters as the time horizon increases or as the regularization parameter decreases.","sentences":["In this paper, we consider reinforcement learning of nonlinear systems with continuous state and action spaces.","We present an episodic learning algorithm, where we for each episode use convex optimization to find a two-layer neural network approximation of the optimal $Q$-function.","The convex optimization approach guarantees that the weights calculated at each episode are optimal, with respect to the given sampled states and actions of the current episode.","For stable nonlinear systems, we show that the algorithm converges and that the converging parameters of the trained neural network can be made arbitrarily close to the optimal neural network parameters.","In particular, if the regularization parameter is $\\rho$ and the time horizon is $T$, then the parameters of the trained neural network converge to $w$, where the distance between $w$ from the optimal parameters $w^\\star$ is bounded by $\\mathcal{O}(\\rho T^{-1})$. That is, when the number of episodes goes to infinity, there exists a constant $C$ such that \\[\\|w-w^\\star\\| \\le C\\cdot\\frac{\\rho}{T}.\\]","In particular, our algorithm converges arbitrarily close to the optimal neural network parameters as the time horizon increases or as the regularization parameter decreases."],"url":"http://arxiv.org/abs/2402.19212v1","category":"math.OC"}
{"created":"2024-02-29 14:31:19","title":"Boom and bust cycles due to pseudospectra of matrices with unimodular spectra","abstract":"We discuss dynamics obtained by increasing powers of non-normal matrices that are roots of the identity, and therefore have all eigenvalues on the unit circle. Naively, one would expect that the expectation value of such powers cannot grow as one increases the power. We demonstrate that, rather counterintuitively, a completely opposite behavior is possible. In the limit of infinitely large matrices one can have an exponential growth. For finite matrices this exponential growth is a part of repeating cycles of exponential growths followed by exponential decays. The effect can occur if the spectrum is different than the pseudospectrum, with the exponential growth rate being given by the pseudospectrum. We show that this effect appears in a class of transfer matrices appearing in studies of two-dimensional non-interacting systems, for a matrix describing the Ehrenfest urn, as well as in previously observed purity dynamics in a staircase random circuit.","sentences":["We discuss dynamics obtained by increasing powers of non-normal matrices that are roots of the identity, and therefore have all eigenvalues on the unit circle.","Naively, one would expect that the expectation value of such powers cannot grow as one increases the power.","We demonstrate that, rather counterintuitively, a completely opposite behavior is possible.","In the limit of infinitely large matrices one can have an exponential growth.","For finite matrices this exponential growth is a part of repeating cycles of exponential growths followed by exponential decays.","The effect can occur if the spectrum is different than the pseudospectrum, with the exponential growth rate being given by the pseudospectrum.","We show that this effect appears in a class of transfer matrices appearing in studies of two-dimensional non-interacting systems, for a matrix describing the Ehrenfest urn, as well as in previously observed purity dynamics in a staircase random circuit."],"url":"http://arxiv.org/abs/2402.19201v1","category":"math-ph"}
{"created":"2024-02-29 14:28:32","title":"Heat-charge separation in a hybrid superconducting quantum Hall setup","abstract":"Separating heat from charge in a material is an extremely challenging task since they are transported by the very same carriers, i.e. electrons or holes. In this Letter we show that such separation can reach 100% efficiency in a hybrid superconducting quantum Hall setup, provided that the quantum Hall system is tuned to integer filling factor. We present microscopic calculations for a three-terminal setup to illustrate our idea.","sentences":["Separating heat from charge in a material is an extremely challenging task since they are transported by the very same carriers, i.e. electrons or holes.","In this Letter we show that such separation can reach 100% efficiency in a hybrid superconducting quantum Hall setup, provided that the quantum Hall system is tuned to integer filling factor.","We present microscopic calculations for a three-terminal setup to illustrate our idea."],"url":"http://arxiv.org/abs/2402.19198v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 14:23:02","title":"Long-range translational order and hyperuniformity in two-dimensional chiral active crystal","abstract":"We numerically study two-dimensional athermal chiral active particles at high densities. The particles in this system perform the circular motion with frequency $\\Omega$. The system is known to undergo a nonequilibrium transition from the absorbing phase to the diffusing fluid phase that is accompanied by the suppression of density fluctuations called hyperuniformity [Q.-L. Lei et al., Sci. Adv. 5, eaau7423 (2019)]. We show that the system in the fluid phase crystallizes with increasing density. Surprisingly, the resulting crystal possesses long-range translational order even in two dimensions due to the suppression of the long-wavelength displacement fluctuations associated with hyperuniformity. We also find that $\\Omega = 0$ is singular, and the system never crystallizes because, in this limit, the system can be regarded as a quenched random system for which the lower critical dimension is known to be $4$. Our numerical results are quantitatively explained by a linear elastic theory.","sentences":["We numerically study two-dimensional athermal chiral active particles at high densities.","The particles in this system perform the circular motion with frequency $\\Omega$. The system is known to undergo a nonequilibrium transition from the absorbing phase to the diffusing fluid phase that is accompanied by the suppression of density fluctuations called hyperuniformity [Q.-L. Lei et al., Sci. Adv. 5, eaau7423 (2019)].","We show that the system in the fluid phase crystallizes with increasing density.","Surprisingly, the resulting crystal possesses long-range translational order even in two dimensions due to the suppression of the long-wavelength displacement fluctuations associated with hyperuniformity.","We also find that $\\Omega = 0$ is singular, and the system never crystallizes because, in this limit, the system can be regarded as a quenched random system for which the lower critical dimension is known to be $4$. Our numerical results are quantitatively explained by a linear elastic theory."],"url":"http://arxiv.org/abs/2402.19192v1","category":"cond-mat.soft"}
{"created":"2024-02-29 14:14:57","title":"Link Recommendation to Augment Influence Diffusion with Provable Guarantees","abstract":"Link recommendation systems in online social networks (OSNs), such as Facebook's ``People You May Know'', Twitter's ``Who to Follow'', and Instagram's ``Suggested Accounts'', facilitate the formation of new connections among users. This paper addresses the challenge of link recommendation for the purpose of social influence maximization. In particular, given a graph $G$ and the seed set $S$, our objective is to select $k$ edges that connect seed nodes and ordinary nodes to optimize the influence dissemination of the seed set. This problem, referred to as influence maximization with augmentation (IMA), has been proven to be NP-hard.   In this paper, we propose an algorithm, namely \\textsf{AIS}, consisting of an efficient estimator for augmented influence estimation and an accelerated sampling approach. \\textsf{AIS} provides a $(1-1/\\mathrm{e}-\\varepsilon)$-approximate solution with a high probability of $1-\\delta$, and runs in $O(k^2 (m+n) \\log (n / \\delta) / \\varepsilon^2 + k \\left|E_{\\mathcal{C}}\\right|)$ time assuming that the influence of any singleton node is smaller than that of the seed set. To the best of our knowledge, this is the first algorithm that can be implemented on large graphs containing millions of nodes while preserving strong theoretical guarantees. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithm.","sentences":["Link recommendation systems in online social networks (OSNs), such as Facebook's ``People You May Know'', Twitter's ``Who to Follow'', and Instagram's ``Suggested Accounts'', facilitate the formation of new connections among users.","This paper addresses the challenge of link recommendation for the purpose of social influence maximization.","In particular, given a graph $G$ and the seed set $S$, our objective is to select $k$ edges that connect seed nodes and ordinary nodes to optimize the influence dissemination of the seed set.","This problem, referred to as influence maximization with augmentation (IMA), has been proven to be NP-hard.   ","In this paper, we propose an algorithm, namely \\textsf{AIS}, consisting of an efficient estimator for augmented influence estimation and an accelerated sampling approach.","\\textsf{AIS} provides a $(1-1/\\mathrm{e}-\\varepsilon)$-approximate solution with a high probability of $1-\\delta$, and runs in $O(k^2 (m+n) \\log (n / \\delta) / \\varepsilon^2 + k \\left|E_{\\mathcal{C}}\\right|)$ time assuming that the influence of any singleton node is smaller than that of the seed set.","To the best of our knowledge, this is the first algorithm that can be implemented on large graphs containing millions of nodes while preserving strong theoretical guarantees.","We conduct extensive experiments to demonstrate the effectiveness and efficiency of our proposed algorithm."],"url":"http://arxiv.org/abs/2402.19189v1","category":"cs.SI"}
{"created":"2024-02-29 14:13:18","title":"Controlling polymer translocation with crowded medium and polymer length asymmetry","abstract":"Polymer translocation in crowded environments is a ubiquitous phenomenon in biological systems. We studied polymer translocation through a pore in free, one-sided (asymmetric), and two-sided (symmetric) crowded environments. Extensive Langevin dynamics simulation is employed to model the dynamics of the flexible polymer and crowding particles. We studied how crowding size and packing fraction play a crucial role in the translocation process. After determining the standard scaling properties of the translocation probability, time, and MSD, we observed that the translocation rate and bead velocities are location-dependent as we move along the polymer backbone, even in a crowd-free environment. Counter-intuitively, translocation rate and bead velocities showed the opposite behavior; for example, middle monomers near the pore exhibit maximum bead velocity and minimum translocation rate. Free energy calculation for asymmetrically placed polymer indicates there exists a critical number of segments that make the polymer to prefer the receiver side for translocation. For one-sided crowding, we have identified a critical crowding size above which there exists a non-zero probability to translocate into the crowding side instead of the free side. Moreover, we have observed that shifting the polymer towards the crowded side compensates for one-sided crowding, yielding an equal probability akin to a crowder-free system. Under two-sided crowding, the mechanism of how a slight variation in crowder size and packing fraction can force a polymer to switch its translocation direction is proposed, which has not been explored before. Using this control we achieved an equal translocation probability like a crowd-free scenario. These conspicuous yet counter-intuitive phenomena are rationalized by simple theoretical arguments based on osmotic pressure and radial entropic forces.","sentences":["Polymer translocation in crowded environments is a ubiquitous phenomenon in biological systems.","We studied polymer translocation through a pore in free, one-sided (asymmetric), and two-sided (symmetric) crowded environments.","Extensive Langevin dynamics simulation is employed to model the dynamics of the flexible polymer and crowding particles.","We studied how crowding size and packing fraction play a crucial role in the translocation process.","After determining the standard scaling properties of the translocation probability, time, and MSD, we observed that the translocation rate and bead velocities are location-dependent as we move along the polymer backbone, even in a crowd-free environment.","Counter-intuitively, translocation rate and bead velocities showed the opposite behavior; for example, middle monomers near the pore exhibit maximum bead velocity and minimum translocation rate.","Free energy calculation for asymmetrically placed polymer indicates there exists a critical number of segments that make the polymer to prefer the receiver side for translocation.","For one-sided crowding, we have identified a critical crowding size above which there exists a non-zero probability to translocate into the crowding side instead of the free side.","Moreover, we have observed that shifting the polymer towards the crowded side compensates for one-sided crowding, yielding an equal probability akin to a crowder-free system.","Under two-sided crowding, the mechanism of how a slight variation in crowder size and packing fraction can force a polymer to switch its translocation direction is proposed, which has not been explored before.","Using this control we achieved an equal translocation probability like a crowd-free scenario.","These conspicuous yet counter-intuitive phenomena are rationalized by simple theoretical arguments based on osmotic pressure and radial entropic forces."],"url":"http://arxiv.org/abs/2402.19187v1","category":"cond-mat.soft"}
{"created":"2024-02-29 14:02:25","title":"Structure of Periodic Orbit Families in the Hill Restricted 4-Body Problem","abstract":"The Hill Restricted 4-Body Problem (HR4BP) is a coherent time-periodic model that can be used to represent motion in the Sun-Earth-Moon (SEM) system. Periodic orbits were computed in this model to better understand the periodic orbit family structures that exist in these types of systems. First, periodic orbits in the Circular Restricted 3-Body Problem (CR3BP) representation of the Earth-Moon (EM) system were identified. A Melnikov-type function was used to identify a set of candidate points on the EM CR3BP periodic orbits to start a continuation algorithm. A pseudo-arclength continuation scheme was then used to obtain the corresponding periodic orbit families in the HR4BP when including the effect of the Sun. Bifurcation points were identified in the computed families to obtain additional orbit families.","sentences":["The Hill Restricted 4-Body Problem (HR4BP) is a coherent time-periodic model that can be used to represent motion in the Sun-Earth-Moon (SEM) system.","Periodic orbits were computed in this model to better understand the periodic orbit family structures that exist in these types of systems.","First, periodic orbits in the Circular Restricted 3-Body Problem (CR3BP) representation of the Earth-Moon (EM) system were identified.","A Melnikov-type function was used to identify a set of candidate points on the EM CR3BP periodic orbits to start a continuation algorithm.","A pseudo-arclength continuation scheme was then used to obtain the corresponding periodic orbit families in the HR4BP when including the effect of the Sun.","Bifurcation points were identified in the computed families to obtain additional orbit families."],"url":"http://arxiv.org/abs/2402.19181v1","category":"math.DS"}
{"created":"2024-02-29 13:57:09","title":"Deep Mapper Graph and its Application to Visualize Plausible Pathways on High-Dimensional Distribution with Small Time-Complexity","abstract":"Mapper is a topology based data analysis method that extracts topological features from high-dimensional data. The Mapper algorithm requires a filter function that maps the dataset to a Euclidian space and a clustering method, that is performed on the original dataset. This produces a graph which represents the shape of the original data. In this work, we use Mapper to uncover the conformational change of protein structures and we choose the filter function from a parameterized family, based on a deep neural network architecture. By optimizing its parameters with respect to an Energy-loss function derived from a theoretical background, we produce a Mapper graph that unveils the conformational pathways undertaken by the studied protein. Our method tackles conformational pathway detection in a unsupervised manner and therefore greatly reduces the manual and time costs necessary in this task.","sentences":["Mapper is a topology based data analysis method that extracts topological features from high-dimensional data.","The Mapper algorithm requires a filter function that maps the dataset to a Euclidian space and a clustering method, that is performed on the original dataset.","This produces a graph which represents the shape of the original data.","In this work, we use Mapper to uncover the conformational change of protein structures and we choose the filter function from a parameterized family, based on a deep neural network architecture.","By optimizing its parameters with respect to an Energy-loss function derived from a theoretical background, we produce a Mapper graph that unveils the conformational pathways undertaken by the studied protein.","Our method tackles conformational pathway detection in a unsupervised manner and therefore greatly reduces the manual and time costs necessary in this task."],"url":"http://arxiv.org/abs/2402.19177v1","category":"q-bio.QM"}
{"created":"2024-02-29 13:56:35","title":"Proximal Dogleg Opportunistic Majorization for Nonconvex and Nonsmooth Optimization","abstract":"We consider minimizing a function consisting of a quadratic term and a proximable term which is possibly nonconvex and nonsmooth. This problem is also known as scaled proximal operator. Despite its simple form, existing methods suffer from slow convergence or high implementation complexity or both. To overcome these limitations, we develop a fast and user-friendly second-order proximal algorithm. Key innovation involves building and solving a series of opportunistically majorized problems along a hybrid Newton direction. The approach directly uses the precise Hessian of the quadratic term, and calculates the inverse only once, eliminating the iterative numerical approximation of the Hessian, a common practice in quasi-Newton methods. The algorithm's convergence to a critical point is established, and local convergence rate is derived based on the Kurdyka-Lojasiewicz property of the objective function. Numerical comparisons are conducted on well-known optimization problems. The results demonstrate that the proposed algorithm not only achieves a faster convergence but also tends to converge to a better local optimum compare to benchmark algorithms.","sentences":["We consider minimizing a function consisting of a quadratic term and a proximable term which is possibly nonconvex and nonsmooth.","This problem is also known as scaled proximal operator.","Despite its simple form, existing methods suffer from slow convergence or high implementation complexity or both.","To overcome these limitations, we develop a fast and user-friendly second-order proximal algorithm.","Key innovation involves building and solving a series of opportunistically majorized problems along a hybrid Newton direction.","The approach directly uses the precise Hessian of the quadratic term, and calculates the inverse only once, eliminating the iterative numerical approximation of the Hessian, a common practice in quasi-Newton methods.","The algorithm's convergence to a critical point is established, and local convergence rate is derived based on the Kurdyka-Lojasiewicz property of the objective function.","Numerical comparisons are conducted on well-known optimization problems.","The results demonstrate that the proposed algorithm not only achieves a faster convergence but also tends to converge to a better local optimum compare to benchmark algorithms."],"url":"http://arxiv.org/abs/2402.19176v1","category":"math.OC"}
{"created":"2024-02-29 13:50:54","title":"Disturbance Decoupling Problem for $n$-link chain pendulum on a cart system","abstract":"A disturbance decoupling problem for a $n$-link chain pendulum on a cart is considered. A model of the cart developed in a coordinate-free framework and the linearized equations of this system are considered from [1]. It is shown that it is possible to design a suitable state feedback such that the angular position or velocity of the $n^{th}$-link can always be decoupled from the disturbance coming at the cart.","sentences":["A disturbance decoupling problem for a $n$-link chain pendulum on a cart is considered.","A model of the cart developed in a coordinate-free framework and the linearized equations of this system are considered from [1].","It is shown that it is possible to design a suitable state feedback such that the angular position or velocity of the $n^{th}$-link can always be decoupled from the disturbance coming at the cart."],"url":"http://arxiv.org/abs/2402.19168v1","category":"eess.SY"}
{"created":"2024-02-29 13:41:34","title":"Tailor-designed models for the turbulent velocity gradient through normalizing flow","abstract":"Small-scale turbulence can be comprehensively described in terms of velocity gradients, which makes them an appealing starting point for low-dimensional modeling. Typical models consist of stochastic equations based on closures for non-local pressure and viscous contributions. The fidelity of the resulting models depends on the accuracy of the underlying modeling assumptions. Here, we discuss an alternative data-driven approach leveraging machine learning to derive a velocity gradient model which captures its statistics by construction. We use a normalizing flow to learn the velocity gradient probability density function (PDF) from direct numerical simulation (DNS) of incompressible turbulence. Then, by using the equation for the single-time PDF of the velocity gradient, we construct a deterministic, yet chaotic, dynamical system featuring the learned steady-state PDF by design. Finally, utilizing gauge terms for the velocity gradient single-time statistics, we optimize the time correlations as obtained from our model against the DNS data. As a result, the model time realizations statistically closely resemble the time series from DNS.","sentences":["Small-scale turbulence can be comprehensively described in terms of velocity gradients, which makes them an appealing starting point for low-dimensional modeling.","Typical models consist of stochastic equations based on closures for non-local pressure and viscous contributions.","The fidelity of the resulting models depends on the accuracy of the underlying modeling assumptions.","Here, we discuss an alternative data-driven approach leveraging machine learning to derive a velocity gradient model which captures its statistics by construction.","We use a normalizing flow to learn the velocity gradient probability density function (PDF) from direct numerical simulation (DNS) of incompressible turbulence.","Then, by using the equation for the single-time PDF of the velocity gradient, we construct a deterministic, yet chaotic, dynamical system featuring the learned steady-state PDF by design.","Finally, utilizing gauge terms for the velocity gradient single-time statistics, we optimize the time correlations as obtained from our model against the DNS data.","As a result, the model time realizations statistically closely resemble the time series from DNS."],"url":"http://arxiv.org/abs/2402.19158v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 13:37:25","title":"Bialy-Mironov type rigidity for centrally symmetric symplectic billiards","abstract":"The aim of the present paper is to establish a Bialy-Mironov type rigidity for centrally symmetric symplectic billiards. For a centrally symmetric $C^2$ strongly-convex domain $D$ with boundary $\\partial D$, assume that the symplectic billiard map has a (simple) continuous invariant curve $\\delta \\subset \\mathcal{P}$ of rotation number $1/4$ (winding once around $\\partial D$) and consisting only of $4$-periodic orbits. If one of the parts between $\\delta$ and each boundary of the phase-space is entirely foliated by continuous invariant closed (not null-homotopic) curves, then $\\partial D$ is an ellipse. The differences with Birkhoff billiards are essentially two: it is possible to assume the existence of the foliation in one of the parts of the phase-space detected by the curve $\\delta$, and the result is obtained by tracing back the problem directly to the totally integrable case.","sentences":["The aim of the present paper is to establish a Bialy-Mironov type rigidity for centrally symmetric symplectic billiards.","For a centrally symmetric $C^2$ strongly-convex domain $D$ with boundary $\\partial D$, assume that the symplectic billiard map has a (simple) continuous invariant curve $\\delta \\subset \\mathcal{P}$ of rotation number $1/4$ (winding once around $\\partial D$) and consisting only of $4$-periodic orbits.","If one of the parts between $\\delta$ and each boundary of the phase-space is entirely foliated by continuous invariant closed (not null-homotopic) curves, then $\\partial D$ is an ellipse.","The differences with Birkhoff billiards are essentially two: it is possible to assume the existence of the foliation in one of the parts of the phase-space detected by the curve $\\delta$, and the result is obtained by tracing back the problem directly to the totally integrable case."],"url":"http://arxiv.org/abs/2402.19154v1","category":"math.DS"}
{"created":"2024-02-29 13:32:53","title":"Approximations of symbolic substitution systems in one dimension","abstract":"Periodic approximations of quasicrystals are a powerful tool in analyzing spectra of Schr\\\"odinger operators arising from quasicrystals, given the known theory for periodic crystals. Namely, we seek periodic operators whose spectra approximate the spectrum of the limiting operator (of the quasicrystal). This naturally leads to study the convergence of the underlying dynamical systems. We treat dynamical systems which are based on one-dimensional substitutions. We first find natural candidates of dynamical subsystems to approximate the substitution dynamical system. Subsequently, we offer a characterization of their convergence and provide estimates for the rate of convergence. We apply the proposed theory to some guiding examples.","sentences":["Periodic approximations of quasicrystals are a powerful tool in analyzing spectra of Schr\\\"odinger operators arising from quasicrystals, given the known theory for periodic crystals.","Namely, we seek periodic operators whose spectra approximate the spectrum of the limiting operator (of the quasicrystal).","This naturally leads to study the convergence of the underlying dynamical systems.","We treat dynamical systems which are based on one-dimensional substitutions.","We first find natural candidates of dynamical subsystems to approximate the substitution dynamical system.","Subsequently, we offer a characterization of their convergence and provide estimates for the rate of convergence.","We apply the proposed theory to some guiding examples."],"url":"http://arxiv.org/abs/2402.19151v1","category":"math.DS"}
{"created":"2024-02-29 13:31:41","title":"Experimental Test of Quantum Nonlocality from Contextuality","abstract":"There are two powerful arguments against the possibility of extending quantum mechanics, the violation of Bell inequalities and the Kochen-Specker theorem, but the connection between the two remains confused. Following the distinctive strategy proposed by Cabello [Phys. Rev. Lett. 127, 070401 (2021)], Bell inequalities can be violated by state-independent contextuality sets. However, the experimental realization of such ideas is challenging as it requires high-dimensional entanglement. Orbital angular momentum provides an unlimited state space and the number of effective dimensions can be readily tailored as required. We performed an experimental test of non-locality based on Bell inequalities from contextuality, using orbital angular momentum entanglement in a bipartite photonic system. Our experiment not only shows a new way to produce non-locality but also connects contextuality and non-locality, two fundamental quantum resources that are critical for quantum computation and secure communication tasks.","sentences":["There are two powerful arguments against the possibility of extending quantum mechanics, the violation of Bell inequalities and the Kochen-Specker theorem, but the connection between the two remains confused.","Following the distinctive strategy proposed by Cabello [Phys. Rev. Lett.","127, 070401 (2021)], Bell inequalities can be violated by state-independent contextuality sets.","However, the experimental realization of such ideas is challenging as it requires high-dimensional entanglement.","Orbital angular momentum provides an unlimited state space and the number of effective dimensions can be readily tailored as required.","We performed an experimental test of non-locality based on Bell inequalities from contextuality, using orbital angular momentum entanglement in a bipartite photonic system.","Our experiment not only shows a new way to produce non-locality but also connects contextuality and non-locality, two fundamental quantum resources that are critical for quantum computation and secure communication tasks."],"url":"http://arxiv.org/abs/2402.19149v1","category":"quant-ph"}
{"created":"2024-02-29 13:06:34","title":"Maps preserving ascent/descent of triple Jordan product","abstract":"Let $\\mathcal{X}$ be a real or complex Banach space with $ \\dim \\mathcal{X}\\geq 3$. We give a complete description of surjective mappings on $\\mathcal{B(X)}$ that preserve the ascent of Jordan triple product of operators or, preserve the descent of Jordan triple product of operators.","sentences":["Let $\\mathcal{X}$ be a real or complex Banach space with $ \\dim \\mathcal{X}\\geq 3$.","We give a complete description of surjective mappings on $\\mathcal{B(X)}$ that preserve the ascent of Jordan triple product of operators or, preserve the descent of Jordan triple product of operators."],"url":"http://arxiv.org/abs/2402.19130v1","category":"math.FA"}
{"created":"2024-02-29 13:06:14","title":"ARMCHAIR: integrated inverse reinforcement learning and model predictive control for human-robot collaboration","abstract":"One of the key issues in human-robot collaboration is the development of computational models that allow robots to predict and adapt to human behavior. Much progress has been achieved in developing such models, as well as control techniques that address the autonomy problems of motion planning and decision-making in robotics. However, the integration of computational models of human behavior with such control techniques still poses a major challenge, resulting in a bottleneck for efficient collaborative human-robot teams. In this context, we present a novel architecture for human-robot collaboration: Adaptive Robot Motion for Collaboration with Humans using Adversarial Inverse Reinforcement learning (ARMCHAIR). Our solution leverages adversarial inverse reinforcement learning and model predictive control to compute optimal trajectories and decisions for a mobile multi-robot system that collaborates with a human in an exploration task. During the mission, ARMCHAIR operates without human intervention, autonomously identifying the necessity to support and acting accordingly. Our approach also explicitly addresses the network connectivity requirement of the human-robot team. Extensive simulation-based evaluations demonstrate that ARMCHAIR allows a group of robots to safely support a simulated human in an exploration scenario, preventing collisions and network disconnections, and improving the overall performance of the task.","sentences":["One of the key issues in human-robot collaboration is the development of computational models that allow robots to predict and adapt to human behavior.","Much progress has been achieved in developing such models, as well as control techniques that address the autonomy problems of motion planning and decision-making in robotics.","However, the integration of computational models of human behavior with such control techniques still poses a major challenge, resulting in a bottleneck for efficient collaborative human-robot teams.","In this context, we present a novel architecture for human-robot collaboration: Adaptive Robot Motion for Collaboration with Humans using Adversarial Inverse Reinforcement learning (ARMCHAIR).","Our solution leverages adversarial inverse reinforcement learning and model predictive control to compute optimal trajectories and decisions for a mobile multi-robot system that collaborates with a human in an exploration task.","During the mission, ARMCHAIR operates without human intervention, autonomously identifying the necessity to support and acting accordingly.","Our approach also explicitly addresses the network connectivity requirement of the human-robot team.","Extensive simulation-based evaluations demonstrate that ARMCHAIR allows a group of robots to safely support a simulated human in an exploration scenario, preventing collisions and network disconnections, and improving the overall performance of the task."],"url":"http://arxiv.org/abs/2402.19128v1","category":"cs.RO"}
{"created":"2024-02-29 13:02:59","title":"Selective Rotation and Attractive Persistent Currents in Anti-Dipolar Ring Supersolids","abstract":"A repulsively interacting Bose-Einstein condensate on a ring is well known to show persistent currents. For attractive interactions, however, a bound state may form that renders the rotation classical. Here we show that in a multiply-connected confinement, the strong in-plane attraction of an {\\it anti-dipolar }condensate can form stacks of ring-shaped droplets which may coherently overlap to form a supersolid along the azimuthal symmetry axis of the system. Intriguingly, the functional behavior of the energy-angular momentum dispersion of the anti-dipolar ring condensate differs from that of a usual repulsive superfluid. The periodic maxima between persistent flow and the non-rotating ground state flatten significantly and the typical pronounced cusps in the energy dispersion also occur in the rotationally symmetric supersolid state. A weak link results in the reduction of this minimum, shifting it to smaller angular momenta. With an asymmetric link potential one can selectively induce superfluid and rigid-body rotation in different layers within the same system. This intriguing setup offers new perspectives for atomtronics applications.","sentences":["A repulsively interacting Bose-Einstein condensate on a ring is well known to show persistent currents.","For attractive interactions, however, a bound state may form that renders the rotation classical.","Here we show that in a multiply-connected confinement, the strong in-plane attraction of an {\\it anti-dipolar }condensate can form stacks of ring-shaped droplets which may coherently overlap to form a supersolid along the azimuthal symmetry axis of the system.","Intriguingly, the functional behavior of the energy-angular momentum dispersion of the anti-dipolar ring condensate differs from that of a usual repulsive superfluid.","The periodic maxima between persistent flow and the non-rotating ground state flatten significantly and the typical pronounced cusps in the energy dispersion also occur in the rotationally symmetric supersolid state.","A weak link results in the reduction of this minimum, shifting it to smaller angular momenta.","With an asymmetric link potential one can selectively induce superfluid and rigid-body rotation in different layers within the same system.","This intriguing setup offers new perspectives for atomtronics applications."],"url":"http://arxiv.org/abs/2402.19126v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-29 12:57:32","title":"A Naive Approach for Automatic Line-level Code Completion","abstract":"Coding is an integral aspect of programming. A programmer can automatically complete a code fragment after writing a few tokens, and the process of automatic completion is known as code completion. Several research studies on code completion have previously been conducted for method body completion and method parameter completion. However, this fundamental study explores the automatic completion of any program statement that might not even be part of a method.   The goal is to provide suggestions to the programmer for completing code throughout the codebase by identifying and analyzing code similarities. The proposed methodology can be regarded as a fundamental framework for automated code completion. From the investigation of hundreds of revisions of four subject systems written in C and Java, it is observed that the proposed method can automatically complete around 22% of code statements with an average accuracy of 87% that a programmer writes during development, accelerating software development time. The empirical analysis further demonstrates that the approach can be used with programming language neutrality.   The study concludes by illustrating that taking 10 characters as prefixes before invoking completion provides maximum precision.","sentences":["Coding is an integral aspect of programming.","A programmer can automatically complete a code fragment after writing a few tokens, and the process of automatic completion is known as code completion.","Several research studies on code completion have previously been conducted for method body completion and method parameter completion.","However, this fundamental study explores the automatic completion of any program statement that might not even be part of a method.   ","The goal is to provide suggestions to the programmer for completing code throughout the codebase by identifying and analyzing code similarities.","The proposed methodology can be regarded as a fundamental framework for automated code completion.","From the investigation of hundreds of revisions of four subject systems written in C and Java, it is observed that the proposed method can automatically complete around 22% of code statements with an average accuracy of 87% that a programmer writes during development, accelerating software development time.","The empirical analysis further demonstrates that the approach can be used with programming language neutrality.   ","The study concludes by illustrating that taking 10 characters as prefixes before invoking completion provides maximum precision."],"url":"http://arxiv.org/abs/2402.19120v1","category":"cs.SE"}
{"created":"2024-02-29 12:45:24","title":"Unraveling electronic correlations in warm dense quantum plasmas","abstract":"The study of matter at extreme densities and temperatures has emerged as a highly active frontier at the interface of plasma physics, material science and quantum chemistry with direct relevance for planetary modeling and inertial confinement fusion.   A particular feature of such warm dense matter is the complex interplay of strong Coulomb interactions, quantum effects, and thermal excitations, rendering its rigorous theoretical description a formidable challenge. Here, we report a breakthrough in path integral Monte Carlo simulations that allows us to unravel this intricate interplay for light elements without nodal restrictions. This new capability gives us access to electronic correlations previously unattainable. As an example, we apply our method to strongly compressed beryllium to describe x-ray Thomson scattering (XRTS) data obtained at the National Ignition Facility. We find excellent agreement between simulation and experiment. Our analysis shows an unprecedented level of consistency for independent observations without the need for any empirical input parameters.","sentences":["The study of matter at extreme densities and temperatures has emerged as a highly active frontier at the interface of plasma physics, material science and quantum chemistry with direct relevance for planetary modeling and inertial confinement fusion.   ","A particular feature of such warm dense matter is the complex interplay of strong Coulomb interactions, quantum effects, and thermal excitations, rendering its rigorous theoretical description a formidable challenge.","Here, we report a breakthrough in path integral Monte Carlo simulations that allows us to unravel this intricate interplay for light elements without nodal restrictions.","This new capability gives us access to electronic correlations previously unattainable.","As an example, we apply our method to strongly compressed beryllium to describe x-ray Thomson scattering (XRTS) data obtained at the National Ignition Facility.","We find excellent agreement between simulation and experiment.","Our analysis shows an unprecedented level of consistency for independent observations without the need for any empirical input parameters."],"url":"http://arxiv.org/abs/2402.19113v1","category":"physics.plasm-ph"}
{"created":"2024-02-29 12:41:54","title":"Temporal-Aware Deep Reinforcement Learning for Energy Storage Bidding in Energy and Contingency Reserve Markets","abstract":"The battery energy storage system (BESS) has immense potential for enhancing grid reliability and security through its participation in the electricity market. BESS often seeks various revenue streams by taking part in multiple markets to unlock its full potential, but effective algorithms for joint-market participation under price uncertainties are insufficiently explored in the existing research. To bridge this gap, we develop a novel BESS joint bidding strategy that utilizes deep reinforcement learning (DRL) to bid in the spot and contingency frequency control ancillary services (FCAS) markets. Our approach leverages a transformer-based temporal feature extractor to effectively respond to price fluctuations in seven markets simultaneously and helps DRL learn the best BESS bidding strategy in joint-market participation. Additionally, unlike conventional \"black-box\" DRL model, our approach is more interpretable and provides valuable insights into the temporal bidding behavior of BESS in the dynamic electricity market. We validate our method using realistic market prices from the Australian National Electricity Market. The results show that our strategy outperforms benchmarks, including both optimization-based and other DRL-based strategies, by substantial margins. Our findings further suggest that effective temporal-aware bidding can significantly increase profits in the spot and contingency FCAS markets compared to individual market participation.","sentences":["The battery energy storage system (BESS) has immense potential for enhancing grid reliability and security through its participation in the electricity market.","BESS often seeks various revenue streams by taking part in multiple markets to unlock its full potential, but effective algorithms for joint-market participation under price uncertainties are insufficiently explored in the existing research.","To bridge this gap, we develop a novel BESS joint bidding strategy that utilizes deep reinforcement learning (DRL) to bid in the spot and contingency frequency control ancillary services (FCAS) markets.","Our approach leverages a transformer-based temporal feature extractor to effectively respond to price fluctuations in seven markets simultaneously and helps DRL learn the best BESS bidding strategy in joint-market participation.","Additionally, unlike conventional \"black-box\" DRL model, our approach is more interpretable and provides valuable insights into the temporal bidding behavior of BESS in the dynamic electricity market.","We validate our method using realistic market prices from the Australian National Electricity Market.","The results show that our strategy outperforms benchmarks, including both optimization-based and other DRL-based strategies, by substantial margins.","Our findings further suggest that effective temporal-aware bidding can significantly increase profits in the spot and contingency FCAS markets compared to individual market participation."],"url":"http://arxiv.org/abs/2402.19110v1","category":"eess.SY"}
{"created":"2024-02-29 12:38:57","title":"Rahmani Sort: A Novel Variant of Insertion Sort Algorithm with O(nlogn) Complexity","abstract":"Various decision support systems are available that implement Data Mining and Data Warehousing techniques for diving into the sea of data for getting useful patterns of knowledge (pearls). Classification, regression, clustering, and many other algorithms are used to enhance the precision and accuracy of the decision process. So, there is scope for increasing the response time of the decision process, especially in mission-critical operations. If data are ordered with suitable and efficient sorting operation, the response time of the decision process can be minimized. Insertion sort is much more suitable for such applications due to its simple and straight logic along with its dynamic nature suitable for list implementation. But it is slower than merge sort and quick sort. The main reasons this is slow: firstly, a sequential search is used to find the actual position of the next key element into the sorted left subarray and secondly, shifting of elements is required by one position towards the right for accommodating the newly inserted element. Therefore, I propose a new algorithm by using a novel technique of binary search mechanism for finding the sorted location of the next key item into the previously sorted left subarray much quicker than the conventional insertion sort algorithm. Performance measurement in terms of the actual running time of the new algorithm has been compared with those of other conventional sorting algorithms apart from the insertion sort. The results obtained on various sample data show that the new algorithm is better in performance than the conventional insertion sort and merge sort algorithms.","sentences":["Various decision support systems are available that implement Data Mining and Data Warehousing techniques for diving into the sea of data for getting useful patterns of knowledge (pearls).","Classification, regression, clustering, and many other algorithms are used to enhance the precision and accuracy of the decision process.","So, there is scope for increasing the response time of the decision process, especially in mission-critical operations.","If data are ordered with suitable and efficient sorting operation, the response time of the decision process can be minimized.","Insertion sort is much more suitable for such applications due to its simple and straight logic along with its dynamic nature suitable for list implementation.","But it is slower than merge sort and quick sort.","The main reasons this is slow: firstly, a sequential search is used to find the actual position of the next key element into the sorted left subarray and secondly, shifting of elements is required by one position towards the right for accommodating the newly inserted element.","Therefore, I propose a new algorithm by using a novel technique of binary search mechanism for finding the sorted location of the next key item into the previously sorted left subarray much quicker than the conventional insertion sort algorithm.","Performance measurement in terms of the actual running time of the new algorithm has been compared with those of other conventional sorting algorithms apart from the insertion sort.","The results obtained on various sample data show that the new algorithm is better in performance than the conventional insertion sort and merge sort algorithms."],"url":"http://arxiv.org/abs/2402.19107v1","category":"cs.CR"}
{"created":"2024-02-29 12:29:58","title":"Effective Two-Stage Knowledge Transfer for Multi-Entity Cross-Domain Recommendation","abstract":"In recent years, the recommendation content on e-commerce platforms has become increasingly rich -- a single user feed may contain multiple entities, such as selling products, short videos, and content posts. To deal with the multi-entity recommendation problem, an intuitive solution is to adopt the shared-network-based architecture for joint training. The idea is to transfer the extracted knowledge from one type of entity (source entity) to another (target entity). However, different from the conventional same-entity cross-domain recommendation, multi-entity knowledge transfer encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product while missing for content posts), making the existing methods no longer appropriate. Recent researchers have also experimented with the pre-training and fine-tuning paradigm. Again, they only consider the scenarios with the same entity type and feature systems, which is inappropriate in our case. To this end, we design a pre-training & fine-tuning based Multi-entity Knowledge Transfer framework called MKT. MKT utilizes a multi-entity pre-training module to extract transferable knowledge across different entities. In particular, a feature alignment module is first applied to scale and align different feature schemas. Afterward, a couple of knowledge extractors are employed to extract the common and entity-specific knowledge. In the end, the extracted common knowledge is adopted for target entity model training. Through extensive offline and online experiments, we demonstrated the superiority of MKT over multiple State-Of-The-Art methods.","sentences":["In recent years, the recommendation content on e-commerce platforms has become increasingly rich -- a single user feed may contain multiple entities, such as selling products, short videos, and content posts.","To deal with the multi-entity recommendation problem, an intuitive solution is to adopt the shared-network-based architecture for joint training.","The idea is to transfer the extracted knowledge from one type of entity (source entity) to another (target entity).","However, different from the conventional same-entity cross-domain recommendation, multi-entity knowledge transfer encounters several important issues: (1) data distributions of the source entity and target entity are naturally different, making the shared-network-based joint training susceptible to the negative transfer issue, (2) more importantly, the corresponding feature schema of each entity is not exactly aligned (e.g., price is an essential feature for selling product while missing for content posts), making the existing methods no longer appropriate.","Recent researchers have also experimented with the pre-training and fine-tuning paradigm.","Again, they only consider the scenarios with the same entity type and feature systems, which is inappropriate in our case.","To this end, we design a pre-training & fine-tuning based Multi-entity Knowledge Transfer framework called MKT.","MKT utilizes a multi-entity pre-training module to extract transferable knowledge across different entities.","In particular, a feature alignment module is first applied to scale and align different feature schemas.","Afterward, a couple of knowledge extractors are employed to extract the common and entity-specific knowledge.","In the end, the extracted common knowledge is adopted for target entity model training.","Through extensive offline and online experiments, we demonstrated the superiority of MKT over multiple State-Of-The-Art methods."],"url":"http://arxiv.org/abs/2402.19101v1","category":"cs.IR"}
{"created":"2024-02-29 12:28:36","title":"The Reservoir of the Per-emb-2 Streamer","abstract":"Streamers bring gas from outer regions to protostellar systems and could change the chemical composition around protostars and protoplanetary disks. We have carried out mapping observations of carbon-chain species (HC$_3$N, HC$_5$N, CCH, and CCS) in the 3mm and 7mm bands toward the streamer flowing to the Class 0 young stellar object (YSO) Per-emb-2 with the Nobeyama 45m radio telescope. A region with a diameter of $\\sim0.04$ pc is located north with a distance of $\\sim 20,500$ au from the YSO. The streamer connects to this north region which is the origin of the streamer. The reservoir has high density and low temperature ($n_{\\rm {H}_2} \\approx 1.9 \\times 10^4$ cm$^{-3}$, $T_{\\rm {kin}} = 10$ K), which are similar to those of early stage starless cores. By comparisons with the observed abundance ratios of CCS/HC$_3$N to the chemical simulations, the reservoir and streamer are found to be chemically young. The total mass available for the streamer is derived to be $24-34$ M$_{\\odot}$. If all of the gas in the reservoir will accrete onto the Per-emb-2 protostellar system, the lifetime of the streamer has been estimated at ($1.1 - 3.2$)$\\times10^{5}$ yr, suggesting that the mass accretion via the streamer would continue until the end of the Class I stage.","sentences":["Streamers bring gas from outer regions to protostellar systems and could change the chemical composition around protostars and protoplanetary disks.","We have carried out mapping observations of carbon-chain species (HC$_3$N, HC$_5$N, CCH, and CCS) in the 3mm and 7mm bands toward the streamer flowing to the Class 0 young stellar object (YSO) Per-emb-2 with the Nobeyama 45m radio telescope.","A region with a diameter of $\\sim0.04$ pc is located north with a distance of $\\sim 20,500$ au from the YSO.","The streamer connects to this north region which is the origin of the streamer.","The reservoir has high density and low temperature ($n_{\\rm {H}_2} \\approx 1.9 \\times 10^4$ cm$^{-3}$, $T_{\\rm {kin}} = 10$ K), which are similar to those of early stage starless cores.","By comparisons with the observed abundance ratios of CCS/HC$_3$N to the chemical simulations, the reservoir and streamer are found to be chemically young.","The total mass available for the streamer is derived to be $24-34$ M$_{\\odot}$.","If all of the gas in the reservoir will accrete onto the Per-emb-2 protostellar system, the lifetime of the streamer has been estimated at ($1.1 - 3.2$)$\\times10^{5}$ yr, suggesting that the mass accretion via the streamer would continue until the end of the Class I stage."],"url":"http://arxiv.org/abs/2402.19099v1","category":"astro-ph.GA"}
{"created":"2024-02-29 12:27:42","title":"Symmetries and exact solutions of the diffusive Holling-Tanner prey-predator model","abstract":"We consider the classical Holling-Tanner model extended on 1D space by introducing the diffusion term. Making a reasonable simplification, the diffusive Holling-Tanner system is studied by means of symmetry based methods. Lie and Q-conditional (nonclassical) symmetries are identified. The symmetries obtained are applied for finding a wide range of exact solutions, their properties are studied and a possible biological interpretation is proposed. 3D plots of the most interesting solutions are drown as well.","sentences":["We consider the classical Holling-Tanner model extended on 1D space by introducing the diffusion term.","Making a reasonable simplification, the diffusive Holling-Tanner system is studied by means of symmetry based methods.","Lie and Q-conditional (nonclassical) symmetries are identified.","The symmetries obtained are applied for finding a wide range of exact solutions, their properties are studied and a possible biological interpretation is proposed.","3D plots of the most interesting solutions are drown as well."],"url":"http://arxiv.org/abs/2402.19098v1","category":"math-ph"}
{"created":"2024-02-29 12:12:42","title":"Semiclassical expansion for exactly solvable differential operators","abstract":"Below we study a linear differential equation $\\MM (v(z,\\eta))=\\eta^M{v(z,\\eta)}$, where $\\eta>0$ is a large spectral parameter and $\\MM=\\sum_{k=1}^{M}\\rho_{k}(z)\\frac{d^k}{dz^k},\\; M\\ge 2$ is a differential operator with polynomial coefficients such that the leading coefficient $\\rho_M(z)$ is a monic complex-valued polynomial with $\\dgr{\\rho_M }=M$ and other $\\rho_k(z)$'s are complex-valued polynomials with $\\dgr{\\rho_k }\\leq k$. We prove the Borel summability of its WKB-solutions in the Stokes regions. For $M=3$ under the assumption that $\\rho_M$ has simple zeros, we give the full description of the Stokes complex (i.e. the union of all Stokes curves) of this equation. Finally, we show that for the Euler-Cauchy equations, their WKB-solutions converge in the usual sense.","sentences":["Below we study a linear differential equation $\\MM (v(z,\\eta))=\\eta^M{v(z,\\eta)}$, where $\\eta>0$ is a large spectral parameter and $\\MM=\\sum_{k=1}^{M}\\rho_{k}(z)\\frac{d^k}{dz^k},\\; M\\ge 2$ is a differential operator with polynomial coefficients such that the leading coefficient $\\rho_M(z)$ is a monic complex-valued polynomial with $\\dgr{\\rho_M }=M$ and other $\\rho_k(z)$'s are complex-valued polynomials with $\\dgr{\\rho_k }\\leq k$.","We prove the Borel summability of its WKB-solutions in the Stokes regions.","For $M=3$ under the assumption that $\\rho_M$ has simple zeros, we give the full description of the Stokes complex (i.e. the union of all Stokes curves) of this equation.","Finally, we show that for the Euler-Cauchy equations, their WKB-solutions converge in the usual sense."],"url":"http://arxiv.org/abs/2402.19087v1","category":"math.CA"}
{"created":"2024-02-29 12:08:42","title":"MIMDRAM: An End-to-End Processing-Using-DRAM System for High-Throughput, Energy-Efficient and Programmer-Transparent Multiple-Instruction Multiple-Data Processing","abstract":"Processing-using-DRAM (PUD) is a processing-in-memory (PIM) approach that uses a DRAM array's massive internal parallelism to execute very-wide data-parallel operations, in a single-instruction multiple-data (SIMD) fashion. However, DRAM rows' large and rigid granularity limit the effectiveness and applicability of PUD in three ways. First, since applications have varying degrees of SIMD parallelism, PUD execution often leads to underutilization, throughput loss, and energy waste. Second, most PUD architectures are limited to the execution of parallel map operations. Third, the need to feed the wide DRAM row with tens of thousands of data elements combined with the lack of adequate compiler support for PUD systems create a programmability barrier.   Our goal is to design a flexible PUD system that overcomes the limitations caused by the large and rigid granularity of PUD. To this end, we propose MIMDRAM, a hardware/software co-designed PUD system that introduces new mechanisms to allocate and control only the necessary resources for a given PUD operation. The key idea of MIMDRAM is to leverage fine-grained DRAM (i.e., the ability to independently access smaller segments of a large DRAM row) for PUD computation. MIMDRAM exploits this key idea to enable a multiple-instruction multiple-data (MIMD) execution model in each DRAM subarray.   We evaluate MIMDRAM using twelve real-world applications and 495 multi-programmed application mixes. Our evaluation shows that MIMDRAM provides 34x the performance, 14.3x the energy efficiency, 1.7x the throughput, and 1.3x the fairness of a state-of-the-art PUD framework, along with 30.6x and 6.8x the energy efficiency of a high-end CPU and GPU, respectively. MIMDRAM adds small area cost to a DRAM chip (1.11%) and CPU die (0.6%).","sentences":["Processing-using-DRAM (PUD) is a processing-in-memory (PIM) approach that uses a DRAM array's massive internal parallelism to execute very-wide data-parallel operations, in a single-instruction multiple-data (SIMD) fashion.","However, DRAM rows' large and rigid granularity limit the effectiveness and applicability of PUD in three ways.","First, since applications have varying degrees of SIMD parallelism, PUD execution often leads to underutilization, throughput loss, and energy waste.","Second, most PUD architectures are limited to the execution of parallel map operations.","Third, the need to feed the wide DRAM row with tens of thousands of data elements combined with the lack of adequate compiler support for PUD systems create a programmability barrier.   ","Our goal is to design a flexible PUD system that overcomes the limitations caused by the large and rigid granularity of PUD.","To this end, we propose MIMDRAM, a hardware/software co-designed PUD system that introduces new mechanisms to allocate and control only the necessary resources for a given PUD operation.","The key idea of MIMDRAM is to leverage fine-grained DRAM (i.e., the ability to independently access smaller segments of a large DRAM row) for PUD computation.","MIMDRAM exploits this key idea to enable a multiple-instruction multiple-data (MIMD) execution model in each DRAM subarray.   ","We evaluate MIMDRAM using twelve real-world applications and 495 multi-programmed application mixes.","Our evaluation shows that MIMDRAM provides 34x the performance, 14.3x the energy efficiency, 1.7x the throughput, and 1.3x the fairness of a state-of-the-art PUD framework, along with 30.6x and 6.8x the energy efficiency of a high-end CPU and GPU, respectively.","MIMDRAM adds small area cost to a DRAM chip (1.11%) and CPU die (0.6%)."],"url":"http://arxiv.org/abs/2402.19080v1","category":"cs.AR"}
{"created":"2024-02-29 12:01:23","title":"Hunting for exocomet transits in the TESS database using the Random Forest method","abstract":"This study introduces an approach to detecting exocomet transits in the dataset of the Transiting Exoplanet Survey Satellite (TESS), specifically within its Sector 1. Given the limited number of exocomet transits detected in the observed light curves, creating a sufficient training sample for the machine learning method was challenging. We developed a unique training sample by encapsulating simulated asymmetric transit profiles into observed light curves, thereby creating realistic data for the model training. To analyze these light curves, we employed the TSFresh software, which was a tool for extracting key features that were then used to refine our Random Forest model training. Considering that cometary transits typically exhibit a small depth, less than 1% of the star's brightness, we chose to limit our sample to the CDPP parameter. Our study focused on two target samples: light curves with a CDPP of less than 40 ppm and light curves with a CDPP of up to 150 ppm. Each sample was accompanied by a corresponding training set. This methodology achieved an accuracy of approximately 96%, with both precision and recall rates exceeding 95% and a balanced F1-score of around 96%. This level of accuracy was effective in distinguishing between 'exocomet candidate' and 'non-candidate' classifications for light curves with a CDPP of less than 40 ppm, and our model identified 12 potential exocomet candidates. However, when applying machine learning to less accurate light curves (CDPP up to 150 ppm), we noticed a significant increase in curves that could not be confidently classified, but even in this case, our model identified 20 potential exocomet candidates. These promising results within Sector 1 motivate us to extend our analysis across all TESS sectors to detect and study comet-like activity in the extrasolar planetary systems.","sentences":["This study introduces an approach to detecting exocomet transits in the dataset of the Transiting Exoplanet Survey Satellite (TESS), specifically within its Sector 1.","Given the limited number of exocomet transits detected in the observed light curves, creating a sufficient training sample for the machine learning method was challenging.","We developed a unique training sample by encapsulating simulated asymmetric transit profiles into observed light curves, thereby creating realistic data for the model training.","To analyze these light curves, we employed the TSFresh software, which was a tool for extracting key features that were then used to refine our Random Forest model training.","Considering that cometary transits typically exhibit a small depth, less than 1% of the star's brightness, we chose to limit our sample to the CDPP parameter.","Our study focused on two target samples: light curves with a CDPP of less than 40 ppm and light curves with a CDPP of up to 150 ppm.","Each sample was accompanied by a corresponding training set.","This methodology achieved an accuracy of approximately 96%, with both precision and recall rates exceeding 95% and a balanced F1-score of around 96%.","This level of accuracy was effective in distinguishing between 'exocomet candidate' and 'non-candidate' classifications for light curves with a CDPP of less than 40 ppm, and our model identified 12 potential exocomet candidates.","However, when applying machine learning to less accurate light curves (CDPP up to 150 ppm), we noticed a significant increase in curves that could not be confidently classified, but even in this case, our model identified 20 potential exocomet candidates.","These promising results within Sector 1 motivate us to extend our analysis across all TESS sectors to detect and study comet-like activity in the extrasolar planetary systems."],"url":"http://arxiv.org/abs/2402.19075v1","category":"astro-ph.EP"}
{"created":"2024-02-29 11:55:46","title":"Dynamical Systems on Compact Metrizable Groups","abstract":"This paper mainly studies the theory of dynamical systems on compact metrizable groups, with a special focus on extending the maximal entropy theorem and the ergodicity of invariant measure convolution from the isomorphic case to the surjective homomorphism case. It explores in detail the dynamical systems under surjective homomorphisms, especially the variation of measure entropy. Let $G_{2}$ be a compact metric group acting freely on $G_{1}$, and the continuous mapping $T_{2}$ $G_{2}-$commutes with $T_{1}$, where $T_{2}: G_{2} \\to G_{2}$ is a surjective homomorphism. If $\\mu_{0} \\in M(T_{0})$, we prove that when $\\mu \\in M(T_{1}, \\mu_{0})$, the measure entropy $h(T_{1}, \\mu_{0}')$ is always greater than or equal to $h(T_{1}, \\mu)$; if $\\mu_{0}'$ is ergodic with respect to $T_{1}$, $\\mu \\ne \\mu_{0}'$, the Haar measure $m$ on $G_{2}$ is ergodic with respect to $T_{2}$, and $h(T_{1}, \\mu_{0}') < \\infty$, it can be concluded that the measure entropy $h(T_{1}, \\mu_{0}')$ strictly exceeds $h(T_{1}, \\mu)$.   Finally, this paper also specifically discusses the ergodicity problem of measure convolution. Let $T$ be a surjective homomorphism on $G$. If $(G, T, \\mathcal{F}, \\mu)$ and $(G, T, \\mathcal{F}, \\nu)$ are disjoint ergodic dynamical systems, then $\\mu * \\nu$ is ergodic. Through a proof by contradiction, the study demonstrates that under the condition that $T$ is a surjective homomorphism on $G$, the measure convolution of two disjoint ergodic dynamical systems also maintains ergodicity. These results extend Kenneth R. Berg's findings on the maximal entropy theorem and the ergodicity of measure convolution to the case of surjective homomorphisms.","sentences":["This paper mainly studies the theory of dynamical systems on compact metrizable groups, with a special focus on extending the maximal entropy theorem and the ergodicity of invariant measure convolution from the isomorphic case to the surjective homomorphism case.","It explores in detail the dynamical systems under surjective homomorphisms, especially the variation of measure entropy.","Let $G_{2}$ be a compact metric group acting freely on $G_{1}$, and the continuous mapping $T_{2}$ $G_{2}-$commutes with $T_{1}$, where $T_{2}: G_{2} \\to G_{2}$ is a surjective homomorphism.","If $\\mu_{0} \\in M(T_{0})$, we prove that when $\\mu \\in M(T_{1}, \\mu_{0})$, the measure entropy $h(T_{1}, \\mu_{0}')$ is always greater than or equal to $h(T_{1}, \\mu)$; if $\\mu_{0}'$ is ergodic with respect to $T_{1}$, $\\mu \\ne \\mu_{0}'$, the Haar measure $m$ on $G_{2}$ is ergodic with respect to $T_{2}$, and $h(T_{1}, \\mu_{0}') < \\infty$, it can be concluded that the measure entropy $h(T_{1}, \\mu_{0}')$ strictly exceeds $h(T_{1}, \\mu)$.   Finally, this paper also specifically discusses the ergodicity problem of measure convolution.","Let $T$ be a surjective homomorphism on $G$. If $(G, T, \\mathcal{F}, \\mu)$ and $(G, T, \\mathcal{F}, \\nu)$ are disjoint ergodic dynamical systems, then $\\mu * \\nu$ is ergodic.","Through a proof by contradiction, the study demonstrates that under the condition that $T$ is a surjective homomorphism on $G$, the measure convolution of two disjoint ergodic dynamical systems also maintains ergodicity.","These results extend Kenneth R. Berg's findings on the maximal entropy theorem and the ergodicity of measure convolution to the case of surjective homomorphisms."],"url":"http://arxiv.org/abs/2402.19074v1","category":"math.DS"}
{"created":"2024-02-29 11:49:11","title":"A Multi-Model Ensemble System for the outer Heliosphere (MMESH): Solar Wind Conditions near Jupiter","abstract":"How the solar wind influences the magnetospheres of the outer planets is a fundamentally important question, but is difficult to answer in the absence of consistent, simultaneous monitoring of the upstream solar wind and the large-scale dynamics internal to the magnetosphere. To compensate for the relative lack of in-situ data, propagation models are often used to estimate the ambient solar wind conditions at the outer planets for comparison to remote observations or in-situ measurements. This introduces another complication: the propagation of near-Earth solar wind measurements introduces difficult-to-assess uncertainties. Here, we present the Multi-Model Ensemble System for the outer Heliosphere (MMESH) to begin to address these issues, along with the resultant multi-model ensemble (MME) of the solar wind conditions near Jupiter. MMESH accepts as input any number of solar wind models together with contemporaneous in-situ spacecraft data. From these, the system characterizes typical uncertainties in model timing, quantifies how these uncertainties vary under different conditions, attempts to correct for systematic biases in the input model timing, and composes a MME with uncertainties from the results. For the case of the Jupiter-MME presented here, three solar wind propagation models were compared to in-situ measurements from the near-Jupiter spacecraft Ulysses and Juno which span diverse geometries and phases of the solar cycle, amounting to more than 14,000 hours of data over 2.5 decades. The MME gives the most-probable near-Jupiter solar wind conditions for times within the tested epoch, outperforming the input models and returning quantified estimates of uncertainty.","sentences":["How the solar wind influences the magnetospheres of the outer planets is a fundamentally important question, but is difficult to answer in the absence of consistent, simultaneous monitoring of the upstream solar wind and the large-scale dynamics internal to the magnetosphere.","To compensate for the relative lack of in-situ data, propagation models are often used to estimate the ambient solar wind conditions at the outer planets for comparison to remote observations or in-situ measurements.","This introduces another complication: the propagation of near-Earth solar wind measurements introduces difficult-to-assess uncertainties.","Here, we present the Multi-Model Ensemble System for the outer Heliosphere (MMESH) to begin to address these issues, along with the resultant multi-model ensemble (MME) of the solar wind conditions near Jupiter.","MMESH accepts as input any number of solar wind models together with contemporaneous in-situ spacecraft data.","From these, the system characterizes typical uncertainties in model timing, quantifies how these uncertainties vary under different conditions, attempts to correct for systematic biases in the input model timing, and composes a MME with uncertainties from the results.","For the case of the Jupiter-MME presented here, three solar wind propagation models were compared to in-situ measurements from the near-Jupiter spacecraft Ulysses and Juno which span diverse geometries and phases of the solar cycle, amounting to more than 14,000 hours of data over 2.5 decades.","The MME gives the most-probable near-Jupiter solar wind conditions for times within the tested epoch, outperforming the input models and returning quantified estimates of uncertainty."],"url":"http://arxiv.org/abs/2402.19069v1","category":"physics.space-ph"}
{"created":"2024-02-29 11:32:30","title":"Quantum coherence and entanglement under the influence of decoherence","abstract":"In this work, we delve into the dynamic traits of the relative entropy of quantum coherence (REQC) as the quantum system interacts with the different noisy channels, drawing comparisons with entanglement (concurrence). The research results demonstrate the broader prevalence and stronger robustness of the REQC as opposed to concurrence. It's worth noting that the bit flip channel cannot uphold a constant nonzero frozen the REQC, besides, the concurrence follows a pattern of temporary reduction to zero, followed by recovery after a certain time span. More importantly, the REQC maintains its presence consistently until reaching a critical threshold, whereas concurrence experiences completely attenuation to zero under the influence of phase damping and amplitude damping channels.","sentences":["In this work, we delve into the dynamic traits of the relative entropy of quantum coherence (REQC) as the quantum system interacts with the different noisy channels, drawing comparisons with entanglement (concurrence).","The research results demonstrate the broader prevalence and stronger robustness of the REQC as opposed to concurrence.","It's worth noting that the bit flip channel cannot uphold a constant nonzero frozen the REQC, besides, the concurrence follows a pattern of temporary reduction to zero, followed by recovery after a certain time span.","More importantly, the REQC maintains its presence consistently until reaching a critical threshold, whereas concurrence experiences completely attenuation to zero under the influence of phase damping and amplitude damping channels."],"url":"http://arxiv.org/abs/2402.19055v1","category":"quant-ph"}
{"created":"2024-02-29 11:30:13","title":"Geometric approach for the identification of Hamiltonian systems of quasi-Painlev\u00e9 type","abstract":"Some new Hamiltonian systems of quasi-Painlev\\'e type are presented and their Okamoto's space of initial conditions computed. Using the geometric approach that was introduced originally for the identification problem of Painlev\\'e equations, comparing the irreducible components of the inaccessible divisors arising in the blow-up process, we find bi-rational, symplectic coordinate changes between some of these systems that give rise to the same global symplectic structure. This scheme thus gives a method for identifying Hamiltonian systems up to bi-rational symplectic maps, which is performed in this article for systems of quasi-Painlev\\'e type having singularities that are either square-root type algebraic poles or ordinary poles.","sentences":["Some new Hamiltonian systems of quasi-Painlev\\'e type are presented and their Okamoto's space of initial conditions computed.","Using the geometric approach that was introduced originally for the identification problem of Painlev\\'e equations, comparing the irreducible components of the inaccessible divisors arising in the blow-up process, we find bi-rational, symplectic coordinate changes between some of these systems that give rise to the same global symplectic structure.","This scheme thus gives a method for identifying Hamiltonian systems up to bi-rational symplectic maps, which is performed in this article for systems of quasi-Painlev\\'e type having singularities that are either square-root type algebraic poles or ordinary poles."],"url":"http://arxiv.org/abs/2402.19053v1","category":"math.CA"}
{"created":"2024-02-29 11:28:24","title":"The Shigesada-Kawasaki-Teramoto model: conditional symmetries, exact solutions and their properties","abstract":"We study a simplification of the well-known Shigesada-Kawasaki-Teramoto model, which consists of two nonlinear reaction-diffusion equations with cross-diffusion. A complete set of Q-conditional (nonclassical) symmetries is derived using an algorithm adopted for the construction of conditional symmetries. The symmetries obtained are applied for finding a wide range of exact solutions, possible biological interpretation of some of which being presented. Moreover, an alternative application of the simplified model related to the polymerisation process is suggested and exact solutions are found in this case as well.","sentences":["We study a simplification of the well-known Shigesada-Kawasaki-Teramoto model, which consists of two nonlinear reaction-diffusion equations with cross-diffusion.","A complete set of Q-conditional (nonclassical) symmetries is derived using an algorithm adopted for the construction of conditional symmetries.","The symmetries obtained are applied for finding a wide range of exact solutions, possible biological interpretation of some of which being presented.","Moreover, an alternative application of the simplified model related to the polymerisation process is suggested and exact solutions are found in this case as well."],"url":"http://arxiv.org/abs/2402.19050v1","category":"math-ph"}
{"created":"2024-02-29 11:20:16","title":"Theoretical Foundations of Deep Selective State-Space Models","abstract":"Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.","sentences":["Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data.","Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers.","Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters.","In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.","Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants."],"url":"http://arxiv.org/abs/2402.19047v1","category":"cs.LG"}
{"created":"2024-02-29 11:08:39","title":"H$_2$ and CO$_2$ Network Strategies for the European Energy System","abstract":"Hydrogen and carbon dioxide transport can both play an essential role in climate-neutral energy systems. Hydrogen networks help serve regions with high energy demand, while excess emissions are transported away in carbon dioxide networks. For the synthesis of carbonaceous fuels, it is less clear which input should be transported: hydrogen to carbon point sources or carbon to low-cost hydrogen. We explore both networks' potential synergies and competition in a cost-optimal carbon-neutral European energy system. In direct comparison, a hydrogen network is more cost-effective than a carbon network, since it serves to transport hydrogen to demand and to point source of carbon for utilization. However, in a hybrid scenario where both networks are present, the carbon network effectively complements the hydrogen network, promoting carbon capture from biomass and reducing reliance on direct air capture. Our analysis suggests integrating hydrogen and carbon dioxide networks into European energy policy for a robust, carbon-neutral or carbon-negative future energy system.","sentences":["Hydrogen and carbon dioxide transport can both play an essential role in climate-neutral energy systems.","Hydrogen networks help serve regions with high energy demand, while excess emissions are transported away in carbon dioxide networks.","For the synthesis of carbonaceous fuels, it is less clear which input should be transported: hydrogen to carbon point sources or carbon to low-cost hydrogen.","We explore both networks' potential synergies and competition in a cost-optimal carbon-neutral European energy system.","In direct comparison, a hydrogen network is more cost-effective than a carbon network, since it serves to transport hydrogen to demand and to point source of carbon for utilization.","However, in a hybrid scenario where both networks are present, the carbon network effectively complements the hydrogen network, promoting carbon capture from biomass and reducing reliance on direct air capture.","Our analysis suggests integrating hydrogen and carbon dioxide networks into European energy policy for a robust, carbon-neutral or carbon-negative future energy system."],"url":"http://arxiv.org/abs/2402.19042v1","category":"physics.soc-ph"}
{"created":"2024-02-29 11:02:47","title":"A Deep-Learning Technique to Locate Cryptographic Operations in Side-Channel Traces","abstract":"Side-channel attacks allow extracting secret information from the execution of cryptographic primitives by correlating the partially known computed data and the measured side-channel signal. However, to set up a successful side-channel attack, the attacker has to perform i) the challenging task of locating the time instant in which the target cryptographic primitive is executed inside a side-channel trace and then ii)the time-alignment of the measured data on that time instant. This paper presents a novel deep-learning technique to locate the time instant in which the target computed cryptographic operations are executed in the side-channel trace. In contrast to state-of-the-art solutions, the proposed methodology works even in the presence of trace deformations obtained through random delay insertion techniques. We validated our proposal through a successful attack against a variety of unprotected and protected cryptographic primitives that have been executed on an FPGA-implemented system-on-chip featuring a RISC-V CPU.","sentences":["Side-channel attacks allow extracting secret information from the execution of cryptographic primitives by correlating the partially known computed data and the measured side-channel signal.","However, to set up a successful side-channel attack, the attacker has to perform i) the challenging task of locating the time instant in which the target cryptographic primitive is executed inside a side-channel trace and then ii)the time-alignment of the measured data on that time instant.","This paper presents a novel deep-learning technique to locate the time instant in which the target computed cryptographic operations are executed in the side-channel trace.","In contrast to state-of-the-art solutions, the proposed methodology works even in the presence of trace deformations obtained through random delay insertion techniques.","We validated our proposal through a successful attack against a variety of unprotected and protected cryptographic primitives that have been executed on an FPGA-implemented system-on-chip featuring a RISC-V CPU."],"url":"http://arxiv.org/abs/2402.19037v1","category":"cs.CR"}
{"created":"2024-02-29 10:42:18","title":"A Faster Algorithm for the Free Energy in One-Dimensional Quantum Systems","abstract":"We consider the problem of approximating the free energy density of a translation-invariant, one-dimensional quantum spin system with finite range. While the complexity of this problem is nontrivial due to its close connection to problems with known hardness results, a classical subpolynomial-time algorithm has recently been proposed [Fawzi et al., 2022]. Combining several algorithmic techniques previously used for related problems, we propose an algorithm outperforming this result asymptotically and give rigorous bounds on its runtime. Our main techniques are the use of Araki expansionals, known from results on the nonexistence of phase transitions, and a matrix product operator construction. We also review a related approach using the Quantum Belief Propagation [Kuwahara et al., 2018], which in combination with our findings yields an equivalent result.","sentences":["We consider the problem of approximating the free energy density of a translation-invariant, one-dimensional quantum spin system with finite range.","While the complexity of this problem is nontrivial due to its close connection to problems with known hardness results, a classical subpolynomial-time algorithm has recently been proposed","[Fawzi et al., 2022].","Combining several algorithmic techniques previously used for related problems, we propose an algorithm outperforming this result asymptotically and give rigorous bounds on its runtime.","Our main techniques are the use of Araki expansionals, known from results on the nonexistence of phase transitions, and a matrix product operator construction.","We also review a related approach using the Quantum Belief Propagation","[Kuwahara et al., 2018], which in combination with our findings yields an equivalent result."],"url":"http://arxiv.org/abs/2402.19030v1","category":"quant-ph"}
{"created":"2024-02-29 10:33:29","title":"Jointly Learning Selection Matrices For Transmitters, Receivers And Fourier Coefficients In Multichannel Imaging","abstract":"Strategic subsampling has become a focal point due to its effectiveness in compressing data, particularly in the Full Matrix Capture (FMC) approach in ultrasonic imaging. This paper introduces the Joint Deep Probabilistic Subsampling (J-DPS) method, which aims to learn optimal selection matrices simultaneously for transmitters, receivers, and Fourier coefficients. This task-based algorithm is realized by introducing a specialized measurement model and integrating a customized Complex Learned FISTA (CL-FISTA) network. We propose a parallel network architecture, partitioned into three segments corresponding to the three matrices, all working toward a shared optimization objective with adjustable loss allocation. A synthetic dataset is designed to reflect practical scenarios, and we provide quantitative comparisons with a traditional CRB-based algorithm, standard DPS, and J-DPS.","sentences":["Strategic subsampling has become a focal point due to its effectiveness in compressing data, particularly in the Full Matrix Capture (FMC) approach in ultrasonic imaging.","This paper introduces the Joint Deep Probabilistic Subsampling (J-DPS) method, which aims to learn optimal selection matrices simultaneously for transmitters, receivers, and Fourier coefficients.","This task-based algorithm is realized by introducing a specialized measurement model and integrating a customized Complex Learned FISTA (CL-FISTA) network.","We propose a parallel network architecture, partitioned into three segments corresponding to the three matrices, all working toward a shared optimization objective with adjustable loss allocation.","A synthetic dataset is designed to reflect practical scenarios, and we provide quantitative comparisons with a traditional CRB-based algorithm, standard DPS, and J-DPS."],"url":"http://arxiv.org/abs/2402.19023v1","category":"eess.IV"}
{"created":"2024-02-29 10:33:04","title":"Mode-resolved thermometry of trapped ion with Deep Learning","abstract":"In trapped ion system, accurate thermometry of ion is crucial for evaluating the system state and precisely performing quantum operations. However, when the motional state of a single ion is far away from the ground state, the spatial dimension of the phonon state sharply increases, making it difficult to realize accurate and mode-resolved thermometry with existing methods. In this work, we apply deep learning for the first time to the thermometry of trapped ion, providing an efficient and mode-resolved method for accurately estimating large mean phonon numbers. Our trained neural network model can be directly applied to other experimental setups without retraining or post-processing, as long as the related parameters are covered by the model's effective range, and it can also be conveniently extended to other parameter ranges. We have conducted experimental verification based on our surface trap, of which the result has shown the accuracy and efficiency of the method for thermometry of single ion under large mean phonon number, and its mode resolution characteristic can make it better applied to the characterization of system parameters, such as evaluating cooling effectiveness, analyzing surface trap noise.","sentences":["In trapped ion system, accurate thermometry of ion is crucial for evaluating the system state and precisely performing quantum operations.","However, when the motional state of a single ion is far away from the ground state, the spatial dimension of the phonon state sharply increases, making it difficult to realize accurate and mode-resolved thermometry with existing methods.","In this work, we apply deep learning for the first time to the thermometry of trapped ion, providing an efficient and mode-resolved method for accurately estimating large mean phonon numbers.","Our trained neural network model can be directly applied to other experimental setups without retraining or post-processing, as long as the related parameters are covered by the model's effective range, and it can also be conveniently extended to other parameter ranges.","We have conducted experimental verification based on our surface trap, of which the result has shown the accuracy and efficiency of the method for thermometry of single ion under large mean phonon number, and its mode resolution characteristic can make it better applied to the characterization of system parameters, such as evaluating cooling effectiveness, analyzing surface trap noise."],"url":"http://arxiv.org/abs/2402.19022v1","category":"quant-ph"}
{"created":"2024-02-29 10:30:02","title":"Unsupervised Learning of High-resolution Light Field Imaging via Beam Splitter-based Hybrid Lenses","abstract":"In this paper, we design a beam splitter-based hybrid light field imaging prototype to record 4D light field image and high-resolution 2D image simultaneously, and make a hybrid light field dataset. The 2D image could be considered as the high-resolution ground truth corresponding to the low-resolution central sub-aperture image of 4D light field image. Subsequently, we propose an unsupervised learning-based super-resolution framework with the hybrid light field dataset, which adaptively settles the light field spatial super-resolution problem with a complex degradation model. Specifically, we design two loss functions based on pre-trained models that enable the super-resolution network to learn the detailed features and light field parallax structure with only one ground truth. Extensive experiments demonstrate the same superiority of our approach with supervised learning-based state-of-the-art ones. To our knowledge, it is the first end-to-end unsupervised learning-based spatial super-resolution approach in light field imaging research, whose input is available from our beam splitter-based hybrid light field system. The hardware and software together may help promote the application of light field super-resolution to a great extent.","sentences":["In this paper, we design a beam splitter-based hybrid light field imaging prototype to record 4D light field image and high-resolution 2D image simultaneously, and make a hybrid light field dataset.","The 2D image could be considered as the high-resolution ground truth corresponding to the low-resolution central sub-aperture image of 4D light field image.","Subsequently, we propose an unsupervised learning-based super-resolution framework with the hybrid light field dataset, which adaptively settles the light field spatial super-resolution problem with a complex degradation model.","Specifically, we design two loss functions based on pre-trained models that enable the super-resolution network to learn the detailed features and light field parallax structure with only one ground truth.","Extensive experiments demonstrate the same superiority of our approach with supervised learning-based state-of-the-art ones.","To our knowledge, it is the first end-to-end unsupervised learning-based spatial super-resolution approach in light field imaging research, whose input is available from our beam splitter-based hybrid light field system.","The hardware and software together may help promote the application of light field super-resolution to a great extent."],"url":"http://arxiv.org/abs/2402.19020v1","category":"eess.IV"}
{"created":"2024-02-29 10:16:34","title":"Ultraviolet Positioning via TDOA: Error Analysis and System Prototype","abstract":"This work performs the design, real-time hardware realization, and experimental evaluation of a positioning system by ultra-violet (UV) communication under photon-level signal detection. The positioning is based on time-difference of arrival (TDOA) principle. Time division-based transmission of synchronization sequence from three transmitters with known positions is applied. We investigate the positioning error via decomposing it into two parts, the transmitter-side timing error and the receiver-side synchronization error. The theoretical average error matches well with the simulation results, which indicates that theoretical fitting can provide reliable guidance and prediction for hardware experiments. We also conduct real-time hardware realization of the TDOA-based positioning system using Field Programmable Gate Array (FPGA), which is experimentally evaluated via outdoor experiments. Experimental results match well with the theoretical and simulation results.","sentences":["This work performs the design, real-time hardware realization, and experimental evaluation of a positioning system by ultra-violet (UV) communication under photon-level signal detection.","The positioning is based on time-difference of arrival (TDOA) principle.","Time division-based transmission of synchronization sequence from three transmitters with known positions is applied.","We investigate the positioning error via decomposing it into two parts, the transmitter-side timing error and the receiver-side synchronization error.","The theoretical average error matches well with the simulation results, which indicates that theoretical fitting can provide reliable guidance and prediction for hardware experiments.","We also conduct real-time hardware realization of the TDOA-based positioning system using Field Programmable Gate Array (FPGA), which is experimentally evaluated via outdoor experiments.","Experimental results match well with the theoretical and simulation results."],"url":"http://arxiv.org/abs/2402.19013v1","category":"eess.SY"}
{"created":"2024-02-29 10:12:23","title":"Ruledger: Ensuring Execution Integrity in Trigger-Action IoT Platforms","abstract":"Smart home IoT systems utilize trigger-action platforms, e.g., IFTTT, to manage devices from various vendors. However, they may be abused by triggering malicious rule execution with forged IoT devices or events violating the execution integrity and the intentions of the users. To address this issue, we propose a ledger based IoT platform called Ruledger, which ensures the correct execution of rules by verifying the authenticity of the corresponding information. Ruledger utilizes smart contracts to enforce verifying the information associated with rule executions, e.g., the user and configuration information from users, device events, and triggers in the trigger-action platforms. In particular, we develop three algorithms to enable ledger-wallet based applications for Ruledger and guarantee that the records used for verification are stateful and correct. Thus, the execution integrity of rules is ensured even if devices and platforms in the smart home systems are compromised. We prototype Ruledger in a real IoT platform, i.e., IFTTT, and evaluate the performance with various settings. The experimental results demonstrate Ruledger incurs an average of 12.53% delay, which is acceptable for smart home systems.","sentences":["Smart home IoT systems utilize trigger-action platforms, e.g., IFTTT, to manage devices from various vendors.","However, they may be abused by triggering malicious rule execution with forged IoT devices or events violating the execution integrity and the intentions of the users.","To address this issue, we propose a ledger based IoT platform called Ruledger, which ensures the correct execution of rules by verifying the authenticity of the corresponding information.","Ruledger utilizes smart contracts to enforce verifying the information associated with rule executions, e.g., the user and configuration information from users, device events, and triggers in the trigger-action platforms.","In particular, we develop three algorithms to enable ledger-wallet based applications for Ruledger and guarantee that the records used for verification are stateful and correct.","Thus, the execution integrity of rules is ensured even if devices and platforms in the smart home systems are compromised.","We prototype Ruledger in a real IoT platform, i.e., IFTTT, and evaluate the performance with various settings.","The experimental results demonstrate Ruledger incurs an average of 12.53% delay, which is acceptable for smart home systems."],"url":"http://arxiv.org/abs/2402.19011v1","category":"cs.CR"}
{"created":"2024-02-29 10:03:57","title":"DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments","abstract":"Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI. Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancy from real-world situations. To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios. Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints. Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles. This novel functionality enables evaluation of the agents' collision avoidance abilities in dynamic environments. We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy. Our dataset could be found at https://DOZE-Dataset.github.io/.","sentences":["Zero-Shot Object Navigation (ZSON) requires agents to autonomously locate and approach unseen objects in unfamiliar environments and has emerged as a particularly challenging task within the domain of Embodied AI.","Existing datasets for developing ZSON algorithms lack consideration of dynamic obstacles, object attribute diversity, and scene texts, thus exhibiting noticeable discrepancy from real-world situations.","To address these issues, we propose a Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments (DOZE) that comprises ten high-fidelity 3D scenes with over 18k tasks, aiming to mimic complex, dynamic real-world scenarios.","Specifically, DOZE scenes feature multiple moving humanoid obstacles, a wide array of open-vocabulary objects, diverse distinct-attribute objects, and valuable textual hints.","Besides, different from existing datasets that only provide collision checking between the agent and static obstacles, we enhance DOZE by integrating capabilities for detecting collisions between the agent and moving obstacles.","This novel functionality enables evaluation of the agents' collision avoidance abilities in dynamic environments.","We test four representative ZSON methods on DOZE, revealing substantial room for improvement in existing approaches concerning navigation efficiency, safety, and object recognition accuracy.","Our dataset could be found at https://DOZE-Dataset.github.io/."],"url":"http://arxiv.org/abs/2402.19007v1","category":"cs.CV"}
{"created":"2024-02-29 09:55:46","title":"RSAM-Seg: A SAM-based Approach with Prior Knowledge Integration for Remote Sensing Image Semantic Segmentation","abstract":"The development of high-resolution remote sensing satellites has provided great convenience for research work related to remote sensing. Segmentation and extraction of specific targets are essential tasks when facing the vast and complex remote sensing images. Recently, the introduction of Segment Anything Model (SAM) provides a universal pre-training model for image segmentation tasks. While the direct application of SAM to remote sensing image segmentation tasks does not yield satisfactory results, we propose RSAM-Seg, which stands for Remote Sensing SAM with Semantic Segmentation, as a tailored modification of SAM for the remote sensing field and eliminates the need for manual intervention to provide prompts. Adapter-Scale, a set of supplementary scaling modules, are proposed in the multi-head attention blocks of the encoder part of SAM. Furthermore, Adapter-Feature are inserted between the Vision Transformer (ViT) blocks. These modules aim to incorporate high-frequency image information and image embedding features to generate image-informed prompts. Experiments are conducted on four distinct remote sensing scenarios, encompassing cloud detection, field monitoring, building detection and road mapping tasks . The experimental results not only showcase the improvement over the original SAM and U-Net across cloud, buildings, fields and roads scenarios, but also highlight the capacity of RSAM-Seg to discern absent areas within the ground truth of certain datasets, affirming its potential as an auxiliary annotation method. In addition, the performance in few-shot scenarios is commendable, underscores its potential in dealing with limited datasets.","sentences":["The development of high-resolution remote sensing satellites has provided great convenience for research work related to remote sensing.","Segmentation and extraction of specific targets are essential tasks when facing the vast and complex remote sensing images.","Recently, the introduction of Segment Anything Model (SAM) provides a universal pre-training model for image segmentation tasks.","While the direct application of SAM to remote sensing image segmentation tasks does not yield satisfactory results, we propose RSAM-Seg, which stands for Remote Sensing SAM with Semantic Segmentation, as a tailored modification of SAM for the remote sensing field and eliminates the need for manual intervention to provide prompts.","Adapter-Scale, a set of supplementary scaling modules, are proposed in the multi-head attention blocks of the encoder part of SAM.","Furthermore, Adapter-Feature are inserted between the Vision Transformer (ViT) blocks.","These modules aim to incorporate high-frequency image information and image embedding features to generate image-informed prompts.","Experiments are conducted on four distinct remote sensing scenarios, encompassing cloud detection, field monitoring, building detection and road mapping tasks .","The experimental results not only showcase the improvement over the original SAM and U-Net across cloud, buildings, fields and roads scenarios, but also highlight the capacity of RSAM-Seg to discern absent areas within the ground truth of certain datasets, affirming its potential as an auxiliary annotation method.","In addition, the performance in few-shot scenarios is commendable, underscores its potential in dealing with limited datasets."],"url":"http://arxiv.org/abs/2402.19004v1","category":"cs.CV"}
{"created":"2024-02-29 09:52:00","title":"A note on subgroups whose quotients are quasi-lines","abstract":"We study finitely generated pairs of groups $H \\leq G$ such that the Schreier graph of $H$ is quasi-isometric to a line. Under this hypothesis, we show that $H$ is a virtual fiber subgroup if and only if $G$ contains infinitely many double cosets of $H$. Along the way, we prove that if a group acts essentially on a finite dimensional CAT(0) cube complex with no facing triples then it virtually surjects onto the integers with kernel commensurable to a hyperplane stabiliser.","sentences":["We study finitely generated pairs of groups $H \\leq G$ such that the Schreier graph of $H$ is quasi-isometric to a line.","Under this hypothesis, we show that $H$ is a virtual fiber subgroup if and only if $G$ contains infinitely many double cosets of $H$. Along the way, we prove that if a group acts essentially on a finite dimensional CAT(0) cube complex with no facing triples then it virtually surjects onto the integers with kernel commensurable to a hyperplane stabiliser."],"url":"http://arxiv.org/abs/2402.19000v1","category":"math.GR"}
{"created":"2024-02-29 09:43:54","title":"Extracting quantum-critical properties from directly evaluated enhanced perturbative continuous unitary transformations","abstract":"Directly evaluated enhanced perturbative continuous unitary transformations (deepCUTs) are used to calculate non-perturbatively extrapolated numerical data for the ground-state energy and the energy gap. The data coincides with the perturbative series up to the order with respect to which the deepCUT is truncated. We develop a general scheme to extract quantum-critical properties from the deepCUT data based on critical scaling and a strict correspondence between the truncation used for deepCUT and the length scale of correlations at the critical point. We apply our approach to transverse-field Ising models (TFIMs) as paradigmatic systems for quantum phase transitions of various universality classes depending on the lattice geometry and the choice of antiferromagnetic or ferromagnetic coupling. In particular, we focus on the quantum phase diagram of the bilayer antiferromagnetic TFIM on the triangular lattice with an Ising-type interlayer coupling. Without a field, the model is known to host a classically disordered ground state, and in the limit of decoupled layers it exhibits the 3d-XY 'order by disorder' transition of the corresponding single-layer model. Our starting point for the unknown parts of the phase diagram is a high-order perturbative calculation about the limit of isolated dimers where the model is in a gapped phase.","sentences":["Directly evaluated enhanced perturbative continuous unitary transformations (deepCUTs) are used to calculate non-perturbatively extrapolated numerical data for the ground-state energy and the energy gap.","The data coincides with the perturbative series up to the order with respect to which the deepCUT is truncated.","We develop a general scheme to extract quantum-critical properties from the deepCUT data based on critical scaling and a strict correspondence between the truncation used for deepCUT and the length scale of correlations at the critical point.","We apply our approach to transverse-field Ising models (TFIMs) as paradigmatic systems for quantum phase transitions of various universality classes depending on the lattice geometry and the choice of antiferromagnetic or ferromagnetic coupling.","In particular, we focus on the quantum phase diagram of the bilayer antiferromagnetic TFIM on the triangular lattice with an Ising-type interlayer coupling.","Without a field, the model is known to host a classically disordered ground state, and in the limit of decoupled layers it exhibits the 3d-XY 'order by disorder' transition of the corresponding single-layer model.","Our starting point for the unknown parts of the phase diagram is a high-order perturbative calculation about the limit of isolated dimers where the model is in a gapped phase."],"url":"http://arxiv.org/abs/2402.18989v1","category":"cond-mat.str-el"}
{"created":"2024-02-29 09:40:58","title":"Atom interferometry at arbitrary orientations and rotation rates","abstract":"The exquisite precision of atom interferometers has sparked the interest of a large community for use cases ranging from fundamental physics to geodesy and inertial navigation. However, their practical use for onboard applications is still limited, not least because rotation and acceleration are intertwined in a single phase shift in free-fall atom interferometers, which makes the extraction of a useful signal more challenging. Moreover, the spatial separation of the wave packets due to rotations leads to a loss of signal. Here we present an atom interferometer operating over a large range of random angles, rotation rates and accelerations. An accurate model of the expected phase shift allows us to untangle the rotation and acceleration signals. We also implement a real-time compensation system using two fibre-optic gyroscopes and a tip-tilt platform to rotate the reference mirror and maintain the full contrast of the atom interferometer. Using these theoretical and practical tools, we reconstruct the fringes and demonstrate a single-shot sensitivity to acceleration of 24 $\\mu$g, for a total interrogation time of 2T = 20 ms, for angles and rotation rates reaching 30$^\\circ$ and 14 $^\\circ$/s respectively. Our hybrid rotating atom interferometer unlocks the full potential of quantum inertial sensors for onboard applications, such as autonomous navigation or gravity mapping.","sentences":["The exquisite precision of atom interferometers has sparked the interest of a large community for use cases ranging from fundamental physics to geodesy and inertial navigation.","However, their practical use for onboard applications is still limited, not least because rotation and acceleration are intertwined in a single phase shift in free-fall atom interferometers, which makes the extraction of a useful signal more challenging.","Moreover, the spatial separation of the wave packets due to rotations leads to a loss of signal.","Here we present an atom interferometer operating over a large range of random angles, rotation rates and accelerations.","An accurate model of the expected phase shift allows us to untangle the rotation and acceleration signals.","We also implement a real-time compensation system using two fibre-optic gyroscopes and a tip-tilt platform to rotate the reference mirror and maintain the full contrast of the atom interferometer.","Using these theoretical and practical tools, we reconstruct the fringes and demonstrate a single-shot sensitivity to acceleration of 24 $\\mu$g, for a total interrogation time of 2T = 20 ms, for angles and rotation rates reaching 30$^\\circ$ and 14 $^\\circ$/s respectively.","Our hybrid rotating atom interferometer unlocks the full potential of quantum inertial sensors for onboard applications, such as autonomous navigation or gravity mapping."],"url":"http://arxiv.org/abs/2402.18988v1","category":"physics.atom-ph"}
{"created":"2024-02-29 09:40:40","title":"The Catalan's triangle system, the Catalan's trapezoids and (q,2)--Fock space","abstract":"We provide an explicit formulation for the solution to the Catalan's triangle system using Catalan's trapezoids and a specified boundary condition. Additionally, we study this system with various boundary conditions obtained by utilizing different types of Fock spaces.","sentences":["We provide an explicit formulation for the solution to the Catalan's triangle system using Catalan's trapezoids and a specified boundary condition.","Additionally, we study this system with various boundary conditions obtained by utilizing different types of Fock spaces."],"url":"http://arxiv.org/abs/2402.18987v1","category":"math.CO"}
{"created":"2024-02-29 09:40:07","title":"Always be Pre-Training: Representation Learning for Network Intrusion Detection with GNNs","abstract":"Graph neural network-based network intrusion detection systems have recently demonstrated state-of-the-art performance on benchmark datasets. Nevertheless, these methods suffer from a reliance on target encoding for data pre-processing, limiting widespread adoption due to the associated need for annotated labels--a cost-prohibitive requirement. In this work, we propose a solution involving in-context pre-training and the utilization of dense representations for categorical features to jointly overcome the label-dependency limitation. Our approach exhibits remarkable data efficiency, achieving over 98% of the performance of the supervised state-of-the-art with less than 4% labeled data on the NF-UQ-NIDS-V2 dataset.","sentences":["Graph neural network-based network intrusion detection systems have recently demonstrated state-of-the-art performance on benchmark datasets.","Nevertheless, these methods suffer from a reliance on target encoding for data pre-processing, limiting widespread adoption due to the associated need for annotated labels--a cost-prohibitive requirement.","In this work, we propose a solution involving in-context pre-training and the utilization of dense representations for categorical features to jointly overcome the label-dependency limitation.","Our approach exhibits remarkable data efficiency, achieving over 98% of the performance of the supervised state-of-the-art with less than 4% labeled data on the NF-UQ-NIDS-V2 dataset."],"url":"http://arxiv.org/abs/2402.18986v1","category":"cs.CR"}
{"created":"2024-02-29 09:38:41","title":"Graph Burning: Bounds and Hardness","abstract":"The burning number of a graph $G$, denoted by $b(G)$, is the minimum number of steps required to burn all the vertices of a graph where in each step the existing fire spreads to all the adjacent vertices and one additional vertex can be burned as a new fire source. In this paper, we study the burning number problem both from an algorithmic and a structural point of view. The decision problem of computing the burning number of an input graph is known to be NP-Complete for trees with maximum degree at most three and interval graphs. Here, we prove that this problem is NP-Complete even when restricted to connected proper interval graphs and connected cubic graphs. The well-known burning number conjecture asserts that all the vertices of any graph of order $n$ can be burned in $\\lceil \\sqrt{n}~\\rceil$ steps. In line with this conjecture, upper and lower bounds of $b(G)$ are well-studied for various special graph classes. Here, we provide an improved upper bound for the burning number of connected $P_k$-free graphs and show that the bound is tight up to an additive constant $1$. Finally, we study two variants of the problem, namely edge burning (only edges are burned) and total burning (both vertices and edges are burned). In particular, we establish their relationship with the burning number problem and evaluate the complexity of these variants.","sentences":["The burning number of a graph $G$, denoted by $b(G)$, is the minimum number of steps required to burn all the vertices of a graph where in each step the existing fire spreads to all the adjacent vertices and one additional vertex can be burned as a new fire source.","In this paper, we study the burning number problem both from an algorithmic and a structural point of view.","The decision problem of computing the burning number of an input graph is known to be NP-Complete for trees with maximum degree at most three and interval graphs.","Here, we prove that this problem is NP-Complete even when restricted to connected proper interval graphs and connected cubic graphs.","The well-known burning number conjecture asserts that all the vertices of any graph of order $n$ can be burned in $\\lceil \\sqrt{n}~\\rceil$ steps.","In line with this conjecture, upper and lower bounds of $b(G)$ are well-studied for various special graph classes.","Here, we provide an improved upper bound for the burning number of connected $P_k$-free graphs and show that the bound is tight up to an additive constant $1$. Finally, we study two variants of the problem, namely edge burning (only edges are burned) and total burning (both vertices and edges are burned).","In particular, we establish their relationship with the burning number problem and evaluate the complexity of these variants."],"url":"http://arxiv.org/abs/2402.18984v1","category":"math.CO"}
{"created":"2024-02-29 09:38:37","title":"Free energy expansions of a conditional GinUE and large deviations of the smallest eigenvalue of the LUE","abstract":"We consider a planar Coulomb gas ensemble of size $N$ with the inverse temperature $\\beta=2$ and external potential $Q(z)=|z|^2-2c \\log|z-a|$, where $c>0$ and $a \\in \\mathbb{C}$. Equivalently, this model can be realised as $N$ eigenvalues of the complex Ginibre matrix of size $(c+1) N \\times (c+1) N$ conditioned to have deterministic eigenvalue $a$ with multiplicity $cN$. Depending on the values of $c$ and $a$, the droplet reveals a phase transition: it is doubly connected in the post-critical regime and simply connected in the pre-critical regime. In both regimes, we derive precise large-$N$ expansions of the free energy up to the $O(1)$ term, providing a non-radially symmetric example that confirms the Zabrodin-Wiegmann conjecture made for general planar Coulomb gas ensembles. As a consequence, our results provide asymptotic behaviours of moments of the characteristic polynomial of the complex Ginibre matrix, where the powers are of order $O(N)$. Furthermore, by combining with a duality formula, we obtain precise large deviation probabilities of the smallest eigenvalue of the Laguerre unitary ensemble. Our proof is based on a refined Riemann-Hilbert analysis for planar orthogonal polynomials using the partial Schlesinger transform.","sentences":["We consider a planar Coulomb gas ensemble of size $N$ with the inverse temperature $\\beta=2$ and external potential $Q(z)=|z|^2-2c \\log|z-a|$, where $c>0$ and $a \\in \\mathbb{C}$. Equivalently, this model can be realised as $N$ eigenvalues of the complex Ginibre matrix of size $(c+1) N \\times (c+1) N$ conditioned to have deterministic eigenvalue $a$ with multiplicity $cN$. Depending on the values of $c$ and $a$, the droplet reveals a phase transition: it is doubly connected in the post-critical regime and simply connected in the pre-critical regime.","In both regimes, we derive precise large-$N$ expansions of the free energy up to the $O(1)$ term, providing a non-radially symmetric example that confirms the Zabrodin-Wiegmann conjecture made for general planar Coulomb gas ensembles.","As a consequence, our results provide asymptotic behaviours of moments of the characteristic polynomial of the complex Ginibre matrix, where the powers are of order $O(N)$.","Furthermore, by combining with a duality formula, we obtain precise large deviation probabilities of the smallest eigenvalue of the Laguerre unitary ensemble.","Our proof is based on a refined Riemann-Hilbert analysis for planar orthogonal polynomials using the partial Schlesinger transform."],"url":"http://arxiv.org/abs/2402.18983v1","category":"math-ph"}
{"created":"2024-02-29 09:26:46","title":"Graph Generation via Spectral Diffusion","abstract":"In this paper, we present GRASP, a novel graph generative model based on 1) the spectral decomposition of the graph Laplacian matrix and 2) a diffusion process. Specifically, we propose to use a denoising model to sample eigenvectors and eigenvalues from which we can reconstruct the graph Laplacian and adjacency matrix. Our permutation invariant model can also handle node features by concatenating them to the eigenvectors of each node. Using the Laplacian spectrum allows us to naturally capture the structural characteristics of the graph and work directly in the node space while avoiding the quadratic complexity bottleneck that limits the applicability of other methods. This is achieved by truncating the spectrum, which as we show in our experiments results in a faster yet accurate generative process. An extensive set of experiments on both synthetic and real world graphs demonstrates the strengths of our model against state-of-the-art alternatives.","sentences":["In this paper, we present GRASP, a novel graph generative model based on 1) the spectral decomposition of the graph Laplacian matrix and 2) a diffusion process.","Specifically, we propose to use a denoising model to sample eigenvectors and eigenvalues from which we can reconstruct the graph Laplacian and adjacency matrix.","Our permutation invariant model can also handle node features by concatenating them to the eigenvectors of each node.","Using the Laplacian spectrum allows us to naturally capture the structural characteristics of the graph and work directly in the node space while avoiding the quadratic complexity bottleneck that limits the applicability of other methods.","This is achieved by truncating the spectrum, which as we show in our experiments results in a faster yet accurate generative process.","An extensive set of experiments on both synthetic and real world graphs demonstrates the strengths of our model against state-of-the-art alternatives."],"url":"http://arxiv.org/abs/2402.18974v1","category":"cs.LG"}
{"created":"2024-02-29 09:26:41","title":"Privacy Management and Interface Design for a Smart House","abstract":"In today's life, more and more people tend to opt for a smart house. In this way, the idea of including technology has become popular worldwide. Despite this concept's many benefits, managing security remains an essential problem due to the shared activities. The Internet of Things system behind a smart house is based on several sensors to measure temperature, humidity, air quality, and movement. Because of being supervised every day through sensors and controlling their house only with a simple click, many people can be afraid of this new approach in terms of their privacy, and this fact can constrain them from following their habits. The security aspects should be constantly analyzed to keep the data's confidentiality and make people feel safe in their own houses. In this context, the current paper puts light on an alternative design of a platform in which the safety of homeowners is the primary purpose, and they maintain complete control over the data generated by smart devices. The current research highlights the role of security and interface design in controlling a smart house. The study underscores the importance of providing an interface that can be used easily by any person to manage data and live activities in a modern residence in an era dominated by continuously developing technology.","sentences":["In today's life, more and more people tend to opt for a smart house.","In this way, the idea of including technology has become popular worldwide.","Despite this concept's many benefits, managing security remains an essential problem due to the shared activities.","The Internet of Things system behind a smart house is based on several sensors to measure temperature, humidity, air quality, and movement.","Because of being supervised every day through sensors and controlling their house only with a simple click, many people can be afraid of this new approach in terms of their privacy, and this fact can constrain them from following their habits.","The security aspects should be constantly analyzed to keep the data's confidentiality and make people feel safe in their own houses.","In this context, the current paper puts light on an alternative design of a platform in which the safety of homeowners is the primary purpose, and they maintain complete control over the data generated by smart devices.","The current research highlights the role of security and interface design in controlling a smart house.","The study underscores the importance of providing an interface that can be used easily by any person to manage data and live activities in a modern residence in an era dominated by continuously developing technology."],"url":"http://arxiv.org/abs/2402.18973v1","category":"cs.CR"}
{"created":"2024-02-29 09:25:50","title":"Relationship between chain reactions in a nuclear reactor and multifractality","abstract":"A multifractal model of neutron evolution in a reactor is considered. For chain reactions, the dimension of the multifractal carrier, information and correlation dimensions, the entropy of the fractal set, the maximum and minimum values of the dimension, the multifractal spectrum function and other characteristics of multifractal neutron behavior are found. The use of geometric characteristics of a multifractal makes it possible to describe a stochastic system of hierarchically subordinate statistical ensembles, characterized by Cayley trees. A stationary distribution over hierarchical levels is established, which reduces to the Tsallis power law. Some possibilities for using fractal patterns in the theory of nuclear reactors are indicated.","sentences":["A multifractal model of neutron evolution in a reactor is considered.","For chain reactions, the dimension of the multifractal carrier, information and correlation dimensions, the entropy of the fractal set, the maximum and minimum values of the dimension, the multifractal spectrum function and other characteristics of multifractal neutron behavior are found.","The use of geometric characteristics of a multifractal makes it possible to describe a stochastic system of hierarchically subordinate statistical ensembles, characterized by Cayley trees.","A stationary distribution over hierarchical levels is established, which reduces to the Tsallis power law.","Some possibilities for using fractal patterns in the theory of nuclear reactors are indicated."],"url":"http://arxiv.org/abs/2402.18972v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-29 09:12:12","title":"Vector Valued G\u00e5rding Inequality for pseudo-differential operators on compact homogeneous manifolds","abstract":"We obtain sufficient conditions in order to obtain a sharp G\\r{a}rding inequality for pseudo-differential operators acting on vector-valued functions on compact Lie groups. As a consequence, we obtain a sharp G\\r{a}rding inequality for compact homogeneous vector bundles and compact homogeneous manifolds. The sharp G\\r{a}rding inequality is the strongest lower bound estimate known to hold for systems on $\\mathbb{R}^n$, and the aim of this paper is to extend this property to systems on compact Lie groups and compact homogeneous manifolds. As an application, we establish existence and uniqueness of solution to a class of systems of initial value problems of pseudo-differential equations on compact Lie groups and compact homogeneous manifolds.","sentences":["We obtain sufficient conditions in order to obtain a sharp G\\r{a}rding inequality for pseudo-differential operators acting on vector-valued functions on compact Lie groups.","As a consequence, we obtain a sharp G\\r{a}rding inequality for compact homogeneous vector bundles and compact homogeneous manifolds.","The sharp G\\r{a}rding inequality is the strongest lower bound estimate known to hold for systems on $\\mathbb{R}^n$, and the aim of this paper is to extend this property to systems on compact Lie groups and compact homogeneous manifolds.","As an application, we establish existence and uniqueness of solution to a class of systems of initial value problems of pseudo-differential equations on compact Lie groups and compact homogeneous manifolds."],"url":"http://arxiv.org/abs/2402.18966v1","category":"math.AP"}
{"created":"2024-02-29 08:34:55","title":"Getting Saturated with Induction","abstract":"Induction in saturation-based first-order theorem proving is a new exciting direction in the automation of inductive reasoning. In this paper we survey our work on integrating induction directly into the saturation-based proof search framework of first-order theorem proving. We describe our induction inference rules proving properties with inductively defined datatypes and integers. We also present additional reasoning heuristics for strengthening inductive reasoning, as well as for using induction hypotheses and recursive function definitions for guiding induction. We present exhaustive experimental results demonstrating the practical impact of our approach as implemented within Vampire.   This is an extended version of a Principles of Systems Design 2022 paper with the same title and the same authors.","sentences":["Induction in saturation-based first-order theorem proving is a new exciting direction in the automation of inductive reasoning.","In this paper we survey our work on integrating induction directly into the saturation-based proof search framework of first-order theorem proving.","We describe our induction inference rules proving properties with inductively defined datatypes and integers.","We also present additional reasoning heuristics for strengthening inductive reasoning, as well as for using induction hypotheses and recursive function definitions for guiding induction.","We present exhaustive experimental results demonstrating the practical impact of our approach as implemented within Vampire.   ","This is an extended version of a Principles of Systems Design 2022 paper with the same title and the same authors."],"url":"http://arxiv.org/abs/2402.18954v1","category":"cs.LO"}
{"created":"2024-02-29 08:29:03","title":"Percept, Chat, and then Adapt: Multimodal Knowledge Transfer of Foundation Models for Open-World Video Recognition","abstract":"Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations. Alternatively, foundation models with rich knowledge have recently shown their generalization power. However, how to apply such knowledge has not been fully explored for open-world video recognition. To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition. We name it PCA, based on three stages of Percept, Chat, and Adapt. First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge. Second, we generate rich linguistic semantics as external textual knowledge in Chat stage. Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks. We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe. Our approach achieves state-of-the-art performance on all three datasets.","sentences":["Open-world video recognition is challenging since traditional networks are not generalized well on complex environment variations.","Alternatively, foundation models with rich knowledge have recently shown their generalization power.","However, how to apply such knowledge has not been fully explored for open-world video recognition.","To this end, we propose a generic knowledge transfer pipeline, which progressively exploits and integrates external multimodal knowledge from foundation models to boost open-world video recognition.","We name it PCA, based on three stages of Percept, Chat, and Adapt.","First, we perform Percept process to reduce the video domain gap and obtain external visual knowledge.","Second, we generate rich linguistic semantics as external textual knowledge in Chat stage.","Finally, we blend external multimodal knowledge in Adapt stage, by inserting multimodal knowledge adaptation modules into networks.","We conduct extensive experiments on three challenging open-world video benchmarks, i.e., TinyVIRAT, ARID, and QV-Pipe.","Our approach achieves state-of-the-art performance on all three datasets."],"url":"http://arxiv.org/abs/2402.18951v1","category":"cs.CV"}
{"created":"2024-02-29 08:26:39","title":"Towards the boundary of the fine curve graph","abstract":"The fine curve graph was introduced as a geometric tool to study the homeomorphisms of surfaces. In this paper we study the Gromov boundary of this space and the local topology near points associated with minimal measurable foliations. We then give several applications including finding explicit elements with positive stable commutator length, and proving a Tits alternative for subgroups of $\\textrm{Homeo}(S)$ containing a pseudo-Anosov map, generalizing a result of Hurtado-Xue.","sentences":["The fine curve graph was introduced as a geometric tool to study the homeomorphisms of surfaces.","In this paper we study the Gromov boundary of this space and the local topology near points associated with minimal measurable foliations.","We then give several applications including finding explicit elements with positive stable commutator length, and proving a Tits alternative for subgroups of $\\textrm{Homeo}(S)$ containing a pseudo-Anosov map, generalizing a result of Hurtado-Xue."],"url":"http://arxiv.org/abs/2402.18948v1","category":"math.GT"}
{"created":"2024-02-29 08:25:32","title":"Real-Time Adaptive Safety-Critical Control with Gaussian Processes in High-Order Uncertain Models","abstract":"This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments. Our approach consists of two phases. The initial phase is centered on a novel sparse Gaussian process (GP) framework. We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability. Subsequently, the hyperparameters of the Gaussian model are trained with a specially compound kernel, and the Gaussian model's online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from new samples, in conjunction with the learned hyperparameters. In the second phase, we propose a safety filter based on high-order control barrier functions (HOCBFs), synergized with the previously trained learning model. By leveraging the compound kernel from the first phase, we effectively address the inherent limitations of GPs in handling high-dimensional problems for real-time applications. The derived controller ensures a rigorous lower bound on the probability of satisfying the safety specification. Finally, the efficacy of our proposed algorithm is demonstrated through real-time obstacle avoidance experiments executed using both a simulation platform and a real-world 7-DOF robot.","sentences":["This paper presents an adaptive online learning framework for systems with uncertain parameters to ensure safety-critical control in non-stationary environments.","Our approach consists of two phases.","The initial phase is centered on a novel sparse Gaussian process (GP) framework.","We first integrate a forgetting factor to refine a variational sparse GP algorithm, thus enhancing its adaptability.","Subsequently, the hyperparameters of the Gaussian model are trained with a specially compound kernel, and the Gaussian model's online inferential capability and computational efficiency are strengthened by updating a solitary inducing point derived from new samples, in conjunction with the learned hyperparameters.","In the second phase, we propose a safety filter based on high-order control barrier functions (HOCBFs), synergized with the previously trained learning model.","By leveraging the compound kernel from the first phase, we effectively address the inherent limitations of GPs in handling high-dimensional problems for real-time applications.","The derived controller ensures a rigorous lower bound on the probability of satisfying the safety specification.","Finally, the efficacy of our proposed algorithm is demonstrated through real-time obstacle avoidance experiments executed using both a simulation platform and a real-world 7-DOF robot."],"url":"http://arxiv.org/abs/2402.18946v1","category":"cs.LG"}
{"created":"2024-02-29 08:18:08","title":"Three-dimensional atomic interface between metal and oxide in Zr-ZrO2 nanoparticles","abstract":"Metal-oxide interfaces with poor coherency have unique properties comparing to the bulk materials and offer broad applications in the fields of heterogeneous catalysis, battery, and electronics. However, current understanding of the three-dimensional (3D) atomic metal-oxide interfaces remains limited because of their inherent structural complexity and limitations of conventional two-dimensional imaging techniques. Here, we determine the 3D atomic structure of metal-oxide interfaces in zirconium-zirconia nanoparticles using atomic-resolution electron tomography. We quantitatively analyze the atomic concentration and the degree of oxidation, and find the coherency and translational symmetry of the interfaces are broken. Moreover, we observe porous structures such as Zr vacancies and nano-pores and investigate their distribution. Our findings provide a clear 3D atomic picture of metal-oxide interface with direct experimental evidence. We anticipate this work could encourage future studies on fundamental problems of oxides such as interfacial structures in semiconductor and atomic motion during oxidation process.","sentences":["Metal-oxide interfaces with poor coherency have unique properties comparing to the bulk materials and offer broad applications in the fields of heterogeneous catalysis, battery, and electronics.","However, current understanding of the three-dimensional (3D) atomic metal-oxide interfaces remains limited because of their inherent structural complexity and limitations of conventional two-dimensional imaging techniques.","Here, we determine the 3D atomic structure of metal-oxide interfaces in zirconium-zirconia nanoparticles using atomic-resolution electron tomography.","We quantitatively analyze the atomic concentration and the degree of oxidation, and find the coherency and translational symmetry of the interfaces are broken.","Moreover, we observe porous structures such as Zr vacancies and nano-pores and investigate their distribution.","Our findings provide a clear 3D atomic picture of metal-oxide interface with direct experimental evidence.","We anticipate this work could encourage future studies on fundamental problems of oxides such as interfacial structures in semiconductor and atomic motion during oxidation process."],"url":"http://arxiv.org/abs/2402.18943v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 08:12:02","title":"End-to-End Quantum Vision Transformer: Towards Practical Quantum Speedup in Large-Scale Models","abstract":"The field of quantum deep learning presents significant opportunities for advancing computational capabilities, yet it faces a major obstacle in the form of the ``information loss problem'' due to the inherent limitations of the necessary quantum tomography in scaling quantum deep neural networks. This paper introduces an end-to-end Quantum Vision Transformer (QViT), which incorporates an innovative quantum residual connection technique, to overcome these challenges and therefore optimize quantum computing processes in deep learning. Our thorough complexity analysis of the QViT reveals a theoretically exponential and empirically polynomial speedup, showcasing the model's efficiency and potential in quantum computing applications. We conducted extensive numerical tests on modern, large-scale transformers and datasets, establishing the QViT as a pioneering advancement in applying quantum deep neural networks in practical scenarios. Our work provides a comprehensive quantum deep learning paradigm, which not only demonstrates the versatility of current quantum linear algebra algorithms but also promises to enhance future research and development in quantum deep learning.","sentences":["The field of quantum deep learning presents significant opportunities for advancing computational capabilities, yet it faces a major obstacle in the form of the ``information loss problem'' due to the inherent limitations of the necessary quantum tomography in scaling quantum deep neural networks.","This paper introduces an end-to-end Quantum Vision Transformer (QViT), which incorporates an innovative quantum residual connection technique, to overcome these challenges and therefore optimize quantum computing processes in deep learning.","Our thorough complexity analysis of the QViT reveals a theoretically exponential and empirically polynomial speedup, showcasing the model's efficiency and potential in quantum computing applications.","We conducted extensive numerical tests on modern, large-scale transformers and datasets, establishing the QViT as a pioneering advancement in applying quantum deep neural networks in practical scenarios.","Our work provides a comprehensive quantum deep learning paradigm, which not only demonstrates the versatility of current quantum linear algebra algorithms but also promises to enhance future research and development in quantum deep learning."],"url":"http://arxiv.org/abs/2402.18940v1","category":"quant-ph"}
{"created":"2024-02-29 08:03:04","title":"Miniaturized on-chip spectrometer enabled by electrochromic modulation","abstract":"Miniaturized on-chip spectrometers with small footprints, lightweight, and low cost are in great demand for portable optical sensing, lab-on-chip systems, and so on. Such miniaturized spectrometers are usually based on engineered spectral response units and then reconstruct unknown spectra with algorithms. However, due to the limited footprints of computational on-chip spectrometers, the recovered spectral resolution is limited by the number of integrated spectral response units/filters. Thus, it is challenging to improve the spectral resolution without increasing the number of used filters. Here we present a computational on-chip spectrometer using electrochromic filters that can be electrochemically modulated to increase the efficient sampling number for higher spectral resolution. These filters are directly integrated on top of the photodetector pixels, and the spectral modulation of the filters results from redox reactions during the dual injection of ions and electrons into the electrochromic material. We experimentally demonstrate that the spectral resolution of the proposed spectrometer can be effectively improved as the number of applied voltages increases. The average difference of the peak wavelengths between the reconstructed and the reference spectra decreases from 14.48 nm to 2.57 nm. We also demonstrate the proposed spectrometer can be worked with only four or two filter units, assisted by electrochromic modulation. This strategy suggests a new way to enhance the performance of miniaturized spectrometers with tunable spectral filters for high resolution, low-cost, and portable spectral sensing, and would also inspire the exploration of other stimulus responses such as photochromic and force-chromic, etc, on computational spectrometers.","sentences":["Miniaturized on-chip spectrometers with small footprints, lightweight, and low cost are in great demand for portable optical sensing, lab-on-chip systems, and so on.","Such miniaturized spectrometers are usually based on engineered spectral response units and then reconstruct unknown spectra with algorithms.","However, due to the limited footprints of computational on-chip spectrometers, the recovered spectral resolution is limited by the number of integrated spectral response units/filters.","Thus, it is challenging to improve the spectral resolution without increasing the number of used filters.","Here we present a computational on-chip spectrometer using electrochromic filters that can be electrochemically modulated to increase the efficient sampling number for higher spectral resolution.","These filters are directly integrated on top of the photodetector pixels, and the spectral modulation of the filters results from redox reactions during the dual injection of ions and electrons into the electrochromic material.","We experimentally demonstrate that the spectral resolution of the proposed spectrometer can be effectively improved as the number of applied voltages increases.","The average difference of the peak wavelengths between the reconstructed and the reference spectra decreases from 14.48 nm to 2.57 nm.","We also demonstrate the proposed spectrometer can be worked with only four or two filter units, assisted by electrochromic modulation.","This strategy suggests a new way to enhance the performance of miniaturized spectrometers with tunable spectral filters for high resolution, low-cost, and portable spectral sensing, and would also inspire the exploration of other stimulus responses such as photochromic and force-chromic, etc, on computational spectrometers."],"url":"http://arxiv.org/abs/2402.18935v1","category":"physics.optics"}
{"created":"2024-02-29 08:01:31","title":"Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration","abstract":"Establishing dense anatomical correspondence across distinct imaging modalities is a foundational yet challenging procedure for numerous medical image analysis studies and image-guided radiotherapy. Existing multi-modality image registration algorithms rely on statistical-based similarity measures or local structural image representations. However, the former is sensitive to locally varying noise, while the latter is not discriminative enough to cope with complex anatomical structures in multimodal scans, causing ambiguity in determining the anatomical correspondence across scans with different modalities. In this paper, we propose a modality-agnostic structural representation learning method, which leverages Deep Neighbourhood Self-similarity (DNS) and anatomy-aware contrastive learning to learn discriminative and contrast-invariance deep structural image representations (DSIR) without the need for anatomical delineations or pre-aligned training images. We evaluate our method on multiphase CT, abdomen MR-CT, and brain MR T1w-T2w registration. Comprehensive results demonstrate that our method is superior to the conventional local structural representation and statistical-based similarity measures in terms of discriminability and accuracy.","sentences":["Establishing dense anatomical correspondence across distinct imaging modalities is a foundational yet challenging procedure for numerous medical image analysis studies and image-guided radiotherapy.","Existing multi-modality image registration algorithms rely on statistical-based similarity measures or local structural image representations.","However, the former is sensitive to locally varying noise, while the latter is not discriminative enough to cope with complex anatomical structures in multimodal scans, causing ambiguity in determining the anatomical correspondence across scans with different modalities.","In this paper, we propose a modality-agnostic structural representation learning method, which leverages Deep Neighbourhood Self-similarity (DNS) and anatomy-aware contrastive learning to learn discriminative and contrast-invariance deep structural image representations (DSIR) without the need for anatomical delineations or pre-aligned training images.","We evaluate our method on multiphase CT, abdomen MR-CT, and brain MR T1w-T2w registration.","Comprehensive results demonstrate that our method is superior to the conventional local structural representation and statistical-based similarity measures in terms of discriminability and accuracy."],"url":"http://arxiv.org/abs/2402.18933v1","category":"cs.CV"}
{"created":"2024-02-29 07:43:19","title":"Global well-posedness and long time behavior of 2D MHD equations with partial dissipation in half space","abstract":"In this paper, we obtain the low order global well-posedness and the asymptotic behavior of solution of 2D MHD problem with partial dissipation in half space with non-slip boundary condition. When magnetic field equal zero, the system be reduced to partial dissipation Navier-Stokes equation, so this result also implies the stabilizing effects of magnetic field in electrically conducting fluids. We use the resolvent estimate method to obtain the long time behavior for the solution of weak diffusion system, which is not necessary to prove global well-posedness.","sentences":["In this paper, we obtain the low order global well-posedness and the asymptotic behavior of solution of 2D MHD problem with partial dissipation in half space with non-slip boundary condition.","When magnetic field equal zero, the system be reduced to partial dissipation Navier-Stokes equation, so this result also implies the stabilizing effects of magnetic field in electrically conducting fluids.","We use the resolvent estimate method to obtain the long time behavior for the solution of weak diffusion system, which is not necessary to prove global well-posedness."],"url":"http://arxiv.org/abs/2402.18928v1","category":"math.AP"}
{"created":"2024-02-29 07:29:53","title":"Electromagnetically induced transparency with magnetically-induced $\u0394F=0, m_F=0 \\rightarrow m_F=0$ probe transition","abstract":"Interest in magnetically induced (MI) transitions of alkali metal atoms is caused by the fact that their intensities can exceed the intensities of regular atomic transitions in a wide range of magnetic field (200 - 4000 G). The goal of this work was to form and study, for the first time, an electromagnetically induced transparency (EIT) resonance in a strong magnetic field using a probe radiation tuned to |Fg = F; mF = 0-> Fe = F; mF = 0> MI transition, which is forbidden in zero magnetic field. Two narrow-band linearly-polarized cw diode lasers were used to form a EIT resonance on a lambda-type system of Cs atomic D2 line in a strong transverse magnetic field (up to 1000 G). The resonance was formed in Cs atomic vapor nanocell with the atomic vapor column thickness of 850 nm.","sentences":["Interest in magnetically induced (MI) transitions of alkali metal atoms is caused by the fact that their intensities can exceed the intensities of regular atomic transitions in a wide range of magnetic field (200 - 4000 G).","The goal of this work was to form and study, for the first time, an electromagnetically induced transparency (EIT) resonance in a strong magnetic field using a probe radiation tuned to |Fg = F; mF = 0-> Fe = F; mF = 0> MI transition, which is forbidden in zero magnetic field.","Two narrow-band linearly-polarized cw diode lasers were used to form a EIT resonance on a lambda-type system of Cs atomic D2 line in a strong transverse magnetic field (up to 1000 G).","The resonance was formed in Cs atomic vapor nanocell with the atomic vapor column thickness of 850 nm."],"url":"http://arxiv.org/abs/2402.18924v1","category":"physics.atom-ph"}
{"created":"2024-02-29 07:29:28","title":"A Simple yet Effective Network based on Vision Transformer for Camouflaged Object and Salient Object Detection","abstract":"Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades. Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image. Previous works achieved good performance by stacking various hand-designed modules and multi-scale features. However, these carefully-designed complex networks often performed well on one task but not on another. In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones. Furthermore, to enhance the Transformer's ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM). We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target objects according to their size. Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD. Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method. The code is available at https://github.com/linuxsino/SENet.","sentences":["Camouflaged object detection (COD) and salient object detection (SOD) are two distinct yet closely-related computer vision tasks widely studied during the past decades.","Though sharing the same purpose of segmenting an image into binary foreground and background regions, their distinction lies in the fact that COD focuses on concealed objects hidden in the image, while SOD concentrates on the most prominent objects in the image.","Previous works achieved good performance by stacking various hand-designed modules and multi-scale features.","However, these carefully-designed complex networks often performed well on one task but not on another.","In this work, we propose a simple yet effective network (SENet) based on vision Transformer (ViT), by employing a simple design of an asymmetric ViT-based encoder-decoder structure, we yield competitive results on both tasks, exhibiting greater versatility than meticulously crafted ones.","Furthermore, to enhance the Transformer's ability to model local information, which is important for pixel-level binary segmentation tasks, we propose a local information capture module (LICM).","We also propose a dynamic weighted loss (DW loss) based on Binary Cross-Entropy (BCE) and Intersection over Union (IoU) loss, which guides the network to pay more attention to those smaller and more difficult-to-find target objects according to their size.","Moreover, we explore the issue of joint training of SOD and COD, and propose a preliminary solution to the conflict in joint training, further improving the performance of SOD.","Extensive experiments on multiple benchmark datasets demonstrate the effectiveness of our method.","The code is available at https://github.com/linuxsino/SENet."],"url":"http://arxiv.org/abs/2402.18922v1","category":"cs.CV"}
{"created":"2024-02-29 07:17:04","title":"Stop Relying on No-Choice and Do not Repeat the Moves: Optimal, Efficient and Practical Algorithms for Assortment Optimization","abstract":"We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization. The framework is useful in various real-world applications including ad placement, online retail, recommender systems, fine-tuning language models, amongst many. The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee. E.g., popularly used assortment selection algorithms often require the presence of a `strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications. In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \\emph{Plackett Luce} (PL) based user choices. We designed a novel concentration guarantee for estimating the score parameters of the PL model using `\\emph{Pairwise Rank-Breaking}', which builds the foundation of our proposed algorithms. Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods. Empirical evaluations corroborate our findings and outperform the existing baselines.","sentences":["We address the problem of active online assortment optimization problem with preference feedback, which is a framework for modeling user choices and subsetwise utility maximization.","The framework is useful in various real-world applications including ad placement, online retail, recommender systems, fine-tuning language models, amongst many.","The problem, although has been studied in the past, lacks an intuitive and practical solution approach with simultaneously efficient algorithm and optimal regret guarantee.","E.g., popularly used assortment selection algorithms often require the presence of a `strong reference' which is always included in the choice sets, further they are also designed to offer the same assortments repeatedly until the reference item gets selected -- all such requirements are quite unrealistic for practical applications.","In this paper, we designed efficient algorithms for the problem of regret minimization in assortment selection with \\emph{Plackett Luce} (PL) based user choices.","We designed a novel concentration guarantee for estimating the score parameters of the PL model using `\\emph{Pairwise Rank-Breaking}', which builds the foundation of our proposed algorithms.","Moreover, our methods are practical, provably optimal, and devoid of the aforementioned limitations of the existing methods.","Empirical evaluations corroborate our findings and outperform the existing baselines."],"url":"http://arxiv.org/abs/2402.18917v1","category":"cs.LG"}
{"created":"2024-02-29 07:12:48","title":"Observational constrained Weyl type $f(Q,T)$ gravity cosmological model and the dynamical system analysis","abstract":"Using the cosmological date sets, the cosmological parameters are constrained in this paper, with some well known form of Hubble parameter. To understand the dynamics of the Weyl type $f(Q,T)$, functional form $f(Q,T)$ has been introduced, where $Q$ and $T$ respectively represents the nonmetricity scalar and trace of energy-momentum tensor. Using the constrained values of the parameters, the other geometrical parameters are analysed and the accelerating behaviour has been shown. Further to get the complete evolutionary behaviour of the Universe, the dynamical system analysis has been performed.","sentences":["Using the cosmological date sets, the cosmological parameters are constrained in this paper, with some well known form of Hubble parameter.","To understand the dynamics of the Weyl type $f(Q,T)$, functional form $f(Q,T)$ has been introduced, where $Q$ and $T$ respectively represents the nonmetricity scalar and trace of energy-momentum tensor.","Using the constrained values of the parameters, the other geometrical parameters are analysed and the accelerating behaviour has been shown.","Further to get the complete evolutionary behaviour of the Universe, the dynamical system analysis has been performed."],"url":"http://arxiv.org/abs/2402.18915v1","category":"gr-qc"}
{"created":"2024-02-29 07:10:39","title":"Magnetism, heat capacity and electronic structure of EuCd$_2$P$_2$ in view of its colossal magnetoresistance","abstract":"The mechanism of the peculiar transport properties around the magnetic ordering temperature of semiconducting antiferromagnetic EuCd$_2$P$_2$ is not yet understood. With a huge peak in the resistivity observed above the N\\'eel temperature, $T_{\\rm N}=10.6\\,\\rm K$, it exhibits a colossal magnetoresistance effect. Recent reports on observations of ferromagnetic contributions above $T_{\\rm N}$ as well as metallic behavior below this temperature have motivated us to perform a comprehensive characterization of this material, including its resistivity, heat capacity, magnetic properties and electronic structure. Our transport measurements revealed quite different temperature dependence of resistivity with the maximum at $14\\,\\rm K$ instead of previously reported $18\\,\\rm K$. Low-field susceptibility data support the presence of static ferromagnetism above $T_{\\rm N}$ and show a complex behavior of the material at small applied magnetic fields. Namely, signatures of reorientation of magnetic domains are observed up to $T=16\\,\\rm K$. Our magnetization measurements indicate a magnetocrystalline anisotropy which also leads to a preferred alignment of the magnetic clusters above $T_{\\rm N}$. The momentum-resolved photoemission experiments at temperatures from $24\\,\\rm K$ down to $2.5\\,\\rm K$ indicate the permanent presence of a fundamental band gap without change of the electronic structure when going through $T_N$ that is in contradiction with previous results. We performed \\textit{ab initio} band structure calculations which are in good agreement with the measured photoemission data when assuming an antiferromagnetic ground state. Calculations for the ferromagnetic phase show a much smaller bandgap, indicating the importance of possible ferromagnetic contributions for the explanation of the colossal magnetoresistance effect in the related EuZn$_2$P$_2$.","sentences":["The mechanism of the peculiar transport properties around the magnetic ordering temperature of semiconducting antiferromagnetic EuCd$_2$P$_2$ is not yet understood.","With a huge peak in the resistivity observed above the N\\'eel temperature, $T_{\\rm N}=10.6\\,\\rm K$, it exhibits a colossal magnetoresistance effect.","Recent reports on observations of ferromagnetic contributions above $T_{\\rm N}$ as well as metallic behavior below this temperature have motivated us to perform a comprehensive characterization of this material, including its resistivity, heat capacity, magnetic properties and electronic structure.","Our transport measurements revealed quite different temperature dependence of resistivity with the maximum at $14\\,\\rm K$ instead of previously reported $18\\,\\rm K$. Low-field susceptibility data support the presence of static ferromagnetism above $T_{\\rm N}$ and show a complex behavior of the material at small applied magnetic fields.","Namely, signatures of reorientation of magnetic domains are observed up to $T=16\\,\\rm K$.","Our magnetization measurements indicate a magnetocrystalline anisotropy which also leads to a preferred alignment of the magnetic clusters above $T_{\\rm N}$.","The momentum-resolved photoemission experiments at temperatures from $24\\,\\rm K$ down to $2.5\\,\\rm K$ indicate the permanent presence of a fundamental band gap without change of the electronic structure when going through $T_N$ that is in contradiction with previous results.","We performed \\textit{ab initio} band structure calculations which are in good agreement with the measured photoemission data when assuming an antiferromagnetic ground state.","Calculations for the ferromagnetic phase show a much smaller bandgap, indicating the importance of possible ferromagnetic contributions for the explanation of the colossal magnetoresistance effect in the related EuZn$_2$P$_2$."],"url":"http://arxiv.org/abs/2402.18911v1","category":"cond-mat.str-el"}
{"created":"2024-02-29 07:06:53","title":"Boundary estimates and Green function's expansion for elliptic systems with random coefficients","abstract":"Focused on elliptic operators with stationary random coefficients of integrable correlations, stemming from stochastic homogenization theory, this paper primarily aims to investigate boundary estimates. As practical applications, we establish decay estimates for Green functions in both the quenched and annealed senses, as well as, some useful annealed estimates (including CLT-scaling) for boundary correctors. By extending Bella-Giunti-Otto's lemma in \\cite{Bella-Giunti-Otto17} (to the version with a boundary condition), we ultimately obtain an error estimate for two-scale expansions of Green functions at mixed derivatives level, and thereby build connections to other interesting fields.","sentences":["Focused on elliptic operators with stationary random coefficients of integrable correlations, stemming from stochastic homogenization theory, this paper primarily aims to investigate boundary estimates.","As practical applications, we establish decay estimates for Green functions in both the quenched and annealed senses, as well as, some useful annealed estimates (including CLT-scaling) for boundary correctors.","By extending Bella-Giunti-Otto's lemma in \\cite{Bella-Giunti-Otto17} (to the version with a boundary condition), we ultimately obtain an error estimate for two-scale expansions of Green functions at mixed derivatives level, and thereby build connections to other interesting fields."],"url":"http://arxiv.org/abs/2402.18907v1","category":"math.AP"}
{"created":"2024-02-29 06:56:58","title":"An Adaptive Hybrid Genetic and Large Neighborhood Search Approach for Multi-Attribute Vehicle Routing Problems","abstract":"Known for its dynamic utilization of destroy and repair operators, the Adaptive Large Neighborhood Search (ALNS) seeks to unearth high-quality solutions and has thus gained widespread acceptance as a meta-heuristic tool for tackling complex Combinatorial Optimization Problems (COPs). However, challenges arise when applying uniform parameters and acceptance criteria to diverse instances of the same COP, resulting in inconsistent performance outcomes. To address this inherent limitation, we propose the Adaptive Hybrid Genetic Search and Large Neighborhood Search (AHGSLNS), a novel approach designed to adapt ALNS parameters and acceptance criteria to the specific nuances of distinct COP instances. Our evaluation focuses on the Multi-Attribute Vehicle Routing Problem, a classical COP prevalent in real-world semi-automated storage and retrieval robotics systems. Empirical findings showcase that AHGSLNS not only competes effectively with ALNS under varying parameters but also exhibits superior performance in terms of convergence and stability. In alignment with our dedication to research transparency, the implementation of the proposed approach will be made publicly available.","sentences":["Known for its dynamic utilization of destroy and repair operators, the Adaptive Large Neighborhood Search (ALNS) seeks to unearth high-quality solutions and has thus gained widespread acceptance as a meta-heuristic tool for tackling complex Combinatorial Optimization Problems (COPs).","However, challenges arise when applying uniform parameters and acceptance criteria to diverse instances of the same COP, resulting in inconsistent performance outcomes.","To address this inherent limitation, we propose the Adaptive Hybrid Genetic Search and Large Neighborhood Search (AHGSLNS), a novel approach designed to adapt ALNS parameters and acceptance criteria to the specific nuances of distinct COP instances.","Our evaluation focuses on the Multi-Attribute Vehicle Routing Problem, a classical COP prevalent in real-world semi-automated storage and retrieval robotics systems.","Empirical findings showcase that AHGSLNS not only competes effectively with ALNS under varying parameters but also exhibits superior performance in terms of convergence and stability.","In alignment with our dedication to research transparency, the implementation of the proposed approach will be made publicly available."],"url":"http://arxiv.org/abs/2402.18903v1","category":"math.OC"}
{"created":"2024-02-29 06:51:55","title":"Beauty-charm Meson Family with Coupled Channel Effects and Their Strong Decays","abstract":"We systematically calculate the spectrum and hadronic decays of the beauty-charm system in a coupled channel framework. The unquenched effects are induced by the $^3P_0$ model. Our results can good explain the observed $B_c$ meson spectrum. For the coupled channel components, we predicted the $1S$ is about $4\\%$, the $2S$, $1P$, $2P$, $1D$, and $2D$ states are about $14\\%$, $10\\%$, $33\\%$, and $17\\%$ respectively. For the $3S$, $2P$ and $2D$ states, the strong decay is allowed, The hadronic decay widths of the $3^1S_0$, $3^3S_1$, $2^3P_2$ states are about 110 MeV, 69 MeV, and 3 MeV, respectively. While the decay widths of the $2^3D_1$, $2D$, $2D^\\prime$, and $2^3D_2$ states are 60 MeV, 149 MeV, 65 MeV, and 72 MeV, respectively.","sentences":["We systematically calculate the spectrum and hadronic decays of the beauty-charm system in a coupled channel framework.","The unquenched effects are induced by the $^3P_0$ model.","Our results can good explain the observed $B_c$ meson spectrum.","For the coupled channel components, we predicted the $1S$ is about $4\\%$, the $2S$, $1P$, $2P$, $1D$, and $2D$ states are about $14\\%$, $10\\%$, $33\\%$, and $17\\%$ respectively.","For the $3S$, $2P$ and $2D$ states, the strong decay is allowed, The hadronic decay widths of the $3^1S_0$, $3^3S_1$, $2^3P_2$ states are about 110 MeV, 69 MeV, and 3 MeV, respectively.","While the decay widths of the $2^3D_1$, $2D$, $2D^\\prime$, and $2^3D_2$ states are 60 MeV, 149 MeV, 65 MeV, and 72 MeV, respectively."],"url":"http://arxiv.org/abs/2402.18898v1","category":"hep-ph"}
{"created":"2024-02-29 06:47:28","title":"On the evolution of expected values in open quantum systems","abstract":"We derive a generalization of the Ehrenfest theorem valid for open quantum systems. From this result, we identify three factors contributing to the evolution of expected values: explicit time dependence of the observable, thermal interaction, and quantum coherence. When considering the local Hamiltonian as the observable, we obtain an alternative version of the first law of thermodynamics. In some cases, the non-thermal contributions to the energy rate of change can be expressed as the expected value of a Hermitian operator, so the power performed by the system can be considered a quantum observable. As an application, the pure dephasing process is reinterpreted from this perspective.","sentences":["We derive a generalization of the Ehrenfest theorem valid for open quantum systems.","From this result, we identify three factors contributing to the evolution of expected values: explicit time dependence of the observable, thermal interaction, and quantum coherence.","When considering the local Hamiltonian as the observable, we obtain an alternative version of the first law of thermodynamics.","In some cases, the non-thermal contributions to the energy rate of change can be expressed as the expected value of a Hermitian operator, so the power performed by the system can be considered a quantum observable.","As an application, the pure dephasing process is reinterpreted from this perspective."],"url":"http://arxiv.org/abs/2402.18895v1","category":"quant-ph"}
{"created":"2024-02-29 06:31:57","title":"Direct Visualization of Disorder Driven Electronic Liquid Crystal Phases in Dirac Nodal Line Semimetal GdSbTe","abstract":"Electronic liquid crystal (ELC) phases are spontaneous symmetry breaking states believed to arise from strong electron correlation in quantum materials such as cuprates and iron pnictides. Here, we report a direct observation of ELC phases in a Dirac nodal line (DNL) semimetal GdSbxTe2-x. As topological materials with symmetry protected Dirac or Weyl fermions are mostly weakly correlated, the discovery of real-space electronic nanostructures displaying incommensurate smectic charge modulation and intense local nematic order are anomalous and raise questions on the origin of emergent ELC phases. Specifically, we demonstrate how chemical substitution generates these symmetry breaking phases before the system undergoes a charge density wave - orthorhombic structural transition. We further show how dopants can induce nematicity via quasiparticle scattering interference. Our results highlight the importance of impurities in realizing ELC phases and present a new material platform for exploring the interplay among quenched disorder, topology and electron correlation.","sentences":["Electronic liquid crystal (ELC) phases are spontaneous symmetry breaking states believed to arise from strong electron correlation in quantum materials such as cuprates and iron pnictides.","Here, we report a direct observation of ELC phases in a Dirac nodal line (DNL) semimetal GdSbxTe2-x.","As topological materials with symmetry protected Dirac or Weyl fermions are mostly weakly correlated, the discovery of real-space electronic nanostructures displaying incommensurate smectic charge modulation and intense local nematic order are anomalous and raise questions on the origin of emergent ELC phases.","Specifically, we demonstrate how chemical substitution generates these symmetry breaking phases before the system undergoes a charge density wave - orthorhombic structural transition.","We further show how dopants can induce nematicity via quasiparticle scattering interference.","Our results highlight the importance of impurities in realizing ELC phases and present a new material platform for exploring the interplay among quenched disorder, topology and electron correlation."],"url":"http://arxiv.org/abs/2402.18893v1","category":"cond-mat.str-el"}
{"created":"2024-02-29 06:30:27","title":"Benchmarking phonon anharmonicity in machine learning interatomic potentials","abstract":"Machine learning approaches have recently emerged as powerful tools to probe structure-property relationships in crystals and molecules. Specifically, Machine learning interatomic potentials (MLIP) can accurately reproduce first-principles data at a cost similar to that of conventional interatomic potential approaches. While MLIP have been extensively tested across various classes of materials and molecules, a clear characterization of the anharmonic terms encoded in the MLIP is lacking. Here, we benchmark popular MLIP using the anharmonic vibrational Hamiltonian of ThO$_2$ in the fluorite crystal structure. This anharmonic Hamiltonian was constructed from density functional theory (DFT) using our highly accurate and efficient irreducible derivative methods, and then used to generate molecular dynamics trajectories. This data set was used to train three classes of MLIP: Gaussian Approximation Potentials, Artificial Neural Networks (ANN), and Graph Neural Networks (GNN). The results were assessed by directly comparing phonons and their interactions, as well as phonon linewidths, phonon lineshifts, and thermal conductivity. The models were also trained on a DFT molecular dynamics dataset, demonstrating good agreement up to fifth-order for the ANN and GNN. Our analysis demonstrates that MLIP have great potential for accurately characterizing anharmonicity in materials systems at a fraction of the cost of conventional first principles-based approaches.","sentences":["Machine learning approaches have recently emerged as powerful tools to probe structure-property relationships in crystals and molecules.","Specifically, Machine learning interatomic potentials (MLIP) can accurately reproduce first-principles data at a cost similar to that of conventional interatomic potential approaches.","While MLIP have been extensively tested across various classes of materials and molecules, a clear characterization of the anharmonic terms encoded in the MLIP is lacking.","Here, we benchmark popular MLIP using the anharmonic vibrational Hamiltonian of ThO$_2$ in the fluorite crystal structure.","This anharmonic Hamiltonian was constructed from density functional theory (DFT) using our highly accurate and efficient irreducible derivative methods, and then used to generate molecular dynamics trajectories.","This data set was used to train three classes of MLIP:","Gaussian Approximation Potentials, Artificial Neural Networks (ANN), and Graph Neural Networks (GNN).","The results were assessed by directly comparing phonons and their interactions, as well as phonon linewidths, phonon lineshifts, and thermal conductivity.","The models were also trained on a DFT molecular dynamics dataset, demonstrating good agreement up to fifth-order for the ANN and GNN.","Our analysis demonstrates that MLIP have great potential for accurately characterizing anharmonicity in materials systems at a fraction of the cost of conventional first principles-based approaches."],"url":"http://arxiv.org/abs/2402.18891v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 06:19:46","title":"Powering Monolithic and Hybrid Organic Optical Waveguides via Integrated Focused Micro-LEDs for Sustainable Photonic Circuits","abstract":"In the domain of mechanophotonics, achieving real-time applicability of organic crystals in visible light communication (VLC) technologies necessitates affordable light-emitting diodes (LEDs) as sources of light to run photonic devices through sustainable methods. Here in, we demonstrate an efficient strategy to excite (Z)-3-(3',5'-bis(trifluoromethyl)-[1,1'-biphenyl]-4-yl)-2-(4-methoxyphenyl) acrylonitrile (CF3OMe), 9,10-bis(phenylethynyl)anthracene (BPEA) and 2,2'-((1E,1'E)-hydrazine-1,2-diylidenebis(methaneylylidene))diphenol (SAA) flexible crystal waveguides utilizing UV LED source and transduce respective blue, orange and yellow fluorescence signals. The capability of the focused LED lies in its ability to (i) energize mechanically bent crystals at an angle of 180{\\deg}, (ii) evanescently excite the FL of a SAA waveguide using the FL of CF3OMe waveguide through energy transfer, and (iii) excite and split different signals in a 2X2 hybrid directional coupler based on SSA-BPEA crystals. These demonstrations underscore the practicality of the proposed technique for sustainable applications in photonic systems related to VLC.","sentences":["In the domain of mechanophotonics, achieving real-time applicability of organic crystals in visible light communication (VLC) technologies necessitates affordable light-emitting diodes (LEDs) as sources of light to run photonic devices through sustainable methods.","Here in, we demonstrate an efficient strategy to excite (Z)-3-(3',5'-bis(trifluoromethyl)-[1,1'-biphenyl]-4-yl)-2-(4-methoxyphenyl) acrylonitrile (CF3OMe), 9,10-bis(phenylethynyl)anthracene (BPEA) and 2,2'-((1E,1'E)-hydrazine-1,2-diylidenebis(methaneylylidene))diphenol (SAA) flexible crystal waveguides utilizing UV LED source and transduce respective blue, orange and yellow fluorescence signals.","The capability of the focused LED lies in its ability to (i) energize mechanically bent crystals at an angle of 180{\\deg}, (ii) evanescently excite the FL of a SAA waveguide using the FL of CF3OMe waveguide through energy transfer, and (iii) excite and split different signals in a 2X2 hybrid directional coupler based on SSA-BPEA crystals.","These demonstrations underscore the practicality of the proposed technique for sustainable applications in photonic systems related to VLC."],"url":"http://arxiv.org/abs/2402.18889v1","category":"physics.optics"}
{"created":"2024-02-29 06:05:13","title":"Quantum droplets with magnetic vortices in spinor dipolar Bose-Einstein condensates","abstract":"Motivated by the recent experimental realization of a Bose-Einstein condensate (BEC) of europium atoms, we investigate the self-bound droplet state of a europium BEC with spin degrees of freedom. Under a sufficiently weak magnetic field, the droplet has a torus shape with circulating spin vectors, which is referred to as a magnetic vortex. The ground state transforms from the torus to cigar shape through bistability with an increase in the magnetic field. Dynamical change of the magnetic field causes the torus to rotate due to the Einstein-de Haas effect. The magnetic vortices form a supersolid in a confined system.","sentences":["Motivated by the recent experimental realization of a Bose-Einstein condensate (BEC) of europium atoms, we investigate the self-bound droplet state of a europium BEC with spin degrees of freedom.","Under a sufficiently weak magnetic field, the droplet has a torus shape with circulating spin vectors, which is referred to as a magnetic vortex.","The ground state transforms from the torus to cigar shape through bistability with an increase in the magnetic field.","Dynamical change of the magnetic field causes the torus to rotate due to the Einstein-de Haas effect.","The magnetic vortices form a supersolid in a confined system."],"url":"http://arxiv.org/abs/2402.18885v1","category":"cond-mat.quant-gas"}
{"created":"2024-02-29 06:01:57","title":"Efficient Processing of Subsequent Densest Subgraph Query","abstract":"Dense subgraph extraction is a fundamental problem in graph analysis and data mining, aimed at identifying cohesive and densely connected substructures within a given graph. It plays a crucial role in various domains, including social network analysis, biological network analysis, recommendation systems, and community detection. However, extracting a subgraph with the highest node similarity is a lack of exploration. To address this problem, we studied the Member Selection Problem and extended it with a dynamic constraint variant. By incorporating dynamic constraints, our algorithm can adapt to changing conditions or requirements, allowing for more flexible and personalized subgraph extraction. This approach enables the algorithm to provide tailored solutions that meet specific needs, even in scenarios where constraints may vary over time. We also provide the theoretical analysis to show that our algorithm is 1/3-approximation. Eventually, the experiments show that our algorithm is effective and efficient in tackling the member selection problem with dynamic constraints.","sentences":["Dense subgraph extraction is a fundamental problem in graph analysis and data mining, aimed at identifying cohesive and densely connected substructures within a given graph.","It plays a crucial role in various domains, including social network analysis, biological network analysis, recommendation systems, and community detection.","However, extracting a subgraph with the highest node similarity is a lack of exploration.","To address this problem, we studied the Member Selection Problem and extended it with a dynamic constraint variant.","By incorporating dynamic constraints, our algorithm can adapt to changing conditions or requirements, allowing for more flexible and personalized subgraph extraction.","This approach enables the algorithm to provide tailored solutions that meet specific needs, even in scenarios where constraints may vary over time.","We also provide the theoretical analysis to show that our algorithm is 1/3-approximation.","Eventually, the experiments show that our algorithm is effective and efficient in tackling the member selection problem with dynamic constraints."],"url":"http://arxiv.org/abs/2402.18883v1","category":"cs.DS"}
{"created":"2024-02-29 05:59:55","title":"Exploring the evolution of structure growth in the universe with field-fluid interactions through dynamical stability analysis","abstract":"We investigate an interacting quintessence dark energy - dark matter scenario and its impact on structure formation by analyzing the evolution of scalar perturbations. The interaction is introduced by incorporating a non-zero source term into the continuity equations of the two sectors (with opposite signs), modeled as $\\bar{Q}_0 \\equiv \\alpha\\bar{\\rho}_{\\rm m}(H + \\kappa\\dot{\\phi})$. The coupling parameter $\\alpha$ and the parameter $\\lambda$ involved in quintessence potential $V(\\phi) = V_0e^{-\\lambda\\kappa\\phi}$, play crucial roles in governing the dynamics of evolution examined within the present framework. The cosmic evolution, within this context, is depicted as a first-order autonomous system of equations involving appropriately chosen dynamical variables. We analyzed the associated stability characteristics and growth rate of perturbations and obtained domains in the ($\\alpha-\\lambda$) parameter space for which fixed points can exhibit stable and non-phantom accelerating solutions. Depending on its magnitude, the coupling parameter $\\alpha$ has the potential to change the characteristics of certain critical points, altering them from attractors to repellers. This model effectively captures the evolutionary features of the universe across its various phases at both the background and perturbation levels. The issue of cosmic coincidence can also be addressed within the framework of this model. We also observed that for a moderate strength of coupling, the growth rate of matter perturbation extends into the distant future.","sentences":["We investigate an interacting quintessence dark energy - dark matter scenario and its impact on structure formation by analyzing the evolution of scalar perturbations.","The interaction is introduced by incorporating a non-zero source term into the continuity equations of the two sectors (with opposite signs), modeled as $\\bar{Q}_0 \\equiv \\alpha\\bar{\\rho}_{\\rm m}(H +","\\kappa\\dot{\\phi})$.","The coupling parameter $\\alpha$ and the parameter $\\lambda$ involved in quintessence potential $V(\\phi) = V_0e^{-\\lambda\\kappa\\phi}$, play crucial roles in governing the dynamics of evolution examined within the present framework.","The cosmic evolution, within this context, is depicted as a first-order autonomous system of equations involving appropriately chosen dynamical variables.","We analyzed the associated stability characteristics and growth rate of perturbations and obtained domains in the ($\\alpha-\\lambda$) parameter space for which fixed points can exhibit stable and non-phantom accelerating solutions.","Depending on its magnitude, the coupling parameter $\\alpha$ has the potential to change the characteristics of certain critical points, altering them from attractors to repellers.","This model effectively captures the evolutionary features of the universe across its various phases at both the background and perturbation levels.","The issue of cosmic coincidence can also be addressed within the framework of this model.","We also observed that for a moderate strength of coupling, the growth rate of matter perturbation extends into the distant future."],"url":"http://arxiv.org/abs/2402.18882v1","category":"gr-qc"}
{"created":"2024-02-29 05:57:35","title":"Dose Prediction Driven Radiotherapy Paramters Regression via Intra- and Inter-Relation Modeling","abstract":"Deep learning has facilitated the automation of radiotherapy by predicting accurate dose distribution maps. However, existing methods fail to derive the desirable radiotherapy parameters that can be directly input into the treatment planning system (TPS), impeding the full automation of radiotherapy. To enable more thorough automatic radiotherapy, in this paper, we propose a novel two-stage framework to directly regress the radiotherapy parameters, including a dose map prediction stage and a radiotherapy parameters regression stage. In stage one, we combine transformer and convolutional neural network (CNN) to predict realistic dose maps with rich global and local information, providing accurate dosimetric knowledge for the subsequent parameters regression. In stage two, two elaborate modules, i.e., an intra-relation modeling (Intra-RM) module and an inter-relation modeling (Inter-RM) module, are designed to exploit the organ-specific and organ-shared features for precise parameters regression. Experimental results on a rectal cancer dataset demonstrate the effectiveness of our method.","sentences":["Deep learning has facilitated the automation of radiotherapy by predicting accurate dose distribution maps.","However, existing methods fail to derive the desirable radiotherapy parameters that can be directly input into the treatment planning system (TPS), impeding the full automation of radiotherapy.","To enable more thorough automatic radiotherapy, in this paper, we propose a novel two-stage framework to directly regress the radiotherapy parameters, including a dose map prediction stage and a radiotherapy parameters regression stage.","In stage one, we combine transformer and convolutional neural network (CNN) to predict realistic dose maps with rich global and local information, providing accurate dosimetric knowledge for the subsequent parameters regression.","In stage two, two elaborate modules, i.e., an intra-relation modeling (Intra-RM) module and an inter-relation modeling (Inter-RM) module, are designed to exploit the organ-specific and organ-shared features for precise parameters regression.","Experimental results on a rectal cancer dataset demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2402.18879v1","category":"cs.CV"}
{"created":"2024-02-29 05:44:41","title":"Loss-aware Curriculum Learning for Heterogeneous Graph Neural Networks","abstract":"Heterogeneous Graph Neural Networks (HGNNs) are a class of deep learning models designed specifically for heterogeneous graphs, which are graphs that contain different types of nodes and edges. This paper investigates the application of curriculum learning techniques to improve the performance and robustness of Heterogeneous Graph Neural Networks (GNNs). To better classify the quality of the data, we design a loss-aware training schedule, named LTS that measures the quality of every nodes of the data and incorporate the training dataset into the model in a progressive manner that increases difficulty step by step. LTS can be seamlessly integrated into various frameworks, effectively reducing bias and variance, mitigating the impact of noisy data, and enhancing overall accuracy. Our findings demonstrate the efficacy of curriculum learning in enhancing HGNNs capabilities for analyzing complex graph-structured data. The code is public at https: //github.com/LARS-research/CLGNN/.","sentences":["Heterogeneous Graph Neural Networks (HGNNs) are a class of deep learning models designed specifically for heterogeneous graphs, which are graphs that contain different types of nodes and edges.","This paper investigates the application of curriculum learning techniques to improve the performance and robustness of Heterogeneous Graph Neural Networks (GNNs).","To better classify the quality of the data, we design a loss-aware training schedule, named LTS that measures the quality of every nodes of the data and incorporate the training dataset into the model in a progressive manner that increases difficulty step by step.","LTS can be seamlessly integrated into various frameworks, effectively reducing bias and variance, mitigating the impact of noisy data, and enhancing overall accuracy.","Our findings demonstrate the efficacy of curriculum learning in enhancing HGNNs capabilities for analyzing complex graph-structured data.","The code is public at https: //github.com/LARS-research/CLGNN/."],"url":"http://arxiv.org/abs/2402.18875v1","category":"cs.LG"}
{"created":"2024-02-29 05:39:05","title":"Evaluating the Gilbert-Varshamov Bound for Constrained Systems","abstract":"We revisit the well-known Gilbert-Varshamov (GV) bound for constrained systems. In 1991, Kolesnik and Krachkovsky showed that GV bound can be determined via the solution of some optimization problem. Later, Marcus and Roth (1992) modified the optimization problem and improved the GV bound in many instances. In this work, we provide explicit numerical procedures to solve these two optimization problems and hence, compute the bounds. We then show the procedures can be further simplified when we plot the respective curves. In the case where the graph presentation comprise a single state, we provide explicit formulas for both bounds.","sentences":["We revisit the well-known Gilbert-Varshamov (GV) bound for constrained systems.","In 1991, Kolesnik and Krachkovsky showed that GV bound can be determined via the solution of some optimization problem.","Later, Marcus and Roth (1992) modified the optimization problem and improved the GV bound in many instances.","In this work, we provide explicit numerical procedures to solve these two optimization problems and hence, compute the bounds.","We then show the procedures can be further simplified when we plot the respective curves.","In the case where the graph presentation comprise a single state, we provide explicit formulas for both bounds."],"url":"http://arxiv.org/abs/2402.18869v1","category":"cs.IT"}
{"created":"2024-02-29 05:38:35","title":"High-Fidelity Detection on $^{171} \\mathrm{Yb}^+$ Qubit via $^2D_{3/2}$ Shelving","abstract":"High-fidelity detection of quantum states is indispensable for implementing quantum error correction, a prerequisite for fault-tolerant quantum computation. For promising trapped ion qubits, however, the detection fidelity is inherently limited by state leakage. Here, we propose an efficient approach to enhance the fidelity of detecting $^{171} \\mathrm{Yb}^+$ qubits through $^2D_{3/2}$ state shelving techniques. Leveraging selective shelving and state-dependent fluorescence, we mitigate the impact of state leakage and experimentally realize a fidelity of 99.88(2)%, while over 99.99% fidelity is predicted by utilizing state-of-the-art hardwares. Meanwhile, we demonstrate the feasibility of mid-circuit measurements, a crucial step for recent implementations of quantum error correction, by mapping the hyperfine qubit to metastable levels. Our research provides an essential component for realizing fault-tolerant quantum information processing with trapped-ion systems in the near future.","sentences":["High-fidelity detection of quantum states is indispensable for implementing quantum error correction, a prerequisite for fault-tolerant quantum computation.","For promising trapped ion qubits, however, the detection fidelity is inherently limited by state leakage.","Here, we propose an efficient approach to enhance the fidelity of detecting $^{171} \\mathrm{Yb}^+$ qubits through $^2D_{3/2}$ state shelving techniques.","Leveraging selective shelving and state-dependent fluorescence, we mitigate the impact of state leakage and experimentally realize a fidelity of 99.88(2)%, while over 99.99% fidelity is predicted by utilizing state-of-the-art hardwares.","Meanwhile, we demonstrate the feasibility of mid-circuit measurements, a crucial step for recent implementations of quantum error correction, by mapping the hyperfine qubit to metastable levels.","Our research provides an essential component for realizing fault-tolerant quantum information processing with trapped-ion systems in the near future."],"url":"http://arxiv.org/abs/2402.18868v1","category":"quant-ph"}
{"created":"2024-02-29 05:36:08","title":"Message-Enhanced DeGroot Model","abstract":"Understanding the impact of messages on agents' opinions over social networks is important. However, to our best knowledge, there has been limited quantitative investigation into this phenomenon in the prior works. To address this gap, this paper proposes the Message-Enhanced DeGroot model. The Bounded Brownian Message model provides a quantitative description of the message evolution, jointly considering temporal continuity, randomness, and polarization from mass media theory. The Message-Enhanced DeGroot model, combining the Bounded Brownian Message model with the traditional DeGroot model, quantitatively describes the evolution of agents' opinions under the influence of messages. We theoretically study the probability distribution and statistics of the messages and agents' opinions and quantitatively analyze the impact of messages on opinions. We also conduct simulations to validate our analyses.","sentences":["Understanding the impact of messages on agents' opinions over social networks is important.","However, to our best knowledge, there has been limited quantitative investigation into this phenomenon in the prior works.","To address this gap, this paper proposes the Message-Enhanced DeGroot model.","The Bounded Brownian Message model provides a quantitative description of the message evolution, jointly considering temporal continuity, randomness, and polarization from mass media theory.","The Message-Enhanced DeGroot model, combining the Bounded Brownian Message model with the traditional DeGroot model, quantitatively describes the evolution of agents' opinions under the influence of messages.","We theoretically study the probability distribution and statistics of the messages and agents' opinions and quantitatively analyze the impact of messages on opinions.","We also conduct simulations to validate our analyses."],"url":"http://arxiv.org/abs/2402.18867v1","category":"eess.SP"}
{"created":"2024-02-29 05:34:05","title":"Dr. Strategy: Model-Based Generalist Agents with Strategic Dreaming","abstract":"Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent. However, there has not been much effort toward enhancing the strategy of dreaming itself. Therefore, it is a question whether and how an agent can \"dream better\" in a more structured and strategic way. In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy. The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming. This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy. With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way. In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks. The source code will be available at https://github.com/ahn-ml/drstrategy","sentences":["Model-based reinforcement learning (MBRL) has been a primary approach to ameliorating the sample efficiency issue as well as to make a generalist agent.","However, there has not been much effort toward enhancing the strategy of dreaming itself.","Therefore, it is a question whether and how an agent can \"dream better\" in a more structured and strategic way.","In this paper, inspired by the observation from cognitive science suggesting that humans use a spatial divide-and-conquer strategy in planning, we propose a new MBRL agent, called Dr. Strategy, which is equipped with a novel Dreaming Strategy.","The proposed agent realizes a version of divide-and-conquer-like strategy in dreaming.","This is achieved by learning a set of latent landmarks and then utilizing these to learn a landmark-conditioned highway policy.","With the highway policy, the agent can first learn in the dream to move to a landmark, and from there it tackles the exploration and achievement task in a more focused way.","In experiments, we show that the proposed model outperforms prior pixel-based MBRL methods in various visually complex and partially observable navigation tasks.","The source code will be available at https://github.com/ahn-ml/drstrategy"],"url":"http://arxiv.org/abs/2402.18866v1","category":"cs.LG"}
{"created":"2024-02-29 05:27:03","title":"Privacy-Preserving Autoencoder for Collaborative Object Detection","abstract":"Privacy is a crucial concern in collaborative machine vision where a part of a Deep Neural Network (DNN) model runs on the edge, and the rest is executed on the cloud. In such applications, the machine vision model does not need the exact visual content to perform its task. Taking advantage of this potential, private information could be removed from the data insofar as it does not significantly impair the accuracy of the machine vision system. In this paper, we present an autoencoder-style network integrated within an object detection pipeline, which generates a latent representation of the input image that preserves task-relevant information while removing private information. Our approach employs an adversarial training strategy that not only removes private information from the bottleneck of the autoencoder but also promotes improved compression efficiency for feature channels coded by conventional codecs like VVC-Intra. We assess the proposed system using a realistic evaluation framework for privacy, directly measuring face and license plate recognition accuracy. Experimental results show that our proposed method is able to reduce the bitrate significantly at the same object detection accuracy compared to coding the input images directly, while keeping the face and license plate recognition accuracy on the images recovered from the bottleneck features low, implying strong privacy protection.","sentences":["Privacy is a crucial concern in collaborative machine vision where a part of a Deep Neural Network (DNN) model runs on the edge, and the rest is executed on the cloud.","In such applications, the machine vision model does not need the exact visual content to perform its task.","Taking advantage of this potential, private information could be removed from the data insofar as it does not significantly impair the accuracy of the machine vision system.","In this paper, we present an autoencoder-style network integrated within an object detection pipeline, which generates a latent representation of the input image that preserves task-relevant information while removing private information.","Our approach employs an adversarial training strategy that not only removes private information from the bottleneck of the autoencoder but also promotes improved compression efficiency for feature channels coded by conventional codecs like VVC-Intra.","We assess the proposed system using a realistic evaluation framework for privacy, directly measuring face and license plate recognition accuracy.","Experimental results show that our proposed method is able to reduce the bitrate significantly at the same object detection accuracy compared to coding the input images directly, while keeping the face and license plate recognition accuracy on the images recovered from the bottleneck features low, implying strong privacy protection."],"url":"http://arxiv.org/abs/2402.18864v1","category":"eess.IV"}
{"created":"2024-02-29 05:17:36","title":"Taking Second-life Batteries from Exhausted to Empowered using Experiments, Data Analysis, and Health Estimation","abstract":"The reuse of retired electric vehicle (EV) batteries in electric grid energy storage emerges as a promising strategy to address environmental concerns and boost economic value. This study concentrates on devising health monitoring algorithms for retired batteries (BMS$_2$) deployed in grid storage applications. Over 15 months of testing, we compile, analyze, and publicly share a dataset of second-life (SL) batteries, implementing a cycling protocol simulating grid energy storage load profiles within a 3 V-4 V voltage window. Four machine learning-based health estimation models, relying on BMS$_2$ features and initial capacity, are developed and compared, with the selected model achieving a Mean Absolute Percentage Error (MAPE) below 2.3% on test data. Additionally, an adaptive online health estimation algorithm is proposed by integrating a clustering-based method, limiting estimation errors during online deployment. These results constitute an initial proof of concept, showcasing the feasibility of repurposing retired batteries for second-life applications. Based on obtained data and representative power demand, these SL batteries exhibit the potential, under specific conditions, for over a decade of grid energy storage use.","sentences":["The reuse of retired electric vehicle (EV) batteries in electric grid energy storage emerges as a promising strategy to address environmental concerns and boost economic value.","This study concentrates on devising health monitoring algorithms for retired batteries (BMS$_2$) deployed in grid storage applications.","Over 15 months of testing, we compile, analyze, and publicly share a dataset of second-life (SL) batteries, implementing a cycling protocol simulating grid energy storage load profiles within a 3 V-4 V voltage window.","Four machine learning-based health estimation models, relying on BMS$_2$ features and initial capacity, are developed and compared, with the selected model achieving a Mean Absolute Percentage Error (MAPE) below 2.3% on test data.","Additionally, an adaptive online health estimation algorithm is proposed by integrating a clustering-based method, limiting estimation errors during online deployment.","These results constitute an initial proof of concept, showcasing the feasibility of repurposing retired batteries for second-life applications.","Based on obtained data and representative power demand, these SL batteries exhibit the potential, under specific conditions, for over a decade of grid energy storage use."],"url":"http://arxiv.org/abs/2402.18859v1","category":"cs.LG"}
{"created":"2024-02-29 05:11:32","title":"Graphics Processing Unit/Artificial Neural Network-accelerated large-eddy simulation of turbulent combustion: Application to swirling premixed flames","abstract":"Within the scope of reacting flow simulations, the real-time direct integration (DI) of stiff ordinary differential equations (ODE) for the computation of chemical kinetics stands as the primary demand on computational resources. Meanwhile, as the number of transport equations that need to be solved increases, the computational cost grows more substantially, particularly for those combustion models involving direct coupling of chemistry and flow such as the transported probability density function model. In the current study, an integrated Graphics Processing Unit-Artificial Neural Network (GPU-ANN) framework is introduced to comply with heavy computational costs while maintaining high fidelity. Within this framework, a GPU-based solver is employed to solve partial differential equations and compute thermal and transport properties, and an ANN is utilized to replace the calculation of reaction rates. Large eddy simulations of two swirling flames provide a robust validation, affirming and extending the GPU-ANN approach's applicability to challenging scenarios. The simulation results demonstrate a strong correlation in the macro flame structure and statistical characteristics between the GPU-ANN approach and the traditional Central Processing Unit (CPU)-based solver with DI. This comparison indicates that the GPU-ANN approach is capable of attaining the same degree of precision as the conventional CPU-DI solver, even in more complex scenarios. In addition, the overall speed-up factor for the GPU-ANN approach is over two orders of magnitude. This study establishes the potential groundwork for widespread application of the proposed GPU-ANN approach in combustion simulations, addressing various and complex scenarios based on detailed chemistry, while significantly reducing computational costs.","sentences":["Within the scope of reacting flow simulations, the real-time direct integration (DI) of stiff ordinary differential equations (ODE) for the computation of chemical kinetics stands as the primary demand on computational resources.","Meanwhile, as the number of transport equations that need to be solved increases, the computational cost grows more substantially, particularly for those combustion models involving direct coupling of chemistry and flow such as the transported probability density function model.","In the current study, an integrated Graphics Processing Unit-Artificial Neural Network (GPU-ANN) framework is introduced to comply with heavy computational costs while maintaining high fidelity.","Within this framework, a GPU-based solver is employed to solve partial differential equations and compute thermal and transport properties, and an ANN is utilized to replace the calculation of reaction rates.","Large eddy simulations of two swirling flames provide a robust validation, affirming and extending the GPU-ANN approach's applicability to challenging scenarios.","The simulation results demonstrate a strong correlation in the macro flame structure and statistical characteristics between the GPU-ANN approach and the traditional Central Processing Unit (CPU)-based solver with DI.","This comparison indicates that the GPU-ANN approach is capable of attaining the same degree of precision as the conventional CPU-DI solver, even in more complex scenarios.","In addition, the overall speed-up factor for the GPU-ANN approach is over two orders of magnitude.","This study establishes the potential groundwork for widespread application of the proposed GPU-ANN approach in combustion simulations, addressing various and complex scenarios based on detailed chemistry, while significantly reducing computational costs."],"url":"http://arxiv.org/abs/2402.18858v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 05:08:59","title":"Anatomy-guided fiber trajectory distribution estimation for cranial nerves tractography","abstract":"Diffusion MRI tractography is an important tool for identifying and analyzing the intracranial course of cranial nerves (CNs). However, the complex environment of the skull base leads to ambiguous spatial correspondence between diffusion directions and fiber geometry, and existing diffusion tractography methods of CNs identification are prone to producing erroneous trajectories and missing true positive connections. To overcome the above challenge, we propose a novel CNs identification framework with anatomy-guided fiber trajectory distribution, which incorporates anatomical shape prior knowledge during the process of CNs tracing to build diffusion tensor vector fields. We introduce higher-order streamline differential equations for continuous flow field representations to directly characterize the fiber trajectory distribution of CNs from the tract-based level. The experimental results on the vivo HCP dataset and the clinical MDM dataset demonstrate that the proposed method reduces false-positive fiber production compared to competing methods and produces reconstructed CNs (i.e. CN II, CN III, CN V, and CN VII/VIII) that are judged to better correspond to the known anatomy.","sentences":["Diffusion MRI tractography is an important tool for identifying and analyzing the intracranial course of cranial nerves (CNs).","However, the complex environment of the skull base leads to ambiguous spatial correspondence between diffusion directions and fiber geometry, and existing diffusion tractography methods of CNs identification are prone to producing erroneous trajectories and missing true positive connections.","To overcome the above challenge, we propose a novel CNs identification framework with anatomy-guided fiber trajectory distribution, which incorporates anatomical shape prior knowledge during the process of CNs tracing to build diffusion tensor vector fields.","We introduce higher-order streamline differential equations for continuous flow field representations to directly characterize the fiber trajectory distribution of CNs from the tract-based level.","The experimental results on the vivo HCP dataset and the clinical MDM dataset demonstrate that the proposed method reduces false-positive fiber production compared to competing methods and produces reconstructed CNs (i.e. CN II, CN III, CN V, and CN VII/VIII) that are judged to better correspond to the known anatomy."],"url":"http://arxiv.org/abs/2402.18856v1","category":"eess.IV"}
{"created":"2024-02-29 05:07:19","title":"Work Sum Rule for Open Quantum Systems","abstract":"A key question in the thermodynamics of open quantum systems is how to partition thermodynamic quantities such as entropy, work, and internal energy between the system and its environment. We show that the only partition under which entropy is non-singular is based on a partition of Hilbert-space, which assigns half the system-environment coupling to the system and half to the environment. However, quantum work partitions non-trivially under Hilbert-space partition, and we derive a Work Sum Rule that accounts for quantum work at a distance. All state functions of the system are shown to be path independent once this nonlocal quantum work is properly accounted for. The thermodynamics of two classes of quasi-statically driven open quantum systems is analyzed: systems with a finite environment in the grand canonical ensemble, and systems with an unbounded environment. Our results are illustrated with applications to a time-dependent two-level system and the driven resonant-level model.","sentences":["A key question in the thermodynamics of open quantum systems is how to partition thermodynamic quantities such as entropy, work, and internal energy between the system and its environment.","We show that the only partition under which entropy is non-singular is based on a partition of Hilbert-space, which assigns half the system-environment coupling to the system and half to the environment.","However, quantum work partitions non-trivially under Hilbert-space partition, and we derive a Work Sum Rule that accounts for quantum work at a distance.","All state functions of the system are shown to be path independent once this nonlocal quantum work is properly accounted for.","The thermodynamics of two classes of quasi-statically driven open quantum systems is analyzed: systems with a finite environment in the grand canonical ensemble, and systems with an unbounded environment.","Our results are illustrated with applications to a time-dependent two-level system and the driven resonant-level model."],"url":"http://arxiv.org/abs/2402.18855v1","category":"quant-ph"}
{"created":"2024-02-29 18:59:30","title":"Impact of weak lensing on bright standard siren analyses","abstract":"Gravitational waves from binary mergers at cosmological distances will experience weak lensing by large scale structure. This causes a (de-)magnification, $\\mu$, of the wave amplitude, and a degenerate modification to the inferred luminosity distance $d_L$. To address this the uncertainty on $d_L$ is increased according to the dispersion of the magnification distribution at the source redshift, $\\sigma_\\mu$. But this term is dependent on cosmological parameters that are being constrained by gravitational wave \"standard sirens\", such as the Hubble parameter $H_0$, and the matter density fraction $\\Omega_m$. $\\sigma_\\mu$ is also sensitive to the resolution of the simulation used for its calculation. Tension in the measured value of $H_0$ from independent datasets, and the present use of outdated cosmological simulations, suggest $\\sigma_\\mu$ could be underestimated. We consider two classes of standard siren, supermassive black hole binary and binary neutron star mergers. Underestimating $H_0$ and $\\Omega_m$ when calculating $\\sigma_\\mu$ increases the probability of finding a residual lensing bias on these parameters greater than $1\\sigma$ by 1.5-3 times. Underestimating $\\sigma_\\mu$ by using low resolution/small sky-area simulations can also significantly increase the probability of biased results. For neutron star mergers, the spread of possible biases is 0.25 km/s/Mpc, comparable to the forecasted uncertainty. Left uncorrected this effect limits the use of BNS mergers for precision cosmology. For supermassive black hole binaries, the spread of possible biases on $H_0$ is significant, 5 km/s/Mpc, but $O(200)$ observations are needed to reduce the variance below the bias. To achieve accurate sub-percent level precision on cosmological parameters using standard sirens, first much improved knowledge on the form of the magnification distribution and its dependence on cosmology is needed.","sentences":["Gravitational waves from binary mergers at cosmological distances will experience weak lensing by large scale structure.","This causes a (de-)magnification, $\\mu$, of the wave amplitude, and a degenerate modification to the inferred luminosity distance $d_L$. To address this the uncertainty on $d_L$ is increased according to the dispersion of the magnification distribution at the source redshift, $\\sigma_\\mu$. But this term is dependent on cosmological parameters that are being constrained by gravitational wave \"standard sirens\", such as the Hubble parameter $H_0$, and the matter density fraction $\\Omega_m$. $\\sigma_\\mu$ is also sensitive to the resolution of the simulation used for its calculation.","Tension in the measured value of $H_0$ from independent datasets, and the present use of outdated cosmological simulations, suggest $\\sigma_\\mu$ could be underestimated.","We consider two classes of standard siren, supermassive black hole binary and binary neutron star mergers.","Underestimating $H_0$ and $\\Omega_m$ when calculating $\\sigma_\\mu$ increases the probability of finding a residual lensing bias on these parameters greater than $1\\sigma$ by 1.5-3 times.","Underestimating $\\sigma_\\mu$ by using low resolution/small sky-area simulations can also significantly increase the probability of biased results.","For neutron star mergers, the spread of possible biases is 0.25 km/s/Mpc, comparable to the forecasted uncertainty.","Left uncorrected this effect limits the use of BNS mergers for precision cosmology.","For supermassive black hole binaries, the spread of possible biases on $H_0$ is significant, 5 km/s/Mpc, but $O(200)$ observations are needed to reduce the variance below the bias.","To achieve accurate sub-percent level precision on cosmological parameters using standard sirens, first much improved knowledge on the form of the magnification distribution and its dependence on cosmology is needed."],"url":"http://arxiv.org/abs/2402.19476v1","category":"astro-ph.CO"}
{"created":"2024-02-29 18:52:56","title":"Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks","abstract":"Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one task. Hence, there is a plethora of recent advances with different intentions - that often entirely deviate from practical behavior. This paper conducts a comprehensive evaluation of numerous uncertainty estimators across diverse tasks on ImageNet. We find that, despite promising theoretical endeavors, disentanglement is not yet achieved in practice. Additionally, we reveal which uncertainty estimators excel at which specific tasks, providing insights for practitioners and guiding future research toward task-centric and disentangled uncertainty estimation methods. Our code is available at https://github.com/bmucsanyi/bud.","sentences":["Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification.","The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one task.","Hence, there is a plethora of recent advances with different intentions - that often entirely deviate from practical behavior.","This paper conducts a comprehensive evaluation of numerous uncertainty estimators across diverse tasks on ImageNet.","We find that, despite promising theoretical endeavors, disentanglement is not yet achieved in practice.","Additionally, we reveal which uncertainty estimators excel at which specific tasks, providing insights for practitioners and guiding future research toward task-centric and disentangled uncertainty estimation methods.","Our code is available at https://github.com/bmucsanyi/bud."],"url":"http://arxiv.org/abs/2402.19460v1","category":"cs.LG"}
{"created":"2024-02-29 18:51:37","title":"Higher-group global symmetry and the bosonic M5 brane","abstract":"Higher-group symmetries are combinations of higher-form symmetries which appear in various field theories. In this paper, we explain how higher-group symmetries arise in 10d and 11d supergravities when the latter are coupled to brane sources. Motivated by this observation, we study field theories at zero and finite temperature invariant under a class of continuous Abelian higher-group symmetries. We restrict the analysis to the low-energy regime where the dynamical field content exclusively consists of Goldstone fields arising from the spontaneous breaking of higher-group and spacetime symmetries. Invariant quantities are constructed and the phases of matter are classified according to the pattern of spontaneous symmetry breaking. With respect to supergravity, we highlight how such Goldstone effective theories provide a symmetry-based interpretation for the theories living on D/M-branes. As an explicit example we construct a 6-group invariant action for the bosonic M5 brane, consistent with the self-duality of the 3-form field strength on the brane. While the self-duality condition in the bosonic case needs to be imposed externally as a constraint at zero temperature, we find an equilibrium effective action for the bosonic M5 brane at finite temperature that inherently implements self-duality.","sentences":["Higher-group symmetries are combinations of higher-form symmetries which appear in various field theories.","In this paper, we explain how higher-group symmetries arise in 10d and 11d supergravities when the latter are coupled to brane sources.","Motivated by this observation, we study field theories at zero and finite temperature invariant under a class of continuous Abelian higher-group symmetries.","We restrict the analysis to the low-energy regime where the dynamical field content exclusively consists of Goldstone fields arising from the spontaneous breaking of higher-group and spacetime symmetries.","Invariant quantities are constructed and the phases of matter are classified according to the pattern of spontaneous symmetry breaking.","With respect to supergravity, we highlight how such Goldstone effective theories provide a symmetry-based interpretation for the theories living on D/M-branes.","As an explicit example we construct a 6-group invariant action for the bosonic M5 brane, consistent with the self-duality of the 3-form field strength on the brane.","While the self-duality condition in the bosonic case needs to be imposed externally as a constraint at zero temperature, we find an equilibrium effective action for the bosonic M5 brane at finite temperature that inherently implements self-duality."],"url":"http://arxiv.org/abs/2402.19458v1","category":"hep-th"}
{"created":"2024-02-29 18:06:13","title":"In-beam performance of a Resistive Plate Chamber operated with eco-friendly gas mixtures","abstract":"ALICE (A Large Ion Collider Experiment) studies the Quark-Gluon Plasma (QGP): a deconfined state of matter obtained in ultra-relativistic heavy-ion collisions. One of the probes for QGP study are quarkonia and open heavy flavour, of which ALICE exploits the muonic decay. A set of Resistive Plate Chambers (RPCs), placed in the forward rapidity region of the ALICE detector, is used for muon identification purposes. The correct operation of these detectors is ensured by the choice of the proper gas mixture. Currently they are operated with a mixture of C$_{2}$H$_{2}$F$_{4}$, i-C$_{4}$H$_{10}$ and SF$_{6}$ but, starting from 2017, new EU regulations have enforced a progressive phase-out of C$_{2}$H$_{2}$F$_{4}$ because of its large Global Warming Potential (GWP), making it difficult and costly to purchase. CERN asked LHC experiments to reduce greenhouse gases emissions, to which RPC operation contributes significantly. A possible candidate for C$_{2}$H$_{2}$F$_{4}$ replacement is the C$_{3}$H$_{2}$F$_{4}$ (diluted with other gases, such as CO$_{2}$), which has been extensively tested using cosmic rays. Promising gas mixtures have been devised; the next crucial steps are the detailed in-beam characterization of such mixtures as well as the study of their performance under increasing irradiation levels. This contribution will describe the methodology and results of beam tests carried out at the CERN GIF++ (equipped with a high activity $^{137}$Cs source and muon beam) with an ALICE-like RPC prototype, operated with several mixtures with varying proportions of CO$_{2}$, C$_{3}$H$_{2}$F$_{4}$, i-C$_{4}$H$_{10}$ and SF$_{6}$ . Absorbed currents, efficiencies, prompt charges, cluster sizes, time resolutions and rate capabilities will be presented, both from digitized (for detailed shape and charge analysis) and discriminated (using the same front-end electronics as employed in ALICE) signals.","sentences":["ALICE (A Large Ion Collider Experiment) studies the Quark-Gluon Plasma (QGP): a deconfined state of matter obtained in ultra-relativistic heavy-ion collisions.","One of the probes for QGP study are quarkonia and open heavy flavour, of which ALICE exploits the muonic decay.","A set of Resistive Plate Chambers (RPCs), placed in the forward rapidity region of the ALICE detector, is used for muon identification purposes.","The correct operation of these detectors is ensured by the choice of the proper gas mixture.","Currently they are operated with a mixture of C$_{2}$H$_{2}$F$_{4}$, i-C$_{4}$H$_{10}$ and SF$_{6}$ but, starting from 2017, new EU regulations have enforced a progressive phase-out of C$_{2}$H$_{2}$F$_{4}$ because of its large Global Warming Potential (GWP), making it difficult and costly to purchase.","CERN asked LHC experiments to reduce greenhouse gases emissions, to which RPC operation contributes significantly.","A possible candidate for C$_{2}$H$_{2}$F$_{4}$ replacement is the C$_{3}$H$_{2}$F$_{4}$ (diluted with other gases, such as CO$_{2}$), which has been extensively tested using cosmic rays.","Promising gas mixtures have been devised; the next crucial steps are the detailed in-beam characterization of such mixtures as well as the study of their performance under increasing irradiation levels.","This contribution will describe the methodology and results of beam tests carried out at the CERN GIF++ (equipped with a high activity $^{137}$Cs source and muon beam) with an ALICE-like RPC prototype, operated with several mixtures with varying proportions of CO$_{2}$, C$_{3}$H$_{2}$F$_{4}$, i-C$_{4}$H$_{10}$ and SF$_{6}$ .","Absorbed currents, efficiencies, prompt charges, cluster sizes, time resolutions and rate capabilities will be presented, both from digitized (for detailed shape and charge analysis) and discriminated (using the same front-end electronics as employed in ALICE) signals."],"url":"http://arxiv.org/abs/2402.19408v1","category":"hep-ex"}
{"created":"2024-02-29 18:03:23","title":"Navigating Hallucinations for Reasoning of Unintentional Activities","abstract":"In this work we present a novel task of understanding unintentional human activities in videos. We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional. We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination. We further propose a novel prompting technique,termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning. To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability. We perform our experiments on two different datasets, OOPs and UCF-Crimes, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations.","sentences":["In this work we present a novel task of understanding unintentional human activities in videos.","We formalize this problem as a reasoning task under zero-shot scenario, where given a video of an unintentional activity we want to know why it transitioned from intentional to unintentional.","We first evaluate the effectiveness of current state-of-the-art Large Multimodal Models on this reasoning task and observe that they suffer from hallucination.","We further propose a novel prompting technique,termed as Dream of Thoughts (DoT), which allows the model to navigate through hallucinated thoughts to achieve better reasoning.","To evaluate the performance on this task, we also introduce three different specialized metrics designed to quantify the models reasoning capability.","We perform our experiments on two different datasets, OOPs and UCF-Crimes, and our findings show that DOT prompting technique is able to outperform standard prompting, while minimizing hallucinations."],"url":"http://arxiv.org/abs/2402.19405v1","category":"cs.CV"}
{"created":"2024-02-29 17:50:30","title":"Single-electron states of phosphorus-atom arrays in silicon","abstract":"We characterize the single-electron energies and the wavefunction structure of arrays with two, three, and four phosphorus atoms in silicon by implementing atomistic tight-binding calculations and analyzing wavefunction overlaps to identify the single-dopant states that hybridize to make the array states. The energy spectrum and wavefunction overlap variation as a function of dopant separation for these arrays shows that hybridization mostly occurs between single-dopant states of the same type, with some cross-hybridization between $A_1$ and $E$ states occurring at short separations. We also observe energy crossings between hybrid states of different types as a function of impurity separation. We then extract tunneling rates for electrons in different dopants by mapping the state energies into hopping Hamiltonians in the site representation. Significantly, we find that diagonal and nearest neighbor tunneling rates are similar in magnitude in a square array. Our analysis also accounts for the shift of the on-site energy at each phosphorus atom resulting from the nuclear potential of the other dopants. This approach constitutes a solid protocol to map the electron energies and wavefunction structure into Fermi-Hubbard Hamiltonians needed to implement and validate analog quantum simulations in these devices.","sentences":["We characterize the single-electron energies and the wavefunction structure of arrays with two, three, and four phosphorus atoms in silicon by implementing atomistic tight-binding calculations and analyzing wavefunction overlaps to identify the single-dopant states that hybridize to make the array states.","The energy spectrum and wavefunction overlap variation as a function of dopant separation for these arrays shows that hybridization mostly occurs between single-dopant states of the same type, with some cross-hybridization between $A_1$ and $E$ states occurring at short separations.","We also observe energy crossings between hybrid states of different types as a function of impurity separation.","We then extract tunneling rates for electrons in different dopants by mapping the state energies into hopping Hamiltonians in the site representation.","Significantly, we find that diagonal and nearest neighbor tunneling rates are similar in magnitude in a square array.","Our analysis also accounts for the shift of the on-site energy at each phosphorus atom resulting from the nuclear potential of the other dopants.","This approach constitutes a solid protocol to map the electron energies and wavefunction structure into Fermi-Hubbard Hamiltonians needed to implement and validate analog quantum simulations in these devices."],"url":"http://arxiv.org/abs/2402.19392v1","category":"quant-ph"}
{"created":"2024-02-29 17:13:59","title":"A new analytical model of the cosmic-ray energy flux for Galactic diffuse radio emission","abstract":"Low-frequency radio observations of diffuse synchrotron radiation offer a unique vantage point for investigating the intricate relationship between gas and magnetic fields in the formation of structures within the Galaxy, spanning from the diffuse interstellar medium (ISM) to star-forming regions. Achieving this pivotal objective hinges on a comprehensive understanding of cosmic-ray properties, which dictate the effective energy distribution of relativistic electrons, primarily responsible for the observable synchrotron radiation. Notably, cosmic-ray electrons (CRe) with energies between 100 MeV and 10 GeV play a crucial role in determining the majority of the sky brightness below the GHz range. However, their energy flux ($j_e$) remains elusive due to solar modulation. We propose deriving observational constraints on this energy gap of interstellar CRe through the brightness temperature spectral index of low-frequency radio emission, here denoted as $\\beta_{\\rm obs}$. We introduce a new parametric analytical model that fits available data of $j_e$ in accordance with the $\\beta_{\\rm obs}$ values measured in the literature between 50 MHz to 1 GHz for diffuse emission in the Milky Way. Our model allows to account for multiple observations considering magnetic-field strengths consistent with existing measurements below 10 $\\mu$G. We present a first all-sky map of the average component of the magnetic field perpendicular to the line of sight and validate our methodology against state-of-the art numerical simulations of the diffuse ISM. This research makes headway in modeling Galactic diffuse emission with a practical parametric form. It provides essential insights in preparation for the imminent arrival of the Square Kilometre Array.","sentences":["Low-frequency radio observations of diffuse synchrotron radiation offer a unique vantage point for investigating the intricate relationship between gas and magnetic fields in the formation of structures within the Galaxy, spanning from the diffuse interstellar medium (ISM) to star-forming regions.","Achieving this pivotal objective hinges on a comprehensive understanding of cosmic-ray properties, which dictate the effective energy distribution of relativistic electrons, primarily responsible for the observable synchrotron radiation.","Notably, cosmic-ray electrons (CRe) with energies between 100 MeV and 10 GeV play a crucial role in determining the majority of the sky brightness below the GHz range.","However, their energy flux ($j_e$) remains elusive due to solar modulation.","We propose deriving observational constraints on this energy gap of interstellar CRe through the brightness temperature spectral index of low-frequency radio emission, here denoted as $\\beta_{\\rm obs}$.","We introduce a new parametric analytical model that fits available data of $j_e$ in accordance with the $\\beta_{\\rm obs}$ values measured in the literature between 50 MHz to 1 GHz for diffuse emission in the Milky Way.","Our model allows to account for multiple observations considering magnetic-field strengths consistent with existing measurements below 10 $\\mu$G. We present a first all-sky map of the average component of the magnetic field perpendicular to the line of sight and validate our methodology against state-of-the art numerical simulations of the diffuse ISM.","This research makes headway in modeling Galactic diffuse emission with a practical parametric form.","It provides essential insights in preparation for the imminent arrival of the Square Kilometre Array."],"url":"http://arxiv.org/abs/2402.19367v1","category":"astro-ph.GA"}
{"created":"2024-02-29 17:07:01","title":"Trans-series from condensates","abstract":"The Shifman--Vainshtein--Zakharov (SVZ) sum rules provide a method to obtain trans-series expansions in many quantum field theories, in which exponentially small corrections are calculated by combining the operator product expansion with the assumption of vacuum condensates. In some solvable models, exact expressions for trans-series can be obtained from non-perturbative results, and this makes it possible to test the SVZ method by comparing its predictions to these exact trans-series. In this paper we perform such a precision test in the example of the fermion self-energy in the Gross--Neveu model. Its exact trans-series expansion can be extracted from the large $N$ solution, at the first non-trivial order in $1/N$. It is given by an infinite series of exponentially small corrections involving factorially divergent power series in the 't Hooft parameter. We show that the first two corrections are associated to two-quark and four-quark condensates, and we reproduce the corresponding power series exactly, and at all loops, by using the SVZ method. In addition, the numerical values of the condensates can be extracted from the exact result, up to order $1/N$.","sentences":["The Shifman--Vainshtein--Zakharov (SVZ) sum rules provide a method to obtain trans-series expansions in many quantum field theories, in which exponentially small corrections are calculated by combining the operator product expansion with the assumption of vacuum condensates.","In some solvable models, exact expressions for trans-series can be obtained from non-perturbative results, and this makes it possible to test the SVZ method by comparing its predictions to these exact trans-series.","In this paper we perform such a precision test in the example of the fermion self-energy in the Gross--Neveu model.","Its exact trans-series expansion can be extracted from the large $N$ solution, at the first non-trivial order in $1/N$. It is given by an infinite series of exponentially small corrections involving factorially divergent power series in the 't Hooft parameter.","We show that the first two corrections are associated to two-quark and four-quark condensates, and we reproduce the corresponding power series exactly, and at all loops, by using the SVZ method.","In addition, the numerical values of the condensates can be extracted from the exact result, up to order $1/N$."],"url":"http://arxiv.org/abs/2402.19356v1","category":"hep-th"}
{"created":"2024-02-29 16:36:51","title":"Compact Speech Translation Models via Discrete Speech Units Pretraining","abstract":"Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST). However, they also impose a large memory footprint, hindering on-device deployment. In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU). We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data. The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model. Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization. In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings. Evaluation on CoVoST-2 X-En shows that our method is >$0.5$ BLEU better than a ST model that directly finetune the SSL model, given only half the model size, and on a par with ASR pretraining.","sentences":["Using Self-Supervised Learning (SSL) as model initialization is now common to obtain strong results in Speech Translation (ST).","However, they also impose a large memory footprint, hindering on-device deployment.","In this paper, we leverage the SSL models by pretraining smaller models on their Discrete Speech Units (DSU).","We pretrain encoder-decoder models on 1) Filterbank-to-DSU and 2) DSU-to-Translation data, and take the encoder from 1) and the decoder from 2) to initialise a new model, finetuning this on limited speech-translation data.","The final model becomes compact by using the DSU pretraining to distil the knowledge of the SSL model.","Our method has several benefits over using DSU as model inputs, such as shorter inference pipeline and robustness over (DSU) tokenization.","In contrast to ASR pretraining, it does not require transcripts, making it applicable to low-resource settings.","Evaluation on CoVoST-2 X-En shows that our method is >$0.5$ BLEU better than a ST model that directly finetune the SSL model, given only half the model size, and on a par with ASR pretraining."],"url":"http://arxiv.org/abs/2402.19333v1","category":"cs.CL"}
{"created":"2024-02-29 15:51:01","title":"The Equation of Motion for Taut-Line Buzzers","abstract":"Equations of motion are developed for the oscillatory rotation of a disk suspended between twisted strings kept under tension by a hanging mass, to which additional forces may be applied. In the absence of forcing, damped harmonic oscillations are observed to decay with an exponential time envelope for two different string types. This is consistent with damping caused by string viscosity, rather than air turbulence, and may be quantified in terms of a quality factor. To test the proposed equation of motion and model for viscous damping within the string, we measure both the natural oscillation frequency and the quality factor for widely varied values of string length, string radius, disk moment of inertia, and hanging mass. The data are found to scale in good accord with predictions. A variation where rotational kinetic energy is converted back and forth to spring potential energy is also discussed.","sentences":["Equations of motion are developed for the oscillatory rotation of a disk suspended between twisted strings kept under tension by a hanging mass, to which additional forces may be applied.","In the absence of forcing, damped harmonic oscillations are observed to decay with an exponential time envelope for two different string types.","This is consistent with damping caused by string viscosity, rather than air turbulence, and may be quantified in terms of a quality factor.","To test the proposed equation of motion and model for viscous damping within the string, we measure both the natural oscillation frequency and the quality factor for widely varied values of string length, string radius, disk moment of inertia, and hanging mass.","The data are found to scale in good accord with predictions.","A variation where rotational kinetic energy is converted back and forth to spring potential energy is also discussed."],"url":"http://arxiv.org/abs/2402.19285v1","category":"cond-mat.soft"}
{"created":"2024-02-29 15:48:11","title":"Mobile Health Text Misinformation Identification Using Mobile Data Mining","abstract":"More than six million people died of the COVID-19 by April 2022. The heavy casualties have put people on great and urgent alert and people try to find all kinds of information to keep them from being inflected by the coronavirus. This research tries to find out whether the mobile health text information sent to peoples devices is correct as smartphones becoming the major information source for people. The proposed method uses various mobile information retrieval and data mining technologies including lexical analysis, stopword elimination, stemming, and decision trees to classify the mobile health text information to one of the following classes: (i) true, (ii) fake, (iii) misinformative, (iv) disinformative, and (v) neutral. Experiment results show the accuracy of the proposed method is above the threshold value 50 percentage, but is not optimal. It is because the problem, mobile text misinformation identification, is intrinsically difficult.","sentences":["More than six million people died of the COVID-19 by April 2022.","The heavy casualties have put people on great and urgent alert and people try to find all kinds of information to keep them from being inflected by the coronavirus.","This research tries to find out whether the mobile health text information sent to peoples devices is correct as smartphones becoming the major information source for people.","The proposed method uses various mobile information retrieval and data mining technologies including lexical analysis, stopword elimination, stemming, and decision trees to classify the mobile health text information to one of the following classes: (i) true, (ii) fake, (iii) misinformative, (iv) disinformative, and (v) neutral.","Experiment results show the accuracy of the proposed method is above the threshold value 50 percentage, but is not optimal.","It is because the problem, mobile text misinformation identification, is intrinsically difficult."],"url":"http://arxiv.org/abs/2402.19280v1","category":"cs.CY"}
{"created":"2024-02-29 15:46:48","title":"SIFT-Aided Rectified 2D-DIC for Displacement and Strain Measurements in Asphalt Concrete Testing","abstract":"Two-dimensional digital image correlation (2D-DIC) is a widely used optical technique to measure displacement and strain during asphalt concrete (AC) testing. An accurate 2-D DIC measurement can only be achieved when the camera's principal axis is perpendicular to the planar specimen surface. However, this requirement may not be met during testing due to device constraints. This paper proposes a simple and reliable method to correct errors induced by non-perpendicularity. The method is based on image feature matching and rectification. No additional equipment is needed. A theoretical error analysis was conducted to quantify the effect of a non-perpendicular camera alignment on measurement accuracy. The proposed method was validated numerically using synthetic images and experimentally in an AC fracture test. It achieved relatively high accuracy, even under considerable camera rotation angle and large deformation. As a pre-processing technique, the proposed method showed promising performance in assisting the recently developed CrackPropNet for automated crack propagation measurement under a non-perpendicular camera alignment.","sentences":["Two-dimensional digital image correlation (2D-DIC) is a widely used optical technique to measure displacement and strain during asphalt concrete (AC) testing.","An accurate 2-D DIC measurement can only be achieved when the camera's principal axis is perpendicular to the planar specimen surface.","However, this requirement may not be met during testing due to device constraints.","This paper proposes a simple and reliable method to correct errors induced by non-perpendicularity.","The method is based on image feature matching and rectification.","No additional equipment is needed.","A theoretical error analysis was conducted to quantify the effect of a non-perpendicular camera alignment on measurement accuracy.","The proposed method was validated numerically using synthetic images and experimentally in an AC fracture test.","It achieved relatively high accuracy, even under considerable camera rotation angle and large deformation.","As a pre-processing technique, the proposed method showed promising performance in assisting the recently developed CrackPropNet for automated crack propagation measurement under a non-perpendicular camera alignment."],"url":"http://arxiv.org/abs/2402.19279v1","category":"cs.CV"}
{"created":"2024-02-29 15:35:40","title":"T3DNet: Compressing Point Cloud Models for Lightweight 3D Recognition","abstract":"3D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3D sensing on mobile devices. However, existing 3D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and non-real-time latency. There has been a lack of research on how to compress 3D point cloud models into lightweight models. In this paper, we propose a method called T3DNet (Tiny 3D Network with augmEntation and disTillation) to address this issue. We find that the tiny model after network augmentation is much easier for a teacher to distill. Instead of gradually reducing the parameters through techniques such as pruning or quantization, we pre-define a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model. We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN. Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods. Amazingly, our T3DNet is 58 times smaller and 54 times faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset.","sentences":["3D point cloud has been widely used in many mobile application scenarios, including autonomous driving and 3D sensing on mobile devices.","However, existing 3D point cloud models tend to be large and cumbersome, making them hard to deploy on edged devices due to their high memory requirements and non-real-time latency.","There has been a lack of research on how to compress 3D point cloud models into lightweight models.","In this paper, we propose a method called T3DNet (Tiny 3D Network with augmEntation and disTillation) to address this issue.","We find that the tiny model after network augmentation is much easier for a teacher to distill.","Instead of gradually reducing the parameters through techniques such as pruning or quantization, we pre-define a tiny model and improve its performance through auxiliary supervision from augmented networks and the original model.","We evaluate our method on several public datasets, including ModelNet40, ShapeNet, and ScanObjectNN.","Our method can achieve high compression rates without significant accuracy sacrifice, achieving state-of-the-art performances on three datasets against existing methods.","Amazingly, our T3DNet is 58 times smaller and 54 times faster than the original model yet with only 1.4% accuracy descent on the ModelNet40 dataset."],"url":"http://arxiv.org/abs/2402.19264v1","category":"cs.CV"}
{"created":"2024-02-29 15:32:02","title":"Masks, Signs, And Learning Rate Rewinding","abstract":"Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks. While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures. To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization. The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones. In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations.","sentences":["Learning Rate Rewinding (LRR) has been established as a strong variant of Iterative Magnitude Pruning (IMP) to find lottery tickets in deep overparameterized neural networks.","While both iterative pruning schemes couple structure and parameter learning, understanding how LRR excels in both aspects can bring us closer to the design of more flexible deep learning algorithms that can optimize diverse sets of sparse architectures.","To this end, we conduct experiments that disentangle the effect of mask learning and parameter optimization and how both benefit from overparameterization.","The ability of LRR to flip parameter signs early and stay robust to sign perturbations seems to make it not only more effective in mask identification but also in optimizing diverse sets of masks, including random ones.","In support of this hypothesis, we prove in a simplified single hidden neuron setting that LRR succeeds in more cases than IMP, as it can escape initially problematic sign configurations."],"url":"http://arxiv.org/abs/2402.19262v1","category":"cs.LG"}
{"created":"2024-02-29 15:31:46","title":"YSR Bond Qubit in a Double Quantum Dot with cQED Operation","abstract":"Connecting two half-filled quantum dots to two superconducting leads induces a competition of bonds, with the dots forming either an interdot exchange bond or two individual Yu-Shiba-Rusinov (YSR) screening bonds with the leads. Defining a qubit using these singlet parity bonding states provides dot charge noise protection, attributed to the chargeless nature of the screening quasiparticles, and magnetic noise protection, as the bonds guard against magnetic polarization. In this paper, we propose embedding a Double Quantum Dot (DQD) Josephson junction in parallel with a transmon to enable circuit Quantum Electrodynamics (cQED) measurements and operation of a YSR bond qubit. We demonstrate that, under realistic parameters, two-tone spectroscopy of the DQD can be performed, revealing a significant parameter regime suitable for qubit operation. Additionally, coherent manipulations of the bond states can be achieved through dot gates, and single-shot readout is enabled by measurements of a capacitively coupled resonator. Finally, we analyze noise sources and estimate gate noise on couplings as the primary source of qubit decoherence. Since this qubit is protected against nuclear Overhauser fields and does not rely on spin-orbit interactions for operation, a broader range of material platforms becomes available compared to current Andreev spin qubits.","sentences":["Connecting two half-filled quantum dots to two superconducting leads induces a competition of bonds, with the dots forming either an interdot exchange bond or two individual Yu-Shiba-Rusinov (YSR) screening bonds with the leads.","Defining a qubit using these singlet parity bonding states provides dot charge noise protection, attributed to the chargeless nature of the screening quasiparticles, and magnetic noise protection, as the bonds guard against magnetic polarization.","In this paper, we propose embedding a Double Quantum Dot (DQD) Josephson junction in parallel with a transmon to enable circuit Quantum Electrodynamics (cQED) measurements and operation of a YSR bond qubit.","We demonstrate that, under realistic parameters, two-tone spectroscopy of the DQD can be performed, revealing a significant parameter regime suitable for qubit operation.","Additionally, coherent manipulations of the bond states can be achieved through dot gates, and single-shot readout is enabled by measurements of a capacitively coupled resonator.","Finally, we analyze noise sources and estimate gate noise on couplings as the primary source of qubit decoherence.","Since this qubit is protected against nuclear Overhauser fields and does not rely on spin-orbit interactions for operation, a broader range of material platforms becomes available compared to current Andreev spin qubits."],"url":"http://arxiv.org/abs/2402.19261v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 15:06:46","title":"Shared lightweight autonomous vehicles for urban food deliveries: A simulation study","abstract":"In recent years, the rapid growth of on-demand deliveries, especially in food deliveries, has spurred the exploration of innovative mobility solutions. In this context, lightweight autonomous vehicles have emerged as a potential alternative. However, their fleet-level behavior remains largely unexplored. To address this gap, we have developed an agent-based model and an environmental impact study assessing the fleet performance of lightweight autonomous food delivery vehicles. This model explores critical factors such as fleet sizing, service level, operational strategies, and environmental impacts. We have applied this model to a case study in Cambridge, MA, USA, where results indicate that there could be environmental benefits in replacing traditional car-based deliveries with shared lightweight autonomous vehicle fleets. Lastly, we introduce an interactive platform that offers a user-friendly means of comprehending the model's performance and potential trade-offs, which can help inform decision-makers in the evolving landscape of food delivery innovation.","sentences":["In recent years, the rapid growth of on-demand deliveries, especially in food deliveries, has spurred the exploration of innovative mobility solutions.","In this context, lightweight autonomous vehicles have emerged as a potential alternative.","However, their fleet-level behavior remains largely unexplored.","To address this gap, we have developed an agent-based model and an environmental impact study assessing the fleet performance of lightweight autonomous food delivery vehicles.","This model explores critical factors such as fleet sizing, service level, operational strategies, and environmental impacts.","We have applied this model to a case study in Cambridge, MA, USA, where results indicate that there could be environmental benefits in replacing traditional car-based deliveries with shared lightweight autonomous vehicle fleets.","Lastly, we introduce an interactive platform that offers a user-friendly means of comprehending the model's performance and potential trade-offs, which can help inform decision-makers in the evolving landscape of food delivery innovation."],"url":"http://arxiv.org/abs/2402.19233v1","category":"cs.CY"}
{"created":"2024-02-29 15:05:59","title":"Trained Random Forests Completely Reveal your Dataset","abstract":"We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest. Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn. To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective. We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming -- an approach rooted in constraint propagation and solution-domain reduction. Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction. This holds true even with a small number of trees. Even with bootstrap aggregation, the majority of the data can also be reconstructed. These findings underscore a critical vulnerability inherent in widely adopted ensemble methods, warranting attention and mitigation. Although the potential for such reconstruction attacks has been discussed in privacy research, our study provides clear empirical evidence of their practicability.","sentences":["We introduce an optimization-based reconstruction attack capable of completely or near-completely reconstructing a dataset utilized for training a random forest.","Notably, our approach relies solely on information readily available in commonly used libraries such as scikit-learn.","To achieve this, we formulate the reconstruction problem as a combinatorial problem under a maximum likelihood objective.","We demonstrate that this problem is NP-hard, though solvable at scale using constraint programming -- an approach rooted in constraint propagation and solution-domain reduction.","Through an extensive computational investigation, we demonstrate that random forests trained without bootstrap aggregation but with feature randomization are susceptible to a complete reconstruction.","This holds true even with a small number of trees.","Even with bootstrap aggregation, the majority of the data can also be reconstructed.","These findings underscore a critical vulnerability inherent in widely adopted ensemble methods, warranting attention and mitigation.","Although the potential for such reconstruction attacks has been discussed in privacy research, our study provides clear empirical evidence of their practicability."],"url":"http://arxiv.org/abs/2402.19232v1","category":"cs.LG"}
{"created":"2024-02-29 14:55:04","title":"Edit and Alphabet-Ordering Sensitivity of Lex-parse","abstract":"We investigate the compression sensitivity [Akagi et al., 2023] of lex-parse [Navarro et al., 2021] for two operations: (1) single character edit and (2) modification of the alphabet ordering, and give tight upper and lower bounds for both operations. For both lower bounds, we use the family of Fibonacci words. For the bounds on edit operations, our analysis makes heavy use of properties of the Lyndon factorization of Fibonacci words to characterize the structure of lex-parse.","sentences":["We investigate the compression sensitivity [Akagi et al., 2023] of lex-parse [Navarro et al., 2021] for two operations: (1) single character edit and (2) modification of the alphabet ordering, and give tight upper and lower bounds for both operations.","For both lower bounds, we use the family of Fibonacci words.","For the bounds on edit operations, our analysis makes heavy use of properties of the Lyndon factorization of Fibonacci words to characterize the structure of lex-parse."],"url":"http://arxiv.org/abs/2402.19223v1","category":"cs.DS"}
{"created":"2024-02-29 14:52:46","title":"Thermal stabilities Landscape of A$_2$BB$^{\\prime}$O$_6$ compounds","abstract":"Perovskite oxides have been extensively studied for their wide range of compositions and structures, as well as their valuable properties for various applications. Expanding from single perovskite ABO$_3$ to double perovskite $A_2BB^{\\prime}$O$_6$ significantly enhances the ability to tailor specific physical and chemical properties. However, the vast number of potential compositions of $A_2BB^{\\prime}$O$_6$ makes it impractical to explore them all experimentally. In this study, we conducted high-throughput calculations to systematically investigate the structures and stabilities of 4,900 $A_2BB^{\\prime}$O$_6$ compositions (with $A$ = Ca, Sr, Ba, and La; $B$ and $B^{\\prime}$ representing metal elements) through over 42,000 density functional theory (DFT) calculations. Our analysis lead to the discovery of more than 1,500 new synthesizable $A_2BB^{\\prime}$O$_6$ compounds, with over 1,100 of them exhibiting double perovskite structures, predominantly in the $P2_1/c$ space group. By leveraging the high-throughput dataset, we developed machine learning models that achieved mean absolute errors of 0.0444 and 0.0330 eV/atom for formation energy and decomposition energy, respectively. Using these models, we identified 803 stable or metastable compositions beyond the chemical space covered in our initial calculations, with 612 of them having DFT-validated decomposition energies below 0.1 eV/atom, resulting in a success rate of 76.2 \\%. This study delineates the stability landscape of $A_2BB^{\\prime}$O$_6$ compounds and offers new insights for the exploration of these materials.","sentences":["Perovskite oxides have been extensively studied for their wide range of compositions and structures, as well as their valuable properties for various applications.","Expanding from single perovskite ABO$_3$ to double perovskite $A_2BB^{\\prime}$O$_6$ significantly enhances the ability to tailor specific physical and chemical properties.","However, the vast number of potential compositions of $A_2BB^{\\prime}$O$_6$ makes it impractical to explore them all experimentally.","In this study, we conducted high-throughput calculations to systematically investigate the structures and stabilities of 4,900 $A_2BB^{\\prime}$O$_6$ compositions (with $A$ = Ca, Sr, Ba, and La; $B$ and $B^{\\prime}$ representing metal elements) through over 42,000 density functional theory (DFT) calculations.","Our analysis lead to the discovery of more than 1,500 new synthesizable $A_2BB^{\\prime}$O$_6$ compounds, with over 1,100 of them exhibiting double perovskite structures, predominantly in the $P2_1/c$ space group.","By leveraging the high-throughput dataset, we developed machine learning models that achieved mean absolute errors of 0.0444 and 0.0330 eV/atom for formation energy and decomposition energy, respectively.","Using these models, we identified 803 stable or metastable compositions beyond the chemical space covered in our initial calculations, with 612 of them having DFT-validated decomposition energies below 0.1 eV/atom, resulting in a success rate of 76.2 \\%.","This study delineates the stability landscape of $A_2BB^{\\prime}$O$_6$ compounds and offers new insights for the exploration of these materials."],"url":"http://arxiv.org/abs/2402.19220v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 14:42:37","title":"A Bayesian approach with Gaussian priors to the inverse problem of source identification in elliptic PDEs","abstract":"We consider the statistical linear inverse problem of making inference on an unknown source function in an elliptic partial differential equation from noisy observations of its solution. We employ nonparametric Bayesian procedures based on Gaussian priors, leading to convenient conjugate formulae for posterior inference. We review recent results providing theoretical guarantees on the quality of the resulting posterior-based estimation and uncertainty quantification, and we discuss the application of the theory to the important classes of Gaussian series priors defined on the Dirichlet-Laplacian eigenbasis and Mat\\'ern process priors. We provide an implementation of posterior inference for both classes of priors, and investigate its performance in a numerical simulation study.","sentences":["We consider the statistical linear inverse problem of making inference on an unknown source function in an elliptic partial differential equation from noisy observations of its solution.","We employ nonparametric Bayesian procedures based on Gaussian priors, leading to convenient conjugate formulae for posterior inference.","We review recent results providing theoretical guarantees on the quality of the resulting posterior-based estimation and uncertainty quantification, and we discuss the application of the theory to the important classes of Gaussian series priors defined on the Dirichlet-Laplacian eigenbasis and Mat\\'ern process priors.","We provide an implementation of posterior inference for both classes of priors, and investigate its performance in a numerical simulation study."],"url":"http://arxiv.org/abs/2402.19214v1","category":"math.ST"}
{"created":"2024-02-29 14:19:59","title":"Prediction of vaccination coverage level in the heterogeneous mixing population","abstract":"Heterogeneity of population is a key factor in modeling the transmission of disease among the population and has huge impact on the outcome of the transmission. In order to investigate the decision making process in the heterogeneous mixing population regarding whether to be vaccinated or not, we propose the modeling framework which includes the epidemic models and the game theoretical analysis. We consider two sources of heterogeneity in this paper: the different activity levels and the different relative vaccination costs. It is interesting to observe that, if both sources of heterogeneity are considered, there exist a finite number of Nash equilibria (evolutionary stable strategies (ESS)) of the vaccination game. While if only the difference of activity levels is considered, there are infinitely many Nash equilibira. For the latter case, the initial condition of the decision making process becomes highly sensitive. In the application of public health management, the inclusion of population heterogeneity significantly complicates the prediction of the overall vaccine coverage level.","sentences":["Heterogeneity of population is a key factor in modeling the transmission of disease among the population and has huge impact on the outcome of the transmission.","In order to investigate the decision making process in the heterogeneous mixing population regarding whether to be vaccinated or not, we propose the modeling framework which includes the epidemic models and the game theoretical analysis.","We consider two sources of heterogeneity in this paper: the different activity levels and the different relative vaccination costs.","It is interesting to observe that, if both sources of heterogeneity are considered, there exist a finite number of Nash equilibria (evolutionary stable strategies (ESS)) of the vaccination game.","While if only the difference of activity levels is considered, there are infinitely many Nash equilibira.","For the latter case, the initial condition of the decision making process becomes highly sensitive.","In the application of public health management, the inclusion of population heterogeneity significantly complicates the prediction of the overall vaccine coverage level."],"url":"http://arxiv.org/abs/2402.19190v1","category":"q-bio.PE"}
{"created":"2024-02-29 14:03:03","title":"Quantitative homogenization for log-normal coefficients via Malliavin calculus: the one-dimensional case","abstract":"The quantitative analysis of stochastic homogenization problems has been a very active field in the last fifteen years. Whereas the first results were motivated by applied questions (namely, the numerical approximation of homogenized coefficients), the more recent achievements in the field are much more analytically-driven and focus on the subtle interplay between PDE analysis (and in particular elliptic regularity theory) and probability (concentration, stochastic cancellations, scaling limits). The aim of this article is threefold. First we provide a complete and self-contained analysis for the popular example of log-normal coefficients with possibly fat tails in dimension $d=1$, establishing new results on the accuracy of the two-scale expansion and characterizing fluctuations (in the perspective of uncertainty quantification). Second, we work in a context where explicit formulas allow us to by-pass analytical difficulties and therefore mostly focus on the probabilistic side of the theory. Last, the one-dimensional setting gives intuition on the available results in higher dimension (provided results are correctly reformulated) to which we give precise entries to the recent literature.","sentences":["The quantitative analysis of stochastic homogenization problems has been a very active field in the last fifteen years.","Whereas the first results were motivated by applied questions (namely, the numerical approximation of homogenized coefficients), the more recent achievements in the field are much more analytically-driven and focus on the subtle interplay between PDE analysis (and in particular elliptic regularity theory) and probability (concentration, stochastic cancellations, scaling limits).","The aim of this article is threefold.","First we provide a complete and self-contained analysis for the popular example of log-normal coefficients with possibly fat tails in dimension $d=1$, establishing new results on the accuracy of the two-scale expansion and characterizing fluctuations (in the perspective of uncertainty quantification).","Second, we work in a context where explicit formulas allow us to by-pass analytical difficulties and therefore mostly focus on the probabilistic side of the theory.","Last, the one-dimensional setting gives intuition on the available results in higher dimension (provided results are correctly reformulated) to which we give precise entries to the recent literature."],"url":"http://arxiv.org/abs/2402.19182v1","category":"math.AP"}
{"created":"2024-02-29 13:48:24","title":"5.0 \u03bcm emitting Interband Cascade Lasers with Superlattice and Bulk AlGaAsSb Claddings","abstract":"We present a comparison between interband cascade lasers (ICLs) with a 6-stage active region emitting at 5 {\\mu}m with AlSb/InAs superlattice claddings and bulk Al_0.85 Ga_0.15 As_0.07 Sb_0.93 claddings. Utilizing bulk AlGaAsSb claddings with its lower refractive index compared to the mostly used AlSb/InAs superlattice claddings, the mode-confinement in the active region increases by 14.4 % resulting in an improvement of the lasing threshold current density. For broad area laser and under pulsed excitation, the ICL with AlGaAsSb claddings shows a lower threshold current density of J_th=396 A/cm^2 compared to J_th=521 A/cm^2 for the reference ICL with superlattice claddings. Additionally, a higher characteristic temperature was obtained for the ICL with bulk claddings. A measured pulsed operation is observed up to 65 C.","sentences":["We present a comparison between interband cascade lasers (ICLs) with a 6-stage active region emitting at 5 {\\mu}m with AlSb/InAs superlattice claddings and bulk Al_0.85 Ga_0.15 As_0.07 Sb_0.93 claddings.","Utilizing bulk AlGaAsSb claddings with its lower refractive index compared to the mostly used AlSb/InAs superlattice claddings, the mode-confinement in the active region increases by 14.4 % resulting in an improvement of the lasing threshold current density.","For broad area laser and under pulsed excitation, the ICL with AlGaAsSb claddings shows a lower threshold current density of J_th=396 A/cm^2 compared to J_th=521 A/cm^2 for the reference ICL with superlattice claddings.","Additionally, a higher characteristic temperature was obtained for the ICL with bulk claddings.","A measured pulsed operation is observed up to 65 C."],"url":"http://arxiv.org/abs/2402.19165v1","category":"physics.app-ph"}
{"created":"2024-02-29 13:44:19","title":"Effective Message Hiding with Order-Preserving Mechanisms","abstract":"Message hiding, a technique that conceals secret message bits within a cover image, aims to achieve an optimal balance among message capacity, recovery accuracy, and imperceptibility. While convolutional neural networks have notably improved message capacity and imperceptibility, achieving high recovery accuracy remains challenging. This challenge arises because convolutional operations struggle to preserve the sequential order of message bits and effectively address the discrepancy between these two modalities. To address this, we propose StegaFormer, an innovative MLP-based framework designed to preserve bit order and enable global fusion between modalities. Specifically, StegaFormer incorporates three crucial components: Order-Preserving Message Encoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF). OPME and OPMD aim to preserve the order of message bits by segmenting the entire sequence into equal-length segments and incorporating sequential information during encoding and decoding. Meanwhile, GMIF employs a cross-modality fusion mechanism to effectively fuse the features from the two uncorrelated modalities. Experimental results on the COCO and DIV2K datasets demonstrate that StegaFormer surpasses existing state-of-the-art methods in terms of recovery accuracy, message capacity, and imperceptibility. We will make our code publicly available.","sentences":["Message hiding, a technique that conceals secret message bits within a cover image, aims to achieve an optimal balance among message capacity, recovery accuracy, and imperceptibility.","While convolutional neural networks have notably improved message capacity and imperceptibility, achieving high recovery accuracy remains challenging.","This challenge arises because convolutional operations struggle to preserve the sequential order of message bits and effectively address the discrepancy between these two modalities.","To address this, we propose StegaFormer, an innovative MLP-based framework designed to preserve bit order and enable global fusion between modalities.","Specifically, StegaFormer incorporates three crucial components: Order-Preserving Message Encoder (OPME), Decoder (OPMD) and Global Message-Image Fusion (GMIF).","OPME and OPMD aim to preserve the order of message bits by segmenting the entire sequence into equal-length segments and incorporating sequential information during encoding and decoding.","Meanwhile, GMIF employs a cross-modality fusion mechanism to effectively fuse the features from the two uncorrelated modalities.","Experimental results on the COCO and DIV2K datasets demonstrate that StegaFormer surpasses existing state-of-the-art methods in terms of recovery accuracy, message capacity, and imperceptibility.","We will make our code publicly available."],"url":"http://arxiv.org/abs/2402.19160v1","category":"cs.CV"}
{"created":"2024-02-29 13:26:47","title":"Weakly Supervised Monocular 3D Detection with a Single-View Image","abstract":"Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes. Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications. We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data. One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference. In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively. Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods.","sentences":["Monocular 3D detection (M3D) aims for precise 3D object localization from a single-view image which usually involves labor-intensive annotation of 3D detection boxes.","Weakly supervised M3D has recently been studied to obviate the 3D annotation process by leveraging many existing 2D annotations, but it often requires extra training data such as LiDAR point clouds or multi-view images which greatly degrades its applicability and usability in various applications.","We propose SKD-WM3D, a weakly supervised monocular 3D detection framework that exploits depth information to achieve M3D with a single-view image exclusively without any 3D annotations or other training data.","One key design in SKD-WM3D is a self-knowledge distillation framework, which transforms image features into 3D-like representations by fusing depth information and effectively mitigates the inherent depth ambiguity in monocular scenarios with little computational overhead in inference.","In addition, we design an uncertainty-aware distillation loss and a gradient-targeted transfer modulation strategy which facilitate knowledge acquisition and knowledge transfer, respectively.","Extensive experiments show that SKD-WM3D surpasses the state-of-the-art clearly and is even on par with many fully supervised methods."],"url":"http://arxiv.org/abs/2402.19144v1","category":"cs.CV"}
{"created":"2024-02-29 13:19:04","title":"Radiation Hardness Studies of RPC Based on Diamond-Like Carbon Electrodes for MEG II Experiment","abstract":"A novel type of resistive plate chamber, based on diamond-like carbon (DLC) electrodes is under development for background identification in the MEG II experiment. The DLC-RPC is required to have a radiation hardness to mass irradiation since it is planned to be placed in a high-intensity and low-momentum muon beam. In this study, the aging test using a high-intensity X-ray beam was conducted to evaluate the radiation hardness of the DLC-RPC. The accumulated charge due to X-ray irradiation reached about 54 C/cm$^2$, which is approximately half of the one-year irradiation dose expected in the MEG II experiment. As a result, the degradation of the gas gain was observed due to fluorine deposition and insulators formed on the DLC electrodes. In addition, discharges via spacers were also observed repeatedly and interrupted the DLC-RPC operation.","sentences":["A novel type of resistive plate chamber, based on diamond-like carbon (DLC) electrodes is under development for background identification in the MEG II experiment.","The DLC-RPC is required to have a radiation hardness to mass irradiation since it is planned to be placed in a high-intensity and low-momentum muon beam.","In this study, the aging test using a high-intensity X-ray beam was conducted to evaluate the radiation hardness of the DLC-RPC.","The accumulated charge due to X-ray irradiation reached about 54 C/cm$^2$, which is approximately half of the one-year irradiation dose expected in the MEG II experiment.","As a result, the degradation of the gas gain was observed due to fluorine deposition and insulators formed on the DLC electrodes.","In addition, discharges via spacers were also observed repeatedly and interrupted the DLC-RPC operation."],"url":"http://arxiv.org/abs/2402.19136v1","category":"physics.ins-det"}
{"created":"2024-02-29 13:00:30","title":"Sensing atomic superfluid rotation beyond the standard quantum limit","abstract":"Atomic superfluids formed using Bose-Einstein condensates (BECs) in a ring trap are currently being investigated in the context of superfluid hydrodynamics, quantum sensing and matter-wave interferometry. The characterization of the rotational properties of such superfluids is important, but can presently only be performed by using optical absorption imaging, which completely destroys the condensate. Recent studies have proposed coupling the ring BEC to optical cavity modes carrying orbital angular momentum to make minimally destructive measurements of the condensate rotation. The sensitivity of these proposals, however, is bounded below by the standard quantum limit set by the combination of laser shot noise and radiation pressure noise. In this work, we provide a theoretical framework that exploits the fact that the interaction between the scattered modes of the condensate and the light reduces to effective optomechanical equations of motion. We present a detailed theoretical analysis to demonstrate that the use of squeezed light and backaction evasion techniques allows the angular momentum of the condensate to be sensed with noise well below the standard quantum limit. Our proposal is relevant to atomtronics, quantum sensing and quantum information.","sentences":["Atomic superfluids formed using Bose-Einstein condensates (BECs) in a ring trap are currently being investigated in the context of superfluid hydrodynamics, quantum sensing and matter-wave interferometry.","The characterization of the rotational properties of such superfluids is important, but can presently only be performed by using optical absorption imaging, which completely destroys the condensate.","Recent studies have proposed coupling the ring BEC to optical cavity modes carrying orbital angular momentum to make minimally destructive measurements of the condensate rotation.","The sensitivity of these proposals, however, is bounded below by the standard quantum limit set by the combination of laser shot noise and radiation pressure noise.","In this work, we provide a theoretical framework that exploits the fact that the interaction between the scattered modes of the condensate and the light reduces to effective optomechanical equations of motion.","We present a detailed theoretical analysis to demonstrate that the use of squeezed light and backaction evasion techniques allows the angular momentum of the condensate to be sensed with noise well below the standard quantum limit.","Our proposal is relevant to atomtronics, quantum sensing and quantum information."],"url":"http://arxiv.org/abs/2402.19123v1","category":"quant-ph"}
{"created":"2024-02-29 12:45:19","title":"The effect of nonlocal disk processes on the volatile CHNOS budgets of planetesimal-forming material","abstract":"The abundances of volatile CHNOS have a profound effect on the interior structure and evolution of a planet. Therefore, it is key to investigate the behavior of the abundances of these elements in the solid phase in the earliest stages of planet formation. However, the processes in planet-forming disks that shape these abundances are highly coupled and nonlocal. We aim to quantify the effects of the interplay between dynamical, collision, ice processing on the abundances of CHNOS in local disk solids as a function of position throughout the planet-forming region. We used SHAMPOO (StocHAstic Monomer PrOcessOr), which tracks the ice budgets of CHNOS-bearing molecules of a dust monomer as it undergoes nonlocal disk processing in a Class I disk. We used a large set of individual monomer evolutionary trajectories to make inferences about the properties of the local dust populations. We find that spatially, monomers can travel larger distances farther out in the disk, leading to a larger spread in positions of origin for a dust population at, for example, r=50 AU compared to r=2 AU. However, chemically, the inner disk (r<10 AU) is more nonlocal due to the closer spacing of ice lines in this disk region. We find that the ice mass associated with individual chemical species can change significantly. The largest differences with local dust were found near ice lines where the collisional timescale is comparable to the adsorption and desorption timescales. Here, aggregates may become significantly depleted in ice as a consequence of microscopic collisional mixing, a previously unknown effect where monomers are stored away in aggregate interiors through rapid cycles of coagulation and fragmentation. This suggests that ice processing is highly coupled to collisional processing in this disk region, which implies that the interiors of dust aggregates must be considered and not just their surfaces.","sentences":["The abundances of volatile CHNOS have a profound effect on the interior structure and evolution of a planet.","Therefore, it is key to investigate the behavior of the abundances of these elements in the solid phase in the earliest stages of planet formation.","However, the processes in planet-forming disks that shape these abundances are highly coupled and nonlocal.","We aim to quantify the effects of the interplay between dynamical, collision, ice processing on the abundances of CHNOS in local disk solids as a function of position throughout the planet-forming region.","We used SHAMPOO (StocHAstic Monomer PrOcessOr), which tracks the ice budgets of CHNOS-bearing molecules of a dust monomer as it undergoes nonlocal disk processing in a Class I disk.","We used a large set of individual monomer evolutionary trajectories to make inferences about the properties of the local dust populations.","We find that spatially, monomers can travel larger distances farther out in the disk, leading to a larger spread in positions of origin for a dust population at, for example, r=50 AU compared to r=2 AU.","However, chemically, the inner disk (r<10 AU) is more nonlocal due to the closer spacing of ice lines in this disk region.","We find that the ice mass associated with individual chemical species can change significantly.","The largest differences with local dust were found near ice lines where the collisional timescale is comparable to the adsorption and desorption timescales.","Here, aggregates may become significantly depleted in ice as a consequence of microscopic collisional mixing, a previously unknown effect where monomers are stored away in aggregate interiors through rapid cycles of coagulation and fragmentation.","This suggests that ice processing is highly coupled to collisional processing in this disk region, which implies that the interiors of dust aggregates must be considered and not just their surfaces."],"url":"http://arxiv.org/abs/2402.19112v1","category":"astro-ph.EP"}
{"created":"2024-02-29 12:23:35","title":"Investigating the track structure of carbon ions at HIT using the PTB ion counter nanodosimeter. Part2: Geant4, track structure simulation and measurement of ionization cluster size distributions for different secondary particle background","abstract":"Using the PTB Ion Counter nanodosimeter at the Heidelberg Ion-Beam Therapy Center (HIT), the track structure (TS) of therapeutic carbon ions (C12) after penetrating a layer of simulated tissue was investigated for the first time. Experiments performed at different positions of the pristine C12 Bragg peak were reported in the first part of the paper. In this part, simulations with Geant4 are reported, for investigating the composition of the radiation field in the nanodosimeter and provide input data for TS simulations using the PTra code. The focus is on comparing these simulations with a second set of experiments aimed at assessing the impact of different secondary particle backgrounds on measurements at the same C12 energy in the detector. The Geant4 simulations show that the mean energy of C12 in the nanodosimeter was between 120 MeV and 320 MeV lower than the value of 1 GeV predicted by SRIM. While these findings align with the observed change in the mean measured ionization cluster size ($M_1$) for C12 traversing the nanodosimeter, TS simulations yield much lower values for the expected $M_1$ under these conditions. The TS simulations further indicate that the observed enhanced $M_1$ for C12 passing the target at a nonzero impact parameter is largely due to secondary heavy-charged particles. A smaller contribution to this background comes from C12 missing the nanodosimeter's trigger detector in the experiments. The results of the Geant4 simulations show that only about a third of the C12 traversing the nanodosimeter hit the trigger detector. The background owing to hidden coincidences between C12 registered in the trigger detector and those missing it or secondary heavy-charged particles from different events only explains the enhanced $M_1$ in the penumbra of the ion tracks but not the difference observed when C12 traverse the target volume.","sentences":["Using the PTB Ion Counter nanodosimeter at the Heidelberg Ion-Beam Therapy Center (HIT), the track structure (TS) of therapeutic carbon ions (C12) after penetrating a layer of simulated tissue was investigated for the first time.","Experiments performed at different positions of the pristine C12 Bragg peak were reported in the first part of the paper.","In this part, simulations with Geant4 are reported, for investigating the composition of the radiation field in the nanodosimeter and provide input data for TS simulations using the PTra code.","The focus is on comparing these simulations with a second set of experiments aimed at assessing the impact of different secondary particle backgrounds on measurements at the same C12 energy in the detector.","The Geant4 simulations show that the mean energy of C12 in the nanodosimeter was between 120 MeV and 320 MeV lower than the value of 1 GeV predicted by SRIM.","While these findings align with the observed change in the mean measured ionization cluster size ($M_1$) for C12 traversing the nanodosimeter, TS simulations yield much lower values for the expected $M_1$ under these conditions.","The TS simulations further indicate that the observed enhanced $M_1$ for C12 passing the target at a nonzero impact parameter is largely due to secondary heavy-charged particles.","A smaller contribution to this background comes from C12 missing the nanodosimeter's trigger detector in the experiments.","The results of the Geant4 simulations show that only about a third of the C12 traversing the nanodosimeter hit the trigger detector.","The background owing to hidden coincidences between C12 registered in the trigger detector and those missing it or secondary heavy-charged particles from different events only explains the enhanced $M_1$ in the penumbra of the ion tracks but not the difference observed when C12 traverse the target volume."],"url":"http://arxiv.org/abs/2402.19094v1","category":"physics.med-ph"}
{"created":"2024-02-29 12:17:54","title":"Best Arm Identification with Resource Constraints","abstract":"Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem. The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull. We make two novel contributions. We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm. Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption.","sentences":["Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem.","The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull.","We make two novel contributions.","We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR).","The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm.","Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption."],"url":"http://arxiv.org/abs/2402.19090v1","category":"cs.LG"}
{"created":"2024-02-29 11:49:03","title":"Two-scale model of quasi-steady flow of electrolyte in weakly piezoelectric porous media","abstract":"This paper presents a new homogenized model of two-component electrolyte transport through a weakly piezoelectric porous medium. The model relevant to the microscopic scale describes quasi-stationary states of the medium while reflecting essential physical phenomena, such as electrochemical interactions in a dilute Newtonian solvent under assumptions of slow flow. The dimensional analysis of the mathematical model introduces scaling of the viscosity, electric permittivity, piezoelectric coupling, and dielectric tensor. The micromodel is linearized around the reference state represented by the thermodynamic equilibrium and further upscaled through the asymptotic homogenization method. Due to the scaling of parameters, the derived limit model retains the characteristic length associated with the pore size and the electric double-layer thickness. The upscaling procedure gives a fully coupled macroscopic model describing the electrolyte flow in terms of a global pressure and streaming potentials of the two ionic species in the weakly piezoelectric matrix. By virtue of the characteristic responses, quantities of interest are reconstructed at the microscopic scale using the resolved macroscopic fields. The coupling between electrochemical and mechanical phenomena influenced by the skeleton piezoelectricity is illustrated using numerical examples. The model, which was motivated by the cortical bone porous tissue, is widely applicable in designing new biomaterials involving piezoelectric stimulation.","sentences":["This paper presents a new homogenized model of two-component electrolyte transport through a weakly piezoelectric porous medium.","The model relevant to the microscopic scale describes quasi-stationary states of the medium while reflecting essential physical phenomena, such as electrochemical interactions in a dilute Newtonian solvent under assumptions of slow flow.","The dimensional analysis of the mathematical model introduces scaling of the viscosity, electric permittivity, piezoelectric coupling, and dielectric tensor.","The micromodel is linearized around the reference state represented by the thermodynamic equilibrium and further upscaled through the asymptotic homogenization method.","Due to the scaling of parameters, the derived limit model retains the characteristic length associated with the pore size and the electric double-layer thickness.","The upscaling procedure gives a fully coupled macroscopic model describing the electrolyte flow in terms of a global pressure and streaming potentials of the two ionic species in the weakly piezoelectric matrix.","By virtue of the characteristic responses, quantities of interest are reconstructed at the microscopic scale using the resolved macroscopic fields.","The coupling between electrochemical and mechanical phenomena influenced by the skeleton piezoelectricity is illustrated using numerical examples.","The model, which was motivated by the cortical bone porous tissue, is widely applicable in designing new biomaterials involving piezoelectric stimulation."],"url":"http://arxiv.org/abs/2402.19068v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 11:23:41","title":"Enhancing key rates of QKD protocol by Coincidence Detection","abstract":"In theory, quantum key distribution (QKD) provides unconditional security; however, its practical implementations are susceptible to exploitable vulnerabilities. This investigation tackles the constraints in practical QKD implementations using weak coherent pulses. We improve on the conventional approach of using decoy pulses by integrating it with the coincidence detection (CD) protocol. Additionally, we introduce an easy-to-implement algorithm to compute asymptotic key rates for the protocol. Furthermore, we have carried out an experimental implementation of the protocol, where we demonstrate that monitoring coincidences in the decoy state protocol leads to enhanced key rates under realistic experimental conditions.","sentences":["In theory, quantum key distribution (QKD) provides unconditional security; however, its practical implementations are susceptible to exploitable vulnerabilities.","This investigation tackles the constraints in practical QKD implementations using weak coherent pulses.","We improve on the conventional approach of using decoy pulses by integrating it with the coincidence detection (CD) protocol.","Additionally, we introduce an easy-to-implement algorithm to compute asymptotic key rates for the protocol.","Furthermore, we have carried out an experimental implementation of the protocol, where we demonstrate that monitoring coincidences in the decoy state protocol leads to enhanced key rates under realistic experimental conditions."],"url":"http://arxiv.org/abs/2402.19049v1","category":"quant-ph"}
{"created":"2024-02-29 11:19:48","title":"On the Improvement of Predictive Modeling Using Bayesian Stacking and Posterior Predictive Checking","abstract":"Model uncertainty is pervasive in real world analysis situations and is an often-neglected issue in applied statistics. However, standard approaches to the research process do not address the inherent uncertainty in model building and, thus, can lead to overconfident and misleading analysis interpretations. One strategy to incorporate more flexible models is to base inferences on predictive modeling. This approach provides an alternative to existing explanatory models, as inference is focused on the posterior predictive distribution of the response variable. Predictive modeling can advance explanatory ambitions in the social sciences and in addition enrich the understanding of social phenomena under investigation. Bayesian stacking is a methodological approach rooted in Bayesian predictive modeling. In this paper, we outline the method of Bayesian stacking but add to it the approach of posterior predictive checking (PPC) as a means of assessing the predictive quality of those elements of the stacking ensemble that are important to the research question. Thus, we introduce a viable workflow for incorporating PPC into predictive modeling using Bayesian stacking without presuming the existence of a true model. We apply these tools to the PISA 2018 data to investigate potential inequalities in reading competency with respect to gender and socio-economic background. Our empirical example serves as rough guideline for practitioners who want to implement the concepts of predictive modeling and model uncertainty in their work to similar research questions.","sentences":["Model uncertainty is pervasive in real world analysis situations and is an often-neglected issue in applied statistics.","However, standard approaches to the research process do not address the inherent uncertainty in model building and, thus, can lead to overconfident and misleading analysis interpretations.","One strategy to incorporate more flexible models is to base inferences on predictive modeling.","This approach provides an alternative to existing explanatory models, as inference is focused on the posterior predictive distribution of the response variable.","Predictive modeling can advance explanatory ambitions in the social sciences and in addition enrich the understanding of social phenomena under investigation.","Bayesian stacking is a methodological approach rooted in Bayesian predictive modeling.","In this paper, we outline the method of Bayesian stacking but add to it the approach of posterior predictive checking (PPC) as a means of assessing the predictive quality of those elements of the stacking ensemble that are important to the research question.","Thus, we introduce a viable workflow for incorporating PPC into predictive modeling using Bayesian stacking without presuming the existence of a true model.","We apply these tools to the PISA 2018 data to investigate potential inequalities in reading competency with respect to gender and socio-economic background.","Our empirical example serves as rough guideline for practitioners who want to implement the concepts of predictive modeling and model uncertainty in their work to similar research questions."],"url":"http://arxiv.org/abs/2402.19046v1","category":"stat.ME"}
{"created":"2024-02-29 11:19:12","title":"Noise-induced survival resonances during fractional killing of cell populations","abstract":"Fractional killing in response to drugs is a hallmark of non-genetic cellular heterogeneity. Yet how individual lineages evade drug treatment, as observed in bacteria and cancer cells, is not quantitatively understood. We analyse a stochastic population model with age-dependent division and death rates and characterise the emergence of fractional killing as a stochastic phenomenon under constant and periodic drug environments. In constant environments, increasing cell cycle noise induces a phase transition from complete to fractional killing, while increasing death noise can induce the reverse transition. In periodic drug environments, we discover survival resonance phenomena that give rise to peaks in the survival probabilities at division or death times that are multiples of the environment duration not seen in unstructured populations.","sentences":["Fractional killing in response to drugs is a hallmark of non-genetic cellular heterogeneity.","Yet how individual lineages evade drug treatment, as observed in bacteria and cancer cells, is not quantitatively understood.","We analyse a stochastic population model with age-dependent division and death rates and characterise the emergence of fractional killing as a stochastic phenomenon under constant and periodic drug environments.","In constant environments, increasing cell cycle noise induces a phase transition from complete to fractional killing, while increasing death noise can induce the reverse transition.","In periodic drug environments, we discover survival resonance phenomena that give rise to peaks in the survival probabilities at division or death times that are multiples of the environment duration not seen in unstructured populations."],"url":"http://arxiv.org/abs/2402.19045v1","category":"q-bio.PE"}
{"created":"2024-02-29 11:03:44","title":"Pressure-induced optical anisotropy of HfS$_2$","abstract":"The effect of pressure on Raman scattering (RS) in the bulk HfS$_2$ is investigated under hydrostatic and non-hydrostatic conditions. The RS lineshape does not change significantly in the hydrostatic regime, showing a systematic blueshift of the spectral features. In a non-hydrostatic environment, seven peaks emerge in the spectrum ($P$=7 GPa) dominating the lineshape up to $P$=10.5 GPa. The change in the RS lineshape manifests a pressure-induced phase transition in HfS$_2$. The simultaneous observation of both low-pressure (LP) and high-pressure (HP) related RS peaks suggests the corresponding coexistence of two different phases over a large pressure range. We found that the HP-related phase is metastable, persisting during the decompression cycle down to $P$=1.2 GPa with the LP-related features finally recovering at even lower pressures. The angle-resolved polarized RS (ARPRS) performed under $P$=7.4 GPa revealed a strong in-plane anisotropy of both the LP-related A$_{1g}$ mode and the HP peaks. The anisotropy is related to the possible distortion of the structure induced by the non-hydrostatic component of the pressure. We describe the obtained results by the influence of the non-hydrostatic pressure on the observed phase transition. We interpret our results in terms of a distorted $Pnma$ phase as a possible HP induced structure of HfS$_2$.","sentences":["The effect of pressure on Raman scattering (RS) in the bulk HfS$_2$ is investigated under hydrostatic and non-hydrostatic conditions.","The RS lineshape does not change significantly in the hydrostatic regime, showing a systematic blueshift of the spectral features.","In a non-hydrostatic environment, seven peaks emerge in the spectrum ($P$=7 GPa) dominating the lineshape up to $P$=10.5 GPa.","The change in the RS lineshape manifests a pressure-induced phase transition in HfS$_2$. The simultaneous observation of both low-pressure (LP) and high-pressure (HP) related RS peaks suggests the corresponding coexistence of two different phases over a large pressure range.","We found that the HP-related phase is metastable, persisting during the decompression cycle down to $P$=1.2 GPa with the LP-related features finally recovering at even lower pressures.","The angle-resolved polarized RS (ARPRS) performed under $P$=7.4 GPa revealed a strong in-plane anisotropy of both the LP-related A$_{1g}$ mode and the HP peaks.","The anisotropy is related to the possible distortion of the structure induced by the non-hydrostatic component of the pressure.","We describe the obtained results by the influence of the non-hydrostatic pressure on the observed phase transition.","We interpret our results in terms of a distorted $Pnma$ phase as a possible HP induced structure of HfS$_2$."],"url":"http://arxiv.org/abs/2402.19040v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 11:03:02","title":"Triplet fermions in MXenes: The Applications for spintronic-based devices","abstract":"We investigate the electronic properties of MXenes by three bands tight-binding model of \\d_{z^2} , \\d_{xy} , and \\d_{x^2-y^2} orbitals. The three corresponding bands touch each other at high symmetry K point in the case of absence of spin-orbit interaction. The proper parameters can be obtained by Slater-Koster parameters related to chemical bonding, \\pi, \\sigma, and \\delta bonds. The model calculated for these band structures make an agreement with the same trend as discussed in DFT calculation which the hopping parameters may be identified roughly by fermi velocity. Furthermore, the triplet fermion occurs around K point hosting by flat band, leading to super-Klein tunnelling and anti-super-Klein tunnelling for gapped and gapless pseudospin-1 fermion, respectively. These may apply for nanodevices operated by spin polarization which is more stable than that of the conventional two-dimensional materials.","sentences":["We investigate the electronic properties of MXenes by three bands tight-binding model of \\d_{z^2} , \\d_{xy} , and \\d_{x^2-y^2} orbitals.","The three corresponding bands touch each other at high symmetry K point in the case of absence of spin-orbit interaction.","The proper parameters can be obtained by Slater-Koster parameters related to chemical bonding, \\pi, \\sigma, and \\delta bonds.","The model calculated for these band structures make an agreement with the same trend as discussed in DFT calculation which the hopping parameters may be identified roughly by fermi velocity.","Furthermore, the triplet fermion occurs around K point hosting by flat band, leading to super-Klein tunnelling and anti-super-Klein tunnelling for gapped and gapless pseudospin-1 fermion, respectively.","These may apply for nanodevices operated by spin polarization which is more stable than that of the conventional two-dimensional materials."],"url":"http://arxiv.org/abs/2402.19039v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 10:48:43","title":"Effective Results in The Metric Theory of Quantitative Diophantine Approximation","abstract":"Many results related to quantitative problems in the metric theory of Diophantine approximation are asymptotic, such as the number of rational solutions to certain inequalities grows with the same rate almost everywhere modulo an asymptotic error term. The error term incorporates an implicit constant that varies from one point to another. This means that applications of these results does not give concrete bounds when applied to, say a finite sum, or when applied to counting the number of solutions up to a finite point for a given inequality. This paper addresses this problem and makes the tools and their results effective, by making the implicit constant explicit outside of an exceptional subset of Lebesgue measure at most $\\delta>0$, an arbitrarily small constant chosen in advance. We deduce from this the fully effective results for Schmidt's Theorem, quantitative Koukoulopoulos-Maynard Theorem and quantitative results on $M_{0}$-sets; we also provide effective results regarding statistics of normal numbers and strong law of large numbers.","sentences":["Many results related to quantitative problems in the metric theory of Diophantine approximation are asymptotic, such as the number of rational solutions to certain inequalities grows with the same rate almost everywhere modulo an asymptotic error term.","The error term incorporates an implicit constant that varies from one point to another.","This means that applications of these results does not give concrete bounds when applied to, say a finite sum, or when applied to counting the number of solutions up to a finite point for a given inequality.","This paper addresses this problem and makes the tools and their results effective, by making the implicit constant explicit outside of an exceptional subset of Lebesgue measure at most $\\delta>0$, an arbitrarily small constant chosen in advance.","We deduce from this the fully effective results for Schmidt's Theorem, quantitative Koukoulopoulos-Maynard Theorem and quantitative results on $M_{0}$-sets; we also provide effective results regarding statistics of normal numbers and strong law of large numbers."],"url":"http://arxiv.org/abs/2402.19032v1","category":"math.NT"}
{"created":"2024-02-29 10:31:36","title":"Enhancing the Power of Gaussian Graphical Model Inference by Modeling the Graph Structure","abstract":"For the problem of inferring a Gaussian graphical model (GGM), this work explores the application of a recent approach from the multiple testing literature for graph inference. The main idea of the method by Rebafka et al. (2022) is to model the data by a latent variable model, the so-called noisy stochastic block model (NSBM), and then use the associated ${\\ell}$-values to infer the graph. The inferred graph controls the false discovery rate, that means that the proportion of falsely declared edges does not exceed a user-defined nominal level. Here it is shown that any test statistic from the GGM literature can be used as input for the NSBM approach to perform GGM inference. To make the approach feasible in practice, a new, computationally efficient inference algorithm for the NSBM is developed relying on a greedy approach to maximize the integrated complete-data likelihood. Then an extensive numerical study illustrates that the NSBM approach outperforms the state of the art for any of the here considered GGM-test statistics. In particular in sparse settings and on real datasets a significant gain in power is observed.","sentences":["For the problem of inferring a Gaussian graphical model (GGM), this work explores the application of a recent approach from the multiple testing literature for graph inference.","The main idea of the method by Rebafka et al. (2022) is to model the data by a latent variable model, the so-called noisy stochastic block model (NSBM), and then use the associated ${\\ell}$-values to infer the graph.","The inferred graph controls the false discovery rate, that means that the proportion of falsely declared edges does not exceed a user-defined nominal level.","Here it is shown that any test statistic from the GGM literature can be used as input for the NSBM approach to perform GGM inference.","To make the approach feasible in practice, a new, computationally efficient inference algorithm for the NSBM is developed relying on a greedy approach to maximize the integrated complete-data likelihood.","Then an extensive numerical study illustrates that the NSBM approach outperforms the state of the art for any of the here considered GGM-test statistics.","In particular in sparse settings and on real datasets a significant gain in power is observed."],"url":"http://arxiv.org/abs/2402.19021v1","category":"stat.ME"}
{"created":"2024-02-29 10:23:23","title":"SPriFed-OMP: A Differentially Private Federated Learning Algorithm for Sparse Basis Recovery","abstract":"Sparse basis recovery is a classical and important statistical learning problem when the number of model dimensions $p$ is much larger than the number of samples $n$. However, there has been little work that studies sparse basis recovery in the Federated Learning (FL) setting, where the client data's differential privacy (DP) must also be simultaneously protected. In particular, the performance guarantees of existing DP-FL algorithms (such as DP-SGD) will degrade significantly when $p \\gg n$, and thus, they will fail to learn the true underlying sparse model accurately. In this work, we develop a new differentially private sparse basis recovery algorithm for the FL setting, called SPriFed-OMP. SPriFed-OMP converts OMP (Orthogonal Matching Pursuit) to the FL setting. Further, it combines SMPC (secure multi-party computation) and DP to ensure that only a small amount of noise needs to be added in order to achieve differential privacy. As a result, SPriFed-OMP can efficiently recover the true sparse basis for a linear model with only $n = O(\\sqrt{p})$ samples. We further present an enhanced version of our approach, SPriFed-OMP-GRAD based on gradient privatization, that improves the performance of SPriFed-OMP. Our theoretical analysis and empirical results demonstrate that both SPriFed-OMP and SPriFed-OMP-GRAD terminate in a small number of steps, and they significantly outperform the previous state-of-the-art DP-FL solutions in terms of the accuracy-privacy trade-off.","sentences":["Sparse basis recovery is a classical and important statistical learning problem when the number of model dimensions $p$ is much larger than the number of samples $n$. However, there has been little work that studies sparse basis recovery in the Federated Learning (FL) setting, where the client data's differential privacy (DP) must also be simultaneously protected.","In particular, the performance guarantees of existing DP-FL algorithms (such as DP-SGD) will degrade significantly when $p \\gg n$, and thus, they will fail to learn the true underlying sparse model accurately.","In this work, we develop a new differentially private sparse basis recovery algorithm for the FL setting, called SPriFed-OMP.","SPriFed-OMP converts OMP (Orthogonal Matching Pursuit) to the FL setting.","Further, it combines SMPC (secure multi-party computation) and DP to ensure that only a small amount of noise needs to be added in order to achieve differential privacy.","As a result, SPriFed-OMP can efficiently recover the true sparse basis for a linear model with only $n = O(\\sqrt{p})$ samples.","We further present an enhanced version of our approach, SPriFed-OMP-GRAD based on gradient privatization, that improves the performance of SPriFed-OMP.","Our theoretical analysis and empirical results demonstrate that both SPriFed-OMP and SPriFed-OMP-GRAD terminate in a small number of steps, and they significantly outperform the previous state-of-the-art DP-FL solutions in terms of the accuracy-privacy trade-off."],"url":"http://arxiv.org/abs/2402.19016v1","category":"cs.LG"}
{"created":"2024-02-29 09:47:59","title":"Motion of test particles in quasi anti-de Sitter regular black holes","abstract":"We explore the characteristics of two novel regular spacetimes that exhibit a non-zero vacuum energy term, under the form of a (quasi) anti-de Sitter phase. Specifically, the first metric is spherical, while the second, derived by applying the generalized Newman-Janis algorithm to the first, is axisymmetric. We show that the equations of state of the effective fluids associated with the two metrics asymptotically tend to negative values, resembling quintessence. In addition, we study test particle motions, illustrating the main discrepancies among our models and more conventional metrics exhibiting non-vanishing anti-de Sitter phase.","sentences":["We explore the characteristics of two novel regular spacetimes that exhibit a non-zero vacuum energy term, under the form of a (quasi) anti-de Sitter phase.","Specifically, the first metric is spherical, while the second, derived by applying the generalized Newman-Janis algorithm to the first, is axisymmetric.","We show that the equations of state of the effective fluids associated with the two metrics asymptotically tend to negative values, resembling quintessence.","In addition, we study test particle motions, illustrating the main discrepancies among our models and more conventional metrics exhibiting non-vanishing anti-de Sitter phase."],"url":"http://arxiv.org/abs/2402.18997v1","category":"gr-qc"}
{"created":"2024-02-29 09:44:32","title":"Enhanced metamagnetic shape memory effect in Heusler-type Ni37Co11Mn43Sn9 polycrystalline ferromagnetic shape memory alloy","abstract":"Polycrystalline Ni-Co-Mn-Sn based ferromagnetic shape memory alloys (FSMAs) show promise as actuator materials, but their practical application involving magnetic field induced strain (MFIS) is often limited by three factors: the requirement for high magnetic fields (> 5 T), martensitic transition temperature away from room temperature, and limited recovery of pre-strain applied to the martensite phase. Current work investigates the martensitic transition (MT) and shape memory effect under the application of magnetic field for bulk polycrystalline Ni37Co11Mn43Sn9 alloy. The outcome of the study reveals a metamagnetic transition from the martensitic phase to the austenitic phase at a low field of 2.8 T at 300 K which results 0.25% spontaneous MFIS. Interestingly, 1.3% pre-strained specimen registers a 100% recovery with the application of magnetic field of 4.5 T. Furthermore, the pre-strained specimen exhibited a two-way shape memory effect between a strain value of 1.0% to 1.55% during the field loading and unloading sequences. Notably, this study also demonstrates, to the best of our knowledge , for the first time, that the spontaneous strain and pre-strain add together. This finding paves the way for achieving a giant MFIS by pre-straining a Ni-Mn-Sn/In class of FSMAs which shows large spontaneous MFIS.","sentences":["Polycrystalline Ni-Co-Mn-Sn based ferromagnetic shape memory alloys (FSMAs) show promise as actuator materials, but their practical application involving magnetic field induced strain (MFIS) is often limited by three factors: the requirement for high magnetic fields (> 5 T), martensitic transition temperature away from room temperature, and limited recovery of pre-strain applied to the martensite phase.","Current work investigates the martensitic transition (MT) and shape memory effect under the application of magnetic field for bulk polycrystalline Ni37Co11Mn43Sn9 alloy.","The outcome of the study reveals a metamagnetic transition from the martensitic phase to the austenitic phase at a low field of 2.8 T at 300 K which results 0.25% spontaneous MFIS.","Interestingly, 1.3% pre-strained specimen registers a 100% recovery with the application of magnetic field of 4.5 T. Furthermore, the pre-strained specimen exhibited a two-way shape memory effect between a strain value of 1.0% to 1.55% during the field loading and unloading sequences.","Notably, this study also demonstrates, to the best of our knowledge , for the first time, that the spontaneous strain and pre-strain add together.","This finding paves the way for achieving a giant MFIS by pre-straining a Ni-Mn-Sn/In class of FSMAs which shows large spontaneous MFIS."],"url":"http://arxiv.org/abs/2402.18992v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 08:58:57","title":"MambaStock: Selective state space model for stock prediction","abstract":"The stock market plays a pivotal role in economic development, yet its intricate volatility poses challenges for investors. Consequently, research and accurate predictions of stock price movements are crucial for mitigating risks. Traditional time series models fall short in capturing nonlinearity, leading to unsatisfactory stock predictions. This limitation has spurred the widespread adoption of neural networks for stock prediction, owing to their robust nonlinear generalization capabilities. Recently, Mamba, a structured state space sequence model with a selection mechanism and scan module (S6), has emerged as a powerful tool in sequence modeling tasks. Leveraging this framework, this paper proposes a novel Mamba-based model for stock price prediction, named MambaStock. The proposed MambaStock model effectively mines historical stock market data to predict future stock prices without handcrafted features or extensive preprocessing procedures. Empirical studies on several stocks indicate that the MambaStock model outperforms previous methods, delivering highly accurate predictions. This enhanced accuracy can assist investors and institutions in making informed decisions, aiming to maximize returns while minimizing risks. This work underscores the value of Mamba in time-series forecasting. Source code is available at https://github.com/zshicode/MambaStock.","sentences":["The stock market plays a pivotal role in economic development, yet its intricate volatility poses challenges for investors.","Consequently, research and accurate predictions of stock price movements are crucial for mitigating risks.","Traditional time series models fall short in capturing nonlinearity, leading to unsatisfactory stock predictions.","This limitation has spurred the widespread adoption of neural networks for stock prediction, owing to their robust nonlinear generalization capabilities.","Recently, Mamba, a structured state space sequence model with a selection mechanism and scan module (S6), has emerged as a powerful tool in sequence modeling tasks.","Leveraging this framework, this paper proposes a novel Mamba-based model for stock price prediction, named MambaStock.","The proposed MambaStock model effectively mines historical stock market data to predict future stock prices without handcrafted features or extensive preprocessing procedures.","Empirical studies on several stocks indicate that the MambaStock model outperforms previous methods, delivering highly accurate predictions.","This enhanced accuracy can assist investors and institutions in making informed decisions, aiming to maximize returns while minimizing risks.","This work underscores the value of Mamba in time-series forecasting.","Source code is available at https://github.com/zshicode/MambaStock."],"url":"http://arxiv.org/abs/2402.18959v1","category":"cs.CE"}
{"created":"2024-02-29 08:51:51","title":"WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concepts","abstract":"Recent advancements in neural networks have showcased their remarkable capabilities across various domains. Despite these successes, the \"black box\" problem still remains. Addressing this, we propose a novel framework, WWW, that offers the 'what', 'where', and 'why' of the neural network decisions in human-understandable terms. Specifically, WWW utilizes adaptive selection for concept discovery, employing adaptive cosine similarity and thresholding techniques to effectively explain 'what'. To address the 'where' and 'why', we proposed a novel combination of neuron activation maps (NAMs) with Shapley values, generating localized concept maps and heatmaps for individual inputs. Furthermore, WWW introduces a method for predicting uncertainty, leveraging heatmap similarities to estimate 'how' reliable the prediction is. Experimental evaluations of WWW demonstrate superior performance in both quantitative and qualitative metrics, outperforming existing methods in interpretability. WWW provides a unified solution for explaining 'what', 'where', and 'why', introducing a method for localized explanations from global interpretations and offering a plug-and-play solution adaptable to various architectures.","sentences":["Recent advancements in neural networks have showcased their remarkable capabilities across various domains.","Despite these successes, the \"black box\" problem still remains.","Addressing this, we propose a novel framework, WWW, that offers the 'what', 'where', and 'why' of the neural network decisions in human-understandable terms.","Specifically, WWW utilizes adaptive selection for concept discovery, employing adaptive cosine similarity and thresholding techniques to effectively explain 'what'.","To address the 'where' and 'why', we proposed a novel combination of neuron activation maps (NAMs) with Shapley values, generating localized concept maps and heatmaps for individual inputs.","Furthermore, WWW introduces a method for predicting uncertainty, leveraging heatmap similarities to estimate 'how' reliable the prediction is.","Experimental evaluations of WWW demonstrate superior performance in both quantitative and qualitative metrics, outperforming existing methods in interpretability.","WWW provides a unified solution for explaining 'what', 'where', and 'why', introducing a method for localized explanations from global interpretations and offering a plug-and-play solution adaptable to various architectures."],"url":"http://arxiv.org/abs/2402.18956v1","category":"cs.CV"}
{"created":"2024-02-29 08:28:04","title":"PopALM: Popularity-Aligned Language Models for Social Media Trendy Response Prediction","abstract":"Social media platforms are daily exhibiting millions of events. To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events. While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through reinforcement learning. Recognizing the noisy labels from user \"likes\", we tailor-make curriculum learning in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training. In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models.","sentences":["Social media platforms are daily exhibiting millions of events.","To preliminarily predict the mainstream public reaction to these events, we study trendy response prediction to automatically generate top-liked user replies to social media events.","While previous works focus on generating responses without factoring in popularity, we propose Popularity-Aligned Language Models (PopALM) to distinguish responses liked by a larger audience through reinforcement learning.","Recognizing the noisy labels from user \"likes\", we tailor-make curriculum learning in proximal policy optimization (PPO) to help models capture the essential samples for easy-to-hard training.","In experiments, we build a large-scale Weibo dataset for trendy response prediction, and its results show that PopALM can help boost the performance of advanced language models."],"url":"http://arxiv.org/abs/2402.18950v1","category":"cs.CL"}
{"created":"2024-02-29 08:27:01","title":"Improving Group Connectivity for Generalization of Federated Deep Learning","abstract":"Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propose FedGuCci and FedGuCci+, improving group connectivity for better generalization. It is shown that our methods can boost the generalization of FL under client heterogeneity across various tasks (4 CV datasets and 6 NLP datasets), models (both convolutional and transformer-based), and training paradigms (both from-scratch and pretrain-finetune).","sentences":["Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion.","The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications.","In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model.","The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks.","Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL).","Based on the findings, we propose FedGuCci and FedGuCci+, improving group connectivity for better generalization.","It is shown that our methods can boost the generalization of FL under client heterogeneity across various tasks (4 CV datasets and 6 NLP datasets), models (both convolutional and transformer-based), and training paradigms (both from-scratch and pretrain-finetune)."],"url":"http://arxiv.org/abs/2402.18949v1","category":"cs.LG"}
{"created":"2024-02-29 07:31:59","title":"PCDepth: Pattern-based Complementary Learning for Monocular Depth Estimation by Best of Both Worlds","abstract":"Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination. Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding. However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels. For example, event data is likely to complement contours of scene objects. In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth). Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance. Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios. Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth. Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios.","sentences":["Event cameras can record scene dynamics with high temporal resolution, providing rich scene details for monocular depth estimation (MDE) even at low-level illumination.","Therefore, existing complementary learning approaches for MDE fuse intensity information from images and scene details from event data for better scene understanding.","However, most methods directly fuse two modalities at pixel level, ignoring that the attractive complementarity mainly impacts high-level patterns that only occupy a few pixels.","For example, event data is likely to complement contours of scene objects.","In this paper, we discretize the scene into a set of high-level patterns to explore the complementarity and propose a Pattern-based Complementary learning architecture for monocular Depth estimation (PCDepth).","Concretely, PCDepth comprises two primary components: a complementary visual representation learning module for discretizing the scene into high-level patterns and integrating complementary patterns across modalities and a refined depth estimator aimed at scene reconstruction and depth prediction while maintaining an efficiency-accuracy balance.","Through pattern-based complementary learning, PCDepth fully exploits two modalities and achieves more accurate predictions than existing methods, especially in challenging nighttime scenarios.","Extensive experiments on MVSEC and DSEC datasets verify the effectiveness and superiority of our PCDepth.","Remarkably, compared with state-of-the-art, PCDepth achieves a 37.9% improvement in accuracy in MVSEC nighttime scenarios."],"url":"http://arxiv.org/abs/2402.18925v1","category":"cs.CV"}
{"created":"2024-02-29 07:29:27","title":"Semi-Supervised U-statistics","abstract":"Semi-supervised datasets are ubiquitous across diverse domains where obtaining fully labeled data is costly or time-consuming. The prevalence of such datasets has consistently driven the demand for new tools and methods that exploit the potential of unlabeled data. Responding to this demand, we introduce semi-supervised U-statistics enhanced by the abundance of unlabeled data, and investigate their statistical properties. We show that the proposed approach is asymptotically Normal and exhibits notable efficiency gains over classical U-statistics by effectively integrating various powerful prediction tools into the framework. To understand the fundamental difficulty of the problem, we derive minimax lower bounds in semi-supervised settings and showcase that our procedure is semi-parametrically efficient under regularity conditions. Moreover, tailored to bivariate kernels, we propose a refined approach that outperforms the classical U-statistic across all degeneracy regimes, and demonstrate its optimality properties. Simulation studies are conducted to corroborate our findings and to further demonstrate our framework.","sentences":["Semi-supervised datasets are ubiquitous across diverse domains where obtaining fully labeled data is costly or time-consuming.","The prevalence of such datasets has consistently driven the demand for new tools and methods that exploit the potential of unlabeled data.","Responding to this demand, we introduce semi-supervised U-statistics enhanced by the abundance of unlabeled data, and investigate their statistical properties.","We show that the proposed approach is asymptotically Normal and exhibits notable efficiency gains over classical U-statistics by effectively integrating various powerful prediction tools into the framework.","To understand the fundamental difficulty of the problem, we derive minimax lower bounds in semi-supervised settings and showcase that our procedure is semi-parametrically efficient under regularity conditions.","Moreover, tailored to bivariate kernels, we propose a refined approach that outperforms the classical U-statistic across all degeneracy regimes, and demonstrate its optimality properties.","Simulation studies are conducted to corroborate our findings and to further demonstrate our framework."],"url":"http://arxiv.org/abs/2402.18921v1","category":"math.ST"}
{"created":"2024-02-29 06:58:21","title":"False Discovery Rate Control for Confounder Selection Using Mirror Statistics","abstract":"While data-driven confounder selection requires careful consideration, it is frequently employed in observational studies to adjust for confounding factors. Widely recognized criteria for confounder selection include the minimal set approach, which involves selecting variables relevant to both treatment and outcome, and the union set approach, which involves selecting variables for either treatment or outcome. These approaches are often implemented using heuristics and off-the-shelf statistical methods, where the degree of uncertainty may not be clear. In this paper, we focus on the false discovery rate (FDR) to measure uncertainty in confounder selection. We define the FDR specific to confounder selection and propose methods based on the mirror statistic, a recently developed approach for FDR control that does not rely on p-values. The proposed methods are free from p-values and require only the assumption of some symmetry in the distribution of the mirror statistic. It can be easily combined with sparse estimation and other methods that involve difficulties in deriving p-values. The properties of the proposed method are investigated by exhaustive numerical experiments. Particularly in high-dimensional data scenarios, our method outperforms conventional methods.","sentences":["While data-driven confounder selection requires careful consideration, it is frequently employed in observational studies to adjust for confounding factors.","Widely recognized criteria for confounder selection include the minimal set approach, which involves selecting variables relevant to both treatment and outcome, and the union set approach, which involves selecting variables for either treatment or outcome.","These approaches are often implemented using heuristics and off-the-shelf statistical methods, where the degree of uncertainty may not be clear.","In this paper, we focus on the false discovery rate (FDR) to measure uncertainty in confounder selection.","We define the FDR specific to confounder selection and propose methods based on the mirror statistic, a recently developed approach for FDR control that does not rely on p-values.","The proposed methods are free from p-values and require only the assumption of some symmetry in the distribution of the mirror statistic.","It can be easily combined with sparse estimation and other methods that involve difficulties in deriving p-values.","The properties of the proposed method are investigated by exhaustive numerical experiments.","Particularly in high-dimensional data scenarios, our method outperforms conventional methods."],"url":"http://arxiv.org/abs/2402.18904v1","category":"stat.ME"}
{"created":"2024-02-29 06:13:10","title":"Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos","abstract":"Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance accuracy and reduce uncertainty by explicitly addressing the diversity of data distributions, all while maintaining minimal computational overhead in environments characterized by heterogeneous data silos. Through experiments conducted on five datasets, our method has demonstrated its superiority, achieving significant improvements in accuracy (by 3%--22.1%) and uncertainty reduction (by 38.83%--96.24%), thereby outperforming contemporary state-of-the-art methods. The source code is available at https://github.com/destiny301/uefl.","sentences":["Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos.","While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions.","In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions.","Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL).","This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty.","Our approach aims to simultaneously enhance accuracy and reduce uncertainty by explicitly addressing the diversity of data distributions, all while maintaining minimal computational overhead in environments characterized by heterogeneous data silos.","Through experiments conducted on five datasets, our method has demonstrated its superiority, achieving significant improvements in accuracy (by 3%--22.1%) and uncertainty reduction (by 38.83%--96.24%), thereby outperforming contemporary state-of-the-art methods.","The source code is available at https://github.com/destiny301/uefl."],"url":"http://arxiv.org/abs/2402.18888v1","category":"cs.LG"}
{"created":"2024-02-29 06:02:45","title":"Supervised Contrastive Representation Learning: Landscape Analysis with Unconstrained Features","abstract":"Recent findings reveal that over-parameterized deep neural networks, trained beyond zero training-error, exhibit a distinctive structural pattern at the final layer, termed as Neural-collapse (NC). These results indicate that the final hidden-layer outputs in such networks display minimal within-class variations over the training set. While existing research extensively investigates this phenomenon under cross-entropy loss, there are fewer studies focusing on its contrastive counterpart, supervised contrastive (SC) loss. Through the lens of NC, this paper employs an analytical approach to study the solutions derived from optimizing the SC loss. We adopt the unconstrained features model (UFM) as a representative proxy for unveiling NC-related phenomena in sufficiently over-parameterized deep networks. We show that, despite the non-convexity of SC loss minimization, all local minima are global minima. Furthermore, the minimizer is unique (up to a rotation). We prove our results by formalizing a tight convex relaxation of the UFM. Finally, through this convex formulation, we delve deeper into characterizing the properties of global solutions under label-imbalanced training data.","sentences":["Recent findings reveal that over-parameterized deep neural networks, trained beyond zero training-error, exhibit a distinctive structural pattern at the final layer, termed as Neural-collapse (NC).","These results indicate that the final hidden-layer outputs in such networks display minimal within-class variations over the training set.","While existing research extensively investigates this phenomenon under cross-entropy loss, there are fewer studies focusing on its contrastive counterpart, supervised contrastive (SC) loss.","Through the lens of NC, this paper employs an analytical approach to study the solutions derived from optimizing the SC loss.","We adopt the unconstrained features model (UFM) as a representative proxy for unveiling NC-related phenomena in sufficiently over-parameterized deep networks.","We show that, despite the non-convexity of SC loss minimization, all local minima are global minima.","Furthermore, the minimizer is unique (up to a rotation).","We prove our results by formalizing a tight convex relaxation of the UFM.","Finally, through this convex formulation, we delve deeper into characterizing the properties of global solutions under label-imbalanced training data."],"url":"http://arxiv.org/abs/2402.18884v1","category":"cs.LG"}
{"created":"2024-02-29 05:03:20","title":"Spectropolarimeter on a 2--4 m class telescope and proposed science cases","abstract":"We propose a spectropolarimeter a covering wavelength range of 3200--7000 {\\AA} [3200{\\AA} chosen as lower limit to go to the atmospheric cut-off. It's ``needed\" for some Serkowski curves and would make the instrument even more unique] for a 2-4~m class telescope. In this article, we discuss the science cases which will be covered with this proposed instrument. The technical requirements and analysis plan for each science case is also discussed. This spectropolarimeter targeting exciting galactic and extra-galactic research, will be unique instrument on a 2-4~m facilities.","sentences":["We propose a spectropolarimeter a covering wavelength range of 3200--7000 {\\AA} [3200{\\AA} chosen as lower limit to go to the atmospheric cut-off.","It's ``needed\" for some Serkowski curves and would make the instrument even more unique] for a 2-4~m class telescope.","In this article, we discuss the science cases which will be covered with this proposed instrument.","The technical requirements and analysis plan for each science case is also discussed.","This spectropolarimeter targeting exciting galactic and extra-galactic research, will be unique instrument on a 2-4~m facilities."],"url":"http://arxiv.org/abs/2402.18854v1","category":"astro-ph.GA"}
{"created":"2024-02-29 04:40:25","title":"Multi-Fidelity Residual Neural Processes for Scalable Surrogate Modeling","abstract":"Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the highest fidelity level by combining data from multiple sources. Traditional methods relying on Gaussian processes can hardly scale to high-dimensional data. Deep learning approaches utilize neural network based encoders and decoders to improve scalability. These approaches share encoded representations across fidelities without including corresponding decoder parameters. At the highest fidelity, the representations are decoded with different parameters, making the shared information inherently inaccurate. This hinders inference performance, especially in out-of-distribution scenarios when the highest fidelity data has limited domain coverage. To address these limitations, we propose Multi-fidelity Residual Neural Processes (MFRNP), a novel multi-fidelity surrogate modeling framework. MFRNP optimizes lower fidelity decoders for accurate information sharing by aggregating lower fidelity surrogate outputs and models residual between the aggregation and ground truth on the highest fidelity. We show that MFRNP significantly outperforms current state-of-the-art in learning partial differential equations and a real-world climate modeling task.","sentences":["Multi-fidelity surrogate modeling aims to learn an accurate surrogate at the highest fidelity level by combining data from multiple sources.","Traditional methods relying on Gaussian processes can hardly scale to high-dimensional data.","Deep learning approaches utilize neural network based encoders and decoders to improve scalability.","These approaches share encoded representations across fidelities without including corresponding decoder parameters.","At the highest fidelity, the representations are decoded with different parameters, making the shared information inherently inaccurate.","This hinders inference performance, especially in out-of-distribution scenarios when the highest fidelity data has limited domain coverage.","To address these limitations, we propose Multi-fidelity Residual Neural Processes (MFRNP), a novel multi-fidelity surrogate modeling framework.","MFRNP optimizes lower fidelity decoders for accurate information sharing by aggregating lower fidelity surrogate outputs and models residual between the aggregation and ground truth on the highest fidelity.","We show that MFRNP significantly outperforms current state-of-the-art in learning partial differential equations and a real-world climate modeling task."],"url":"http://arxiv.org/abs/2402.18846v1","category":"cs.LG"}
{"created":"2024-02-29 03:40:25","title":"Envisioning the Applications and Implications of Generative AI for News Media","abstract":"This article considers the increasing use of algorithmic decision-support systems and synthetic media in the newsroom, and explores how generative models can help reporters and editors across a range of tasks from the conception of a news story to its distribution. Specifically, we draw from a taxonomy of tasks associated with news production, and discuss where generative models could appropriately support reporters, the journalistic and ethical values that must be preserved within these interactions, and the resulting implications for design contributions in this area in the future. Our essay is relevant to practitioners and researchers as they consider using generative AI systems to support different tasks and workflows.","sentences":["This article considers the increasing use of algorithmic decision-support systems and synthetic media in the newsroom, and explores how generative models can help reporters and editors across a range of tasks from the conception of a news story to its distribution.","Specifically, we draw from a taxonomy of tasks associated with news production, and discuss where generative models could appropriately support reporters, the journalistic and ethical values that must be preserved within these interactions, and the resulting implications for design contributions in this area in the future.","Our essay is relevant to practitioners and researchers as they consider using generative AI systems to support different tasks and workflows."],"url":"http://arxiv.org/abs/2402.18835v1","category":"cs.CY"}
{"created":"2024-02-29 03:39:11","title":"A single-particle energy-conserving dissipative particle dynamics approach for simulating thermophoresis of nanoparticles in polymer networks","abstract":"Thermophoresis is an effective method to drive the motion of nanoparticles in fluids. The transport of nanoparticles in polymer networks has significant fundamental and applied importance in biology and medicine, and can be described as Brownian particles crossing entropic barriers. This study proposes a novel extension of dissipative particle dynamics (DPD), called the single-particle energy-conserving dissipative particle dynamics (seDPD), which combines the features of single-particle dissipative particle dynamics (sDPD) and energy-conserving dissipative particle dynamics (eDPD) to simulate the thermophoresis of nanoparticles under temperature gradients. The reliability of the seDPD method is verified by considering the viscosity, thermal diffusivity, and hydrodynamic drag force on the nanoparticles. Using this method, the transport of nanoparticles driven by the thermophoretic force across the polymer network is simulated. The results show that the nanoparticles exhibit the phenomenon of giant acceleration of diffusion (GAD) in the polymer network, indicating that Brownian particles can exhibit GAD when crossing entropic barriers.","sentences":["Thermophoresis is an effective method to drive the motion of nanoparticles in fluids.","The transport of nanoparticles in polymer networks has significant fundamental and applied importance in biology and medicine, and can be described as Brownian particles crossing entropic barriers.","This study proposes a novel extension of dissipative particle dynamics (DPD), called the single-particle energy-conserving dissipative particle dynamics (seDPD), which combines the features of single-particle dissipative particle dynamics (sDPD) and energy-conserving dissipative particle dynamics (eDPD) to simulate the thermophoresis of nanoparticles under temperature gradients.","The reliability of the seDPD method is verified by considering the viscosity, thermal diffusivity, and hydrodynamic drag force on the nanoparticles.","Using this method, the transport of nanoparticles driven by the thermophoretic force across the polymer network is simulated.","The results show that the nanoparticles exhibit the phenomenon of giant acceleration of diffusion (GAD) in the polymer network, indicating that Brownian particles can exhibit GAD when crossing entropic barriers."],"url":"http://arxiv.org/abs/2402.18834v1","category":"cond-mat.soft"}
{"created":"2024-02-29 03:32:20","title":"A direct probe of $\u039b$ potential in nuclear medium","abstract":"Using the Li\\`{e}ge intranuclear-cascade model together with the ablation model ABLA, an investigation is conducted into the effects of $\\Lambda$ potential in $\\Lambda$-nucleus and $\\Lambda$-hypernucleus-nucleus collisions across various beam energies. The findings show that the angle and transverse-momentum distributions of scattered $\\Lambda$ hyperon, the scattering cross section of the $\\Lambda$ hyperon in $\\Lambda$-nucleus collisions as well as the rapidity distribution of $\\Lambda$ hyperon in $\\Lambda$-hypernucleus-nucleus collisions are significantly influenced by the strength of the $\\Lambda$ potential in these scattering reactions across various beam energies. These demonstrations, unhindered by the uncertainties of $\\Lambda$ and hypernuclei productions in nuclear medium, allow for a direct investigation of the $\\Lambda$ potential, especially its momentum dependence. The criticality of probing the $\\Lambda$ potential is closely associated with the resolution of the \"hyperon puzzle\" in neutron stars.","sentences":["Using the Li\\`{e}ge intranuclear-cascade model together with the ablation model ABLA, an investigation is conducted into the effects of $\\Lambda$ potential in $\\Lambda$-nucleus and $\\Lambda$-hypernucleus-nucleus collisions across various beam energies.","The findings show that the angle and transverse-momentum distributions of scattered $\\Lambda$ hyperon, the scattering cross section of the $\\Lambda$ hyperon in $\\Lambda$-nucleus collisions as well as the rapidity distribution of $\\Lambda$ hyperon in $\\Lambda$-hypernucleus-nucleus collisions are significantly influenced by the strength of the $\\Lambda$ potential in these scattering reactions across various beam energies.","These demonstrations, unhindered by the uncertainties of $\\Lambda$ and hypernuclei productions in nuclear medium, allow for a direct investigation of the $\\Lambda$ potential, especially its momentum dependence.","The criticality of probing the $\\Lambda$ potential is closely associated with the resolution of the \"hyperon puzzle\" in neutron stars."],"url":"http://arxiv.org/abs/2402.18831v1","category":"nucl-th"}
{"created":"2024-02-29 03:06:10","title":"Dual Operating Modes of In-Context Learning","abstract":"In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill. Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time. We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously. Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions. We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples. Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution. With the closed-form expression, we obtain a quantitative understanding of the two operating modes of ICL. Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the ICL risk initially increases and then decreases with more in-context examples. Our model offers a plausible explanation for this \"early ascent\" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples. We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels. Lastly, we validate our findings and predictions via experiments involving Transformers and large language models.","sentences":["In-context learning (ICL) exhibits dual operating modes: task learning, i.e., acquiring a new skill from in-context samples, and task retrieval, i.e., locating and activating a relevant pretrained skill.","Recent theoretical work investigates various mathematical models to analyze ICL, but existing models explain only one operating mode at a time.","We introduce a probabilistic model, with which one can explain the dual operating modes of ICL simultaneously.","Focusing on in-context learning of linear functions, we extend existing models for pretraining data by introducing multiple task groups and task-dependent input distributions.","We then analyze the behavior of the optimally pretrained model under the squared loss, i.e., the MMSE estimator of the label given in-context examples.","Regarding pretraining task distribution as prior and in-context examples as the observation, we derive the closed-form expression of the task posterior distribution.","With the closed-form expression, we obtain a quantitative understanding of the two operating modes of ICL.","Furthermore, we shed light on an unexplained phenomenon observed in practice: under certain settings, the ICL risk initially increases and then decreases with more in-context examples.","Our model offers a plausible explanation for this \"early ascent\" phenomenon: a limited number of in-context samples may lead to the retrieval of an incorrect skill, thereby increasing the risk, which will eventually diminish as task learning takes effect with more in-context samples.","We also theoretically analyze ICL with biased labels, e.g., zero-shot ICL, where in-context examples are assigned random labels.","Lastly, we validate our findings and predictions via experiments involving Transformers and large language models."],"url":"http://arxiv.org/abs/2402.18819v1","category":"cs.LG"}
{"created":"2024-02-29 02:31:42","title":"The numeraire e-variable","abstract":"We consider testing a composite null hypothesis $\\mathcal{P}$ against a point alternative $\\mathbb{Q}$. This paper establishes a powerful and general result: under no conditions whatsoever on $\\mathcal{P}$ or $\\mathbb{Q}$, we show that there exists a special e-variable $X^*$ that we call the numeraire. It is strictly positive and for every $\\mathbb{P} \\in \\mathcal{P}$, $\\mathbb{E}_\\mathbb{P}[X^*] \\le 1$ (the e-variable property), while for every other e-variable $X$, we have $\\mathbb{E}_\\mathbb{Q}[X/X^*] \\le 1$ (the numeraire property). In particular, this implies $\\mathbb{E}_\\mathbb{Q}[\\log(X/X^*)] \\le 0$ (log-optimality). $X^*$ also identifies a particular sub-probability measure $\\mathbb{P}^*$ via the density $d \\mathbb{P}^*/d \\mathbb{Q} = 1/X^*$. As a result, $X^*$ can be seen as a generalized likelihood ratio of $\\mathbb{Q}$ against $\\mathcal{P}$. We show that $\\mathbb{P}^*$ coincides with the reverse information projection (RIPr) when additional assumptions are made that are required for the latter to exist. Thus $\\mathbb{P}^*$ is a natural definition of the RIPr in the absence of any assumptions on $\\mathcal{P}$ or $\\mathbb{Q}$. In addition to the abstract theory, we provide several tools for finding the numeraire in concrete cases. We discuss several nonparametric examples where we can indeed identify the numeraire, despite not having a reference measure. We end with a more general optimality theory that goes beyond the ubiquitous logarithmic utility. We focus on certain power utilities, leading to reverse R\\'enyi projections in place of the RIPr, which also always exists.","sentences":["We consider testing a composite null hypothesis $\\mathcal{P}$ against a point alternative $\\mathbb{Q}$. This paper establishes a powerful and general result: under no conditions whatsoever on $\\mathcal{P}$ or $\\mathbb{Q}$, we show that there exists a special e-variable $X^*$ that we call the numeraire.","It is strictly positive and for every $\\mathbb{P} \\in \\mathcal{P}$, $\\mathbb{E}_\\mathbb{P}[X^*] \\le 1$ (the e-variable property), while for every other e-variable $X$, we have $\\mathbb{E}_\\mathbb{Q}[X/X^*] \\le 1$ (the numeraire property).","In particular, this implies $\\mathbb{E}_\\mathbb{Q}[\\log(X/X^*)] \\le 0$ (log-optimality).","$X^*$ also identifies a particular sub-probability measure $\\mathbb{P}^*$ via the density $d \\mathbb{P}^*/d \\mathbb{Q} = 1/X^*$. As a result, $X^*$ can be seen as a generalized likelihood ratio of $\\mathbb{Q}$ against $\\mathcal{P}$. We show that $\\mathbb{P}^*$ coincides with the reverse information projection (RIPr) when additional assumptions are made that are required for the latter to exist.","Thus $\\mathbb{P}^*$ is a natural definition of the RIPr in the absence of any assumptions on $\\mathcal{P}$ or $\\mathbb{Q}$. In addition to the abstract theory, we provide several tools for finding the numeraire in concrete cases.","We discuss several nonparametric examples where we can indeed identify the numeraire, despite not having a reference measure.","We end with a more general optimality theory that goes beyond the ubiquitous logarithmic utility.","We focus on certain power utilities, leading to reverse R\\'enyi projections in place of the RIPr, which also always exists."],"url":"http://arxiv.org/abs/2402.18810v1","category":"math.ST"}
{"created":"2024-02-29 02:30:39","title":"Entanglement-enabled advantage for learning a bosonic random displacement channel","abstract":"We show that quantum entanglement can provide an exponential advantage in learning properties of a bosonic continuous-variable (CV) system. The task we consider is estimating a probabilistic mixture of displacement operators acting on $n$ bosonic modes, called a random displacement channel. We prove that if the $n$ modes are not entangled with an ancillary quantum memory, then the channel must be sampled a number of times exponential in $n$ in order to estimate its characteristic function to reasonable precision; this lower bound on sample complexity applies even if the channel inputs and measurements performed on channel outputs are chosen adaptively. On the other hand, we present a simple entanglement-assisted scheme that only requires a number of samples independent of $n$, given a sufficient amount of squeezing. This establishes an exponential separation in sample complexity. We then analyze the effect of photon loss and show that the entanglement-assisted scheme is still significantly more efficient than any lossless entanglement-free scheme under mild experimental conditions. Our work illuminates the role of entanglement in learning continuous-variable systems and points toward experimentally feasible demonstrations of provable entanglement-enabled advantage using CV quantum platforms.","sentences":["We show that quantum entanglement can provide an exponential advantage in learning properties of a bosonic continuous-variable (CV) system.","The task we consider is estimating a probabilistic mixture of displacement operators acting on $n$ bosonic modes, called a random displacement channel.","We prove that if the $n$ modes are not entangled with an ancillary quantum memory, then the channel must be sampled a number of times exponential in $n$ in order to estimate its characteristic function to reasonable precision; this lower bound on sample complexity applies even if the channel inputs and measurements performed on channel outputs are chosen adaptively.","On the other hand, we present a simple entanglement-assisted scheme that only requires a number of samples independent of $n$, given a sufficient amount of squeezing.","This establishes an exponential separation in sample complexity.","We then analyze the effect of photon loss and show that the entanglement-assisted scheme is still significantly more efficient than any lossless entanglement-free scheme under mild experimental conditions.","Our work illuminates the role of entanglement in learning continuous-variable systems and points toward experimentally feasible demonstrations of provable entanglement-enabled advantage using CV quantum platforms."],"url":"http://arxiv.org/abs/2402.18809v1","category":"quant-ph"}
{"created":"2024-02-29 02:19:55","title":"VEC-SBM: Optimal Community Detection with Vectorial Edges Covariates","abstract":"Social networks are often associated with rich side information, such as texts and images. While numerous methods have been developed to identify communities from pairwise interactions, they usually ignore such side information. In this work, we study an extension of the Stochastic Block Model (SBM), a widely used statistical framework for community detection, that integrates vectorial edges covariates: the Vectorial Edges Covariates Stochastic Block Model (VEC-SBM). We propose a novel algorithm based on iterative refinement techniques and show that it optimally recovers the latent communities under the VEC-SBM. Furthermore, we rigorously assess the added value of leveraging edge's side information in the community detection process. We complement our theoretical results with numerical experiments on synthetic and semi-synthetic data.","sentences":["Social networks are often associated with rich side information, such as texts and images.","While numerous methods have been developed to identify communities from pairwise interactions, they usually ignore such side information.","In this work, we study an extension of the Stochastic Block Model (SBM), a widely used statistical framework for community detection, that integrates vectorial edges covariates: the Vectorial Edges Covariates Stochastic Block Model (VEC-SBM).","We propose a novel algorithm based on iterative refinement techniques and show that it optimally recovers the latent communities under the VEC-SBM.","Furthermore, we rigorously assess the added value of leveraging edge's side information in the community detection process.","We complement our theoretical results with numerical experiments on synthetic and semi-synthetic data."],"url":"http://arxiv.org/abs/2402.18805v1","category":"cs.SI"}
{"created":"2024-02-29 01:33:08","title":"FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning","abstract":"Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization and graph pruning, two static compilation optimizations, to minimize the memory overhead and latency for co-serving. Compared to existing systems, FlexLLM's co-serving approach reduces the activation GPU memory overhead by up to 8x, and the end-to-end GPU memory requirement of finetuning by up to 36% while maintaining a low inference latency and improving finetuning throughput. For example, under a heavy inference workload, FlexLLM can still preserve more than 80% of the peak finetuning throughput, whereas existing systems cannot make any progress with finetuning. The source code of FlexLLM is publicly available at https://github.com/flexflow/FlexFlow.","sentences":["Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks.","Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks.","This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests.","As a result, shared GPU resources are underutilized, leading to inefficiencies.","To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration.","Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving.","To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization and graph pruning, two static compilation optimizations, to minimize the memory overhead and latency for co-serving.","Compared to existing systems, FlexLLM's co-serving approach reduces the activation GPU memory overhead by up to 8x, and the end-to-end GPU memory requirement of finetuning by up to 36% while maintaining a low inference latency and improving finetuning throughput.","For example, under a heavy inference workload, FlexLLM can still preserve more than 80% of the peak finetuning throughput, whereas existing systems cannot make any progress with finetuning.","The source code of FlexLLM is publicly available at https://github.com/flexflow/FlexFlow."],"url":"http://arxiv.org/abs/2402.18789v1","category":"cs.DC"}
{"created":"2024-02-29 01:30:08","title":"Inferring Structure of Cortical Neuronal Networks from Firing Data: A Statistical Physics Approach","abstract":"Understanding the relation between cortical neuronal network structure and neuronal activity is a fundamental unresolved question in neuroscience, with implications to our understanding of the mechanism by which neuronal networks evolve over time, spontaneously or under stimulation. It requires a method for inferring the structure and composition of a network from neuronal activities. Tracking the evolution of networks and their changing functionality will provide invaluable insight into the occurrence of plasticity and the underlying learning process. We devise a probabilistic method for inferring the effective network structure by integrating techniques from Bayesian statistics, statistical physics and principled machine learning. The method and resulting algorithm allow one to infer the effective network structure, identify the excitatory and inhibitory nature of its constituents, and predict neuronal spiking activities by employing the inferred structure. We validate the method and algorithm's performance using synthetic data, spontaneous activity of an in silico emulator and realistic in vitro neuronal networks of modular and homogeneous connectivity, demonstrating excellent structure inference and activity prediction. We also show that our method outperforms commonly used existing methods for inferring neuronal network structure. Inferring the evolving effective structure of neuronal networks will provide new insight into the learning process due to stimulation in general and will facilitate the development of neuron-based circuits with computing capabilities.","sentences":["Understanding the relation between cortical neuronal network structure and neuronal activity is a fundamental unresolved question in neuroscience, with implications to our understanding of the mechanism by which neuronal networks evolve over time, spontaneously or under stimulation.","It requires a method for inferring the structure and composition of a network from neuronal activities.","Tracking the evolution of networks and their changing functionality will provide invaluable insight into the occurrence of plasticity and the underlying learning process.","We devise a probabilistic method for inferring the effective network structure by integrating techniques from Bayesian statistics, statistical physics and principled machine learning.","The method and resulting algorithm allow one to infer the effective network structure, identify the excitatory and inhibitory nature of its constituents, and predict neuronal spiking activities by employing the inferred structure.","We validate the method and algorithm's performance using synthetic data, spontaneous activity of an in silico emulator and realistic in vitro neuronal networks of modular and homogeneous connectivity, demonstrating excellent structure inference and activity prediction.","We also show that our method outperforms commonly used existing methods for inferring neuronal network structure.","Inferring the evolving effective structure of neuronal networks will provide new insight into the learning process due to stimulation in general and will facilitate the development of neuron-based circuits with computing capabilities."],"url":"http://arxiv.org/abs/2402.18788v1","category":"physics.soc-ph"}
{"created":"2024-02-29 01:27:38","title":"Enhancing the \"Immunity\" of Mixture-of-Experts Networks for Adversarial Defense","abstract":"Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs) to adversarial examples, which can easily fool DNNs into making incorrect predictions. To mitigate this deficiency, we propose a novel adversarial defense method called \"Immunity\" (Innovative MoE with MUtual information \\& positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture in this work. The key enhancements to the standard MoE are two-fold: 1) integrating of Random Switch Gates (RSGs) to obtain diverse network structures via random permutation of RSG parameters at evaluation time, despite of RSGs being determined after one-time training; 2) devising innovative Mutual Information (MI)-based and Position Stability-based loss functions by capitalizing on Grad-CAM's explanatory power to increase the diversity and the causality of expert networks. Notably, our MI-based loss operates directly on the heatmaps, thereby inducing subtler negative impacts on the classification performance when compared to other losses of the same type, theoretically. Extensive evaluation validates the efficacy of the proposed approach in improving adversarial robustness against a wide range of attacks.","sentences":["Recent studies have revealed the vulnerability of Deep Neural Networks (DNNs) to adversarial examples, which can easily fool DNNs into making incorrect predictions.","To mitigate this deficiency, we propose a novel adversarial defense method called \"Immunity\" (Innovative MoE with MUtual information \\& positioN stabilITY) based on a modified Mixture-of-Experts (MoE) architecture in this work.","The key enhancements to the standard MoE are two-fold: 1) integrating of Random Switch Gates (RSGs) to obtain diverse network structures via random permutation of RSG parameters at evaluation time, despite of RSGs being determined after one-time training; 2) devising innovative Mutual Information (MI)-based and Position Stability-based loss functions by capitalizing on Grad-CAM's explanatory power to increase the diversity and the causality of expert networks.","Notably, our MI-based loss operates directly on the heatmaps, thereby inducing subtler negative impacts on the classification performance when compared to other losses of the same type, theoretically.","Extensive evaluation validates the efficacy of the proposed approach in improving adversarial robustness against a wide range of attacks."],"url":"http://arxiv.org/abs/2402.18787v1","category":"cs.LG"}
{"created":"2024-02-29 01:18:43","title":"On the semiclassical bounce with strong minimal assumptions","abstract":"We explore the possibility of avoiding cosmological singularity with a bounce solution in the early Universe. The main finding is that simple and well-known semiclassical correction, which describes the mixing of radiation and gravity in the effective action, may provide an analytic solution with a bounce. The solution requires a positive beta function for the total radiation term and the contraction of the Universe at the initial instant. The numerical estimate shows that the bounce may occur in an acceptable range of energies, but only under strong assumptions about the particle physics beyond the Standard Model.","sentences":["We explore the possibility of avoiding cosmological singularity with a bounce solution in the early Universe.","The main finding is that simple and well-known semiclassical correction, which describes the mixing of radiation and gravity in the effective action, may provide an analytic solution with a bounce.","The solution requires a positive beta function for the total radiation term and the contraction of the Universe at the initial instant.","The numerical estimate shows that the bounce may occur in an acceptable range of energies, but only under strong assumptions about the particle physics beyond the Standard Model."],"url":"http://arxiv.org/abs/2402.18785v1","category":"gr-qc"}
{"created":"2024-02-29 01:07:29","title":"Conjectural Online Learning with First-order Beliefs in Asymmetric Information Stochastic Games","abstract":"Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players). Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium. To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs. COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies. Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning. We prove that conjecture in COL is asymptotically consistent with the information feedback in the sense of a relaxed Bayesian consistency. The resulting empirical strategy profile converges to the Berk-Nash equilibrium, a solution concept characterizing rationality under subjectivity. Experimental results from an intrusion response use case demonstrate COL's superiority over state-of-the-art reinforcement learning methods against nonstationary attacks.","sentences":["Stochastic games arise in many complex socio-technical systems, such as cyber-physical systems and IT infrastructures, where information asymmetry presents challenges for decision-making entities (players).","Existing computational methods for asymmetric information stochastic games (AISG) are primarily offline, targeting special classes of AISGs to avoid belief hierarchies, and lack online adaptability to deviations from equilibrium.","To address this limitation, we propose a conjectural online learning (COL), a learning scheme for generic AISGs.","COL, structured as a forecaster-actor-critic (FAC) architecture, utilizes first-order beliefs over the hidden states and subjective forecasts of the opponent's strategies.","Against the conjectured opponent, COL updates strategies in an actor-critic approach using online rollout and calibrates conjectures through Bayesian learning.","We prove that conjecture in COL is asymptotically consistent with the information feedback in the sense of a relaxed Bayesian consistency.","The resulting empirical strategy profile converges to the Berk-Nash equilibrium, a solution concept characterizing rationality under subjectivity.","Experimental results from an intrusion response use case demonstrate COL's superiority over state-of-the-art reinforcement learning methods against nonstationary attacks."],"url":"http://arxiv.org/abs/2402.18781v1","category":"cs.GT"}
{"created":"2024-02-29 00:31:26","title":"The Situate AI Guidebook: Co-Designing a Toolkit to Support Multi-Stakeholder Early-stage Deliberations Around Public Sector AI Proposals","abstract":"Public sector agencies are rapidly deploying AI systems to augment or automate critical decisions in real-world contexts like child welfare, criminal justice, and public health. A growing body of work documents how these AI systems often fail to improve services in practice. These failures can often be traced to decisions made during the early stages of AI ideation and design, such as problem formulation. However, today, we lack systematic processes to support effective, early-stage decision-making about whether and under what conditions to move forward with a proposed AI project. To understand how to scaffold such processes in real-world settings, we worked with public sector agency leaders, AI developers, frontline workers, and community advocates across four public sector agencies and three community advocacy groups in the United States. Through an iterative co-design process, we created the Situate AI Guidebook: a structured process centered around a set of deliberation questions to scaffold conversations around (1) goals and intended use or a proposed AI system, (2) societal and legal considerations, (3) data and modeling constraints, and (4) organizational governance factors. We discuss how the guidebook's design is informed by participants' challenges, needs, and desires for improved deliberation processes. We further elaborate on implications for designing responsible AI toolkits in collaboration with public sector agency stakeholders and opportunities for future work to expand upon the guidebook. This design approach can be more broadly adopted to support the co-creation of responsible AI toolkits that scaffold key decision-making processes surrounding the use of AI in the public sector and beyond.","sentences":["Public sector agencies are rapidly deploying AI systems to augment or automate critical decisions in real-world contexts like child welfare, criminal justice, and public health.","A growing body of work documents how these AI systems often fail to improve services in practice.","These failures can often be traced to decisions made during the early stages of AI ideation and design, such as problem formulation.","However, today, we lack systematic processes to support effective, early-stage decision-making about whether and under what conditions to move forward with a proposed AI project.","To understand how to scaffold such processes in real-world settings, we worked with public sector agency leaders, AI developers, frontline workers, and community advocates across four public sector agencies and three community advocacy groups in the United States.","Through an iterative co-design process, we created the Situate AI Guidebook: a structured process centered around a set of deliberation questions to scaffold conversations around (1) goals and intended use or a proposed AI system, (2) societal and legal considerations, (3) data and modeling constraints, and (4) organizational governance factors.","We discuss how the guidebook's design is informed by participants' challenges, needs, and desires for improved deliberation processes.","We further elaborate on implications for designing responsible AI toolkits in collaboration with public sector agency stakeholders and opportunities for future work to expand upon the guidebook.","This design approach can be more broadly adopted to support the co-creation of responsible AI toolkits that scaffold key decision-making processes surrounding the use of AI in the public sector and beyond."],"url":"http://arxiv.org/abs/2402.18774v1","category":"cs.HC"}
{"created":"2024-02-29 00:25:26","title":"NARUTO: Neural Active Reconstruction from Uncertain Target Observations","abstract":"We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction. Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment. By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning. Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity. We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy. Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on benchmark datasets like Replica and MP3D.","sentences":["We present NARUTO, a neural active reconstruction system that combines a hybrid neural representation with uncertainty learning, enabling high-fidelity surface reconstruction.","Our approach leverages a multi-resolution hash-grid as the mapping backbone, chosen for its exceptional convergence speed and capacity to capture high-frequency local features.","The centerpiece of our work is the incorporation of an uncertainty learning module that dynamically quantifies reconstruction uncertainty while actively reconstructing the environment.","By harnessing learned uncertainty, we propose a novel uncertainty aggregation strategy for goal searching and efficient path planning.","Our system autonomously explores by targeting uncertain observations and reconstructs environments with remarkable completeness and fidelity.","We also demonstrate the utility of this uncertainty-aware approach by enhancing SOTA neural SLAM systems through an active ray sampling strategy.","Extensive evaluations of NARUTO in various environments, using an indoor scene simulator, confirm its superior performance and state-of-the-art status in active reconstruction, as evidenced by its impressive results on benchmark datasets like Replica and MP3D."],"url":"http://arxiv.org/abs/2402.18771v1","category":"cs.CV"}
{"created":"2024-02-29 00:19:13","title":"Advancing Generative AI for Portuguese with Open Decoder Gerv\u00e1sio PT*","abstract":"To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect. To develop this decoder, which we named Gerv\\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper. All versions of Gerv\\'asio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese.","sentences":["To advance the neural decoding of Portuguese, in this paper we present a fully open Transformer-based, instruction-tuned decoder model that sets a new state of the art in this respect.","To develop this decoder, which we named Gerv\\'asio PT*, a strong LLaMA~2 7B model was used as a starting point, and its further improvement through additional training was done over language resources that include new instruction data sets of Portuguese prepared for this purpose, which are also contributed in this paper.","All versions of Gerv\\'asio are open source and distributed for free under an open license, including for either research or commercial usage, and can be run on consumer-grade hardware, thus seeking to contribute to the advancement of research and innovation in language technology for Portuguese."],"url":"http://arxiv.org/abs/2402.18766v1","category":"cs.CL"}
{"created":"2024-02-28 23:34:51","title":"How Much Annotation is Needed to Compare Summarization Models?","abstract":"Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace. In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose. In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization. Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples. The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks. We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates according to human preference.","sentences":["Modern instruction-tuned models have become highly capable in text generation tasks such as summarization, and are expected to be released at a steady pace.","In practice one may now wish to choose confidently, but with minimal effort, the best performing summarization model when applied to a new domain or purpose.","In this work, we empirically investigate the test sample size necessary to select a preferred model in the context of news summarization.","Empirical results reveal that comparative evaluation converges quickly for both automatic and human evaluation, with clear preferences for a system emerging from under 100 examples.","The human preference data allows us to quantify how well automatic scores can reproduce preference rankings across a variety of downstream summarization tasks.","We find that, while automatic metrics are stable at smaller sample sizes, only some automatic metrics are able to moderately predict model win rates according to human preference."],"url":"http://arxiv.org/abs/2402.18756v1","category":"cs.CL"}
{"created":"2024-02-28 23:28:44","title":"Extending QGroundControl for Automated Mission Planning of UAVs","abstract":"Unmanned Aerial Vehicle (UAVs) have become very popular in the last decade due to some advantages such as strong terrain adaptation, low cost, zero casualties, and so on. One of the most interesting advances in this field is the automation of mission planning (task allocation) and real-time replanning, which are highly useful to increase the autonomy of the vehicle and reduce the operator workload. These automated mission planning and replanning systems require a Human Computer Interface (HCI) that facilitates the visualization and selection of plans that will be executed by the vehicles. In addition, most missions should be assessed before their real-life execution. This paper extends QGroundControl, an open-source simulation environment for flight control of multiple vehicles, by adding a mission designer that permits the operator to build complex missions with tasks and other scenario items; an interface for automated mission planning and replanning, which works as a test bed for different algorithms, and a Decision Support System (DSS) that helps the operator in the selection of the plan. In this work, a complete guide of these systems and some practical use cases are provided.","sentences":["Unmanned Aerial Vehicle (UAVs) have become very popular in the last decade due to some advantages such as strong terrain adaptation, low cost, zero casualties, and so on.","One of the most interesting advances in this field is the automation of mission planning (task allocation) and real-time replanning, which are highly useful to increase the autonomy of the vehicle and reduce the operator workload.","These automated mission planning and replanning systems require a Human Computer Interface (HCI) that facilitates the visualization and selection of plans that will be executed by the vehicles.","In addition, most missions should be assessed before their real-life execution.","This paper extends QGroundControl, an open-source simulation environment for flight control of multiple vehicles, by adding a mission designer that permits the operator to build complex missions with tasks and other scenario items; an interface for automated mission planning and replanning, which works as a test bed for different algorithms, and a Decision Support System (DSS) that helps the operator in the selection of the plan.","In this work, a complete guide of these systems and some practical use cases are provided."],"url":"http://arxiv.org/abs/2402.18754v1","category":"cs.RO"}
{"created":"2024-02-28 23:26:27","title":"Pre-training Differentially Private Models with Limited Public Data","abstract":"The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection. While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage. Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process.   In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement. We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy. Empirically, using only 10\\% of public data, our strategy can achieve DP accuracy of 41.5\\% on ImageNet-21k (with $\\epsilon=8$), as well as non-DP accuracy of 55.7\\% and and 60.0\\% on downstream tasks Places365 and iNaturalist-2021, respectively, on par with state-of-the-art standard pre-training and substantially outperforming existing DP pre-trained models.","sentences":["The superior performance of large foundation models relies on the use of massive amounts of high-quality data, which often contain sensitive, private and copyrighted material that requires formal protection.","While differential privacy (DP) is a prominent method to gauge the degree of security provided to the models, its application is commonly limited to the model fine-tuning stage, due to the performance degradation when applying DP during the pre-training stage.","Consequently, DP is yet not capable of protecting a substantial portion of the data used during the initial pre-training process.   ","In this work, we first provide a theoretical understanding of the efficacy of DP training by analyzing the per-iteration loss improvement.","We make a key observation that DP optimizers' performance degradation can be significantly mitigated by the use of limited public data, which leads to a novel DP continual pre-training strategy.","Empirically, using only 10\\% of public data, our strategy can achieve DP accuracy of 41.5\\% on ImageNet-21k (with $\\epsilon=8$), as well as non-DP accuracy of 55.7\\% and and 60.0\\% on downstream tasks Places365 and iNaturalist-2021, respectively, on par with state-of-the-art standard pre-training and substantially outperforming existing DP pre-trained models."],"url":"http://arxiv.org/abs/2402.18752v1","category":"cs.LG"}
{"created":"2024-02-28 23:05:27","title":"Weighted strategies to guide a multi-objective evolutionary algorithm for multi-UAV mission planning","abstract":"Management and mission planning over a swarm of unmanned aerial vehicle (UAV) remains to date as a challenging research trend in what regards to this particular type of aircrafts. These vehicles are controlled by a number of ground control station (GCS), from which they are commanded to cooperatively perform different tasks in specific geographic areas of interest. Mathematically the problem of coordinating and assigning tasks to a swarm of UAV can be modeled as a constraint satisfaction problem, whose complexity and multiple conflicting criteria has hitherto motivated the adoption of multi-objective solvers such as multi-objective evolutionary algorithm (MOEA). The encoding approach consists of different alleles representing the decision variables, whereas the fitness function checks that all constraints are fulfilled, minimizing the optimization criteria of the problem. In problems of high complexity involving several tasks, UAV and GCS, where the space of search is huge compared to the space of valid solutions, the convergence rate of the algorithm increases significantly. To overcome this issue, this work proposes a weighted random generator for the creation and mutation of new individuals. The main objective of this work is to reduce the convergence rate of the MOEA solver for multi-UAV mission planning using weighted random strategies that focus the search on potentially better regions of the solution space. Extensive experimental results over a diverse range of scenarios evince the benefits of the proposed approach, which notably improves this convergence rate with respect to a na\\\"ive MOEA approach.","sentences":["Management and mission planning over a swarm of unmanned aerial vehicle (UAV) remains to date as a challenging research trend in what regards to this particular type of aircrafts.","These vehicles are controlled by a number of ground control station (GCS), from which they are commanded to cooperatively perform different tasks in specific geographic areas of interest.","Mathematically the problem of coordinating and assigning tasks to a swarm of UAV can be modeled as a constraint satisfaction problem, whose complexity and multiple conflicting criteria has hitherto motivated the adoption of multi-objective solvers such as multi-objective evolutionary algorithm (MOEA).","The encoding approach consists of different alleles representing the decision variables, whereas the fitness function checks that all constraints are fulfilled, minimizing the optimization criteria of the problem.","In problems of high complexity involving several tasks, UAV and GCS, where the space of search is huge compared to the space of valid solutions, the convergence rate of the algorithm increases significantly.","To overcome this issue, this work proposes a weighted random generator for the creation and mutation of new individuals.","The main objective of this work is to reduce the convergence rate of the MOEA solver for multi-UAV mission planning using weighted random strategies that focus the search on potentially better regions of the solution space.","Extensive experimental results over a diverse range of scenarios evince the benefits of the proposed approach, which notably improves this convergence rate with respect to a na\\\"ive MOEA approach."],"url":"http://arxiv.org/abs/2402.18749v1","category":"cs.NE"}
{"created":"2024-02-28 23:00:57","title":"Accelerating Computer Architecture Simulation through Machine Learning","abstract":"This paper presents our approach to accelerate computer architecture simulation by leveraging machine learning techniques. Traditional computer architecture simulations are time-consuming, making it challenging to explore different design choices efficiently. Our proposed model utilizes a combination of application features and micro-architectural features to predict the performance of an application. These features are derived from simulations of a small portion of the application. We demonstrate the effectiveness of our approach by building and evaluating a machine learning model that offers significant speedup in architectural exploration. This model demonstrates the ability to predict IPC values for the testing data with a root mean square error of less than 0.1.","sentences":["This paper presents our approach to accelerate computer architecture simulation by leveraging machine learning techniques.","Traditional computer architecture simulations are time-consuming, making it challenging to explore different design choices efficiently.","Our proposed model utilizes a combination of application features and micro-architectural features to predict the performance of an application.","These features are derived from simulations of a small portion of the application.","We demonstrate the effectiveness of our approach by building and evaluating a machine learning model that offers significant speedup in architectural exploration.","This model demonstrates the ability to predict IPC values for the testing data with a root mean square error of less than 0.1."],"url":"http://arxiv.org/abs/2402.18746v1","category":"cs.AR"}
{"created":"2024-02-28 22:59:29","title":"Degree-heterogeneous Latent Class Analysis for High-dimensional Discrete Data","abstract":"The latent class model is a widely used mixture model for multivariate discrete data. Besides the existence of qualitatively heterogeneous latent classes, real data often exhibit additional quantitative heterogeneity nested within each latent class. The modern latent class analysis also faces extra challenges, including the high-dimensionality, sparsity, and heteroskedastic noise inherent in discrete data. Motivated by these phenomena, we introduce the Degree-heterogeneous Latent Class Model and propose a spectral approach to clustering and statistical inference in the challenging high-dimensional sparse data regime. We propose an easy-to-implement HeteroClustering algorithm. It uses heteroskedastic PCA with L2 normalization to remove degree effects and perform clustering in the top singular subspace of the data matrix. We establish an exponential error rate for HeteroClustering, leading to exact clustering under minimal signal-to-noise conditions. We further investigate the estimation and inference of the high-dimensional continuous item parameters in the model, which are crucial to interpreting and finding useful markers for latent classes. We provide comprehensive procedures for global testing and multiple testing of these parameters with valid error controls. The superior performance of our methods is demonstrated through extensive simulations and applications to three diverse real-world datasets from political voting records, genetic variations, and single-cell sequencing.","sentences":["The latent class model is a widely used mixture model for multivariate discrete data.","Besides the existence of qualitatively heterogeneous latent classes, real data often exhibit additional quantitative heterogeneity nested within each latent class.","The modern latent class analysis also faces extra challenges, including the high-dimensionality, sparsity, and heteroskedastic noise inherent in discrete data.","Motivated by these phenomena, we introduce the Degree-heterogeneous Latent Class Model and propose a spectral approach to clustering and statistical inference in the challenging high-dimensional sparse data regime.","We propose an easy-to-implement HeteroClustering algorithm.","It uses heteroskedastic PCA with L2 normalization to remove degree effects and perform clustering in the top singular subspace of the data matrix.","We establish an exponential error rate for HeteroClustering, leading to exact clustering under minimal signal-to-noise conditions.","We further investigate the estimation and inference of the high-dimensional continuous item parameters in the model, which are crucial to interpreting and finding useful markers for latent classes.","We provide comprehensive procedures for global testing and multiple testing of these parameters with valid error controls.","The superior performance of our methods is demonstrated through extensive simulations and applications to three diverse real-world datasets from political voting records, genetic variations, and single-cell sequencing."],"url":"http://arxiv.org/abs/2402.18745v1","category":"stat.ME"}
{"created":"2024-02-28 22:19:55","title":"A Priori Uncertainty Quantification of Reacting Turbulence Closure Models using Bayesian Neural Networks","abstract":"While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data available from direct numerical simulation (DNS) create opportunities to leverage data-driven modeling techniques. Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen. Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and out-of-distribution regimes. In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model. In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames. We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models. We also propose a method for the incorporation of out-of-distribution information in a BNN. The efficacy of the model is demonstrated by a priori evaluation on a dataset consisting of a variety of flame conditions and fuels.","sentences":["While many physics-based closure model forms have been posited for the sub-filter scale (SFS) in large eddy simulation (LES), vast amounts of data available from direct numerical simulation (DNS) create opportunities to leverage data-driven modeling techniques.","Albeit flexible, data-driven models still depend on the dataset and the functional form of the model chosen.","Increased adoption of such models requires reliable uncertainty estimates both in the data-informed and out-of-distribution regimes.","In this work, we employ Bayesian neural networks (BNNs) to capture both epistemic and aleatoric uncertainties in a reacting flow model.","In particular, we model the filtered progress variable scalar dissipation rate which plays a key role in the dynamics of turbulent premixed flames.","We demonstrate that BNN models can provide unique insights about the structure of uncertainty of the data-driven closure models.","We also propose a method for the incorporation of out-of-distribution information in a BNN.","The efficacy of the model is demonstrated by a priori evaluation on a dataset consisting of a variety of flame conditions and fuels."],"url":"http://arxiv.org/abs/2402.18729v1","category":"physics.flu-dyn"}
{"created":"2024-02-28 21:42:01","title":"Small and Large Dust Cavities in Disks around mid-M Stars in Taurus","abstract":"High-angular resolution imaging by ALMA has revealed the near-universality and diversity of substructures in protoplanetary disks. However, disks around M-type pre-main-sequence stars are still poorly sampled, despite the prevalence of M-dwarfs in the galaxy. Here we present high-resolution (~50 mas, 8 au) ALMA Band 6 observations of six disks around mid-M stars in Taurus. We detect dust continuum emission in all six disks, 12CO in five disks, and 13CO line in two disks. The size ratios between gas and dust disks range from 1.6 to 5.1. The ratio of about 5 for 2M0436 and 2M0450 indicates efficient dust radial drift. Four disks show rings and cavities and two disks are smooth. The cavity sizes occupy a wide range: 60 au for 2M0412, and ~10 au for 2M0434, 2M0436 and 2M0508. Detailed visibility modeling indicates that small cavities of 1.7 and 5.7 au may hide in the two smooth disks 2M0450 and CIDA 12. We perform radiative transfer fitting of the infrared SEDs to constrain the cavity sizes, finding that micron-sized dust grains may have smaller cavities than millimeter grains. Planet-disk interactions are the preferred explanation to produce the large 60 au cavity, while other physics could be responsible for the three ~10 au cavities under current observations and theories. Currently, disks around mid-to-late M stars in Taurus show a higher detection frequency of cavities than earlier type stars, although a more complete sample is needed to evaluate any dependence of substructure on stellar mass.","sentences":["High-angular resolution imaging by ALMA has revealed the near-universality and diversity of substructures in protoplanetary disks.","However, disks around M-type pre-main-sequence stars are still poorly sampled, despite the prevalence of M-dwarfs in the galaxy.","Here we present high-resolution (~50 mas, 8 au) ALMA Band 6 observations of six disks around mid-M stars in Taurus.","We detect dust continuum emission in all six disks, 12CO in five disks, and 13CO line in two disks.","The size ratios between gas and dust disks range from 1.6 to 5.1.","The ratio of about 5 for 2M0436 and 2M0450 indicates efficient dust radial drift.","Four disks show rings and cavities and two disks are smooth.","The cavity sizes occupy a wide range: 60 au for 2M0412, and ~10 au for 2M0434, 2M0436 and 2M0508.","Detailed visibility modeling indicates that small cavities of 1.7 and 5.7 au may hide in the two smooth disks 2M0450 and CIDA 12.","We perform radiative transfer fitting of the infrared SEDs to constrain the cavity sizes, finding that micron-sized dust grains may have smaller cavities than millimeter grains.","Planet-disk interactions are the preferred explanation to produce the large 60 au cavity, while other physics could be responsible for the three ~10 au cavities under current observations and theories.","Currently, disks around mid-to-late M stars in Taurus show a higher detection frequency of cavities than earlier type stars, although a more complete sample is needed to evaluate any dependence of substructure on stellar mass."],"url":"http://arxiv.org/abs/2402.18720v1","category":"astro-ph.EP"}
{"created":"2024-02-28 21:29:16","title":"Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks","abstract":"Backdoor attacks allow an attacker to embed a specific vulnerability in a machine learning algorithm, activated when an attacker-chosen pattern is presented, causing a specific misprediction. The need to identify backdoors in biometric scenarios has led us to propose a novel technique with different trade-offs. In this paper we propose to use model pairs on open-set classification tasks for detecting backdoors. Using a simple linear operation to project embeddings from a probe model's embedding space to a reference model's embedding space, we can compare both embeddings and compute a similarity score. We show that this score, can be an indicator for the presence of a backdoor despite models being of different architectures, having been trained independently and on different datasets. Additionally, we show that backdoors can be detected even when both models are backdoored. The source code is made available for reproducibility purposes.","sentences":["Backdoor attacks allow an attacker to embed a specific vulnerability in a machine learning algorithm, activated when an attacker-chosen pattern is presented, causing a specific misprediction.","The need to identify backdoors in biometric scenarios has led us to propose a novel technique with different trade-offs.","In this paper we propose to use model pairs on open-set classification tasks for detecting backdoors.","Using a simple linear operation to project embeddings from a probe model's embedding space to a reference model's embedding space, we can compare both embeddings and compute a similarity score.","We show that this score, can be an indicator for the presence of a backdoor despite models being of different architectures, having been trained independently and on different datasets.","Additionally, we show that backdoors can be detected even when both models are backdoored.","The source code is made available for reproducibility purposes."],"url":"http://arxiv.org/abs/2402.18718v1","category":"cs.CV"}
{"created":"2024-02-28 21:23:40","title":"A quantum algorithm for learning a graph of bounded degree","abstract":"We are presented with a graph, $G$, on $n$ vertices with $m$ edges whose edge set is unknown. Our goal is to learn the edges of $G$ with as few queries to an oracle as possible. When we submit a set $S$ of vertices to the oracle, it tells us whether or not $S$ induces at least one edge in $G$. This so-called OR-query model has been well studied, with Angluin and Chen giving an upper bound on the number of queries needed of $O(m \\log n)$ for a general graph $G$ with $m$ edges.   When we allow ourselves to make *quantum* queries (we may query subsets in superposition), then we can achieve speedups over the best possible classical algorithms. In the case where $G$ has maximum degree $d$ and is $O(1)$-colorable, Montanaro and Shao presented an algorithm that learns the edges of $G$ in at most $\\tilde{O}(d^2m^{3/4})$ quantum queries. This gives an upper bound of $\\tilde{O}(m^{3/4})$ quantum queries when $G$ is a matching or a Hamiltonian cycle, which is far away from the lower bound of $\\Omega(\\sqrt{m})$ queries given by Ambainis and Montanaro.   We improve on the work of Montanaro and Shao in the case where $G$ has bounded degree. In particular, we present a randomized algorithm that, with high probability, learns cycles and matchings in $\\tilde{O}(\\sqrt{m})$ quantum queries, matching the theoretical lower bound up to logarithmic factors.","sentences":["We are presented with a graph, $G$, on $n$ vertices with $m$ edges whose edge set is unknown.","Our goal is to learn the edges of $G$ with as few queries to an oracle as possible.","When we submit a set $S$ of vertices to the oracle, it tells us whether or not $S$ induces at least one edge in $G$. This so-called OR-query model has been well studied, with Angluin and Chen giving an upper bound on the number of queries needed of $O(m \\log n)$ for a general graph $G$ with $m$ edges.   ","When we allow ourselves to make *quantum* queries (we may query subsets in superposition), then we can achieve speedups over the best possible classical algorithms.","In the case where $G$ has maximum degree $d$ and is $O(1)$-colorable, Montanaro and Shao presented an algorithm that learns the edges of $G$ in at most $\\tilde{O}(d^2m^{3/4})$ quantum queries.","This gives an upper bound of $\\tilde{O}(m^{3/4})$ quantum queries when $G$ is a matching or a Hamiltonian cycle, which is far away from the lower bound of $\\Omega(\\sqrt{m})$ queries given by Ambainis and Montanaro.   ","We improve on the work of Montanaro and Shao in the case where $G$ has bounded degree.","In particular, we present a randomized algorithm that, with high probability, learns cycles and matchings in $\\tilde{O}(\\sqrt{m})$ quantum queries, matching the theoretical lower bound up to logarithmic factors."],"url":"http://arxiv.org/abs/2402.18714v1","category":"quant-ph"}
{"created":"2024-02-28 21:15:05","title":"Hefty: A Modular Reconfigurable Robot for Advancing Robot Manipulation in Agriculture","abstract":"This paper presents a modular, reconfigurable robot platform for robot manipulation in agriculture. While robot manipulation promises great advancements in automating challenging, complex tasks that are currently best left to humans, it is also an expensive capital investment for researchers and users because it demands significantly varying robot configurations depending on the task. Modular robots provide a way to obtain multiple configurations and reduce costs by enabling incremental acquisition of only the necessary modules. The robot we present, Hefty, is designed to be modular and reconfigurable. It is designed for both researchers and end-users as a means to improve technology transfer from research to real-world application. This paper provides a detailed design and integration process, outlining the critical design decisions that enable modularity in the mobility of the robot as well as its sensor payload, power systems, computing, and fixture mounting. We demonstrate the utility of the robot by presenting five configurations used in multiple real-world agricultural robotics applications.","sentences":["This paper presents a modular, reconfigurable robot platform for robot manipulation in agriculture.","While robot manipulation promises great advancements in automating challenging, complex tasks that are currently best left to humans, it is also an expensive capital investment for researchers and users because it demands significantly varying robot configurations depending on the task.","Modular robots provide a way to obtain multiple configurations and reduce costs by enabling incremental acquisition of only the necessary modules.","The robot we present, Hefty, is designed to be modular and reconfigurable.","It is designed for both researchers and end-users as a means to improve technology transfer from research to real-world application.","This paper provides a detailed design and integration process, outlining the critical design decisions that enable modularity in the mobility of the robot as well as its sensor payload, power systems, computing, and fixture mounting.","We demonstrate the utility of the robot by presenting five configurations used in multiple real-world agricultural robotics applications."],"url":"http://arxiv.org/abs/2402.18710v1","category":"cs.RO"}
{"created":"2024-02-28 21:14:28","title":"Nonlinear identification algorithm for online and offline study of pulmonary mechanical ventilation","abstract":"This work presents an algorithm for determining the parameters of a nonlinear dynamic model of the respiratory system in patients undergoing assisted ventilation. Using the pressure and flow signals measured at the mouth, the model's quadratic pressure-volume characteristic is fit to this data in each respiratory cycle by appropriate estimates of the model parameters. Parameter changes during ventilation can thus also be detected. The algorithm is first refined and assessed using data derived from simulated patients represented through a sigmoidal pressure-volume characteristic with hysteresis. As satisfactory results are achieved with the simulated data, the algorithm is evaluated with real data obtained from actual patients undergoing assisted ventilation. The proposed nonlinear dynamic model and associated parameter estimation algorithm yield closer fits than the static linear models computed by respiratory machines, with only a minor increase in computation. They also provide more information to the physician, such as the pressure-volume (P-V) curvature and the condition of the lung (whether normal, under-inflated, or over-inflated). This information can be used to provide safer ventilation for patients, for instance by ventilating them in the linear region of the respiratory system.","sentences":["This work presents an algorithm for determining the parameters of a nonlinear dynamic model of the respiratory system in patients undergoing assisted ventilation.","Using the pressure and flow signals measured at the mouth, the model's quadratic pressure-volume characteristic is fit to this data in each respiratory cycle by appropriate estimates of the model parameters.","Parameter changes during ventilation can thus also be detected.","The algorithm is first refined and assessed using data derived from simulated patients represented through a sigmoidal pressure-volume characteristic with hysteresis.","As satisfactory results are achieved with the simulated data, the algorithm is evaluated with real data obtained from actual patients undergoing assisted ventilation.","The proposed nonlinear dynamic model and associated parameter estimation algorithm yield closer fits than the static linear models computed by respiratory machines, with only a minor increase in computation.","They also provide more information to the physician, such as the pressure-volume (P-V) curvature and the condition of the lung (whether normal, under-inflated, or over-inflated).","This information can be used to provide safer ventilation for patients, for instance by ventilating them in the linear region of the respiratory system."],"url":"http://arxiv.org/abs/2402.18709v1","category":"eess.SY"}
{"created":"2024-02-28 20:58:20","title":"Zero-error communication, scrambling, and ergodicity","abstract":"The long term behaviour of a quantum channel under iterations (i.e. under repeated applications of itself) yields a plethora of interesting properties. These include ergodicity, mixing, eventual scrambling, becoming strictly positive, and the vanishing of its one-shot zero error capacities. We derive relations between these seemingly different properties and find novel bounds on indices which quantify the minimum number of iterations needed for the onset of some of these properties. We obtain a lower bound on the one-shot zero-error classical capacity of $n$ iterations of an ergodic channel (for any positive integer $n$) in terms of the cardinality of its peripheral spectrum. We also find upper bounds on the minimum number of iterations needed for the one-shot capacities of any channel to stabilize. We consider two classes of quantum channels, satisfying certain symmetries, for which upper bounds on the above indices are optimal, since they reduce to the corresponding indices for a stochastic matrix (for which the bounds are known to be optimal). As an auxiliary result, we obtain a trade-off relation between the one-shot zero error classical and quantum capacities of a quantum channel.","sentences":["The long term behaviour of a quantum channel under iterations (i.e. under repeated applications of itself) yields a plethora of interesting properties.","These include ergodicity, mixing, eventual scrambling, becoming strictly positive, and the vanishing of its one-shot zero error capacities.","We derive relations between these seemingly different properties and find novel bounds on indices which quantify the minimum number of iterations needed for the onset of some of these properties.","We obtain a lower bound on the one-shot zero-error classical capacity of $n$ iterations of an ergodic channel (for any positive integer $n$) in terms of the cardinality of its peripheral spectrum.","We also find upper bounds on the minimum number of iterations needed for the one-shot capacities of any channel to stabilize.","We consider two classes of quantum channels, satisfying certain symmetries, for which upper bounds on the above indices are optimal, since they reduce to the corresponding indices for a stochastic matrix (for which the bounds are known to be optimal).","As an auxiliary result, we obtain a trade-off relation between the one-shot zero error classical and quantum capacities of a quantum channel."],"url":"http://arxiv.org/abs/2402.18703v1","category":"quant-ph"}
{"created":"2024-02-28 20:24:56","title":"Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting","abstract":"A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results. However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF. Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates. When IPF fails to converge on sparse network data, we introduce a principled algorithm that guarantees IPF converges under minimal changes to the network structure. Finally, we conduct experiments with synthetic and real-world data, which demonstrate the practical value of our theoretical and algorithmic contributions.","sentences":["A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums).","Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results.","However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network?","In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF.","Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates.","When IPF fails to converge on sparse network data, we introduce a principled algorithm that guarantees IPF converges under minimal changes to the network structure.","Finally, we conduct experiments with synthetic and real-world data, which demonstrate the practical value of our theoretical and algorithmic contributions."],"url":"http://arxiv.org/abs/2402.18697v1","category":"stat.ML"}
{"created":"2024-02-28 20:09:14","title":"The VOROS: Lifting ROC curves to 3D","abstract":"The area under the ROC curve is a common measure that is often used to rank the relative performance of different binary classifiers. However, as has been also previously noted, it can be a measure that ill-captures the benefits of different classifiers when either the true class values or misclassification costs are highly unbalanced between the two classes. We introduce a third dimension to capture these costs, and lift the ROC curve to a ROC surface in a natural way. We study both this surface and introduce the VOROS, the volume over this ROC surface, as a 3D generalization of the 2D area under the ROC curve. For problems where there are only bounds on the expected costs or class imbalances, we restrict consideration to the volume of the appropriate subregion of the ROC surface. We show how the VOROS can better capture the costs of different classifiers on both a classical and a modern example dataset.","sentences":["The area under the ROC curve is a common measure that is often used to rank the relative performance of different binary classifiers.","However, as has been also previously noted, it can be a measure that ill-captures the benefits of different classifiers when either the true class values or misclassification costs are highly unbalanced between the two classes.","We introduce a third dimension to capture these costs, and lift the ROC curve to a ROC surface in a natural way.","We study both this surface and introduce the VOROS, the volume over this ROC surface, as a 3D generalization of the 2D area under the ROC curve.","For problems where there are only bounds on the expected costs or class imbalances, we restrict consideration to the volume of the appropriate subregion of the ROC surface.","We show how the VOROS can better capture the costs of different classifiers on both a classical and a modern example dataset."],"url":"http://arxiv.org/abs/2402.18689v1","category":"cs.LG"}
{"created":"2024-02-28 20:06:24","title":"Constraints on the mass of a bosonic dark matter candidate within the DD2Y-T model","abstract":"Dark matter remains a topic of ongoing controversy. It has gained attention in the theoretical description of compact objects such as neutron stars with cores of very dense matter. Various candidates have been proposed for dark matter in the scientific literature. Among them, the sexaquark has been identified as a potential bosonic particle capable of being formed in neutron star matter based on its mass characteristics. In this study, we investigate the viability of the sexaquark as a candidate for dark matter, particularly under certain density conditions. Our goal is to address the challenges associated with the formation of a bosonic particle in a highly dense medium without compromising the stability of the neutron star. To achieve this, we introduce a straightforward linear mass shift for the sexaquark within the hadronic equation of state, utilizing a relativistic density functional approach. In our investigation, it is observed that the inclusion of Sexaquark as a candidate for dark matter within the hadronic matter equation of state, although featuring a repulsive interaction with baryonic matter, softens the equation of state. We suppose that the strength of the interaction of dark matter with baryonic matter increases linearly with the baryon density. We observe that raising the effective mass of the Sexaquark, as a result of increasing its vacuum mass, causes an increased stiffening of the equation of state as compared to the case of a constant mass. We determine the lower and upper mass boundaries for this bosonic dark matter based on observational constraints for neutron stars within the DD2Y-T model when a phase transition to quark matter phase is employed.","sentences":["Dark matter remains a topic of ongoing controversy.","It has gained attention in the theoretical description of compact objects such as neutron stars with cores of very dense matter.","Various candidates have been proposed for dark matter in the scientific literature.","Among them, the sexaquark has been identified as a potential bosonic particle capable of being formed in neutron star matter based on its mass characteristics.","In this study, we investigate the viability of the sexaquark as a candidate for dark matter, particularly under certain density conditions.","Our goal is to address the challenges associated with the formation of a bosonic particle in a highly dense medium without compromising the stability of the neutron star.","To achieve this, we introduce a straightforward linear mass shift for the sexaquark within the hadronic equation of state, utilizing a relativistic density functional approach.","In our investigation, it is observed that the inclusion of Sexaquark as a candidate for dark matter within the hadronic matter equation of state, although featuring a repulsive interaction with baryonic matter, softens the equation of state.","We suppose that the strength of the interaction of dark matter with baryonic matter increases linearly with the baryon density.","We observe that raising the effective mass of the Sexaquark, as a result of increasing its vacuum mass, causes an increased stiffening of the equation of state as compared to the case of a constant mass.","We determine the lower and upper mass boundaries for this bosonic dark matter based on observational constraints for neutron stars within the DD2Y-T model when a phase transition to quark matter phase is employed."],"url":"http://arxiv.org/abs/2402.18686v1","category":"nucl-th"}
{"created":"2024-02-28 20:01:33","title":"Acoustic tactile sensing for mobile robot wheels","abstract":"Tactile sensing in mobile robots remains under-explored, mainly due to challenges related to sensor integration and the complexities of distributed sensing. In this work, we present a tactile sensing architecture for mobile robots based on wheel-mounted acoustic waveguides. Our sensor architecture enables tactile sensing along the entire circumference of a wheel with a single active component: an off-the-shelf acoustic rangefinder. We present findings showing that our sensor, mounted on the wheel of a mobile robot, is capable of discriminating between different terrains, detecting and classifying obstacles with different geometries, and performing collision detection via contact localization. We also present a comparison between our sensor and sensors traditionally used in mobile robots, and point to the potential for sensor fusion approaches that leverage the unique capabilities of our tactile sensing architecture. Our findings demonstrate that autonomous mobile robots can further leverage our sensor architecture for diverse mapping tasks requiring knowledge of terrain material, surface topology, and underlying structure.","sentences":["Tactile sensing in mobile robots remains under-explored, mainly due to challenges related to sensor integration and the complexities of distributed sensing.","In this work, we present a tactile sensing architecture for mobile robots based on wheel-mounted acoustic waveguides.","Our sensor architecture enables tactile sensing along the entire circumference of a wheel with a single active component: an off-the-shelf acoustic rangefinder.","We present findings showing that our sensor, mounted on the wheel of a mobile robot, is capable of discriminating between different terrains, detecting and classifying obstacles with different geometries, and performing collision detection via contact localization.","We also present a comparison between our sensor and sensors traditionally used in mobile robots, and point to the potential for sensor fusion approaches that leverage the unique capabilities of our tactile sensing architecture.","Our findings demonstrate that autonomous mobile robots can further leverage our sensor architecture for diverse mapping tasks requiring knowledge of terrain material, surface topology, and underlying structure."],"url":"http://arxiv.org/abs/2402.18682v1","category":"cs.RO"}
{"created":"2024-02-28 19:46:21","title":"RORA: Robust Free-Text Rationale Evaluation","abstract":"Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels. To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional V-information \\citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that RORA aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.","sentences":["Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model's decision-making.","However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge.","Existing evaluation metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the labels.","To address this problem, we propose RORA, a Robust free-text Rationale evaluation against label leakage.","RORA quantifies the new information supplied by a rationale to justify the label.","This is achieved by assessing the conditional V-information \\citep{hewitt-etal-2021-conditional} with a predictive family robust against leaky features that can be exploited by a small model.","RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage.","We also show that RORA aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales."],"url":"http://arxiv.org/abs/2402.18678v1","category":"cs.CL"}
{"created":"2024-02-28 19:28:27","title":"Simple linear attention language models balance the recall-throughput tradeoff","abstract":"Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.","sentences":["Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context.","However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption.","In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall.","By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability.","We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall.","We propose BASED a simple architecture combining linear and sliding window attention.","By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other.","We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.","Implementations of linear attention are often less efficient than optimized standard attention implementations.","To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models.","Code for this work is provided at: https://github.com/HazyResearch/based."],"url":"http://arxiv.org/abs/2402.18668v1","category":"cs.CL"}
{"created":"2024-02-28 19:21:38","title":"Linear shrinkage for optimization in high dimensions","abstract":"In large-scale, data-driven applications, parameters are often only known approximately due to noise and limited data samples. In this paper, we focus on high-dimensional optimization problems with linear constraints under uncertain conditions. To find high quality solutions for which the violation of the true constraints is limited, we develop a linear shrinkage method that blends random matrix theory and robust optimization principles. It aims to minimize the Frobenius distance between the estimated and the true parameter matrix, especially when dealing with a large and comparable number of constraints and variables. This data-driven method excels in simulations, showing superior noise resilience and more stable performance in both obtaining high quality solutions and adhering to the true constraints compared to traditional robust optimization. Our findings highlight the effectiveness of our method in improving the robustness and reliability of optimization in high-dimensional, data-driven scenarios.","sentences":["In large-scale, data-driven applications, parameters are often only known approximately due to noise and limited data samples.","In this paper, we focus on high-dimensional optimization problems with linear constraints under uncertain conditions.","To find high quality solutions for which the violation of the true constraints is limited, we develop a linear shrinkage method that blends random matrix theory and robust optimization principles.","It aims to minimize the Frobenius distance between the estimated and the true parameter matrix, especially when dealing with a large and comparable number of constraints and variables.","This data-driven method excels in simulations, showing superior noise resilience and more stable performance in both obtaining high quality solutions and adhering to the true constraints compared to traditional robust optimization.","Our findings highlight the effectiveness of our method in improving the robustness and reliability of optimization in high-dimensional, data-driven scenarios."],"url":"http://arxiv.org/abs/2402.18666v1","category":"math.OC"}
{"created":"2024-02-28 19:18:27","title":"Efficient learning of quantum states prepared with few fermionic non-Gaussian gates","abstract":"The experimental realization of increasingly complex quantum states underscores the pressing need for new methods of state learning and verification. In one such framework, quantum state tomography, the aim is to learn the full quantum state from data obtained by measurements. Without prior assumptions on the state, this task is prohibitively hard. Here, we present an efficient algorithm for learning states on $n$ fermion modes prepared by any number of Gaussian and at most $t$ non-Gaussian gates. By Jordan-Wigner mapping, this also includes $n$-qubit states prepared by nearest-neighbour matchgate circuits with at most $t$ SWAP-gates. Our algorithm is based exclusively on single-copy measurements and produces a classical representation of a state, guaranteed to be close in trace distance to the target state. The sample and time complexity of our algorithm is $\\mathrm{poly}(n,2^t)$; thus if $t=O(\\log(n))$, it is efficient. We also show that, if $t$ scales slightly more than logarithmically, any learning algorithm to solve the same task must be inefficient, under common cryptographic assumptions. We also provide an efficient property testing algorithm that, given access to copies of a state, determines whether such state is far or close to the set of states for which our learning algorithm works. Beyond tomography, our work sheds light on the structure of states prepared with few non-Gaussian gates and offers an improved upper bound on their circuit complexity.","sentences":["The experimental realization of increasingly complex quantum states underscores the pressing need for new methods of state learning and verification.","In one such framework, quantum state tomography, the aim is to learn the full quantum state from data obtained by measurements.","Without prior assumptions on the state, this task is prohibitively hard.","Here, we present an efficient algorithm for learning states on $n$ fermion modes prepared by any number of Gaussian and at most $t$ non-Gaussian gates.","By Jordan-Wigner mapping, this also includes $n$-qubit states prepared by nearest-neighbour matchgate circuits with at most $t$ SWAP-gates.","Our algorithm is based exclusively on single-copy measurements and produces a classical representation of a state, guaranteed to be close in trace distance to the target state.","The sample and time complexity of our algorithm is $\\mathrm{poly}(n,2^t)$; thus if $t=O(\\log(n))$, it is efficient.","We also show that, if $t$ scales slightly more than logarithmically, any learning algorithm to solve the same task must be inefficient, under common cryptographic assumptions.","We also provide an efficient property testing algorithm that, given access to copies of a state, determines whether such state is far or close to the set of states for which our learning algorithm works.","Beyond tomography, our work sheds light on the structure of states prepared with few non-Gaussian gates and offers an improved upper bound on their circuit complexity."],"url":"http://arxiv.org/abs/2402.18665v1","category":"quant-ph"}
{"created":"2024-02-28 19:13:45","title":"Versatile mixed methods for compressible flows","abstract":"Versatile mixed finite element methods were originally developed by Chen and Williams for isothermal incompressible flows in \"Versatile mixed methods for the incompressible Navier-Stokes equations,\" Computers & Mathematics with Applications, Volume 80, 2020. Thereafter, these methods were extended by Miller, Chen, and Williams to non-isothermal incompressible flows in \"Versatile mixed methods for non-isothermal incompressible flows,\" Computers & Mathematics with Applications, Volume 125, 2022. The main advantage of these methods lies in their flexibility. Unlike traditional mixed methods, they retain the divergence terms in the momentum and temperature equations. As a result, the favorable properties of the schemes are maintained even in the presence of non-zero divergence. This makes them an ideal candidate for an extension to compressible flows, in which the divergence does not generally vanish. In the present article, we finally construct the fully-compressible extension of the methods. In addition, we demonstrate the excellent performance of the resulting methods for weakly-compressible flows that arise near the incompressible limit, as well as more strongly-compressible flows that arise near Mach 0.5.","sentences":["Versatile mixed finite element methods were originally developed by Chen and Williams for isothermal incompressible flows in \"Versatile mixed methods for the incompressible Navier-Stokes equations,\" Computers & Mathematics with Applications, Volume 80, 2020.","Thereafter, these methods were extended by Miller, Chen, and Williams to non-isothermal incompressible flows in \"Versatile mixed methods for non-isothermal incompressible flows,\" Computers & Mathematics with Applications, Volume 125, 2022.","The main advantage of these methods lies in their flexibility.","Unlike traditional mixed methods, they retain the divergence terms in the momentum and temperature equations.","As a result, the favorable properties of the schemes are maintained even in the presence of non-zero divergence.","This makes them an ideal candidate for an extension to compressible flows, in which the divergence does not generally vanish.","In the present article, we finally construct the fully-compressible extension of the methods.","In addition, we demonstrate the excellent performance of the resulting methods for weakly-compressible flows that arise near the incompressible limit, as well as more strongly-compressible flows that arise near Mach 0.5."],"url":"http://arxiv.org/abs/2402.18660v1","category":"math.NA"}
{"created":"2024-02-29 18:41:35","title":"Global well-posedness for supercritical SQG with perturbations of radially symmetric data","abstract":"We study the global well-posedness of the supercritical dissipative surface quasi-geostrophic (SQG) equation, a key model in geophysical fluid dynamics. While local well-posedness is known, achieving global well-posedness for large initial data remains open. Motivated by enhanced decay in radial solutions, we aim to establish global well-posedness for small perturbations of potentially large radial data. Our main result shows that for small perturbations of radial data, the SQG equation admits a unique global solution.","sentences":["We study the global well-posedness of the supercritical dissipative surface quasi-geostrophic (SQG) equation, a key model in geophysical fluid dynamics.","While local well-posedness is known, achieving global well-posedness for large initial data remains open.","Motivated by enhanced decay in radial solutions, we aim to establish global well-posedness for small perturbations of potentially large radial data.","Our main result shows that for small perturbations of radial data, the SQG equation admits a unique global solution."],"url":"http://arxiv.org/abs/2402.19439v1","category":"math.AP"}
{"created":"2024-02-29 17:53:19","title":"Tunable compact on-chip superconducting switch","abstract":"We develop a compact four-port superconducting switch with a tunable operating frequency in the range of 4.8 GHz -- 7.3 GHz. Isolation between channel exceeds 20~dB over a bandwidth of several hundred megahertz, exceeding 40 dB at some frequencies. The footprint of the device is $80\\times420~\\mu$m. The tunability requires only a global flux bias without either permanent magnets or micro-electromechanical structures. As the switch is superconducting, the heat dissipation during operation is negligible. The device can operate at up to -80~dBm, which is equal to $2.5\\times 10^6$ photons at 6 GHz per microsecond. The device show a possibility to be operated as a beamsplitter with tunable splitting ratio.","sentences":["We develop a compact four-port superconducting switch with a tunable operating frequency in the range of 4.8 GHz -- 7.3 GHz.","Isolation between channel exceeds 20~dB over a bandwidth of several hundred megahertz, exceeding 40 dB at some frequencies.","The footprint of the device is $80\\times420~\\mu$m.","The tunability requires only a global flux bias without either permanent magnets or micro-electromechanical structures.","As the switch is superconducting, the heat dissipation during operation is negligible.","The device can operate at up to -80~dBm, which is equal to $2.5\\times 10^6$ photons at 6 GHz per microsecond.","The device show a possibility to be operated as a beamsplitter with tunable splitting ratio."],"url":"http://arxiv.org/abs/2402.19394v1","category":"quant-ph"}
{"created":"2024-02-29 17:45:45","title":"Loss-induced quantum information jet in an infinite temperature Hubbard chain","abstract":"Information propagation in the one-dimensional infinite temperature Hubbard model with a dissipative particle sink at the end of a semi-infinite chain is studied. In the strongly interacting limit, the two-site mutual information and the operator entanglement entropy exhibit a rich structure with two propagating information fronts and superimposed interference fringes. A classical reversible cellular automaton model quantitatively captures the transport and the slow, classical part of the correlations, but fails to describe the rapidly propagating information jet. The fast quantum jet resembles coherent free particle propagation, with the accompanying long-ranged interference fringes that are exponentially damped by short-ranged spin correlations in the many-body background.","sentences":["Information propagation in the one-dimensional infinite temperature Hubbard model with a dissipative particle sink at the end of a semi-infinite chain is studied.","In the strongly interacting limit, the two-site mutual information and the operator entanglement entropy exhibit a rich structure with two propagating information fronts and superimposed interference fringes.","A classical reversible cellular automaton model quantitatively captures the transport and the slow, classical part of the correlations, but fails to describe the rapidly propagating information jet.","The fast quantum jet resembles coherent free particle propagation, with the accompanying long-ranged interference fringes that are exponentially damped by short-ranged spin correlations in the many-body background."],"url":"http://arxiv.org/abs/2402.19390v1","category":"cond-mat.str-el"}
{"created":"2024-02-29 17:01:28","title":"Block Spectral Stresses (BSS) estimation for shock-capturing and turbulent modeling","abstract":"A new combined sub-filter scale turbulence and shock-capturing model is developed for high-order finite volume numerics, extending previous work to unstructured solvers. Block Spectral Stresses (BSS) method relies on the spectra of the velocity gradients to estimate the subfilter scale stresses, heat-flux, and pressure-work based on the resolved field. The method is able to capture shocks with numerical order up to 25 and in a shock-vortex interaction simulation is able to capture the shock and not interfere with the vortex structure. In turbulence calculations the new method is compared with Smagorinsky, dynamic Smagorinsky, and Vreman methods adapted to a block spectral code. In the simulations of homogeneous isotropic turbulence, the new model is worse than the others when on coarse meshes and better on finer ones. Instead, for supersonic and hypersonic channel flow the case is the opposite because as expected the sub-filter terms are mostly depend on the numerical order and not the mesh resolution.","sentences":["A new combined sub-filter scale turbulence and shock-capturing model is developed for high-order finite volume numerics, extending previous work to unstructured solvers.","Block Spectral Stresses (BSS) method relies on the spectra of the velocity gradients to estimate the subfilter scale stresses, heat-flux, and pressure-work based on the resolved field.","The method is able to capture shocks with numerical order up to 25 and in a shock-vortex interaction simulation is able to capture the shock and not interfere with the vortex structure.","In turbulence calculations the new method is compared with Smagorinsky, dynamic Smagorinsky, and Vreman methods adapted to a block spectral code.","In the simulations of homogeneous isotropic turbulence, the new model is worse than the others when on coarse meshes and better on finer ones.","Instead, for supersonic and hypersonic channel flow the case is the opposite because as expected the sub-filter terms are mostly depend on the numerical order and not the mesh resolution."],"url":"http://arxiv.org/abs/2402.19354v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 15:21:41","title":"Noisy intermediate-scale quantum simulation of the one-dimensional wave equation","abstract":"We design and implement quantum circuits for the simulation of the one-dimensional wave equation on the Quantinuum H1-1 quantum computer. The circuit depth of our approach scales as $O(n^{2})$ for $n$ qubits representing the solution on $2^n$ grid points, and leads to infidelities of $O(2^{-4n} t^{2})$ for simulation time $t$ assuming smooth initial conditions. By varying the qubit count we study the interplay between the algorithmic and physical gate errors to identify the optimal working point of minimum total error. Our approach to simulating the wave equation can readily be adapted to other quantum processors and serve as an application-oriented benchmark.","sentences":["We design and implement quantum circuits for the simulation of the one-dimensional wave equation on the Quantinuum H1-1 quantum computer.","The circuit depth of our approach scales as $O(n^{2})$ for $n$ qubits representing the solution on $2^n$ grid points, and leads to infidelities of $O(2^{-4n} t^{2})$ for simulation time $t$ assuming smooth initial conditions.","By varying the qubit count we study the interplay between the algorithmic and physical gate errors to identify the optimal working point of minimum total error.","Our approach to simulating the wave equation can readily be adapted to other quantum processors and serve as an application-oriented benchmark."],"url":"http://arxiv.org/abs/2402.19247v1","category":"quant-ph"}
{"created":"2024-02-29 15:05:11","title":"CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition","abstract":"Over the past decade, most methods in visual place recognition (VPR) have used neural networks to produce feature representations. These networks typically produce a global representation of a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination), which limits their robustness in challenging scenes. In this paper, we propose a robust global representation method with cross-image correlation awareness for VPR, named CricaVPR. Our method uses the self-attention mechanism to correlate multiple images within a batch. These images can be taken in the same place with different conditions or viewpoints, or even captured from different places. Therefore, our method can utilize the cross-image variations as a cue to guide the representation learning, which ensures more robust features are produced. To further facilitate the robustness, we propose a multi-scale convolution-enhanced adaptation method to adapt pre-trained visual foundation models to the VPR task, which introduces the multi-scale local information to further enhance the cross-image correlation-aware representation. Experimental results show that our method outperforms state-of-the-art methods by a large margin with significantly less training time. Our method achieves 94.5% R@1 on Pitts30k using 512-dim global features. The code is released at https://github.com/Lu-Feng/CricaVPR.","sentences":["Over the past decade, most methods in visual place recognition (VPR) have used neural networks to produce feature representations.","These networks typically produce a global representation of a place image using only this image itself and neglect the cross-image variations (e.g. viewpoint and illumination), which limits their robustness in challenging scenes.","In this paper, we propose a robust global representation method with cross-image correlation awareness for VPR, named CricaVPR.","Our method uses the self-attention mechanism to correlate multiple images within a batch.","These images can be taken in the same place with different conditions or viewpoints, or even captured from different places.","Therefore, our method can utilize the cross-image variations as a cue to guide the representation learning, which ensures more robust features are produced.","To further facilitate the robustness, we propose a multi-scale convolution-enhanced adaptation method to adapt pre-trained visual foundation models to the VPR task, which introduces the multi-scale local information to further enhance the cross-image correlation-aware representation.","Experimental results show that our method outperforms state-of-the-art methods by a large margin with significantly less training time.","Our method achieves 94.5% R@1 on Pitts30k using 512-dim global features.","The code is released at https://github.com/Lu-Feng/CricaVPR."],"url":"http://arxiv.org/abs/2402.19231v1","category":"cs.CV"}
{"created":"2024-02-29 12:58:37","title":"How small droplets form in turbulent multiphase flows","abstract":"The formation of small droplets and bubbles in turbulent flows is a crucial process in geophysics and engineering, whose underlying physical mechanism remains a puzzle. In this letter, we address this problem by means of high-resolution numerical simulations, comparing a realistic multiphase configuration with a numerical experiment in which we attenuate the presence of strong velocity gradients either across the whole mixture or in the disperse phase only. Our results show unambiguously that the formation of small droplets is governed by the internal dynamics which occur during the break-up of large drops and that the high vorticity and the extreme dissipation associated to these events are the consequence and not the cause of the breakup.","sentences":["The formation of small droplets and bubbles in turbulent flows is a crucial process in geophysics and engineering, whose underlying physical mechanism remains a puzzle.","In this letter, we address this problem by means of high-resolution numerical simulations, comparing a realistic multiphase configuration with a numerical experiment in which we attenuate the presence of strong velocity gradients either across the whole mixture or in the disperse phase only.","Our results show unambiguously that the formation of small droplets is governed by the internal dynamics which occur during the break-up of large drops and that the high vorticity and the extreme dissipation associated to these events are the consequence and not the cause of the breakup."],"url":"http://arxiv.org/abs/2402.19121v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 12:09:25","title":"VideoMAC: Video Masked Autoencoders Meet ConvNets","abstract":"Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos. Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder. In this paper, we propose a new approach termed as \\textbf{VideoMAC}, which combines video masked autoencoders with resource-friendly ConvNets. Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames. To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders. Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos. Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\\textbf{5.2\\%} / \\textbf{6.4\\%} $\\mathcal{J}\\&\\mathcal{F}$), body part propagation (+\\textbf{6.3\\%} / \\textbf{3.1\\%} mIoU), and human pose tracking (+\\textbf{10.2\\%} / \\textbf{11.1\\%} PCK@0.1).","sentences":["Recently, the advancement of self-supervised learning techniques, like masked autoencoders (MAE), has greatly influenced visual representation learning for images and videos.","Nevertheless, it is worth noting that the predominant approaches in existing masked image / video modeling rely excessively on resource-intensive vision transformers (ViTs) as the feature encoder.","In this paper, we propose a new approach termed as \\textbf{VideoMAC}, which combines video masked autoencoders with resource-friendly ConvNets.","Specifically, VideoMAC employs symmetric masking on randomly sampled pairs of video frames.","To prevent the issue of mask pattern dissipation, we utilize ConvNets which are implemented with sparse convolutional operators as encoders.","Simultaneously, we present a simple yet effective masked video modeling (MVM) approach, a dual encoder architecture comprising an online encoder and an exponential moving average target encoder, aimed to facilitate inter-frame reconstruction consistency in videos.","Additionally, we demonstrate that VideoMAC, empowering classical (ResNet) / modern (ConvNeXt) convolutional encoders to harness the benefits of MVM, outperforms ViT-based approaches on downstream tasks, including video object segmentation (+\\textbf{5.2\\%} / \\textbf{6.4\\%} $\\mathcal{J}\\&\\mathcal{F}$), body part propagation (+\\textbf{6.3\\%} / \\textbf{3.1\\%} mIoU), and human pose tracking (+\\textbf{10.2\\%} / \\textbf{11.1\\%} PCK@0.1)."],"url":"http://arxiv.org/abs/2402.19082v1","category":"cs.CV"}
{"created":"2024-02-29 10:15:58","title":"Algorithmically Expressive, Always-Terminating Model for Reversible Computation","abstract":"Concerning classical computational models able to express all the Primitive Recursive Functions (PRF), there are interesting results regarding limits on their algorithmic expressiveness or, equivalently, efficiency, namely the ability to express algorithms with minimal computational cost. By introducing the reversible programming model Forest, at our knowledge, we provide a first study of analogous properties, adapted to the context of reversible computational models that can represent all the functions in PRF. Firstly, we show that Forest extends Matos' linear reversible computational model MSRL, the very extension being a guaranteed terminating iteration that can be halted by means of logical predicates. The consequence is that Forest is PRF complete, because MSRL is. Secondly, we show that Forest is strictly algorithmically more expressive than MSRL: it can encode a reversible algorithm for the minimum between two integers in optimal time, while MSRL cannot.","sentences":["Concerning classical computational models able to express all the Primitive Recursive Functions (PRF), there are interesting results regarding limits on their algorithmic expressiveness or, equivalently, efficiency, namely the ability to express algorithms with minimal computational cost.","By introducing the reversible programming model Forest, at our knowledge, we provide a first study of analogous properties, adapted to the context of reversible computational models that can represent all the functions in PRF.","Firstly, we show that Forest extends Matos' linear reversible computational model MSRL, the very extension being a guaranteed terminating iteration that can be halted by means of logical predicates.","The consequence is that Forest is PRF complete, because MSRL is.","Secondly, we show that Forest is strictly algorithmically more expressive than MSRL: it can encode a reversible algorithm for the minimum between two integers in optimal time, while MSRL cannot."],"url":"http://arxiv.org/abs/2402.19012v1","category":"cs.PL"}
{"created":"2024-02-29 09:31:14","title":"Calculated brightness temperatures of solar structures compared with ALMA and Mets\u00e4hovi measurements","abstract":"The Atacama Large Millimeter/submillimeter Array (ALMA) allows for solar observations in the wavelength range of 0.3$-$10 mm, giving us a new view of the chromosphere. The measured brightness temperature at various frequencies can be fitted with theoretical models of density and temperature versus height. We use the available ALMA and Mets\\\"ahovi measurements of selected solar structures (quiet sun (QS), active regions (AR) devoid of sunspots, and coronal holes (CH)). The measured QS brightness temperature in the ALMA wavelength range agrees well with the predictions of the semiempirical Avrett$-$Tian$-$Landi$-$Curdt$-$W\\\"ulser (ATLCW) model, better than previous models such as the Avrett$-$Loeser (AL) or Fontenla$-$Avrett$-$Loeser model (FAL). We scaled the ATLCW model in density and temperature to fit the observations of the other structures. For ARs, the fitted models require 9%$-$13% higher electron densities and 9%$-$10% higher electron temperatures, consistent with expectations. The CH fitted models require electron densities 2%$-$40% lower than the QS level, while the predicted electron temperatures, although somewhat lower, do not deviate significantly from the QS model. Despite the limitations of the one-dimensional ATLCW model, we confirm that this model and its appropriate adaptations are sufficient for describing the basic physical properties of the solar structures.","sentences":["The Atacama Large Millimeter/submillimeter Array (ALMA) allows for solar observations in the wavelength range of 0.3$-$10 mm, giving us a new view of the chromosphere.","The measured brightness temperature at various frequencies can be fitted with theoretical models of density and temperature versus height.","We use the available ALMA and Mets\\\"ahovi measurements of selected solar structures (quiet sun (QS), active regions (AR) devoid of sunspots, and coronal holes (CH)).","The measured QS brightness temperature in the ALMA wavelength range agrees well with the predictions of the semiempirical Avrett$-$Tian$-$Landi$-$Curdt$-$W\\\"ulser (ATLCW) model, better than previous models such as the Avrett$-$Loeser (AL) or Fontenla$-$Avrett$-$Loeser model (FAL).","We scaled the ATLCW model in density and temperature to fit the observations of the other structures.","For ARs, the fitted models require 9%$-$13% higher electron densities and 9%$-$10% higher electron temperatures, consistent with expectations.","The CH fitted models require electron densities 2%$-$40% lower than the QS level, while the predicted electron temperatures, although somewhat lower, do not deviate significantly from the QS model.","Despite the limitations of the one-dimensional ATLCW model, we confirm that this model and its appropriate adaptations are sufficient for describing the basic physical properties of the solar structures."],"url":"http://arxiv.org/abs/2402.18978v1","category":"astro-ph.SR"}
{"created":"2024-02-29 08:05:33","title":"Equivalence of ADER and Lax-Wendroff in DG / FR framework for linear problems","abstract":"ADER (Arbitrary high order by DERivatives) and Lax-Wendroff (LW) schemes are two high order single stage methods for solving time dependent partial differential equations. ADER is based on solving a locally implicit equation to obtain a space-time predictor solution while LW is based on an explicit Taylor's expansion in time. We cast the corrector step of ADER Discontinuous Galerkin (DG) scheme into an equivalent quadrature free Flux Reconstruction (FR) framework and then show that the obtained ADER-FR scheme is equivalent to the LWFR scheme with D2 dissipation numerical flux for linear problems. This also implies that the two schemes have the same Fourier stability limit for time step size. The equivalence is verified by numerical experiments.","sentences":["ADER (Arbitrary high order by DERivatives) and Lax-Wendroff (LW) schemes are two high order single stage methods for solving time dependent partial differential equations.","ADER is based on solving a locally implicit equation to obtain a space-time predictor solution while LW is based on an explicit Taylor's expansion in time.","We cast the corrector step of ADER Discontinuous Galerkin (DG) scheme into an equivalent quadrature free Flux Reconstruction (FR) framework and then show that the obtained ADER-FR scheme is equivalent to the LWFR scheme with D2 dissipation numerical flux for linear problems.","This also implies that the two schemes have the same Fourier stability limit for time step size.","The equivalence is verified by numerical experiments."],"url":"http://arxiv.org/abs/2402.18937v1","category":"math.NA"}
{"created":"2024-02-29 07:45:02","title":"Variable-Rate Learned Image Compression with Multi-Objective Optimization and Quantization-Reconstruction Offsets","abstract":"Achieving successful variable bitrate compression with computationally simple algorithms from a single end-to-end learned image or video compression model remains a challenge. Many approaches have been proposed, including conditional auto-encoders, channel-adaptive gains for the latent tensor or uniformly quantizing all elements of the latent tensor. This paper follows the traditional approach to vary a single quantization step size to perform uniform quantization of all latent tensor elements. However, three modifications are proposed to improve the variable rate compression performance. First, multi objective optimization is used for (post) training. Second, a quantization-reconstruction offset is introduced into the quantization operation. Third, variable rate quantization is used also for the hyper latent. All these modifications can be made on a pre-trained single-rate compression model by performing post training. The algorithms are implemented into three well-known image compression models and the achieved variable rate compression results indicate negligible or minimal compression performance loss compared to training multiple models. (Codes will be shared at https://github.com/InterDigitalInc/CompressAI)","sentences":["Achieving successful variable bitrate compression with computationally simple algorithms from a single end-to-end learned image or video compression model remains a challenge.","Many approaches have been proposed, including conditional auto-encoders, channel-adaptive gains for the latent tensor or uniformly quantizing all elements of the latent tensor.","This paper follows the traditional approach to vary a single quantization step size to perform uniform quantization of all latent tensor elements.","However, three modifications are proposed to improve the variable rate compression performance.","First, multi objective optimization is used for (post) training.","Second, a quantization-reconstruction offset is introduced into the quantization operation.","Third, variable rate quantization is used also for the hyper latent.","All these modifications can be made on a pre-trained single-rate compression model by performing post training.","The algorithms are implemented into three well-known image compression models and the achieved variable rate compression results indicate negligible or minimal compression performance loss compared to training multiple models.","(Codes will be shared at https://github.com/InterDigitalInc/CompressAI)"],"url":"http://arxiv.org/abs/2402.18930v1","category":"eess.IV"}
{"created":"2024-02-29 05:25:04","title":"Towards Backward-Compatible Continual Learning of Image Compression","abstract":"This paper explores the possibility of extending the capability of pre-trained neural image compressors (e.g., adapting to new data or target bitrates) without breaking backward compatibility, the ability to decode bitstreams encoded by the original model. We refer to this problem as continual learning of image compression. Our initial findings show that baseline solutions, such as end-to-end fine-tuning, do not preserve the desired backward compatibility. To tackle this, we propose a knowledge replay training strategy that effectively addresses this issue. We also design a new model architecture that enables more effective continual learning than existing baselines. Experiments are conducted for two scenarios: data-incremental learning and rate-incremental learning. The main conclusion of this paper is that neural image compressors can be fine-tuned to achieve better performance (compared to their pre-trained version) on new data and rates without compromising backward compatibility. Our code is available at https://gitlab.com/viper-purdue/continual-compression","sentences":["This paper explores the possibility of extending the capability of pre-trained neural image compressors (e.g., adapting to new data or target bitrates) without breaking backward compatibility, the ability to decode bitstreams encoded by the original model.","We refer to this problem as continual learning of image compression.","Our initial findings show that baseline solutions, such as end-to-end fine-tuning, do not preserve the desired backward compatibility.","To tackle this, we propose a knowledge replay training strategy that effectively addresses this issue.","We also design a new model architecture that enables more effective continual learning than existing baselines.","Experiments are conducted for two scenarios: data-incremental learning and rate-incremental learning.","The main conclusion of this paper is that neural image compressors can be fine-tuned to achieve better performance (compared to their pre-trained version) on new data and rates without compromising backward compatibility.","Our code is available at https://gitlab.com/viper-purdue/continual-compression"],"url":"http://arxiv.org/abs/2402.18862v1","category":"eess.IV"}
{"created":"2024-02-29 04:09:50","title":"Exploring rare-earth Kitaev magnets by massive-scale computational analysis","abstract":"The Kitaev honeycomb model plays a pivotal role in the quest for quantum spin liquids, in which fractional quasiparticles would provide applications in decoherence-free topological quantum computing. The key ingredient is the bond-dependent Ising-type interactions, dubbed the Kitaev interactions, which require strong entanglement between spin and orbital degrees of freedom. This study investigates the identification and design of rare-earth materials displaying robust Kitaev interactions. We scrutinize all possible $4f$ electron configurations, which require up to $6+$ million intermediate states in the perturbation processes, by developing a parallel computational program designed for massive scale calculations. Our analysis reveals a predominant interplay between the isotropic Heisenberg $J$ and anisotropic Kitaev $K$ interactions across all realizations of the Kramers doublets. Remarkably, instances featuring $4f^3$ and $4f^{11}$ configurations showcase the prevalence of $K$ over $J$, presenting unexpected prospects for exploring the Kitaev QSLs in compounds including Nd$^{3+}$ and Er$^{3+}$, respectively. Beyond the Kitaev model, our computational program also proves adaptable to a wide range of $4f$-electron magnets.","sentences":["The Kitaev honeycomb model plays a pivotal role in the quest for quantum spin liquids, in which fractional quasiparticles would provide applications in decoherence-free topological quantum computing.","The key ingredient is the bond-dependent Ising-type interactions, dubbed the Kitaev interactions, which require strong entanglement between spin and orbital degrees of freedom.","This study investigates the identification and design of rare-earth materials displaying robust Kitaev interactions.","We scrutinize all possible $4f$ electron configurations, which require up to $6+$ million intermediate states in the perturbation processes, by developing a parallel computational program designed for massive scale calculations.","Our analysis reveals a predominant interplay between the isotropic Heisenberg $J$ and anisotropic Kitaev $K$ interactions across all realizations of the Kramers doublets.","Remarkably, instances featuring $4f^3$ and $4f^{11}$ configurations showcase the prevalence of $K$ over $J$, presenting unexpected prospects for exploring the Kitaev QSLs in compounds including Nd$^{3+}$ and Er$^{3+}$, respectively.","Beyond the Kitaev model, our computational program also proves adaptable to a wide range of $4f$-electron magnets."],"url":"http://arxiv.org/abs/2402.18837v1","category":"cond-mat.str-el"}
{"created":"2024-02-29 03:31:41","title":"Training-set-free two-stage deep learning for Spectroscopic data de-noising","abstract":"De-noising is a prominent step in the spectra post-processing procedure. Previous machine learning-based methods are fast but mostly based on supervised learning and require a training set that may be typically expensive in real experimental measurements. Unsupervised learning-based algorithms are slow and require many iterations to achieve convergence. Here, we bridge this gap by proposing a training-set-free two-stage deep learning method. We show that the fuzzy fixed input in previous methods can be improved by introducing an adaptive prior. Combined with more advanced optimization techniques, our approach can achieve five times acceleration compared to previous work. Theoretically, we study the landscape of a corresponding non-convex linear problem, and our results indicates that this problem has benign geometry for first-order algorithms to converge.","sentences":["De-noising is a prominent step in the spectra post-processing procedure.","Previous machine learning-based methods are fast but mostly based on supervised learning and require a training set that may be typically expensive in real experimental measurements.","Unsupervised learning-based algorithms are slow and require many iterations to achieve convergence.","Here, we bridge this gap by proposing a training-set-free two-stage deep learning method.","We show that the fuzzy fixed input in previous methods can be improved by introducing an adaptive prior.","Combined with more advanced optimization techniques, our approach can achieve five times acceleration compared to previous work.","Theoretically, we study the landscape of a corresponding non-convex linear problem, and our results indicates that this problem has benign geometry for first-order algorithms to converge."],"url":"http://arxiv.org/abs/2402.18830v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 02:44:42","title":"Protein Multimer Structure Prediction via Prompt Learning","abstract":"Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes. It has been empirically confirmed that the multimer structure prediction~(MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions~(PPIs). However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a \\textit{poor generalization} to the MSP task. To address this challenge, we aim to extend the PPI knowledge to multimers of different scales~(i.e., chain numbers). Specifically, we propose \\textbf{\\textsc{PromptMSP}}, a pre-training and \\textbf{Prompt} tuning framework for \\textbf{M}ultimer \\textbf{S}tructure \\textbf{P}rediction. First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively. We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales. We provide a meta-learning strategy to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers. Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models. The code, data and checkpoints are released at \\url{https://github.com/zqgao22/PromptMSP}.","sentences":["Understanding the 3D structures of protein multimers is crucial, as they play a vital role in regulating various cellular processes.","It has been empirically confirmed that the multimer structure prediction~(MSP) can be well handled in a step-wise assembly fashion using provided dimer structures and predicted protein-protein interactions~(PPIs).","However, due to the biological gap in the formation of dimers and larger multimers, directly applying PPI prediction techniques can often cause a \\textit{poor generalization} to the MSP task.","To address this challenge, we aim to extend the PPI knowledge to multimers of different scales~(i.e., chain numbers).","Specifically, we propose \\textbf{\\textsc{PromptMSP}}, a pre-training and \\textbf{Prompt} tuning framework for \\textbf{M}ultimer \\textbf{S}tructure \\textbf{P}rediction.","First, we tailor the source and target tasks for effective PPI knowledge learning and efficient inference, respectively.","We design PPI-inspired prompt learning to narrow the gaps of two task formats and generalize the PPI knowledge to multimers of different scales.","We provide a meta-learning strategy to learn a reliable initialization of the prompt model, enabling our prompting framework to effectively adapt to limited data for large-scale multimers.","Empirically, we achieve both significant accuracy (RMSD and TM-Score) and efficiency improvements compared to advanced MSP models.","The code, data and checkpoints are released at \\url{https://github.com/zqgao22/PromptMSP}."],"url":"http://arxiv.org/abs/2402.18813v1","category":"cs.CE"}
{"created":"2024-02-29 02:31:54","title":"BFRFormer: Transformer-based generator for Real-World Blind Face Restoration","abstract":"Blind face restoration is a challenging task due to the unknown and complex degradation. Although face prior-based methods and reference-based methods have recently demonstrated high-quality results, the restored images tend to contain over-smoothed results and lose identity-preserved details when the degradation is severe. It is observed that this is attributed to short-range dependencies, the intrinsic limitation of convolutional neural networks. To model long-range dependencies, we propose a Transformer-based blind face restoration method, named BFRFormer, to reconstruct images with more identity-preserved details in an end-to-end manner. In BFRFormer, to remove blocking artifacts, the wavelet discriminator and aggregated attention module are developed, and spectral normalization and balanced consistency regulation are adaptively applied to address the training instability and over-fitting problem, respectively. Extensive experiments show that our method outperforms state-of-the-art methods on a synthetic dataset and four real-world datasets. The source code, Casia-Test dataset, and pre-trained models are released at https://github.com/s8Znk/BFRFormer.","sentences":["Blind face restoration is a challenging task due to the unknown and complex degradation.","Although face prior-based methods and reference-based methods have recently demonstrated high-quality results, the restored images tend to contain over-smoothed results and lose identity-preserved details when the degradation is severe.","It is observed that this is attributed to short-range dependencies, the intrinsic limitation of convolutional neural networks.","To model long-range dependencies, we propose a Transformer-based blind face restoration method, named BFRFormer, to reconstruct images with more identity-preserved details in an end-to-end manner.","In BFRFormer, to remove blocking artifacts, the wavelet discriminator and aggregated attention module are developed, and spectral normalization and balanced consistency regulation are adaptively applied to address the training instability and over-fitting problem, respectively.","Extensive experiments show that our method outperforms state-of-the-art methods on a synthetic dataset and four real-world datasets.","The source code, Casia-Test dataset, and pre-trained models are released at https://github.com/s8Znk/BFRFormer."],"url":"http://arxiv.org/abs/2402.18811v1","category":"cs.CV"}
{"created":"2024-02-29 01:51:21","title":"An Adaptive Orthogonal Basis Method for Computing Multiple Solutions of Differential Equations with polynomial nonlinearities","abstract":"This paper presents an innovative approach, the Adaptive Orthogonal Basis Method, tailored for computing multiple solutions to differential equations characterized by polynomial nonlinearities. Departing from conventional practices of predefining candidate basis pools, our novel method adaptively computes bases, considering the equation's nature and structural characteristics of the solution. It further leverages companion matrix techniques to generate initial guesses for subsequent computations. Thus this approach not only yields numerous initial guesses for solving such equations but also adapts orthogonal basis functions to effectively address discretized nonlinear systems. Through a series of numerical experiments, this paper demonstrates the method's effectiveness and robustness. By reducing computational costs in various applications, this novel approach opens new avenues for uncovering multiple solutions to differential equations with polynomial nonlinearities.","sentences":["This paper presents an innovative approach, the Adaptive Orthogonal Basis Method, tailored for computing multiple solutions to differential equations characterized by polynomial nonlinearities.","Departing from conventional practices of predefining candidate basis pools, our novel method adaptively computes bases, considering the equation's nature and structural characteristics of the solution.","It further leverages companion matrix techniques to generate initial guesses for subsequent computations.","Thus this approach not only yields numerous initial guesses for solving such equations but also adapts orthogonal basis functions to effectively address discretized nonlinear systems.","Through a series of numerical experiments, this paper demonstrates the method's effectiveness and robustness.","By reducing computational costs in various applications, this novel approach opens new avenues for uncovering multiple solutions to differential equations with polynomial nonlinearities."],"url":"http://arxiv.org/abs/2402.18793v1","category":"math.NA"}
{"created":"2024-02-28 19:00:55","title":"Spatial Variation-Aware Read Disturbance Defenses: Experimental Analysis of Real DRAM Chips and Implications on Future Solutions","abstract":"Read disturbance in modern DRAM chips is a widespread phenomenon and is reliably used for breaking memory isolation, a fundamental building block for building robust systems. RowHammer and RowPress are two examples of read disturbance in DRAM where repeatedly accessing (hammering) or keeping active (pressing) a memory location induces bitflips in other memory locations. Unfortunately, shrinking technology node size exacerbates read disturbance in DRAM chips over generations. As a result, existing defense mechanisms suffer from significant performance and energy overheads, limited effectiveness, or prohibitively high hardware complexity.   In this paper, we tackle these shortcomings by leveraging the spatial variation in read disturbance across different memory locations in real DRAM chips. To do so, we 1) present the first rigorous real DRAM chip characterization study of spatial variation of read disturbance and 2) propose Sv\\\"ard, a new mechanism that dynamically adapts the aggressiveness of existing solutions based on the row-level read disturbance profile. Our experimental characterization on 144 real DDR4 DRAM chips representing 10 chip designs demonstrates a large variation in read disturbance vulnerability across different memory locations: in the part of memory with the worst read disturbance vulnerability, 1) up to 2x the number of bitflips can occur and 2) bitflips can occur at an order of magnitude fewer accesses, compared to the memory locations with the least vulnerability to read disturbance. Sv\\\"ard leverages this variation to reduce the overheads of five state-of-the-art read disturbance solutions, and thus significantly increases system performance.","sentences":["Read disturbance in modern DRAM chips is a widespread phenomenon and is reliably used for breaking memory isolation, a fundamental building block for building robust systems.","RowHammer and RowPress are two examples of read disturbance in DRAM where repeatedly accessing (hammering) or keeping active (pressing) a memory location induces bitflips in other memory locations.","Unfortunately, shrinking technology node size exacerbates read disturbance in DRAM chips over generations.","As a result, existing defense mechanisms suffer from significant performance and energy overheads, limited effectiveness, or prohibitively high hardware complexity.   ","In this paper, we tackle these shortcomings by leveraging the spatial variation in read disturbance across different memory locations in real DRAM chips.","To do so, we 1) present the first rigorous real DRAM chip characterization study of spatial variation of read disturbance and 2) propose Sv\\\"ard, a new mechanism that dynamically adapts the aggressiveness of existing solutions based on the row-level read disturbance profile.","Our experimental characterization on 144 real DDR4 DRAM chips representing 10 chip designs demonstrates a large variation in read disturbance vulnerability across different memory locations: in the part of memory with the worst read disturbance vulnerability, 1) up to 2x the number of bitflips can occur and 2) bitflips can occur at an order of magnitude fewer accesses, compared to the memory locations with the least vulnerability to read disturbance.","Sv\\\"ard leverages this variation to reduce the overheads of five state-of-the-art read disturbance solutions, and thus significantly increases system performance."],"url":"http://arxiv.org/abs/2402.18652v1","category":"cs.CR"}
{"created":"2024-02-28 18:19:33","title":"Boundary Treatment for Variational Quantum Simulations of Partial Differential Equations on Quantum Computers","abstract":"The paper presents a variational quantum algorithm to solve initial-boundary value problems described by second-order partial differential equations. The approach uses hybrid classical/quantum hardware that is well suited for quantum computers of the current noisy intermediate-scale quantum era. The partial differential equation is initially translated into an optimal control problem with a modular control-to-state operator (ansatz). The objective function and its derivatives required by the optimizer can efficiently be evaluated on a quantum computer by measuring an ancilla qubit, while the optimization procedure employs classical hardware. The focal aspect of the study is the treatment of boundary conditions, which is tailored to the properties of the quantum hardware using a correction technique. For this purpose, the boundary conditions and the discretized terms of the partial differential equation are decomposed into a sequence of unitary operations and subsequently compiled into quantum gates. The accuracy and gate complexity of the approach are assessed for second-order partial differential equations by classically emulating the quantum hardware. The examples include steady and unsteady diffusive transport equations for a scalar property in combination with various Dirichlet, Neumann, or Robin conditions. The results of this flexible approach display a robust behavior and a strong predictive accuracy in combination with a remarkable polylog complexity scaling in the number of qubits of the involved quantum circuits. Remaining challenges refer to adaptive ansatz strategies that speed up the optimization procedure.","sentences":["The paper presents a variational quantum algorithm to solve initial-boundary value problems described by second-order partial differential equations.","The approach uses hybrid classical/quantum hardware that is well suited for quantum computers of the current noisy intermediate-scale quantum era.","The partial differential equation is initially translated into an optimal control problem with a modular control-to-state operator (ansatz).","The objective function and its derivatives required by the optimizer can efficiently be evaluated on a quantum computer by measuring an ancilla qubit, while the optimization procedure employs classical hardware.","The focal aspect of the study is the treatment of boundary conditions, which is tailored to the properties of the quantum hardware using a correction technique.","For this purpose, the boundary conditions and the discretized terms of the partial differential equation are decomposed into a sequence of unitary operations and subsequently compiled into quantum gates.","The accuracy and gate complexity of the approach are assessed for second-order partial differential equations by classically emulating the quantum hardware.","The examples include steady and unsteady diffusive transport equations for a scalar property in combination with various Dirichlet, Neumann, or Robin conditions.","The results of this flexible approach display a robust behavior and a strong predictive accuracy in combination with a remarkable polylog complexity scaling in the number of qubits of the involved quantum circuits.","Remaining challenges refer to adaptive ansatz strategies that speed up the optimization procedure."],"url":"http://arxiv.org/abs/2402.18619v1","category":"quant-ph"}
{"created":"2024-02-28 12:06:08","title":"Investigation of Adapter for Automatic Speech Recognition in Noisy Environment","abstract":"Adapting an automatic speech recognition (ASR) system to unseen noise environments is crucial. Integrating adapters into neural networks has emerged as a potent technique for transfer learning. This study thoroughly investigates adapter-based ASR adaptation in noisy environments. We conducted experiments using the CHiME--4 dataset. The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers. The simulated data helps the system to improve its performance under real noise conditions. Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data. Multi-condition training is still useful for adapter training. Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements.","sentences":["Adapting an automatic speech recognition (ASR) system to unseen noise environments is crucial.","Integrating adapters into neural networks has emerged as a potent technique for transfer learning.","This study thoroughly investigates adapter-based ASR adaptation in noisy environments.","We conducted experiments using the CHiME--4 dataset.","The results show that inserting the adapter in the shallow layer yields superior effectiveness, and there is no significant difference between adapting solely within the shallow layer and adapting across all layers.","The simulated data helps the system to improve its performance under real noise conditions.","Nonetheless, when the amount of data is the same, the real data is more effective than the simulated data.","Multi-condition training is still useful for adapter training.","Furthermore, integrating adapters into speech enhancement-based ASR systems yields substantial improvements."],"url":"http://arxiv.org/abs/2402.18275v2","category":"cs.SD"}
{"created":"2024-02-28 02:04:03","title":"Analytic solutions for the linearized first-order magnetohydrodynamics and implications for causality and stability","abstract":"We solve the first-order relativistic magnetohydrodynamics (MHD) within the linear-mode analysis performed near an equilibrium configuration in the fluid rest frame. We find two complete sets of analytic solutions for the four and two coupled modes with seven dissipative transport coefficients. The former set has been missing in the literature for a long time. Our method provides a simple and general algorithm for the solution search on an order-by-order basis in the derivative expansion, and can be applied to general sets of hydrodynamic equations. We also find that the small-momentum expansions of the solutions break down when the momentum direction is nearly perpendicular to an equilibrium magnetic field due to the presence of another small quantity, that is, a trigonometric function representing the anisotropy. We elaborate on the angle dependence of the solutions and provide alternative series representations that work near the right angle. Finally, we discuss the issues of causality and stability based on our analytic solutions and recent developments in the literature.","sentences":["We solve the first-order relativistic magnetohydrodynamics (MHD) within the linear-mode analysis performed near an equilibrium configuration in the fluid rest frame.","We find two complete sets of analytic solutions for the four and two coupled modes with seven dissipative transport coefficients.","The former set has been missing in the literature for a long time.","Our method provides a simple and general algorithm for the solution search on an order-by-order basis in the derivative expansion, and can be applied to general sets of hydrodynamic equations.","We also find that the small-momentum expansions of the solutions break down when the momentum direction is nearly perpendicular to an equilibrium magnetic field due to the presence of another small quantity, that is, a trigonometric function representing the anisotropy.","We elaborate on the angle dependence of the solutions and provide alternative series representations that work near the right angle.","Finally, we discuss the issues of causality and stability based on our analytic solutions and recent developments in the literature."],"url":"http://arxiv.org/abs/2402.18601v1","category":"physics.plasm-ph"}
{"created":"2024-02-27 18:55:13","title":"Quantum Circuit Discovery for Fault-Tolerant Logical State Preparation with Reinforcement Learning","abstract":"One of the key aspects in the realization of large-scale fault-tolerant quantum computers is quantum error correction (QEC). The first essential step of QEC is to encode the logical state into physical qubits in a fault-tolerant manner. Recently, flag-based protocols have been introduced that use ancillary qubits to flag harmful errors. However, there is no clear recipe for finding a compact quantum circuit with flag-based protocols for fault-tolerant logical state preparation. It is even more difficult when we consider the hardware constraints, such as qubit connectivity and gate set. In this work, we propose and explore reinforcement learning (RL) to automatically discover compact and hardware-adapted quantum circuits that fault-tolerantly prepare the logical state of a QEC code. We show that RL discovers circuits with fewer gates and ancillary qubits than published results without and with hardware constraints of up to 15 physical qubits. Furthermore, RL allows for straightforward exploration of different qubit connectivities and the use of transfer learning to accelerate the discovery. More generally, our work opens the door towards the use of RL for the discovery of fault-tolerant quantum circuits for addressing tasks beyond state preparation, including magic state preparation, logical gate synthesis, or syndrome measurement.","sentences":["One of the key aspects in the realization of large-scale fault-tolerant quantum computers is quantum error correction (QEC).","The first essential step of QEC is to encode the logical state into physical qubits in a fault-tolerant manner.","Recently, flag-based protocols have been introduced that use ancillary qubits to flag harmful errors.","However, there is no clear recipe for finding a compact quantum circuit with flag-based protocols for fault-tolerant logical state preparation.","It is even more difficult when we consider the hardware constraints, such as qubit connectivity and gate set.","In this work, we propose and explore reinforcement learning (RL) to automatically discover compact and hardware-adapted quantum circuits that fault-tolerantly prepare the logical state of a QEC code.","We show that RL discovers circuits with fewer gates and ancillary qubits than published results without and with hardware constraints of up to 15 physical qubits.","Furthermore, RL allows for straightforward exploration of different qubit connectivities and the use of transfer learning to accelerate the discovery.","More generally, our work opens the door towards the use of RL for the discovery of fault-tolerant quantum circuits for addressing tasks beyond state preparation, including magic state preparation, logical gate synthesis, or syndrome measurement."],"url":"http://arxiv.org/abs/2402.17761v1","category":"quant-ph"}
{"created":"2024-02-27 18:29:07","title":"LoDIP: Low light phase retrieval with deep image prior","abstract":"Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage. However, most PR methods struggle in low dose scenario due to the presence of very high shot noise. Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging. But these depend on a time series of measurements, rendering them unsuitable for single-image applications. Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context. Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR. In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval. Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios.","sentences":["Phase retrieval (PR) is a fundamental challenge in scientific imaging, enabling nanoscale techniques like coherent diffractive imaging (CDI).","Imaging at low radiation doses becomes important in applications where samples are susceptible to radiation damage.","However, most PR methods struggle in low dose scenario due to the presence of very high shot noise.","Advancements in the optical data acquisition setup, exemplified by in-situ CDI, have shown potential for low-dose imaging.","But these depend on a time series of measurements, rendering them unsuitable for single-image applications.","Similarly, on the computational front, data-driven phase retrieval techniques are not readily adaptable to the single-image context.","Deep learning based single-image methods, such as deep image prior, have been effective for various imaging tasks but have exhibited limited success when applied to PR.","In this work, we propose LoDIP which combines the in-situ CDI setup with the power of implicit neural priors to tackle the problem of single-image low-dose phase retrieval.","Quantitative evaluations demonstrate the superior performance of LoDIP on this task as well as applicability to real experimental scenarios."],"url":"http://arxiv.org/abs/2402.17745v1","category":"physics.comp-ph"}
{"created":"2024-02-27 18:18:23","title":"reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use","abstract":"The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies. We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants.","sentences":["The escalating prevalence of cannabis use, and associated cannabis-use disorder (CUD), poses a significant public health challenge globally.","With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objective within the 2030 United Nations Agenda for Sustainable Development Goals (SDG).","In this work, we develop an online reinforcement learning (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver personalized mobile health interventions aimed at reducing cannabis use among EAs.","reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments.","Moreover, reBandit employs Empirical Bayes and optimization techniques to autonomously update its hyper-parameters online.","To evaluate the performance of our algorithm, we construct a simulation testbed using data from a prior study, and compare against commonly used algorithms in mobile health studies.","We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants."],"url":"http://arxiv.org/abs/2402.17739v1","category":"cs.AI"}
{"created":"2024-02-27 17:55:33","title":"The SMART approach to instance-optimal online learning","abstract":"We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy. We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm. Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem. We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy. We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound.","sentences":["We devise an online learning algorithm -- titled Switching via Monotone Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow-the-leader (FTL) policy and the worst case guarantee of any other input policy.","We show that the regret of the SMART policy on any input sequence is within a multiplicative factor $e/(e-1) \\approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy.","This implies a strictly stronger guarantee than typical `best-of-both-worlds' bounds as the guarantee holds for every input sequence regardless of how it is generated.","SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to the worst-case algorithm.","Our approach and results follow from an operational reduction of instance optimal online learning to competitive analysis for the ski-rental problem.","We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a $1.43$-fraction of the minimum regret achieved by FTL and the minimax-optimal policy.","We also present a modification of SMART that combines FTL with a ``small-loss\" algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound."],"url":"http://arxiv.org/abs/2402.17720v1","category":"cs.LG"}
{"created":"2024-02-27 17:36:01","title":"Adaptive quantization with mixed-precision based on low-cost proxy","abstract":"It is critical to deploy complicated neural network models on hardware with limited resources. This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules. The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques. Integer linear programming is used to fine-tune the quantization across different layers. Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters. Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models. Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices.","sentences":["It is critical to deploy complicated neural network models on hardware with limited resources.","This paper proposes a novel model quantization method, named the Low-Cost Proxy-Based Adaptive Mixed-Precision Model Quantization (LCPAQ), which contains three key modules.","The hardware-aware module is designed by considering the hardware limitations, while an adaptive mixed-precision quantization module is developed to evaluate the quantization sensitivity by using the Hessian matrix and Pareto frontier techniques.","Integer linear programming is used to fine-tune the quantization across different layers.","Then the low-cost proxy neural architecture search module efficiently explores the ideal quantization hyperparameters.","Experiments on the ImageNet demonstrate that the proposed LCPAQ achieves comparable or superior quantization accuracy to existing mixed-precision models.","Notably, LCPAQ achieves 1/200 of the search time compared with existing methods, which provides a shortcut in practical quantization use for resource-limited devices."],"url":"http://arxiv.org/abs/2402.17706v1","category":"cs.CV"}
{"created":"2024-02-27 17:26:33","title":"Real-time Low-latency Music Source Separation using Hybrid Spectrogram-TasNet","abstract":"There have been significant advances in deep learning for music demixing in recent years. However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows. In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case. Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data. These results demonstrate the potential of efficient demixing for real-time low-latency music applications.","sentences":["There have been significant advances in deep learning for music demixing in recent years.","However, there has been little attention given to how these neural networks can be adapted for real-time low-latency applications, which could be helpful for hearing aids, remixing audio streams and live shows.","In this paper, we investigate the various challenges involved in adapting current demixing models in the literature for this use case.","Subsequently, inspired by the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain Audio Separation Network HS-TasNet, which utilises the advantages of spectral and waveform domains.","For a latency of 23 ms, the HS-TasNet obtains an overall signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases to 5.55 with additional training data.","These results demonstrate the potential of efficient demixing for real-time low-latency music applications."],"url":"http://arxiv.org/abs/2402.17701v1","category":"eess.AS"}
{"created":"2024-02-27 17:23:40","title":"Gradient-based Discrete Sampling with Automatic Cyclical Scheduling","abstract":"Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities. While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information. To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions. Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions.","sentences":["Discrete distributions, particularly in high-dimensional deep models, are often highly multimodal due to inherent discontinuities.","While gradient-based discrete sampling has proven effective, it is susceptible to becoming trapped in local modes due to the gradient information.","To tackle this challenge, we propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions.","Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring ``balanced\" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning.","We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions.","Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions."],"url":"http://arxiv.org/abs/2402.17699v1","category":"cs.LG"}
{"created":"2024-02-27 17:11:39","title":"Adaptive waveform inversion for transmitted wave data","abstract":"Adaptive Waveform Inversion applied to transient transmitted wave data yields estimates of index of refraction (or wave velocity) similar to those obtained by travel time inversion, provided that the data contain a single smooth wavefront.","sentences":["Adaptive Waveform Inversion applied to transient transmitted wave data yields estimates of index of refraction (or wave velocity) similar to those obtained by travel time inversion, provided that the data contain a single smooth wavefront."],"url":"http://arxiv.org/abs/2402.17696v1","category":"math.OC"}
{"created":"2024-02-27 17:07:18","title":"Autonomous Vehicles: Evolution of Artificial Intelligence and Learning Algorithms","abstract":"The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies. Central to this evolution is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy. This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements. Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles. It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles. The study presents statistical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry. Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time. It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level. Additionally, the document discusses the variation in software package sizes across different autonomy levels","sentences":["The advent of autonomous vehicles has heralded a transformative era in transportation, reshaping the landscape of mobility through cutting-edge technologies.","Central to this evolution is the integration of Artificial Intelligence (AI) and learning algorithms, propelling vehicles into realms of unprecedented autonomy.","This paper provides a comprehensive exploration of the evolutionary trajectory of AI within autonomous vehicles, tracing the journey from foundational principles to the most recent advancements.","Commencing with a current landscape overview, the paper delves into the fundamental role of AI in shaping the autonomous decision-making capabilities of vehicles.","It elucidates the steps involved in the AI-powered development life cycle in vehicles, addressing ethical considerations and bias in AI-driven software development for autonomous vehicles.","The study presents statistical insights into the usage and types of AI/learning algorithms over the years, showcasing the evolving research landscape within the automotive industry.","Furthermore, the paper highlights the pivotal role of parameters in refining algorithms for both trucks and cars, facilitating vehicles to adapt, learn, and improve performance over time.","It concludes by outlining different levels of autonomy, elucidating the nuanced usage of AI and learning algorithms, and automating key tasks at each level.","Additionally, the document discusses the variation in software package sizes across different autonomy levels"],"url":"http://arxiv.org/abs/2402.17690v2","category":"cs.LG"}
{"created":"2024-02-27 16:36:53","title":"Multi-Agent Deep Reinforcement Learning for Distributed Satellite Routing","abstract":"This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents. Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online.","sentences":["This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL) approach for routing in Low Earth Orbit Satellite Constellations (LSatCs).","Each satellite is an independent decision-making agent with a partial knowledge of the environment, and supported by feedback received from the nearby agents.","Building on our previous work that introduced a Q-routing solution, the contribution of this paper is to extend it to a deep learning framework able to quickly adapt to the network and traffic changes, and based on two phases: (1) An offline exploration learning phase that relies on a global Deep Neural Network (DNN) to learn the optimal paths at each possible position and congestion level; (2) An online exploitation phase with local, on-board, pre-trained DNNs.","Results show that MA-DRL efficiently learns optimal routes offline that are then loaded for an efficient distributed routing online."],"url":"http://arxiv.org/abs/2402.17666v1","category":"cs.LG"}
{"created":"2024-02-27 16:24:28","title":"Confidence-Aware Multi-Field Model Calibration","abstract":"Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding. However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases. Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands. Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance. In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics. It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field. Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations.","sentences":["Accurately predicting the probabilities of user feedback, such as clicks and conversions, is critical for ad ranking and bidding.","However, there often exist unwanted mismatches between predicted probabilities and true likelihoods due to the shift of data distributions and intrinsic model biases.","Calibration aims to address this issue by post-processing model predictions, and field-aware calibration can adjust model output on different feature field values to satisfy fine-grained advertising demands.","Unfortunately, the observed samples corresponding to certain field values can be too limited to make confident calibrations, which may yield bias amplification and online disturbance.","In this paper, we propose a confidence-aware multi-field calibration method, which adaptively adjusts the calibration intensity based on the confidence levels derived from sample statistics.","It also utilizes multiple feature fields for joint model calibration with awareness of their importance to mitigate the data sparsity effect of a single field.","Extensive offline and online experiments show the superiority of our method in boosting advertising performance and reducing prediction deviations."],"url":"http://arxiv.org/abs/2402.17655v1","category":"cs.LG"}
{"created":"2024-02-27 16:16:00","title":"Recurrent chaotic clustering and slow chaos in adaptive networks","abstract":"Adaptive dynamical networks are network systems in which the structure co-evolves and interacts with the dynamical state of the nodes. We study an adaptive dynamical network in which the structure changes on a slower time scale relative to the fast dynamics of the nodes. We identify a phenomenon we refer to as recurrent adaptive chaotic clustering (RACC), in which chaos is observed on a slow time scale, while the fast time scale exhibits regular dynamics. Such slow chaos is further characterized by long (relative to the fast time scale) regimes of frequency clusters or frequency-synchronized dynamics, interrupted by fast jumps between these regimes. We also determine parameter values where the time intervals between jumps are chaotic and show that such a state is robust to changes in parameters and initial conditions.","sentences":["Adaptive dynamical networks are network systems in which the structure co-evolves and interacts with the dynamical state of the nodes.","We study an adaptive dynamical network in which the structure changes on a slower time scale relative to the fast dynamics of the nodes.","We identify a phenomenon we refer to as recurrent adaptive chaotic clustering (RACC), in which chaos is observed on a slow time scale, while the fast time scale exhibits regular dynamics.","Such slow chaos is further characterized by long (relative to the fast time scale) regimes of frequency clusters or frequency-synchronized dynamics, interrupted by fast jumps between these regimes.","We also determine parameter values where the time intervals between jumps are chaotic and show that such a state is robust to changes in parameters and initial conditions."],"url":"http://arxiv.org/abs/2402.17646v1","category":"nlin.AO"}
{"created":"2024-02-27 15:52:59","title":"CustomSketching: Sketch Concept Extraction for Sketch-based Image Synthesis and Editing","abstract":"Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images. However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details). In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture. This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level. To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts. Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction. We employ a shape loss and a regularization loss to balance fidelity and editability during optimization. Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines.","sentences":["Personalization techniques for large text-to-image (T2I) models allow users to incorporate new concepts from reference images.","However, existing methods primarily rely on textual descriptions, leading to limited control over customized images and failing to support fine-grained and local editing (e.g., shape, pose, and details).","In this paper, we identify sketches as an intuitive and versatile representation that can facilitate such control, e.g., contour lines capturing shape information and flow lines representing texture.","This motivates us to explore a novel task of sketch concept extraction: given one or more sketch-image pairs, we aim to extract a special sketch concept that bridges the correspondence between the images and sketches, thus enabling sketch-based image synthesis and editing at a fine-grained level.","To accomplish this, we introduce CustomSketching, a two-stage framework for extracting novel sketch concepts.","Considering that an object can often be depicted by a contour for general shapes and additional strokes for internal details, we introduce a dual-sketch representation to reduce the inherent ambiguity in sketch depiction.","We employ a shape loss and a regularization loss to balance fidelity and editability during optimization.","Through extensive experiments, a user study, and several applications, we show our method is effective and superior to the adapted baselines."],"url":"http://arxiv.org/abs/2402.17624v1","category":"cs.CV"}
{"created":"2024-02-27 15:43:53","title":"Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation","abstract":"Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases. To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged. Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains. Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network. We show test-time task-adaption is the key for successful CD-FSS instead. Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone. To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers. Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task.","sentences":["Few-shot segmentation performance declines substantially when facing images from a domain different than the training domain, effectively limiting real-world use cases.","To alleviate this, recently cross-domain few-shot segmentation (CD-FSS) has emerged.","Works that address this task mainly attempted to learn segmentation on a source domain in a manner that generalizes across domains.","Surprisingly, we can outperform these approaches while eliminating the training stage and removing their main segmentation network.","We show test-time task-adaption is the key for successful CD-FSS instead.","Task-adaption is achieved by appending small networks to the feature pyramid of a conventionally classification-pretrained backbone.","To avoid overfitting to the few labeled samples in supervised fine-tuning, consistency across augmented views of input images serves as guidance while learning the parameters of the attached layers.","Despite our self-restriction not to use any images other than the few labeled samples at test time, we achieve new state-of-the-art performance in CD-FSS, evidencing the need to rethink approaches for the task."],"url":"http://arxiv.org/abs/2402.17614v1","category":"cs.CV"}
{"created":"2024-02-27 15:33:26","title":"Radar Resource Management for Active Tracking Using Split-Aperture Phased Arrays","abstract":"Flexible front-end technology will become available in future multifunction radar systems to improve adaptability to the operational theatre. A potential concept to utilize this flexibility is to subdivide radar tasks spatially over the array, the so-called split-aperture phased array (SAPA) concept. As radars are generally designed for their worst-case scenario, e.g., small targets at a large range, the power-aperture budget can be excessive for targets that do not fall within that class. To increase efficiency of the time budget of the radar front-end, the SAPA concept could be applied. In this paper, the SAPA concept is explored to assign radar resources for active tracking tasks of many targets. To do so, we formulate and solve the radar resource management problem for the SAPA concept by employing the quality of service based resource allocation model (Q-RAM) framework. It will be demonstrated by a simulation example that a radar can maintain a larger numbers of active tracking tasks when using the SAPA concept compared to the case that only the full array can be used per task.","sentences":["Flexible front-end technology will become available in future multifunction radar systems to improve adaptability to the operational theatre.","A potential concept to utilize this flexibility is to subdivide radar tasks spatially over the array, the so-called split-aperture phased array (SAPA) concept.","As radars are generally designed for their worst-case scenario, e.g., small targets at a large range, the power-aperture budget can be excessive for targets that do not fall within that class.","To increase efficiency of the time budget of the radar front-end, the SAPA concept could be applied.","In this paper, the SAPA concept is explored to assign radar resources for active tracking tasks of many targets.","To do so, we formulate and solve the radar resource management problem for the SAPA concept by employing the quality of service based resource allocation model (Q-RAM) framework.","It will be demonstrated by a simulation example that a radar can maintain a larger numbers of active tracking tasks when using the SAPA concept compared to the case that only the full array can be used per task."],"url":"http://arxiv.org/abs/2402.17607v1","category":"eess.SP"}
{"created":"2024-02-27 15:02:17","title":"An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains","abstract":"3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving. Including 3D information via Lidar sensors improves accuracy greatly. However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications. There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies. Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies. We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data. We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs.","sentences":["3D Object Detectors (3D-OD) are crucial for understanding the environment in many robotic tasks, especially autonomous driving.","Including 3D information via Lidar sensors improves accuracy greatly.","However, such detectors perform poorly on domains they were not trained on, i.e. different locations, sensors, weather, etc., limiting their reliability in safety-critical applications.","There exist methods to adapt 3D-ODs to these domains; however, these methods treat 3D-ODs as a black box, neglecting underlying architectural decisions and source-domain training strategies.","Instead, we dive deep into the details of 3D-ODs, focusing our efforts on fundamental factors that influence robustness prior to domain adaptation.   ","We systematically investigate four design choices (and the interplay between them) often overlooked in 3D-OD robustness and domain adaptation: architecture, voxel encoding, data augmentations, and anchor strategies.","We assess their impact on the robustness of nine state-of-the-art 3D-ODs across six benchmarks encompassing three types of domain gaps - sensor type, weather, and location.   ","Our main findings are: (1) transformer backbones with local point features are more robust than 3D CNNs, (2) test-time anchor size adjustment is crucial for adaptation across geographical locations, significantly boosting scores without retraining, (3) source-domain augmentations allow the model to generalize to low-resolution sensors, and (4) surprisingly, robustness to bad weather is improved when training directly on more clean weather data than on training with bad weather data.","We outline our main conclusions and findings to provide practical guidance on developing more robust 3D-ODs."],"url":"http://arxiv.org/abs/2402.17562v1","category":"cs.CV"}
{"created":"2024-02-27 14:51:56","title":"Scribble Hides Class: Promoting Scribble-Based Weakly-Supervised Semantic Segmentation with Its Class Label","abstract":"Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives. Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision. However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation. In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision. Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space. To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary. Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network.","sentences":["Scribble-based weakly-supervised semantic segmentation using sparse scribble supervision is gaining traction as it reduces annotation costs when compared to fully annotated alternatives.","Existing methods primarily generate pseudo-labels by diffusing labeled pixels to unlabeled ones with local cues for supervision.","However, this diffusion process fails to exploit global semantics and class-specific cues, which are important for semantic segmentation.","In this study, we propose a class-driven scribble promotion network, which utilizes both scribble annotations and pseudo-labels informed by image-level classes and global semantics for supervision.","Directly adopting pseudo-labels might misguide the segmentation model, thus we design a localization rectification module to correct foreground representations in the feature space.","To further combine the advantages of both supervisions, we also introduce a distance entropy loss for uncertainty reduction, which adapts per-pixel confidence weights according to the reliable region determined by the scribble and pseudo-label's boundary.","Experiments on the ScribbleSup dataset with different qualities of scribble annotations outperform all the previous methods, demonstrating the superiority and robustness of our method.","The code is available at https://github.com/Zxl19990529/Class-driven-Scribble-Promotion-Network."],"url":"http://arxiv.org/abs/2402.17555v1","category":"cs.CV"}
{"created":"2024-02-27 14:34:14","title":"Adapting Learned Image Codecs to Screen Content via Adjustable Transformations","abstract":"As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications. To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow. We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts. Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters.","sentences":["As learned image codecs (LICs) become more prevalent, their low coding efficiency for out-of-distribution data becomes a bottleneck for some applications.","To improve the performance of LICs for screen content (SC) images without breaking backwards compatibility, we propose to introduce parameterized and invertible linear transformations into the coding pipeline without changing the underlying baseline codec's operation flow.","We design two neural networks to act as prefilters and postfilters in our setup to increase the coding efficiency and help with the recovery from coding artifacts.","Our end-to-end trained solution achieves up to 10% bitrate savings on SC compression compared to the baseline LICs while introducing only 1% extra parameters."],"url":"http://arxiv.org/abs/2402.17544v1","category":"eess.IV"}
{"created":"2024-02-27 14:05:05","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Point Cloud Analysis","abstract":"Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes. Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information. This paper presents an advanced sampler that achieves both high accuracy and efficiency. The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues. Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio. This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes. Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency. Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency. Code will be available at https://github.com/yhc2021/AVS-Net.","sentences":["Efficient downsampling plays a crucial role in point cloud learning, particularly for large-scale 3D scenes.","Existing downsampling methods either require a huge computational burden or sacrifice fine-grained geometric information.","This paper presents an advanced sampler that achieves both high accuracy and efficiency.","The proposed method utilizes voxel-based sampling as a foundation, but effectively addresses the challenges regarding voxel size determination and the preservation of critical geometric cues.","Specifically, we propose a Voxel Adaptation Module that adaptively adjusts voxel sizes with the reference of point-based downsampling ratio.","This ensures the sampling results exhibit a favorable distribution for comprehending various 3D objects or scenes.","Additionally, we introduce a network compatible with arbitrary voxel sizes for sampling and feature extraction while maintaining high efficiency.","Our method achieves state-of-the-art accuracy on the ShapeNetPart and ScanNet benchmarks with promising efficiency.","Code will be available at https://github.com/yhc2021/AVS-Net."],"url":"http://arxiv.org/abs/2402.17521v1","category":"cs.CV"}
{"created":"2024-02-27 13:57:16","title":"Integrated, bright, broadband parametric down-conversion source for quantum metrology and spectroscopy","abstract":"Broadband quantum light is a vital resource for quantum metrology and spectroscopy applications such as quantum optical coherence tomography or entangled two photon absorption. For entangled two photon absorption in particular, very high photon flux combined with high time-frequency entanglement is crucial for observing a signal. So far these conditions could be met by using high power lasers driving degenerate, type 0 bulk-crystal spontaneous parametric down conversion (SPDC) sources. This naturally limits the available wavelength ranges and precludes deterministic splitting of the generated output photons. In this work we demonstrate an integrated two-colour SPDC source utilising a group-velocity matched lithium niobate waveguide, reaching both exceptional brightness $1.52\\cdot10^6\\frac{\\mathrm{pairs}}{\\mathrm{s\\,mW\\,GHz}}$ and large bandwidth ($7.8\\,$THz FWHM) while pumped with a few mW of continuous wave (CW) laser light. By converting a narrow band pump to broadband pulses the created photon pairs show correlation times of $\\Delta \\tau \\approx 120\\,\\text{fs}$ while maintaining the narrow bandwidth $\\Delta \\omega_p \\ll 1\\,\\text{MHz}$ of the CW pump light, yielding strong time-frequency entanglement. Furthermore our process can be adapted to a wide range of central wavelengths.","sentences":["Broadband quantum light is a vital resource for quantum metrology and spectroscopy applications such as quantum optical coherence tomography or entangled two photon absorption.","For entangled two photon absorption in particular, very high photon flux combined with high time-frequency entanglement is crucial for observing a signal.","So far these conditions could be met by using high power lasers driving degenerate, type 0 bulk-crystal spontaneous parametric down conversion (SPDC) sources.","This naturally limits the available wavelength ranges and precludes deterministic splitting of the generated output photons.","In this work we demonstrate an integrated two-colour SPDC source utilising a group-velocity matched lithium niobate waveguide, reaching both exceptional brightness $1.52\\cdot10^6\\frac{\\mathrm{pairs}}{\\mathrm{s\\,mW\\,GHz}}$ and large bandwidth ($7.8\\,$THz FWHM) while pumped with a few mW of continuous wave (CW) laser light.","By converting a narrow band pump to broadband pulses the created photon pairs show correlation times of $\\Delta \\tau \\approx 120\\,\\text{fs}$ while maintaining the narrow bandwidth $\\Delta \\omega_p \\ll 1\\,\\text{MHz}$ of the CW pump light, yielding strong time-frequency entanglement.","Furthermore our process can be adapted to a wide range of central wavelengths."],"url":"http://arxiv.org/abs/2402.17515v1","category":"quant-ph"}
{"created":"2024-02-27 13:55:17","title":"Robust Unsupervised Crowd Counting and Localization with Adaptive Resolution SAM","abstract":"The existing crowd counting models require extensive training data, which is time-consuming to annotate. To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models. However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas. To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes. Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks. Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks. Finally, we propose an iterative method for generating pseudo-labels. This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage. Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods. This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available.","sentences":["The existing crowd counting models require extensive training data, which is time-consuming to annotate.","To tackle this issue, we propose a simple yet effective crowd counting method by utilizing the Segment-Everything-Everywhere Model (SEEM), an adaptation of the Segmentation Anything Model (SAM), to generate pseudo-labels for training crowd counting models.","However, our initial investigation reveals that SEEM's performance in dense crowd scenes is limited, primarily due to the omission of many persons in high-density areas.","To overcome this limitation, we propose an adaptive resolution SEEM to handle the scale variations, occlusions, and overlapping of people within crowd scenes.","Alongside this, we introduce a robust localization method, based on Gaussian Mixture Models, for predicting the head positions in the predicted people masks.","Given the mask and point pseudo-labels, we propose a robust loss function, which is designed to exclude uncertain regions based on SEEM's predictions, thereby enhancing the training process of the counting networks.","Finally, we propose an iterative method for generating pseudo-labels.","This method aims at improving the quality of the segmentation masks by identifying more tiny persons in high-density regions, which are often missed in the first pseudo-labeling stage.","Overall, our proposed method achieves the best unsupervised performance in crowd counting, while also being comparable results to some supervised methods.","This makes it a highly effective and versatile tool for crowd counting, especially in situations where labeled data is not available."],"url":"http://arxiv.org/abs/2402.17514v1","category":"cs.CV"}
{"created":"2024-02-27 13:41:32","title":"FedLPPA: Learning Personalized Prompt and Aggregation for Federated Weakly-supervised Medical Image Segmentation","abstract":"Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training. However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts. In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc. A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated. In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation. In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity. Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form. Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis. Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training. Our code and data will be available.","sentences":["Federated learning (FL) effectively mitigates the data silo challenge brought about by policies and privacy concerns, implicitly harnessing more data for deep model training.","However, traditional centralized FL models grapple with diverse multi-center data, especially in the face of significant data heterogeneity, notably in medical contexts.","In the realm of medical image segmentation, the growing imperative to curtail annotation costs has amplified the importance of weakly-supervised techniques which utilize sparse annotations such as points, scribbles, etc.","A pragmatic FL paradigm shall accommodate diverse annotation formats across different sites, which research topic remains under-investigated.","In such context, we propose a novel personalized FL framework with learnable prompt and aggregation (FedLPPA) to uniformly leverage heterogeneous weak supervision for medical image segmentation.","In FedLPPA, a learnable universal knowledge prompt is maintained, complemented by multiple learnable personalized data distribution prompts and prompts representing the supervision sparsity.","Integrated with sample features through a dual-attention mechanism, those prompts empower each local task decoder to adeptly adjust to both the local distribution and the supervision form.","Concurrently, a dual-decoder strategy, predicated on prompt similarity, is introduced for enhancing the generation of pseudo-labels in weakly-supervised learning, alleviating overfitting and noise accumulation inherent to local data, while an adaptable aggregation method is employed to customize the task decoder on a parameter-wise basis.","Extensive experiments on three distinct medical image segmentation tasks involving different modalities underscore the superiority of FedLPPA, with its efficacy closely parallels that of fully supervised centralized training.","Our code and data will be available."],"url":"http://arxiv.org/abs/2402.17502v1","category":"cs.CV"}
{"created":"2024-02-27 13:34:08","title":"Predicting Instability in Complex Oscillator Networks: Limitations and Potentials of Network Measures and Machine Learning","abstract":"A central question of network science is how functional properties of systems arise from their structure. For networked dynamical systems, structure is typically quantified with network measures. A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture. Here we collect 46 relevant network measures and find that no small subset can reliably predict stability. The performance of GNNs can only be matched by combining all network measures and nodewise machine learning. However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies. This suggests that correlations of network measures and function may be misleading, and that GNNs capture the causal relationship between structure and stability substantially better.","sentences":["A central question of network science is how functional properties of systems arise from their structure.","For networked dynamical systems, structure is typically quantified with network measures.","A functional property that is of theoretical and practical interest for oscillatory systems is the stability of synchrony to localized perturbations.","Recently, Graph Neural Networks (GNNs) have been shown to predict this stability successfully; at the same time, network measures have struggled to paint a clear picture.","Here we collect 46 relevant network measures and find that no small subset can reliably predict stability.","The performance of GNNs can only be matched by combining all network measures and nodewise machine learning.","However, unlike GNNs, this approach fails to extrapolate from network ensembles to several real power grid topologies.","This suggests that correlations of network measures and function may be misleading, and that GNNs capture the causal relationship between structure and stability substantially better."],"url":"http://arxiv.org/abs/2402.17500v1","category":"nlin.AO"}
{"created":"2024-02-27 13:22:51","title":"REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering","abstract":"Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs). Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents). To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA). As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems. Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents. Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training. By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents. Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches. Our code and data can be accessed at https://github.com/RUCAIBox/REAR.","sentences":["Considering the limited internal parametric knowledge, retrieval-augmented generation (RAG) has been widely used to extend the knowledge scope of large language models (LLMs).","Despite the extensive efforts on RAG research, in existing methods, LLMs cannot precisely assess the relevance of retrieved documents, thus likely leading to misleading or even incorrect utilization of external knowledge (i.e., retrieved documents).","To address this issue, in this paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for open-domain question answering (QA).","As the key motivation, we aim to enhance the self-awareness of source relevance for LLMs, so as to adaptively utilize external knowledge in RAG systems.","Specially, we develop a new architecture for LLM based RAG system, by incorporating a specially designed rank head that precisely assesses the relevance of retrieved documents.","Furthermore, we propose an improved training method based on bi-granularity relevance fusion and noise-resistant training.","By combining the improvements in both architecture and training, our proposed REAR can better utilize external knowledge by effectively perceiving the relevance of retrieved documents.","Experiments on four open-domain QA tasks show that REAR significantly outperforms previous a number of competitive RAG approaches.","Our code and data can be accessed at https://github.com/RUCAIBox/REAR."],"url":"http://arxiv.org/abs/2402.17497v1","category":"cs.CL"}
{"created":"2024-02-27 13:18:00","title":"Prescribing Large Language Models for Perioperative Care: What's The Right Dose for Pre-trained Models?","abstract":"Postoperative risk predictions can inform effective perioperative care management and planning. We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies. The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days. For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia. Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning. Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC. Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care.","sentences":["Postoperative risk predictions can inform effective perioperative care management and planning.","We aimed to assess whether clinical large language models (LLMs) can predict postoperative risks using clinical texts with various training strategies.","The main cohort involved 84,875 records from Barnes Jewish Hospital (BJH) system between 2018 and 2021.","Methods were replicated on Beth Israel Deaconess's MIMIC dataset.","Both studies had mean duration of follow-up based on the length of postoperative ICU stay less than 7 days.","For the BJH dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and pneumonia.","Three domain adaptation and finetuning strategies were implemented for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives; incorporating labels with semi-supervised fine-tuning; and foundational modelling through multi-task learning.","Model performance was compared using the area under the receiver operating characteristic curve (AUROC) and the area under the precision recall curve (AUPRC) for classification tasks, and mean squared error (MSE) and R2 for regression tasks.","Pre-trained LLMs outperformed traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and 14% for AUPRC.","Adapting models further improved performance: (1) self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2) semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and 2.6% for AUPRC, compared to self-supervised finetuning.","Pre-trained clinical LLMs offer opportunities for postoperative risk predictions in unforeseen data, with peaks in foundational models indicating the potential of task-agnostic learning towards the generalizability of LLMs in perioperative care."],"url":"http://arxiv.org/abs/2402.17493v2","category":"cs.CL"}
{"created":"2024-02-27 13:08:26","title":"Numerical Schemes for 3-Wave Kinetic Equations: A Complete Treatment of the Collision Operator","abstract":"In our previous work, numerical schemes for a simplified version of 3-wave kinetic equations, in which only the simple forward-cascade terms of the collision operators are kept, have been successfully designed, especially to capture the long time dynamics of the equation given the multiple blow-up time phenomenon. In this second work in the series, we propose numerical treatments for the complete 3-wave kinetic equations, in which the complete, much more complicated collision operators are fully considered based on a novel conservative form of the equation. We then derive an implicit finite volume scheme to solve the equation. The new discretization uses an adaptive time-stepping method which allows for the simulations to be carried to very long times. Our computed solutions are compared with previously derived long-time asymptotic estimates for the decay rate of total energy of time-dependent solutions of 3-wave kinetic equations and found to be in excellent agreement.","sentences":["In our previous work, numerical schemes for a simplified version of 3-wave kinetic equations, in which only the simple forward-cascade terms of the collision operators are kept, have been successfully designed, especially to capture the long time dynamics of the equation given the multiple blow-up time phenomenon.","In this second work in the series, we propose numerical treatments for the complete 3-wave kinetic equations, in which the complete, much more complicated collision operators are fully considered based on a novel conservative form of the equation.","We then derive an implicit finite volume scheme to solve the equation.","The new discretization uses an adaptive time-stepping method which allows for the simulations to be carried to very long times.","Our computed solutions are compared with previously derived long-time asymptotic estimates for the decay rate of total energy of time-dependent solutions of 3-wave kinetic equations and found to be in excellent agreement."],"url":"http://arxiv.org/abs/2402.17481v1","category":"math.NA"}
{"created":"2024-02-27 12:52:44","title":"Bit Distribution Study and Implementation of Spatial Quality Map in the JPEG-AI Standardization","abstract":"Currently, there is a high demand for neural network-based image compression codecs. These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks. The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI. The JPEG-AI verification model has been released and is currently under development for standardization. Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point. Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point. However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes. As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality. Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y.","sentences":["Currently, there is a high demand for neural network-based image compression codecs.","These codecs employ non-linear transforms to create compact bit representations and facilitate faster coding speeds on devices compared to the hand-crafted transforms used in classical frameworks.","The scientific and industrial communities are highly interested in these properties, leading to the standardization effort of JPEG-AI.","The JPEG-AI verification model has been released and is currently under development for standardization.","Utilizing neural networks, it can outperform the classic codec VVC intra by over 10% BD-rate operating at base operation point.","Researchers attribute this success to the flexible bit distribution in the spatial domain, in contrast to VVC intra's anchor that is generated with a constant quality point.","However, our study reveals that VVC intra displays a more adaptable bit distribution structure through the implementation of various block sizes.","As a result of our observations, we have proposed a spatial bit allocation method to optimize the JPEG-AI verification model's bit distribution and enhance the visual quality.","Furthermore, by applying the VVC bit distribution strategy, the objective performance of JPEG-AI verification mode can be further improved, resulting in a maximum gain of 0.45 dB in PSNR-Y."],"url":"http://arxiv.org/abs/2402.17470v1","category":"cs.CV"}
{"created":"2024-02-27 12:48:01","title":"Natural Language Processing Methods for Symbolic Music Generation and Information Retrieval: a Survey","abstract":"Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP). This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data. However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR. Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music. These analogies are also reflected through similar tasks in MIR and NLP. This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes. We first propose an overview of representations of symbolic music adapted from natural language sequential representations. Such representations are designed by considering the specificities of symbolic music. These representations are then processed by models. Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks. We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms. We finally present a discussion surrounding the effective use of NLP tools for symbolic music data. This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR.","sentences":["Several adaptations of Transformers models have been developed in various domains since its breakthrough in Natural Language Processing (NLP).","This trend has spread into the field of Music Information Retrieval (MIR), including studies processing music data.","However, the practice of leveraging NLP tools for symbolic music data is not novel in MIR.","Music has been frequently compared to language, as they share several similarities, including sequential representations of text and music.","These analogies are also reflected through similar tasks in MIR and NLP.","This survey reviews NLP methods applied to symbolic music generation and information retrieval studies following two axes.","We first propose an overview of representations of symbolic music adapted from natural language sequential representations.","Such representations are designed by considering the specificities of symbolic music.","These representations are then processed by models.","Such models, possibly originally developed for text and adapted for symbolic music, are trained on various tasks.","We describe these models, in particular deep learning models, through different prisms, highlighting music-specialized mechanisms.","We finally present a discussion surrounding the effective use of NLP tools for symbolic music data.","This includes technical issues regarding NLP methods and fundamental differences between text and music, which may open several doors for further research into more effectively adapting NLP tools to symbolic MIR."],"url":"http://arxiv.org/abs/2402.17467v1","category":"cs.IR"}
{"created":"2024-02-27 12:26:07","title":"DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning","abstract":"In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models. Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario. To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism. Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage. In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively.","sentences":["In this work, we investigate the potential of large language models (LLMs) based agents to automate data science tasks, with the goal of comprehending task requirements, then building and training the best-fit machine learning models.","Despite their widespread success, existing LLM agents are hindered by generating unreasonable experiment plans within this scenario.","To this end, we present DS-Agent, a novel automatic framework that harnesses LLM agent and case-based reasoning (CBR).","In the development stage, DS-Agent follows the CBR framework to structure an automatic iteration pipeline, which can flexibly capitalize on the expert knowledge from Kaggle, and facilitate consistent performance improvement through the feedback mechanism.","Moreover, DS-Agent implements a low-resource deployment stage with a simplified CBR paradigm to adapt past successful solutions from the development stage for direct code generation, significantly reducing the demand on foundational capabilities of LLMs.","Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success rate in the development stage, while attaining 36% improvement on average one pass rate across alternative LLMs in the deployment stage.","In both stages, DS-Agent achieves the best rank in performance, costing \\$1.60 and \\$0.13 per run with GPT-4, respectively."],"url":"http://arxiv.org/abs/2402.17453v1","category":"cs.LG"}
{"created":"2024-02-27 10:55:07","title":"LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning","abstract":"Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts. Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block. A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning. Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts. This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks. Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding. This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories. To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks. Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance.","sentences":["Visual Prompt Tuning (VPT) techniques have gained prominence for their capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual tasks using specialized learnable tokens termed as prompts.","Contemporary VPT methodologies, especially when employed with self-supervised vision transformers, often default to the introduction of new learnable prompts or gated prompt tokens predominantly sourced from the model's previous block.","A pivotal oversight in such approaches is their failure to harness the potential of long-range previous blocks as sources of prompts within each self-supervised ViT.","To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning (LSPT) - a revolutionary approach to visual representation learning.","Drawing inspiration from the intricacies of the human brain, LSPT ingeniously incorporates long-term gated prompts.","This feature serves as temporal coding, curbing the risk of forgetting parameters acquired from earlier blocks.","Further enhancing its prowess, LSPT brings into play patch tokens, serving as spatial coding.","This is strategically designed to perpetually amass class-conscious features, thereby fortifying the model's prowess in distinguishing and identifying visual categories.","To validate the efficacy of our proposed method, we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.","Our empirical findings underscore the superiority of LSPT, showcasing its ability to set new benchmarks in visual prompt tuning performance."],"url":"http://arxiv.org/abs/2402.17406v1","category":"cs.CV"}
{"created":"2024-02-27 10:48:56","title":"Beacon, a lightweight deep reinforcement learning benchmark library for flow control","abstract":"Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments. Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community. Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis. To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements. In this contribution, the seven considered problems are described, and reference control solutions are provided. The sources for the following work are available at https://github.com/jviquerat/beacon.","sentences":["Recently, the increasing use of deep reinforcement learning for flow control problems has led to a new area of research, focused on the coupling and the adaptation of the existing algorithms to the control of numerical fluid dynamics environments.","Although still in its infancy, the field has seen multiple successes in a short time span, and its fast development pace can certainly be partly imparted to the open-source effort that drives the expansion of the community.","Yet, this emerging domain still misses a common ground to (i) ensure the reproducibility of the results, and (ii) offer a proper ad-hoc benchmarking basis.","To this end, we propose Beacon, an open-source benchmark library composed of seven lightweight 1D and 2D flow control problems with various characteristics, action and observation space characteristics, and CPU requirements.","In this contribution, the seven considered problems are described, and reference control solutions are provided.","The sources for the following work are available at https://github.com/jviquerat/beacon."],"url":"http://arxiv.org/abs/2402.17402v1","category":"physics.comp-ph"}
{"created":"2024-02-27 10:47:24","title":"Investigating Continual Pretraining in Large Language Models: Insights and Implications","abstract":"This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning. We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field.","sentences":["This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training.","Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification.","Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios.","To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation.","We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models.","Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning.","We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field."],"url":"http://arxiv.org/abs/2402.17400v1","category":"cs.CL"}
{"created":"2024-02-27 09:55:34","title":"CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks","abstract":"Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories. Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data. In extensive experiments, we show that CGGM's synthetic data outperforms state-of-the-art methods across various metrics. Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance.","sentences":["Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT).","Generative models are often used to address the issue of imbalanced node categories in dynamic graphs.","Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes.","This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class.","The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure.","The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information.","Labels are transformed into embedding vectors, serving as conditional constraints to control the generation of synthetic data across multiple categories.","Using a multi-stage loss, the distribution of synthetic data is adjusted to closely resemble that of real data.","In extensive experiments, we show that CGGM's synthetic data outperforms state-of-the-art methods across various metrics.","Our results demonstrate efficient generation of diverse data categories, robustly enhancing multi-category classification model performance."],"url":"http://arxiv.org/abs/2402.17363v1","category":"cs.RO"}
{"created":"2024-02-27 09:52:27","title":"SoFA: Shielded On-the-fly Alignment via Priority Rule Following","abstract":"The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values. This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards. This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions. Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules. Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence. Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately.","sentences":["The alignment problem in Large Language Models (LLMs) involves adapting them to the broad spectrum of human values.","This requirement challenges existing alignment methods due to diversity of preferences and regulatory standards.","This paper introduces a novel alignment paradigm, priority rule following, which defines rules as the primary control mechanism in each dialog, prioritizing them over user instructions.","Our preliminary analysis reveals that even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding and prioritizing the rules.","Therefore, we present PriorityDistill, a semi-automated approach for distilling priority following signals from LLM simulations to ensure robust rule integration and adherence.","Our experiments show that this method not only effectively minimizes misalignments utilizing only one general rule but also adapts smoothly to various unseen rules, ensuring they are shielded from hijacking and that the model responds appropriately."],"url":"http://arxiv.org/abs/2402.17358v1","category":"cs.CL"}
{"created":"2024-02-27 08:47:19","title":"Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation","abstract":"The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.","sentences":["The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices.","Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides.","However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance.","Thus, one has to adapt the edge models promptly to attain promising performance.","Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance.","To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios.","In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online.","In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion.","Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy.","Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA."],"url":"http://arxiv.org/abs/2402.17316v2","category":"cs.CV"}
{"created":"2024-02-27 08:20:45","title":"ArcSin: Adaptive ranged cosine Similarity injected noise for Language-Driven Visual Tasks","abstract":"In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE). We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding. To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin). First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity. Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale. This dual strategy effectively widens the scope of the original domain while safeguarding content integrity. Our empirical results demonstrate that these models closely rival those trained on images in terms of performance. Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively. Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks. The code will be released.","sentences":["In this study, we address the challenging task of bridging the modality gap between learning from language and inference for visual tasks, including Visual Question Answering (VQA), Image Captioning (IC) and Visual Entailment (VE).","We train models for these tasks in a zero-shot cross-modal transfer setting, a domain where the previous state-of-the-art method relied on the fixed scale noise injection, often compromising the semantic content of the original modality embedding.","To combat it, we propose a novel method called Adaptive ranged cosine Similarity injected noise (ArcSin).","First, we introduce an innovative adaptive noise scale that effectively generates the textual elements with more variability while preserving the original text feature's integrity.","Second, a similarity pool strategy is employed, expanding the domain generalization potential by broadening the overall noise scale.","This dual strategy effectively widens the scope of the original domain while safeguarding content integrity.","Our empirical results demonstrate that these models closely rival those trained on images in terms of performance.","Specifically, our method exhibits substantial improvements over the previous state-of-the-art, achieving gains of 1.9 and 1.1 CIDEr points in S-Cap and M-Cap, respectively.","Additionally, we observe increases of 1.5 percentage points (pp), 1.4 pp, and 1.4 pp in accuracy for VQA, VQA-E, and VE, respectively, pushing the boundaries of what is achievable within the constraints of image-trained model benchmarks.","The code will be released."],"url":"http://arxiv.org/abs/2402.17298v1","category":"cs.CV"}
{"created":"2024-02-29 18:45:17","title":"Solution of the Diophantine equation $x^2 + p^k=y^n$","abstract":"The main aim of this article is to find all solutions of the Diophantine equation $x^2 + p^k=y^n$ where $p \\equiv 1 \\pmod 4$, $\\frac{p-1}{3}$ is a perfect square and the class number of $\\mathbb{Z}[\\sqrt{-p}]$ is $2$. In this article, I used a method involving prime factorization and class numbers which is different from using congruent number argument which is widely used in this type of problem.","sentences":["The main aim of this article is to find all solutions of the Diophantine equation $x^2 + p^k=y^n$ where $p \\equiv 1 \\pmod 4$, $\\frac{p-1}{3}$ is a perfect square and the class number of $\\mathbb{Z}[\\sqrt{-p}]$ is $2$. In this article, I used a method involving prime factorization and class numbers which is different from using congruent number argument which is widely used in this type of problem."],"url":"http://arxiv.org/abs/2402.19445v1","category":"math.NT"}
{"created":"2024-02-29 18:43:43","title":"3D Gaussian Model for Animation and Texturing","abstract":"3D Gaussian Splatting has made a marked impact on neural rendering by achieving impressive fidelity and performance. Despite this achievement, however, it is not readily applicable to developing interactive applications. Real-time applications like XR apps and games require functions such as animation, UV-mapping, and model editing simultaneously manipulated through the usage of a 3D model. We propose a modeling that is analogous to typical 3D models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable proxy for novel animation and texture transfer. By binding the 3D Gaussians in texture space and re-projecting them back to world space through implicit shell mapping, we show how our 3D modeling can serve as a valid rendering methodology for interactive applications. It is further noted that recently, 3D mesh reconstruction works have been able to produce high-quality mesh for rendering. Our work, on the other hand, only requires an approximated geometry for rendering an object in high fidelity. Applicationwise, we will show that our proxy-based 3DGM is capable of driving novel animation without animated training data and texture transferring via UV mapping of the 3D Gaussians. We believe the result indicates the potential of our work for enabling interactive applications for 3D Gaussian Splatting.","sentences":["3D Gaussian Splatting has made a marked impact on neural rendering by achieving impressive fidelity and performance.","Despite this achievement, however, it is not readily applicable to developing interactive applications.","Real-time applications like XR apps and games require functions such as animation, UV-mapping, and model editing simultaneously manipulated through the usage of a 3D model.","We propose a modeling that is analogous to typical 3D models, which we call 3D Gaussian Model (3DGM); it provides a manipulatable proxy for novel animation and texture transfer.","By binding the 3D Gaussians in texture space and re-projecting them back to world space through implicit shell mapping, we show how our 3D modeling can serve as a valid rendering methodology for interactive applications.","It is further noted that recently, 3D mesh reconstruction works have been able to produce high-quality mesh for rendering.","Our work, on the other hand, only requires an approximated geometry for rendering an object in high fidelity.","Applicationwise, we will show that our proxy-based 3DGM is capable of driving novel animation without animated training data and texture transferring via UV mapping of the 3D Gaussians.","We believe the result indicates the potential of our work for enabling interactive applications for 3D Gaussian Splatting."],"url":"http://arxiv.org/abs/2402.19441v1","category":"cs.GR"}
{"created":"2024-02-29 18:24:46","title":"Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models","abstract":"Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.","sentences":["Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale.","We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention.","Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens.","We also show that Griffin can extrapolate on sequences significantly longer than those seen during training.","Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput.","We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training."],"url":"http://arxiv.org/abs/2402.19427v1","category":"cs.LG"}
{"created":"2024-02-29 18:00:27","title":"Assessing Visually-Continuous Corruption Robustness of Neural Networks Relative to Human Performance","abstract":"While Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet, they often lack robustness against image corruption, i.e., corruption robustness. Yet such robustness is seemingly effortless for human perception. In this paper, we propose visually-continuous corruption robustness (VCR) -- an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e., from the original image to the full distortion of all perceived visual information), along with two novel human-aware metrics for NN evaluation. To compare VCR of NNs with human perception, we conducted extensive experiments on 14 commonly used image corruptions with 7,718 human participants and state-of-the-art robust NN models with different training objectives (e.g., standard, adversarial, corruption robustness), different architectures (e.g., convolution NNs, vision transformers), and different amounts of training data augmentation. Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing benchmarks; as a result, 2) the gap between NN and human robustness is larger than previously known; and finally, 3) some image corruptions have a similar impact on human perception, offering opportunities for more cost-effective robustness assessments. Our validation set with 14 image corruptions, human robustness data, and the evaluation code is provided as a toolbox and a benchmark.","sentences":["While Neural Networks (NNs) have surpassed human accuracy in image classification on ImageNet, they often lack robustness against image corruption, i.e., corruption robustness.","Yet such robustness is seemingly effortless for human perception.","In this paper, we propose visually-continuous corruption robustness (VCR) -- an extension of corruption robustness to allow assessing it over the wide and continuous range of changes that correspond to the human perceptive quality (i.e., from the original image to the full distortion of all perceived visual information), along with two novel human-aware metrics for NN evaluation.","To compare VCR of NNs with human perception, we conducted extensive experiments on 14 commonly used image corruptions with 7,718 human participants and state-of-the-art robust NN models with different training objectives (e.g., standard, adversarial, corruption robustness), different architectures (e.g., convolution NNs, vision transformers), and different amounts of training data augmentation.","Our study showed that: 1) assessing robustness against continuous corruption can reveal insufficient robustness undetected by existing benchmarks; as a result, 2) the gap between NN and human robustness is larger than previously known; and finally, 3) some image corruptions have a similar impact on human perception, offering opportunities for more cost-effective robustness assessments.","Our validation set with 14 image corruptions, human robustness data, and the evaluation code is provided as a toolbox and a benchmark."],"url":"http://arxiv.org/abs/2402.19401v1","category":"cs.CV"}
{"created":"2024-02-29 17:55:16","title":"Machine Learning for Quantitative MR Image Reconstruction","abstract":"In the last years, the design of image reconstruction methods in the field of quantitative Magnetic Resonance Imaging (qMRI) has experienced a paradigm shift. Often, when dealing with (quantitative) MR image reconstruction problems, one is concerned with solving one or a couple of ill-posed inverse problems which require the use of advanced regularization methods. An increasing amount of attention is nowadays put on the development of data-driven methods using Neural Networks (NNs) to learn meaningful prior information without the need to explicitly model hand-crafted priors. In addition, the available hardware and computational resources nowadays offer the possibility to learn regularization models in a so-called model-aware fashion, which is a unique key feature that distinguishes these models from regularization methods learned in a more classical, model-agnostic manner. Model-aware methods are not only tailored to the considered data, but also to the class of considered imaging problems and nowadays constitute the state-of-the-art in image reconstruction methods. In the following chapter, we provide the reader with an extensive overview of methods that can be employed for (quantitative) MR image reconstruction, also highlighting their advantages and limitations both from a theoretical and computational point of view.","sentences":["In the last years, the design of image reconstruction methods in the field of quantitative Magnetic Resonance Imaging (qMRI) has experienced a paradigm shift.","Often, when dealing with (quantitative) MR image reconstruction problems, one is concerned with solving one or a couple of ill-posed inverse problems which require the use of advanced regularization methods.","An increasing amount of attention is nowadays put on the development of data-driven methods using Neural Networks (NNs) to learn meaningful prior information without the need to explicitly model hand-crafted priors.","In addition, the available hardware and computational resources nowadays offer the possibility to learn regularization models in a so-called model-aware fashion, which is a unique key feature that distinguishes these models from regularization methods learned in a more classical, model-agnostic manner.","Model-aware methods are not only tailored to the considered data, but also to the class of considered imaging problems and nowadays constitute the state-of-the-art in image reconstruction methods.","In the following chapter, we provide the reader with an extensive overview of methods that can be employed for (quantitative) MR image reconstruction, also highlighting their advantages and limitations both from a theoretical and computational point of view."],"url":"http://arxiv.org/abs/2402.19396v1","category":"math.OC"}
{"created":"2024-02-29 17:18:39","title":"Scattering model of scintillation arcs in pulsar secondary spectra","abstract":"The dynamic spectra of pulsars frequently exhibit diverse interference patterns, often associated with parabolic arcs in the Fourier-transformed (secondary) spectra. In our approach, we extend beyond the traditional Fresnel-Kirchhoff method by using the Green's function of the Helmholtz equation. Through advanced numerical techniques, we model both the dynamic and secondary spectra, providing a comprehensive framework that describes all components of the latter spectra in terms of physical quantities. Additionally, we provide a thorough analytical explanation of the secondary spectrum.","sentences":["The dynamic spectra of pulsars frequently exhibit diverse interference patterns, often associated with parabolic arcs in the Fourier-transformed (secondary) spectra.","In our approach, we extend beyond the traditional Fresnel-Kirchhoff method by using the Green's function of the Helmholtz equation.","Through advanced numerical techniques, we model both the dynamic and secondary spectra, providing a comprehensive framework that describes all components of the latter spectra in terms of physical quantities.","Additionally, we provide a thorough analytical explanation of the secondary spectrum."],"url":"http://arxiv.org/abs/2402.19370v1","category":"astro-ph.HE"}
{"created":"2024-02-29 17:13:25","title":"Arrow Matrix Decomposition: A Novel Approach for Communication-Efficient Sparse Matrix Multiplication","abstract":"We propose a novel approach to iterated sparse matrix dense matrix multiplication, a fundamental computational kernel in scientific computing and graph neural network training. In cases where matrix sizes exceed the memory of a single compute node, data transfer becomes a bottleneck. An approach based on dense matrix multiplication algorithms leads to suboptimal scalability and fails to exploit the sparsity in the problem. To address these challenges, we propose decomposing the sparse matrix into a small number of highly structured matrices called arrow matrices, which are connected by permutations. Our approach enables communication-avoiding multiplications, achieving a polynomial reduction in communication volume per iteration for matrices corresponding to planar graphs and other minor-excluded families of graphs. Our evaluation demonstrates that our approach outperforms a state-of-the-art method for sparse matrix multiplication on matrices with hundreds of millions of rows, offering near-linear strong and weak scaling.","sentences":["We propose a novel approach to iterated sparse matrix dense matrix multiplication, a fundamental computational kernel in scientific computing and graph neural network training.","In cases where matrix sizes exceed the memory of a single compute node, data transfer becomes a bottleneck.","An approach based on dense matrix multiplication algorithms leads to suboptimal scalability and fails to exploit the sparsity in the problem.","To address these challenges, we propose decomposing the sparse matrix into a small number of highly structured matrices called arrow matrices, which are connected by permutations.","Our approach enables communication-avoiding multiplications, achieving a polynomial reduction in communication volume per iteration for matrices corresponding to planar graphs and other minor-excluded families of graphs.","Our evaluation demonstrates that our approach outperforms a state-of-the-art method for sparse matrix multiplication on matrices with hundreds of millions of rows, offering near-linear strong and weak scaling."],"url":"http://arxiv.org/abs/2402.19364v1","category":"cs.DC"}
{"created":"2024-02-29 16:53:00","title":"Recanting witness and natural direct effects: Violations of assumptions or definitions?","abstract":"There have been numerous publications on the advantages and disadvantages of estimating natural (pure) effects compared to controlled effects. One of the main criticisms of natural effects is that it requires an additional assumption for identifiability, namely that the exposure does not cause a confounder of the mediator-outcome relationship. However, every analysis in every study should begin with a research question expressed in ordinary language. Researchers then develop/use mathematical expressions or estimators to best answer these ordinary language questions. When a recanting witness is present, the paper illustrates that there are no violations of assumptions. Rather, using directed acyclic graphs, the typical estimators for natural effects are simply no longer answering any meaningful question. Although some might view this as semantics, the proposed approach illustrates why the more recent methods of path-specific effects and separable effects are more valid and transparent compared to previous methods for decomposition analysis.","sentences":["There have been numerous publications on the advantages and disadvantages of estimating natural (pure) effects compared to controlled effects.","One of the main criticisms of natural effects is that it requires an additional assumption for identifiability, namely that the exposure does not cause a confounder of the mediator-outcome relationship.","However, every analysis in every study should begin with a research question expressed in ordinary language.","Researchers then develop/use mathematical expressions or estimators to best answer these ordinary language questions.","When a recanting witness is present, the paper illustrates that there are no violations of assumptions.","Rather, using directed acyclic graphs, the typical estimators for natural effects are simply no longer answering any meaningful question.","Although some might view this as semantics, the proposed approach illustrates why the more recent methods of path-specific effects and separable effects are more valid and transparent compared to previous methods for decomposition analysis."],"url":"http://arxiv.org/abs/2402.19346v1","category":"stat.ME"}
{"created":"2024-02-29 16:10:35","title":"Viscoelastic response and anisotropic hydrodynamics in Weyl semimetals","abstract":"We study viscoelastic response in Weyl semimetals with broken time-reversal symmetry. Topology and anisotropy of the Fermi surface are manifested in the viscoelasticity tensor of the electron fluid. In the dynamic (inter-band) part of this tensor, the anisotropy leads to a qualitatively different, compared to isotropic models, scaling with frequency and the Fermi energy. While components of the viscosity tensor determined by the Fermi surface properties agree in the Kubo and kinetic formalisms, the latter misses the anomalous Hall viscosity determined by filled states below the Fermi surface. The anisotropy of the dispersion relation is also manifested in the acceleration and relaxation terms of the hydrodynamic equations.","sentences":["We study viscoelastic response in Weyl semimetals with broken time-reversal symmetry.","Topology and anisotropy of the Fermi surface are manifested in the viscoelasticity tensor of the electron fluid.","In the dynamic (inter-band) part of this tensor, the anisotropy leads to a qualitatively different, compared to isotropic models, scaling with frequency and the Fermi energy.","While components of the viscosity tensor determined by the Fermi surface properties agree in the Kubo and kinetic formalisms, the latter misses the anomalous Hall viscosity determined by filled states below the Fermi surface.","The anisotropy of the dispersion relation is also manifested in the acceleration and relaxation terms of the hydrodynamic equations."],"url":"http://arxiv.org/abs/2402.19304v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 15:13:31","title":"Bosonic dark matter dynamics in hybrid neutron stars","abstract":"This research studies the intricate interplay between dark and baryonic matter within hybrid neutron stars enriched by anisotropic bosonic dark matter halos. Our modelling, guided by the equation of state with a free parameter, reveals diverse mass-radius correlations for these astronomical objects. A pivotal result is the influence of dark matter characteristics - whether condensed or dispersed - on the observable attributes of neutron stars based on their masses. Our investigation into anisotropic models, which offer a notably authentic representation of dark matter anisotropy, reveals a unique low-density core halo profile, distinguishing it from alternative approaches. Insights gleaned from galactic clusters have further refined our understanding of the bosonic dark matter paradigm. Observational constraints derived from the dynamics of galaxy clusters have been fundamental in defining the dark matter particle mass to lie between 0.05 GeV and 0.5 GeV and the scattering length to range from 0.9 fm to 3 fm. Using terrestrial Bose-Einstein condensate experiments, we have narrowed down the properties of bosonic dark matter, especially in the often overlooked 3 to 30 GeV mass range. Our findings fortify the understanding of dark and baryonic matter synergies in hybrid neutron stars, establishing a robust foundation for future astrophysical pursuits.","sentences":["This research studies the intricate interplay between dark and baryonic matter within hybrid neutron stars enriched by anisotropic bosonic dark matter halos.","Our modelling, guided by the equation of state with a free parameter, reveals diverse mass-radius correlations for these astronomical objects.","A pivotal result is the influence of dark matter characteristics - whether condensed or dispersed - on the observable attributes of neutron stars based on their masses.","Our investigation into anisotropic models, which offer a notably authentic representation of dark matter anisotropy, reveals a unique low-density core halo profile, distinguishing it from alternative approaches.","Insights gleaned from galactic clusters have further refined our understanding of the bosonic dark matter paradigm.","Observational constraints derived from the dynamics of galaxy clusters have been fundamental in defining the dark matter particle mass to lie between 0.05 GeV and 0.5 GeV and the scattering length to range from 0.9 fm to 3 fm.","Using terrestrial Bose-Einstein condensate experiments, we have narrowed down the properties of bosonic dark matter, especially in the often overlooked 3 to 30 GeV mass range.","Our findings fortify the understanding of dark and baryonic matter synergies in hybrid neutron stars, establishing a robust foundation for future astrophysical pursuits."],"url":"http://arxiv.org/abs/2402.19238v1","category":"astro-ph.HE"}
{"created":"2024-02-29 14:46:53","title":"Quantum Coherent States and Path Integral Method to Stochastically Determine the Anisotropic Volume Expansion in Lithiated Silicon Nanowires","abstract":"This computational research study will analyze the multi-physics of lithium ion insertion into a silicon nanowire in an attempt to explain the electrochemical kinetics at the nanoscale and quantum level. The electron coherent states and a quantum field version of photon density waves will be the joining theories that will explain the electron-photon interaction within the lithium-silicon lattice structure. These two quantum particles will be responsible for the photon absorption rate of silicon atoms that are hypothesized to be the leading cause of breaking diatomic silicon covalent bonds that ultimately leads to volume expansion. It will be demonstrated through the combination of Maxwell stress tensor, optical amplification and path integrals that a stochastic analyze using a variety of Poisson distributions that the anisotropic expansion rates in the <110>, <111> and <112> orthogonal directions confirms the findings ascertained in previous works made by other research groups. The computational findings presented in this work are similar to those which were discovered experimentally using transmission electron microscopy (TEM) and simulation models that used density functional theory (DFT) and molecular dynamics (MD). The refractive index and electric susceptibility parameters of lithiated silicon are interwoven in the first principle theoretical equations and appears frequently throughout this research presentation, which should serve to demonstrate the importance of these parameters in the understanding of this component in lithium ion batteries.","sentences":["This computational research study will analyze the multi-physics of lithium ion insertion into a silicon nanowire in an attempt to explain the electrochemical kinetics at the nanoscale and quantum level.","The electron coherent states and a quantum field version of photon density waves will be the joining theories that will explain the electron-photon interaction within the lithium-silicon lattice structure.","These two quantum particles will be responsible for the photon absorption rate of silicon atoms that are hypothesized to be the leading cause of breaking diatomic silicon covalent bonds that ultimately leads to volume expansion.","It will be demonstrated through the combination of Maxwell stress tensor, optical amplification and path integrals that a stochastic analyze using a variety of Poisson distributions that the anisotropic expansion rates in the <110>, <111> and <112> orthogonal directions confirms the findings ascertained in previous works made by other research groups.","The computational findings presented in this work are similar to those which were discovered experimentally using transmission electron microscopy (TEM) and simulation models that used density functional theory (DFT) and molecular dynamics (MD).","The refractive index and electric susceptibility parameters of lithiated silicon are interwoven in the first principle theoretical equations and appears frequently throughout this research presentation, which should serve to demonstrate the importance of these parameters in the understanding of this component in lithium ion batteries."],"url":"http://arxiv.org/abs/2402.19217v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 14:33:20","title":"On non-negative solutions of stochastic Volterra equations with jumps and non-Lipschitz coefficients","abstract":"We consider one-dimensional stochastic Volterra equations with jumps for which we establish conditions upon the convolution kernel and coefficients for the strong existence and pathwise uniqueness of a non-negative c\\`adl\\`ag solution. By using the approach recently developed in arXiv:2302.07758, we show the strong existence by using a nonnegative approximation of the equation whose convergence is proved via a variant of the Yamada--Watanabe approximation technique. We apply our results to L\\'evy-driven stochastic Volterra equations. In particular, we are able to define a Volterra extension of the so-called alpha-stable Cox--Ingersoll--Ross process, which is especially used for applications in Mathematical Finance.","sentences":["We consider one-dimensional stochastic Volterra equations with jumps for which we establish conditions upon the convolution kernel and coefficients for the strong existence and pathwise uniqueness of a non-negative c\\`adl\\`ag solution.","By using the approach recently developed in arXiv:2302.07758, we show the strong existence by using a nonnegative approximation of the equation whose convergence is proved via a variant of the Yamada--Watanabe approximation technique.","We apply our results to L\\'evy-driven stochastic Volterra equations.","In particular, we are able to define a Volterra extension of the so-called alpha-stable Cox--Ingersoll--Ross process, which is especially used for applications in Mathematical Finance."],"url":"http://arxiv.org/abs/2402.19203v1","category":"math.PR"}
{"created":"2024-02-29 13:59:35","title":"Boundary Regularity of Harmonic maps from $RCD(K,N)$-space to $CAT(0)$-space","abstract":"We establish the boundary regularity of harmonic maps from $RCD(K, N)$ metric measure spaces into $CAT(0)$ metric spaces.","sentences":["We establish the boundary regularity of harmonic maps from $RCD(K, N)$ metric measure spaces into $CAT(0)$ metric spaces."],"url":"http://arxiv.org/abs/2402.19179v1","category":"math.DG"}
{"created":"2024-02-29 13:48:23","title":"Horizontal semiconcavity for the square of Carnot-Carath\u00e9odory distance on ideal Carnot groups and applications to Hamilton-Jacobi equations","abstract":"We show that the square of Carnot-Carath\\'eodory distance from the origin, in ideal Carnot groups, enjoys the horizontal semiconcavity (h-semiconcavity) everywhere in the group including the origin. We apply this property to show h-semiconcavity for the solutions of a class of non-coercive evolutive Hamilton-Jacobi equations, by using the associated Hopf-Lax solutions.","sentences":["We show that the square of Carnot-Carath\\'eodory distance from the origin, in ideal Carnot groups, enjoys the horizontal semiconcavity (h-semiconcavity) everywhere in the group including the origin.","We apply this property to show h-semiconcavity for the solutions of a class of non-coercive evolutive Hamilton-Jacobi equations, by using the associated Hopf-Lax solutions."],"url":"http://arxiv.org/abs/2402.19164v1","category":"math.AP"}
{"created":"2024-02-29 13:39:03","title":"A rigorous approach to the sharp interface limit for phase-field models of tumor growth","abstract":"In this paper we consider two diffuse interface models for tumor growth coupling a Cahn-Hilliard type equation for the tumor phase parameter to a reaction-diffusion type equation for the nutrient. The models are distinguished by the presence of two different coupling source terms. For such problems, we address the question of the limit, as the diffuse interface parameter tends to zero, from diffuse interface models to sharp interface ones, justifying rigorously what was deduced via formal asymptotics in [18]. The resulting evolutions turn out to be varifold solutions to Mullins-Sekerka type flows for the tumor region suitably coupled with the equation for the nutrient.","sentences":["In this paper we consider two diffuse interface models for tumor growth coupling a Cahn-Hilliard type equation for the tumor phase parameter to a reaction-diffusion type equation for the nutrient.","The models are distinguished by the presence of two different coupling source terms.","For such problems, we address the question of the limit, as the diffuse interface parameter tends to zero, from diffuse interface models to sharp interface ones, justifying rigorously what was deduced via formal asymptotics in [18].","The resulting evolutions turn out to be varifold solutions to Mullins-Sekerka type flows for the tumor region suitably coupled with the equation for the nutrient."],"url":"http://arxiv.org/abs/2402.19156v1","category":"math.AP"}
{"created":"2024-02-29 13:31:56","title":"Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts","abstract":"Large Multimodal Models (LMMs) rely on pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) to perform amazing emergent abilities on various multimodal tasks in the joint space of vision and language. However, the Typographic Attack, which shows disruption to VLMs, has also been certified as a security vulnerability to LMMs. In this work, we first comprehensively investigate the distractibility of LMMs by typography. In particular, we introduce the Typographic Dataset designed to evaluate distractibility across various multi-modal subtasks, such as object recognition, visual attributes detection, enumeration, arithmetic computation, and commonsense reasoning. To further study the effect of typographic patterns on performance, we also scrutinize the effect of tuning various typographic factors, encompassing font size, color, opacity, and spatial positioning of typos. We discover that LMMs can partially distinguish visual contents and typos when confronting typographic attacks, which suggests that embeddings from vision encoders contain enough information to distinguish visual contents and typos in images. Inspired by such phenomena, we demonstrate that CLIP's performance of zero-shot classification on typo-ridden images can be significantly improved by providing more informative texts to match images. Furthermore, we also prove that LMMs can utilize more informative prompts to leverage information in embeddings to differentiate between visual content and typos. Finally, we propose a prompt information enhancement method that can effectively mitigate the effects of typography.","sentences":["Large Multimodal Models (LMMs) rely on pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) to perform amazing emergent abilities on various multimodal tasks in the joint space of vision and language.","However, the Typographic Attack, which shows disruption to VLMs, has also been certified as a security vulnerability to LMMs.","In this work, we first comprehensively investigate the distractibility of LMMs by typography.","In particular, we introduce the Typographic Dataset designed to evaluate distractibility across various multi-modal subtasks, such as object recognition, visual attributes detection, enumeration, arithmetic computation, and commonsense reasoning.","To further study the effect of typographic patterns on performance, we also scrutinize the effect of tuning various typographic factors, encompassing font size, color, opacity, and spatial positioning of typos.","We discover that LMMs can partially distinguish visual contents and typos when confronting typographic attacks, which suggests that embeddings from vision encoders contain enough information to distinguish visual contents and typos in images.","Inspired by such phenomena, we demonstrate that CLIP's performance of zero-shot classification on typo-ridden images can be significantly improved by providing more informative texts to match images.","Furthermore, we also prove that LMMs can utilize more informative prompts to leverage information in embeddings to differentiate between visual content and typos.","Finally, we propose a prompt information enhancement method that can effectively mitigate the effects of typography."],"url":"http://arxiv.org/abs/2402.19150v1","category":"cs.CV"}
{"created":"2024-02-29 13:24:08","title":"A Unified Evaluation Framework for Spiking Neural Network Hardware Accelerators Based on Emerging Non-Volatile Memory Devices","abstract":"Spiking Neural Networks (SNNs) have emerged as a promising paradigm, offering event-driven and energy-efficient computation. In recent studies, various devices tailored for SNN synapses and neurons have been proposed, leveraging the unique characteristics of emerging non-volatile memory (eNVM) technologies. While substantial progress has been made in exploring the capabilities of SNNs and designing dedicated hardware components, there exists a critical gap in establishing a unified approach for evaluating hardware-level metrics. Specifically, metrics such as latency, and energy consumption, are pivotal in assessing the practical viability and efficiency of the constructed neural network. In this article, we address this gap by presenting a comprehensive framework for evaluating hardware-level metrics in SNNs based on non-volatile memory devices. We systematically analyze the impact of synaptic and neuronal components on energy consumption providing a unified perspective for assessing the overall efficiency of the network. In this study, our emphasis lies on the neuron and synaptic device based on magnetic skyrmions. Nevertheless, our framework is versatile enough to encompass other emerging devices as well. Utilizing our proposed skyrmionic devices, the constructed SNN demonstrates an inference accuracy of approximately 98% and achieves energy consumption on the order of pJ when processing the Modified National Institute of Standards and Technology (MNIST) handwritten digit dataset.","sentences":["Spiking Neural Networks (SNNs) have emerged as a promising paradigm, offering event-driven and energy-efficient computation.","In recent studies, various devices tailored for SNN synapses and neurons have been proposed, leveraging the unique characteristics of emerging non-volatile memory (eNVM) technologies.","While substantial progress has been made in exploring the capabilities of SNNs and designing dedicated hardware components, there exists a critical gap in establishing a unified approach for evaluating hardware-level metrics.","Specifically, metrics such as latency, and energy consumption, are pivotal in assessing the practical viability and efficiency of the constructed neural network.","In this article, we address this gap by presenting a comprehensive framework for evaluating hardware-level metrics in SNNs based on non-volatile memory devices.","We systematically analyze the impact of synaptic and neuronal components on energy consumption providing a unified perspective for assessing the overall efficiency of the network.","In this study, our emphasis lies on the neuron and synaptic device based on magnetic skyrmions.","Nevertheless, our framework is versatile enough to encompass other emerging devices as well.","Utilizing our proposed skyrmionic devices, the constructed SNN demonstrates an inference accuracy of approximately 98% and achieves energy consumption on the order of pJ when processing the Modified National Institute of Standards and Technology (MNIST) handwritten digit dataset."],"url":"http://arxiv.org/abs/2402.19139v1","category":"cond-mat.other"}
{"created":"2024-02-29 12:24:20","title":"A Protein Structure Prediction Approach Leveraging Transformer and CNN Integration","abstract":"Proteins are essential for life, and their structure determines their function. The protein secondary structure is formed by the folding of the protein primary structure, and the protein tertiary structure is formed by the bending and folding of the secondary structure. Therefore, the study of protein secondary structure is very helpful to the overall understanding of protein structure. Although the accuracy of protein secondary structure prediction has continuously improved with the development of machine learning and deep learning, progress in the field of protein structure prediction, unfortunately, remains insufficient to meet the large demand for protein information. Therefore, based on the advantages of deep learning-based methods in feature extraction and learning ability, this paper adopts a two-dimensional fusion deep neural network model, DstruCCN, which uses Convolutional Neural Networks (CCN) and a supervised Transformer protein language model for single-sequence protein structure prediction. The training features of the two are combined to predict the protein Transformer binding site matrix, and then the three-dimensional structure is reconstructed using energy minimization.","sentences":["Proteins are essential for life, and their structure determines their function.","The protein secondary structure is formed by the folding of the protein primary structure, and the protein tertiary structure is formed by the bending and folding of the secondary structure.","Therefore, the study of protein secondary structure is very helpful to the overall understanding of protein structure.","Although the accuracy of protein secondary structure prediction has continuously improved with the development of machine learning and deep learning, progress in the field of protein structure prediction, unfortunately, remains insufficient to meet the large demand for protein information.","Therefore, based on the advantages of deep learning-based methods in feature extraction and learning ability, this paper adopts a two-dimensional fusion deep neural network model, DstruCCN, which uses Convolutional Neural Networks (CCN) and a supervised Transformer protein language model for single-sequence protein structure prediction.","The training features of the two are combined to predict the protein Transformer binding site matrix, and then the three-dimensional structure is reconstructed using energy minimization."],"url":"http://arxiv.org/abs/2402.19095v1","category":"q-bio.BM"}
{"created":"2024-02-29 11:49:39","title":"Sharp interface limit for $1$D stochastic Allen-Cahn equation in full small noise regime","abstract":"We study the sharp interface limit for the $1$D stochastic Allen-Cahn equation, and extend earlier work by Funaki to the full small noise regime. The main new idea is the construction of a series of functional correctors, which are designed to recursively cancel potential divergences.   In addition, in order to show these correctors are well-behaved, we develop a systematic decomposition of functional derivatives of the deterministic Allen-Cahn flow of all orders. This decomposition is of its own interest, and may be useful in other situations as well.","sentences":["We study the sharp interface limit for the $1$D stochastic Allen-Cahn equation, and extend earlier work by Funaki to the full small noise regime.","The main new idea is the construction of a series of functional correctors, which are designed to recursively cancel potential divergences.   ","In addition, in order to show these correctors are well-behaved, we develop a systematic decomposition of functional derivatives of the deterministic Allen-Cahn flow of all orders.","This decomposition is of its own interest, and may be useful in other situations as well."],"url":"http://arxiv.org/abs/2402.19070v1","category":"math.PR"}
{"created":"2024-02-29 11:33:11","title":"Recovering the Polytropic Exponent in the Porous Medium Equation: Asymptotic Approach","abstract":"In this paper we consider the time dependent Porous Medium Equation, $u_t = \\Delta u^\\gamma$ with real polytropic exponent $\\gamma>1$, subject to a homogeneous Dirichlet boundary condition. We are interested in recovering $\\gamma$ from the knowledge of the solution $u$ at a given large time $T$. Based on an asymptotic inequality satisfied by the solution $u(T)$, we propose a numerical algorithm allowing us to recover $\\gamma$. An upper bound for the error between the exact and recovered $\\gamma$ is then showed. Finally, numerical investigations are carried out in two dimensions.","sentences":["In this paper we consider the time dependent Porous Medium Equation, $u_t = \\Delta u^\\gamma$ with real polytropic exponent $\\gamma>1$, subject to a homogeneous Dirichlet boundary condition.","We are interested in recovering $\\gamma$ from the knowledge of the solution $u$ at a given large time $T$. Based on an asymptotic inequality satisfied by the solution $u(T)$, we propose a numerical algorithm allowing us to recover $\\gamma$. An upper bound for the error between the exact and recovered $\\gamma$ is then showed.","Finally, numerical investigations are carried out in two dimensions."],"url":"http://arxiv.org/abs/2402.19056v1","category":"math.NA"}
{"created":"2024-02-29 10:29:05","title":"Machine learning-enabled exploration of mesoscale architectures in amphiphilic-molecule self-assembly","abstract":"Amphiphilic molecules spontaneously form self-assembled structures of various shapes depending on their molecular structures, the temperature, and other physical conditions. The functionalities of these structures are dictated by their formations and their properties must be evaluated for reproduction using molecular simulations. However, the assessment of such intricate structures involves many procedural steps. This study investigates the potential of machine-learning models to extract structural features from mesoscale non-ordered self-assembled structures, and suggests a methodology in which machine-learning models for the structural analysis of self-assembled structures are trained on particle types and coordinate data. In the proposed approach, graph neural networks are utilised to extract local structural data for analysis. In simulations using several hundred self-assembled structures of up to 4050 coarse-grained particles, local structures are successfully extracted and classified with up to 78.35 % accuracy. As the machine-learning models learn structural characteristics without the need for human-made feature engineering, the proposed method has important potential applications in the field of materials science.","sentences":["Amphiphilic molecules spontaneously form self-assembled structures of various shapes depending on their molecular structures, the temperature, and other physical conditions.","The functionalities of these structures are dictated by their formations and their properties must be evaluated for reproduction using molecular simulations.","However, the assessment of such intricate structures involves many procedural steps.","This study investigates the potential of machine-learning models to extract structural features from mesoscale non-ordered self-assembled structures, and suggests a methodology in which machine-learning models for the structural analysis of self-assembled structures are trained on particle types and coordinate data.","In the proposed approach, graph neural networks are utilised to extract local structural data for analysis.","In simulations using several hundred self-assembled structures of up to 4050 coarse-grained particles, local structures are successfully extracted and classified with up to 78.35 % accuracy.","As the machine-learning models learn structural characteristics without the need for human-made feature engineering, the proposed method has important potential applications in the field of materials science."],"url":"http://arxiv.org/abs/2402.19019v1","category":"cond-mat.soft"}
{"created":"2024-02-29 09:35:42","title":"Splitting integrators for linear Vlasov equations with stochastic perturbations","abstract":"We consider a class of linear Vlasov partial differential equations driven by Wiener noise. Different types of stochastic perturbations are treated: additive noise, multiplicative It\\^o and Stratonovich noise, and transport noise. We propose to employ splitting integrators for the temporal discretization of these stochastic partial differential equations. These integrators are designed in order to preserve qualitative properties of the exact solutions depending on the stochastic perturbation, such as preservation of norms or positivity of the solutions. We provide numerical experiments in order to illustrate the properties of the proposed integrators and investigate mean-square rates of convergence.","sentences":["We consider a class of linear Vlasov partial differential equations driven by Wiener noise.","Different types of stochastic perturbations are treated: additive noise, multiplicative It\\^o and Stratonovich noise, and transport noise.","We propose to employ splitting integrators for the temporal discretization of these stochastic partial differential equations.","These integrators are designed in order to preserve qualitative properties of the exact solutions depending on the stochastic perturbation, such as preservation of norms or positivity of the solutions.","We provide numerical experiments in order to illustrate the properties of the proposed integrators and investigate mean-square rates of convergence."],"url":"http://arxiv.org/abs/2402.18982v1","category":"math.NA"}
{"created":"2024-02-29 09:12:37","title":"Ambisonics Networks -- The Effect Of Radial Functions Regularization","abstract":"Ambisonics, a popular format of spatial audio, is the spherical harmonic (SH) representation of the plane wave density function of a sound field. Many algorithms operate in the SH domain and utilize the Ambisonics as their input signal. The process of encoding Ambisonics from a spherical microphone array involves dividing by the radial functions, which may amplify noise at low frequencies. This can be overcome by regularization, with the downside of introducing errors to the Ambisonics encoding. This paper aims to investigate the impact of different ways of regularization on Deep Neural Network (DNN) training and performance. Ideally, these networks should be robust to the way of regularization. Simulated data of a single speaker in a room and experimental data from the LOCATA challenge were used to evaluate this robustness on an example algorithm of speaker localization based on the direct-path dominance (DPD) test. Results show that performance may be sensitive to the way of regularization, and an informed approach is proposed and investigated, highlighting the importance of regularization information.","sentences":["Ambisonics, a popular format of spatial audio, is the spherical harmonic (SH) representation of the plane wave density function of a sound field.","Many algorithms operate in the SH domain and utilize the Ambisonics as their input signal.","The process of encoding Ambisonics from a spherical microphone array involves dividing by the radial functions, which may amplify noise at low frequencies.","This can be overcome by regularization, with the downside of introducing errors to the Ambisonics encoding.","This paper aims to investigate the impact of different ways of regularization on Deep Neural Network (DNN) training and performance.","Ideally, these networks should be robust to the way of regularization.","Simulated data of a single speaker in a room and experimental data from the LOCATA challenge were used to evaluate this robustness on an example algorithm of speaker localization based on the direct-path dominance (DPD) test.","Results show that performance may be sensitive to the way of regularization, and an informed approach is proposed and investigated, highlighting the importance of regularization information."],"url":"http://arxiv.org/abs/2402.18968v1","category":"eess.AS"}
{"created":"2024-02-29 09:08:13","title":"Induced Gravitational Wave interpretation of PTA data: a complete study for general equation of state","abstract":"We thoroughly study the induced gravitational wave interpretation of the possible gravitational wave background reported by PTA collaborations, considering the unknown equation of state $w$ of the early universe. We perform a Bayesian analysis of the NANOGrav data using the publicly available \\textsc{PTArcade} code together with \\textsc{SIGWfast} for the numerical integration of the induced gravitational wave spectrum. We focus on two cases: a monochromatic and a log-normal primordial spectrum of fluctuations. For the log-normal spectrum, we show that, while the results are not very sensitive to $w$ when the GW peak is close to the PTA window, radiation domination is out of the $2\\sigma$ contours when only the infra-red power-law tail contributes. For the monochromatic spectrum, the $2\\sigma$ bounds yield $0.1\\lesssim w\\lesssim0.9$ so that radiation domination is close to the central value. We also investigate the primordial black hole (PBH) counterpart using the peak formalism. We show that, in general terms, a larger width and stiffer equation of state alleviates the overproduction of PBHs. No PBH overproduction requires $w\\gtrsim0.42$ up to 2-$\\sigma$ level for the monochromatic spectrum. Furthermore, including bounds from the cosmic microwave background, we find in general that the mass range of the PBH counterpart is bounded by $10^{-5} M_\\odot\\lesssim M_{\\rm PBH}\\lesssim10^{-1} M_\\odot$. Lastly, we find that the PTA signal can explain the microlensing events reported by OGLE for $0.42\\lesssim w\\lesssim 0.50$. Our work showcases a complete treatment of induced gravitational waves and primordial black holes for general $w$ for future data analysis.","sentences":["We thoroughly study the induced gravitational wave interpretation of the possible gravitational wave background reported by PTA collaborations, considering the unknown equation of state $w$ of the early universe.","We perform a Bayesian analysis of the NANOGrav data using the publicly available \\textsc{PTArcade} code together with \\textsc{SIGWfast} for the numerical integration of the induced gravitational wave spectrum.","We focus on two cases: a monochromatic and a log-normal primordial spectrum of fluctuations.","For the log-normal spectrum, we show that, while the results are not very sensitive to $w$ when the GW peak is close to the PTA window, radiation domination is out of the $2\\sigma$ contours when only the infra-red power-law tail contributes.","For the monochromatic spectrum, the $2\\sigma$ bounds yield $0.1\\lesssim w\\lesssim0.9$ so that radiation domination is close to the central value.","We also investigate the primordial black hole (PBH) counterpart using the peak formalism.","We show that, in general terms, a larger width and stiffer equation of state alleviates the overproduction of PBHs.","No PBH overproduction requires $w\\gtrsim0.42$ up to 2-$\\sigma$ level for the monochromatic spectrum.","Furthermore, including bounds from the cosmic microwave background, we find in general that the mass range of the PBH counterpart is bounded by $10^{-5} M_\\odot\\lesssim M_{\\rm PBH}\\lesssim10^{-1} M_\\odot$.","Lastly, we find that the PTA signal can explain the microlensing events reported by OGLE for $0.42\\lesssim w\\lesssim 0.50$.","Our work showcases a complete treatment of induced gravitational waves and primordial black holes for general $w$ for future data analysis."],"url":"http://arxiv.org/abs/2402.18965v1","category":"astro-ph.CO"}
{"created":"2024-02-29 07:45:44","title":"On the discrete analogues of Appell function $F_4$","abstract":"In this paper, we study the Appell function $F_4$ from discrete point of view. In particular, we obtain regions of convergence, difference-differential equations, finite and infinite summation formulas and a list of recursion relations satisfied by the discrete analogues of Appell function $F_4$.","sentences":["In this paper, we study the Appell function $F_4$ from discrete point of view.","In particular, we obtain regions of convergence, difference-differential equations, finite and infinite summation formulas and a list of recursion relations satisfied by the discrete analogues of Appell function $F_4$."],"url":"http://arxiv.org/abs/2402.18931v1","category":"math.CA"}
{"created":"2024-02-29 07:11:55","title":"Smooth Structures on $M^n\\times\\mathbb{S}^k$","abstract":"This paper explores various differentiable structures on the product manifold $M \\times \\mathbb{S}^k$, where $M$ is either a 4-dimensional closed oriented manifold or a simply connected 5-dimensional closed manifold. We identify the possible stable homotopy types of $M$ and use it to calculate the concordance inertia group and the concordance structure set of $M\\times\\mathbb{S}^k$ for $1\\leq k\\leq 10.$ These calculations enable us to further classify all manifolds that are homeomorphic to $\\mathbb{C}P^2\\times\\mathbb{S}^k$, up to diffeomorphism, for each $4\\leq k\\leq 6$.","sentences":["This paper explores various differentiable structures on the product manifold $M \\times \\mathbb{S}^k$, where $M$ is either a 4-dimensional closed oriented manifold or a simply connected 5-dimensional closed manifold.","We identify the possible stable homotopy types of $M$ and use it to calculate the concordance inertia group and the concordance structure set of $M\\times\\mathbb{S}^k$ for $1\\leq k\\leq 10.$ These calculations enable us to further classify all manifolds that are homeomorphic to $\\mathbb{C}P^2\\times\\mathbb{S}^k$, up to diffeomorphism, for each $4\\leq k\\leq 6$."],"url":"http://arxiv.org/abs/2402.18914v1","category":"math.AT"}
{"created":"2024-02-29 07:02:23","title":"Neutrino Mixing and Resonant Leptogenesis in Inverse Seesaw and \u0394(54) Flavor Symmetry","abstract":"The current work involves augmenting the $\\Delta(54)$ discrete flavor model by incorporating two Standard Model Higgs particles into the Inverse Seesaw mechanism. We introduced Weyl fermions and Vector like fermions, which are gauge singlets in the Standard Model and produces Majorana mass terms in our lagrangian. The resulting mass matrix deviates from the tribimaximal neutrino mixing pattern producing a non-zero reactor angle ($\\theta_{13}$) . We have determined the effective Majorana neutrino mass, which is the parameter of relevance in neutrinoless double beta decay investigations, using the model's limited six-dimensional parameter space. We additionally investigate the possibility of baryogenesis in the proposed framework via resonant leptogenesis. We have the non-zero value for resonantly enhanced CP asymmetry originating from the decay of right-handed neutrinos at the TeV scale, accounting for flavor effects. The evolution of lepton asymmetry is systematically analyzed by numerically solving a set of Boltzmann equations, leading to the determination of the baryon asymmetry with a magnitude of $ \\lvert \\eta_B \\rvert \\approx 6 \\times 10^{-10}$. This outcome is achieved by selecting specific values for the right-handed neutrino mass $M_1 = 10$ TeV and mass splitting, $d \\approx 10^{-8}$.","sentences":["The current work involves augmenting the $\\Delta(54)$ discrete flavor model by incorporating two Standard Model Higgs particles into the Inverse Seesaw mechanism.","We introduced Weyl fermions and Vector like fermions, which are gauge singlets in the Standard Model and produces Majorana mass terms in our lagrangian.","The resulting mass matrix deviates from the tribimaximal neutrino mixing pattern producing a non-zero reactor angle ($\\theta_{13}$) .","We have determined the effective Majorana neutrino mass, which is the parameter of relevance in neutrinoless double beta decay investigations, using the model's limited six-dimensional parameter space.","We additionally investigate the possibility of baryogenesis in the proposed framework via resonant leptogenesis.","We have the non-zero value for resonantly enhanced CP asymmetry originating from the decay of right-handed neutrinos at the TeV scale, accounting for flavor effects.","The evolution of lepton asymmetry is systematically analyzed by numerically solving a set of Boltzmann equations, leading to the determination of the baryon asymmetry with a magnitude of $ \\lvert \\eta_B","\\rvert \\approx 6 \\times","10^{-10}$.","This outcome is achieved by selecting specific values for the right-handed neutrino mass $M_1 = 10$ TeV and mass splitting, $d \\approx 10^{-8}$."],"url":"http://arxiv.org/abs/2402.18906v1","category":"hep-ph"}
{"created":"2024-02-29 06:11:21","title":"BP-DeepONet: A new method for cuffless blood pressure estimation using the physcis-informed DeepONet","abstract":"Cardiovascular diseases (CVDs) are the leading cause of death worldwide, with blood pressure serving as a crucial indicator. Arterial blood pressure (ABP) waveforms provide continuous pressure measurements throughout the cardiac cycle and offer valuable diagnostic insights. Consequently, there is a significant demand for non-invasive and cuff-less methods to measure ABP waveforms continuously. Accurate prediction of ABP waveforms can also improve the estimation of mean blood pressure, an essential cardiovascular health characteristic.   This study proposes a novel framework based on the physics-informed DeepONet approach to predict ABP waveforms. Unlike previous methods, our approach requires the predicted ABP waveforms to satisfy the Navier-Stokes equation with a time-periodic condition and a Windkessel boundary condition. Notably, our framework is the first to predict ABP waveforms continuously, both with location and time, within the part of the artery that is being simulated. Furthermore, our method only requires ground truth data at the outlet boundary and can handle periodic conditions with varying periods. Incorporating the Windkessel boundary condition in our solution allows for generating natural physical reflection waves, which closely resemble measurements observed in real-world cases. Moreover, accurately estimating the hyper-parameters in the Navier-Stokes equation for our simulations poses a significant challenge. To overcome this obstacle, we introduce the concept of meta-learning, enabling the neural networks to learn these parameters during the training process.","sentences":["Cardiovascular diseases (CVDs) are the leading cause of death worldwide, with blood pressure serving as a crucial indicator.","Arterial blood pressure (ABP) waveforms provide continuous pressure measurements throughout the cardiac cycle and offer valuable diagnostic insights.","Consequently, there is a significant demand for non-invasive and cuff-less methods to measure ABP waveforms continuously.","Accurate prediction of ABP waveforms can also improve the estimation of mean blood pressure, an essential cardiovascular health characteristic.   ","This study proposes a novel framework based on the physics-informed DeepONet approach to predict ABP waveforms.","Unlike previous methods, our approach requires the predicted ABP waveforms to satisfy the Navier-Stokes equation with a time-periodic condition and a Windkessel boundary condition.","Notably, our framework is the first to predict ABP waveforms continuously, both with location and time, within the part of the artery that is being simulated.","Furthermore, our method only requires ground truth data at the outlet boundary and can handle periodic conditions with varying periods.","Incorporating the Windkessel boundary condition in our solution allows for generating natural physical reflection waves, which closely resemble measurements observed in real-world cases.","Moreover, accurately estimating the hyper-parameters in the Navier-Stokes equation for our simulations poses a significant challenge.","To overcome this obstacle, we introduce the concept of meta-learning, enabling the neural networks to learn these parameters during the training process."],"url":"http://arxiv.org/abs/2402.18886v1","category":"cs.LG"}
{"created":"2024-02-29 05:25:23","title":"Probabilistic Lipschitzness and the Stable Rank for Comparing Explanation Models","abstract":"Explainability models are now prevalent within machine learning to address the black-box nature of neural networks. The question now is which explainability model is most effective. Probabilistic Lipschitzness has demonstrated that the smoothness of a neural network is fundamentally linked to the quality of post hoc explanations. In this work, we prove theoretical lower bounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and SmoothGrad. We propose a novel metric using probabilistic Lipschitzness, normalised astuteness, to compare the robustness of explainability models. Further, we prove a link between the local Lipschitz constant of a neural network and its stable rank. We then demonstrate that the stable rank of a neural network provides a heuristic for the robustness of explainability models.","sentences":["Explainability models are now prevalent within machine learning to address the black-box nature of neural networks.","The question now is which explainability model is most effective.","Probabilistic Lipschitzness has demonstrated that the smoothness of a neural network is fundamentally linked to the quality of post hoc explanations.","In this work, we prove theoretical lower bounds on the probabilistic Lipschitzness of Integrated Gradients, LIME and SmoothGrad.","We propose a novel metric using probabilistic Lipschitzness, normalised astuteness, to compare the robustness of explainability models.","Further, we prove a link between the local Lipschitz constant of a neural network and its stable rank.","We then demonstrate that the stable rank of a neural network provides a heuristic for the robustness of explainability models."],"url":"http://arxiv.org/abs/2402.18863v1","category":"cs.LG"}
{"created":"2024-02-29 05:24:35","title":"Universal Translational and Rotational Mobility Expressions of Phoretic and Self-phoretic Particles with Arbitrary Interaction Potentials","abstract":"The mobility of externally-driven phoretic propulsion of particles is evaluated by simultaneously solving the solute conservation equation, interaction potential equation, and the modified Stokes equation. While accurate, this approach is cumbersome, especially when the interaction potential decays slowly compared to the particle size. In contrast to external phoresis, the motion of self-phoretic particles is typically estimated by relating the translation and rotation velocities with the local slip velocity. While this approach is convenient and thus widely used, it is only valid when the interaction decay length is significantly smaller than the particle size. Here, by employing the Lorentz reciprocal theorem, we combine the benefits of two approaches and derive unified mobility expressions with arbitrary interaction potentials such that the expressions predict the translation and rotation velocities for both externally driven and self-propelling particles. We show that these expressions can conveniently recover the well-known mobility relationships of external electrophoresis and diffusiophoresis for arbitrary double-layer thickness. Additionally, we show that for a spherical microswimmer, our derived expressions relax to the slip velocity calculations in the limit of thin interaction lengthscales. We also employ the derived mobility expressions to calculate the velocities of an autophoretic Janus particle. We find that there is significant dampening in the translation velocity even when the interaction length is an order of magnitude larger than the particle size. Finally, we study the motion of a catalytically self-propelled particle, while it also propels due to external concentration gradients, and demonstrate how the two propulsion modes compete with each other.","sentences":["The mobility of externally-driven phoretic propulsion of particles is evaluated by simultaneously solving the solute conservation equation, interaction potential equation, and the modified Stokes equation.","While accurate, this approach is cumbersome, especially when the interaction potential decays slowly compared to the particle size.","In contrast to external phoresis, the motion of self-phoretic particles is typically estimated by relating the translation and rotation velocities with the local slip velocity.","While this approach is convenient and thus widely used, it is only valid when the interaction decay length is significantly smaller than the particle size.","Here, by employing the Lorentz reciprocal theorem, we combine the benefits of two approaches and derive unified mobility expressions with arbitrary interaction potentials such that the expressions predict the translation and rotation velocities for both externally driven and self-propelling particles.","We show that these expressions can conveniently recover the well-known mobility relationships of external electrophoresis and diffusiophoresis for arbitrary double-layer thickness.","Additionally, we show that for a spherical microswimmer, our derived expressions relax to the slip velocity calculations in the limit of thin interaction lengthscales.","We also employ the derived mobility expressions to calculate the velocities of an autophoretic Janus particle.","We find that there is significant dampening in the translation velocity even when the interaction length is an order of magnitude larger than the particle size.","Finally, we study the motion of a catalytically self-propelled particle, while it also propels due to external concentration gradients, and demonstrate how the two propulsion modes compete with each other."],"url":"http://arxiv.org/abs/2402.18861v1","category":"physics.flu-dyn"}
{"created":"2024-02-29 05:21:43","title":"Error estimation for finite element method on meshes that contain thin elements","abstract":"In an error estimation of finite element solutions to the Poisson equation, we usually impose the shape regularity assumption on the meshes to be used. In this paper, we show that even if the shape regularity condition is violated, the standard error estimation can be obtained if \"bad\" elements (elements that violate the shape regularity or maximum angle condition) are covered virtually by \"good\" simplices. A numerical experiment confirms the theoretical result.","sentences":["In an error estimation of finite element solutions to the Poisson equation, we usually impose the shape regularity assumption on the meshes to be used.","In this paper, we show that even if the shape regularity condition is violated, the standard error estimation can be obtained if \"bad\" elements (elements that violate the shape regularity or maximum angle condition) are covered virtually by \"good\" simplices.","A numerical experiment confirms the theoretical result."],"url":"http://arxiv.org/abs/2402.18860v1","category":"math.NA"}
{"created":"2024-02-29 04:29:42","title":"A variation of parameters formula for nonautonomous linear impulsive differential equations with piecewise constant arguments of generalized type","abstract":"In this work, we give a variation of parameters formula for nonautonomous linear impulsive differential equations with piecewise constant arguments of generalized type. We cover several cases of differential equations with deviated arguments investigated before as particular cases. We also give some examples showing the applicability of our results.","sentences":["In this work, we give a variation of parameters formula for nonautonomous linear impulsive differential equations with piecewise constant arguments of generalized type.","We cover several cases of differential equations with deviated arguments investigated before as particular cases.","We also give some examples showing the applicability of our results."],"url":"http://arxiv.org/abs/2402.18843v1","category":"math.DS"}
{"created":"2024-02-29 04:12:32","title":"Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation","abstract":"The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead. However, the theory of the guidance-based method not only requires the user to fine-tune the \"guidance strength,\" but its target vector field does not necessarily correspond to the conditional distribution used in training. In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods. Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching. This theory naturally derives a method that aims to match the matrix field as opposed to the vector field. Our framework ensures the continuity of the generated conditional distribution through the existence of flow between conditional distributions. We will present our theory through experiments and mathematical results.","sentences":["The task of conditional generation is one of the most important applications of generative models, and numerous methods have been developed to date based on the celebrated diffusion models, with the guidance-based classifier-free method taking the lead.","However, the theory of the guidance-based method not only requires the user to fine-tune the \"guidance strength,\" but its target vector field does not necessarily correspond to the conditional distribution used in training.","In this paper, we develop the theory of conditional generation based on Flow Matching, a current strong contender of diffusion methods.","Motivated by the interpretation of a probability path as a distribution on path space, we establish a novel theory of flow-based generation of conditional distribution by employing the mathematical framework of generalized continuity equation instead of the continuity equation in flow matching.","This theory naturally derives a method that aims to match the matrix field as opposed to the vector field.","Our framework ensures the continuity of the generated conditional distribution through the existence of flow between conditional distributions.","We will present our theory through experiments and mathematical results."],"url":"http://arxiv.org/abs/2402.18839v1","category":"cs.LG"}
{"created":"2024-02-29 03:14:42","title":"Learning protocols for the fast and efficient control of active matter","abstract":"We show that it is possible to learn protocols that effect fast and efficient state-to-state transformations in simulation models of active particles. By encoding the protocol in the form of a neural network we use evolutionary methods to identify protocols that take active particles from one steady state to another, as quickly as possible or with as little energy expended as possible. Our results show that protocols identified by a flexible neural-network ansatz, which allows the optimization of multiple control parameters and the emergence of sharp features, are more efficient than protocols derived recently by constrained analytical methods. Our learning scheme is straightforward to use in experiment, suggesting a way of designing protocols for the efficient manipulation of active matter in the laboratory.","sentences":["We show that it is possible to learn protocols that effect fast and efficient state-to-state transformations in simulation models of active particles.","By encoding the protocol in the form of a neural network we use evolutionary methods to identify protocols that take active particles from one steady state to another, as quickly as possible or with as little energy expended as possible.","Our results show that protocols identified by a flexible neural-network ansatz, which allows the optimization of multiple control parameters and the emergence of sharp features, are more efficient than protocols derived recently by constrained analytical methods.","Our learning scheme is straightforward to use in experiment, suggesting a way of designing protocols for the efficient manipulation of active matter in the laboratory."],"url":"http://arxiv.org/abs/2402.18823v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-29 02:04:48","title":"Allen-Cahn equation and degenerate minimal hypersurface","abstract":"In this short note, we present new observations and examples concerning the existence and rigidity of solutions to the Allen-Cahn equation with degenerate minimal hypersurfaces as their limit interfaces.","sentences":["In this short note, we present new observations and examples concerning the existence and rigidity of solutions to the Allen-Cahn equation with degenerate minimal hypersurfaces as their limit interfaces."],"url":"http://arxiv.org/abs/2402.18799v1","category":"math.DG"}
{"created":"2024-02-29 01:54:35","title":"Resolved Near-infrared Stellar Photometry from the Magellan Telescope for 13 Nearby Galaxies: JAGB Method Distances","abstract":"We present near-infrared JHK photometry for the resolved stellar populations in 13 nearby galaxies: NGC 6822, IC 1613, NGC 3109, Sextans B, Sextans A, NGC 300, NGC 55, NGC 7793, NGC 247, NGC 5253, Cen A, NGC 1313, and M83, acquired from the 6.5m Baade-Magellan telescope. We measure distances to each galaxy using the J-region asymptotic giant branch (JAGB) method, a new standard candle that leverages the constant luminosities of color-selected, carbon-rich AGB stars. While only single-epoch, random-phase photometry is necessary to derive JAGB distances, our photometry is time-averaged over multiple epochs, thereby decreasing the contribution of the JAGB stars' intrinsic variability to the measured dispersions in their observed luminosity functions. To cross-validate these distances, we also measure near-infrared tip of the red giant branch (TRGB) distances to these galaxies. The residuals obtained from subtracting the distance moduli from the two methods yield an RMS scatter of $\\sigma_{JAGB - TRGB}= \\pm 0.07$ mag. Therefore, all systematics in either the JAGB method and TRGB method (e.g., crowding, differential reddening, star formation histories) must be contained within these $\\pm0.07$ mag bounds for this sample of galaxies because the JAGB and TRGB distance indicators are drawn from entirely distinct stellar populations, and are thus affected by these systematics independently. Finally, the composite JAGB star luminosity function formed from this diverse sample of galaxies is well-described by a Gaussian function with a modal value of $M_J = -6.20 \\pm 0.003$ mag (stat), indicating the underlying JAGB star luminosity function of a well-sampled full star formation history is highly symmetric and Gaussian, based on over 6,700 JAGB stars in the composite sample.","sentences":["We present near-infrared JHK photometry for the resolved stellar populations in 13 nearby galaxies: NGC 6822, IC 1613, NGC 3109, Sextans B, Sextans A, NGC 300, NGC 55, NGC 7793, NGC 247, NGC 5253, Cen A, NGC 1313, and M83, acquired from the 6.5m Baade-Magellan telescope.","We measure distances to each galaxy using the J-region asymptotic giant branch (JAGB) method, a new standard candle that leverages the constant luminosities of color-selected, carbon-rich AGB stars.","While only single-epoch, random-phase photometry is necessary to derive JAGB distances, our photometry is time-averaged over multiple epochs, thereby decreasing the contribution of the JAGB stars' intrinsic variability to the measured dispersions in their observed luminosity functions.","To cross-validate these distances, we also measure near-infrared tip of the red giant branch (TRGB) distances to these galaxies.","The residuals obtained from subtracting the distance moduli from the two methods yield an RMS scatter of $\\sigma_{JAGB - TRGB}= \\pm 0.07$ mag.","Therefore, all systematics in either the JAGB method and TRGB method (e.g., crowding, differential reddening, star formation histories) must be contained within these $\\pm0.07$ mag bounds for this sample of galaxies because the JAGB and TRGB distance indicators are drawn from entirely distinct stellar populations, and are thus affected by these systematics independently.","Finally, the composite JAGB star luminosity function formed from this diverse sample of galaxies is well-described by a Gaussian function with a modal value of $M_J = -6.20 \\pm 0.003$ mag (stat), indicating the underlying JAGB star luminosity function of a well-sampled full star formation history is highly symmetric and Gaussian, based on over 6,700 JAGB stars in the composite sample."],"url":"http://arxiv.org/abs/2402.18794v1","category":"astro-ph.GA"}
{"created":"2024-02-29 01:49:18","title":"MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks","abstract":"Deep neural networks have been proven to be vulnerable to adversarial examples and various methods have been proposed to defend against adversarial attacks for natural language processing tasks. However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task. In this paper, we propose a malicious perturbation based adversarial training method (MPAT) for building robust deep neural networks against textual adversarial attacks. Specifically, we construct a multi-level malicious example generation strategy to generate adversarial examples with malicious perturbations, which are used instead of original inputs for model training. Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task. We conduct comprehensive experiments to evaluate our defense method by attacking five victim models on three benchmark datasets. The result demonstrates that our method is more effective against malicious adversarial attacks compared with previous defense methods while maintaining or further improving the performance on the original task.","sentences":["Deep neural networks have been proven to be vulnerable to adversarial examples and various methods have been proposed to defend against adversarial attacks for natural language processing tasks.","However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task.","In this paper, we propose a malicious perturbation based adversarial training method (MPAT) for building robust deep neural networks against textual adversarial attacks.","Specifically, we construct a multi-level malicious example generation strategy to generate adversarial examples with malicious perturbations, which are used instead of original inputs for model training.","Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task.","We conduct comprehensive experiments to evaluate our defense method by attacking five victim models on three benchmark datasets.","The result demonstrates that our method is more effective against malicious adversarial attacks compared with previous defense methods while maintaining or further improving the performance on the original task."],"url":"http://arxiv.org/abs/2402.18792v1","category":"cs.LG"}
{"created":"2024-02-29 01:39:32","title":"Topological interpretation of extremal and Davies-type phase transitions of black holes","abstract":"Topological arguments are currently being used as a novel scheme to discern the properties of black holes while ignoring their detailed structure and specific field equations. Among various avenues of black hole physics, where this novel approach is being utilized, the phase transition in black hole thermodynamics lies at the forefront. There are several types of phase transition in black holes; such as the van der Waals type phase transition, Davies-type phase transition, extremal phase transition, and Hawking-Page (HP) transition. So far, the topological interpretation, where the critical point has been identified with the non-zero topological charge, has been obtained only for the van der Waals type phase transition and HP transition in different spacetimes. To complete the picture, here we provide the same interpretation for two other phase transitions: Davies-type phase transition and extremal phase transition. The entire analysis is general and is valid for any spacetime where these types of phase transitions are observed. More importantly, our analysis suggests that amid the apparent differences in these phase transitions, they share the same topological characteristics, \\textit{i.e.} non-zero topological charge arising from different thermodynamic potentials in different types of phase transition.","sentences":["Topological arguments are currently being used as a novel scheme to discern the properties of black holes while ignoring their detailed structure and specific field equations.","Among various avenues of black hole physics, where this novel approach is being utilized, the phase transition in black hole thermodynamics lies at the forefront.","There are several types of phase transition in black holes; such as the van der Waals type phase transition, Davies-type phase transition, extremal phase transition, and Hawking-Page (HP) transition.","So far, the topological interpretation, where the critical point has been identified with the non-zero topological charge, has been obtained only for the van der Waals type phase transition and HP transition in different spacetimes.","To complete the picture, here we provide the same interpretation for two other phase transitions: Davies-type phase transition and extremal phase transition.","The entire analysis is general and is valid for any spacetime where these types of phase transitions are observed.","More importantly, our analysis suggests that amid the apparent differences in these phase transitions, they share the same topological characteristics, \\textit{i.e.} non-zero topological charge arising from different thermodynamic potentials in different types of phase transition."],"url":"http://arxiv.org/abs/2402.18791v1","category":"gr-qc"}
{"created":"2024-02-29 00:39:20","title":"Achieving quantized transport in Floquet topological insulators via energy filters","abstract":"Due to photon-assisted transport processes, chiral edge modes induced by periodic driving do not directly mediate quantized transport. Here we show how narrow bandwidth \"energy filters\" can restore quantization by suppressing photon assisted transport through Floquet sidebands. We derive a Floquet Landauer type equation to describe transport through such an energy-filtered setup, and show how the filter can be integrated out to yield a sharply energy-dependent renormalized system-lead coupling. We show analytically and through numerical simulations that a nearly quantized conductance can be achieved in both off-resonantly and resonantly induced quasienergy gaps when filters are introduced. The conductance approaches the appropriate quantized value on each plateau with increasing system and filter size. We introduce a \"Floquet distribution function\" and show both analytically and numerically that it approaches the equilibrium Fermi-Dirac form when narrow-band filters are introduced, highlighting the mechanism that restores quantized transport.","sentences":["Due to photon-assisted transport processes, chiral edge modes induced by periodic driving do not directly mediate quantized transport.","Here we show how narrow bandwidth \"energy filters\" can restore quantization by suppressing photon assisted transport through Floquet sidebands.","We derive a Floquet Landauer type equation to describe transport through such an energy-filtered setup, and show how the filter can be integrated out to yield a sharply energy-dependent renormalized system-lead coupling.","We show analytically and through numerical simulations that a nearly quantized conductance can be achieved in both off-resonantly and resonantly induced quasienergy gaps when filters are introduced.","The conductance approaches the appropriate quantized value on each plateau with increasing system and filter size.","We introduce a \"Floquet distribution function\" and show both analytically and numerically that it approaches the equilibrium Fermi-Dirac form when narrow-band filters are introduced, highlighting the mechanism that restores quantized transport."],"url":"http://arxiv.org/abs/2402.18776v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 00:02:33","title":"Disentangling the Causes of Plasticity Loss in Neural Networks","abstract":"Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \\textit{stationary} data distribution. In settings where this assumption is violated, e.g.\\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds. One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses. While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network? This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms. We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment.","sentences":["Underpinning the past decades of work on the design, initialization, and optimization of neural networks is a seemingly innocuous assumption: that the network is trained on a \\textit{stationary} data distribution.","In settings where this assumption is violated, e.g.\\ deep reinforcement learning, learning algorithms become unstable and brittle with respect to hyperparameters and even random seeds.","One factor driving this instability is the loss of plasticity, meaning that updating the network's predictions in response to new information becomes more difficult as training progresses.","While many recent works provide analyses and partial solutions to this phenomenon, a fundamental question remains unanswered: to what extent do known mechanisms of plasticity loss overlap, and how can mitigation strategies be combined to best maintain the trainability of a network?","This paper addresses these questions, showing that loss of plasticity can be decomposed into multiple independent mechanisms and that, while intervening on any single mechanism is insufficient to avoid the loss of plasticity in all cases, intervening on multiple mechanisms in conjunction results in highly robust learning algorithms.","We show that a combination of layer normalization and weight decay is highly effective at maintaining plasticity in a variety of synthetic nonstationary learning tasks, and further demonstrate its effectiveness on naturally arising nonstationarities, including reinforcement learning in the Arcade Learning Environment."],"url":"http://arxiv.org/abs/2402.18762v1","category":"cs.LG"}
{"created":"2024-02-29 00:01:33","title":"Exploration of Learned Lifting-Based Transform Structures for Fully Scalable and Accessible Wavelet-Like Image Compression","abstract":"This paper provides a comprehensive study on features and performance of different ways to incorporate neural networks into lifting-based wavelet-like transforms, within the context of fully scalable and accessible image compression. Specifically, we explore different arrangements of lifting steps, as well as various network architectures for learned lifting operators. Moreover, we examine the impact of the number of learned lifting steps, the number of channels, the number of layers and the support of kernels in each learned lifting operator. To facilitate the study, we investigate two generic training methodologies that are simultaneously appropriate to a wide variety of lifting structures considered. Experimental results ultimately suggest that retaining fixed lifting steps from the base wavelet transform is highly beneficial. Moreover, we demonstrate that employing more learned lifting steps and more layers in each learned lifting operator do not contribute strongly to the compression performance. However, benefits can be obtained by utilizing more channels in each learned lifting operator. Ultimately, the learned wavelet-like transform proposed in this paper achieves over 25% bit-rate savings compared to JPEG 2000 with compact spatial support.","sentences":["This paper provides a comprehensive study on features and performance of different ways to incorporate neural networks into lifting-based wavelet-like transforms, within the context of fully scalable and accessible image compression.","Specifically, we explore different arrangements of lifting steps, as well as various network architectures for learned lifting operators.","Moreover, we examine the impact of the number of learned lifting steps, the number of channels, the number of layers and the support of kernels in each learned lifting operator.","To facilitate the study, we investigate two generic training methodologies that are simultaneously appropriate to a wide variety of lifting structures considered.","Experimental results ultimately suggest that retaining fixed lifting steps from the base wavelet transform is highly beneficial.","Moreover, we demonstrate that employing more learned lifting steps and more layers in each learned lifting operator do not contribute strongly to the compression performance.","However, benefits can be obtained by utilizing more channels in each learned lifting operator.","Ultimately, the learned wavelet-like transform proposed in this paper achieves over 25% bit-rate savings compared to JPEG 2000 with compact spatial support."],"url":"http://arxiv.org/abs/2402.18761v1","category":"eess.IV"}
{"created":"2024-02-28 23:18:15","title":"Multi-Sensor and Multi-temporal High-Throughput Phenotyping for Monitoring and Early Detection of Water-Limiting Stress in Soybean","abstract":"Soybean production is susceptible to biotic and abiotic stresses, exacerbated by extreme weather events. Water limiting stress, i.e. drought, emerges as a significant risk for soybean production, underscoring the need for advancements in stress monitoring for crop breeding and production. This project combines multi-modal information to identify the most effective and efficient automated methods to investigate drought response. We investigated a set of diverse soybean accessions using multiple sensors in a time series high-throughput phenotyping manner to: (1) develop a pipeline for rapid classification of soybean drought stress symptoms, and (2) investigate methods for early detection of drought stress. We utilized high-throughput time-series phenotyping using UAVs and sensors in conjunction with machine learning (ML) analytics, which offered a swift and efficient means of phenotyping. The red-edge and green bands were most effective to classify canopy wilting stress. The Red-Edge Chlorophyll Vegetation Index (RECI) successfully differentiated susceptible and tolerant soybean accessions prior to visual symptom development. We report pre-visual detection of soybean wilting using a combination of different vegetation indices. These results can contribute to early stress detection methodologies and rapid classification of drought responses in screening nurseries for breeding and production applications.","sentences":["Soybean production is susceptible to biotic and abiotic stresses, exacerbated by extreme weather events.","Water limiting stress, i.e. drought, emerges as a significant risk for soybean production, underscoring the need for advancements in stress monitoring for crop breeding and production.","This project combines multi-modal information to identify the most effective and efficient automated methods to investigate drought response.","We investigated a set of diverse soybean accessions using multiple sensors in a time series high-throughput phenotyping manner to: (1) develop a pipeline for rapid classification of soybean drought stress symptoms, and (2) investigate methods for early detection of drought stress.","We utilized high-throughput time-series phenotyping using UAVs and sensors in conjunction with machine learning (ML) analytics, which offered a swift and efficient means of phenotyping.","The red-edge and green bands were most effective to classify canopy wilting stress.","The Red-Edge Chlorophyll Vegetation Index (RECI) successfully differentiated susceptible and tolerant soybean accessions prior to visual symptom development.","We report pre-visual detection of soybean wilting using a combination of different vegetation indices.","These results can contribute to early stress detection methodologies and rapid classification of drought responses in screening nurseries for breeding and production applications."],"url":"http://arxiv.org/abs/2402.18751v1","category":"cs.LG"}
{"created":"2024-02-28 23:11:22","title":"Wormholes supported by small extra dimensions","abstract":"Holding a Morris-Thorne wormhole open requires a violation of the null energy condition, calling for the need for so-called exotic matter near the throat. Many researchers consider exotic matter to be completely unphysical in classical general relativity. It has been shown, however, that the existence of an extra macroscopic dimension can resolve this issue: the throat could be lined with ordinary matter, while the extra dimension is then responsible for the unavoidable energy violation. The purpose of this paper is to show that the extra dimension can be microscopic, a result that is consistent with string theory.","sentences":["Holding a Morris-Thorne wormhole open requires a violation of the null energy condition, calling for the need for so-called exotic matter near the throat.","Many researchers consider exotic matter to be completely unphysical in classical general relativity.","It has been shown, however, that the existence of an extra macroscopic dimension can resolve this issue: the throat could be lined with ordinary matter, while the extra dimension is then responsible for the unavoidable energy violation.","The purpose of this paper is to show that the extra dimension can be microscopic, a result that is consistent with string theory."],"url":"http://arxiv.org/abs/2402.18750v1","category":"gr-qc"}
{"created":"2024-02-28 22:40:29","title":"Sixth-order parabolic equation on an interval: Eigenfunction expansion, Green's function, and intermediate asymptotics for a finite thin film with elastic resistance","abstract":"A linear sixth-order partial differential equation (PDE) of ``parabolic'' type describes the dynamics of thin liquid films beneath surfaces with elastic bending resistance when deflections from the equilibrium film height are small. On a finite domain, the associated sixth-order Sturm--Liouville eigenvalue value problem is self-adjoint for the boundary conditions corresponding to a thin film in a closed trough, and the eigenfunctions form a complete orthonormal set. Using these eigenfunctions, we derive the Green's function for the governing sixth-order PDE on a finite interval and compare it to the known infinite-line solution. Further, we propose a Galerkin spectral method based on the constructed sixth-order eigenfunctions and their derivative expansions. The system of ordinary differential equations for the time-dependent expansion coefficients is solved by standard numerical methods. The numerical approach is applied to versions of the governing PDE with a second-order derivative (in addition to the sixth-order one), which arises from gravity acting on the film. In the absence of gravity, we demonstrate the self-similar intermediate asymptotics of initially localized disturbances on the film surface, at least until the disturbances ``feel'' the finite boundaries, and show that the derived Green's function is the global attractor for such solutions. In the presence of gravity, we use the proposed spectral numerical method to demonstrate that self-similar behavior persists, albeit for shortened intervals of time, even for large values of the gravity-to-bending ratio.","sentences":["A linear sixth-order partial differential equation (PDE) of ``parabolic'' type describes the dynamics of thin liquid films beneath surfaces with elastic bending resistance when deflections from the equilibrium film height are small.","On a finite domain, the associated sixth-order Sturm--Liouville eigenvalue value problem is self-adjoint for the boundary conditions corresponding to a thin film in a closed trough, and the eigenfunctions form a complete orthonormal set.","Using these eigenfunctions, we derive the Green's function for the governing sixth-order PDE on a finite interval and compare it to the known infinite-line solution.","Further, we propose a Galerkin spectral method based on the constructed sixth-order eigenfunctions and their derivative expansions.","The system of ordinary differential equations for the time-dependent expansion coefficients is solved by standard numerical methods.","The numerical approach is applied to versions of the governing PDE with a second-order derivative (in addition to the sixth-order one), which arises from gravity acting on the film.","In the absence of gravity, we demonstrate the self-similar intermediate asymptotics of initially localized disturbances on the film surface, at least until the disturbances ``feel'' the finite boundaries, and show that the derived Green's function is the global attractor for such solutions.","In the presence of gravity, we use the proposed spectral numerical method to demonstrate that self-similar behavior persists, albeit for shortened intervals of time, even for large values of the gravity-to-bending ratio."],"url":"http://arxiv.org/abs/2402.18740v1","category":"math.NA"}
{"created":"2024-02-28 22:37:52","title":"QCD at Finite Temperature and Density -- Equation of State","abstract":"As an important set of thermodynamic quantities, knowledge of the equation of state over a broad range of temperatures and chemical potentials in the QCD phase diagram is crucial for our understanding of strongly-interacting matter. There is a good understanding from first-principles results in lattice QCD, perturbative QCD and chiral effective field theory about the equation of state. However, these approaches are valid in different regimes of the phase diagram, and therefore, a method of providing an equation of state that covers a full range of the phase diagram involves matching together these results with appropriate models in order to fill in the gaps between these regions. Furthermore, with such equations of state, important questions about QCD phase structure can begin to be addressed, such as whether there is a critical point in the QCD phase diagram. In this contribution to the proceedings, equations of state from first-principles and effective theories will be discussed in order to understand how QCD thermodynamics is affected by the presence of a critical point.","sentences":["As an important set of thermodynamic quantities, knowledge of the equation of state over a broad range of temperatures and chemical potentials in the QCD phase diagram is crucial for our understanding of strongly-interacting matter.","There is a good understanding from first-principles results in lattice QCD, perturbative QCD and chiral effective field theory about the equation of state.","However, these approaches are valid in different regimes of the phase diagram, and therefore, a method of providing an equation of state that covers a full range of the phase diagram involves matching together these results with appropriate models in order to fill in the gaps between these regions.","Furthermore, with such equations of state, important questions about QCD phase structure can begin to be addressed, such as whether there is a critical point in the QCD phase diagram.","In this contribution to the proceedings, equations of state from first-principles and effective theories will be discussed in order to understand how QCD thermodynamics is affected by the presence of a critical point."],"url":"http://arxiv.org/abs/2402.18738v1","category":"hep-ph"}
{"created":"2024-02-28 21:44:58","title":"A collocation method for nonlinear tensor differential equations on low-rank manifolds","abstract":"We present a new method to compute the solution to a nonlinear tensor differential equation with dynamical low-rank approximation. The idea of dynamical low-rank approximation is to project the differential equation onto the tangent space of a low-rank tensor manifold at each time. Traditionally, an orthogonal projection onto the tangent space is employed, which is challenging to compute for nonlinear differential equations. We introduce a novel interpolatory projection onto the tangent space that is easily computed for many nonlinear differential equations and satisfies the differential equation at a set of carefully selected indices. To select these indices, we devise a new algorithm based on the discrete empirical interpolation method (DEIM) that parameterizes any tensor train and its tangent space with tensor cross interpolants. We demonstrate the proposed method with applications to tensor differential equations arising from the discretization of partial differential equations.","sentences":["We present a new method to compute the solution to a nonlinear tensor differential equation with dynamical low-rank approximation.","The idea of dynamical low-rank approximation is to project the differential equation onto the tangent space of a low-rank tensor manifold at each time.","Traditionally, an orthogonal projection onto the tangent space is employed, which is challenging to compute for nonlinear differential equations.","We introduce a novel interpolatory projection onto the tangent space that is easily computed for many nonlinear differential equations and satisfies the differential equation at a set of carefully selected indices.","To select these indices, we devise a new algorithm based on the discrete empirical interpolation method (DEIM) that parameterizes any tensor train and its tangent space with tensor cross interpolants.","We demonstrate the proposed method with applications to tensor differential equations arising from the discretization of partial differential equations."],"url":"http://arxiv.org/abs/2402.18721v1","category":"math.NA"}
{"created":"2024-02-28 21:14:21","title":"Bluebell: An Alliance of Relational Lifting and Independence For Probabilistic Reasoning","abstract":"We present Bluebell, a program logic for reasoning about probabilistic programs where unary and relational styles of reasoning come together to create new reasoning tools. Unary-style reasoning is very expressive and is powered by foundational mechanisms to reason about probabilistic behaviour like independence and conditioning. The relational style of reasoning, on the other hand, naturally shines when the properties of interest compare the behaviour of similar programs (e.g. when proving differential privacy) managing to avoid having to characterize the output distributions of the individual programs. So far, the two styles of reasoning have largely remained separate in the many program logics designed for the deductive verification of probabilistic programs. In Bluebell, we unify these styles of reasoning through the introduction of a new modality called \"joint conditioning\" that can encode and illuminate the rich interaction between conditional independence and relational liftings; the two powerhouses from the two styles of reasoning.","sentences":["We present Bluebell, a program logic for reasoning about probabilistic programs where unary and relational styles of reasoning come together to create new reasoning tools.","Unary-style reasoning is very expressive and is powered by foundational mechanisms to reason about probabilistic behaviour like independence and conditioning.","The relational style of reasoning, on the other hand, naturally shines when the properties of interest compare the behaviour of similar programs (e.g. when proving differential privacy) managing to avoid having to characterize the output distributions of the individual programs.","So far, the two styles of reasoning have largely remained separate in the many program logics designed for the deductive verification of probabilistic programs.","In Bluebell, we unify these styles of reasoning through the introduction of a new modality called \"joint conditioning\" that can encode and illuminate the rich interaction between conditional independence and relational liftings; the two powerhouses from the two styles of reasoning."],"url":"http://arxiv.org/abs/2402.18708v1","category":"cs.LO"}
{"created":"2024-02-28 21:03:17","title":"The porous medium equation on noncompact manifolds with nonnegative Ricci curvature: a Green function approach","abstract":"We consider the porous medium equation (PME) on complete noncompact manifolds $M$ of nonnegative Ricci curvature. We require nonparabolicity of the manifold and construct a natural space $X$ of functions, strictly larger than $L^1$, in which the Green function on $M$ appears as a weight, such that the PME admits a solution in the weak dual (i.e. potential) sense whenever the initial datum $u_0$ is nonnegative and belongs to $X$. Smoothing estimates are also proved to hold both for $L^1$ data, where they take into account the volume growth of Riemannian balls giving rise to bounds which are shown to be sharp in a suitable sense, and for data belonging to $X$ as well.","sentences":["We consider the porous medium equation (PME) on complete noncompact manifolds $M$ of nonnegative Ricci curvature.","We require nonparabolicity of the manifold and construct a natural space $X$ of functions, strictly larger than $L^1$, in which the Green function on $M$ appears as a weight, such that the PME admits a solution in the weak dual (i.e. potential) sense whenever the initial datum $u_0$ is nonnegative and belongs to $X$. Smoothing estimates are also proved to hold both for $L^1$ data, where they take into account the volume growth of Riemannian balls giving rise to bounds which are shown to be sharp in a suitable sense, and for data belonging to $X$ as well."],"url":"http://arxiv.org/abs/2402.18706v1","category":"math.AP"}
{"created":"2024-02-28 20:24:26","title":"Exploring the distribution and impact of bosonic dark matter in neutron stars","abstract":"The presence of dark matter (DM) within neutron stars (NSs) can be introduced by different accumulation scenarios in which DM and baryonic matter (BM) may interact only through the gravitational force. In this work, we consider asymmetric self-interacting bosonic DM which can reside as a dense core inside the NS or form an extended halo around it. It is seen that depending on the boson mass ($m_{\\chi}$), self-coupling constant ($\\lambda$) and DM fraction ($F_{\\chi}$), the maximum mass, radius and tidal deformability of NSs with DM admixture will be altered significantly. The impact of DM causes some modifications in the observable features induced solely by the BM component. Here, we focus on the widely used nuclear matter equation of state (EoS) called DD2 for describing NS matter. We show that by involving DM in NSs, the corresponding observational parameters will be changed to be consistent with the latest multi-messenger observations of NSs. It is seen that for $m_{\\chi}\\gtrsim200$ MeV and $\\lambda\\lesssim2\\pi$, DM admixed NSs with $4\\%\\lesssim F_{\\chi}\\lesssim20\\%$ are consistent with the maximum mass and tidal deformability constraints.","sentences":["The presence of dark matter (DM) within neutron stars (NSs) can be introduced by different accumulation scenarios in which DM and baryonic matter (BM) may interact only through the gravitational force.","In this work, we consider asymmetric self-interacting bosonic DM which can reside as a dense core inside the NS or form an extended halo around it.","It is seen that depending on the boson mass ($m_{\\chi}$), self-coupling constant ($\\lambda$) and DM fraction ($F_{\\chi}$), the maximum mass, radius and tidal deformability of NSs with DM admixture will be altered significantly.","The impact of DM causes some modifications in the observable features induced solely by the BM component.","Here, we focus on the widely used nuclear matter equation of state (EoS) called DD2 for describing NS matter.","We show that by involving DM in NSs, the corresponding observational parameters will be changed to be consistent with the latest multi-messenger observations of NSs.","It is seen that for $m_{\\chi}\\gtrsim200$ MeV and $\\lambda\\lesssim2\\pi$, DM admixed NSs with $4\\%\\lesssim F_{\\chi}\\lesssim20\\%$ are consistent with the maximum mass and tidal deformability constraints."],"url":"http://arxiv.org/abs/2402.18696v1","category":"astro-ph.HE"}
{"created":"2024-02-28 19:14:12","title":"Parametrically long lifetime of superdiffusion in non-integrable spin chains","abstract":"Superdiffusion is surprisingly easily observed even in systems without the integrability underpinning this phenomenon. Indeed, the classical Heisenberg chain -- one of the simplest many-body systems, and firmly believed to be non-integrable -- evinces a long-lived regime of anomalous, superdiffusive spin dynamics at finite temperature. Similarly, superdiffusion persists for long timescales, even at high temperature, for small perturbations around a related integrable model. Eventually, however, ordinary diffusion is believed to be asymptotically restored. We examine the timescales governing the lifetime of the superdiffusive regime, and argue that it diverges algebraically fast -- both in deviation from the integrable limit, and at low temperature, where we find $t^* \\sim T^{-\\zeta}$ with an exponent possibly as large as $\\zeta = 8$. This can render the crossover to ordinary diffusion practically inaccessible.","sentences":["Superdiffusion is surprisingly easily observed even in systems without the integrability underpinning this phenomenon.","Indeed, the classical Heisenberg chain -- one of the simplest many-body systems, and firmly believed to be non-integrable -- evinces a long-lived regime of anomalous, superdiffusive spin dynamics at finite temperature.","Similarly, superdiffusion persists for long timescales, even at high temperature, for small perturbations around a related integrable model.","Eventually, however, ordinary diffusion is believed to be asymptotically restored.","We examine the timescales governing the lifetime of the superdiffusive regime, and argue that it diverges algebraically fast -- both in deviation from the integrable limit, and at low temperature, where we find $t^* \\sim T^{-\\zeta}$ with an exponent possibly as large as $\\zeta = 8$.","This can render the crossover to ordinary diffusion practically inaccessible."],"url":"http://arxiv.org/abs/2402.18662v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-28 19:01:00","title":"CHEX-MATE: Robust reconstruction of temperature profiles in galaxy clusters with XMM-Newton","abstract":"The \"Cluster HEritage project with \\xmm: Mass Assembly and Thermodynamics at the Endpoint of structure formation\" (CHEX-MATE) is a multi-year Heritage program, to obtain homogeneous XMM-Newton observations of a representative sample of 118 galaxy clusters. The observations are tuned to reconstruct the distribution of the main thermodynamic quantities of the ICM up to $R_{500}$ and to obtain individual mass measurements, via the hydrostatic-equilibrium equation, with a precision of 15-20%. Temperature profiles are a necessary ingredient for the scientific goals of the project and it is thus crucial to derive the best possible temperature measurements from our data. This is why we have built a new pipeline for spectral extraction and analysis of XMM-Newton data, based on a new physically motivated background model and on a Bayesian approach with Markov Chain Monte Carlo (MCMC) methods, that we present in this paper for the first time. We applied this new method to a subset of 30 galaxy clusters representative of the CHEX-MATE sample and show that we can obtain reliable temperature measurements up to regions where the source intensity is as low as 20% of the background, keeping systematic errors below 10%. We compare the median profile of our sample and the best fit slope at large radii with literature results and we find a good agreement with other measurements based on XMM-Newton data. Conversely, when we exclude from our analysis the most contaminated regions, where the source intensity is below 20 of the background, we find significantly flatter profiles, in agreement with predictions from numerical simulations and independent measurements with a combination of Sunyaev-Zeldovich and X-ray imaging data.","sentences":["The \"Cluster HEritage project with \\xmm: Mass Assembly and Thermodynamics at the Endpoint of structure formation\" (CHEX-MATE) is a multi-year Heritage program, to obtain homogeneous XMM-Newton observations of a representative sample of 118 galaxy clusters.","The observations are tuned to reconstruct the distribution of the main thermodynamic quantities of the ICM up to $R_{500}$ and to obtain individual mass measurements, via the hydrostatic-equilibrium equation, with a precision of 15-20%.","Temperature profiles are a necessary ingredient for the scientific goals of the project and it is thus crucial to derive the best possible temperature measurements from our data.","This is why we have built a new pipeline for spectral extraction and analysis of XMM-Newton data, based on a new physically motivated background model and on a Bayesian approach with Markov Chain Monte Carlo (MCMC) methods, that we present in this paper for the first time.","We applied this new method to a subset of 30 galaxy clusters representative of the CHEX-MATE sample and show that we can obtain reliable temperature measurements up to regions where the source intensity is as low as 20% of the background, keeping systematic errors below 10%.","We compare the median profile of our sample and the best fit slope at large radii with literature results and we find a good agreement with other measurements based on XMM-Newton data.","Conversely, when we exclude from our analysis the most contaminated regions, where the source intensity is below 20 of the background, we find significantly flatter profiles, in agreement with predictions from numerical simulations and independent measurements with a combination of Sunyaev-Zeldovich and X-ray imaging data."],"url":"http://arxiv.org/abs/2402.18653v1","category":"astro-ph.CO"}
{"created":"2024-02-28 19:00:01","title":"El Gordo needs El Anzuelo: Probing the structure of cluster members with multi-band extended arcs in JWST data","abstract":"Gravitational lensing by galaxy clusters involves hundreds of galaxies over a large redshift range and increases the likelihood of rare phenomena (supernovae, microlensing, dark substructures, etc.). Characterizing the mass and light distributions of foreground and background objects often requires a combination of high-resolution data and advanced modeling techniques. We present the detailed analysis of El Anzuelo, a prominent quintuply imaged dusty star forming galaxy ($z_{\\rm s}=2.29$), mainly lensed by three members of the massive galaxy cluster ACT-CL$\\,$J0102$-$4915, also known as El Gordo ($z_{\\rm d}=0.87$). We leverage JWST/NIRCam data containing previously unseen lensing features using a Bayesian, multi-wavelength, differentiable and GPU-accelerated modeling framework that combines Herculens (lens modeling) and NIFTy (field model and inference) software packages. For one of the deflectors, we complement lensing constraints with stellar kinematics measured from VLT/MUSE data. In our lens model, we explicitly include the mass distribution of the cluster, locally corrected by a constant shear field. We find that the two main deflectors (L1 and L2) have logarithmic mass density slopes steeper than isothermal, with $\\gamma_{\\rm L1} = 2.23\\pm0.05$ and $\\gamma_{\\rm L2} = 2.21\\pm0.04$. We argue that such steep density profiles can arise due to tidally truncated mass distributions, which we probe thanks to the cluster lensing boost and the strong asymmetry of the lensing configuration. Moreover, our three-dimensional source model captures most of the surface brightness of the lensed galaxy, revealing a clump of at most $400$ parsecs at the source redshift, visible at wavelengths $\\lambda_{\\rm rest}\\gtrsim0.6$ $\\mu$m. Finally, we caution on using point-like features within extended arcs to constrain galaxy-scale lens models before securing them with extended arc modeling.","sentences":["Gravitational lensing by galaxy clusters involves hundreds of galaxies over a large redshift range and increases the likelihood of rare phenomena (supernovae, microlensing, dark substructures, etc.).","Characterizing the mass and light distributions of foreground and background objects often requires a combination of high-resolution data and advanced modeling techniques.","We present the detailed analysis of El Anzuelo, a prominent quintuply imaged dusty star forming galaxy ($z_{\\rm s}=2.29$), mainly lensed by three members of the massive galaxy cluster ACT-CL$\\,$J0102$-$4915, also known as El Gordo ($z_{\\rm d}=0.87$).","We leverage JWST/NIRCam data containing previously unseen lensing features using a Bayesian, multi-wavelength, differentiable and GPU-accelerated modeling framework that combines Herculens (lens modeling) and NIFTy (field model and inference) software packages.","For one of the deflectors, we complement lensing constraints with stellar kinematics measured from VLT/MUSE data.","In our lens model, we explicitly include the mass distribution of the cluster, locally corrected by a constant shear field.","We find that the two main deflectors (L1 and L2) have logarithmic mass density slopes steeper than isothermal, with $\\gamma_{\\rm L1} = 2.23\\pm0.05$ and $\\gamma_{\\rm L2} = 2.21\\pm0.04$.","We argue that such steep density profiles can arise due to tidally truncated mass distributions, which we probe thanks to the cluster lensing boost and the strong asymmetry of the lensing configuration.","Moreover, our three-dimensional source model captures most of the surface brightness of the lensed galaxy, revealing a clump of at most $400$ parsecs at the source redshift, visible at wavelengths $\\lambda_{\\rm rest}\\gtrsim0.6$ $\\mu$m.","Finally, we caution on using point-like features within extended arcs to constrain galaxy-scale lens models before securing them with extended arc modeling."],"url":"http://arxiv.org/abs/2402.18636v1","category":"astro-ph.GA"}
{"created":"2024-02-28 19:00:00","title":"Using Rest-Frame Optical and NIR Data from the RAISIN Survey to Explore the Redshift Evolution of Dust Laws in SN Ia Host Galaxies","abstract":"We use rest-frame optical and near-infrared (NIR) observations of 42 Type Ia supernovae (SNe Ia) from the Carnegie Supernova Project at low-$z$ and 37 from the RAISIN Survey at high-$z$ to investigate correlations between SN Ia host galaxy dust, host mass, and redshift. This is the first time the SN Ia host galaxy dust extinction law at high-$z$ has been estimated using combined optical and rest-frame NIR data ($YJ$-band). We use the BayeSN hierarchical model to leverage the data's wide rest-frame wavelength range (extending to $\\sim$1.0-1.2 microns for the RAISIN sample at $0.2\\lesssim z\\lesssim0.6$). By contrasting the RAISIN and CSP data, we constrain the population distributions of the host dust $R_V$ parameter for both redshift ranges. We place a limit on the difference in population mean $R_V$ between RAISIN and CSP of $-1.16<\\Delta\\mu(R_V)<1.38$ with 95% posterior probability. For RAISIN we estimate $\\mu(R_V)=2.58\\pm0.57$, and constrain the population standard deviation to $\\sigma(R_V)<0.90~[2.42]$ at the 68 [95]% level. Given that we are only able to constrain the size of the low- to high-$z$ shift in $\\mu(R_V)$ to $\\lesssim1.4$ - which could still propagate to a substantial bias in the equation of state parameter $w$ - these and other recent results motivate continued effort to obtain rest-frame NIR data at low and high redshifts (e.g. using the Roman Space Telescope).","sentences":["We use rest-frame optical and near-infrared (NIR) observations of 42 Type Ia supernovae (SNe Ia) from the Carnegie Supernova Project at low-$z$ and 37 from the RAISIN Survey at high-$z$ to investigate correlations between SN Ia host galaxy dust, host mass, and redshift.","This is the first time the SN Ia host galaxy dust extinction law at high-$z$ has been estimated using combined optical and rest-frame NIR data ($YJ$-band).","We use the BayeSN hierarchical model to leverage the data's wide rest-frame wavelength range (extending to $\\sim$1.0-1.2 microns for the RAISIN sample at $0.2\\lesssim z\\lesssim0.6$).","By contrasting the RAISIN and CSP data, we constrain the population distributions of the host dust $R_V$ parameter for both redshift ranges.","We place a limit on the difference in population mean $R_V$ between RAISIN and CSP of $-1.16<\\Delta\\mu(R_V)<1.38$ with 95% posterior probability.","For RAISIN we estimate $\\mu(R_V)=2.58\\pm0.57$, and constrain the population standard deviation to $\\sigma(R_V)<0.90~[2.42]$ at the 68 [95]% level.","Given that we are only able to constrain the size of the low- to high-$z$ shift in $\\mu(R_V)$ to $\\lesssim1.4$ - which could still propagate to a substantial bias in the equation of state parameter $w$ - these and other recent results motivate continued effort to obtain rest-frame NIR data at low and high redshifts (e.g. using the Roman Space Telescope)."],"url":"http://arxiv.org/abs/2402.18624v1","category":"astro-ph.CO"}
{"created":"2024-02-28 19:00:00","title":"Light quark mediated Higgs boson production in association with a jet at the next-to-next-leading order and beyond","abstract":"We study the light quark effect on the Higgs boson production in association with a jet at the LHC in the intermediate transverse momentum region between the quark and the Higgs boson mass scales. Though the effect is suppressed by the small Yukawa coupling, it is enhanced by large logarithms of the quark mass ratio to the Higgs boson mass or transverse momentum. Following a remarkable success of the logarithmic expansion [39] for the prediction of the next-to-next-to-leading bottom quark contribution to the total cross section of the Higgs boson production we extend the analysis to its kinematical distributions. A new factorization formula is derived for the light quark mediated $gg\\to Hg$ amplitudes and the differential cross section of the process is computed in the logarithmic approximation, which is used for an estimate of the bottom quark effect at the next-to-next-to-leading order.","sentences":["We study the light quark effect on the Higgs boson production in association with a jet at the LHC in the intermediate transverse momentum region between the quark and the Higgs boson mass scales.","Though the effect is suppressed by the small Yukawa coupling, it is enhanced by large logarithms of the quark mass ratio to the Higgs boson mass or transverse momentum.","Following a remarkable success of the logarithmic expansion [39] for the prediction of the next-to-next-to-leading bottom quark contribution to the total cross section of the Higgs boson production we extend the analysis to its kinematical distributions.","A new factorization formula is derived for the light quark mediated $gg\\to Hg$ amplitudes and the differential cross section of the process is computed in the logarithmic approximation, which is used for an estimate of the bottom quark effect at the next-to-next-to-leading order."],"url":"http://arxiv.org/abs/2402.18625v1","category":"hep-ph"}
{"created":"2024-02-28 15:15:38","title":"HemaGraph: Breaking Barriers in Hematologic Single Cell Classification with Graph Attention","abstract":"In the realm of hematologic cell populations classification, the intricate patterns within flow cytometry data necessitate advanced analytical tools. This paper presents 'HemaGraph', a novel framework based on Graph Attention Networks (GATs) for single-cell multi-class classification of hematological cells from flow cytometry data. Harnessing the power of GATs, our method captures subtle cell relationships, offering highly accurate patient profiling. Based on evaluation of data from 30 patients, HemaGraph demonstrates classification performance across five different cell classes, outperforming traditional methodologies and state-of-the-art methods. Moreover, the uniqueness of this framework lies in the training and testing phase of HemaGraph, where it has been applied for extremely large graphs, containing up to hundreds of thousands of nodes and two million edges, to detect low frequency cell populations (e.g. 0.01% for one population), with accuracies reaching 98%. Our findings underscore the potential of HemaGraph in improving hematoligic multi-class classification, paving the way for patient-personalized interventions. To the best of our knowledge, this is the first effort to use GATs, and Graph Neural Networks (GNNs) in general, to classify cell populations from single-cell flow cytometry data. We envision applying this method to single-cell data from larger cohort of patients and on other hematologic diseases.","sentences":["In the realm of hematologic cell populations classification, the intricate patterns within flow cytometry data necessitate advanced analytical tools.","This paper presents 'HemaGraph', a novel framework based on Graph Attention Networks (GATs) for single-cell multi-class classification of hematological cells from flow cytometry data.","Harnessing the power of GATs, our method captures subtle cell relationships, offering highly accurate patient profiling.","Based on evaluation of data from 30 patients, HemaGraph demonstrates classification performance across five different cell classes, outperforming traditional methodologies and state-of-the-art methods.","Moreover, the uniqueness of this framework lies in the training and testing phase of HemaGraph, where it has been applied for extremely large graphs, containing up to hundreds of thousands of nodes and two million edges, to detect low frequency cell populations (e.g. 0.01% for one population), with accuracies reaching 98%.","Our findings underscore the potential of HemaGraph in improving hematoligic multi-class classification, paving the way for patient-personalized interventions.","To the best of our knowledge, this is the first effort to use GATs, and Graph Neural Networks (GNNs) in general, to classify cell populations from single-cell flow cytometry data.","We envision applying this method to single-cell data from larger cohort of patients and on other hematologic diseases."],"url":"http://arxiv.org/abs/2402.18611v1","category":"q-bio.QM"}
{"created":"2024-02-28 15:10:25","title":"Why Attention Graphs Are All We Need: Pioneering Hierarchical Classification of Hematologic Cell Populations with LeukoGraph","abstract":"In the complex landscape of hematologic samples such as peripheral blood or bone marrow, cell classification, delineating diverse populations into a hierarchical structure, presents profound challenges. This study presents LeukoGraph, a recently developed framework designed explicitly for this purpose employing graph attention networks (GATs) to navigate hierarchical classification (HC) complexities. Notably, LeukoGraph stands as a pioneering effort, marking the application of graph neural networks (GNNs) for hierarchical inference on graphs, accommodating up to one million nodes and millions of edges, all derived from flow cytometry data. LeukoGraph intricately addresses a classification paradigm where for example four different cell populations undergo flat categorization, while a fifth diverges into two distinct child branches, exemplifying the nuanced hierarchical structure inherent in complex datasets. The technique is more general than this example. A hallmark achievement of LeukoGraph is its F-score of 98%, significantly outclassing prevailing state-of-the-art methodologies. Crucially, LeukoGraph's prowess extends beyond theoretical innovation, showcasing remarkable precision in predicting both flat and hierarchical cell types across flow cytometry datasets from 30 distinct patients. This precision is further underscored by LeukoGraph's ability to maintain a correct label ratio, despite the inherent challenges posed by hierarchical classifications.","sentences":["In the complex landscape of hematologic samples such as peripheral blood or bone marrow, cell classification, delineating diverse populations into a hierarchical structure, presents profound challenges.","This study presents LeukoGraph, a recently developed framework designed explicitly for this purpose employing graph attention networks (GATs) to navigate hierarchical classification (HC) complexities.","Notably, LeukoGraph stands as a pioneering effort, marking the application of graph neural networks (GNNs) for hierarchical inference on graphs, accommodating up to one million nodes and millions of edges, all derived from flow cytometry data.","LeukoGraph intricately addresses a classification paradigm where for example four different cell populations undergo flat categorization, while a fifth diverges into two distinct child branches, exemplifying the nuanced hierarchical structure inherent in complex datasets.","The technique is more general than this example.","A hallmark achievement of LeukoGraph is its F-score of 98%, significantly outclassing prevailing state-of-the-art methodologies.","Crucially, LeukoGraph's prowess extends beyond theoretical innovation, showcasing remarkable precision in predicting both flat and hierarchical cell types across flow cytometry datasets from 30 distinct patients.","This precision is further underscored by LeukoGraph's ability to maintain a correct label ratio, despite the inherent challenges posed by hierarchical classifications."],"url":"http://arxiv.org/abs/2402.18610v1","category":"cs.LG"}
{"created":"2024-02-29 18:58:26","title":"Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress","abstract":"Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: Sort \\& Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across 31,000 models demonstrate that S&S achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours (1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong benchmarks offer a robust, practical solution to the \"benchmark exhaustion\" problem.","sentences":["Standardized benchmarks drive progress in machine learning.","However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies.","In our work, we seek to mitigate this challenge by compiling ever-expanding large-scale benchmarks called Lifelong Benchmarks.","As exemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet, containing (for now) 1.69M and 1.98M test samples, respectively.","While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set.","To address this challenge, we also introduce an efficient evaluation framework: Sort \\& Search (S&S), which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking.","Extensive empirical evaluations across 31,000 models demonstrate that S&S achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours (1000x reduction) on a single A100 GPU, with low approximation error.","As such, lifelong benchmarks offer a robust, practical solution to the \"benchmark exhaustion\" problem."],"url":"http://arxiv.org/abs/2402.19472v1","category":"cs.LG"}
{"created":"2024-02-29 18:54:53","title":"SeMoLi: What Moves Together Belongs Together","abstract":"We tackle semi-supervised object detection based on motion cues. Recent results suggest that heuristic-based clustering methods in conjunction with object trackers can be used to pseudo-label instances of moving objects and use these as supervisory signals to train 3D object detectors in Lidar data without manual supervision. We re-think this approach and suggest that both, object detection, as well as motion-inspired pseudo-labeling, can be tackled in a data-driven manner. We leverage recent advances in scene flow estimation to obtain point trajectories from which we extract long-term, class-agnostic motion patterns. Revisiting correlation clustering in the context of message passing networks, we learn to group those motion patterns to cluster points to object instances. By estimating the full extent of the objects, we obtain per-scan 3D bounding boxes that we use to supervise a Lidar object detection network. Our method not only outperforms prior heuristic-based approaches (57.5 AP, +14 improvement over prior work), more importantly, we show we can pseudo-label and train object detectors across datasets.","sentences":["We tackle semi-supervised object detection based on motion cues.","Recent results suggest that heuristic-based clustering methods in conjunction with object trackers can be used to pseudo-label instances of moving objects and use these as supervisory signals to train 3D object detectors in Lidar data without manual supervision.","We re-think this approach and suggest that both, object detection, as well as motion-inspired pseudo-labeling, can be tackled in a data-driven manner.","We leverage recent advances in scene flow estimation to obtain point trajectories from which we extract long-term, class-agnostic motion patterns.","Revisiting correlation clustering in the context of message passing networks, we learn to group those motion patterns to cluster points to object instances.","By estimating the full extent of the objects, we obtain per-scan 3D bounding boxes that we use to supervise a Lidar object detection network.","Our method not only outperforms prior heuristic-based approaches (57.5 AP, +14 improvement over prior work), more importantly, we show we can pseudo-label and train object detectors across datasets."],"url":"http://arxiv.org/abs/2402.19463v1","category":"cs.CV"}
{"created":"2024-02-29 18:47:52","title":"Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models","abstract":"Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens. We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics. When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones. As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent. On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes. To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models. We further study this phenomenon on a linear classification with cross-entropy loss, showing that heavy-tailed class imbalance leads to ill-conditioning, and that the normalization used by Adam can counteract it.","sentences":["Adam has been shown to outperform gradient descent in optimizing large language transformers empirically, and by a larger margin than on other tasks, but it is unclear why this happens.","We show that the heavy-tailed class imbalance found in language modeling tasks leads to difficulties in the optimization dynamics.","When training with gradient descent, the loss associated with infrequent words decreases slower than the loss associated with frequent ones.","As most samples come from relatively infrequent words, the average loss decreases slowly with gradient descent.","On the other hand, Adam and sign-based methods do not suffer from this problem and improve predictions on all classes.","To establish that this behavior is indeed caused by class imbalance, we show empirically that it persist through different architectures and data types, on language transformers, vision CNNs, and linear models.","We further study this phenomenon on a linear classification with cross-entropy loss, showing that heavy-tailed class imbalance leads to ill-conditioning, and that the normalization used by Adam can counteract it."],"url":"http://arxiv.org/abs/2402.19449v1","category":"cs.LG"}
{"created":"2024-02-29 18:14:11","title":"Vision-Radio Experimental Infrastructure Architecture Towards 6G","abstract":"Telecommunications and computer vision have evolved separately so far. Yet, with the shift to sub-terahertz (sub-THz) and terahertz (THz) radio communications, there is an opportunity to explore computer vision technologies together with radio communications, considering the dependency of both technologies on Line of Sight. The combination of radio sensing and computer vision can address challenges such as obstructions and poor lighting. Also, machine learning algorithms, capable of processing multimodal data, play a crucial role in deriving insights from raw and low-level sensing data, offering a new level of abstraction that can enhance various applications and use cases such as beamforming and terminal handovers.   This paper introduces CONVERGE, a pioneering vision-radio paradigm that bridges this gap by leveraging Integrated Sensing and Communication (ISAC) to facilitate a dual \"View-to-Communicate, Communicate-to-View\" approach. CONVERGE offers tools that merge wireless communications and computer vision, establishing a novel Research Infrastructure (RI) that will be open to the scientific community and capable of providing open datasets. This new infrastructure will support future research in 6G and beyond concerning multiple verticals, such as telecommunications, automotive, manufacturing, media, and health.","sentences":["Telecommunications and computer vision have evolved separately so far.","Yet, with the shift to sub-terahertz (sub-THz) and terahertz (THz) radio communications, there is an opportunity to explore computer vision technologies together with radio communications, considering the dependency of both technologies on Line of Sight.","The combination of radio sensing and computer vision can address challenges such as obstructions and poor lighting.","Also, machine learning algorithms, capable of processing multimodal data, play a crucial role in deriving insights from raw and low-level sensing data, offering a new level of abstraction that can enhance various applications and use cases such as beamforming and terminal handovers.   ","This paper introduces CONVERGE, a pioneering vision-radio paradigm that bridges this gap by leveraging Integrated Sensing and Communication (ISAC) to facilitate a dual \"View-to-Communicate, Communicate-to-View\" approach.","CONVERGE offers tools that merge wireless communications and computer vision, establishing a novel Research Infrastructure (RI) that will be open to the scientific community and capable of providing open datasets.","This new infrastructure will support future research in 6G and beyond concerning multiple verticals, such as telecommunications, automotive, manufacturing, media, and health."],"url":"http://arxiv.org/abs/2402.19416v1","category":"cs.NI"}
{"created":"2024-02-29 16:15:34","title":"Loss-Free Machine Unlearning","abstract":"We present a machine unlearning approach that is both retraining- and label-free. Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance. This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model. Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available. Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity. We evaluate our method in a range of experiments using ResNet18 and Vision Transformer. Results show our label-free method is competitive with existing state-of-the-art approaches.","sentences":["We present a machine unlearning approach that is both retraining- and label-free.","Most existing machine unlearning approaches require a model to be fine-tuned to remove information while preserving performance.","This is computationally expensive and necessitates the storage of the whole dataset for the lifetime of the model.","Retraining-free approaches often utilise Fisher information, which is derived from the loss and requires labelled data which may not be available.","Thus, we present an extension to the Selective Synaptic Dampening algorithm, substituting the diagonal of the Fisher information matrix for the gradient of the l2 norm of the model output to approximate sensitivity.","We evaluate our method in a range of experiments using ResNet18 and Vision Transformer.","Results show our label-free method is competitive with existing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.19308v1","category":"cs.LG"}
{"created":"2024-02-29 16:12:56","title":"New Pathways in Neutrino Physics via Quantum-Encoded Data Analysis","abstract":"Ever-increasing amount of data is produced by particle detectors in their quest to unveil the laws of Nature. The large data rate requires the use of specialized triggers that promptly reduce the data rate to a manageable level; however, in doing so, unexpected new phenomena may escape detection. Additionally, the large data rate is increasingly difficult to analyze effectively, which has led to a recent revolution on machine learning techniques. Here, we present a methodology based on recent quantum compression techniques that has the capacity to store exponentially more amount of information than classically available methods. To demonstrate this, we encode the full neutrino telescope event information using parity observables in an IBM quantum processor using 8 qubits. Then we show that we can recover the information stored on the quantum computer with a fidelity of 84%. Finally, we illustrate the use of our protocol by performing a classification task that separates electron-neutrino events to muon-neutrinos events in a neutrino telescope. This new capability would eventually allow us to solve the street light effect in particle physics, where we only record signatures of particles with which we are familiar.","sentences":["Ever-increasing amount of data is produced by particle detectors in their quest to unveil the laws of Nature.","The large data rate requires the use of specialized triggers that promptly reduce the data rate to a manageable level; however, in doing so, unexpected new phenomena may escape detection.","Additionally, the large data rate is increasingly difficult to analyze effectively, which has led to a recent revolution on machine learning techniques.","Here, we present a methodology based on recent quantum compression techniques that has the capacity to store exponentially more amount of information than classically available methods.","To demonstrate this, we encode the full neutrino telescope event information using parity observables in an IBM quantum processor using 8 qubits.","Then we show that we can recover the information stored on the quantum computer with a fidelity of 84%.","Finally, we illustrate the use of our protocol by performing a classification task that separates electron-neutrino events to muon-neutrinos events in a neutrino telescope.","This new capability would eventually allow us to solve the street light effect in particle physics, where we only record signatures of particles with which we are familiar."],"url":"http://arxiv.org/abs/2402.19306v1","category":"hep-ex"}
{"created":"2024-02-29 15:39:40","title":"Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching","abstract":"Geometric knowledge has been shown to be beneficial for the stereo matching task. However, prior attempts to integrate geometric insights into stereo matching algorithms have largely focused on geometric knowledge from single images while crucial cross-view factors such as occlusion and matching uniqueness have been overlooked. To address this gap, we propose a novel Intra-view and Cross-view Geometric knowledge learning Network (ICGNet), specifically crafted to assimilate both intra-view and cross-view geometric knowledge. ICGNet harnesses the power of interest points to serve as a channel for intra-view geometric understanding. Simultaneously, it employs the correspondences among these points to capture cross-view geometric relationships. This dual incorporation empowers the proposed ICGNet to leverage both intra-view and cross-view geometric knowledge in its learning process, substantially improving its ability to estimate disparities. Our extensive experiments demonstrate the superiority of the ICGNet over contemporary leading models.","sentences":["Geometric knowledge has been shown to be beneficial for the stereo matching task.","However, prior attempts to integrate geometric insights into stereo matching algorithms have largely focused on geometric knowledge from single images while crucial cross-view factors such as occlusion and matching uniqueness have been overlooked.","To address this gap, we propose a novel Intra-view and Cross-view Geometric knowledge learning Network (ICGNet), specifically crafted to assimilate both intra-view and cross-view geometric knowledge.","ICGNet harnesses the power of interest points to serve as a channel for intra-view geometric understanding.","Simultaneously, it employs the correspondences among these points to capture cross-view geometric relationships.","This dual incorporation empowers the proposed ICGNet to leverage both intra-view and cross-view geometric knowledge in its learning process, substantially improving its ability to estimate disparities.","Our extensive experiments demonstrate the superiority of the ICGNet over contemporary leading models."],"url":"http://arxiv.org/abs/2402.19270v1","category":"cs.CV"}
{"created":"2024-02-29 15:26:03","title":"Machine learning for modular multiplication","abstract":"Motivated by cryptographic applications, we investigate two machine learning approaches to modular multiplication: namely circular regression and a sequence-to-sequence transformer model. The limited success of both methods demonstrated in our results gives evidence for the hardness of tasks involving modular multiplication upon which cryptosystems are based.","sentences":["Motivated by cryptographic applications, we investigate two machine learning approaches to modular multiplication: namely circular regression and a sequence-to-sequence transformer model.","The limited success of both methods demonstrated in our results gives evidence for the hardness of tasks involving modular multiplication upon which cryptosystems are based."],"url":"http://arxiv.org/abs/2402.19254v1","category":"cs.LG"}
{"created":"2024-02-29 14:58:15","title":"Investigating Gender Fairness in Machine Learning-driven Personalized Care for Chronic Pain","abstract":"This study investigates gender fairness in personalized pain care recommendations using machine learning algorithms. Leveraging a contextual bandits framework, personalized recommendations are formulated and evaluated using LinUCB algorithm on a dataset comprising interactions with $164$ patients across $10$ sessions each. Results indicate that while adjustments to algorithm parameters influence the quality of pain care recommendations, this impact remains consistent across genders. However, when certain patient information, such as self-reported pain measurements, is absent, the quality of pain care recommendations for women is notably inferior to that for men.","sentences":["This study investigates gender fairness in personalized pain care recommendations using machine learning algorithms.","Leveraging a contextual bandits framework, personalized recommendations are formulated and evaluated using LinUCB algorithm on a dataset comprising interactions with $164$ patients across $10$ sessions each.","Results indicate that while adjustments to algorithm parameters influence the quality of pain care recommendations, this impact remains consistent across genders.","However, when certain patient information, such as self-reported pain measurements, is absent, the quality of pain care recommendations for women is notably inferior to that for men."],"url":"http://arxiv.org/abs/2402.19226v1","category":"cs.LG"}
{"created":"2024-02-29 14:34:43","title":"DeepEMC-T2 Mapping: Deep Learning-Enabled T2 Mapping Based on Echo Modulation Curve Modeling","abstract":"Purpose: Echo modulation curve (EMC) modeling can provide accurate and reproducible quantification of T2 relaxation times. The standard EMC-T2 mapping framework, however, requires sufficient echoes and cumbersome pixel-wise dictionary-matching steps. This work proposes a deep learning version of EMC-T2 mapping, called DeepEMC-T2 mapping, to efficiently estimate accurate T2 maps from fewer echoes without a dictionary.   Methods: DeepEMC-T2 mapping was developed using a modified U-Net to estimate both T2 and Proton Density (PD) maps directly from multi-echo spin-echo (MESE) images. The modified U-Net employs several new features to improve the accuracy of T2/PD estimation. MESE datasets from 68 subjects were used for training and evaluation of the DeepEMC-T2 mapping technique. Multiple experiments were conducted to evaluate the impact of the proposed new features on DeepEMC-T2 mapping.   Results: DeepEMC-T2 mapping achieved T2 estimation errors ranging from 3%-12% in different T2 ranges and 0.8%-1.7% for PD estimation with 10/7/5/3 echoes, which yielded more accurate parameter estimation than standard EMC-T2 mapping. The new features proposed in DeepEMC-T2 mapping enabled improved parameter estimation. The use of a larger echo spacing with fewer echoes can maintain the accuracy of T2 and PD estimations while reducing the number of 180-degree refocusing pulses.   Conclusions: DeepEMC-T2 mapping enables simplified, efficient, and accurate T2 quantification directly from MESE images without a time-consuming dictionary-matching step and requires fewer echoes. This allows for increased volumetric coverage and/or decreased SAR by reducing the number of 180-degree refocusing pulses.","sentences":["Purpose: Echo modulation curve (EMC) modeling can provide accurate and reproducible quantification of T2 relaxation times.","The standard EMC-T2 mapping framework, however, requires sufficient echoes and cumbersome pixel-wise dictionary-matching steps.","This work proposes a deep learning version of EMC-T2 mapping, called DeepEMC-T2 mapping, to efficiently estimate accurate T2 maps from fewer echoes without a dictionary.   ","Methods: DeepEMC-T2 mapping was developed using a modified U-Net to estimate both T2 and Proton Density (PD) maps directly from multi-echo spin-echo (MESE) images.","The modified U-Net employs several new features to improve the accuracy of T2/PD estimation.","MESE datasets from 68 subjects were used for training and evaluation of the DeepEMC-T2 mapping technique.","Multiple experiments were conducted to evaluate the impact of the proposed new features on DeepEMC-T2 mapping.   ","Results: DeepEMC-T2 mapping achieved T2 estimation errors ranging from 3%-12% in different T2 ranges and 0.8%-1.7% for PD estimation with 10/7/5/3 echoes, which yielded more accurate parameter estimation than standard EMC-T2 mapping.","The new features proposed in DeepEMC-T2 mapping enabled improved parameter estimation.","The use of a larger echo spacing with fewer echoes can maintain the accuracy of T2 and PD estimations while reducing the number of 180-degree refocusing pulses.   ","Conclusions: DeepEMC-T2 mapping enables simplified, efficient, and accurate T2 quantification directly from MESE images without a time-consuming dictionary-matching step and requires fewer echoes.","This allows for increased volumetric coverage and/or decreased SAR by reducing the number of 180-degree refocusing pulses."],"url":"http://arxiv.org/abs/2402.19205v1","category":"eess.IV"}
{"created":"2024-02-29 13:38:07","title":"Beyond Language Models: Byte Models are Digital World Simulators","abstract":"Traditional deep learning often overlooks bytes, the basic units of the digital world, where all forms of information and operations are encoded and manipulated in binary format. Inspired by the success of next token prediction in natural language processing, we introduce bGPT, a model with next byte prediction to simulate the digital world. bGPT matches specialized models in performance across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithm or hardware behaviour. It has almost flawlessly replicated the process of converting symbolic music data, achieving a low error rate of 0.0011 bits per byte in converting ABC notation to MIDI format. In addition, bGPT demonstrates exceptional capabilities in simulating CPU behaviour, with an accuracy exceeding 99.99% in executing various operations. Leveraging next byte prediction, models like bGPT can directly learn from vast binary data, effectively simulating the intricate patterns of the digital world.","sentences":["Traditional deep learning often overlooks bytes, the basic units of the digital world, where all forms of information and operations are encoded and manipulated in binary format.","Inspired by the success of next token prediction in natural language processing, we introduce bGPT, a model with next byte prediction to simulate the digital world.","bGPT matches specialized models in performance across various modalities, including text, audio, and images, and offers new possibilities for predicting, simulating, and diagnosing algorithm or hardware behaviour.","It has almost flawlessly replicated the process of converting symbolic music data, achieving a low error rate of 0.0011 bits per byte in converting ABC notation to MIDI format.","In addition, bGPT demonstrates exceptional capabilities in simulating CPU behaviour, with an accuracy exceeding 99.99% in executing various operations.","Leveraging next byte prediction, models like bGPT can directly learn from vast binary data, effectively simulating the intricate patterns of the digital world."],"url":"http://arxiv.org/abs/2402.19155v1","category":"cs.LG"}
{"created":"2024-02-29 13:25:15","title":"ProtoP-OD: Explainable Object Detection with Prototypical Parts","abstract":"Interpretation and visualization of the behavior of detection transformers tends to highlight the locations in the image that the model attends to, but it provides limited insight into the \\emph{semantics} that the model is focusing on. This paper introduces an extension to detection transformers that constructs prototypical local features and uses them in object detection. These custom features, which we call prototypical parts, are designed to be mutually exclusive and align with the classifications of the model. The proposed extension consists of a bottleneck module, the prototype neck, that computes a discretized representation of prototype activations and a new loss term that matches prototypes to object classes. This setup leads to interpretable representations in the prototype neck, allowing visual inspection of the image content perceived by the model and a better understanding of the model's reliability. We show experimentally that our method incurs only a limited performance penalty, and we provide examples that demonstrate the quality of the explanations provided by our method, which we argue outweighs the performance penalty.","sentences":["Interpretation and visualization of the behavior of detection transformers tends to highlight the locations in the image that the model attends to, but it provides limited insight into the \\emph{semantics} that the model is focusing on.","This paper introduces an extension to detection transformers that constructs prototypical local features and uses them in object detection.","These custom features, which we call prototypical parts, are designed to be mutually exclusive and align with the classifications of the model.","The proposed extension consists of a bottleneck module, the prototype neck, that computes a discretized representation of prototype activations and a new loss term that matches prototypes to object classes.","This setup leads to interpretable representations in the prototype neck, allowing visual inspection of the image content perceived by the model and a better understanding of the model's reliability.","We show experimentally that our method incurs only a limited performance penalty, and we provide examples that demonstrate the quality of the explanations provided by our method, which we argue outweighs the performance penalty."],"url":"http://arxiv.org/abs/2402.19142v1","category":"cs.CV"}
{"created":"2024-02-29 11:54:48","title":"Automatic Radar Signal Detection and FFT Estimation using Deep Learning","abstract":"This paper addresses a critical preliminary step in radar signal processing: detecting the presence of a radar signal and robustly estimating its bandwidth. Existing methods which are largely statistical feature-based approaches face challenges in electronic warfare (EW) settings where prior information about signals is lacking. While alternate deep learning based methods focus on more challenging environments, they primarily formulate this as a binary classification problem. In this research, we propose a novel methodology that not only detects the presence of a signal, but also localises it in the time domain and estimates its operating frequency band at that point in time. To achieve robust estimation, we introduce a compound loss function that leverages complementary information from both time-domain and frequency-domain representations. By integrating these approaches, we aim to improve the efficiency and accuracy of radar signal detection and parameter estimation, reducing both unnecessary resource consumption and human effort in downstream tasks.","sentences":["This paper addresses a critical preliminary step in radar signal processing: detecting the presence of a radar signal and robustly estimating its bandwidth.","Existing methods which are largely statistical feature-based approaches face challenges in electronic warfare (EW) settings where prior information about signals is lacking.","While alternate deep learning based methods focus on more challenging environments, they primarily formulate this as a binary classification problem.","In this research, we propose a novel methodology that not only detects the presence of a signal, but also localises it in the time domain and estimates its operating frequency band at that point in time.","To achieve robust estimation, we introduce a compound loss function that leverages complementary information from both time-domain and frequency-domain representations.","By integrating these approaches, we aim to improve the efficiency and accuracy of radar signal detection and parameter estimation, reducing both unnecessary resource consumption and human effort in downstream tasks."],"url":"http://arxiv.org/abs/2402.19073v1","category":"eess.SP"}
{"created":"2024-02-29 09:52:39","title":"Analysis of the Two-Step Heterogeneous Transfer Learning for Laryngeal Blood Vessel Classification: Issue and Improvement","abstract":"Transferring features learned from natural to medical images for classification is common. However, challenges arise due to the scarcity of certain medical image types and the feature disparities between natural and medical images. Two-step transfer learning has been recognized as a promising solution for this issue. However, choosing an appropriate intermediate domain would be critical in further improving the classification performance. In this work, we explore the effectiveness of using color fundus photographs of the diabetic retina dataset as an intermediate domain for two-step heterogeneous learning (THTL) to classify laryngeal vascular images with nine deep-learning models. Experiment results confirm that although the images in both the intermediate and target domains share vascularized characteristics, the accuracy is drastically reduced compared to one-step transfer learning, where only the last layer is fine-tuned (e.g., ResNet18 drops 14.7%, ResNet50 drops 14.8%). By analyzing the Layer Class Activation Maps (LayerCAM), we uncover a novel finding that the prevalent radial vascular pattern in the intermediate domain prevents learning the features of twisted and tangled vessels that distinguish the malignant class in the target domain. To address the performance drop, we propose the Step-Wise Fine-Tuning (SWFT) method on ResNet in the second step of THTL, resulting in substantial accuracy improvements. Compared to THTL's second step, where only the last layer is fine-tuned, accuracy increases by 26.1% for ResNet18 and 20.4% for ResNet50. Additionally, compared to training from scratch, using ImageNet as the source domain could slightly improve classification performance for laryngeal vascular, but the differences are insignificant.","sentences":["Transferring features learned from natural to medical images for classification is common.","However, challenges arise due to the scarcity of certain medical image types and the feature disparities between natural and medical images.","Two-step transfer learning has been recognized as a promising solution for this issue.","However, choosing an appropriate intermediate domain would be critical in further improving the classification performance.","In this work, we explore the effectiveness of using color fundus photographs of the diabetic retina dataset as an intermediate domain for two-step heterogeneous learning (THTL) to classify laryngeal vascular images with nine deep-learning models.","Experiment results confirm that although the images in both the intermediate and target domains share vascularized characteristics, the accuracy is drastically reduced compared to one-step transfer learning, where only the last layer is fine-tuned (e.g., ResNet18 drops 14.7%, ResNet50 drops 14.8%).","By analyzing the Layer Class Activation Maps (LayerCAM), we uncover a novel finding that the prevalent radial vascular pattern in the intermediate domain prevents learning the features of twisted and tangled vessels that distinguish the malignant class in the target domain.","To address the performance drop, we propose the Step-Wise Fine-Tuning (SWFT) method on ResNet in the second step of THTL, resulting in substantial accuracy improvements.","Compared to THTL's second step, where only the last layer is fine-tuned, accuracy increases by 26.1% for ResNet18 and 20.4% for ResNet50.","Additionally, compared to training from scratch, using ImageNet as the source domain could slightly improve classification performance for laryngeal vascular, but the differences are insignificant."],"url":"http://arxiv.org/abs/2402.19001v1","category":"cs.CV"}
{"created":"2024-02-29 09:48:19","title":"COFT-AD: COntrastive Fine-Tuning for Few-Shot Anomaly Detection","abstract":"Existing approaches towards anomaly detection~(AD) often rely on a substantial amount of anomaly-free data to train representation and density models. However, large anomaly-free datasets may not always be available before the inference stage; in which case an anomaly detection model must be trained with only a handful of normal samples, a.k.a. few-shot anomaly detection (FSAD). In this paper, we propose a novel methodology to address the challenge of FSAD which incorporates two important techniques. Firstly, we employ a model pre-trained on a large source dataset to initialize model weights. Secondly, to ameliorate the covariate shift between source and target domains, we adopt contrastive training to fine-tune on the few-shot target domain data. To learn suitable representations for the downstream AD task, we additionally incorporate cross-instance positive pairs to encourage a tight cluster of the normal samples, and negative pairs for better separation between normal and synthesized negative samples. We evaluate few-shot anomaly detection on on 3 controlled AD tasks and 4 real-world AD tasks to demonstrate the effectiveness of the proposed method.","sentences":["Existing approaches towards anomaly detection~(AD) often rely on a substantial amount of anomaly-free data to train representation and density models.","However, large anomaly-free datasets may not always be available before the inference stage; in which case an anomaly detection model must be trained with only a handful of normal samples, a.k.a. few-shot anomaly detection (FSAD).","In this paper, we propose a novel methodology to address the challenge of FSAD which incorporates two important techniques.","Firstly, we employ a model pre-trained on a large source dataset to initialize model weights.","Secondly, to ameliorate the covariate shift between source and target domains, we adopt contrastive training to fine-tune on the few-shot target domain data.","To learn suitable representations for the downstream AD task, we additionally incorporate cross-instance positive pairs to encourage a tight cluster of the normal samples, and negative pairs for better separation between normal and synthesized negative samples.","We evaluate few-shot anomaly detection on on 3 controlled AD tasks and 4 real-world AD tasks to demonstrate the effectiveness of the proposed method."],"url":"http://arxiv.org/abs/2402.18998v1","category":"cs.CV"}
{"created":"2024-02-29 09:14:50","title":"OHTA: One-shot Hand Avatar via Data-driven Implicit Priors","abstract":"In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image. With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical. Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios. To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image. OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors. Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge. OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image. Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation.","sentences":["In this paper, we delve into the creation of one-shot hand avatars, attaining high-fidelity and drivable hand representations swiftly from a single image.","With the burgeoning domains of the digital human, the need for quick and personalized hand avatar creation has become increasingly critical.","Existing techniques typically require extensive input data and may prove cumbersome or even impractical in certain scenarios.","To enhance accessibility, we present a novel method OHTA (One-shot Hand avaTAr) that enables the creation of detailed hand avatars from merely one image.","OHTA tackles the inherent difficulties of this data-limited problem by learning and utilizing data-driven hand priors.","Specifically, we design a hand prior model initially employed for 1) learning various hand priors with available data and subsequently for 2) the inversion and fitting of the target identity with prior knowledge.","OHTA demonstrates the capability to create high-fidelity hand avatars with consistent animatable quality, solely relying on a single image.","Furthermore, we illustrate the versatility of OHTA through diverse applications, encompassing text-to-avatar conversion, hand editing, and identity latent space manipulation."],"url":"http://arxiv.org/abs/2402.18969v1","category":"cs.CV"}
{"created":"2024-02-29 08:52:38","title":"Boosting Semi-Supervised Object Detection in Remote Sensing Images With Active Teaching","abstract":"The lack of object-level annotations poses a significant challenge for object detection in remote sensing images (RSIs). To address this issue, active learning (AL) and semi-supervised learning (SSL) techniques have been proposed to enhance the quality and quantity of annotations. AL focuses on selecting the most informative samples for annotation, while SSL leverages the knowledge from unlabeled samples. In this letter, we propose a novel AL method to boost semi-supervised object detection (SSOD) for remote sensing images with a teacher student network, called SSOD-AT. The proposed method incorporates an RoI comparison module (RoICM) to generate high-confidence pseudo-labels for regions of interest (RoIs). Meanwhile, the RoICM is utilized to identify the top-K uncertain images. To reduce redundancy in the top-K uncertain images for human labeling, a diversity criterion is introduced based on object-level prototypes of different categories using both labeled and pseudo-labeled images. Extensive experiments on DOTA and DIOR, two popular datasets, demonstrate that our proposed method outperforms state-of-the-art methods for object detection in RSIs. Compared with the best performance in the SOTA methods, the proposed method achieves 1 percent improvement in most cases in the whole AL.","sentences":["The lack of object-level annotations poses a significant challenge for object detection in remote sensing images (RSIs).","To address this issue, active learning (AL) and semi-supervised learning (SSL) techniques have been proposed to enhance the quality and quantity of annotations.","AL focuses on selecting the most informative samples for annotation, while SSL leverages the knowledge from unlabeled samples.","In this letter, we propose a novel AL method to boost semi-supervised object detection (SSOD) for remote sensing images with a teacher student network, called SSOD-AT.","The proposed method incorporates an RoI comparison module (RoICM) to generate high-confidence pseudo-labels for regions of interest (RoIs).","Meanwhile, the RoICM is utilized to identify the top-K uncertain images.","To reduce redundancy in the top-K uncertain images for human labeling, a diversity criterion is introduced based on object-level prototypes of different categories using both labeled and pseudo-labeled images.","Extensive experiments on DOTA and DIOR, two popular datasets, demonstrate that our proposed method outperforms state-of-the-art methods for object detection in RSIs.","Compared with the best performance in the SOTA methods, the proposed method achieves 1 percent improvement in most cases in the whole AL."],"url":"http://arxiv.org/abs/2402.18958v1","category":"cs.CV"}
{"created":"2024-02-29 07:34:08","title":"Realization of High-Fidelity CZ Gate based on a Double-Transmon Coupler","abstract":"Striving for higher gate fidelity is crucial not only for enhancing existing noisy intermediate-scale quantum (NISQ) devices but also for unleashing the potential of fault-tolerant quantum computation through quantum error correction. A recently proposed theoretical scheme, the double-transmon coupler (DTC), aims to achieve both suppressed residual interaction and a fast high-fidelity two-qubit gate simultaneously, particularly for highly detuned qubits. Harnessing the state-of-the-art fabrication techniques and a model-free pulse-optimization process based on reinforcement learning, we translate the theoretical DTC scheme into reality, attaining fidelities of 99.92% for a CZ gate and 99.98% for single-qubit gates. The performance of the DTC scheme demonstrates its potential as a competitive building block for superconducting quantum processors.","sentences":["Striving for higher gate fidelity is crucial not only for enhancing existing noisy intermediate-scale quantum (NISQ) devices but also for unleashing the potential of fault-tolerant quantum computation through quantum error correction.","A recently proposed theoretical scheme, the double-transmon coupler (DTC), aims to achieve both suppressed residual interaction and a fast high-fidelity two-qubit gate simultaneously, particularly for highly detuned qubits.","Harnessing the state-of-the-art fabrication techniques and a model-free pulse-optimization process based on reinforcement learning, we translate the theoretical DTC scheme into reality, attaining fidelities of 99.92% for a CZ gate and 99.98% for single-qubit gates.","The performance of the DTC scheme demonstrates its potential as a competitive building block for superconducting quantum processors."],"url":"http://arxiv.org/abs/2402.18926v1","category":"quant-ph"}
{"created":"2024-02-29 07:24:24","title":"Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation","abstract":"While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples. One of the main sources of distribution shift for image classification is the compositional nature of images. Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments. More importantly, these components may have spurious correlations with the label. To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images. Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especially in datapoints on which models have a high confidence). In fact, according to the amount of spurious correlation and the easiness of classification based on the causal or non-causal components, the model usually attends to one of these more (on samples with high confidence). Following this, we first try to identify the causal components of images using class activation maps of models trained with ERM. Afterward, we intervene on images by combining them and retraining the model on the augmented data, including the counterfactual ones. Along with its high interpretability, this work proposes a group-balancing method by intervening on images without requiring group labels or information regarding the spurious features during training. The method has an overall better worst group accuracy compared to previous methods with the same amount of supervision on the group labels in correlation shift.","sentences":["While standard Empirical Risk Minimization (ERM) training is proven effective for image classification on in-distribution data, it fails to perform well on out-of-distribution samples.","One of the main sources of distribution shift for image classification is the compositional nature of images.","Specifically, in addition to the main object or component(s) determining the label, some other image components usually exist, which may lead to the shift of input distribution between train and test environments.","More importantly, these components may have spurious correlations with the label.","To address this issue, we propose Decompose-and-Compose (DaC), which improves robustness to correlation shift by a compositional approach based on combining elements of images.","Based on our observations, models trained with ERM usually highly attend to either the causal components or the components having a high spurious correlation with the label (especially in datapoints on which models have a high confidence).","In fact, according to the amount of spurious correlation and the easiness of classification based on the causal or non-causal components, the model usually attends to one of these more (on samples with high confidence).","Following this, we first try to identify the causal components of images using class activation maps of models trained with ERM.","Afterward, we intervene on images by combining them and retraining the model on the augmented data, including the counterfactual ones.","Along with its high interpretability, this work proposes a group-balancing method by intervening on images without requiring group labels or information regarding the spurious features during training.","The method has an overall better worst group accuracy compared to previous methods with the same amount of supervision on the group labels in correlation shift."],"url":"http://arxiv.org/abs/2402.18919v1","category":"cs.CV"}
{"created":"2024-02-29 06:51:52","title":"Contact-Implicit Model Predictive Control for Dexterous In-hand Manipulation: A Long-Horizon and Robust Approach","abstract":"Dexterous in-hand manipulation is an essential skill of production and life. Nevertheless, the highly stiff and mutable features of contacts cause limitations to real-time contact discovery and inference, which degrades the performance of model-based methods. Inspired by recent advancements in contact-rich locomotion and manipulation, this paper proposes a novel model-based approach to control dexterous in-hand manipulation and overcome the current limitations. The proposed approach has the attractive feature, which allows the robot to robustly execute long-horizon in-hand manipulation without pre-defined contact sequences or separated planning procedures. Specifically, we design a contact-implicit model predictive controller at high-level to generate real-time contact plans, which are executed by the low-level tracking controller. Compared with other model-based methods, such a long-horizon feature enables replanning and robust execution of contact-rich motions to achieve large-displacement in-hand tasks more efficiently; Compared with existing learning-based methods, the proposed approach achieves the dexterity and also generalizes to different objects without any pre-training. Detailed simulations and ablation studies demonstrate the efficiency and effectiveness of our method. It runs at 20Hz on the 23-degree-of-freedom long-horizon in-hand object rotation task.","sentences":["Dexterous in-hand manipulation is an essential skill of production and life.","Nevertheless, the highly stiff and mutable features of contacts cause limitations to real-time contact discovery and inference, which degrades the performance of model-based methods.","Inspired by recent advancements in contact-rich locomotion and manipulation, this paper proposes a novel model-based approach to control dexterous in-hand manipulation and overcome the current limitations.","The proposed approach has the attractive feature, which allows the robot to robustly execute long-horizon in-hand manipulation without pre-defined contact sequences or separated planning procedures.","Specifically, we design a contact-implicit model predictive controller at high-level to generate real-time contact plans, which are executed by the low-level tracking controller.","Compared with other model-based methods, such a long-horizon feature enables replanning and robust execution of contact-rich motions to achieve large-displacement in-hand tasks more efficiently; Compared with existing learning-based methods, the proposed approach achieves the dexterity and also generalizes to different objects without any pre-training.","Detailed simulations and ablation studies demonstrate the efficiency and effectiveness of our method.","It runs at 20Hz on the 23-degree-of-freedom long-horizon in-hand object rotation task."],"url":"http://arxiv.org/abs/2402.18897v1","category":"cs.RO"}
{"created":"2024-02-29 05:58:00","title":"Weak Lensing Constraints on Dark Matter-Baryon Interactions with $N$-Body Simulations and Machine Learning","abstract":"We investigate the elastic scattering cross section between dark matter and protons using the DES Year 3 weak lensing data. This scattering induces a dark acoustic oscillation structure in the matter power spectra. To address non-linear effects at low redshift, we utilize principal component analysis alongside a limited set of $N$-body simulations, improving the reliability of our matter power spectrum prediction. We further perform a robust Markov Chain Monte Carlo analysis to derive the upper bounds on the DM-proton elastic scattering cross-section, assuming different velocity dependencies. Our results, presented as the first Frequentist upper limits, are compared with the ones obtained by Bayesian approach. Compared with the upper limits derived from the Planck cosmic microwave background data, our findings from DES Year 3 data exhibit improvements of up to a factor of five. In addition, we forecast the future sensitivities of the China Space Station Telescope, the upcoming capabilities of this telescope could improve the current limits by approximately one order of magnitude.","sentences":["We investigate the elastic scattering cross section between dark matter and protons using the DES Year 3 weak lensing data.","This scattering induces a dark acoustic oscillation structure in the matter power spectra.","To address non-linear effects at low redshift, we utilize principal component analysis alongside a limited set of $N$-body simulations, improving the reliability of our matter power spectrum prediction.","We further perform a robust Markov Chain Monte Carlo analysis to derive the upper bounds on the DM-proton elastic scattering cross-section, assuming different velocity dependencies.","Our results, presented as the first Frequentist upper limits, are compared with the ones obtained by Bayesian approach.","Compared with the upper limits derived from the Planck cosmic microwave background data, our findings from DES Year 3 data exhibit improvements of up to a factor of five.","In addition, we forecast the future sensitivities of the China Space Station Telescope, the upcoming capabilities of this telescope could improve the current limits by approximately one order of magnitude."],"url":"http://arxiv.org/abs/2402.18880v1","category":"astro-ph.CO"}
{"created":"2024-02-29 05:40:43","title":"LoLiSRFlow: Joint Single Image Low-light Enhancement and Super-resolution via Cross-scale Transformer-based Conditional Flow","abstract":"The visibility of real-world images is often limited by both low-light and low-resolution, however, these issues are only addressed in the literature through Low-Light Enhancement (LLE) and Super- Resolution (SR) methods. Admittedly, a simple cascade of these approaches cannot work harmoniously to cope well with the highly ill-posed problem for simultaneously enhancing visibility and resolution. In this paper, we propose a normalizing flow network, dubbed LoLiSRFLow, specifically designed to consider the degradation mechanism inherent in joint LLE and SR. To break the bonds of the one-to-many mapping for low-light low-resolution images to normal-light high-resolution images, LoLiSRFLow directly learns the conditional probability distribution over a variety of feasible solutions for high-resolution well-exposed images. Specifically, a multi-resolution parallel transformer acts as a conditional encoder that extracts the Retinex-induced resolution-and-illumination invariant map as the previous one. And the invertible network maps the distribution of usually exposed high-resolution images to a latent distribution. The backward inference is equivalent to introducing an additional constrained loss for the normal training route, thus enabling the manifold of the natural exposure of the high-resolution image to be immaculately depicted. We also propose a synthetic dataset modeling the realistic low-light low-resolution degradation, named DFSR-LLE, containing 7100 low-resolution dark-light/high-resolution normal sharp pairs. Quantitative and qualitative experimental results demonstrate the effectiveness of our method on both the proposed synthetic and real datasets.","sentences":["The visibility of real-world images is often limited by both low-light and low-resolution, however, these issues are only addressed in the literature through Low-Light Enhancement (LLE) and Super- Resolution (SR) methods.","Admittedly, a simple cascade of these approaches cannot work harmoniously to cope well with the highly ill-posed problem for simultaneously enhancing visibility and resolution.","In this paper, we propose a normalizing flow network, dubbed LoLiSRFLow, specifically designed to consider the degradation mechanism inherent in joint LLE and SR.","To break the bonds of the one-to-many mapping for low-light low-resolution images to normal-light high-resolution images, LoLiSRFLow directly learns the conditional probability distribution over a variety of feasible solutions for high-resolution well-exposed images.","Specifically, a multi-resolution parallel transformer acts as a conditional encoder that extracts the Retinex-induced resolution-and-illumination invariant map as the previous one.","And the invertible network maps the distribution of usually exposed high-resolution images to a latent distribution.","The backward inference is equivalent to introducing an additional constrained loss for the normal training route, thus enabling the manifold of the natural exposure of the high-resolution image to be immaculately depicted.","We also propose a synthetic dataset modeling the realistic low-light low-resolution degradation, named DFSR-LLE, containing 7100 low-resolution dark-light/high-resolution normal sharp pairs.","Quantitative and qualitative experimental results demonstrate the effectiveness of our method on both the proposed synthetic and real datasets."],"url":"http://arxiv.org/abs/2402.18871v1","category":"eess.IV"}
{"created":"2024-02-29 04:30:39","title":"Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey","abstract":"3D human pose estimation and mesh recovery have attracted widespread research interest in many areas, such as computer vision, autonomous driving, and robotics. Deep learning on 3D human pose estimation and mesh recovery has recently thrived, with numerous methods proposed to address different problems in this area. In this paper, to stimulate future research, we present a comprehensive review of recent progress over the past five years in deep learning methods for this area by delving into over 200 references. To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations. We also present comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions. A regularly updated project page can be found at https://github.com/liuyangme/SOTA-3DHPE-HMR.","sentences":["3D human pose estimation and mesh recovery have attracted widespread research interest in many areas, such as computer vision, autonomous driving, and robotics.","Deep learning on 3D human pose estimation and mesh recovery has recently thrived, with numerous methods proposed to address different problems in this area.","In this paper, to stimulate future research, we present a comprehensive review of recent progress over the past five years in deep learning methods for this area by delving into over 200 references.","To the best of our knowledge, this survey is arguably the first to comprehensively cover deep learning methods for 3D human pose estimation, including both single-person and multi-person approaches, as well as human mesh recovery, encompassing methods based on explicit models and implicit representations.","We also present comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions.","A regularly updated project page can be found at https://github.com/liuyangme/SOTA-3DHPE-HMR."],"url":"http://arxiv.org/abs/2402.18844v1","category":"cs.CV"}
{"created":"2024-02-29 03:53:02","title":"A Model-Based Approach for Improving Reinforcement Learning Efficiency Leveraging Expert Observations","abstract":"This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep reinforcement learning setting to improve sample efficiency. First, we formulate an augmented policy loss combining a maximum entropy reinforcement learning objective with a behavioral cloning loss that leverages a forward dynamics model. Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function. Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various benchmarks by effectively utilizing available expert observations.","sentences":["This paper investigates how to incorporate expert observations (without explicit information on expert actions) into a deep reinforcement learning setting to improve sample efficiency.","First, we formulate an augmented policy loss combining a maximum entropy reinforcement learning objective with a behavioral cloning loss that leverages a forward dynamics model.","Then, we propose an algorithm that automatically adjusts the weights of each component in the augmented loss function.","Experiments on a variety of continuous control tasks demonstrate that the proposed algorithm outperforms various benchmarks by effectively utilizing available expert observations."],"url":"http://arxiv.org/abs/2402.18836v1","category":"cs.LG"}
{"created":"2024-02-29 03:16:47","title":"Batch size invariant Adam","abstract":"We propose a batch size invariant version of Adam, for use in large-scale, distributed settings, in which the mini-batch is divided into micro-batches which are distributed among worker nodes. For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average. Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient. In contrast, the approach proposed here gives batch size invariance without this assumption. We confirm that in practice our scheme gives batch size invariance in a much larger range of scenarios than the previous approach.","sentences":["We propose a batch size invariant version of Adam, for use in large-scale, distributed settings, in which the mini-batch is divided into micro-batches which are distributed among worker nodes.","For the v term, standard Adam first computes the average over micro-batch gradients, then squares, while in the batch size invariant Adam proposed here, we first square the micro-batch gradients, then average.","Previous work (e.g. Malladi et al. 2022) used an alternative approach that involved a square-root scaling of the learning rate, but this approach requires strong assumptions to work; in particular that the gradient variance dominates the square of the expected gradient.","In contrast, the approach proposed here gives batch size invariance without this assumption.","We confirm that in practice our scheme gives batch size invariance in a much larger range of scenarios than the previous approach."],"url":"http://arxiv.org/abs/2402.18824v1","category":"cs.LG"}
{"created":"2024-02-29 03:09:16","title":"Debiased Novel Category Discovering and Localization","abstract":"In recent years, object detection in deep learning has experienced rapid development. However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set. These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors. In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories. We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets. To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner. Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data. Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery. We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art.","sentences":["In recent years, object detection in deep learning has experienced rapid development.","However, most existing object detection models perform well only on closed-set datasets, ignoring a large number of potential objects whose categories are not defined in the training set.","These objects are often identified as background or incorrectly classified as pre-defined categories by the detectors.","In this paper, we focus on the challenging problem of Novel Class Discovery and Localization (NCDL), aiming to train detectors that can detect the categories present in the training data, while also actively discover, localize, and cluster new categories.","We analyze existing NCDL methods and identify the core issue: object detectors tend to be biased towards seen objects, and this leads to the neglect of unseen targets.","To address this issue, we first propose an Debiased Region Mining (DRM) approach that combines class-agnostic Region Proposal Network (RPN) and class-aware RPN in a complementary manner.","Additionally, we suggest to improve the representation network through semi-supervised contrastive learning by leveraging unlabeled data.","Finally, we adopt a simple and efficient mini-batch K-means clustering method for novel class discovery.","We conduct extensive experiments on the NCDL benchmark, and the results demonstrate that the proposed DRM approach significantly outperforms previous methods, establishing a new state-of-the-art."],"url":"http://arxiv.org/abs/2402.18821v1","category":"cs.CV"}
{"created":"2024-02-29 02:57:44","title":"Gradient Alignment for Cross-Domain Face Anti-Spoofing","abstract":"Recent advancements in domain generalization (DG) for face anti-spoofing (FAS) have garnered considerable attention. Traditional methods have focused on designing learning objectives and additional modules to isolate domain-specific features while retaining domain-invariant characteristics in their representations. However, such approaches often lack guarantees of consistent maintenance of domain-invariant features or the complete removal of domain-specific features. Furthermore, most prior works of DG for FAS do not ensure convergence to a local flat minimum, which has been shown to be advantageous for DG. In this paper, we introduce GAC-FAS, a novel learning objective that encourages the model to converge towards an optimal flat minimum without necessitating additional learning modules. Unlike conventional sharpness-aware minimizers, GAC-FAS identifies ascending points for each domain and regulates the generalization gradient updates at these points to align coherently with empirical risk minimization (ERM) gradient updates. This unique approach specifically guides the model to be robust against domain shifts. We demonstrate the efficacy of GAC-FAS through rigorous testing on challenging cross-domain FAS datasets, where it establishes state-of-the-art performance. The code is available at https://github.com/leminhbinh0209/CVPR24-FAS.","sentences":["Recent advancements in domain generalization (DG) for face anti-spoofing (FAS) have garnered considerable attention.","Traditional methods have focused on designing learning objectives and additional modules to isolate domain-specific features while retaining domain-invariant characteristics in their representations.","However, such approaches often lack guarantees of consistent maintenance of domain-invariant features or the complete removal of domain-specific features.","Furthermore, most prior works of DG for FAS do not ensure convergence to a local flat minimum, which has been shown to be advantageous for DG.","In this paper, we introduce GAC-FAS, a novel learning objective that encourages the model to converge towards an optimal flat minimum without necessitating additional learning modules.","Unlike conventional sharpness-aware minimizers, GAC-FAS identifies ascending points for each domain and regulates the generalization gradient updates at these points to align coherently with empirical risk minimization (ERM) gradient updates.","This unique approach specifically guides the model to be robust against domain shifts.","We demonstrate the efficacy of GAC-FAS through rigorous testing on challenging cross-domain FAS datasets, where it establishes state-of-the-art performance.","The code is available at https://github.com/leminhbinh0209/CVPR24-FAS."],"url":"http://arxiv.org/abs/2402.18817v1","category":"cs.CV"}
{"created":"2024-02-29 02:16:57","title":"To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training on Shared Models","abstract":"In fair machine learning, one source of performance disparities between groups is over-fitting to groups with relatively few training samples. We derive group-specific bounds on the generalization error of welfare-centric fair machine learning that benefit from the larger sample size of the majority group. We do this by considering group-specific Rademacher averages over a restricted hypothesis class, which contains the family of models likely to perform well with respect to a fair learning objective (e.g., a power-mean). Our simulations demonstrate these bounds improve over a naive method, as expected by theory, with particularly significant improvement for smaller group sizes.","sentences":["In fair machine learning, one source of performance disparities between groups is over-fitting to groups with relatively few training samples.","We derive group-specific bounds on the generalization error of welfare-centric fair machine learning that benefit from the larger sample size of the majority group.","We do this by considering group-specific Rademacher averages over a restricted hypothesis class, which contains the family of models likely to perform well with respect to a fair learning objective (e.g., a power-mean).","Our simulations demonstrate these bounds improve over a naive method, as expected by theory, with particularly significant improvement for smaller group sizes."],"url":"http://arxiv.org/abs/2402.18803v1","category":"cs.LG"}
{"created":"2024-02-29 02:13:10","title":"BlockEcho: Retaining Long-Range Dependencies for Imputing Block-Wise Missing Data","abstract":"Block-wise missing data poses significant challenges in real-world data imputation tasks. Compared to scattered missing data, block-wise gaps exacerbate adverse effects on subsequent analytic and machine learning tasks, as the lack of local neighboring elements significantly reduces the interpolation capability and predictive power. However, this issue has not received adequate attention. Most SOTA matrix completion methods appeared less effective, primarily due to overreliance on neighboring elements for predictions. We systematically analyze the issue and propose a novel matrix completion method ``BlockEcho\" for a more comprehensive solution. This method creatively integrates Matrix Factorization (MF) within Generative Adversarial Networks (GAN) to explicitly retain long-distance inter-element relationships in the original matrix. Besides, we incorporate an additional discriminator for GAN, comparing the generator's intermediate progress with pre-trained MF results to constrain high-order feature distributions. Subsequently, we evaluate BlockEcho on public datasets across three domains. Results demonstrate superior performance over both traditional and SOTA methods when imputing block-wise missing data, especially at higher missing rates. The advantage also holds for scattered missing data at high missing rates. We also contribute on the analyses in providing theoretical justification on the optimality and convergence of fusing MF and GAN for missing block data.","sentences":["Block-wise missing data poses significant challenges in real-world data imputation tasks.","Compared to scattered missing data, block-wise gaps exacerbate adverse effects on subsequent analytic and machine learning tasks, as the lack of local neighboring elements significantly reduces the interpolation capability and predictive power.","However, this issue has not received adequate attention.","Most SOTA matrix completion methods appeared less effective, primarily due to overreliance on neighboring elements for predictions.","We systematically analyze the issue and propose a novel matrix completion method ``BlockEcho\" for a more comprehensive solution.","This method creatively integrates Matrix Factorization (MF) within Generative Adversarial Networks (GAN) to explicitly retain long-distance inter-element relationships in the original matrix.","Besides, we incorporate an additional discriminator for GAN, comparing the generator's intermediate progress with pre-trained MF results to constrain high-order feature distributions.","Subsequently, we evaluate BlockEcho on public datasets across three domains.","Results demonstrate superior performance over both traditional and SOTA methods when imputing block-wise missing data, especially at higher missing rates.","The advantage also holds for scattered missing data at high missing rates.","We also contribute on the analyses in providing theoretical justification on the optimality and convergence of fusing MF and GAN for missing block data."],"url":"http://arxiv.org/abs/2402.18800v1","category":"cs.LG"}
{"created":"2024-02-29 00:42:33","title":"GDCNet: Calibrationless geometric distortion correction of echo planar imaging data using deep learning","abstract":"Functional magnetic resonance imaging techniques benefit from echo-planar imaging's fast image acquisition but are susceptible to inhomogeneities in the main magnetic field, resulting in geometric distortion and signal loss artifacts in the images. Traditional methods leverage a field map or voxel displacement map for distortion correction. However, voxel displacement map estimation requires additional sequence acquisitions, and the accuracy of the estimation influences correction performance. This work implements a novel approach called GDCNet, which estimates a geometric distortion map by non-linear registration to T1-weighted anatomical images and applies it for distortion correction. GDCNet demonstrated fast distortion correction of functional images in retrospectively and prospectively acquired datasets. Among the compared models, the 2D self-supervised configuration resulted in a statistically significant improvement to normalized mutual information between distortion-corrected functional and T1-weighted images compared to the benchmark methods FUGUE and TOPUP. Furthermore, GDCNet models achieved processing speeds 14 times faster than TOPUP in the prospective dataset.","sentences":["Functional magnetic resonance imaging techniques benefit from echo-planar imaging's fast image acquisition but are susceptible to inhomogeneities in the main magnetic field, resulting in geometric distortion and signal loss artifacts in the images.","Traditional methods leverage a field map or voxel displacement map for distortion correction.","However, voxel displacement map estimation requires additional sequence acquisitions, and the accuracy of the estimation influences correction performance.","This work implements a novel approach called GDCNet, which estimates a geometric distortion map by non-linear registration to T1-weighted anatomical images and applies it for distortion correction.","GDCNet demonstrated fast distortion correction of functional images in retrospectively and prospectively acquired datasets.","Among the compared models, the 2D self-supervised configuration resulted in a statistically significant improvement to normalized mutual information between distortion-corrected functional and T1-weighted images compared to the benchmark methods FUGUE and TOPUP.","Furthermore, GDCNet models achieved processing speeds 14 times faster than TOPUP in the prospective dataset."],"url":"http://arxiv.org/abs/2402.18777v1","category":"eess.IV"}
{"created":"2024-02-28 22:27:49","title":"Priority Sampling of Large Language Models for Compilers","abstract":"Large language models show great potential in generating and optimizing code. Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures. Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability. We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence. Each new sample expands the unexpanded token with the highest probability in the augmented search tree. Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process. Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz. Moreover, it outperforms the autotuner used for the generation of labels for the training of the original model in just 30 samples.","sentences":["Large language models show great potential in generating and optimizing code.","Widely used sampling methods such as Nucleus Sampling increase the diversity of generation but often produce repeated samples for low temperatures and incoherent samples for high temperatures.","Furthermore, the temperature coefficient has to be tuned for each task, limiting its usability.","We present Priority Sampling, a simple and deterministic sampling technique that produces unique samples ordered by the model's confidence.","Each new sample expands the unexpanded token with the highest probability in the augmented search tree.","Additionally, Priority Sampling supports generation based on regular expression that provides a controllable and structured exploration process.","Priority Sampling outperforms Nucleus Sampling for any number of samples, boosting the performance of the original model from 2.87% to 5% improvement over -Oz.","Moreover, it outperforms the autotuner used for the generation of labels for the training of the original model in just 30 samples."],"url":"http://arxiv.org/abs/2402.18734v1","category":"cs.LG"}
{"created":"2024-02-28 21:55:14","title":"Leveraging the turnpike effect for Mean Field Games numerics","abstract":"Recently, a deep-learning algorithm referred to as Deep Galerkin Method (DGM), has gained a lot of attention among those trying to solve numerically Mean Field Games with finite horizon, even if the performance seems to be decreasing significantly with increasing horizon. On the other hand, it has been proven that some specific classes of Mean Field Games enjoy some form of the turnpike property identified over seven decades ago by economists. The gist of this phenomenon is a proof that the solution of an optimal control problem over a long time interval spends most of its time near the stationary solution of the ergodic solution of the corresponding infinite horizon optimization problem. After reviewing the implementation of DGM for finite horizon Mean Field Games, we introduce a ``turnpike-accelerated'' version that incorporates the turnpike estimates in the loss function to be optimized, and we perform a comparative numerical analysis to show the advantages of this accelerated version over the baseline DGM algorithm. We demonstrate on some of the Mean Field Game models with local-couplings known to have the turnpike property, as well as a new class of linear-quadratic models for which we derive explicit turnpike estimates.","sentences":["Recently, a deep-learning algorithm referred to as Deep Galerkin Method (DGM), has gained a lot of attention among those trying to solve numerically Mean Field Games with finite horizon, even if the performance seems to be decreasing significantly with increasing horizon.","On the other hand, it has been proven that some specific classes of Mean Field Games enjoy some form of the turnpike property identified over seven decades ago by economists.","The gist of this phenomenon is a proof that the solution of an optimal control problem over a long time interval spends most of its time near the stationary solution of the ergodic solution of the corresponding infinite horizon optimization problem.","After reviewing the implementation of DGM for finite horizon Mean Field Games, we introduce a ``turnpike-accelerated'' version that incorporates the turnpike estimates in the loss function to be optimized, and we perform a comparative numerical analysis to show the advantages of this accelerated version over the baseline DGM algorithm.","We demonstrate on some of the Mean Field Game models with local-couplings known to have the turnpike property, as well as a new class of linear-quadratic models for which we derive explicit turnpike estimates."],"url":"http://arxiv.org/abs/2402.18725v1","category":"math.OC"}
{"created":"2024-02-28 21:23:15","title":"Identifying Assumptions and Research Dynamics","abstract":"A representative researcher pursuing a question has repeated opportunities for empirical research. To process findings, she must impose an identifying assumption, which ensures that repeated observation would provide a definitive answer to her question. Research designs vary in quality and are implemented only when the assumption is plausible enough according to a KL-divergence-based criterion, and then beliefs are Bayes-updated as if the assumption were perfectly valid. We study the dynamics of this learning process and its induced long-run beliefs. The rate of research cannot uniformly accelerate over time. We characterize environments in which it is stationary. Long-run beliefs can exhibit history-dependence. We apply the model to stylized examples of empirical methodologies: experiments, causal-inference techniques, and (in an extension) ``structural'' identification methods such as ``calibration'' and ``Heckman selection.''","sentences":["A representative researcher pursuing a question has repeated opportunities for empirical research.","To process findings, she must impose an identifying assumption, which ensures that repeated observation would provide a definitive answer to her question.","Research designs vary in quality and are implemented only when the assumption is plausible enough according to a KL-divergence-based criterion, and then beliefs are Bayes-updated as if the assumption were perfectly valid.","We study the dynamics of this learning process and its induced long-run beliefs.","The rate of research cannot uniformly accelerate over time.","We characterize environments in which it is stationary.","Long-run beliefs can exhibit history-dependence.","We apply the model to stylized examples of empirical methodologies: experiments, causal-inference techniques, and (in an extension) ``structural'' identification methods such as ``calibration'' and ``Heckman selection.''"],"url":"http://arxiv.org/abs/2402.18713v1","category":"econ.TH"}
{"created":"2024-02-28 20:36:29","title":"Articulated Object Manipulation with Coarse-to-fine Affordance for Mitigating the Effect of Point Cloud Noise","abstract":"3D articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects.Point-level affordance, which predicts the per-point actionable score and thus proposes the best point to interact with, has demonstrated excellent performance and generalization capabilities in articulated object manipulation. However, a significant challenge remains: while previous works use perfect point cloud generated in simulation, the models cannot directly apply to the noisy point cloud in the real-world.To tackle this challenge, we leverage the property of real-world scanned point cloud that, the point cloud becomes less noisy when the camera is closer to the object. Therefore, we propose a novel coarse-to-fine affordance learning pipeline to mitigate the effect of point cloud noise in two stages. In the first stage, we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate. Then, we move the camera in front of the approximated place, scan a less noisy point cloud containing precise local geometries for manipulation, and learn affordance on such point cloud to propose fine-grained final actions. The proposed method is thoroughly evaluated both using large-scale simulated noisy point clouds mimicking real-world scans, and in the real world scenarios, with superiority over existing methods, demonstrating the effectiveness in tackling the noisy real-world point cloud problem.","sentences":["3D articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects.","Point-level affordance, which predicts the per-point actionable score and thus proposes the best point to interact with, has demonstrated excellent performance and generalization capabilities in articulated object manipulation.","However, a significant challenge remains: while previous works use perfect point cloud generated in simulation, the models cannot directly apply to the noisy point cloud in the real-world.","To tackle this challenge, we leverage the property of real-world scanned point cloud that, the point cloud becomes less noisy when the camera is closer to the object.","Therefore, we propose a novel coarse-to-fine affordance learning pipeline to mitigate the effect of point cloud noise in two stages.","In the first stage, we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate.","Then, we move the camera in front of the approximated place, scan a less noisy point cloud containing precise local geometries for manipulation, and learn affordance on such point cloud to propose fine-grained final actions.","The proposed method is thoroughly evaluated both using large-scale simulated noisy point clouds mimicking real-world scans, and in the real world scenarios, with superiority over existing methods, demonstrating the effectiveness in tackling the noisy real-world point cloud problem."],"url":"http://arxiv.org/abs/2402.18699v1","category":"cs.RO"}
{"created":"2024-02-29 18:25:23","title":"Reduction of Anisotropic Volume Expansion and the Optimization of Specific Charge Capacity in Lithiated Silicon Nanowires","abstract":"This computational research study analyzes the increase of the specific charge capacity that comes with the reduction of the anisotropic volume expansion during lithium ion insertion within silicon nanowires. This research paper is a continuation from previous work that studied the expansion rate and volume increase. It has been determined that when the lithium ion concentration is decreased by regulating the amount of Li ion flux, the lithium ions to silicon atoms ratio, represented by x, decreases within the amorphous lithiated silicon (a-LixSi) material. This results in a decrease in the volumetric strain of the lithiated silicon nanowire as well as a reduction in Maxwell stress that was calculated and Youngs elastic module that was measured experimentally using nanoindentation. The conclusion as will be seen is that as there is a decrease in lithium ion concentration there is a corresponding decrease in anisotropic volume and a resulting increase in specific charge capacity. In fact the amplification of the electromagnetic field due to the electron flux that created detrimental effects for a fully lithiated silicon nanowire at x = 3.75 which resulted in over a 300 percent volume expansion becomes beneficial with the decrease in lithium ion flux as x approaches 0.75, which leads to a marginal volume increase of 25 percent. This could lead to the use of crystalline silicon, c-Si, as an anode material that has been demonstrated in many previous research works to be ten times greater charge capacity than carbon base anode material for lithium ion batteries.","sentences":["This computational research study analyzes the increase of the specific charge capacity that comes with the reduction of the anisotropic volume expansion during lithium ion insertion within silicon nanowires.","This research paper is a continuation from previous work that studied the expansion rate and volume increase.","It has been determined that when the lithium ion concentration is decreased by regulating the amount of Li ion flux, the lithium ions to silicon atoms ratio, represented by x, decreases within the amorphous lithiated silicon (a-LixSi) material.","This results in a decrease in the volumetric strain of the lithiated silicon nanowire as well as a reduction in Maxwell stress that was calculated and Youngs elastic module that was measured experimentally using nanoindentation.","The conclusion as will be seen is that as there is a decrease in lithium ion concentration there is a corresponding decrease in anisotropic volume and a resulting increase in specific charge capacity.","In fact the amplification of the electromagnetic field due to the electron flux that created detrimental effects for a fully lithiated silicon nanowire at x","= 3.75 which resulted in over a 300 percent volume expansion becomes beneficial with the decrease in lithium ion flux as x approaches 0.75, which leads to a marginal volume increase of 25 percent.","This could lead to the use of crystalline silicon, c-Si, as an anode material that has been demonstrated in many previous research works to be ten times greater charge capacity than carbon base anode material for lithium ion batteries."],"url":"http://arxiv.org/abs/2402.19428v1","category":"physics.app-ph"}
{"created":"2024-02-29 17:30:09","title":"Optimized Bayesian Framework for Inverse Heat Transfer Problems Using Reduced Order Methods","abstract":"A stochastic inverse heat transfer problem is formulated to infer the transient heat flux, treated as an unknown Neumann boundary condition. Therefore, an Ensemble-based Simultaneous Input and State Filtering as a Data Assimilation technique is utilized for simultaneous temperature distribution prediction and heat flux estimation. This approach is incorporated with Radial Basis Functions not only to lessen the size of unknown inputs but also to mitigate the computational burden of this technique. The procedure applies to the specific case of a mold used in Continuous Casting machinery, and it is based on the sequential availability of temperature provided by thermocouples inside the mold. Our research represents a significant contribution to achieving probabilistic boundary condition estimation in real-time handling with noisy measurements and errors in the model. We additionally demonstrate the procedure's dependence on some hyperparameters that are not documented in the existing literature. Accurate real-time prediction of the heat flux is imperative for the smooth operation of Continuous Casting machinery at the boundary region where the Continuous Casting mold and the molten steel meet which is not also physically measurable. Thus, this paves the way for efficient real-time monitoring and control, which is critical for preventing caster shutdowns.","sentences":["A stochastic inverse heat transfer problem is formulated to infer the transient heat flux, treated as an unknown Neumann boundary condition.","Therefore, an Ensemble-based Simultaneous Input and State Filtering as a Data Assimilation technique is utilized for simultaneous temperature distribution prediction and heat flux estimation.","This approach is incorporated with Radial Basis Functions not only to lessen the size of unknown inputs but also to mitigate the computational burden of this technique.","The procedure applies to the specific case of a mold used in Continuous Casting machinery, and it is based on the sequential availability of temperature provided by thermocouples inside the mold.","Our research represents a significant contribution to achieving probabilistic boundary condition estimation in real-time handling with noisy measurements and errors in the model.","We additionally demonstrate the procedure's dependence on some hyperparameters that are not documented in the existing literature.","Accurate real-time prediction of the heat flux is imperative for the smooth operation of Continuous Casting machinery at the boundary region where the Continuous Casting mold and the molten steel meet which is not also physically measurable.","Thus, this paves the way for efficient real-time monitoring and control, which is critical for preventing caster shutdowns."],"url":"http://arxiv.org/abs/2402.19381v1","category":"math.NA"}
{"created":"2024-02-29 16:51:38","title":"Multi-frequency tracking via group-sparse optimal transport","abstract":"In this work, we introduce an optimal transport framework for inferring power distributions over both spatial location and temporal frequency. Recently, it has been shown that optimal transport is a powerful tool for estimating spatial spectra that change smoothly over time. In this work, we consider the tracking of the spatio-temporal spectrum corresponding to a small number of moving broad-band signal sources. Typically, such tracking problems are addressed by treating the spatio-temporal power distribution in a frequency-by-frequency manner, allowing to use well-understood models for narrow-band signals. This however leads to decreased target resolution due to inefficient use of the available information. We propose an extension of the optimal transport framework that exploits information from several frequencies simultaneously by estimating a spatio-temporal distribution penalized by a group-sparsity regularizer. This approach finds a spatial spectrum that changes smoothly over time, and at each time instance has a small support that is similar across frequencies. To the best of the authors knowledge, this is the first formulation combining optimal transport and sparsity for solving inverse problems. As is shown on simulated and real data, our method can successfully track targets in scenarios where information from separate frequency bands alone is insufficient.","sentences":["In this work, we introduce an optimal transport framework for inferring power distributions over both spatial location and temporal frequency.","Recently, it has been shown that optimal transport is a powerful tool for estimating spatial spectra that change smoothly over time.","In this work, we consider the tracking of the spatio-temporal spectrum corresponding to a small number of moving broad-band signal sources.","Typically, such tracking problems are addressed by treating the spatio-temporal power distribution in a frequency-by-frequency manner, allowing to use well-understood models for narrow-band signals.","This however leads to decreased target resolution due to inefficient use of the available information.","We propose an extension of the optimal transport framework that exploits information from several frequencies simultaneously by estimating a spatio-temporal distribution penalized by a group-sparsity regularizer.","This approach finds a spatial spectrum that changes smoothly over time, and at each time instance has a small support that is similar across frequencies.","To the best of the authors knowledge, this is the first formulation combining optimal transport and sparsity for solving inverse problems.","As is shown on simulated and real data, our method can successfully track targets in scenarios where information from separate frequency bands alone is insufficient."],"url":"http://arxiv.org/abs/2402.19345v1","category":"math.OC"}
{"created":"2024-02-29 15:26:22","title":"More algorithmic results for problems of spread of influence in edge-weighted graphs with and without incentives","abstract":"Many phenomena in real world social networks are interpreted as spread of influence between activated and non-activated network elements. These phenomena are formulated by combinatorial graphs, where vertices represent the elements and edges represent social ties between elements. A main problem is to study important subsets of elements (target sets or dynamic monopolies) such that their activation spreads to the entire network. In edge-weighted networks the influence between two adjacent vertices depends on the weight of their edge. In models with incentives, the main problem is to minimize total amount of incentives (called optimal target vectors) which can be offered to vertices such that some vertices are activated and their activation spreads to the whole network. Algorithmic study of target sets and vectors is a hot research field. We prove an inapproximability result for optimal target sets in edge weighted networks even for complete graphs. Some other hardness and polynomial time results are presented for optimal target vectors and degenerate threshold assignments in edge-weighted networks.","sentences":["Many phenomena in real world social networks are interpreted as spread of influence between activated and non-activated network elements.","These phenomena are formulated by combinatorial graphs, where vertices represent the elements and edges represent social ties between elements.","A main problem is to study important subsets of elements (target sets or dynamic monopolies) such that their activation spreads to the entire network.","In edge-weighted networks the influence between two adjacent vertices depends on the weight of their edge.","In models with incentives, the main problem is to minimize total amount of incentives (called optimal target vectors) which can be offered to vertices such that some vertices are activated and their activation spreads to the whole network.","Algorithmic study of target sets and vectors is a hot research field.","We prove an inapproximability result for optimal target sets in edge weighted networks even for complete graphs.","Some other hardness and polynomial time results are presented for optimal target vectors and degenerate threshold assignments in edge-weighted networks."],"url":"http://arxiv.org/abs/2402.19257v1","category":"cs.DM"}
{"created":"2024-02-29 14:37:21","title":"Constrained hidden Markov models reveal further Hsp90 protein states","abstract":"Time series of conformational dynamics in proteins are usually evaluated with hidden Markov models (HMMs). This approach works well if the number of states and their connectivity is known. But for the multi-domain protein Hsp90, a standard HMM analysis with optimization of the BIC (Bayesian information criterion) cannot explain long-lived states well. Therefore, here we employ constrained hidden Markov models, which neglect transitions between states by including assumptions. Gradually tuning a model with justified and focused changes allows us to improve its effectiveness and the score of the BIC. This became possible by analyzing time traces with several thousand observable transitions and, therefore, superb statistics. In this scheme, we also monitor the residences in the states reconstructed by the model, aiming to find exponentially distributed dwell times. We show how introducing new states can achieve these statistics but also point out limitations, e.g., for substantial similarity of two states connected to a common neighbor. One of the states displays the lowest free energy and is likely the idle open `waiting state', in which Hsp90 waits for the binding of nucleotides, cochaperones, or clients.","sentences":["Time series of conformational dynamics in proteins are usually evaluated with hidden Markov models (HMMs).","This approach works well if the number of states and their connectivity is known.","But for the multi-domain protein Hsp90, a standard HMM analysis with optimization of the BIC (Bayesian information criterion) cannot explain long-lived states well.","Therefore, here we employ constrained hidden Markov models, which neglect transitions between states by including assumptions.","Gradually tuning a model with justified and focused changes allows us to improve its effectiveness and the score of the BIC.","This became possible by analyzing time traces with several thousand observable transitions and, therefore, superb statistics.","In this scheme, we also monitor the residences in the states reconstructed by the model, aiming to find exponentially distributed dwell times.","We show how introducing new states can achieve these statistics but also point out limitations, e.g., for substantial similarity of two states connected to a common neighbor.","One of the states displays the lowest free energy and is likely the idle open `waiting state', in which Hsp90 waits for the binding of nucleotides, cochaperones, or clients."],"url":"http://arxiv.org/abs/2402.19207v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-29 12:47:29","title":"Universal quantum computation using quantum annealing with the transverse-field Ising Hamiltonian","abstract":"Quantum computation is a promising emerging technology, and by utilizing the principles of quantum mechanics, it is expected to achieve faster computations than classical computers for specific problems. There are two distinct architectures for quantum computation: gate-based quantum computers and quantum annealing. In gate-based quantum computation, we implement a sequence of quantum gates that manipulate qubits. This approach allows us to perform universal quantum computation, yet they pose significant experimental challenges for large-scale integration. On the other hand, with quantum annealing, the solution of the optimization problem can be obtained by preparing the ground state. Conventional quantum annealing devices with transverse-field Ising Hamiltonian, such as those manufactured by D-Wave Inc., achieving around 5000 qubits, are relatively more amenable to large-scale integration but are limited to specific computations. In this paper, we present a practical method for implementing universal quantum computation within the conventional quantum annealing architecture using the transverse-field Ising Hamiltonian. Our innovative approach relies on an adiabatic transformation of the Hamiltonian, changing from transverse fields to a ferromagnetic interaction regime, where the ground states become degenerate. Notably, our proposal is compatible with D-Wave devices, opening up possibilities for realizing large-scale gate-based quantum computers. This research bridges the gap between conventional quantum annealing and gate-based quantum computation, offering a promising path toward the development of scalable quantum computing platforms.","sentences":["Quantum computation is a promising emerging technology, and by utilizing the principles of quantum mechanics, it is expected to achieve faster computations than classical computers for specific problems.","There are two distinct architectures for quantum computation: gate-based quantum computers and quantum annealing.","In gate-based quantum computation, we implement a sequence of quantum gates that manipulate qubits.","This approach allows us to perform universal quantum computation, yet they pose significant experimental challenges for large-scale integration.","On the other hand, with quantum annealing, the solution of the optimization problem can be obtained by preparing the ground state.","Conventional quantum annealing devices with transverse-field Ising Hamiltonian, such as those manufactured by D-Wave Inc., achieving around 5000 qubits, are relatively more amenable to large-scale integration but are limited to specific computations.","In this paper, we present a practical method for implementing universal quantum computation within the conventional quantum annealing architecture using the transverse-field Ising Hamiltonian.","Our innovative approach relies on an adiabatic transformation of the Hamiltonian, changing from transverse fields to a ferromagnetic interaction regime, where the ground states become degenerate.","Notably, our proposal is compatible with D-Wave devices, opening up possibilities for realizing large-scale gate-based quantum computers.","This research bridges the gap between conventional quantum annealing and gate-based quantum computation, offering a promising path toward the development of scalable quantum computing platforms."],"url":"http://arxiv.org/abs/2402.19114v1","category":"quant-ph"}
{"created":"2024-02-29 12:40:58","title":"Confidence and Assurance of Percentiles","abstract":"Confidence interval of mean is often used when quoting statistics. The same rigor is often missing when quoting percentiles and tolerance or percentile intervals. This article derives the expression for confidence in percentiles of a sample population. Confidence intervals of median is compared to those of mean for a few sample distributions. The concept of assurance from reliability engineering is then extended to percentiles. The assurance level of sorted samples simply matches the confidence and percentile levels. Numerical method to compute assurance using Brent's optimization method is provided as an open-source python package.","sentences":["Confidence interval of mean is often used when quoting statistics.","The same rigor is often missing when quoting percentiles and tolerance or percentile intervals.","This article derives the expression for confidence in percentiles of a sample population.","Confidence intervals of median is compared to those of mean for a few sample distributions.","The concept of assurance from reliability engineering is then extended to percentiles.","The assurance level of sorted samples simply matches the confidence and percentile levels.","Numerical method to compute assurance using Brent's optimization method is provided as an open-source python package."],"url":"http://arxiv.org/abs/2402.19109v1","category":"stat.ME"}
{"created":"2024-02-29 11:47:48","title":"Spline-Based Rotor and Stator Optimization of a Permanent Magnet Synchronous Motor","abstract":"This work features the optimization of a Permanent Magnet Synchronous Motor using 2D nonlinear simulations in an Isogeometric Analysis framework. The rotor and stator designs are optimized for both geometric parameters and surface shapes via modifications of control points. The scaling laws for magnetism are employed to allow for axial and radial scaling, enabling a thorough optimization of all critical machine parameters for multiple operating points. The process is carried out in a gradient-based fashion with the objectives of lowering motor material cost, torque ripple and losses. It is shown that the optimization can be efficiently conducted for many optimization variables and all objective values can be reduced.","sentences":["This work features the optimization of a Permanent Magnet Synchronous Motor using 2D nonlinear simulations in an Isogeometric Analysis framework.","The rotor and stator designs are optimized for both geometric parameters and surface shapes via modifications of control points.","The scaling laws for magnetism are employed to allow for axial and radial scaling, enabling a thorough optimization of all critical machine parameters for multiple operating points.","The process is carried out in a gradient-based fashion with the objectives of lowering motor material cost, torque ripple and losses.","It is shown that the optimization can be efficiently conducted for many optimization variables and all objective values can be reduced."],"url":"http://arxiv.org/abs/2402.19065v1","category":"math.OC"}
{"created":"2024-02-29 11:18:28","title":"DMSA -- Dense Multi Scan Adjustment for LiDAR Inertial Odometry and Global Optimization","abstract":"We propose a new method for fine registering multiple point clouds simultaneously. The approach is characterized by being dense, therefore point clouds are not reduced to pre-selected features in advance. Furthermore, the approach is robust against small overlaps and dynamic objects, since no direct correspondences are assumed between point clouds. Instead, all points are merged into a global point cloud, whose scattering is then iteratively reduced. This is achieved by dividing the global point cloud into uniform grid cells whose contents are subsequently modeled by normal distributions. We show that the proposed approach can be used in a sliding window continuous trajectory optimization combined with IMU measurements to obtain a highly accurate and robust LiDAR inertial odometry estimation. Furthermore, we show that the proposed approach is also suitable for large scale keyframe optimization to increase accuracy. We provide the source code and some experimental data on https://github.com/davidskdds/DMSA_LiDAR_SLAM.git.","sentences":["We propose a new method for fine registering multiple point clouds simultaneously.","The approach is characterized by being dense, therefore point clouds are not reduced to pre-selected features in advance.","Furthermore, the approach is robust against small overlaps and dynamic objects, since no direct correspondences are assumed between point clouds.","Instead, all points are merged into a global point cloud, whose scattering is then iteratively reduced.","This is achieved by dividing the global point cloud into uniform grid cells whose contents are subsequently modeled by normal distributions.","We show that the proposed approach can be used in a sliding window continuous trajectory optimization combined with IMU measurements to obtain a highly accurate and robust LiDAR inertial odometry estimation.","Furthermore, we show that the proposed approach is also suitable for large scale keyframe optimization to increase accuracy.","We provide the source code and some experimental data on https://github.com/davidskdds/DMSA_LiDAR_SLAM.git."],"url":"http://arxiv.org/abs/2402.19044v1","category":"cs.RO"}
{"created":"2024-02-29 09:44:26","title":"Variability mitigation in epitaxial-heterostructure-based spin qubit devices via gate layout optimization","abstract":"The scalability of spin qubit devices is conditioned by qubit-to-qubit variability. Disorder in the host materials indeed affects the wave functions of the confined carriers, which leads to variations in their charge and spin properties. Charge disorder in the amorphous oxides is particularly detrimental owing to its long-range influence. Here we analyze the effects of charge traps at the semiconductor/oxide interface, which are generally believed to play a dominant role in variability. We consider multiple random distributions of these interface traps and numerically calculate their impact on the chemical potentials, detuning and tunnel coupling of two adjacent quantum dots in SiGe heterostructure. Our results highlight the beneficial screening effect of the metal gates. The surface of the heterostructure shall, therefore, be covered as much as possible by the gates in order to limit variability. We propose an alternative layout with tip-shaped gates that maximizes the coverage of the semiconductor/oxide interface and outperforms the usual planar layout in some regimes. This highlights the importance of design in the management of device-to-device variability.","sentences":["The scalability of spin qubit devices is conditioned by qubit-to-qubit variability.","Disorder in the host materials indeed affects the wave functions of the confined carriers, which leads to variations in their charge and spin properties.","Charge disorder in the amorphous oxides is particularly detrimental owing to its long-range influence.","Here we analyze the effects of charge traps at the semiconductor/oxide interface, which are generally believed to play a dominant role in variability.","We consider multiple random distributions of these interface traps and numerically calculate their impact on the chemical potentials, detuning and tunnel coupling of two adjacent quantum dots in SiGe heterostructure.","Our results highlight the beneficial screening effect of the metal gates.","The surface of the heterostructure shall, therefore, be covered as much as possible by the gates in order to limit variability.","We propose an alternative layout with tip-shaped gates that maximizes the coverage of the semiconductor/oxide interface and outperforms the usual planar layout in some regimes.","This highlights the importance of design in the management of device-to-device variability."],"url":"http://arxiv.org/abs/2402.18991v1","category":"cond-mat.mes-hall"}
{"created":"2024-02-29 09:32:34","title":"Percolation threshold and critical point of a nuclear reactor","abstract":"Neutrons in a nuclear reactor move along trajectories corresponding to Cayley trees associated with branching random processes. The probability of percolation, the appearance of such a state of the Bethe lattice in which there is at least one continuous path through neighboring conducting nodes through the entire lattice, corresponds to the probability of the occurrence of a self-sustaining fission chain reaction. At a critical value of the last probability, a (conditionally) infinite cluster of neutrons appears. The probability of percolation, depending on the operating time of the reactor and its size, is associated with the criticality of the reactor. The temporal behavior of the neutron multiplication factor is estimated. Particular attention is paid to the early stages of the development of a self-sustaining chain reaction of nuclear fission. The possibilities of determining the boundaries of the critical region are indicated.","sentences":["Neutrons in a nuclear reactor move along trajectories corresponding to Cayley trees associated with branching random processes.","The probability of percolation, the appearance of such a state of the Bethe lattice in which there is at least one continuous path through neighboring conducting nodes through the entire lattice, corresponds to the probability of the occurrence of a self-sustaining fission chain reaction.","At a critical value of the last probability, a (conditionally) infinite cluster of neutrons appears.","The probability of percolation, depending on the operating time of the reactor and its size, is associated with the criticality of the reactor.","The temporal behavior of the neutron multiplication factor is estimated.","Particular attention is paid to the early stages of the development of a self-sustaining chain reaction of nuclear fission.","The possibilities of determining the boundaries of the critical region are indicated."],"url":"http://arxiv.org/abs/2402.18979v1","category":"cond-mat.stat-mech"}
{"created":"2024-02-29 08:52:12","title":"Vibrational properties differ between halide and chalcogenide perovskite semiconductors, and it matters for optoelectronic performance","abstract":"We report a comparative study of temperature-dependent photoluminescence and structural dynamics of two perovskite semiconductors, the chalcogenide BaZrS$_3$ (BZS) and the halide CsPbBr$_3$ (CPB). These materials have similar crystal structures and direct band gaps, but we find that they have quite distinct optoelectronic and vibrational properties. Both materials exhibit thermally-activated non-radiative recombination, but the non-radiative recombination rate in BZS is between two and four orders-of-magnitude faster than in CPB. Raman spectroscopy reveals that the effects of phonon anharmonicity are far more pronounced in CPB than in BZS. Further, although both materials feature a large dielectric response due to low-energy polar optical phonons, the phonons in CPB are substantially lower in energy than in BZS. Our results suggest that electron-phonon coupling in BZS is more effective at non-radiative recombination than in CPB, and that BZS may also have a substantially higher concentration of non-radiative recombination centers than CPB. The low defect concentration in CPB may be related to the ease of lattice reconfiguration, typified by anharmonic bonding. It remains to be seen to what extent these differences are inherent to the chalcogenide and halide perovskites and to what extent they can be affected by materials processing; comparing BZS single-crystals and thin films provides reason for optimism.","sentences":["We report a comparative study of temperature-dependent photoluminescence and structural dynamics of two perovskite semiconductors, the chalcogenide BaZrS$_3$ (BZS) and the halide CsPbBr$_3$ (CPB).","These materials have similar crystal structures and direct band gaps, but we find that they have quite distinct optoelectronic and vibrational properties.","Both materials exhibit thermally-activated non-radiative recombination, but the non-radiative recombination rate in BZS is between two and four orders-of-magnitude faster than in CPB.","Raman spectroscopy reveals that the effects of phonon anharmonicity are far more pronounced in CPB than in BZS.","Further, although both materials feature a large dielectric response due to low-energy polar optical phonons, the phonons in CPB are substantially lower in energy than in BZS.","Our results suggest that electron-phonon coupling in BZS is more effective at non-radiative recombination than in CPB, and that BZS may also have a substantially higher concentration of non-radiative recombination centers than CPB.","The low defect concentration in CPB may be related to the ease of lattice reconfiguration, typified by anharmonic bonding.","It remains to be seen to what extent these differences are inherent to the chalcogenide and halide perovskites and to what extent they can be affected by materials processing; comparing BZS single-crystals and thin films provides reason for optimism."],"url":"http://arxiv.org/abs/2402.18957v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-02-29 08:37:44","title":"Santal\u00f3 Geometry of Convex Polytopes","abstract":"The Santal\\'o point of a convex polytope is the interior point which leads to a polar dual of minimal volume. This minimization problem is relevant in interior point methods for convex optimization, where the logarithm of the dual volume is known as the universal barrier function. When translating the facet hyperplanes, the Santal\\'o point traces out a semi-algebraic set. We describe and compute this geometry using algebraic and numerical techniques. We exploit connections with statistics, optimization and physics.","sentences":["The Santal\\'o point of a convex polytope is the interior point which leads to a polar dual of minimal volume.","This minimization problem is relevant in interior point methods for convex optimization, where the logarithm of the dual volume is known as the universal barrier function.","When translating the facet hyperplanes, the Santal\\'o point traces out a semi-algebraic set.","We describe and compute this geometry using algebraic and numerical techniques.","We exploit connections with statistics, optimization and physics."],"url":"http://arxiv.org/abs/2402.18955v1","category":"math.AG"}
{"created":"2024-02-29 08:34:11","title":"Robust Experimental Signatures of Phase Transitions in the Variational Quantum Eigensolver","abstract":"The Variational Quantum Eigensolver (VQE) is widely considered to be a promising candidate for a quantum-classical algorithm which could achieve near-term quantum advantage. However, current levels of hardware noise can require extensive application of error-mitigation techniques to achieve reliable computations. In this work, we use several IBM devices to explore a finite-size spin model with multiple `phase-like' regions characterized by distinct ground-state configurations. Using pre-optimized VQE solutions, we demonstrate that in contrast to calculating the energy, where zero-noise extrapolation is required in order to obtain qualitatively accurate yet still unreliable results, calculations of the energy derivative, two-site spin correlation functions, and the fidelity susceptibility yield accurate behavior across multiple regions, even with minimal or no application of error-mitigation approaches. Taken together, these sets of observables could be used to identify level crossings in VQE solutions in a simple and noise-robust manner, with potential near-term application to identifying quantum phase transitions, avoided crossings and non-adiabatic conical intersections in electronic structure calculations.","sentences":["The Variational Quantum Eigensolver (VQE) is widely considered to be a promising candidate for a quantum-classical algorithm which could achieve near-term quantum advantage.","However, current levels of hardware noise can require extensive application of error-mitigation techniques to achieve reliable computations.","In this work, we use several IBM devices to explore a finite-size spin model with multiple `phase-like' regions characterized by distinct ground-state configurations.","Using pre-optimized VQE solutions, we demonstrate that in contrast to calculating the energy, where zero-noise extrapolation is required in order to obtain qualitatively accurate yet still unreliable results, calculations of the energy derivative, two-site spin correlation functions, and the fidelity susceptibility yield accurate behavior across multiple regions, even with minimal or no application of error-mitigation approaches.","Taken together, these sets of observables could be used to identify level crossings in VQE solutions in a simple and noise-robust manner, with potential near-term application to identifying quantum phase transitions, avoided crossings and non-adiabatic conical intersections in electronic structure calculations."],"url":"http://arxiv.org/abs/2402.18953v1","category":"quant-ph"}
{"created":"2024-02-29 08:01:47","title":"RELEAD: Resilient Localization with Enhanced LiDAR Odometry in Adverse Environments","abstract":"LiDAR-based localization is valuable for applications like mining surveys and underground facility maintenance. However, existing methods can struggle when dealing with uninformative geometric structures in challenging scenarios. This paper presents RELEAD, a LiDAR-centric solution designed to address scan-matching degradation. Our method enables degeneracy-free point cloud registration by solving constrained ESIKF updates in the front end and incorporates multisensor constraints, even when dealing with outlier measurements, through graph optimization based on Graduated Non-Convexity (GNC). Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL) for efficient GNC-based optimization. RELEAD has undergone extensive evaluation in degenerate scenarios and has outperformed existing state-of-the-art LiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods.","sentences":["LiDAR-based localization is valuable for applications like mining surveys and underground facility maintenance.","However, existing methods can struggle when dealing with uninformative geometric structures in challenging scenarios.","This paper presents RELEAD, a LiDAR-centric solution designed to address scan-matching degradation.","Our method enables degeneracy-free point cloud registration by solving constrained ESIKF updates in the front end and incorporates multisensor constraints, even when dealing with outlier measurements, through graph optimization based on Graduated Non-Convexity (GNC).","Additionally, we propose a robust Incremental Fixed Lag Smoother (rIFL) for efficient GNC-based optimization.","RELEAD has undergone extensive evaluation in degenerate scenarios and has outperformed existing state-of-the-art LiDAR-Inertial odometry and LiDAR-Visual-Inertial odometry methods."],"url":"http://arxiv.org/abs/2402.18934v1","category":"cs.RO"}
{"created":"2024-02-29 05:00:20","title":"Simultaneous vibrational resonance in the amplitude and phase quadratures of an optical field based on Kerr nonlinearity","abstract":"Vibrational resonance (VR) is a nonlinear phenomenon in which the system response to a weak signal can be resonantly enhanced by applying a high-frequency modulation signal with an appropriate amplitude. The majority of VR research has focused on amplifying the amplitude or intensity of the system response to a weak signal, whereas the study of the phase information of system responses in VR remains limited. Here, we investigate the VR phenomena in both amplitude and phase quadratures of an optical field in a Kerr nonlinear cavity driven by a near-resonant weak signal and a far-detuned modulation signal. Analytical and numerical results demonstrated that the resonant enhancement in the amplitude and phase quadratures of the system response to a weak signal simultaneously occurs as the amplitude of the modulation signal is varied. There is a linear relation between the amplitude and frequency of the modulation signal for achieving an optimal VR effect. Furthermore, we generalized our study to investigate the quadrature at an arbitrary phase and determined that the VR enhancement sensitively depends on the phase. Our findings not only broaden the scope of VR research by incorporating phase information but also introduces an approach for amplifying an optical field by manipulating another optical field.","sentences":["Vibrational resonance (VR) is a nonlinear phenomenon in which the system response to a weak signal can be resonantly enhanced by applying a high-frequency modulation signal with an appropriate amplitude.","The majority of VR research has focused on amplifying the amplitude or intensity of the system response to a weak signal, whereas the study of the phase information of system responses in VR remains limited.","Here, we investigate the VR phenomena in both amplitude and phase quadratures of an optical field in a Kerr nonlinear cavity driven by a near-resonant weak signal and a far-detuned modulation signal.","Analytical and numerical results demonstrated that the resonant enhancement in the amplitude and phase quadratures of the system response to a weak signal simultaneously occurs as the amplitude of the modulation signal is varied.","There is a linear relation between the amplitude and frequency of the modulation signal for achieving an optimal VR effect.","Furthermore, we generalized our study to investigate the quadrature at an arbitrary phase and determined that the VR enhancement sensitively depends on the phase.","Our findings not only broaden the scope of VR research by incorporating phase information but also introduces an approach for amplifying an optical field by manipulating another optical field."],"url":"http://arxiv.org/abs/2402.18852v1","category":"physics.optics"}
{"created":"2024-02-29 04:43:40","title":"Flexible Precoding for Multi-User Movable Antenna Communications","abstract":"This letter rethinks traditional precoding in multi-user wireless communications with movable antennas (MAs). Utilizing MAs for optimal antenna positioning, we introduce a sparse optimization (SO)-based approach focusing on regularized zero-forcing (RZF). This framework targets the optimization of antenna positions and the precoding matrix to minimize inter-user interference and transmit power. We propose an off-grid regularized least squares-based orthogonal matching pursuit (RLS-OMP) method for this purpose. Moreover, we provide deeper insights into antenna position optimization using RLS-OMP, viewed from a subspace projection angle. Overall, our proposed flexible precoding scheme demonstrates a sum rate that exceeds more than twice that of fixed antenna positions.","sentences":["This letter rethinks traditional precoding in multi-user wireless communications with movable antennas (MAs).","Utilizing MAs for optimal antenna positioning, we introduce a sparse optimization (SO)-based approach focusing on regularized zero-forcing (RZF).","This framework targets the optimization of antenna positions and the precoding matrix to minimize inter-user interference and transmit power.","We propose an off-grid regularized least squares-based orthogonal matching pursuit (RLS-OMP) method for this purpose.","Moreover, we provide deeper insights into antenna position optimization using RLS-OMP, viewed from a subspace projection angle.","Overall, our proposed flexible precoding scheme demonstrates a sum rate that exceeds more than twice that of fixed antenna positions."],"url":"http://arxiv.org/abs/2402.18847v1","category":"cs.IT"}
{"created":"2024-02-29 03:20:45","title":"Utilizing Local Hierarchy with Adversarial Training for Hierarchical Text Classification","abstract":"Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure. Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input text which contains fruitful label co-occurrence information. In this work, we introduce this local hierarchy with an adversarial framework. We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information. We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies. Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data.","sentences":["Hierarchical text classification (HTC) is a challenging subtask of multi-label classification due to its complex taxonomic structure.","Nearly all recent HTC works focus on how the labels are structured but ignore the sub-structure of ground-truth labels according to each input text which contains fruitful label co-occurrence information.","In this work, we introduce this local hierarchy with an adversarial framework.","We propose a HiAdv framework that can fit in nearly all HTC models and optimize them with the local hierarchy as auxiliary information.","We test on two typical HTC models and find that HiAdv is effective in all scenarios and is adept at dealing with complex taxonomic hierarchies.","Further experiments demonstrate that the promotion of our framework indeed comes from the local hierarchy and the local hierarchy is beneficial for rare classes which have insufficient training data."],"url":"http://arxiv.org/abs/2402.18825v1","category":"cs.CL"}
{"created":"2024-02-29 01:58:49","title":"ARTiST: Automated Text Simplification for Task Guidance in Augmented Reality","abstract":"Text presented in augmented reality provides in-situ, real-time information for users. However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display. We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality. Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays. Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods. Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality.","sentences":["Text presented in augmented reality provides in-situ, real-time information for users.","However, this content can be challenging to apprehend quickly when engaging in cognitively demanding AR tasks, especially when it is presented on a head-mounted display.","We propose ARTiST, an automatic text simplification system that uses a few-shot prompt and GPT-3 models to specifically optimize the text length and semantic content for augmented reality.","Developed out of a formative study that included seven users and three experts, our system combines a customized error calibration model with a few-shot prompt to integrate the syntactic, lexical, elaborative, and content simplification techniques, and generate simplified AR text for head-worn displays.","Results from a 16-user empirical study showed that ARTiST lightens the cognitive load and improves performance significantly over both unmodified text and text modified via traditional methods.","Our work constitutes a step towards automating the optimization of batch text data for readability and performance in augmented reality."],"url":"http://arxiv.org/abs/2402.18797v1","category":"cs.HC"}
{"created":"2024-02-29 01:55:49","title":"Towards Large-scale Probabilistic Set Covering Problem: An Efficient Benders Decomposition Approach","abstract":"In this paper, we investigate the probabilistic set covering problems (PSCP) in which the right-hand side is a random vector {\\xi} and the covering constraint is required to be satisfied with a prespecified probability. We consider the case arising from sample average approximation (or finite discrete distributions). We develop an effective Benders decomposition (BD) algorithm for solving large-scale PSCPs, which enjoys two key advantages: (i) the number of variables in the underlying Benders reformulation is independent of the scenario size; and (ii) the Benders cuts can be separated by an efficient combinatorial algorithm. For the special case that {\\xi} is a combination of several independent random blocks/subvectors, we explicitly take this kind of block structure into consideration and develop a more efficient BD algorithm. Numerical results on instances with up to one million scenarios demonstrate the effectiveness of the proposed BD algorithms over a black-box MIP solver's branch-and-cut and automatic BD algorithms and a state-of-the-art algorithm in the literature.","sentences":["In this paper, we investigate the probabilistic set covering problems (PSCP) in which the right-hand side is a random vector {\\xi} and the covering constraint is required to be satisfied with a prespecified probability.","We consider the case arising from sample average approximation (or finite discrete distributions).","We develop an effective Benders decomposition (BD) algorithm for solving large-scale PSCPs, which enjoys two key advantages: (i) the number of variables in the underlying Benders reformulation is independent of the scenario size; and (ii) the Benders cuts can be separated by an efficient combinatorial algorithm.","For the special case that {\\xi} is a combination of several independent random blocks/subvectors, we explicitly take this kind of block structure into consideration and develop a more efficient BD algorithm.","Numerical results on instances with up to one million scenarios demonstrate the effectiveness of the proposed BD algorithms over a black-box MIP solver's branch-and-cut and automatic BD algorithms and a state-of-the-art algorithm in the literature."],"url":"http://arxiv.org/abs/2402.18795v1","category":"math.OC"}
{"created":"2024-02-29 01:20:29","title":"OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition","abstract":"Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy. Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks. In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features. It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR. More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner. These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR). Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively.","sentences":["Depression Recognition (DR) poses a considerable challenge, especially in the context of the growing concerns surrounding privacy.","Traditional automatic diagnosis of DR technology necessitates the use of facial images, undoubtedly expose the patient identity features and poses privacy risks.","In order to mitigate the potential risks associated with the inappropriate disclosure of patient facial images, we design a new imaging system to erase the identity information of captured facial images while retain disease-relevant features.","It is irreversible for identity information recovery while preserving essential disease-related characteristics necessary for accurate DR.","More specifically, we try to record a de-identified facial image (erasing the identifiable features as much as possible) by a learnable lens, which is optimized in conjunction with the following DR task as well as a range of face analysis related auxiliary tasks in an end-to-end manner.","These aforementioned strategies form our final Optical deep Depression Recognition network (OpticalDR).","Experiments on CelebA, AVEC 2013, and AVEC 2014 datasets demonstrate that our OpticalDR has achieved state-of-the-art privacy protection performance with an average AUC of 0.51 on popular facial recognition models, and competitive results for DR with MAE/RMSE of 7.53/8.48 on AVEC 2013 and 7.89/8.82 on AVEC 2014, respectively."],"url":"http://arxiv.org/abs/2402.18786v1","category":"cs.CV"}
{"created":"2024-02-29 00:49:20","title":"X-ResQ: Reverse Annealing for Quantum MIMO Detection with Flexible Parallelism","abstract":"Quantum Annealing (QA)-accelerated MIMO detection is an emerging research approach in the context of NextG wireless networks. The opportunity is to enable large MIMO systems and thus improve wireless performance. The approach aims to leverage QA to expedite the computation required for theoretically optimal but computationally-demanding Maximum Likelihood detection to overcome the limitations of the currently deployed linear detectors. This paper presents \\textbf{X-ResQ}, a QA-based MIMO detector system featuring fine-grained quantum task parallelism that is uniquely enabled by the Reverse Annealing (RA) protocol. Unlike prior designs, X-ResQ has many desirable system properties for a parallel QA detector and has effectively improved detection performance as more qubits are assigned. In our evaluations on a state-of-the-art quantum annealer, fully parallel X-ResQ achieves near-optimal throughput (over 10 bits/s/Hz) for $4\\times6$ MIMO with 16-QAM using six levels of parallelism with 240 qubits and $220~\\mu$s QA compute time, achieving 2.5--5$\\times$ gains compared against other tested detectors. For more comprehensive evaluations, we implement and evaluate X-ResQ in the non-quantum digital setting. This non-quantum X-ResQ demonstration showcases the potential to realize ultra-large $1024\\times1024$ MIMO, significantly outperforming other MIMO detectors, including the state-of-the-art RA detector classically implemented in the same way.","sentences":["Quantum Annealing (QA)-accelerated MIMO detection is an emerging research approach in the context of NextG wireless networks.","The opportunity is to enable large MIMO systems and thus improve wireless performance.","The approach aims to leverage QA to expedite the computation required for theoretically optimal but computationally-demanding Maximum Likelihood detection to overcome the limitations of the currently deployed linear detectors.","This paper presents \\textbf{X-ResQ}, a QA-based MIMO detector system featuring fine-grained quantum task parallelism that is uniquely enabled by the Reverse Annealing (RA) protocol.","Unlike prior designs, X-ResQ has many desirable system properties for a parallel QA detector and has effectively improved detection performance as more qubits are assigned.","In our evaluations on a state-of-the-art quantum annealer, fully parallel X-ResQ achieves near-optimal throughput (over 10 bits/s/Hz) for $4\\times6$ MIMO with 16-QAM using six levels of parallelism with 240 qubits and $220~\\mu$s QA compute time, achieving 2.5--5$\\times$ gains compared against other tested detectors.","For more comprehensive evaluations, we implement and evaluate X-ResQ in the non-quantum digital setting.","This non-quantum X-ResQ demonstration showcases the potential to realize ultra-large $1024\\times1024$ MIMO, significantly outperforming other MIMO detectors, including the state-of-the-art RA detector classically implemented in the same way."],"url":"http://arxiv.org/abs/2402.18778v1","category":"cs.NI"}
{"created":"2024-02-29 00:21:22","title":"Particle-conserving quantum circuit ansatz with applications in variational simulation of bosonic systems","abstract":"Constrained problems are frequently encountered in classical and quantum optimization. Particle conservation, in particular, is commonly imposed when studying energy spectra of chemical and solid state systems. Though particle number-constraining techniques have been developed for fermionic (e.g. molecular electronic structure) Hamiltonians, analogous techniques are lacking for non-binary and non-fermionic problems, as in the case of bosonic systems or classical optimization problems over integer variables. Here we introduce the binary encoded multilevel particles circuit ansatz (BEMPA) -- an ansatz which preserves particle count by construction -- for use in quantum variational algorithms. The key insight is to build the circuit blocks by carefully positioning a set of symmetry-preserving 2- and 3-qubit gates. We numerically analyze the problem of finding the ground state eigenvalues -- via the Variational Quantum Eigensolver (VQE) algorithm -- of the Bose-Hubbard Hamiltonian. For a range of model parameters spanning from Mott insulator to superfluid phase, we demonstrate that our proposed circuit ansatz finds the ground state eigenvalues within drastically shorter runtimes compared to penalty-based strategies methods. Finally, we analyze the potential resource benefits of changing the qubit encoding at the end of the optimization routine. Our results attest to the efficacy of BEMPA for simulating bosonic problems for which particle number is preserved.","sentences":["Constrained problems are frequently encountered in classical and quantum optimization.","Particle conservation, in particular, is commonly imposed when studying energy spectra of chemical and solid state systems.","Though particle number-constraining techniques have been developed for fermionic (e.g. molecular electronic structure) Hamiltonians, analogous techniques are lacking for non-binary and non-fermionic problems, as in the case of bosonic systems or classical optimization problems over integer variables.","Here we introduce the binary encoded multilevel particles circuit ansatz (BEMPA) -- an ansatz which preserves particle count by construction -- for use in quantum variational algorithms.","The key insight is to build the circuit blocks by carefully positioning a set of symmetry-preserving 2- and 3-qubit gates.","We numerically analyze the problem of finding the ground state eigenvalues -- via the Variational Quantum Eigensolver (VQE) algorithm -- of the Bose-Hubbard Hamiltonian.","For a range of model parameters spanning from Mott insulator to superfluid phase, we demonstrate that our proposed circuit ansatz finds the ground state eigenvalues within drastically shorter runtimes compared to penalty-based strategies methods.","Finally, we analyze the potential resource benefits of changing the qubit encoding at the end of the optimization routine.","Our results attest to the efficacy of BEMPA for simulating bosonic problems for which particle number is preserved."],"url":"http://arxiv.org/abs/2402.18768v1","category":"quant-ph"}
{"created":"2024-02-29 00:18:57","title":"Limits of noisy quantum metrology with restricted quantum controls","abstract":"The Heisenberg limit (HL) and the standard quantum limit (SQL) are two quantum metrological limits, which describe the scalings of estimation precision $\\Delta \\hat\\theta$ of an unknown parameter $\\theta$ with respect to $n$, the number of one-parameter quantum channels applied. It was known that the HL ($\\Delta \\hat\\theta \\propto 1/n$) is achievable using quantum error correction (QEC) strategies when the ``Hamiltonian-not-in-Kraus-span'' (HNKS) condition is satisfied; and when HNKS is violated, the SQL ($\\Delta \\hat\\theta \\propto 1/n^{1/2}$) is optimal and can be achieved with $n$ repeated measurements. However, it is unknown whether such limits are still achievable using restricted quantum devices where the required QEC operations are not available -- e.g., finite-size devices where only unitary controls are available or where noiseless ancilla is not available. In this work, we identify various new noisy metrological limits for estimating one-parameter qubit channels in different settings with restricted controls. The HL is proven to be unattainable in these cases, indicating the necessity of QEC in achieving the HL. Furthermore, we find a necessary and sufficient condition for qubit channels to attain the SQL, called the ``rotation-generators-not-in-Kraus-span'' (RGNKS) condition. When RGNKS is satisfied, the SQL is achievable using only unitary controls and a single measurement. When RGNKS is violated, the estimation precision (in most cases) has a constant floor when repeated measurements are not allowed. Demonstration of this separation in metrological powers is within reach of current quantum technologies.","sentences":["The Heisenberg limit (HL) and the standard quantum limit (SQL) are two quantum metrological limits, which describe the scalings of estimation precision $\\Delta \\hat\\theta$ of an unknown parameter $\\theta$ with respect to $n$, the number of one-parameter quantum channels applied.","It was known that the HL ($\\Delta \\hat\\theta \\propto 1/n$) is achievable using quantum error correction (QEC) strategies when the ``Hamiltonian-not-in-Kraus-span'' (HNKS) condition is satisfied; and when HNKS is violated, the SQL ($\\Delta \\hat\\theta \\propto 1/n^{1/2}$) is optimal and can be achieved with $n$ repeated measurements.","However, it is unknown whether such limits are still achievable using restricted quantum devices where the required QEC operations are not available -- e.g., finite-size devices where only unitary controls are available or where noiseless ancilla is not available.","In this work, we identify various new noisy metrological limits for estimating one-parameter qubit channels in different settings with restricted controls.","The HL is proven to be unattainable in these cases, indicating the necessity of QEC in achieving the HL.","Furthermore, we find a necessary and sufficient condition for qubit channels to attain the SQL, called the ``rotation-generators-not-in-Kraus-span'' (RGNKS) condition.","When RGNKS is satisfied, the SQL is achievable using only unitary controls and a single measurement.","When RGNKS is violated, the estimation precision (in most cases) has a constant floor when repeated measurements are not allowed.","Demonstration of this separation in metrological powers is within reach of current quantum technologies."],"url":"http://arxiv.org/abs/2402.18765v1","category":"quant-ph"}
{"created":"2024-02-29 00:12:55","title":"An Analytical Approach to (Meta)Relational Models Theory, and its Application to Triple Bottom Line (Profit, People, Planet) -- Towards Social Relations Portfolio Management","abstract":"Investigating the optimal nature of social interactions among generic actors (e.g., people or firms), aiming to achieve specifically-agreed objectives, has been the subject of extensive academic research. Using the relational models theory - comprehensively describing all social interactions among actors as combinations of only four forms of sociality: communal sharing, authority ranking, equality matching, and market pricing - the common approach within the literature revolves around qualitative assessments of the sociality models' configurations most effective in realizing predefined purposes, at times supplemented by empirical data. In this treatment, we formulate this question as a mathematical optimization problem, in order to quantitatively determine the best possible configurations of sociality forms between dyadic actors which would optimize their mutually-agreed objectives. For this purpose, we develop an analytical framework for quantifying the (meta)relational models theory, and mathematically demonstrate that combining the four sociality forms within a specific meaningful social interaction inevitably prompts an inherent tension among them, through a single elementary and universal metarelation. In analogy with financial portfolio management, we subsequently introduce the concept of Social Relations Portfolio (SRP) management, and propose a generalizable procedural methodology capable of quantitatively identifying the efficient SRP for any objective involving meaningful social relations. As an important illustration, the methodology is applied to the Triple Bottom Line paradigm to derive its efficient SRP, guiding practitioners in precisely measuring, monitoring, reporting and (proactively) steering stakeholder management efforts regarding Corporate Social Responsibility (CSR) and Environmental, Social and Governance (ESG) within and / or across organizations.","sentences":["Investigating the optimal nature of social interactions among generic actors (e.g., people or firms), aiming to achieve specifically-agreed objectives, has been the subject of extensive academic research.","Using the relational models theory - comprehensively describing all social interactions among actors as combinations of only four forms of sociality: communal sharing, authority ranking, equality matching, and market pricing - the common approach within the literature revolves around qualitative assessments of the sociality models' configurations most effective in realizing predefined purposes, at times supplemented by empirical data.","In this treatment, we formulate this question as a mathematical optimization problem, in order to quantitatively determine the best possible configurations of sociality forms between dyadic actors which would optimize their mutually-agreed objectives.","For this purpose, we develop an analytical framework for quantifying the (meta)relational models theory, and mathematically demonstrate that combining the four sociality forms within a specific meaningful social interaction inevitably prompts an inherent tension among them, through a single elementary and universal metarelation.","In analogy with financial portfolio management, we subsequently introduce the concept of Social Relations Portfolio (SRP) management, and propose a generalizable procedural methodology capable of quantitatively identifying the efficient SRP for any objective involving meaningful social relations.","As an important illustration, the methodology is applied to the Triple Bottom Line paradigm to derive its efficient SRP, guiding practitioners in precisely measuring, monitoring, reporting and (proactively) steering stakeholder management efforts regarding Corporate Social Responsibility (CSR) and Environmental, Social and Governance (ESG) within and / or across organizations."],"url":"http://arxiv.org/abs/2402.18764v1","category":"physics.soc-ph"}
{"created":"2024-02-28 23:29:27","title":"On Defeating Graph Analysis of Anonymous Transactions","abstract":"In a ring-signature-based anonymous cryptocurrency, signers of a transaction are hidden among a set of potential signers, called a ring, whose size is much smaller than the number of all users. The ring-membership relations specified by the sets of transactions thus induce bipartite transaction graphs, whose distribution is in turn induced by the ring sampler underlying the cryptocurrency.   Since efficient graph analysis could be performed on transaction graphs to potentially deanonymise signers, it is crucial to understand the resistance of (the transaction graphs induced by) a ring sampler against graph analysis. Of particular interest is the class of partitioning ring samplers. Although previous works showed that they provide almost optimal local anonymity, their resistance against global, e.g. graph-based, attacks were unclear.   In this work, we analyse transaction graphs induced by partitioning ring samplers. Specifically, we show (partly analytically and partly empirically) that, somewhat surprisingly, by setting the ring size to be at least logarithmic in the number of users, a graph-analysing adversary is no better than the one that performs random guessing in deanonymisation up to constant factor of 2.","sentences":["In a ring-signature-based anonymous cryptocurrency, signers of a transaction are hidden among a set of potential signers, called a ring, whose size is much smaller than the number of all users.","The ring-membership relations specified by the sets of transactions thus induce bipartite transaction graphs, whose distribution is in turn induced by the ring sampler underlying the cryptocurrency.   ","Since efficient graph analysis could be performed on transaction graphs to potentially deanonymise signers, it is crucial to understand the resistance of (the transaction graphs induced by) a ring sampler against graph analysis.","Of particular interest is the class of partitioning ring samplers.","Although previous works showed that they provide almost optimal local anonymity, their resistance against global, e.g. graph-based, attacks were unclear.   ","In this work, we analyse transaction graphs induced by partitioning ring samplers.","Specifically, we show (partly analytically and partly empirically) that, somewhat surprisingly, by setting the ring size to be at least logarithmic in the number of users, a graph-analysing adversary is no better than the one that performs random guessing in deanonymisation up to constant factor of 2."],"url":"http://arxiv.org/abs/2402.18755v1","category":"cs.CR"}
{"created":"2024-02-28 23:04:07","title":"Fast Bootstrapping Nonparametric Maximum Likelihood for Latent Mixture Models","abstract":"Estimating the mixing density of a latent mixture model is an important task in signal processing. Nonparametric maximum likelihood estimation is one popular approach to this problem. If the latent variable distribution is assumed to be continuous, then bootstrapping can be used to approximate it. However, traditional bootstrapping requires repeated evaluations on resampled data and is not scalable. In this letter, we construct a generative process to rapidly produce nonparametric maximum likelihood bootstrap estimates. Our method requires only a single evaluation of a novel two-stage optimization algorithm. Simulations and real data analyses demonstrate that our procedure accurately estimates the mixing density with little computational cost even when there are a hundred thousand observations.","sentences":["Estimating the mixing density of a latent mixture model is an important task in signal processing.","Nonparametric maximum likelihood estimation is one popular approach to this problem.","If the latent variable distribution is assumed to be continuous, then bootstrapping can be used to approximate it.","However, traditional bootstrapping requires repeated evaluations on resampled data and is not scalable.","In this letter, we construct a generative process to rapidly produce nonparametric maximum likelihood bootstrap estimates.","Our method requires only a single evaluation of a novel two-stage optimization algorithm.","Simulations and real data analyses demonstrate that our procedure accurately estimates the mixing density with little computational cost even when there are a hundred thousand observations."],"url":"http://arxiv.org/abs/2402.18748v1","category":"stat.ME"}
{"created":"2024-02-28 22:38:01","title":"Decomposability of regular graphs to $4$ locally irregular subgraphs","abstract":"A locally irregular graph is a graph whose adjacent vertices have distinct degrees. It was conjectured that every connected graph is edge decomposable to $3$ locally irregular subgraphs, unless it belongs to a certain family of exceptions, including graphs of small maximum degrees, which are not decomposable to any number of such subgraphs. Recently Sedlar and \\v{S}krekovski exhibited a counterexample to the conjecture, which necessitates a decomposition to (at least) $4$ locally irregular subgraphs. We prove that every $d$-regular graph with $d$ large enough, i.e. $d\\geq 54000$, is decomposable to $4$ locally irregular subgraphs. Our proof relies on a mixture of a numerically optimized application of the probabilistic method and certain deterministic results on degree constrained subgraphs due to Addario-Berry, Dalal, McDiarmid, Reed, and Thomason, and to Alon and Wei, introduced in the context of related problems concerning irregular subgraphs.","sentences":["A locally irregular graph is a graph whose adjacent vertices have distinct degrees.","It was conjectured that every connected graph is edge decomposable to $3$ locally irregular subgraphs, unless it belongs to a certain family of exceptions, including graphs of small maximum degrees, which are not decomposable to any number of such subgraphs.","Recently Sedlar and \\v{S}krekovski exhibited a counterexample to the conjecture, which necessitates a decomposition to (at least) $4$ locally irregular subgraphs.","We prove that every $d$-regular graph with $d$ large enough, i.e. $d\\geq 54000$, is decomposable to $4$ locally irregular subgraphs.","Our proof relies on a mixture of a numerically optimized application of the probabilistic method and certain deterministic results on degree constrained subgraphs due to Addario-Berry, Dalal, McDiarmid, Reed, and Thomason, and to Alon and Wei, introduced in the context of related problems concerning irregular subgraphs."],"url":"http://arxiv.org/abs/2402.18739v1","category":"math.CO"}
{"created":"2024-02-28 20:09:56","title":"A light sail astrobiology precursor mission to Enceladus and Europa","abstract":"Icy moons with subsurface oceans of liquid water rank among the most promising astrobiological targets in our Solar System. In this work, we assess the feasibility of deploying laser sail technology in precursor life-detection missions. We investigate such laser sail missions to Enceladus and Europa, as these two moons emit plumes that seem accessible to in situ sampling. Our study suggests that GigaWatt laser technology could accelerate a $100$ kg probe to a speed of $\\sim{30}\\, \\mathrm{km\\, s^{-1}}$, thereupon reaching Europa on timescales of $1$-$4$ years and Enceladus with flight times of $3$-$6$ years. Although the ideal latitudes for the laser array vary, placing the requisite infrastructure close to either the Antarctic or Arctic Circles might represent technically viable options for an Enceladus mission. Crucially, we determine that the minimum encounter velocities with these moons (about ${6}\\,\\mathrm{km\\,s^{-1}}$) may be near-optimal for detecting biomolecular building blocks (e.g., amino acids) in the plumes by means of a mass spectrometer akin to the Surface Dust Analyzer onboard the \\emph{Europa Clipper} mission. In summary, icy moons in the Solar System are potentially well-suited for exploration via the laser sail architecture approach, especially where low encounter speeds and/or multiple missions are desirable.","sentences":["Icy moons with subsurface oceans of liquid water rank among the most promising astrobiological targets in our Solar System.","In this work, we assess the feasibility of deploying laser sail technology in precursor life-detection missions.","We investigate such laser sail missions to Enceladus and Europa, as these two moons emit plumes that seem accessible to in situ sampling.","Our study suggests that GigaWatt laser technology could accelerate a $100$ kg probe to a speed of $\\sim{30}\\, \\mathrm{km\\, s^{-1}}$, thereupon reaching Europa on timescales of $1$-$4$ years and Enceladus with flight times of $3$-$6$ years.","Although the ideal latitudes for the laser array vary, placing the requisite infrastructure close to either the Antarctic or Arctic Circles might represent technically viable options for an Enceladus mission.","Crucially, we determine that the minimum encounter velocities with these moons (about ${6}\\,\\mathrm{km\\,s^{-1}}$) may be near-optimal for detecting biomolecular building blocks (e.g., amino acids) in the plumes by means of a mass spectrometer akin to the Surface Dust Analyzer onboard the \\emph{Europa Clipper} mission.","In summary, icy moons in the Solar System are potentially well-suited for exploration via the laser sail architecture approach, especially where low encounter speeds and/or multiple missions are desirable."],"url":"http://arxiv.org/abs/2402.18691v1","category":"astro-ph.EP"}
{"created":"2024-02-28 19:31:42","title":"Optimization of cosmic filament finders and unbiased recovery of filament phase space profiles using mock filaments","abstract":"Cosmic filaments, the most prominent features of the cosmic web, possibly hold untapped potential for cosmological inference. While it is natural to expect the structure of filaments to show universality similar to that seen in dark matter halos, the lack of agreement between different filament finders on what constitutes a filament has hampered progress on this topic. We initiate a programme to systematically investigate and uncover possible universal features in the phase space structure of cosmic filaments, by generating particle realizations of mock filaments with $\\textit{a priori}$ known properties. Using these, we identify an important source of bias in the extraction of radial density profiles, which occurs when the local curvature $\\kappa$ of the spine exceeds a threshold determined by the filament thickness. This bias exists even for perfectly determined spines, thus affecting $\\textit{all}$ filament finders. We show that this bias can be nearly eliminated by simply discarding the regions with the highest $\\kappa$, with little loss of precision. An additional source of bias is the noise generated by the filament finder when identifying the spine, which depends on both the finder algorithm as well as intrinsic properties of the individual filament. We find that, to mitigate this bias, it is essential not only to smooth the estimated spine, but to $\\textit{optimize}$ this smoothing separately for each filament. We propose a novel optimization based on minimizing the estimated filament thickness, along with Fourier space smoothing. We implement these techniques using two tools, $\\texttt{FilGen}$ which generates mock filaments and $\\texttt{FilAPT}$ which analyses and processes them. We expect these tools to be useful in calibrating the performance of filament finders, thereby enabling searches for filament universality.","sentences":["Cosmic filaments, the most prominent features of the cosmic web, possibly hold untapped potential for cosmological inference.","While it is natural to expect the structure of filaments to show universality similar to that seen in dark matter halos, the lack of agreement between different filament finders on what constitutes a filament has hampered progress on this topic.","We initiate a programme to systematically investigate and uncover possible universal features in the phase space structure of cosmic filaments, by generating particle realizations of mock filaments with $\\textit{a priori}$ known properties.","Using these, we identify an important source of bias in the extraction of radial density profiles, which occurs when the local curvature $\\kappa$ of the spine exceeds a threshold determined by the filament thickness.","This bias exists even for perfectly determined spines, thus affecting $\\textit{all}$ filament finders.","We show that this bias can be nearly eliminated by simply discarding the regions with the highest $\\kappa$, with little loss of precision.","An additional source of bias is the noise generated by the filament finder when identifying the spine, which depends on both the finder algorithm as well as intrinsic properties of the individual filament.","We find that, to mitigate this bias, it is essential not only to smooth the estimated spine, but to $\\textit{optimize}$ this smoothing separately for each filament.","We propose a novel optimization based on minimizing the estimated filament thickness, along with Fourier space smoothing.","We implement these techniques using two tools, $\\texttt{FilGen}$ which generates mock filaments and $\\texttt{FilAPT}$ which analyses and processes them.","We expect these tools to be useful in calibrating the performance of filament finders, thereby enabling searches for filament universality."],"url":"http://arxiv.org/abs/2402.18669v1","category":"astro-ph.CO"}
{"created":"2024-02-28 19:00:09","title":"Design of the 50-meter Atacama Large Aperture Submm Telescope","abstract":"Submillimeter and millimeter wavelengths can reveal a vast range of objects and phenomena that are either too cold, too distant, or too hot and energetic to be measured at visible wavelengths. For decades the astronomical community has highlighted the need for a large, high-throughput submm single dish that can map statistically significant portions of the sky with sufficient surface brightness sensitivity and angular and spectral resolution to probe truly representative source populations. The Atacama Large Aperture Submillimeter Telescope (AtLAST), with its 50-m aperture and $2^\\circ$ maximal field of view, aims to be such a facility. We present here the full design concept for AtLAST, developed through an EU-funded project. Our design approach begins with a long lineage of submm telescopes, relies on calculations and simulations to realize the optics, and uses finite element analysis to optimize the mechanical structure and subsystems. The result is an innovative rocking chair design with six instrument bays, two of which are mounted on Nasmyth platforms. AtLAST will be capable of $3^\\circ\\,\\rm s^{-1}$ scanning and $1^\\circ\\,\\rm s^{-2}$ acceleration, and will feature a surface accuracy of $\\leq 20~\\mu$m half wavefront error allowing observations up to $\\approx 950$~GHz. Further, AtLAST will be a sustainable, visionary facility that will allow upgrades for decades to come. The demanding design requirements for AtLAST, set by transformative science goals, were met by combining novel concepts with lessons learned from past experience. While some aspects require further testing, prototyping, and field demonstrations, we estimate that the design will be construction-ready this decade.","sentences":["Submillimeter and millimeter wavelengths can reveal a vast range of objects and phenomena that are either too cold, too distant, or too hot and energetic to be measured at visible wavelengths.","For decades the astronomical community has highlighted the need for a large, high-throughput submm single dish that can map statistically significant portions of the sky with sufficient surface brightness sensitivity and angular and spectral resolution to probe truly representative source populations.","The Atacama Large Aperture Submillimeter Telescope (AtLAST), with its 50-m aperture and $2^\\circ$ maximal field of view, aims to be such a facility.","We present here the full design concept for AtLAST, developed through an EU-funded project.","Our design approach begins with a long lineage of submm telescopes, relies on calculations and simulations to realize the optics, and uses finite element analysis to optimize the mechanical structure and subsystems.","The result is an innovative rocking chair design with six instrument bays, two of which are mounted on Nasmyth platforms.","AtLAST will be capable of $3^\\circ\\,\\rm s^{-1}$ scanning and $1^\\circ\\,\\rm s^{-2}$ acceleration, and will feature a surface accuracy of $\\leq 20~\\mu$m half wavefront error allowing observations up to $\\approx 950$~GHz.","Further, AtLAST will be a sustainable, visionary facility that will allow upgrades for decades to come.","The demanding design requirements for AtLAST, set by transformative science goals, were met by combining novel concepts with lessons learned from past experience.","While some aspects require further testing, prototyping, and field demonstrations, we estimate that the design will be construction-ready this decade."],"url":"http://arxiv.org/abs/2402.18645v1","category":"astro-ph.IM"}
{"created":"2024-02-28 10:57:30","title":"FORML: A Riemannian Hessian-free Method for Meta-learning with Orthogonality Constraint","abstract":"Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively. However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive. Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection. This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold. Our method significantly reduces the computational load and memory footprint. We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbone network, strengthens the representation reuse of the gradient-based meta-learning methods. Our experimental results across various few-shot learning datasets, demonstrate the superiority of our proposed method compared to the state-of-the-art methods, especially MAML, its Euclidean counterpart.","sentences":["Meta-learning problem is usually formulated as a bi-level optimization in which the task-specific and the meta-parameters are updated in the inner and outer loops of optimization, respectively.","However, performing the optimization in the Riemannian space, where the parameters and meta-parameters are located on Riemannian manifolds is computationally intensive.","Unlike the Euclidean methods, the Riemannian backpropagation needs computing the second-order derivatives that include backward computations through the Riemannian operators such as retraction and orthogonal projection.","This paper introduces a Hessian-free approach that uses a first-order approximation of derivatives on the Stiefel manifold.","Our method significantly reduces the computational load and memory footprint.","We show how using a Stiefel fully-connected layer that enforces orthogonality constraint on the parameters of the last classification layer as the head of the backbone network, strengthens the representation reuse of the gradient-based meta-learning methods.","Our experimental results across various few-shot learning datasets, demonstrate the superiority of our proposed method compared to the state-of-the-art methods, especially MAML, its Euclidean counterpart."],"url":"http://arxiv.org/abs/2402.18605v1","category":"cs.LG"}
