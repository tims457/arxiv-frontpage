{"created":"2024-05-14 17:50:58","title":"A Mimicking Theorem for processes driven by fractional Brownian motion","abstract":"In this paper, we prove a mimicking theorem for stochastic processes with an additive Gaussian noise along with some entropy and transport type estimates. As an application of these results, we prove sharp quantitative propagation of chaos result and derive a formula for the marginal dynamics of collections of locally interacting stochastic differential equations with additive Gaussian noise.","sentences":["In this paper, we prove a mimicking theorem for stochastic processes with an additive Gaussian noise along with some entropy and transport type estimates.","As an application of these results, we prove sharp quantitative propagation of chaos result and derive a formula for the marginal dynamics of collections of locally interacting stochastic differential equations with additive Gaussian noise."],"url":"http://arxiv.org/abs/2405.08803v1","category":"math.PR"}
{"created":"2024-05-14 17:48:08","title":"The Flux Hypothesis for Odd Transport Phenomena","abstract":"Onsager's regression hypothesis makes a fundamental connection between macroscopic transport phenomena and the average relaxation of spontaneous microscopic fluctuations. This relaxation, however, is agnostic to odd transport phenomena, in which fluxes run orthogonal to the gradients driving them. To account for odd transport, we generalize the regression hypothesis, postulating that macroscopic linear constitutive laws are, on average, obeyed by microscopic fluctuations, whether they contribute to relaxation or not. From this \"flux hypothesis,\" Green-Kubo and reciprocal relations follow, elucidating the separate roles of broken time-reversal and parity symmetries underlying various odd transport coefficients. As an application, we derive and verify the Green-Kubo relation for odd collective diffusion in chiral active matter, first in an analytically-tractable model and subsequently through molecular dynamics simulations of concentrated active spinners.","sentences":["Onsager's regression hypothesis makes a fundamental connection between macroscopic transport phenomena and the average relaxation of spontaneous microscopic fluctuations.","This relaxation, however, is agnostic to odd transport phenomena, in which fluxes run orthogonal to the gradients driving them.","To account for odd transport, we generalize the regression hypothesis, postulating that macroscopic linear constitutive laws are, on average, obeyed by microscopic fluctuations, whether they contribute to relaxation or not.","From this \"flux hypothesis,\" Green-Kubo and reciprocal relations follow, elucidating the separate roles of broken time-reversal and parity symmetries underlying various odd transport coefficients.","As an application, we derive and verify the Green-Kubo relation for odd collective diffusion in chiral active matter, first in an analytically-tractable model and subsequently through molecular dynamics simulations of concentrated active spinners."],"url":"http://arxiv.org/abs/2405.08798v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-14 17:45:12","title":"The fundamental martingale with applications to Markov Random Fields","abstract":"We consider collections of SDEs indexed by a graph. Each SDE is driven by an additive Gaussian noise and each drift term interacts with all other SDEs within the graph neighbourhood. We derive the fundamental martingale for a class of Gaussian processes and use this to prove a Girsanov type theorem. Further, we use this to construct a clique factorisation to prove that the law of the interacting SDEs forms a 2-Markov Random Field.","sentences":["We consider collections of SDEs indexed by a graph.","Each SDE is driven by an additive Gaussian noise and each drift term interacts with all other SDEs within the graph neighbourhood.","We derive the fundamental martingale for a class of Gaussian processes and use this to prove a Girsanov type theorem.","Further, we use this to construct a clique factorisation to prove that the law of the interacting SDEs forms a 2-Markov Random Field."],"url":"http://arxiv.org/abs/2405.08795v1","category":"math.PR"}
{"created":"2024-05-14 17:41:07","title":"Towards Enhanced RAC Accessibility: Leveraging Datasets and LLMs","abstract":"This paper explores the potential of large language models (LLMs) to make the Aeronautical Regulations of Colombia (RAC) more accessible. Given the complexity and extensive technicality of the RAC, this study introduces a novel approach to simplifying these regulations for broader understanding. By developing the first-ever RAC database, which contains 24,478 expertly labeled question-and-answer pairs, and fine-tuning LLMs specifically for RAC applications, the paper outlines the methodology for dataset assembly, expert-led annotation, and model training. Utilizing the Gemma1.1 2b model along with advanced techniques like Unsloth for efficient VRAM usage and flash attention mechanisms, the research aims to expedite training processes. This initiative establishes a foundation to enhance the comprehensibility and accessibility of RAC, potentially benefiting novices and reducing dependence on expert consultations for navigating the aviation industry's regulatory landscape.   You can visit the dataset (https://huggingface.co/somosnlp/gemma-1.1-2b-it_ColombiaRAC_FullyCurated_format_chatML_V1) and the model (https://huggingface.co/datasets/somosnlp/ColombiaRAC_FullyCurated) here.","sentences":["This paper explores the potential of large language models (LLMs) to make the Aeronautical Regulations of Colombia (RAC) more accessible.","Given the complexity and extensive technicality of the RAC, this study introduces a novel approach to simplifying these regulations for broader understanding.","By developing the first-ever RAC database, which contains 24,478 expertly labeled question-and-answer pairs, and fine-tuning LLMs specifically for RAC applications, the paper outlines the methodology for dataset assembly, expert-led annotation, and model training.","Utilizing the Gemma1.1 2b model along with advanced techniques like Unsloth for efficient VRAM usage and flash attention mechanisms, the research aims to expedite training processes.","This initiative establishes a foundation to enhance the comprehensibility and accessibility of RAC, potentially benefiting novices and reducing dependence on expert consultations for navigating the aviation industry's regulatory landscape.   ","You can visit the dataset (https://huggingface.co/somosnlp/gemma-1.1-2b-it_ColombiaRAC_FullyCurated_format_chatML_V1) and the model (https://huggingface.co/datasets/somosnlp/ColombiaRAC_FullyCurated) here."],"url":"http://arxiv.org/abs/2405.08792v1","category":"cs.LG"}
{"created":"2024-05-14 17:38:17","title":"Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","abstract":"This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.","sentences":["This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling.","Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically.","We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters.","We also provide an ablation study of KAN-specific parameters impact on performance.","The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics."],"url":"http://arxiv.org/abs/2405.08790v1","category":"eess.SP"}
{"created":"2024-05-14 17:15:28","title":"Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling","abstract":"Deep learning has enabled breakthroughs in automated diagnosis from medical imaging, with many successful applications in ophthalmology. However, standard medical image classification approaches only assess disease presence at the time of acquisition, neglecting the common clinical setting of longitudinal imaging. For slow, progressive eye diseases like age-related macular degeneration (AMD) and primary open-angle glaucoma (POAG), patients undergo repeated imaging over time to track disease progression and forecasting the future risk of developing disease is critical to properly plan treatment. Our proposed Longitudinal Transformer for Survival Analysis (LTSA) enables dynamic disease prognosis from longitudinal medical imaging, modeling the time to disease from sequences of fundus photography images captured over long, irregular time periods. Using longitudinal imaging data from the Age-Related Eye Disease Study (AREDS) and Ocular Hypertension Treatment Study (OHTS), LTSA significantly outperformed a single-image baseline in 19/20 head-to-head comparisons on late AMD prognosis and 18/20 comparisons on POAG prognosis. A temporal attention analysis also suggested that, while the most recent image is typically the most influential, prior imaging still provides additional prognostic value.","sentences":["Deep learning has enabled breakthroughs in automated diagnosis from medical imaging, with many successful applications in ophthalmology.","However, standard medical image classification approaches only assess disease presence at the time of acquisition, neglecting the common clinical setting of longitudinal imaging.","For slow, progressive eye diseases like age-related macular degeneration (AMD) and primary open-angle glaucoma (POAG), patients undergo repeated imaging over time to track disease progression and forecasting the future risk of developing disease is critical to properly plan treatment.","Our proposed Longitudinal Transformer for Survival Analysis (LTSA) enables dynamic disease prognosis from longitudinal medical imaging, modeling the time to disease from sequences of fundus photography images captured over long, irregular time periods.","Using longitudinal imaging data from the Age-Related Eye Disease Study (AREDS) and Ocular Hypertension Treatment Study (OHTS), LTSA significantly outperformed a single-image baseline in 19/20 head-to-head comparisons on late AMD prognosis and 18/20 comparisons on POAG prognosis.","A temporal attention analysis also suggested that, while the most recent image is typically the most influential, prior imaging still provides additional prognostic value."],"url":"http://arxiv.org/abs/2405.08780v1","category":"cs.CV"}
{"created":"2024-05-14 17:00:43","title":"EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training","abstract":"The superior performance of modern visual backbones usually comes with a costly training procedure. We contribute to this issue by generalizing the idea of curriculum learning beyond its original formulation, i.e., training models using easier-to-harder data. Specifically, we reformulate the training curriculum as a soft-selection function, which uncovers progressively more difficult patterns within each example during training, instead of performing easier-to-harder sample selection. Our work is inspired by an intriguing observation on the learning dynamics of visual backbones: during the earlier stages of training, the model predominantly learns to recognize some 'easier-to-learn' discriminative patterns in the data. These patterns, when observed through frequency and spatial domains, incorporate lower-frequency components, and the natural image contents without distortion or data augmentation. Motivated by these findings, we propose a curriculum where the model always leverages all the training data at every learning stage, yet the exposure to the 'easier-to-learn' patterns of each example is initiated first, with harder patterns gradually introduced as training progresses. To implement this idea in a computationally efficient way, we introduce a cropping operation in the Fourier spectrum of the inputs, enabling the model to learn from only the lower-frequency components. Then we show that exposing the contents of natural images can be readily achieved by modulating the intensity of data augmentation. Finally, we integrate these aspects and design curriculum schedules with tailored search algorithms. The resulting method, EfficientTrain++, is simple, general, yet surprisingly effective. It reduces the training time of a wide variety of popular models by 1.5-3.0x on ImageNet-1K/22K without sacrificing accuracy. It also demonstrates efficacy in self-supervised learning (e.g., MAE).","sentences":["The superior performance of modern visual backbones usually comes with a costly training procedure.","We contribute to this issue by generalizing the idea of curriculum learning beyond its original formulation, i.e., training models using easier-to-harder data.","Specifically, we reformulate the training curriculum as a soft-selection function, which uncovers progressively more difficult patterns within each example during training, instead of performing easier-to-harder sample selection.","Our work is inspired by an intriguing observation on the learning dynamics of visual backbones: during the earlier stages of training, the model predominantly learns to recognize some 'easier-to-learn' discriminative patterns in the data.","These patterns, when observed through frequency and spatial domains, incorporate lower-frequency components, and the natural image contents without distortion or data augmentation.","Motivated by these findings, we propose a curriculum where the model always leverages all the training data at every learning stage, yet the exposure to the 'easier-to-learn' patterns of each example is initiated first, with harder patterns gradually introduced as training progresses.","To implement this idea in a computationally efficient way, we introduce a cropping operation in the Fourier spectrum of the inputs, enabling the model to learn from only the lower-frequency components.","Then we show that exposing the contents of natural images can be readily achieved by modulating the intensity of data augmentation.","Finally, we integrate these aspects and design curriculum schedules with tailored search algorithms.","The resulting method, EfficientTrain++, is simple, general, yet surprisingly effective.","It reduces the training time of a wide variety of popular models by 1.5-3.0x on ImageNet-1K/22K without sacrificing accuracy.","It also demonstrates efficacy in self-supervised learning (e.g., MAE)."],"url":"http://arxiv.org/abs/2405.08768v1","category":"cs.CV"}
{"created":"2024-05-14 16:48:56","title":"Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Intent Resolution in LLMs","abstract":"Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors -- human or AI -- to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate pragmatically relevant responses to non-literal language, achieving only 50-55% accuracy on average. While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses. Using chain-of-thought to make models spell out intentions yields much smaller gains (60% for Mistral-Instruct). These findings suggest that LLMs are not yet effective pragmatic interlocutors, highlighting the need for better approaches for modeling intentions and utilizing them for pragmatic generation.","sentences":["Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors -- human or AI -- to understand beyond the literal meaning of words.","While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances.","Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation.","Our findings show that LLMs struggle to generate pragmatically relevant responses to non-literal language, achieving only 50-55% accuracy on average.","While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses.","Using chain-of-thought to make models spell out intentions yields much smaller gains (60% for Mistral-Instruct).","These findings suggest that LLMs are not yet effective pragmatic interlocutors, highlighting the need for better approaches for modeling intentions and utilizing them for pragmatic generation."],"url":"http://arxiv.org/abs/2405.08760v1","category":"cs.CL"}
{"created":"2024-05-14 16:48:46","title":"Optimal Sequential Procedure for Early Detection of Multiple Side Effects","abstract":"In this paper, we propose an optimal sequential procedure for the early detection of potential side effects resulting from the administration of some treatment (e.g. a vaccine, say). The results presented here extend previous results obtained in Wang and Boukai (2024) who study the single side effect case to the case of two (or more) side effects. While the sequential procedure we employ, simultaneously monitors several of the treatment's side effects, the $(\\alpha, \\beta)$-optimal test we propose does not require any information about the inter-correlation between these potential side effects. However, in all of the subsequent analyses, including the derivations of the exact expressions of the Average Sample Number (ASN), the Power function, and the properties of the post-test (or post-detection) estimators, we accounted specifically, for the correlation between the potential side effects. In the real-life application (such as post-marketing surveillance), the number of available observations is large enough to justify asymptotic analyses of the sequential procedure (testing and post-detection estimation) properties. Accordingly, we also derive the consistency and asymptotic normality of our post-test estimators; results which enable us to also provide (asymptotic, post-detection) confidence intervals for the probabilities of various side-effects. Moreover, to compare two specific side effects, their relative risk plays an important role. We derive the distribution of the estimated relative risk in the asymptotic framework to provide appropriate inference. To illustrate the theoretical results presented, we provide two detailed examples based on the data of side effects on COVID-19 vaccine collected in Nigeria (see Nigeria (see Ilori et al. (2022)).","sentences":["In this paper, we propose an optimal sequential procedure for the early detection of potential side effects resulting from the administration of some treatment (e.g. a vaccine, say).","The results presented here extend previous results obtained in Wang and Boukai (2024) who study the single side effect case to the case of two (or more) side effects.","While the sequential procedure we employ, simultaneously monitors several of the treatment's side effects, the $(\\alpha, \\beta)$-optimal test we propose does not require any information about the inter-correlation between these potential side effects.","However, in all of the subsequent analyses, including the derivations of the exact expressions of the Average Sample Number (ASN), the Power function, and the properties of the post-test (or post-detection) estimators, we accounted specifically, for the correlation between the potential side effects.","In the real-life application (such as post-marketing surveillance), the number of available observations is large enough to justify asymptotic analyses of the sequential procedure (testing and post-detection estimation) properties.","Accordingly, we also derive the consistency and asymptotic normality of our post-test estimators; results which enable us to also provide (asymptotic, post-detection) confidence intervals for the probabilities of various side-effects.","Moreover, to compare two specific side effects, their relative risk plays an important role.","We derive the distribution of the estimated relative risk in the asymptotic framework to provide appropriate inference.","To illustrate the theoretical results presented, we provide two detailed examples based on the data of side effects on COVID-19 vaccine collected in Nigeria (see Nigeria (see Ilori et al. (2022))."],"url":"http://arxiv.org/abs/2405.08759v1","category":"stat.ME"}
{"created":"2024-05-14 16:40:37","title":"Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach","abstract":"With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of large language models (LLMs), represents a promising paradigm for enhancing cybersecurity on low-powered edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network.","sentences":["With the proliferation of edge devices, there is a significant increase in attack surface on these devices.","The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of large language models (LLMs), represents a promising paradigm for enhancing cybersecurity on low-powered edge devices.","This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time.","Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally.","LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives.","Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies.","The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge.","Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network."],"url":"http://arxiv.org/abs/2405.08755v1","category":"cs.CR"}
{"created":"2024-05-14 16:33:29","title":"Longitudinal Structure of Quark-Gluon Plasma Unveiled Through Nuclear Deformations","abstract":"The study of quark-gluon plasma (QGP) is hindered by our limited understanding of its initial conditions, particularly its longitudinal structure. We propose a novel approach that entails analyzing collisions involving nuclei of similar masses but different deformations. This strategy allows us to vary the initial conditions and collective expansion of the QGP, while minimizing the influence of non-flow correlations. Using a dynamical transport model, we have for the first time extracted the complete longitudinal structure of elliptic flow ($v_2$). Our findings reveal that although deformation significantly enhances the overall magnitude of $v_2$, it does not alter its longitudinal profile. This approach not only enables the separation of the rapidity dependence of flow from its rapidity decorrelations but also prompts further investigation into other nuclear structural features, such as nuclear skin thickness, to advance our understanding of the QGP's initial conditions.","sentences":["The study of quark-gluon plasma (QGP) is hindered by our limited understanding of its initial conditions, particularly its longitudinal structure.","We propose a novel approach that entails analyzing collisions involving nuclei of similar masses but different deformations.","This strategy allows us to vary the initial conditions and collective expansion of the QGP, while minimizing the influence of non-flow correlations.","Using a dynamical transport model, we have for the first time extracted the complete longitudinal structure of elliptic flow ($v_2$).","Our findings reveal that although deformation significantly enhances the overall magnitude of $v_2$, it does not alter its longitudinal profile.","This approach not only enables the separation of the rapidity dependence of flow from its rapidity decorrelations but also prompts further investigation into other nuclear structural features, such as nuclear skin thickness, to advance our understanding of the QGP's initial conditions."],"url":"http://arxiv.org/abs/2405.08749v1","category":"nucl-th"}
{"created":"2024-05-14 16:30:32","title":"A tunable binaural audio telepresence system capable of balancing immersive and enhanced modes","abstract":"Binaural Audio Telepresence (BAT) aims to encode the acoustic scene at the far end into binaural signals for the user at the near end. BAT encompasses an immense range of applications that can vary between two extreme modes of Immersive BAT (I-BAT) and Enhanced BAT (E-BAT). With I-BAT, our goal is to preserve the full ambience as if we were at the far end, while with E-BAT, our goal is to enhance the far-end conversation with significantly improved speech quality and intelligibility. To this end, this paper presents a tunable BAT system to vary between these two AT modes with a desired application-specific balance. Microphone signals are converted into binaural signals with prescribed ambience factor. A novel Spatial COherence REpresentation (SCORE) is proposed as an input feature for model training so that the network remains robust to different array setups. Experimental results demonstrated the superior performance of the proposed BAT, even when the array configurations were not included in the training phase.","sentences":["Binaural Audio Telepresence (BAT) aims to encode the acoustic scene at the far end into binaural signals for the user at the near end.","BAT encompasses an immense range of applications that can vary between two extreme modes of Immersive BAT (I-BAT) and Enhanced BAT (E-BAT).","With I-BAT, our goal is to preserve the full ambience as if we were at the far end, while with E-BAT, our goal is to enhance the far-end conversation with significantly improved speech quality and intelligibility.","To this end, this paper presents a tunable BAT system to vary between these two AT modes with a desired application-specific balance.","Microphone signals are converted into binaural signals with prescribed ambience factor.","A novel Spatial COherence REpresentation (SCORE) is proposed as an input feature for model training so that the network remains robust to different array setups.","Experimental results demonstrated the superior performance of the proposed BAT, even when the array configurations were not included in the training phase."],"url":"http://arxiv.org/abs/2405.08742v1","category":"eess.AS"}
{"created":"2024-05-14 16:15:31","title":"Targeted Augmentation for Low-Resource Event Extraction","abstract":"Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples. Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance). This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence. Extensive experimental results demonstrate the effectiveness of the proposed paradigm. Furthermore, identified limitations are discussed, shedding light on areas for future improvement.","sentences":["Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples.","Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance).","This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence.","Extensive experimental results demonstrate the effectiveness of the proposed paradigm.","Furthermore, identified limitations are discussed, shedding light on areas for future improvement."],"url":"http://arxiv.org/abs/2405.08729v1","category":"cs.CL"}
{"created":"2024-05-14 16:12:27","title":"I-CTRL: Imitation to Control Humanoid Robots Through Constrained Reinforcement Learning","abstract":"This paper addresses the critical need for refining robot motions that, despite achieving a high visual similarity through human-to-humanoid retargeting methods, fall short of practical execution in the physical realm. Existing techniques in the graphics community often prioritize visual fidelity over physics-based feasibility, posing a significant challenge for deploying bipedal systems in practical applications. Our research introduces a constrained reinforcement learning algorithm to produce physics-based high-quality motion imitation onto legged humanoid robots that enhance motion resemblance while successfully following the reference human trajectory. We name our framework: I-CTRL. By reformulating the motion imitation problem as a constrained refinement over non-physics-based retargeted motions, our framework excels in motion imitation with simple and unique rewards that generalize across four robots. Moreover, our framework can follow large-scale motion datasets with a unique RL agent. The proposed approach signifies a crucial step forward in advancing the control of bipedal robots, emphasizing the importance of aligning visual and physical realism for successful motion imitation.","sentences":["This paper addresses the critical need for refining robot motions that, despite achieving a high visual similarity through human-to-humanoid retargeting methods, fall short of practical execution in the physical realm.","Existing techniques in the graphics community often prioritize visual fidelity over physics-based feasibility, posing a significant challenge for deploying bipedal systems in practical applications.","Our research introduces a constrained reinforcement learning algorithm to produce physics-based high-quality motion imitation onto legged humanoid robots that enhance motion resemblance while successfully following the reference human trajectory.","We name our framework: I-CTRL.","By reformulating the motion imitation problem as a constrained refinement over non-physics-based retargeted motions, our framework excels in motion imitation with simple and unique rewards that generalize across four robots.","Moreover, our framework can follow large-scale motion datasets with a unique RL agent.","The proposed approach signifies a crucial step forward in advancing the control of bipedal robots, emphasizing the importance of aligning visual and physical realism for successful motion imitation."],"url":"http://arxiv.org/abs/2405.08726v1","category":"cs.RO"}
{"created":"2024-05-14 16:09:52","title":"Revisiting Reactor Anti-Neutrino 5 MeV Bump with $^{13}$C Neutral-Current Interaction","abstract":"For the first time, we systematically investigate the potential of neutrino-nucleus neutral current interactions with $^{13}$C to identify the origin of the 5 MeV bump observed in reactor anti-neutrino spectra in the inverse beta decay process. The distinctive signal is obtained from the de-excitation of $^{13}$C$^*$ into the ground state emitting a 3.685 MeV photon in various liquid scintillator detectors. Such an interaction predominantly occurs for the reactor anti-neutrinos within the energy range coinciding with the 5 MeV bump. For a detector that has a capability of 95\\% level photon and electron separation and small thorium contamination below $5 \\times 10^{-17}$ gr/gr located in a site with an overburden of about a few hundred m.w.e, such as the location of near detectors of RENO and Daya Bay will have a great sensitivity to resolve the 5 MeV bump. In addition, we propose a novel approach to track the time evolution of reactor isotopes by analyzing our $^{13}$C signal shedding light on the contributions from $^{235}$U or $^{239}$Pu to the observed bump. This provides an extra powerful tool in both discriminating the flux models and testing any new physics possibilities for the 5 MeV bump at 3$\\sigma$ to 5$\\sigma$ level with much less systematic uncertainties and assuming 10 kt.year of data collection. Our detector requirements are realistic, aligning well with recent studies conducted for existing or forthcoming experiments.","sentences":["For the first time, we systematically investigate the potential of neutrino-nucleus neutral current interactions with $^{13}$C to identify the origin of the 5 MeV bump observed in reactor anti-neutrino spectra in the inverse beta decay process.","The distinctive signal is obtained from the de-excitation of $^{13}$C$^*$ into the ground state emitting a 3.685 MeV photon in various liquid scintillator detectors.","Such an interaction predominantly occurs for the reactor anti-neutrinos within the energy range coinciding with the 5 MeV bump.","For a detector that has a capability of 95\\% level photon and electron separation and small thorium contamination below $5 \\times 10^{-17}$ gr/gr located in a site with an overburden of about a few hundred m.w.e, such as the location of near detectors of RENO and Daya Bay will have a great sensitivity to resolve the 5 MeV bump.","In addition, we propose a novel approach to track the time evolution of reactor isotopes by analyzing our $^{13}$C signal shedding light on the contributions from $^{235}$U or $^{239}$Pu to the observed bump.","This provides an extra powerful tool in both discriminating the flux models and testing any new physics possibilities for the 5 MeV bump at 3$\\sigma$ to 5$\\sigma$ level with much less systematic uncertainties and assuming 10 kt.year of data collection.","Our detector requirements are realistic, aligning well with recent studies conducted for existing or forthcoming experiments."],"url":"http://arxiv.org/abs/2405.08724v1","category":"hep-ph"}
{"created":"2024-05-14 15:04:46","title":"Achieving Fairness Through Channel Pruning for Dermatological Disease Diagnosis","abstract":"Numerous studies have revealed that deep learning-based medical image classification models may exhibit bias towards specific demographic attributes, such as race, gender, and age. Existing bias mitigation methods often achieve high level of fairness at the cost of significant accuracy degradation. In response to this challenge, we propose an innovative and adaptable Soft Nearest Neighbor Loss-based channel pruning framework, which achieves fairness through channel pruning. Traditionally, channel pruning is utilized to accelerate neural network inference. However, our work demonstrates that pruning can also be a potent tool for achieving fairness. Our key insight is that different channels in a layer contribute differently to the accuracy of different groups. By selectively pruning critical channels that lead to the accuracy difference between the privileged and unprivileged groups, we can effectively improve fairness without sacrificing accuracy significantly. Experiments conducted on two skin lesion diagnosis datasets across multiple sensitive attributes validate the effectiveness of our method in achieving state-of-the-art trade-off between accuracy and fairness. Our code is available at https://github.com/Kqp1227/Sensitive-Channel-Pruning.","sentences":["Numerous studies have revealed that deep learning-based medical image classification models may exhibit bias towards specific demographic attributes, such as race, gender, and age.","Existing bias mitigation methods often achieve high level of fairness at the cost of significant accuracy degradation.","In response to this challenge, we propose an innovative and adaptable Soft Nearest Neighbor Loss-based channel pruning framework, which achieves fairness through channel pruning.","Traditionally, channel pruning is utilized to accelerate neural network inference.","However, our work demonstrates that pruning can also be a potent tool for achieving fairness.","Our key insight is that different channels in a layer contribute differently to the accuracy of different groups.","By selectively pruning critical channels that lead to the accuracy difference between the privileged and unprivileged groups, we can effectively improve fairness without sacrificing accuracy significantly.","Experiments conducted on two skin lesion diagnosis datasets across multiple sensitive attributes validate the effectiveness of our method in achieving state-of-the-art trade-off between accuracy and fairness.","Our code is available at https://github.com/Kqp1227/Sensitive-Channel-Pruning."],"url":"http://arxiv.org/abs/2405.08681v1","category":"cs.CV"}
{"created":"2024-05-14 15:00:09","title":"Investigating Design Choices in Joint-Embedding Predictive Architectures for General Audio Representation Learning","abstract":"This paper addresses the problem of self-supervised general-purpose audio representation learning. We explore the use of Joint-Embedding Predictive Architectures (JEPA) for this task, which consists of splitting an input mel-spectrogram into two parts (context and target), computing neural representations for each, and training the neural network to predict the target representations from the context representations. We investigate several design choices within this framework and study their influence through extensive experiments by evaluating our models on various audio classification benchmarks, including environmental sounds, speech and music downstream tasks. We focus notably on which part of the input data is used as context or target and show experimentally that it significantly impacts the model's quality. In particular, we notice that some effective design choices in the image domain lead to poor performance on audio, thus highlighting major differences between these two modalities.","sentences":["This paper addresses the problem of self-supervised general-purpose audio representation learning.","We explore the use of Joint-Embedding Predictive Architectures (JEPA) for this task, which consists of splitting an input mel-spectrogram into two parts (context and target), computing neural representations for each, and training the neural network to predict the target representations from the context representations.","We investigate several design choices within this framework and study their influence through extensive experiments by evaluating our models on various audio classification benchmarks, including environmental sounds, speech and music downstream tasks.","We focus notably on which part of the input data is used as context or target and show experimentally that it significantly impacts the model's quality.","In particular, we notice that some effective design choices in the image domain lead to poor performance on audio, thus highlighting major differences between these two modalities."],"url":"http://arxiv.org/abs/2405.08679v1","category":"cs.SD"}
{"created":"2024-05-14 14:55:57","title":"Expensive Multi-Objective Bayesian Optimization Based on Diffusion Models","abstract":"Multi-objective Bayesian optimization (MOBO) has shown promising performance on various expensive multi-objective optimization problems (EMOPs). However, effectively modeling complex distributions of the Pareto optimal solutions is difficult with limited function evaluations. Existing Pareto set learning algorithms may exhibit considerable instability in such expensive scenarios, leading to significant deviations between the obtained solution set and the Pareto set (PS). In this paper, we propose a novel Composite Diffusion Model based Pareto Set Learning algorithm, namely CDM-PSL, for expensive MOBO. CDM-PSL includes both unconditional and conditional diffusion model for generating high-quality samples. Besides, we introduce an information entropy based weighting method to balance different objectives of EMOPs. This method is integrated with the guiding strategy, ensuring that all the objectives are appropriately balanced and given due consideration during the optimization process; Extensive experimental results on both synthetic benchmarks and real-world problems demonstrates that our proposed algorithm attains superior performance compared with various state-of-the-art MOBO algorithms.","sentences":["Multi-objective Bayesian optimization (MOBO) has shown promising performance on various expensive multi-objective optimization problems (EMOPs).","However, effectively modeling complex distributions of the Pareto optimal solutions is difficult with limited function evaluations.","Existing Pareto set learning algorithms may exhibit considerable instability in such expensive scenarios, leading to significant deviations between the obtained solution set and the Pareto set (PS).","In this paper, we propose a novel Composite Diffusion Model based Pareto Set Learning algorithm, namely CDM-PSL, for expensive MOBO.","CDM-PSL includes both unconditional and conditional diffusion model for generating high-quality samples.","Besides, we introduce an information entropy based weighting method to balance different objectives of EMOPs.","This method is integrated with the guiding strategy, ensuring that all the objectives are appropriately balanced and given due consideration during the optimization process; Extensive experimental results on both synthetic benchmarks and real-world problems demonstrates that our proposed algorithm attains superior performance compared with various state-of-the-art MOBO algorithms."],"url":"http://arxiv.org/abs/2405.08674v1","category":"cs.LG"}
{"created":"2024-05-14 14:51:12","title":"Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research","abstract":"Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs. However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia. To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources. By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings. Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations. Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach. Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm. Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry.","sentences":["Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs.","However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia.","To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework.","GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources.","By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings.","Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations.","Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach.","Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm.","Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry."],"url":"http://arxiv.org/abs/2405.08668v1","category":"cs.CV"}
{"created":"2024-05-14 14:41:58","title":"Gradient Estimation and Variance Reduction in Stochastic and Deterministic Models","abstract":"It seems that in the current age, computers, computation, and data have an increasingly important role to play in scientific research and discovery. This is reflected in part by the rise of machine learning and artificial intelligence, which have become great areas of interest not just for computer science but also for many other fields of study. More generally, there have been trends moving towards the use of bigger, more complex and higher capacity models. It also seems that stochastic models, and stochastic variants of existing deterministic models, have become important research directions in various fields. For all of these types of models, gradient-based optimization remains as the dominant paradigm for model fitting, control, and more. This dissertation considers unconstrained, nonlinear optimization problems, with a focus on the gradient itself, that key quantity which enables the solution of such problems.   In chapter 1, we introduce the notion of reverse differentiation, a term which describes the body of techniques which enables the efficient computation of gradients. We cover relevant techniques both in the deterministic and stochastic cases. We present a new framework for calculating the gradient of problems which involve both deterministic and stochastic elements. In chapter 2, we analyze the properties of the gradient estimator, with a focus on those properties which are typically assumed in convergence proofs of optimization algorithms. Chapter 3 gives various examples of applying our new gradient estimator. We further explore the idea of working with piecewise continuous models, that is, models with distinct branches and if statements which define what specific branch to use.","sentences":["It seems that in the current age, computers, computation, and data have an increasingly important role to play in scientific research and discovery.","This is reflected in part by the rise of machine learning and artificial intelligence, which have become great areas of interest not just for computer science but also for many other fields of study.","More generally, there have been trends moving towards the use of bigger, more complex and higher capacity models.","It also seems that stochastic models, and stochastic variants of existing deterministic models, have become important research directions in various fields.","For all of these types of models, gradient-based optimization remains as the dominant paradigm for model fitting, control, and more.","This dissertation considers unconstrained, nonlinear optimization problems, with a focus on the gradient itself, that key quantity which enables the solution of such problems.   ","In chapter 1, we introduce the notion of reverse differentiation, a term which describes the body of techniques which enables the efficient computation of gradients.","We cover relevant techniques both in the deterministic and stochastic cases.","We present a new framework for calculating the gradient of problems which involve both deterministic and stochastic elements.","In chapter 2, we analyze the properties of the gradient estimator, with a focus on those properties which are typically assumed in convergence proofs of optimization algorithms.","Chapter 3 gives various examples of applying our new gradient estimator.","We further explore the idea of working with piecewise continuous models, that is, models with distinct branches and if statements which define what specific branch to use."],"url":"http://arxiv.org/abs/2405.08661v1","category":"cs.LG"}
{"created":"2024-05-14 14:35:35","title":"Beyond the Black Box: Do More Complex Models Provide Superior XAI Explanations?","abstract":"The increasing complexity of Artificial Intelligence models poses challenges to interpretability, particularly in the healthcare sector. This study investigates the impact of deep learning model complexity and Explainable AI (XAI) efficacy, utilizing four ResNet architectures (ResNet-18, 34, 50, 101). Through methodical experimentation on 4,369 lung X-ray images of COVID-19-infected and healthy patients, the research evaluates models' classification performance and the relevance of corresponding XAI explanations with respect to the ground-truth disease masks. Results indicate that the increase in model complexity is associated with a decrease in classification accuracy and AUC-ROC scores (ResNet-18: 98.4%, 0.997; ResNet-101: 95.9%, 0.988). Notably, in eleven out of twelve statistical tests performed, no statistically significant differences occurred between XAI quantitative metrics - Relevance Rank Accuracy and the proposed Positive Attribution Ratio - across trained models. These results suggest that increased model complexity does not consistently lead to higher performance or relevance of explanations for models' decision-making processes.","sentences":["The increasing complexity of Artificial Intelligence models poses challenges to interpretability, particularly in the healthcare sector.","This study investigates the impact of deep learning model complexity and Explainable AI (XAI) efficacy, utilizing four ResNet architectures (ResNet-18, 34, 50, 101).","Through methodical experimentation on 4,369 lung X-ray images of COVID-19-infected and healthy patients, the research evaluates models' classification performance and the relevance of corresponding XAI explanations with respect to the ground-truth disease masks.","Results indicate that the increase in model complexity is associated with a decrease in classification accuracy and AUC-ROC scores (ResNet-18: 98.4%, 0.997; ResNet-101: 95.9%, 0.988).","Notably, in eleven out of twelve statistical tests performed, no statistically significant differences occurred between XAI quantitative metrics - Relevance Rank Accuracy and the proposed Positive Attribution Ratio - across trained models.","These results suggest that increased model complexity does not consistently lead to higher performance or relevance of explanations for models' decision-making processes."],"url":"http://arxiv.org/abs/2405.08658v1","category":"eess.IV"}
{"created":"2024-05-14 14:34:24","title":"A Distributed Approach to Autonomous Intersection Management via Multi-Agent Reinforcement Learning","abstract":"Autonomous intersection management (AIM) poses significant challenges due to the intricate nature of real-world traffic scenarios and the need for a highly expensive centralised server in charge of simultaneously controlling all the vehicles. This study addresses such issues by proposing a novel distributed approach to AIM utilizing multi-agent reinforcement learning (MARL). We show that by leveraging the 3D surround view technology for advanced assistance systems, autonomous vehicles can accurately navigate intersection scenarios without needing any centralised controller. The contributions of this paper thus include a MARL-based algorithm for the autonomous management of a 4-way intersection and also the introduction of a new strategy called prioritised scenario replay for improved training efficacy. We validate our approach as an innovative alternative to conventional centralised AIM techniques, ensuring the full reproducibility of our results. Specifically, experiments conducted in virtual environments using the SMARTS platform highlight its superiority over benchmarks across various metrics.","sentences":["Autonomous intersection management (AIM) poses significant challenges due to the intricate nature of real-world traffic scenarios and the need for a highly expensive centralised server in charge of simultaneously controlling all the vehicles.","This study addresses such issues by proposing a novel distributed approach to AIM utilizing multi-agent reinforcement learning (MARL).","We show that by leveraging the 3D surround view technology for advanced assistance systems, autonomous vehicles can accurately navigate intersection scenarios without needing any centralised controller.","The contributions of this paper thus include a MARL-based algorithm for the autonomous management of a 4-way intersection and also the introduction of a new strategy called prioritised scenario replay for improved training efficacy.","We validate our approach as an innovative alternative to conventional centralised AIM techniques, ensuring the full reproducibility of our results.","Specifically, experiments conducted in virtual environments using the SMARTS platform highlight its superiority over benchmarks across various metrics."],"url":"http://arxiv.org/abs/2405.08655v1","category":"cs.RO"}
{"created":"2024-05-14 14:32:58","title":"Can we Defend Against the Unknown? An Empirical Study About Threshold Selection for Neural Network Monitoring","abstract":"With the increasing use of neural networks in critical systems, runtime monitoring becomes essential to reject unsafe predictions during inference. Various techniques have emerged to establish rejection scores that maximize the separability between the distributions of safe and unsafe predictions. The efficacy of these approaches is mostly evaluated using threshold-agnostic metrics, such as the area under the receiver operating characteristic curve. However, in real-world applications, an effective monitor also requires identifying a good threshold to transform these scores into meaningful binary decisions. Despite the pivotal importance of threshold optimization, this problem has received little attention. A few studies touch upon this question, but they typically assume that the runtime data distribution mirrors the training distribution, which is a strong assumption as monitors are supposed to safeguard a system against potentially unforeseen threats. In this work, we present rigorous experiments on various image datasets to investigate: 1. The effectiveness of monitors in handling unforeseen threats, which are not available during threshold adjustments. 2. Whether integrating generic threats into the threshold optimization scheme can enhance the robustness of monitors.","sentences":["With the increasing use of neural networks in critical systems, runtime monitoring becomes essential to reject unsafe predictions during inference.","Various techniques have emerged to establish rejection scores that maximize the separability between the distributions of safe and unsafe predictions.","The efficacy of these approaches is mostly evaluated using threshold-agnostic metrics, such as the area under the receiver operating characteristic curve.","However, in real-world applications, an effective monitor also requires identifying a good threshold to transform these scores into meaningful binary decisions.","Despite the pivotal importance of threshold optimization, this problem has received little attention.","A few studies touch upon this question, but they typically assume that the runtime data distribution mirrors the training distribution, which is a strong assumption as monitors are supposed to safeguard a system against potentially unforeseen threats.","In this work, we present rigorous experiments on various image datasets to investigate: 1.","The effectiveness of monitors in handling unforeseen threats, which are not available during threshold adjustments.","2.","Whether integrating generic threats into the threshold optimization scheme can enhance the robustness of monitors."],"url":"http://arxiv.org/abs/2405.08654v1","category":"cs.LG"}
{"created":"2024-05-14 14:25:11","title":"Modeling Realistic Heating Profiles of Transition Region Hot Loops on the Sun: Evidence for Impulsive Heating and Non-equilibrium Ionization","abstract":"The study examines the heating profile of hot solar transition region loops, particularly focusing on transient brightenings observed in IRIS 1400{\\AA} slit-jaw images. The findings challenge the adequacy of simplistic, singular heating mechanisms, revealing that the heating is temporally impulsive and requires a spatially complex profile with multiple heating scales. A forward modeling code is utilized to generate synthetic IRIS emission spectra of these loops based on HYDRAD output, confirming that emitting ions are out of equilibrium. The modeling further indicates that density-dependent dielectronic recombination rates must be included to reproduce the observed line ratios. Collectively, this evidence substantiates that the loops are subject to impulsive heating and that the components of the transiently brightened plasma are driven far from thermal equilibrium. Heating events such as these are ubiquitous in the transition region and the analysis described above provides a robust observational diagnostic tool for characterizing the plasma.","sentences":["The study examines the heating profile of hot solar transition region loops, particularly focusing on transient brightenings observed in IRIS 1400{\\AA} slit-jaw images.","The findings challenge the adequacy of simplistic, singular heating mechanisms, revealing that the heating is temporally impulsive and requires a spatially complex profile with multiple heating scales.","A forward modeling code is utilized to generate synthetic IRIS emission spectra of these loops based on HYDRAD output, confirming that emitting ions are out of equilibrium.","The modeling further indicates that density-dependent dielectronic recombination rates must be included to reproduce the observed line ratios.","Collectively, this evidence substantiates that the loops are subject to impulsive heating and that the components of the transiently brightened plasma are driven far from thermal equilibrium.","Heating events such as these are ubiquitous in the transition region and the analysis described above provides a robust observational diagnostic tool for characterizing the plasma."],"url":"http://arxiv.org/abs/2405.08648v1","category":"astro-ph.SR"}
{"created":"2024-05-14 14:21:43","title":"Thinking Tokens for Language Modeling","abstract":"How much is 56 times 37? Language models often make mistakes in these types of difficult calculations. This is usually explained by their inability to perform complex reasoning. Since language models rely on large training sets and great memorization capability, naturally they are not equipped to run complex calculations. However, one can argue that humans also cannot perform this calculation immediately and require a considerable amount of time to construct the solution. In order to enhance the generalization capability of language models, and as a parallel to human behavior, we propose to use special 'thinking tokens' which allow the model to perform much more calculations whenever a complex problem is encountered.","sentences":["How much is 56 times 37?","Language models often make mistakes in these types of difficult calculations.","This is usually explained by their inability to perform complex reasoning.","Since language models rely on large training sets and great memorization capability, naturally they are not equipped to run complex calculations.","However, one can argue that humans also cannot perform this calculation immediately and require a considerable amount of time to construct the solution.","In order to enhance the generalization capability of language models, and as a parallel to human behavior, we propose to use special 'thinking tokens' which allow the model to perform much more calculations whenever a complex problem is encountered."],"url":"http://arxiv.org/abs/2405.08644v1","category":"cs.CL"}
{"created":"2024-05-14 14:06:26","title":"Literature Review on Maneuver-Based Scenario Description for Automated Driving Simulations","abstract":"The increasing complexity of automated driving functions and their growing operational design domains imply more demanding requirements on their validation. Classical methods such as field tests or formal analyses are not sufficient anymore and need to be complemented by simulations. For simulations, the standard approach is scenario-based testing, as opposed to distance-based testing primarily performed in field tests. Currently, the time evolution of specific scenarios is mainly described using trajectories, which limit or at least hamper generalizations towards variations. As an alternative, maneuver-based approaches have been proposed. We shed light on the state of the art and available foundations for this new method through a literature review of early and recent works related to maneuver-based scenario description. It includes related modeling approaches originally developed for other applications. Current limitations and research gaps are identified.","sentences":["The increasing complexity of automated driving functions and their growing operational design domains imply more demanding requirements on their validation.","Classical methods such as field tests or formal analyses are not sufficient anymore and need to be complemented by simulations.","For simulations, the standard approach is scenario-based testing, as opposed to distance-based testing primarily performed in field tests.","Currently, the time evolution of specific scenarios is mainly described using trajectories, which limit or at least hamper generalizations towards variations.","As an alternative, maneuver-based approaches have been proposed.","We shed light on the state of the art and available foundations for this new method through a literature review of early and recent works related to maneuver-based scenario description.","It includes related modeling approaches originally developed for other applications.","Current limitations and research gaps are identified."],"url":"http://arxiv.org/abs/2405.08626v1","category":"cs.RO"}
{"created":"2024-05-14 13:59:24","title":"ALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation","abstract":"The field of chemistry and Artificial Intelligence (AI) intersection is an area of active research that aims to accelerate scientific discovery. The integration of large language models (LLMs) with scientific modalities has shown significant promise in this endeavour. However, challenges persist in effectively addressing training efficacy and the out-of-distribution problem, particularly as existing approaches rely on larger models and datasets. In this context, we focus on machine language-molecule translation and deploy a novel training approach called contrastive preference optimisation, which avoids generating translations that are merely adequate but not perfect. To ensure generalisability and mitigate memorisation effects, we conduct experiments using only 10\\% of the data. Our results demonstrate that our models achieve up to a 32\\% improvement compared to counterpart models. We also introduce a scalable fine-grained evaluation methodology that accommodates responsibility.","sentences":["The field of chemistry and Artificial Intelligence (AI) intersection is an area of active research that aims to accelerate scientific discovery.","The integration of large language models (LLMs) with scientific modalities has shown significant promise in this endeavour.","However, challenges persist in effectively addressing training efficacy and the out-of-distribution problem, particularly as existing approaches rely on larger models and datasets.","In this context, we focus on machine language-molecule translation and deploy a novel training approach called contrastive preference optimisation, which avoids generating translations that are merely adequate but not perfect.","To ensure generalisability and mitigate memorisation effects, we conduct experiments using only 10\\% of the data.","Our results demonstrate that our models achieve up to a 32\\% improvement compared to counterpart models.","We also introduce a scalable fine-grained evaluation methodology that accommodates responsibility."],"url":"http://arxiv.org/abs/2405.08619v2","category":"cs.CL"}
{"created":"2024-05-14 13:56:12","title":"GN-SINDy: Greedy Sampling Neural Network in Sparse Identification of Nonlinear Partial Differential Equations","abstract":"The sparse identification of nonlinear dynamical systems (SINDy) is a data-driven technique employed for uncovering and representing the fundamental dynamics of intricate systems based on observational data. However, a primary obstacle in the discovery of models for nonlinear partial differential equations (PDEs) lies in addressing the challenges posed by the curse of dimensionality and large datasets. Consequently, the strategic selection of the most informative samples within a given dataset plays a crucial role in reducing computational costs and enhancing the effectiveness of SINDy-based algorithms. To this aim, we employ a greedy sampling approach to the snapshot matrix of a PDE to obtain its valuable samples, which are suitable to train a deep neural network (DNN) in a SINDy framework. SINDy based algorithms often consist of a data collection unit, constructing a dictionary of basis functions, computing the time derivative, and solving a sparse identification problem which ends to regularised least squares minimization. In this paper, we extend the results of a SINDy based deep learning model discovery (DeePyMoD) approach by integrating greedy sampling technique in its data collection unit and new sparsity promoting algorithms in the least squares minimization unit. In this regard we introduce the greedy sampling neural network in sparse identification of nonlinear partial differential equations (GN-SINDy) which blends a greedy sampling method, the DNN, and the SINDy algorithm. In the implementation phase, to show the effectiveness of GN-SINDy, we compare its results with DeePyMoD by using a Python package that is prepared for this purpose on numerous PDE discovery","sentences":["The sparse identification of nonlinear dynamical systems (SINDy) is a data-driven technique employed for uncovering and representing the fundamental dynamics of intricate systems based on observational data.","However, a primary obstacle in the discovery of models for nonlinear partial differential equations (PDEs) lies in addressing the challenges posed by the curse of dimensionality and large datasets.","Consequently, the strategic selection of the most informative samples within a given dataset plays a crucial role in reducing computational costs and enhancing the effectiveness of SINDy-based algorithms.","To this aim, we employ a greedy sampling approach to the snapshot matrix of a PDE to obtain its valuable samples, which are suitable to train a deep neural network (DNN) in a SINDy framework.","SINDy based algorithms often consist of a data collection unit, constructing a dictionary of basis functions, computing the time derivative, and solving a sparse identification problem which ends to regularised least squares minimization.","In this paper, we extend the results of a SINDy based deep learning model discovery (DeePyMoD) approach by integrating greedy sampling technique in its data collection unit and new sparsity promoting algorithms in the least squares minimization unit.","In this regard we introduce the greedy sampling neural network in sparse identification of nonlinear partial differential equations (GN-SINDy) which blends a greedy sampling method, the DNN, and the SINDy algorithm.","In the implementation phase, to show the effectiveness of GN-SINDy, we compare its results with DeePyMoD by using a Python package that is prepared for this purpose on numerous PDE discovery"],"url":"http://arxiv.org/abs/2405.08613v1","category":"math.DS"}
{"created":"2024-05-14 13:42:19","title":"Towards Geometry-Aware Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization","abstract":"Multi-objective combinatorial optimization (MOCO) problems are prevalent in various real-world applications. Most existing neural methods for MOCO problems rely solely on decomposition and utilize precise hypervolume to enhance diversity. However, these methods often approximate only limited regions of the Pareto front and spend excessive time on diversity enhancement because of ambiguous decomposition and time-consuming hypervolume calculation. To address these limitations, we design a Geometry-Aware Pareto set Learning algorithm named GAPL, which provides a novel geometric perspective for neural MOCO via a Pareto attention model based on hypervolume expectation maximization. In addition, we propose a hypervolume residual update strategy to enable the Pareto attention model to capture both local and non-local information of the Pareto set/front. We also design a novel inference approach to further improve quality of the solution set and speed up hypervolume calculation and local subset selection. Experimental results on three classic MOCO problems demonstrate that our GAPL outperforms state-of-the-art neural baselines via superior decomposition and efficient diversity enhancement.","sentences":["Multi-objective combinatorial optimization (MOCO) problems are prevalent in various real-world applications.","Most existing neural methods for MOCO problems rely solely on decomposition and utilize precise hypervolume to enhance diversity.","However, these methods often approximate only limited regions of the Pareto front and spend excessive time on diversity enhancement because of ambiguous decomposition and time-consuming hypervolume calculation.","To address these limitations, we design a Geometry-Aware Pareto set Learning algorithm named GAPL, which provides a novel geometric perspective for neural MOCO via a Pareto attention model based on hypervolume expectation maximization.","In addition, we propose a hypervolume residual update strategy to enable the Pareto attention model to capture both local and non-local information of the Pareto set/front.","We also design a novel inference approach to further improve quality of the solution set and speed up hypervolume calculation and local subset selection.","Experimental results on three classic MOCO problems demonstrate that our GAPL outperforms state-of-the-art neural baselines via superior decomposition and efficient diversity enhancement."],"url":"http://arxiv.org/abs/2405.08604v1","category":"cs.LG"}
{"created":"2024-05-14 13:42:05","title":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine","abstract":"Since the release of ChatGPT and GPT-4, large language models (LLMs) and multimodal large language models (MLLMs) have garnered significant attention due to their powerful and general capabilities in understanding, reasoning, and generation, thereby offering new paradigms for the integration of artificial intelligence with medicine. This survey comprehensively overviews the development background and principles of LLMs and MLLMs, as well as explores their application scenarios, challenges, and future directions in medicine. Specifically, this survey begins by focusing on the paradigm shift, tracing the evolution from traditional models to LLMs and MLLMs, summarizing the model structures to provide detailed foundational knowledge. Subsequently, the survey details the entire process from constructing and evaluating to using LLMs and MLLMs with a clear logic. Following this, to emphasize the significant value of LLMs and MLLMs in healthcare, we survey and summarize 6 promising applications in healthcare. Finally, the survey discusses the challenges faced by medical LLMs and MLLMs and proposes a feasible approach and direction for the subsequent integration of artificial intelligence with medicine. Thus, this survey aims to provide researchers with a valuable and comprehensive reference guide from the perspectives of the background, principles, and clinical applications of LLMs and MLLMs.","sentences":["Since the release of ChatGPT and GPT-4, large language models (LLMs) and multimodal large language models (MLLMs) have garnered significant attention due to their powerful and general capabilities in understanding, reasoning, and generation, thereby offering new paradigms for the integration of artificial intelligence with medicine.","This survey comprehensively overviews the development background and principles of LLMs and MLLMs, as well as explores their application scenarios, challenges, and future directions in medicine.","Specifically, this survey begins by focusing on the paradigm shift, tracing the evolution from traditional models to LLMs and MLLMs, summarizing the model structures to provide detailed foundational knowledge.","Subsequently, the survey details the entire process from constructing and evaluating to using LLMs and MLLMs with a clear logic.","Following this, to emphasize the significant value of LLMs and MLLMs in healthcare, we survey and summarize 6 promising applications in healthcare.","Finally, the survey discusses the challenges faced by medical LLMs and MLLMs and proposes a feasible approach and direction for the subsequent integration of artificial intelligence with medicine.","Thus, this survey aims to provide researchers with a valuable and comprehensive reference guide from the perspectives of the background, principles, and clinical applications of LLMs and MLLMs."],"url":"http://arxiv.org/abs/2405.08603v1","category":"cs.CL"}
{"created":"2024-05-14 13:24:51","title":"EchoTracker: Advancing Myocardial Point Tracking in Echocardiography","abstract":"Tissue tracking in echocardiography is challenging due to the complex cardiac motion and the inherent nature of ultrasound acquisitions. Although optical flow methods are considered state-of-the-art (SOTA), they struggle with long-range tracking, noise occlusions, and drift throughout the cardiac cycle. Recently, novel learning-based point tracking techniques have been introduced to tackle some of these issues. In this paper, we build upon these techniques and introduce EchoTracker, a two-fold coarse-to-fine model that facilitates the tracking of queried points on a tissue surface across ultrasound image sequences. The architecture contains a preliminary coarse initialization of the trajectories, followed by reinforcement iterations based on fine-grained appearance changes. It is efficient, light, and can run on mid-range GPUs. Experiments demonstrate that the model outperforms SOTA methods, with an average position accuracy of 67% and a median trajectory error of 2.86 pixels. Furthermore, we show a relative improvement of 25% when using our model to calculate the global longitudinal strain (GLS) in a clinical test-retest dataset compared to other methods. This implies that learning-based point tracking can potentially improve performance and yield a higher diagnostic and prognostic value for clinical measurements than current techniques. Our source code is available at: https://github.com/riponazad/echotracker/.","sentences":["Tissue tracking in echocardiography is challenging due to the complex cardiac motion and the inherent nature of ultrasound acquisitions.","Although optical flow methods are considered state-of-the-art (SOTA), they struggle with long-range tracking, noise occlusions, and drift throughout the cardiac cycle.","Recently, novel learning-based point tracking techniques have been introduced to tackle some of these issues.","In this paper, we build upon these techniques and introduce EchoTracker, a two-fold coarse-to-fine model that facilitates the tracking of queried points on a tissue surface across ultrasound image sequences.","The architecture contains a preliminary coarse initialization of the trajectories, followed by reinforcement iterations based on fine-grained appearance changes.","It is efficient, light, and can run on mid-range GPUs.","Experiments demonstrate that the model outperforms SOTA methods, with an average position accuracy of 67% and a median trajectory error of 2.86 pixels.","Furthermore, we show a relative improvement of 25% when using our model to calculate the global longitudinal strain (GLS) in a clinical test-retest dataset compared to other methods.","This implies that learning-based point tracking can potentially improve performance and yield a higher diagnostic and prognostic value for clinical measurements than current techniques.","Our source code is available at: https://github.com/riponazad/echotracker/."],"url":"http://arxiv.org/abs/2405.08587v1","category":"cs.CV"}
{"created":"2024-05-14 13:24:19","title":"Cross-Domain Feature Augmentation for Domain Generalization","abstract":"Domain generalization aims to develop models that are robust to distribution shifts. Existing methods focus on learning invariance across domains to enhance model robustness, and data augmentation has been widely used to learn invariant predictors, with most methods performing augmentation in the input space. However, augmentation in the input space has limited diversity whereas in the feature space is more versatile and has shown promising results. Nonetheless, feature semantics is seldom considered and existing feature augmentation methods suffer from a limited variety of augmented features. We decompose features into class-generic, class-specific, domain-generic, and domain-specific components. We propose a cross-domain feature augmentation method named XDomainMix that enables us to increase sample diversity while emphasizing the learning of invariant representations to achieve domain generalization. Experiments on widely used benchmark datasets demonstrate that our proposed method is able to achieve state-of-the-art performance. Quantitative analysis indicates that our feature augmentation approach facilitates the learning of effective models that are invariant across different domains.","sentences":["Domain generalization aims to develop models that are robust to distribution shifts.","Existing methods focus on learning invariance across domains to enhance model robustness, and data augmentation has been widely used to learn invariant predictors, with most methods performing augmentation in the input space.","However, augmentation in the input space has limited diversity whereas in the feature space is more versatile and has shown promising results.","Nonetheless, feature semantics is seldom considered and existing feature augmentation methods suffer from a limited variety of augmented features.","We decompose features into class-generic, class-specific, domain-generic, and domain-specific components.","We propose a cross-domain feature augmentation method named XDomainMix that enables us to increase sample diversity while emphasizing the learning of invariant representations to achieve domain generalization.","Experiments on widely used benchmark datasets demonstrate that our proposed method is able to achieve state-of-the-art performance.","Quantitative analysis indicates that our feature augmentation approach facilitates the learning of effective models that are invariant across different domains."],"url":"http://arxiv.org/abs/2405.08586v1","category":"cs.CV"}
{"created":"2024-05-14 13:24:14","title":"Design of a Multi-User RIS-Aided System with Statistical Channel Knowledge","abstract":"Reconfigurable intelligent surface (RIS) is a promising technology to enhance the spectral and energy efficiency in a wireless communication system. The design of the phase shifts of an RIS in every channel coherence interval demands a huge training overhead, making its deployment practically infeasible. The design complexity can be significantly reduced by exploiting the second-order statistics of the channels. This paper is the extension of our previous work to the design of an RIS for the multi-user setup, where we employ maximisation of the lower bound of the achievable sum-rate of the users. Unlike for the single-user case, obtaining a closed-form expression for the update of the filters and phase shifts is more challenging in the multi-user case. We resort to the fractional programming (FP) approach and the non-convex block coordinate descent (BCD) method to solve the optimisation problem. As the phase shifts of the RIS obtained by the proposed algorithms are based on the statistical channel knowledge, they do not need to be updated in every channel coherence interval.","sentences":["Reconfigurable intelligent surface (RIS) is a promising technology to enhance the spectral and energy efficiency in a wireless communication system.","The design of the phase shifts of an RIS in every channel coherence interval demands a huge training overhead, making its deployment practically infeasible.","The design complexity can be significantly reduced by exploiting the second-order statistics of the channels.","This paper is the extension of our previous work to the design of an RIS for the multi-user setup, where we employ maximisation of the lower bound of the achievable sum-rate of the users.","Unlike for the single-user case, obtaining a closed-form expression for the update of the filters and phase shifts is more challenging in the multi-user case.","We resort to the fractional programming (FP) approach and the non-convex block coordinate descent (BCD) method to solve the optimisation problem.","As the phase shifts of the RIS obtained by the proposed algorithms are based on the statistical channel knowledge, they do not need to be updated in every channel coherence interval."],"url":"http://arxiv.org/abs/2405.08585v1","category":"eess.SP"}
{"created":"2024-05-14 13:18:28","title":"Intelligent Control in 6G Open RAN: Security Risk or Opportunity?","abstract":"The Open Radio Access Network (Open RAN) framework, emerging as the cornerstone for Artificial Intelligence (AI)-enabled Sixth-Generation (6G) mobile networks, heralds a transformative shift in radio access network architecture. As the adoption of Open RAN accelerates, ensuring its security becomes critical. The RAN Intelligent Controller (RIC) plays a central role in Open RAN by improving network efficiency and flexibility. Nevertheless, it also brings about potential security risks that need careful scrutiny. Therefore, it is imperative to evaluate the current state of RIC security comprehensively. This assessment is essential to gain a profound understanding of the security considerations associated with RIC. This survey combines a comprehensive analysis of RAN security, tracing its evolution from 2G to 5G, with an in-depth exploration of RIC security, marking the first comprehensive examination of its kind in the literature. Real-world security incidents involving RIC are vividly illustrated, providing practical insights. The study evaluates the security implications of the RIC within the 6G Open RAN context, addressing security vulnerabilities, mitigation strategies, and potential enhancements. It aims to guide stakeholders in the telecom industry toward a secure and dependable telecommunications infrastructure. The article serves as a valuable reference, shedding light on the RIC's crucial role within the broader network infrastructure and emphasizing security's paramount importance. This survey also explores the promising security opportunities that the RIC presents for enhancing network security and resilience in the context of 6G mobile networks. It outlines open issues, lessons learned, and future research directions in the domain of intelligent control in 6G open RAN, facilitating a comprehensive understanding of this dynamic landscape.","sentences":["The Open Radio Access Network (Open RAN) framework, emerging as the cornerstone for Artificial Intelligence (AI)-enabled Sixth-Generation (6G) mobile networks, heralds a transformative shift in radio access network architecture.","As the adoption of Open RAN accelerates, ensuring its security becomes critical.","The RAN Intelligent Controller (RIC) plays a central role in Open RAN by improving network efficiency and flexibility.","Nevertheless, it also brings about potential security risks that need careful scrutiny.","Therefore, it is imperative to evaluate the current state of RIC security comprehensively.","This assessment is essential to gain a profound understanding of the security considerations associated with RIC.","This survey combines a comprehensive analysis of RAN security, tracing its evolution from 2G to 5G, with an in-depth exploration of RIC security, marking the first comprehensive examination of its kind in the literature.","Real-world security incidents involving RIC are vividly illustrated, providing practical insights.","The study evaluates the security implications of the RIC within the 6G Open RAN context, addressing security vulnerabilities, mitigation strategies, and potential enhancements.","It aims to guide stakeholders in the telecom industry toward a secure and dependable telecommunications infrastructure.","The article serves as a valuable reference, shedding light on the RIC's crucial role within the broader network infrastructure and emphasizing security's paramount importance.","This survey also explores the promising security opportunities that the RIC presents for enhancing network security and resilience in the context of 6G mobile networks.","It outlines open issues, lessons learned, and future research directions in the domain of intelligent control in 6G open RAN, facilitating a comprehensive understanding of this dynamic landscape."],"url":"http://arxiv.org/abs/2405.08577v1","category":"cs.NI"}
{"created":"2024-05-14 13:16:46","title":"Hearing Touch: Audio-Visual Pretraining for Contact-Rich Manipulation","abstract":"Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch. In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing. Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications. In this paper, we address this gap by using contact microphones as an alternative tactile sensor. Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation. To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation. For supplementary information including videos of real robot experiments, please see https://sites.google.com/view/hearing-touch.","sentences":["Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch.","In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing.","Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications.","In this paper, we address this gap by using contact microphones as an alternative tactile sensor.","Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation.","To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation.","For supplementary information including videos of real robot experiments, please see https://sites.google.com/view/hearing-touch."],"url":"http://arxiv.org/abs/2405.08576v1","category":"cs.RO"}
{"created":"2024-05-14 12:52:42","title":"Anytime Sorting Algorithms (Extended Version)","abstract":"This paper addresses the anytime sorting problem, aiming to develop algorithms providing tentative estimates of the sorted list at each execution step. Comparisons are treated as steps, and the Spearman's footrule metric evaluates estimation accuracy. We propose a general approach for making any sorting algorithm anytime and introduce two new algorithms: multizip sort and Corsort. Simulations showcase the superior performance of both algorithms compared to existing methods. Multizip sort keeps a low global complexity, while Corsort produces intermediate estimates surpassing previous algorithms.","sentences":["This paper addresses the anytime sorting problem, aiming to develop algorithms providing tentative estimates of the sorted list at each execution step.","Comparisons are treated as steps, and the Spearman's footrule metric evaluates estimation accuracy.","We propose a general approach for making any sorting algorithm anytime and introduce two new algorithms: multizip sort and Corsort.","Simulations showcase the superior performance of both algorithms compared to existing methods.","Multizip sort keeps a low global complexity, while Corsort produces intermediate estimates surpassing previous algorithms."],"url":"http://arxiv.org/abs/2405.08564v1","category":"cs.DS"}
{"created":"2024-05-14 12:40:25","title":"Learning Multi-Agent Communication from Graph Modeling Perspective","abstract":"In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.","sentences":["In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives.","To enhance coordination among these agents, a distributed communication framework is often employed.","However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts.","In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph.","We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process.","Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner.","Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents."],"url":"http://arxiv.org/abs/2405.08550v1","category":"cs.LG"}
{"created":"2024-05-14 12:30:04","title":"Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends","abstract":"As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing. However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse. This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector. Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse. Specifically, we first analyze the advantages of the Metaverse for industrial production. Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production. Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints. Furthermore, we investigate the extant solutions devised to address them. Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse.","sentences":["As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks.","This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing.","However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse.","This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector.","Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse.","Specifically, we first analyze the advantages of the Metaverse for industrial production.","Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production.","Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints.","Furthermore, we investigate the extant solutions devised to address them.","Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse."],"url":"http://arxiv.org/abs/2405.08542v1","category":"cs.CE"}
{"created":"2024-05-14 12:26:19","title":"Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization","abstract":"Recent advances in knowledge graph embedding (KGE) rely on Euclidean/hyperbolic orthogonal relation transformations to model intrinsic logical patterns and topological structures. However, existing approaches are confined to rigid relational orthogonalization with restricted dimension and homogeneous geometry, leading to deficient modeling capability. In this work, we move beyond these approaches in terms of both dimension and geometry by introducing a powerful framework named GoldE, which features a universal orthogonal parameterization based on a generalized form of Householder reflection. Such parameterization can naturally achieve dimensional extension and geometric unification with theoretical guarantees, enabling our framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity of knowledge graphs. Empirically, GoldE achieves state-of-the-art performance on three standard benchmarks. Codes are available at https://github.com/xxrep/GoldE.","sentences":["Recent advances in knowledge graph embedding (KGE) rely on Euclidean/hyperbolic orthogonal relation transformations to model intrinsic logical patterns and topological structures.","However, existing approaches are confined to rigid relational orthogonalization with restricted dimension and homogeneous geometry, leading to deficient modeling capability.","In this work, we move beyond these approaches in terms of both dimension and geometry by introducing a powerful framework named GoldE, which features a universal orthogonal parameterization based on a generalized form of Householder reflection.","Such parameterization can naturally achieve dimensional extension and geometric unification with theoretical guarantees, enabling our framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity of knowledge graphs.","Empirically, GoldE achieves state-of-the-art performance on three standard benchmarks.","Codes are available at https://github.com/xxrep/GoldE."],"url":"http://arxiv.org/abs/2405.08540v1","category":"cs.LG"}
{"created":"2024-05-14 12:16:32","title":"The TDHF code Sky3D version 1.2","abstract":"The Sky3D code has been widely used to describe nuclear ground states, collective vibrational excitations, and heavy-ion collisions. The approach is based on Skyrme forces or related energy density functionals. The static and dynamic equations are solved on a three-dimensional grid, and pairing is been implemented in the BCS approximation. This updated version of the code aims to facilitate the calculation of nuclear strength functions in the regime of linear response theory, while retaining all existing functionality and use cases. The strength functions are benchmarked against available RPA codes, and the user has the freedom of choice when selecting the nature of external excitation (from monopole to hexadecapole and more). Some utility programs are also provided that calculate the strength function from the time-dependent output of the dynamic calculations of the Sky3D code.","sentences":["The Sky3D code has been widely used to describe nuclear ground states, collective vibrational excitations, and heavy-ion collisions.","The approach is based on Skyrme forces or related energy density functionals.","The static and dynamic equations are solved on a three-dimensional grid, and pairing is been implemented in the BCS approximation.","This updated version of the code aims to facilitate the calculation of nuclear strength functions in the regime of linear response theory, while retaining all existing functionality and use cases.","The strength functions are benchmarked against available RPA codes, and the user has the freedom of choice when selecting the nature of external excitation (from monopole to hexadecapole and more).","Some utility programs are also provided that calculate the strength function from the time-dependent output of the dynamic calculations of the Sky3D code."],"url":"http://arxiv.org/abs/2405.08531v1","category":"physics.comp-ph"}
{"created":"2024-05-14 12:07:07","title":"From Internet of Things Data to Business Processes: Challenges and a Framework","abstract":"The IoT and Business Process Management (BPM) communities co-exist in many shared application domains, such as manufacturing and healthcare. The IoT community has a strong focus on hardware, connectivity and data; the BPM community focuses mainly on finding, controlling, and enhancing the structured interactions among the IoT devices in processes. While the field of Process Mining deals with the extraction of process models and process analytics from process event logs, the data produced by IoT sensors often is at a lower granularity than these process-level events. The fundamental questions about extracting and abstracting process-related data from streams of IoT sensor values are: (1) Which sensor values can be clustered together as part of process events?, (2) Which sensor values signify the start and end of such events?, (3) Which sensor values are related but not essential? This work proposes a framework to semi-automatically perform a set of structured steps to convert low-level IoT sensor data into higher-level process events that are suitable for process mining. The framework is meant to provide a generic sequence of abstract steps to guide the event extraction, abstraction, and correlation, with variation points for plugging in specific analysis techniques and algorithms for each step. To assess the completeness of the framework, we present a set of challenges, how they can be tackled through the framework, and an example on how to instantiate the framework in a real-world demonstration from the field of smart manufacturing. Based on this framework, future research can be conducted in a structured manner through refining and improving individual steps.","sentences":["The IoT and Business Process Management (BPM) communities co-exist in many shared application domains, such as manufacturing and healthcare.","The IoT community has a strong focus on hardware, connectivity and data; the BPM community focuses mainly on finding, controlling, and enhancing the structured interactions among the IoT devices in processes.","While the field of Process Mining deals with the extraction of process models and process analytics from process event logs, the data produced by IoT sensors often is at a lower granularity than these process-level events.","The fundamental questions about extracting and abstracting process-related data from streams of IoT sensor values are: (1) Which sensor values can be clustered together as part of process events?, (2) Which sensor values signify the start and end of such events?, (3) Which sensor values are related but not essential?","This work proposes a framework to semi-automatically perform a set of structured steps to convert low-level IoT sensor data into higher-level process events that are suitable for process mining.","The framework is meant to provide a generic sequence of abstract steps to guide the event extraction, abstraction, and correlation, with variation points for plugging in specific analysis techniques and algorithms for each step.","To assess the completeness of the framework, we present a set of challenges, how they can be tackled through the framework, and an example on how to instantiate the framework in a real-world demonstration from the field of smart manufacturing.","Based on this framework, future research can be conducted in a structured manner through refining and improving individual steps."],"url":"http://arxiv.org/abs/2405.08528v1","category":"cs.SE"}
{"created":"2024-05-14 11:54:46","title":"Empowering Programmable Wireless Environments with Optical Anchor-based Positioning","abstract":"The evolution toward sixth-generation (6G) wireless networks has introduced programmable wireless environments (PWEs) and reconfigurable intelligent surfaces (RISs) as transformative elements for achieving near-deterministic wireless communications. However, the enhanced capabilities of RISs within PWEs, especially as we move toward more complex electromagnetic functions by increasing the number of reflecting elements, underscore the need for high-precision user localization, since inaccurate localization could lead to erroneous configuration of RISs, which would then compromise the effectiveness of PWEs. In this direction, this paper investigates the integration of RISs and optical anchors within PWEs, emphasizing the crucial role of ultra-precise localization in unlocking advanced electromagnetic functionalities. Specifically, we present an in-depth analysis of various localization techniques, both RISbased and RIS-independent, while introducing the concept of empowering PWEs with optical anchors for enhanced localization precision. Our findings highlight that accurate localization is essential to fully exploit the capabilities of RISs, paving the way for future applications. Through this exploration, we contribute to the advancement of PWEs in line with the ambitious goals of the 6G standards and improve the quality of service in next generation wireless networks.","sentences":["The evolution toward sixth-generation (6G) wireless networks has introduced programmable wireless environments (PWEs) and reconfigurable intelligent surfaces (RISs) as transformative elements for achieving near-deterministic wireless communications.","However, the enhanced capabilities of RISs within PWEs, especially as we move toward more complex electromagnetic functions by increasing the number of reflecting elements, underscore the need for high-precision user localization, since inaccurate localization could lead to erroneous configuration of RISs, which would then compromise the effectiveness of PWEs.","In this direction, this paper investigates the integration of RISs and optical anchors within PWEs, emphasizing the crucial role of ultra-precise localization in unlocking advanced electromagnetic functionalities.","Specifically, we present an in-depth analysis of various localization techniques, both RISbased and RIS-independent, while introducing the concept of empowering PWEs with optical anchors for enhanced localization precision.","Our findings highlight that accurate localization is essential to fully exploit the capabilities of RISs, paving the way for future applications.","Through this exploration, we contribute to the advancement of PWEs in line with the ambitious goals of the 6G standards and improve the quality of service in next generation wireless networks."],"url":"http://arxiv.org/abs/2405.08520v1","category":"cs.IT"}
{"created":"2024-05-14 11:37:29","title":"Precarious Experiences: Citizens' Frustrations, Anxieties and Burdens of an Online Welfare Benefit System","abstract":"There is a significant overlap between people who are supported by income-related social welfare benefits, often in precarious situations, and those who experience greater digital exclusion. We report on a study of claimants using the UK's Universal Credit online welfare benefit system designed as, and still, \"digital by default\". Through data collection involving remote interviews (n=11) and online surveys (n=66), we expose claimants' own lived experiences interacting with this system. The claimants explain how digital channels can contribute to an imbalance of power and agency, at a time when their own circumstances mean they have reduced abilities, resources and capacities, and where design choices can adversely affect people's utility to leverage help from their own wider socio-technical ecosystems. We contribute eight recommendations from these accounts to inform the future design and development of digital welfare benefit systems for this population, to reduce digital barriers and harms.","sentences":["There is a significant overlap between people who are supported by income-related social welfare benefits, often in precarious situations, and those who experience greater digital exclusion.","We report on a study of claimants using the UK's Universal Credit online welfare benefit system designed as, and still, \"digital by default\".","Through data collection involving remote interviews (n=11) and online surveys (n=66), we expose claimants' own lived experiences interacting with this system.","The claimants explain how digital channels can contribute to an imbalance of power and agency, at a time when their own circumstances mean they have reduced abilities, resources and capacities, and where design choices can adversely affect people's utility to leverage help from their own wider socio-technical ecosystems.","We contribute eight recommendations from these accounts to inform the future design and development of digital welfare benefit systems for this population, to reduce digital barriers and harms."],"url":"http://arxiv.org/abs/2405.08515v1","category":"cs.HC"}
{"created":"2024-05-14 11:21:52","title":"Growing Artificial Neural Networks for Control: the Role of Neuronal Diversity","abstract":"In biological evolution complex neural structures grow from a handful of cellular ingredients. As genomes in nature are bounded in size, this complexity is achieved by a growth process where cells communicate locally to decide whether to differentiate, proliferate and connect with other cells. This self-organisation is hypothesized to play an important part in the generalisation, and robustness of biological neural networks. Artificial neural networks (ANNs), on the other hand, are traditionally optimized in the space of weights. Thus, the benefits and challenges of growing artificial neural networks remain understudied. Building on the previously introduced Neural Developmental Programs (NDP), in this work we present an algorithm for growing ANNs that solve reinforcement learning tasks. We identify a key challenge: ensuring phenotypic complexity requires maintaining neuronal diversity, but this diversity comes at the cost of optimization stability. To address this, we introduce two mechanisms: (a) equipping neurons with an intrinsic state inherited upon neurogenesis; (b) lateral inhibition, a mechanism inspired by biological growth, which controlls the pace of growth, helping diversity persist. We show that both mechanisms contribute to neuronal diversity and that, equipped with them, NDPs achieve comparable results to existing direct and developmental encodings in complex locomotion tasks","sentences":["In biological evolution complex neural structures grow from a handful of cellular ingredients.","As genomes in nature are bounded in size, this complexity is achieved by a growth process where cells communicate locally to decide whether to differentiate, proliferate and connect with other cells.","This self-organisation is hypothesized to play an important part in the generalisation, and robustness of biological neural networks.","Artificial neural networks (ANNs), on the other hand, are traditionally optimized in the space of weights.","Thus, the benefits and challenges of growing artificial neural networks remain understudied.","Building on the previously introduced Neural Developmental Programs (NDP), in this work we present an algorithm for growing ANNs that solve reinforcement learning tasks.","We identify a key challenge: ensuring phenotypic complexity requires maintaining neuronal diversity, but this diversity comes at the cost of optimization stability.","To address this, we introduce two mechanisms: (a) equipping neurons with an intrinsic state inherited upon neurogenesis; (b) lateral inhibition, a mechanism inspired by biological growth, which controlls the pace of growth, helping diversity persist.","We show that both mechanisms contribute to neuronal diversity and that, equipped with them, NDPs achieve comparable results to existing direct and developmental encodings in complex locomotion tasks"],"url":"http://arxiv.org/abs/2405.08510v1","category":"cs.NE"}
{"created":"2024-05-14 10:29:04","title":"Metastable hierarchy in abstract low-temperature lattice models: an application to Kawasaki dynamics for Ising lattice gas with macrscopic number of particles","abstract":"This article is divided into two parts. In the first part, we study the hierarchical phenomenon of metastability in low-temperature lattice models in the most general setting. Given an abstract dynamical system governed by a Hamiltonian function, we prove that there exists a hierarchical decomposition of the collection of stable plateaux in the system into multiple $\\mathfrak{m}$ levels, such that at each level there exist tunneling metastable transitions between the stable plateaux, which can be characterized by convergence to a simple Markov chain as the inverse temperature $\\beta$ tends to infinity. In the second part, as an application, we characterize the $3$-level metastable hierarchy in Kawasaki dynamics for Ising lattice gas with macroscopic number of particles. We prove that the ground states in this model are those in which the particles line up and form a one-dimensional strip, and identify the full structure relevant to the tunneling transitions between these ground states. In particular, the results differ from the previous work [5] in that the particles in the ground states are likely to form a strip rather than a square droplet. The main tool is the resolvent approach to metastability, recently developed in [24]. Along with the analysis, we present a theorem on the sharp asymptotics of the exit distribution from cycles, which to the author's knowledge is not known in the community and therefore may be of independent interest.","sentences":["This article is divided into two parts.","In the first part, we study the hierarchical phenomenon of metastability in low-temperature lattice models in the most general setting.","Given an abstract dynamical system governed by a Hamiltonian function, we prove that there exists a hierarchical decomposition of the collection of stable plateaux in the system into multiple $\\mathfrak{m}$ levels, such that at each level there exist tunneling metastable transitions between the stable plateaux, which can be characterized by convergence to a simple Markov chain as the inverse temperature $\\beta$ tends to infinity.","In the second part, as an application, we characterize the $3$-level metastable hierarchy in Kawasaki dynamics for Ising lattice gas with macroscopic number of particles.","We prove that the ground states in this model are those in which the particles line up and form a one-dimensional strip, and identify the full structure relevant to the tunneling transitions between these ground states.","In particular, the results differ from the previous work [5] in that the particles in the ground states are likely to form a strip rather than a square droplet.","The main tool is the resolvent approach to metastability, recently developed in [24].","Along with the analysis, we present a theorem on the sharp asymptotics of the exit distribution from cycles, which to the author's knowledge is not known in the community and therefore may be of independent interest."],"url":"http://arxiv.org/abs/2405.08488v1","category":"math.PR"}
{"created":"2024-05-14 10:10:45","title":"RDPN6D: Residual-based Dense Point-wise Network for 6Dof Object Pose Estimation Based on RGB-D Images","abstract":"In this work, we introduce a novel method for calculating the 6DoF pose of an object using a single RGB-D image. Unlike existing methods that either directly predict objects' poses or rely on sparse keypoints for pose recovery, our approach addresses this challenging task using dense correspondence, i.e., we regress the object coordinates for each visible pixel. Our method leverages existing object detection methods. We incorporate a re-projection mechanism to adjust the camera's intrinsic matrix to accommodate cropping in RGB-D images. Moreover, we transform the 3D object coordinates into a residual representation, which can effectively reduce the output space and yield superior performance. We conducted extensive experiments to validate the efficacy of our approach for 6D pose estimation. Our approach outperforms most previous methods, especially in occlusion scenarios, and demonstrates notable improvements over the state-of-the-art methods. Our code is available on https://github.com/AI-Application-and-Integration-Lab/RDPN6D.","sentences":["In this work, we introduce a novel method for calculating the 6DoF pose of an object using a single RGB-D image.","Unlike existing methods that either directly predict objects' poses or rely on sparse keypoints for pose recovery, our approach addresses this challenging task using dense correspondence, i.e., we regress the object coordinates for each visible pixel.","Our method leverages existing object detection methods.","We incorporate a re-projection mechanism to adjust the camera's intrinsic matrix to accommodate cropping in RGB-D images.","Moreover, we transform the 3D object coordinates into a residual representation, which can effectively reduce the output space and yield superior performance.","We conducted extensive experiments to validate the efficacy of our approach for 6D pose estimation.","Our approach outperforms most previous methods, especially in occlusion scenarios, and demonstrates notable improvements over the state-of-the-art methods.","Our code is available on https://github.com/AI-Application-and-Integration-Lab/RDPN6D."],"url":"http://arxiv.org/abs/2405.08483v1","category":"cs.CV"}
{"created":"2024-05-14 09:55:03","title":"Improving the Real-Data Driven Network Evaluation Model for Digital Twin Networks","abstract":"With the emergence and proliferation of new forms of large-scale services such as smart homes, virtual reality/augmented reality, the increasingly complex networks are raising concerns about significant operational costs. As a result, the need for network management automation is emphasized, and Digital Twin Networks (DTN) technology is expected to become the foundation technology for autonomous networks. DTN has the advantage of being able to operate and system networks based on real-time collected data in a closed-loop system, and currently it is mainly designed for optimization scenarios. To improve network performance in optimization scenarios, it is necessary to select appropriate configurations and perform accurate performance evaluation based on real data. However, most network evaluation models currently use simulation data. Meanwhile, according to DTN standards documents, artificial intelligence (AI) models can ensure scalability, real-time performance, and accuracy in large-scale networks. Various AI research and standardization work is ongoing to optimize the use of DTN. When designing AI models, it is crucial to consider the characteristics of the data. This paper presents an autoencoder-based skip connected message passing neural network (AE-SMPN) as a network evaluation model using real network data. The model is created by utilizing graph neural network (GNN) with recurrent neural network (RNN) models to capture the spatiotemporal features of network data. Additionally, an AutoEncoder (AE) is employed to extract initial features. The neural network was trained using the real DTN dataset provided by the Barcelona Neural Networking Center (BNN-UPC), and the paper presents the analysis of the model structure along with experimental results.","sentences":["With the emergence and proliferation of new forms of large-scale services such as smart homes, virtual reality/augmented reality, the increasingly complex networks are raising concerns about significant operational costs.","As a result, the need for network management automation is emphasized, and Digital Twin Networks (DTN) technology is expected to become the foundation technology for autonomous networks.","DTN has the advantage of being able to operate and system networks based on real-time collected data in a closed-loop system, and currently it is mainly designed for optimization scenarios.","To improve network performance in optimization scenarios, it is necessary to select appropriate configurations and perform accurate performance evaluation based on real data.","However, most network evaluation models currently use simulation data.","Meanwhile, according to DTN standards documents, artificial intelligence (AI) models can ensure scalability, real-time performance, and accuracy in large-scale networks.","Various AI research and standardization work is ongoing to optimize the use of DTN.","When designing AI models, it is crucial to consider the characteristics of the data.","This paper presents an autoencoder-based skip connected message passing neural network (AE-SMPN) as a network evaluation model using real network data.","The model is created by utilizing graph neural network (GNN) with recurrent neural network (RNN) models to capture the spatiotemporal features of network data.","Additionally, an AutoEncoder (AE) is employed to extract initial features.","The neural network was trained using the real DTN dataset provided by the Barcelona Neural Networking Center (BNN-UPC), and the paper presents the analysis of the model structure along with experimental results."],"url":"http://arxiv.org/abs/2405.08473v1","category":"cs.LG"}
{"created":"2024-05-14 09:51:09","title":"GPT-3.5 for Grammatical Error Correction","abstract":"This paper investigates the application of GPT-3.5 for Grammatical Error Correction (GEC) in multiple languages in several settings: zero-shot GEC, fine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses generated by other GEC models. In the zero-shot setting, we conduct automatic evaluations of the corrections proposed by GPT-3.5 using several methods: estimating grammaticality with language models (LMs), the Scribendi test, and comparing the semantic embeddings of sentences. GPT-3.5 has a known tendency to over-correct erroneous sentences and propose alternative corrections. For several languages, such as Czech, German, Russian, Spanish, and Ukrainian, GPT-3.5 substantially alters the source sentences, including their semantics, which presents significant challenges for evaluation with reference-based metrics. For English, GPT-3.5 demonstrates high recall, generates fluent corrections, and generally preserves sentence semantics. However, human evaluation for both English and Russian reveals that, despite its strong error-detection capabilities, GPT-3.5 struggles with several error types, including punctuation mistakes, tense errors, syntactic dependencies between words, and lexical compatibility at the sentence level.","sentences":["This paper investigates the application of GPT-3.5 for Grammatical Error Correction (GEC) in multiple languages in several settings: zero-shot GEC, fine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses generated by other GEC models.","In the zero-shot setting, we conduct automatic evaluations of the corrections proposed by GPT-3.5 using several methods: estimating grammaticality with language models (LMs), the Scribendi test, and comparing the semantic embeddings of sentences.","GPT-3.5 has a known tendency to over-correct erroneous sentences and propose alternative corrections.","For several languages, such as Czech, German, Russian, Spanish, and Ukrainian, GPT-3.5 substantially alters the source sentences, including their semantics, which presents significant challenges for evaluation with reference-based metrics.","For English, GPT-3.5 demonstrates high recall, generates fluent corrections, and generally preserves sentence semantics.","However, human evaluation for both English and Russian reveals that, despite its strong error-detection capabilities, GPT-3.5 struggles with several error types, including punctuation mistakes, tense errors, syntactic dependencies between words, and lexical compatibility at the sentence level."],"url":"http://arxiv.org/abs/2405.08469v1","category":"cs.CL"}
{"created":"2024-05-14 09:44:52","title":"Challenges and Opportunities in Text Generation Explainability","abstract":"The necessity for interpretability in natural language processing (NLP) has risen alongside the growing prominence of large language models. Among the myriad tasks within NLP, text generation stands out as a primary objective of autoregressive models. The NLP community has begun to take a keen interest in gaining a deeper understanding of text generation, leading to the development of model-agnostic explainable artificial intelligence (xAI) methods tailored to this task. The design and evaluation of explainability methods are non-trivial since they depend on many factors involved in the text generation process, e.g., the autoregressive model and its stochastic nature. This paper outlines 17 challenges categorized into three groups that arise during the development and assessment of attribution-based explainability methods. These challenges encompass issues concerning tokenization, defining explanation similarity, determining token importance and prediction change metrics, the level of human intervention required, and the creation of suitable test datasets. The paper illustrates how these challenges can be intertwined, showcasing new opportunities for the community. These include developing probabilistic word-level explainability methods and engaging humans in the explainability pipeline, from the data design to the final evaluation, to draw robust conclusions on xAI methods.","sentences":["The necessity for interpretability in natural language processing (NLP) has risen alongside the growing prominence of large language models.","Among the myriad tasks within NLP, text generation stands out as a primary objective of autoregressive models.","The NLP community has begun to take a keen interest in gaining a deeper understanding of text generation, leading to the development of model-agnostic explainable artificial intelligence (xAI) methods tailored to this task.","The design and evaluation of explainability methods are non-trivial since they depend on many factors involved in the text generation process, e.g., the autoregressive model and its stochastic nature.","This paper outlines 17 challenges categorized into three groups that arise during the development and assessment of attribution-based explainability methods.","These challenges encompass issues concerning tokenization, defining explanation similarity, determining token importance and prediction change metrics, the level of human intervention required, and the creation of suitable test datasets.","The paper illustrates how these challenges can be intertwined, showcasing new opportunities for the community.","These include developing probabilistic word-level explainability methods and engaging humans in the explainability pipeline, from the data design to the final evaluation, to draw robust conclusions on xAI methods."],"url":"http://arxiv.org/abs/2405.08468v1","category":"cs.CL"}
{"created":"2024-05-14 09:43:32","title":"Equilibrium Propagation: the Quantum and the Thermal Cases","abstract":"Equilibrium propagation is a recently introduced method to use and train artificial neural networks in which the network is at the minimum (more generally extremum) of an energy functional. Equilibrium propagation has shown good performance on a number of benchmark tasks. Here we extend equilibrium propagation in two directions. First we show that there is a natural quantum generalization of equilibrium propagation in which a quantum neural network is taken to be in the ground state (more generally any eigenstate) of the network Hamiltonian, with a similar training mechanism that exploits the fact that the mean energy is extremal on eigenstates. Second we extend the analysis of equilibrium propagation at finite temperature, showing that thermal fluctuations allow one to naturally train the network without having to clamp the output layer during training. We also study the low temperature limit of equilibrium propagation.","sentences":["Equilibrium propagation is a recently introduced method to use and train artificial neural networks in which the network is at the minimum (more generally extremum) of an energy functional.","Equilibrium propagation has shown good performance on a number of benchmark tasks.","Here we extend equilibrium propagation in two directions.","First we show that there is a natural quantum generalization of equilibrium propagation in which a quantum neural network is taken to be in the ground state (more generally any eigenstate) of the network Hamiltonian, with a similar training mechanism that exploits the fact that the mean energy is extremal on eigenstates.","Second we extend the analysis of equilibrium propagation at finite temperature, showing that thermal fluctuations allow one to naturally train the network without having to clamp the output layer during training.","We also study the low temperature limit of equilibrium propagation."],"url":"http://arxiv.org/abs/2405.08467v1","category":"quant-ph"}
{"created":"2024-05-14 09:42:21","title":"Work-in-Progress: Crash Course: Can (Under Attack) Autonomous Driving Beat Human Drivers?","abstract":"Autonomous driving is a research direction that has gained enormous traction in the last few years thanks to advancements in Artificial Intelligence (AI). Depending on the level of independence from the human driver, several studies show that Autonomous Vehicles (AVs) can reduce the number of on-road crashes and decrease overall fuel emissions by improving efficiency. However, security research on this topic is mixed and presents some gaps. On one hand, these studies often neglect the intrinsic vulnerabilities of AI algorithms, which are known to compromise the security of these systems. On the other, the most prevalent attacks towards AI rely on unrealistic assumptions, such as access to the model parameters or the training dataset. As such, it is unclear if autonomous driving can still claim several advantages over human driving in real-world applications. This paper evaluates the inherent risks in autonomous driving by examining the current landscape of AVs and establishing a pragmatic threat model. Through our analysis, we develop specific claims highlighting the delicate balance between the advantages of AVs and potential security challenges in real-world scenarios. Our evaluation serves as a foundation for providing essential takeaway messages, guiding both researchers and practitioners at various stages of the automation pipeline. In doing so, we contribute valuable insights to advance the discourse on the security and viability of autonomous driving in real-world applications.","sentences":["Autonomous driving is a research direction that has gained enormous traction in the last few years thanks to advancements in Artificial Intelligence (AI).","Depending on the level of independence from the human driver, several studies show that Autonomous Vehicles (AVs) can reduce the number of on-road crashes and decrease overall fuel emissions by improving efficiency.","However, security research on this topic is mixed and presents some gaps.","On one hand, these studies often neglect the intrinsic vulnerabilities of AI algorithms, which are known to compromise the security of these systems.","On the other, the most prevalent attacks towards AI rely on unrealistic assumptions, such as access to the model parameters or the training dataset.","As such, it is unclear if autonomous driving can still claim several advantages over human driving in real-world applications.","This paper evaluates the inherent risks in autonomous driving by examining the current landscape of AVs and establishing a pragmatic threat model.","Through our analysis, we develop specific claims highlighting the delicate balance between the advantages of AVs and potential security challenges in real-world scenarios.","Our evaluation serves as a foundation for providing essential takeaway messages, guiding both researchers and practitioners at various stages of the automation pipeline.","In doing so, we contribute valuable insights to advance the discourse on the security and viability of autonomous driving in real-world applications."],"url":"http://arxiv.org/abs/2405.08466v1","category":"cs.CR"}
{"created":"2024-05-14 09:38:44","title":"How to Surprisingly Consider Recommendations? A Knowledge-Graph-based Approach Relying on Complex Network Metrics","abstract":"Traditional recommendation proposals, including content-based and collaborative filtering, usually focus on similarity between items or users. Existing approaches lack ways of introducing unexpectedness into recommendations, prioritizing globally popular items over exposing users to unforeseen items. This investigation aims to design and evaluate a novel layer on top of recommender systems suited to incorporate relational information and suggest items with a user-defined degree of surprise. We propose a Knowledge Graph (KG) based recommender system by encoding user interactions on item catalogs. Our study explores whether network-level metrics on KGs can influence the degree of surprise in recommendations. We hypothesize that surprisingness correlates with certain network metrics, treating user profiles as subgraphs within a larger catalog KG. The achieved solution reranks recommendations based on their impact on structural graph metrics. Our research contributes to optimizing recommendations to reflect the metrics. We experimentally evaluate our approach on two datasets of LastFM listening histories and synthetic Netflix viewing profiles. We find that reranking items based on complex network metrics leads to a more unexpected and surprising composition of recommendation lists.","sentences":["Traditional recommendation proposals, including content-based and collaborative filtering, usually focus on similarity between items or users.","Existing approaches lack ways of introducing unexpectedness into recommendations, prioritizing globally popular items over exposing users to unforeseen items.","This investigation aims to design and evaluate a novel layer on top of recommender systems suited to incorporate relational information and suggest items with a user-defined degree of surprise.","We propose a Knowledge Graph (KG) based recommender system by encoding user interactions on item catalogs.","Our study explores whether network-level metrics on KGs can influence the degree of surprise in recommendations.","We hypothesize that surprisingness correlates with certain network metrics, treating user profiles as subgraphs within a larger catalog KG.","The achieved solution reranks recommendations based on their impact on structural graph metrics.","Our research contributes to optimizing recommendations to reflect the metrics.","We experimentally evaluate our approach on two datasets of LastFM listening histories and synthetic Netflix viewing profiles.","We find that reranking items based on complex network metrics leads to a more unexpected and surprising composition of recommendation lists."],"url":"http://arxiv.org/abs/2405.08465v1","category":"cs.IR"}
{"created":"2024-05-14 09:31:31","title":"Evaluating LLMs at Evaluating Temporal Generalization","abstract":"The rapid advancement of Large Language Models (LLMs) highlights the urgent need for evolving evaluation methodologies that keep pace with improvements in language comprehension and information processing. However, traditional benchmarks, which are often static, fail to capture the continually changing information landscape, leading to a disparity between the perceived and actual effectiveness of LLMs in ever-changing real-world scenarios. Furthermore, these benchmarks do not adequately measure the models' capabilities over a broader temporal range or their adaptability over time. We examine current LLMs in terms of temporal generalization and bias, revealing that various temporal biases emerge in both language likelihood and prognostic prediction. This serves as a caution for LLM practitioners to pay closer attention to mitigating temporal biases. Also, we propose an evaluation framework Freshbench for dynamically generating benchmarks from the most recent real-world prognostication prediction. Our code is available at https://github.com/FreedomIntelligence/FreshBench. The dataset will be released soon.","sentences":["The rapid advancement of Large Language Models (LLMs) highlights the urgent need for evolving evaluation methodologies that keep pace with improvements in language comprehension and information processing.","However, traditional benchmarks, which are often static, fail to capture the continually changing information landscape, leading to a disparity between the perceived and actual effectiveness of LLMs in ever-changing real-world scenarios.","Furthermore, these benchmarks do not adequately measure the models' capabilities over a broader temporal range or their adaptability over time.","We examine current LLMs in terms of temporal generalization and bias, revealing that various temporal biases emerge in both language likelihood and prognostic prediction.","This serves as a caution for LLM practitioners to pay closer attention to mitigating temporal biases.","Also, we propose an evaluation framework Freshbench for dynamically generating benchmarks from the most recent real-world prognostication prediction.","Our code is available at https://github.com/FreedomIntelligence/FreshBench.","The dataset will be released soon."],"url":"http://arxiv.org/abs/2405.08460v1","category":"cs.CL"}
{"created":"2024-05-14 09:19:46","title":"Far-from-equilibrium complex landscapes","abstract":"Systems with a complex dynamics like glasses or models of biological evolution are often pictured in terms of complex landscapes, with a large number of possible collective states. We show on the example of a stochastic spin model with non-reciprocal and heterogeneous interactions how the complex landscape notion can be generalized far from equilibrium, where collective states may exhibit spontaneous oscillations, often hidden by the presence of disorder. We identify relevant observables, like the density of entropy production, to unveil the presence of oscillations, and we characterize the complex landscape of our model in terms of a configurational entropy, that counts the number of nonequilibrium collective states with a given entropy production density.","sentences":["Systems with a complex dynamics like glasses or models of biological evolution are often pictured in terms of complex landscapes, with a large number of possible collective states.","We show on the example of a stochastic spin model with non-reciprocal and heterogeneous interactions how the complex landscape notion can be generalized far from equilibrium, where collective states may exhibit spontaneous oscillations, often hidden by the presence of disorder.","We identify relevant observables, like the density of entropy production, to unveil the presence of oscillations, and we characterize the complex landscape of our model in terms of a configurational entropy, that counts the number of nonequilibrium collective states with a given entropy production density."],"url":"http://arxiv.org/abs/2405.08452v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-14 09:12:30","title":"Understanding the performance gap between online and offline alignment algorithms","abstract":"Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.","sentences":["Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment.","However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF.","Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods.","This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations.","We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference.","We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification.","This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process.","Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks.","Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms."],"url":"http://arxiv.org/abs/2405.08448v1","category":"cs.LG"}
{"created":"2024-05-14 08:42:49","title":"Impact of Stickers on Multimodal Chat Sentiment Analysis and Intent Recognition: A New Task, Dataset and Baseline","abstract":"Stickers are increasingly used in social media to express sentiment and intent. When finding typing troublesome, people often use a sticker instead. Despite the significant impact of stickers on sentiment analysis and intent recognition, little research has been conducted. To address this gap, we propose a new task: Multimodal chat Sentiment Analysis and Intent Recognition involving Stickers (MSAIRS). Additionally, we introduce a novel multimodal dataset containing Chinese chat records and stickers excerpted from several mainstream social media platforms. Our dataset includes paired data with the same text but different stickers, and various stickers consisting of the same images with different texts, allowing us to better understand the impact of stickers on chat sentiment and intent. We also propose an effective multimodal joint model, MMSAIR, for our task, which is validated on our datasets and indicates that visual information of stickers counts. Our dataset and code will be publicly available.","sentences":["Stickers are increasingly used in social media to express sentiment and intent.","When finding typing troublesome, people often use a sticker instead.","Despite the significant impact of stickers on sentiment analysis and intent recognition, little research has been conducted.","To address this gap, we propose a new task: Multimodal chat Sentiment Analysis and Intent Recognition involving Stickers (MSAIRS).","Additionally, we introduce a novel multimodal dataset containing Chinese chat records and stickers excerpted from several mainstream social media platforms.","Our dataset includes paired data with the same text but different stickers, and various stickers consisting of the same images with different texts, allowing us to better understand the impact of stickers on chat sentiment and intent.","We also propose an effective multimodal joint model, MMSAIR, for our task, which is validated on our datasets and indicates that visual information of stickers counts.","Our dataset and code will be publicly available."],"url":"http://arxiv.org/abs/2405.08427v1","category":"cs.CL"}
{"created":"2024-05-14 08:42:00","title":"On the universal and generalized orbifold Euler characteristics","abstract":"We discuss the universal orbifold Euler characteristic and generalized orbifold Euler characteristics corresponding to finitely generated groups $A$ (the $A$-Euler characteristics). We show that the collection of all $A$-Euler characteristics for $A$ of the form $A'\\times Z$ ($Z$ is the group of integers) with finite $A'$ determine the universal orbifold Euler characteristic.","sentences":["We discuss the universal orbifold Euler characteristic and generalized orbifold Euler characteristics corresponding to finitely generated groups $A$ (the $A$-Euler characteristics).","We show that the collection of all $A$-Euler characteristics for $A$ of the form $A'\\times Z$ ($Z$ is the group of integers) with finite $A'$ determine the universal orbifold Euler characteristic."],"url":"http://arxiv.org/abs/2405.08426v1","category":"math.AG"}
{"created":"2024-05-14 07:55:34","title":"Realtime Global Optimization of a Fail-Safe Emergency Stop Maneuver for Arbitrary Electrical / Electronical Failures in Automated Driving","abstract":"In the event of a critical system failures in auto-mated vehicles, fail-operational or fail-safe measures provide minimum guarantees for the vehicle's performance, depending on which of its subsystems remain operational. Various such methods have been proposed which, upon failure, use different remaining sets of operational subsystems to execute maneuvers that bring the vehicle into a safe state under different environmental conditions. One particular such method proposes a fail-safe emergency stop system that requires no particular electric or electronic subsystem to be available after failure, and still provides a basic situation-dependent emergency stop maneuver. This is achieved by preemptively setting parameters to a hydraulic / mechanical system prior to failure, which after failure executes the preset maneuver \"blindly\". The focus of this paper is the particular challenge of implementing a lightweight planning algorithm that can cope with the complex uncertainties of the given task while still providing a globally optimal solution at regular intervals, based on the perceived and predicted environment of the automated vehicle.","sentences":["In the event of a critical system failures in auto-mated vehicles, fail-operational or fail-safe measures provide minimum guarantees for the vehicle's performance, depending on which of its subsystems remain operational.","Various such methods have been proposed which, upon failure, use different remaining sets of operational subsystems to execute maneuvers that bring the vehicle into a safe state under different environmental conditions.","One particular such method proposes a fail-safe emergency stop system that requires no particular electric or electronic subsystem to be available after failure, and still provides a basic situation-dependent emergency stop maneuver.","This is achieved by preemptively setting parameters to a hydraulic / mechanical system prior to failure, which after failure executes the preset maneuver \"blindly\".","The focus of this paper is the particular challenge of implementing a lightweight planning algorithm that can cope with the complex uncertainties of the given task while still providing a globally optimal solution at regular intervals, based on the perceived and predicted environment of the automated vehicle."],"url":"http://arxiv.org/abs/2405.08401v1","category":"cs.RO"}
{"created":"2024-05-14 07:23:10","title":"CIER: A Novel Experience Replay Approach with Causal Inference in Deep Reinforcement Learning","abstract":"In the training process of Deep Reinforcement Learning (DRL), agents require repetitive interactions with the environment. With an increase in training volume and model complexity, it is still a challenging problem to enhance data utilization and explainability of DRL training. This paper addresses these challenges by focusing on the temporal correlations within the time dimension of time series. We propose a novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences. Furthermore, the subsequences are employed for causal inference to identify fundamental causal factors that significantly impact training outcomes. We design a module to provide feedback on the causality during DRL training. Several experiments demonstrate the feasibility of our approach in common environments, confirming its ability to enhance the effectiveness of DRL training and impart a certain level of explainability to the training process. Additionally, we extended our approach with priority experience replay algorithm, and experimental results demonstrate the continued effectiveness of our approach.","sentences":["In the training process of Deep Reinforcement Learning (DRL), agents require repetitive interactions with the environment.","With an increase in training volume and model complexity, it is still a challenging problem to enhance data utilization and explainability of DRL training.","This paper addresses these challenges by focusing on the temporal correlations within the time dimension of time series.","We propose a novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences.","Furthermore, the subsequences are employed for causal inference to identify fundamental causal factors that significantly impact training outcomes.","We design a module to provide feedback on the causality during DRL training.","Several experiments demonstrate the feasibility of our approach in common environments, confirming its ability to enhance the effectiveness of DRL training and impart a certain level of explainability to the training process.","Additionally, we extended our approach with priority experience replay algorithm, and experimental results demonstrate the continued effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.08380v1","category":"cs.LG"}
{"created":"2024-05-14 07:16:36","title":"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles","abstract":"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.","sentences":["This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals.","This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it.","Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information.","We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy.","We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge.","In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance."],"url":"http://arxiv.org/abs/2405.08373v1","category":"cs.CL"}
{"created":"2024-05-14 07:10:03","title":"Microscopic investigation of wobbling motion in atomic nuclei using the triaxial projected shell model approach","abstract":"A systematic investigation of the wobbling band structures observed in odd-mass nuclei of $^{161,163,165,167}$Lu, $^{167}$Ta $^{131}$Cs, $^{135}$Pr, $^{151}$Eu, $^{183}$Au, $^{133}$Ba, $^{105}$Pd, $^{133}$La, $^{187}$Au and $^{127}$Xe is performed using the triaxial projected shell model (TPSM) approach. It is demonstrated that all the studied band structures have transverse wobbling mode, except for $^{133}$La, $^{187}$Au (negative parity), $^{183}$Au (positive parity) and $^{127}$Xe nuclei where the wobbling frequency increases with spin, indicating that the collective motion has a longitudinal character. To elucidate further the wobbling nature of the band structures, electromagnetic transition probabilities have been evaluated and it is observed that inter-band transitions are dominated by $E2$ rather than $M1$ as expected for a typical signature partner band. It is shown that TPSM approach provides a reasonable description of all the measured properties of the studied nuclei.","sentences":["A systematic investigation of the wobbling band structures observed in odd-mass nuclei of $^{161,163,165,167}$Lu, $^{167}$Ta $^{131}$Cs, $^{135}$Pr, $^{151}$Eu, $^{183}$Au, $^{133}$Ba, $^{105}$Pd, $^{133}$La, $^{187}$Au and $^{127}$Xe is performed using the triaxial projected shell model (TPSM) approach.","It is demonstrated that all the studied band structures have transverse wobbling mode, except for $^{133}$La, $^{187}$Au (negative parity), $^{183}$Au (positive parity) and $^{127}$Xe nuclei where the wobbling frequency increases with spin, indicating that the collective motion has a longitudinal character.","To elucidate further the wobbling nature of the band structures, electromagnetic transition probabilities have been evaluated and it is observed that inter-band transitions are dominated by $E2$ rather than $M1$ as expected for a typical signature partner band.","It is shown that TPSM approach provides a reasonable description of all the measured properties of the studied nuclei."],"url":"http://arxiv.org/abs/2405.08368v1","category":"nucl-th"}
{"created":"2024-05-14 06:55:16","title":"GPS-IDS: An Anomaly-based GPS Spoofing Attack Detection Framework for Autonomous Vehicles","abstract":"Autonomous Vehicles (AVs) heavily rely on sensors and communication networks like Global Positioning System (GPS) to navigate autonomously. Prior research has indicated that networks like GPS are vulnerable to cyber-attacks such as spoofing and jamming, thus posing serious risks like navigation errors and system failures. These threats are expected to intensify with the widespread deployment of AVs, making it crucial to detect and mitigate such attacks. This paper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly Behavior Analysis (ABA)-based intrusion detection framework to detect GPS spoofing attacks on AVs. The framework uses a novel physics-based vehicle behavior model where a GPS navigation model is integrated into the conventional dynamic bicycle model for accurate AV behavior representation. Temporal features derived from this behavior model are analyzed using machine learning to detect normal and abnormal navigation behavior. The performance of the GPS-IDS framework is evaluated on the AV-GPS-Dataset - a real-world dataset collected by the team using an AV testbed. The dataset has been publicly released for the global research community. To the best of our knowledge, this dataset is the first of its kind and will serve as a useful resource to address such security challenges.","sentences":["Autonomous Vehicles (AVs) heavily rely on sensors and communication networks like Global Positioning System (GPS) to navigate autonomously.","Prior research has indicated that networks like GPS are vulnerable to cyber-attacks such as spoofing and jamming, thus posing serious risks like navigation errors and system failures.","These threats are expected to intensify with the widespread deployment of AVs, making it crucial to detect and mitigate such attacks.","This paper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly Behavior Analysis (ABA)-based intrusion detection framework to detect GPS spoofing attacks on AVs.","The framework uses a novel physics-based vehicle behavior model where a GPS navigation model is integrated into the conventional dynamic bicycle model for accurate AV behavior representation.","Temporal features derived from this behavior model are analyzed using machine learning to detect normal and abnormal navigation behavior.","The performance of the GPS-IDS framework is evaluated on the AV-GPS-Dataset - a real-world dataset collected by the team using an AV testbed.","The dataset has been publicly released for the global research community.","To the best of our knowledge, this dataset is the first of its kind and will serve as a useful resource to address such security challenges."],"url":"http://arxiv.org/abs/2405.08359v1","category":"cs.CR"}
{"created":"2024-05-14 06:31:38","title":"Abnormal Respiratory Sound Identification Using Audio-Spectrogram Vision Transformer","abstract":"Respiratory disease, the third leading cause of deaths globally, is considered a high-priority ailment requiring significant research on identification and treatment. Stethoscope-recorded lung sounds and artificial intelligence-powered devices have been used to identify lung disorders and aid specialists in making accurate diagnoses. In this study, audio-spectrogram vision transformer (AS-ViT), a new approach for identifying abnormal respiration sounds, was developed. The sounds of the lungs are converted into visual representations called spectrograms using a technique called short-time Fourier transform (STFT). These images are then analyzed using a model called vision transformer to identify different types of respiratory sounds. The classification was carried out using the ICBHI 2017 database, which includes various types of lung sounds with different frequencies, noise levels, and backgrounds. The proposed AS-ViT method was evaluated using three metrics and achieved 79.1% and 59.8% for 60:40 split ratio and 86.4% and 69.3% for 80:20 split ratio in terms of unweighted average recall and overall scores respectively for respiratory sound detection, surpassing previous state-of-the-art results.","sentences":["Respiratory disease, the third leading cause of deaths globally, is considered a high-priority ailment requiring significant research on identification and treatment.","Stethoscope-recorded lung sounds and artificial intelligence-powered devices have been used to identify lung disorders and aid specialists in making accurate diagnoses.","In this study, audio-spectrogram vision transformer (AS-ViT), a new approach for identifying abnormal respiration sounds, was developed.","The sounds of the lungs are converted into visual representations called spectrograms using a technique called short-time Fourier transform (STFT).","These images are then analyzed using a model called vision transformer to identify different types of respiratory sounds.","The classification was carried out using the ICBHI 2017 database, which includes various types of lung sounds with different frequencies, noise levels, and backgrounds.","The proposed AS-ViT method was evaluated using three metrics and achieved 79.1% and 59.8% for 60:40 split ratio and 86.4% and 69.3% for 80:20 split ratio in terms of unweighted average recall and overall scores respectively for respiratory sound detection, surpassing previous state-of-the-art results."],"url":"http://arxiv.org/abs/2405.08342v1","category":"cs.SD"}
{"created":"2024-05-14 06:16:13","title":"Perivascular space Identification Nnunet for Generalised Usage (PINGU)","abstract":"Perivascular spaces(PVSs) form a central component of the brain\\'s waste clearance system, the glymphatic system. These structures are visible on MRI images, and their morphology is associated with aging and neurological disease. Manual quantification of PVS is time consuming and subjective. Numerous deep learning methods for PVS segmentation have been developed, however the majority have been developed and evaluated on homogenous datasets and high resolution scans, perhaps limiting their applicability for the wide range of image qualities acquired in clinic and research. In this work we train a nnUNet, a top-performing biomedical image segmentation algorithm, on a heterogenous training sample of manually segmented MRI images of a range of different qualities and resolutions from 6 different datasets. These are compared to publicly available deep learning methods for 3D segmentation of PVS. The resulting model, PINGU (Perivascular space Identification Nnunet for Generalised Usage), achieved voxel and cluster level dice scores of 0.50(SD=0.15), 0.63(0.17) in the white matter(WM), and 0.54(0.11), 0.66(0.17) in the basal ganglia(BG). Performance on data from unseen sites was substantially lower for both PINGU(0.20-0.38(WM, voxel), 0.29-0.58(WM, cluster), 0.22-0.36(BG, voxel), 0.46-0.60(BG, cluster)) and the publicly available algorithms(0.18-0.30(WM, voxel), 0.29-0.38(WM cluster), 0.10-0.20(BG, voxel), 0.15-0.37(BG, cluster)), but PINGU strongly outperformed the publicly available algorithms, particularly in the BG. Finally, training PINGU on manual segmentations from a single site with homogenous scan properties gave marginally lower performances on internal cross-validation, but in some cases gave higher performance on external validation. PINGU stands out as broad-use PVS segmentation tool, with particular strength in the BG, an area of PVS related to vascular disease and pathology.","sentences":["Perivascular spaces(PVSs) form a central component of the brain\\'s waste clearance system, the glymphatic system.","These structures are visible on MRI images, and their morphology is associated with aging and neurological disease.","Manual quantification of PVS is time consuming and subjective.","Numerous deep learning methods for PVS segmentation have been developed, however the majority have been developed and evaluated on homogenous datasets and high resolution scans, perhaps limiting their applicability for the wide range of image qualities acquired in clinic and research.","In this work we train a nnUNet, a top-performing biomedical image segmentation algorithm, on a heterogenous training sample of manually segmented MRI images of a range of different qualities and resolutions from 6 different datasets.","These are compared to publicly available deep learning methods for 3D segmentation of PVS.","The resulting model, PINGU (Perivascular space Identification Nnunet for Generalised Usage), achieved voxel and cluster level dice scores of 0.50(SD=0.15), 0.63(0.17) in the white matter(WM), and 0.54(0.11), 0.66(0.17) in the basal ganglia(BG).","Performance on data from unseen sites was substantially lower for both PINGU(0.20-0.38(WM, voxel), 0.29-0.58(WM, cluster), 0.22-0.36(BG, voxel), 0.46-0.60(BG, cluster)) and the publicly available algorithms(0.18-0.30(WM, voxel), 0.29-0.38(WM cluster), 0.10-0.20(BG, voxel), 0.15-0.37(BG, cluster)), but PINGU strongly outperformed the publicly available algorithms, particularly in the BG.","Finally, training PINGU on manual segmentations from a single site with homogenous scan properties gave marginally lower performances on internal cross-validation, but in some cases gave higher performance on external validation.","PINGU stands out as broad-use PVS segmentation tool, with particular strength in the BG, an area of PVS related to vascular disease and pathology."],"url":"http://arxiv.org/abs/2405.08337v1","category":"cs.CV"}
{"created":"2024-05-14 06:09:08","title":"Could Chemical LLMs benefit from Message Passing","abstract":"Pretrained language models (LMs) showcase significant capabilities in processing molecular text, while concurrently, message passing neural networks (MPNNs) demonstrate resilience and versatility in the domain of molecular science. Despite these advancements, we find there are limited studies investigating the bidirectional interactions between molecular structures and their corresponding textual representations. Therefore, in this paper, we propose two strategies to evaluate whether an information integration can enhance the performance: contrast learning, which involves utilizing an MPNN to supervise the training of the LM, and fusion, which exploits information from both models. Our empirical analysis reveals that the integration approaches exhibit superior performance compared to baselines when applied to smaller molecular graphs, while these integration approaches do not yield performance enhancements on large scale graphs.","sentences":["Pretrained language models (LMs) showcase significant capabilities in processing molecular text, while concurrently, message passing neural networks (MPNNs) demonstrate resilience and versatility in the domain of molecular science.","Despite these advancements, we find there are limited studies investigating the bidirectional interactions between molecular structures and their corresponding textual representations.","Therefore, in this paper, we propose two strategies to evaluate whether an information integration can enhance the performance: contrast learning, which involves utilizing an MPNN to supervise the training of the LM, and fusion, which exploits information from both models.","Our empirical analysis reveals that the integration approaches exhibit superior performance compared to baselines when applied to smaller molecular graphs, while these integration approaches do not yield performance enhancements on large scale graphs."],"url":"http://arxiv.org/abs/2405.08334v1","category":"cs.LG"}
{"created":"2024-05-14 05:52:01","title":"Cross-Dataset Generalization For Retinal Lesions Segmentation","abstract":"Identifying lesions in fundus images is an important milestone toward an automated and interpretable diagnosis of retinal diseases. To support research in this direction, multiple datasets have been released, proposing groundtruth maps for different lesions. However, important discrepancies exist between the annotations and raise the question of generalization across datasets. This study characterizes several known datasets and compares different techniques that have been proposed to enhance the generalisation performance of a model, such as stochastic weight averaging, model soups and ensembles. Our results provide insights into how to combine coarsely labelled data with a finely-grained dataset in order to improve the lesions segmentation.","sentences":["Identifying lesions in fundus images is an important milestone toward an automated and interpretable diagnosis of retinal diseases.","To support research in this direction, multiple datasets have been released, proposing groundtruth maps for different lesions.","However, important discrepancies exist between the annotations and raise the question of generalization across datasets.","This study characterizes several known datasets and compares different techniques that have been proposed to enhance the generalisation performance of a model, such as stochastic weight averaging, model soups and ensembles.","Our results provide insights into how to combine coarsely labelled data with a finely-grained dataset in order to improve the lesions segmentation."],"url":"http://arxiv.org/abs/2405.08329v1","category":"cs.CV"}
{"created":"2024-05-14 04:58:23","title":"No-Regret Learning of Nash Equilibrium for Black-Box Games via Gaussian Processes","abstract":"This paper investigates the challenge of learning in black-box games, where the underlying utility function is unknown to any of the agents. While there is an extensive body of literature on the theoretical analysis of algorithms for computing the Nash equilibrium with complete information about the game, studies on Nash equilibrium in black-box games are less common. In this paper, we focus on learning the Nash equilibrium when the only available information about an agent's payoff comes in the form of empirical queries. We provide a no-regret learning algorithm that utilizes Gaussian processes to identify the equilibrium in such games. Our approach not only ensures a theoretical convergence rate but also demonstrates effectiveness across a variety collection of games through experimental validation.","sentences":["This paper investigates the challenge of learning in black-box games, where the underlying utility function is unknown to any of the agents.","While there is an extensive body of literature on the theoretical analysis of algorithms for computing the Nash equilibrium with complete information about the game, studies on Nash equilibrium in black-box games are less common.","In this paper, we focus on learning the Nash equilibrium when the only available information about an agent's payoff comes in the form of empirical queries.","We provide a no-regret learning algorithm that utilizes Gaussian processes to identify the equilibrium in such games.","Our approach not only ensures a theoretical convergence rate but also demonstrates effectiveness across a variety collection of games through experimental validation."],"url":"http://arxiv.org/abs/2405.08318v1","category":"cs.LG"}
{"created":"2024-05-14 04:27:16","title":"A Decoupling and Aggregating Framework for Joint Extraction of Entities and Relations","abstract":"Named Entity Recognition and Relation Extraction are two crucial and challenging subtasks in the field of Information Extraction. Despite the successes achieved by the traditional approaches, fundamental research questions remain open. First, most recent studies use parameter sharing for a single subtask or shared features for both two subtasks, ignoring their semantic differences. Second, information interaction mainly focuses on the two subtasks, leaving the fine-grained informtion interaction among the subtask-specific features of encoding subjects, relations, and objects unexplored. Motivated by the aforementioned limitations, we propose a novel model to jointly extract entities and relations. The main novelties are as follows: (1) We propose to decouple the feature encoding process into three parts, namely encoding subjects, encoding objects, and encoding relations. Thanks to this, we are able to use fine-grained subtask-specific features. (2) We propose novel inter-aggregation and intra-aggregation strategies to enhance the information interaction and construct individual fine-grained subtask-specific features, respectively. The experimental results demonstrate that our model outperforms several previous state-of-the-art models. Extensive additional experiments further confirm the effectiveness of our model.","sentences":["Named Entity Recognition and Relation Extraction are two crucial and challenging subtasks in the field of Information Extraction.","Despite the successes achieved by the traditional approaches, fundamental research questions remain open.","First, most recent studies use parameter sharing for a single subtask or shared features for both two subtasks, ignoring their semantic differences.","Second, information interaction mainly focuses on the two subtasks, leaving the fine-grained informtion interaction among the subtask-specific features of encoding subjects, relations, and objects unexplored.","Motivated by the aforementioned limitations, we propose a novel model to jointly extract entities and relations.","The main novelties are as follows: (1) We propose to decouple the feature encoding process into three parts, namely encoding subjects, encoding objects, and encoding relations.","Thanks to this, we are able to use fine-grained subtask-specific features.","(2) We propose novel inter-aggregation and intra-aggregation strategies to enhance the information interaction and construct individual fine-grained subtask-specific features, respectively.","The experimental results demonstrate that our model outperforms several previous state-of-the-art models.","Extensive additional experiments further confirm the effectiveness of our model."],"url":"http://arxiv.org/abs/2405.08311v1","category":"cs.CL"}
{"created":"2024-05-14 04:23:44","title":"Cross-Category Functional Grasp Tansfer","abstract":"The grasp generation of dexterous hand often requires a large number of grasping annotations. Especially for functional grasp-requiring the grasp pose to be convenient for the subsequent use of the object. However, annotating high DoF dexterous hand pose is rather challenging. This prompt us to explore how people achieve manipulations on new objects based on past grasp experiences. We find that people are adept at discovering and leveraging various similarities between objects when grasping new items, including shape, layout, and grasp type. In light of this, we analyze and collect grasp-related similarity relationships among 51 common tool-like object categories and annotate semantic grasp representation for 1768 objects. These data are organized into the form of a knowledge graph, which helps infer our proposed cross-category functional grasp synthesis. Through extensive experiments, we demonstrate that the grasp-related knowledge indeed contributed to achieving functional grasp transfer across unknown or entirely new categories of objects. We will publicly release the dataset and code to facilitate future research.","sentences":["The grasp generation of dexterous hand often requires a large number of grasping annotations.","Especially for functional grasp-requiring the grasp pose to be convenient for the subsequent use of the object.","However, annotating high DoF dexterous hand pose is rather challenging.","This prompt us to explore how people achieve manipulations on new objects based on past grasp experiences.","We find that people are adept at discovering and leveraging various similarities between objects when grasping new items, including shape, layout, and grasp type.","In light of this, we analyze and collect grasp-related similarity relationships among 51 common tool-like object categories and annotate semantic grasp representation for 1768 objects.","These data are organized into the form of a knowledge graph, which helps infer our proposed cross-category functional grasp synthesis.","Through extensive experiments, we demonstrate that the grasp-related knowledge indeed contributed to achieving functional grasp transfer across unknown or entirely new categories of objects.","We will publicly release the dataset and code to facilitate future research."],"url":"http://arxiv.org/abs/2405.08310v1","category":"cs.RO"}
{"created":"2024-05-14 03:58:19","title":"Computational Thought Experiments for a More Rigorous Philosophy and Science of the Mind","abstract":"We offer philosophical motivations for a method we call Virtual World Cognitive Science (VW CogSci), in which researchers use virtual embodied agents that are embedded in virtual worlds to explore questions in the field of Cognitive Science. We focus on questions about mental and linguistic representation and the ways that such computational modeling can add rigor to philosophical thought experiments, as well as the terminology used in the scientific study of such representations. We find that this method forces researchers to take a god's-eye view when describing dynamical relationships between entities in minds and entities in an environment in a way that eliminates the need for problematic talk of belief and concept types, such as the belief that cats are silly, and the concept CAT, while preserving belief and concept tokens in individual cognizers' minds. We conclude with some further key advantages of VW CogSci for the scientific study of mental and linguistic representation and for Cognitive Science more broadly.","sentences":["We offer philosophical motivations for a method we call Virtual World Cognitive Science (VW CogSci), in which researchers use virtual embodied agents that are embedded in virtual worlds to explore questions in the field of Cognitive Science.","We focus on questions about mental and linguistic representation and the ways that such computational modeling can add rigor to philosophical thought experiments, as well as the terminology used in the scientific study of such representations.","We find that this method forces researchers to take a god's-eye view when describing dynamical relationships between entities in minds and entities in an environment in a way that eliminates the need for problematic talk of belief and concept types, such as the belief that cats are silly, and the concept CAT, while preserving belief and concept tokens in individual cognizers' minds.","We conclude with some further key advantages of VW CogSci for the scientific study of mental and linguistic representation and for Cognitive Science more broadly."],"url":"http://arxiv.org/abs/2405.08304v2","category":"cs.CL"}
{"created":"2024-05-14 03:53:52","title":"Designing Adaptive User Interfaces for mHealth applications targeting chronic disease: A User-Centric Approach","abstract":"mHealth interventions show significant potential to help in the self-management of chronic diseases, but their under use remains a problem. Considering the substantial diversity among individuals dealing with chronic diseases, tailored strategies are essential. \\emph{Adaptive User Interfaces} (AUIs) may help address the diverse and evolving needs of this demographic. To investigate this approach, we developed an AUI prototype informed by existing literature findings. We then used this prototype as the basis for focus group discussions and interview studies with 22 participants managing various chronic diseases, and follow-up surveys of all participants. Through these investigations, we pinpointed key challenges related to the use of AUIs, strategies to improve adaptation design, and potential trade-offs between these challenges and strategies. Concurrently, a quantitative survey was conducted to extract preferences for AUIs in chronic disease-related applications with 90 further participants. This uncovered participants' preferences for various adaptations, data types, collection methods, and involvement levels. Finally, we synthesised these insights and categories, aligning them with existing guidelines and design considerations for mHealth app adaptation design. This resulted in nine guidelines that we refined by a final feedback survey conducted with 20 participants.","sentences":["mHealth interventions show significant potential to help in the self-management of chronic diseases, but their under use remains a problem.","Considering the substantial diversity among individuals dealing with chronic diseases, tailored strategies are essential.","\\emph{Adaptive User Interfaces} (AUIs) may help address the diverse and evolving needs of this demographic.","To investigate this approach, we developed an AUI prototype informed by existing literature findings.","We then used this prototype as the basis for focus group discussions and interview studies with 22 participants managing various chronic diseases, and follow-up surveys of all participants.","Through these investigations, we pinpointed key challenges related to the use of AUIs, strategies to improve adaptation design, and potential trade-offs between these challenges and strategies.","Concurrently, a quantitative survey was conducted to extract preferences for AUIs in chronic disease-related applications with 90 further participants.","This uncovered participants' preferences for various adaptations, data types, collection methods, and involvement levels.","Finally, we synthesised these insights and categories, aligning them with existing guidelines and design considerations for mHealth app adaptation design.","This resulted in nine guidelines that we refined by a final feedback survey conducted with 20 participants."],"url":"http://arxiv.org/abs/2405.08302v1","category":"cs.HC"}
{"created":"2024-05-14 03:42:33","title":"Distance-Restricted Explanations: Theoretical Underpinnings & Efficient Implementation","abstract":"The uses of machine learning (ML) have snowballed in recent years. In many cases, ML models are highly complex, and their operation is beyond the understanding of human decision-makers. Nevertheless, some uses of ML models involve high-stakes and safety-critical applications. Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding the operation of such complex ML models, thus eliciting trust in their operation. Unfortunately, the majority of past XAI work is based on informal approaches, that offer no guarantees of rigor. Unsurprisingly, there exists comprehensive experimental and theoretical evidence confirming that informal methods of XAI can provide human-decision makers with erroneous information. Logic-based XAI represents a rigorous approach to explainability; it is model-based and offers the strongest guarantees of rigor of computed explanations. However, a well-known drawback of logic-based XAI is the complexity of logic reasoning, especially for highly complex ML models. Recent work proposed distance-restricted explanations, i.e. explanations that are rigorous provided the distance to a given input is small enough. Distance-restricted explainability is tightly related with adversarial robustness, and it has been shown to scale for moderately complex ML models, but the number of inputs still represents a key limiting factor. This paper investigates novel algorithms for scaling up the performance of logic-based explainers when computing and enumerating ML model explanations with a large number of inputs.","sentences":["The uses of machine learning (ML) have snowballed in recent years.","In many cases, ML models are highly complex, and their operation is beyond the understanding of human decision-makers.","Nevertheless, some uses of ML models involve high-stakes and safety-critical applications.","Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding the operation of such complex ML models, thus eliciting trust in their operation.","Unfortunately, the majority of past XAI work is based on informal approaches, that offer no guarantees of rigor.","Unsurprisingly, there exists comprehensive experimental and theoretical evidence confirming that informal methods of XAI can provide human-decision makers with erroneous information.","Logic-based XAI represents a rigorous approach to explainability; it is model-based and offers the strongest guarantees of rigor of computed explanations.","However, a well-known drawback of logic-based XAI is the complexity of logic reasoning, especially for highly complex ML models.","Recent work proposed distance-restricted explanations, i.e. explanations that are rigorous provided the distance to a given input is small enough.","Distance-restricted explainability is tightly related with adversarial robustness, and it has been shown to scale for moderately complex ML models, but the number of inputs still represents a key limiting factor.","This paper investigates novel algorithms for scaling up the performance of logic-based explainers when computing and enumerating ML model explanations with a large number of inputs."],"url":"http://arxiv.org/abs/2405.08297v1","category":"cs.LG"}
{"created":"2024-05-14 02:34:56","title":"Automatic Segmentation of the Kidneys and Cystic Renal Lesions on Non-Contrast CT Using a Convolutional Neural Network","abstract":"Objective: Automated segmentation tools are useful for calculating kidney volumes rapidly and accurately. Furthermore, these tools have the power to facilitate large-scale image-based artificial intelligence projects by generating input labels, such as for image registration algorithms. Prior automated segmentation models have largely ignored non-contrast computed tomography (CT) imaging. This work aims to implement and train a deep learning (DL) model to segment the kidneys and cystic renal lesions (CRLs) from non-contrast CT scans.   Methods: Manual segmentation of the kidneys and CRLs was performed on 150 non-contrast abdominal CT scans. The data were divided into an 80/20 train/test split and a deep learning (DL) model was trained to segment the kidneys and CRLs. Various scoring metrics were used to assess model performance, including the Dice Similarity Coefficient (DSC), Jaccard Index (JI), and absolute and percent error kidney volume and lesion volume. Bland-Altman (B-A) analysis was performed to compare manual versus DL-based kidney volumes.   Results: The DL model achieved a median kidney DSC of 0.934, median CRL DSC of 0.711, and total median study DSC of 0.823. Average volume errors were 0.9% for renal parenchyma, 37.0% for CRLs, and 2.2% overall. B-A analysis demonstrated that DL-based volumes tended to be greater than manual volumes, with a mean bias of +3.0 ml (+/- 2 SD of +/- 50.2 ml).   Conclusion: A deep learning model trained to segment kidneys and cystic renal lesions on non-contrast CT examinations was able to provide highly accurate segmentations, with a median kidney Dice Similarity Coefficient of 0.934.   Keywords: deep learning; kidney segmentation; artificial intelligence; convolutional neural networks.","sentences":["Objective: Automated segmentation tools are useful for calculating kidney volumes rapidly and accurately.","Furthermore, these tools have the power to facilitate large-scale image-based artificial intelligence projects by generating input labels, such as for image registration algorithms.","Prior automated segmentation models have largely ignored non-contrast computed tomography (CT) imaging.","This work aims to implement and train a deep learning (DL) model to segment the kidneys and cystic renal lesions (CRLs) from non-contrast CT scans.   ","Methods: Manual segmentation of the kidneys and CRLs was performed on 150 non-contrast abdominal CT scans.","The data were divided into an 80/20 train/test split and a deep learning (DL) model was trained to segment the kidneys and CRLs.","Various scoring metrics were used to assess model performance, including the Dice Similarity Coefficient (DSC), Jaccard Index (JI), and absolute and percent error kidney volume and lesion volume.","Bland-Altman (B-A) analysis was performed to compare manual versus DL-based kidney volumes.   ","Results:","The DL model achieved a median kidney DSC of 0.934, median CRL DSC of 0.711, and total median study DSC of 0.823.","Average volume errors were 0.9% for renal parenchyma, 37.0% for CRLs, and 2.2% overall.","B-A analysis demonstrated that DL-based volumes tended to be greater than manual volumes, with a mean bias of +3.0 ml (+/- 2 SD of +/- 50.2 ml).   ","Conclusion: A deep learning model trained to segment kidneys and cystic renal lesions on non-contrast CT examinations was able to provide highly accurate segmentations, with a median kidney Dice Similarity Coefficient of 0.934.   ","Keywords: deep learning; kidney segmentation; artificial intelligence; convolutional neural networks."],"url":"http://arxiv.org/abs/2405.08282v1","category":"eess.IV"}
{"created":"2024-05-14 02:05:36","title":"VS-Assistant: Versatile Surgery Assistant on the Demand of Surgeons","abstract":"The surgical intervention is crucial to patient healthcare, and many studies have developed advanced algorithms to provide understanding and decision-making assistance for surgeons. Despite great progress, these algorithms are developed for a single specific task and scenario, and in practice require the manual combination of different functions, thus limiting the applicability. Thus, an intelligent and versatile surgical assistant is expected to accurately understand the surgeon's intentions and accordingly conduct the specific tasks to support the surgical process. In this work, by leveraging advanced multimodal large language models (MLLMs), we propose a Versatile Surgery Assistant (VS-Assistant) that can accurately understand the surgeon's intention and complete a series of surgical understanding tasks, e.g., surgical scene analysis, surgical instrument detection, and segmentation on demand. Specifically, to achieve superior surgical multimodal understanding, we devise a mixture of projectors (MOP) module to align the surgical MLLM in VS-Assistant to balance the natural and surgical knowledge. Moreover, we devise a surgical Function-Calling Tuning strategy to enable the VS-Assistant to understand surgical intentions, and thus make a series of surgical function calls on demand to meet the needs of the surgeons. Extensive experiments on neurosurgery data confirm that our VS-Assistant can understand the surgeon's intention more accurately than the existing MLLM, resulting in overwhelming performance in textual analysis and visual tasks. Source code and models will be made public.","sentences":["The surgical intervention is crucial to patient healthcare, and many studies have developed advanced algorithms to provide understanding and decision-making assistance for surgeons.","Despite great progress, these algorithms are developed for a single specific task and scenario, and in practice require the manual combination of different functions, thus limiting the applicability.","Thus, an intelligent and versatile surgical assistant is expected to accurately understand the surgeon's intentions and accordingly conduct the specific tasks to support the surgical process.","In this work, by leveraging advanced multimodal large language models (MLLMs), we propose a Versatile Surgery Assistant (VS-Assistant) that can accurately understand the surgeon's intention and complete a series of surgical understanding tasks, e.g., surgical scene analysis, surgical instrument detection, and segmentation on demand.","Specifically, to achieve superior surgical multimodal understanding, we devise a mixture of projectors (MOP) module to align the surgical MLLM in VS-Assistant to balance the natural and surgical knowledge.","Moreover, we devise a surgical Function-Calling Tuning strategy to enable the VS-Assistant to understand surgical intentions, and thus make a series of surgical function calls on demand to meet the needs of the surgeons.","Extensive experiments on neurosurgery data confirm that our VS-Assistant can understand the surgeon's intention more accurately than the existing MLLM, resulting in overwhelming performance in textual analysis and visual tasks.","Source code and models will be made public."],"url":"http://arxiv.org/abs/2405.08272v1","category":"cs.CV"}
{"created":"2024-05-14 00:57:02","title":"Smart Sampling: Self-Attention and Bootstrapping for Improved Ensembled Q-Learning","abstract":"We present a novel method aimed at enhancing the sample efficiency of ensemble Q learning. Our proposed approach integrates multi-head self-attention into the ensembled Q networks while bootstrapping the state-action pairs ingested by the ensemble. This not only results in performance improvements over the original REDQ (Chen et al. 2021) and its variant DroQ (Hi-raoka et al. 2022), thereby enhancing Q predictions, but also effectively reduces both the average normalized bias and standard deviation of normalized bias within Q-function ensembles. Importantly, our method also performs well even in scenarios with a low update-to-data (UTD) ratio. Notably, the implementation of our proposed method is straightforward, requiring minimal modifications to the base model.","sentences":["We present a novel method aimed at enhancing the sample efficiency of ensemble Q learning.","Our proposed approach integrates multi-head self-attention into the ensembled Q networks while bootstrapping the state-action pairs ingested by the ensemble.","This not only results in performance improvements over the original REDQ (Chen et al. 2021) and its variant DroQ (Hi-raoka et al. 2022), thereby enhancing Q predictions, but also effectively reduces both the average normalized bias and standard deviation of normalized bias within Q-function ensembles.","Importantly, our method also performs well even in scenarios with a low update-to-data (UTD) ratio.","Notably, the implementation of our proposed method is straightforward, requiring minimal modifications to the base model."],"url":"http://arxiv.org/abs/2405.08252v1","category":"cs.LG"}
{"created":"2024-05-14 00:41:16","title":"QLingNet: An efficient and flexible modeling framework for subsonic airfoils","abstract":"Artificial intelligence techniques are considered an effective means to accelerate flow field simulations. However, current deep learning methods struggle to achieve generalization to flow field resolutions while ensuring computational efficiency. This paper presents a deep learning approach for rapid prediction of two types of subsonic flow fields with different resolutions. Unlike convolutional neural networks, the constructed feature extractor integrates features of different spatial scales along the channel dimension, reducing the sensitivity of the deep learning model to resolution while improving computational efficiency. Additionally, to ensure consistency between the input and output resolutions of the deep learning model, a memory pooling strategy is proposed, which ensures accurate reconstruction of flow fields at any resolution. By conducting extensive qualitative and quantitative analyses on a given test dataset, it is demonstrated that the proposed deep learning model can achieve a three-order-of-magnitude speedup compared to CPU-based solvers while adapting to flow fields of arbitrary resolutions. Moreover, the prediction accuracy for pressure exceeds 99\\%, laying the foundation for the development of large-scale models in the field of aerodynamics.","sentences":["Artificial intelligence techniques are considered an effective means to accelerate flow field simulations.","However, current deep learning methods struggle to achieve generalization to flow field resolutions while ensuring computational efficiency.","This paper presents a deep learning approach for rapid prediction of two types of subsonic flow fields with different resolutions.","Unlike convolutional neural networks, the constructed feature extractor integrates features of different spatial scales along the channel dimension, reducing the sensitivity of the deep learning model to resolution while improving computational efficiency.","Additionally, to ensure consistency between the input and output resolutions of the deep learning model, a memory pooling strategy is proposed, which ensures accurate reconstruction of flow fields at any resolution.","By conducting extensive qualitative and quantitative analyses on a given test dataset, it is demonstrated that the proposed deep learning model can achieve a three-order-of-magnitude speedup compared to CPU-based solvers while adapting to flow fields of arbitrary resolutions.","Moreover, the prediction accuracy for pressure exceeds 99\\%, laying the foundation for the development of large-scale models in the field of aerodynamics."],"url":"http://arxiv.org/abs/2405.08248v1","category":"physics.flu-dyn"}
{"created":"2024-05-14 00:39:21","title":"Automated classification of multi-parametric body MRI series","abstract":"Multi-parametric MRI (mpMRI) studies are widely available in clinical practice for the diagnosis of various diseases. As the volume of mpMRI exams increases yearly, there are concomitant inaccuracies that exist within the DICOM header fields of these exams. This precludes the use of the header information for the arrangement of the different series as part of the radiologist's hanging protocol, and clinician oversight is needed for correction. In this pilot work, we propose an automated framework to classify the type of 8 different series in mpMRI studies. We used 1,363 studies acquired by three Siemens scanners to train a DenseNet-121 model with 5-fold cross-validation. Then, we evaluated the performance of the DenseNet-121 ensemble on a held-out test set of 313 mpMRI studies. Our method achieved an average precision of 96.6%, sensitivity of 96.6%, specificity of 99.6%, and F1 score of 96.6% for the MRI series classification task. To the best of our knowledge, we are the first to develop a method to classify the series type in mpMRI studies acquired at the level of the chest, abdomen, and pelvis. Our method has the capability for robust automation of hanging protocols in modern radiology practice.","sentences":["Multi-parametric MRI (mpMRI) studies are widely available in clinical practice for the diagnosis of various diseases.","As the volume of mpMRI exams increases yearly, there are concomitant inaccuracies that exist within the DICOM header fields of these exams.","This precludes the use of the header information for the arrangement of the different series as part of the radiologist's hanging protocol, and clinician oversight is needed for correction.","In this pilot work, we propose an automated framework to classify the type of 8 different series in mpMRI studies.","We used 1,363 studies acquired by three Siemens scanners to train a DenseNet-121 model with 5-fold cross-validation.","Then, we evaluated the performance of the DenseNet-121 ensemble on a held-out test set of 313 mpMRI studies.","Our method achieved an average precision of 96.6%, sensitivity of 96.6%, specificity of 99.6%, and F1 score of 96.6% for the MRI series classification task.","To the best of our knowledge, we are the first to develop a method to classify the series type in mpMRI studies acquired at the level of the chest, abdomen, and pelvis.","Our method has the capability for robust automation of hanging protocols in modern radiology practice."],"url":"http://arxiv.org/abs/2405.08247v1","category":"eess.IV"}
{"created":"2024-05-14 00:22:06","title":"Compositional Text-to-Image Generation with Dense Blob Representations","abstract":"Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding inputs for better controllability. In this work, we propose to decompose a scene into visual primitives - denoted as dense blob representations - that contain fine-grained details of the scene while being modular, human-interpretable, and easy-to-construct. Based on blob representations, we develop a blob-grounded text-to-image diffusion model, termed BlobGEN, for compositional generation. Particularly, we introduce a new masked cross-attention module to disentangle the fusion between blob representations and visual features. To leverage the compositionality of large language models (LLMs), we introduce a new in-context learning approach to generate blob representations from text prompts. Our extensive experiments show that BlobGEN achieves superior zero-shot generation quality and better layout-guided controllability on MS-COCO. When augmented by LLMs, our method exhibits superior numerical and spatial correctness on compositional image generation benchmarks. Project page: https://blobgen-2d.github.io.","sentences":["Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding inputs for better controllability.","In this work, we propose to decompose a scene into visual primitives - denoted as dense blob representations - that contain fine-grained details of the scene while being modular, human-interpretable, and easy-to-construct.","Based on blob representations, we develop a blob-grounded text-to-image diffusion model, termed BlobGEN, for compositional generation.","Particularly, we introduce a new masked cross-attention module to disentangle the fusion between blob representations and visual features.","To leverage the compositionality of large language models (LLMs), we introduce a new in-context learning approach to generate blob representations from text prompts.","Our extensive experiments show that BlobGEN achieves superior zero-shot generation quality and better layout-guided controllability on MS-COCO.","When augmented by LLMs, our method exhibits superior numerical and spatial correctness on compositional image generation benchmarks.","Project page: https://blobgen-2d.github.io."],"url":"http://arxiv.org/abs/2405.08246v1","category":"cs.CV"}
{"created":"2024-05-14 00:20:32","title":"Progressive enhancement and restoration for mural images under low-light and defected conditions based on multi-receptive field strategy","abstract":"Ancient murals are valuable cultural heritage with great archaeological value. They provide insights into ancient religions, ceremonies, folklore, among other things through their content. However, due to long-term oxidation and inadequate protection, ancient murals have suffered continuous damage, including peeling and mold etc. Additionally, since ancient murals were typically painted indoors, the light intensity in images captured by digital devices is often low. The poor visibility hampers the further restoration of damaged areas. To address the escalating damage to ancient frescoes and facilitate batch restoration at archaeological sites, we propose a two-stage restoration model which called MER(Mural Enhancement and Restoration net) for ancient murals that are damaged and have been captured in low light. Our two-stage model not only enhances the visual quality of restored images but also achieves commendable results in relevant metric evaluations compared with other competitors. Furthermore, we have launched a website dedicated to the restoration of ancient mural paintings, utilizing the proposed model. Code is available at https://gitee.com/bbfan2024/MER.git.","sentences":["Ancient murals are valuable cultural heritage with great archaeological value.","They provide insights into ancient religions, ceremonies, folklore, among other things through their content.","However, due to long-term oxidation and inadequate protection, ancient murals have suffered continuous damage, including peeling and mold etc.","Additionally, since ancient murals were typically painted indoors, the light intensity in images captured by digital devices is often low.","The poor visibility hampers the further restoration of damaged areas.","To address the escalating damage to ancient frescoes and facilitate batch restoration at archaeological sites, we propose a two-stage restoration model which called MER(Mural Enhancement and Restoration net) for ancient murals that are damaged and have been captured in low light.","Our two-stage model not only enhances the visual quality of restored images but also achieves commendable results in relevant metric evaluations compared with other competitors.","Furthermore, we have launched a website dedicated to the restoration of ancient mural paintings, utilizing the proposed model.","Code is available at https://gitee.com/bbfan2024/MER.git."],"url":"http://arxiv.org/abs/2405.08245v1","category":"cs.CV"}
{"created":"2024-05-14 00:14:21","title":"Structural fluctuations in active glasses","abstract":"The glassy dynamics of dense active matter have recently become a topic of interest due to their importance in biological processes such as wound healing and tissue development. However, while the liquid-state properties of dense active matter have been studied in relation to the glass transition of active matter, the solid-state properties of active glasses have yet to be understood. In this work, we study the structural fluctuations in the active glasses composed of self-propelled particles. We develop a formalism to describe the solid-state properties of active glasses in the harmonic approximation limit and use it to analyze the displacement fields in the active glasses. Our findings reveal that the dynamics of high-frequency normal modes become quasi-static with respect to the active forces, and consequently, excitations of these modes are significantly suppressed. This leads to a violation of the equipartition law, suppression of particle displacements, and the apparent collective motion of active glasses. Overall, our results provide a fundamental understanding of the solid-state properties of active glasses.","sentences":["The glassy dynamics of dense active matter have recently become a topic of interest due to their importance in biological processes such as wound healing and tissue development.","However, while the liquid-state properties of dense active matter have been studied in relation to the glass transition of active matter, the solid-state properties of active glasses have yet to be understood.","In this work, we study the structural fluctuations in the active glasses composed of self-propelled particles.","We develop a formalism to describe the solid-state properties of active glasses in the harmonic approximation limit and use it to analyze the displacement fields in the active glasses.","Our findings reveal that the dynamics of high-frequency normal modes become quasi-static with respect to the active forces, and consequently, excitations of these modes are significantly suppressed.","This leads to a violation of the equipartition law, suppression of particle displacements, and the apparent collective motion of active glasses.","Overall, our results provide a fundamental understanding of the solid-state properties of active glasses."],"url":"http://arxiv.org/abs/2405.08243v1","category":"cond-mat.soft"}
{"created":"2024-05-14 00:05:58","title":"No Joke: An Embodied Conversational Agent Greeting Older Adults with Humour or a Smile Unrelated to Initial Acceptance","abstract":"Embodied conversation agents (ECAs) are increasingly being developed for older adults as assistants or companions. Older adults may not be familiar with ECAs, influencing uptake and acceptability. First impressions can correlate strongly with subsequent judgments, even of computer agents, and could influence acceptance. Using the circumplex model of affect, we developed three versions of an ECA -- laughing, smiling, and neutral in expression -- to evaluate how positive first impressions affect acceptance. Results from 249 older adults indicated no statistically significant effects except for general attitudes towards technology and intelligent agents. This questions the potential of laughter, jokes, puns, and smiles as a method of initial engagement for older adults.","sentences":["Embodied conversation agents (ECAs) are increasingly being developed for older adults as assistants or companions.","Older adults may not be familiar with ECAs, influencing uptake and acceptability.","First impressions can correlate strongly with subsequent judgments, even of computer agents, and could influence acceptance.","Using the circumplex model of affect, we developed three versions of an ECA -- laughing, smiling, and neutral in expression -- to evaluate how positive first impressions affect acceptance.","Results from 249 older adults indicated no statistically significant effects except for general attitudes towards technology and intelligent agents.","This questions the potential of laughter, jokes, puns, and smiles as a method of initial engagement for older adults."],"url":"http://arxiv.org/abs/2405.08242v1","category":"cs.HC"}
{"created":"2024-05-13 23:38:50","title":"Silver-Tongued and Sundry: Exploring Intersectional Pronouns with ChatGPT","abstract":"ChatGPT is a conversational agent built on a large language model. Trained on a significant portion of human output, ChatGPT can mimic people to a degree. As such, we need to consider what social identities ChatGPT simulates (or can be designed to simulate). In this study, we explored the case of identity simulation through Japanese first-person pronouns, which are tightly connected to social identities in intersectional ways, i.e., intersectional pronouns. We conducted a controlled online experiment where people from two regions in Japan (Kanto and Kinki) witnessed interactions with ChatGPT using ten sets of first-person pronouns. We discovered that pronouns alone can evoke perceptions of social identities in ChatGPT at the intersections of gender, age, region, and formality, with caveats. This work highlights the importance of pronoun use for social identity simulation, provides a language-based methodology for culturally-sensitive persona development, and advances the potential of intersectional identities in intelligent agents.","sentences":["ChatGPT is a conversational agent built on a large language model.","Trained on a significant portion of human output, ChatGPT can mimic people to a degree.","As such, we need to consider what social identities ChatGPT simulates (or can be designed to simulate).","In this study, we explored the case of identity simulation through Japanese first-person pronouns, which are tightly connected to social identities in intersectional ways, i.e., intersectional pronouns.","We conducted a controlled online experiment where people from two regions in Japan (Kanto and Kinki) witnessed interactions with ChatGPT using ten sets of first-person pronouns.","We discovered that pronouns alone can evoke perceptions of social identities in ChatGPT at the intersections of gender, age, region, and formality, with caveats.","This work highlights the importance of pronoun use for social identity simulation, provides a language-based methodology for culturally-sensitive persona development, and advances the potential of intersectional identities in intelligent agents."],"url":"http://arxiv.org/abs/2405.08238v1","category":"cs.HC"}
{"created":"2024-05-13 22:01:03","title":"Interpreting Latent Student Knowledge Representations in Programming Assignments","abstract":"Recent advances in artificial intelligence for education leverage generative large language models, including using them to predict open-ended student responses rather than their correctness only. However, the black-box nature of these models limits the interpretability of the learned student knowledge representations. In this paper, we conduct a first exploration into interpreting latent student knowledge representations by presenting InfoOIRT, an Information regularized Open-ended Item Response Theory model, which encourages the latent student knowledge states to be interpretable while being able to generate student-written code for open-ended programming questions. InfoOIRT maximizes the mutual information between a fixed subset of latent knowledge states enforced with simple prior distributions and generated student code, which encourages the model to learn disentangled representations of salient syntactic and semantic code features including syntactic styles, mastery of programming skills, and code structures. Through experiments on a real-world programming education dataset, we show that InfoOIRT can both accurately generate student code and lead to interpretable student knowledge representations.","sentences":["Recent advances in artificial intelligence for education leverage generative large language models, including using them to predict open-ended student responses rather than their correctness only.","However, the black-box nature of these models limits the interpretability of the learned student knowledge representations.","In this paper, we conduct a first exploration into interpreting latent student knowledge representations by presenting InfoOIRT, an Information regularized Open-ended Item Response Theory model, which encourages the latent student knowledge states to be interpretable while being able to generate student-written code for open-ended programming questions.","InfoOIRT maximizes the mutual information between a fixed subset of latent knowledge states enforced with simple prior distributions and generated student code, which encourages the model to learn disentangled representations of salient syntactic and semantic code features including syntactic styles, mastery of programming skills, and code structures.","Through experiments on a real-world programming education dataset, we show that InfoOIRT can both accurately generate student code and lead to interpretable student knowledge representations."],"url":"http://arxiv.org/abs/2405.08213v1","category":"cs.CL"}
{"created":"2024-05-13 21:47:35","title":"A Semantic and Motion-Aware Spatiotemporal Transformer Network for Action Detection","abstract":"This paper presents a novel spatiotemporal transformer network that introduces several original components to detect actions in untrimmed videos. First, the multi-feature selective semantic attention model calculates the correlations between spatial and motion features to model spatiotemporal interactions between different action semantics properly. Second, the motion-aware network encodes the locations of action semantics in video frames utilizing the motion-aware 2D positional encoding algorithm. Such a motion-aware mechanism memorizes the dynamic spatiotemporal variations in action frames that current methods cannot exploit. Third, the sequence-based temporal attention model captures the heterogeneous temporal dependencies in action frames. In contrast to standard temporal attention used in natural language processing, primarily aimed at finding similarities between linguistic words, the proposed sequence-based temporal attention is designed to determine both the differences and similarities between video frames that jointly define the meaning of actions. The proposed approach outperforms the state-of-the-art solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24, and EPIC-Kitchens.","sentences":["This paper presents a novel spatiotemporal transformer network that introduces several original components to detect actions in untrimmed videos.","First, the multi-feature selective semantic attention model calculates the correlations between spatial and motion features to model spatiotemporal interactions between different action semantics properly.","Second, the motion-aware network encodes the locations of action semantics in video frames utilizing the motion-aware 2D positional encoding algorithm.","Such a motion-aware mechanism memorizes the dynamic spatiotemporal variations in action frames that current methods cannot exploit.","Third, the sequence-based temporal attention model captures the heterogeneous temporal dependencies in action frames.","In contrast to standard temporal attention used in natural language processing, primarily aimed at finding similarities between linguistic words, the proposed sequence-based temporal attention is designed to determine both the differences and similarities between video frames that jointly define the meaning of actions.","The proposed approach outperforms the state-of-the-art solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24, and EPIC-Kitchens."],"url":"http://arxiv.org/abs/2405.08204v1","category":"cs.CV"}
{"created":"2024-05-13 21:30:50","title":"Modeling of Time-varying Wireless Communication Channel with Fading and Shadowing","abstract":"The real-time quantification of the effect of a wireless channel on the transmitting signal is crucial for the analysis and the intelligent design of wireless communication systems for various services. Recent mechanisms to model channel characteristics independent of coding, modulation, signal processing, etc., using deep learning neural networks are promising solutions. However, the current approaches are neither statistically accurate nor able to adapt to the changing environment. In this paper, we propose a new approach that combines a deep learning neural network with a mixture density network model to derive the conditional probability density function (PDF) of receiving power given a communication distance in general wireless communication systems. Furthermore, a deep transfer learning scheme is designed and implemented to allow the channel model to dynamically adapt to changes in communication environments. Extensive experiments on Nakagami fading channel model and Log-normal shadowing channel model with path loss and noise show that the new approach is more statistically accurate, faster, and more robust than the previous deep learning-based channel models.","sentences":["The real-time quantification of the effect of a wireless channel on the transmitting signal is crucial for the analysis and the intelligent design of wireless communication systems for various services.","Recent mechanisms to model channel characteristics independent of coding, modulation, signal processing, etc., using deep learning neural networks are promising solutions.","However, the current approaches are neither statistically accurate nor able to adapt to the changing environment.","In this paper, we propose a new approach that combines a deep learning neural network with a mixture density network model to derive the conditional probability density function (PDF) of receiving power given a communication distance in general wireless communication systems.","Furthermore, a deep transfer learning scheme is designed and implemented to allow the channel model to dynamically adapt to changes in communication environments.","Extensive experiments on Nakagami fading channel model and Log-normal shadowing channel model with path loss and noise show that the new approach is more statistically accurate, faster, and more robust than the previous deep learning-based channel models."],"url":"http://arxiv.org/abs/2405.08199v1","category":"cs.LG"}
{"created":"2024-05-13 21:02:31","title":"Towards Energy-Aware Federated Learning via MARL: A Dual-Selection Approach for Model and Client","abstract":"Although Federated Learning (FL) is promising in knowledge sharing for heterogeneous Artificial Intelligence of Thing (AIoT) devices, their training performance and energy efficacy are severely restricted in practical battery-driven scenarios due to the ``wooden barrel effect'' caused by the mismatch between homogeneous model paradigms and heterogeneous device capability. As a result, due to various kinds of differences among devices, it is hard for existing FL methods to conduct training effectively in energy-constrained scenarios, such as the battery constraints of devices. To tackle the above issues, we propose an energy-aware FL framework named DR-FL, which considers the energy constraints in both clients and heterogeneous deep learning models to enable energy-efficient FL. Unlike Vanilla FL, DR-FL adopts our proposed Muti-Agents Reinforcement Learning (MARL)-based dual-selection method, which allows participated devices to make contributions to the global model effectively and adaptively based on their computing capabilities and energy capacities in a MARL-based manner. Experiments on various well-known datasets show that DR-FL can not only maximise knowledge sharing among heterogeneous models under the energy constraint of large-scale AIoT systems but also improve the model performance of each involved heterogeneous device.","sentences":["Although Federated Learning (FL) is promising in knowledge sharing for heterogeneous Artificial Intelligence of Thing (AIoT) devices, their training performance and energy efficacy are severely restricted in practical battery-driven scenarios due to the ``wooden barrel effect'' caused by the mismatch between homogeneous model paradigms and heterogeneous device capability.","As a result, due to various kinds of differences among devices, it is hard for existing FL methods to conduct training effectively in energy-constrained scenarios, such as the battery constraints of devices.","To tackle the above issues, we propose an energy-aware FL framework named DR-FL, which considers the energy constraints in both clients and heterogeneous deep learning models to enable energy-efficient FL.","Unlike Vanilla FL, DR-FL adopts our proposed Muti-Agents Reinforcement Learning (MARL)-based dual-selection method, which allows participated devices to make contributions to the global model effectively and adaptively based on their computing capabilities and energy capacities in a MARL-based manner.","Experiments on various well-known datasets show that DR-FL can not only maximise knowledge sharing among heterogeneous models under the energy constraint of large-scale AIoT systems but also improve the model performance of each involved heterogeneous device."],"url":"http://arxiv.org/abs/2405.08183v1","category":"cs.LG"}
{"created":"2024-05-13 20:39:27","title":"Estimating Direct and Indirect Causal Effects of Spatiotemporal Interventions in Presence of Spatial Interference","abstract":"Spatial interference (SI) occurs when the treatment at one location affects the outcomes at other locations. Accounting for spatial interference in spatiotemporal settings poses further challenges as interference violates the stable unit treatment value assumption, making it infeasible for standard causal inference methods to quantify the effects of time-varying treatment at spatially varying outcomes. In this paper, we first formalize the concept of spatial interference in case of time-varying treatment assignments by extending the potential outcome framework under the assumption of no unmeasured confounding. We then propose our deep learning based potential outcome model for spatiotemporal causal inference. We utilize latent factor modeling to reduce the bias due to time-varying confounding while leveraging the power of U-Net architecture to capture global and local spatial interference in data over time. Our causal estimators are an extension of average treatment effect (ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial interference on treated and untreated data. Being the first of its kind deep learning based spatiotemporal causal inference technique, our approach shows advantages over several baseline methods based on the experiment results on two synthetic datasets, with and without spatial interference. Our results on real-world climate dataset also align with domain knowledge, further demonstrating the effectiveness of our proposed method.","sentences":["Spatial interference (SI) occurs when the treatment at one location affects the outcomes at other locations.","Accounting for spatial interference in spatiotemporal settings poses further challenges as interference violates the stable unit treatment value assumption, making it infeasible for standard causal inference methods to quantify the effects of time-varying treatment at spatially varying outcomes.","In this paper, we first formalize the concept of spatial interference in case of time-varying treatment assignments by extending the potential outcome framework under the assumption of no unmeasured confounding.","We then propose our deep learning based potential outcome model for spatiotemporal causal inference.","We utilize latent factor modeling to reduce the bias due to time-varying confounding while leveraging the power of U-Net architecture to capture global and local spatial interference in data over time.","Our causal estimators are an extension of average treatment effect (ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial interference on treated and untreated data.","Being the first of its kind deep learning based spatiotemporal causal inference technique, our approach shows advantages over several baseline methods based on the experiment results on two synthetic datasets, with and without spatial interference.","Our results on real-world climate dataset also align with domain knowledge, further demonstrating the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2405.08174v1","category":"cs.LG"}
{"created":"2024-05-13 20:37:04","title":"CANTONMT: Investigating Back-Translation and Model-Switch Mechanisms for Cantonese-English Neural Machine Translation","abstract":"This paper investigates the development and evaluation of machine translation models from Cantonese to English, where we propose a novel approach to tackle low-resource language translations. The main objectives of the study are to develop a model that can effectively translate Cantonese to English and evaluate it against state-of-the-art commercial models. To achieve this, a new parallel corpus has been created by combining different available corpora online with preprocessing and cleaning. In addition, a monolingual Cantonese dataset has been created through web scraping to aid the synthetic parallel corpus generation. Following the data collection process, several approaches, including fine-tuning models, back-translation, and model switch, have been used. The translation quality of models has been evaluated with multiple quality metrics, including lexicon-based metrics (SacreBLEU and hLEPOR) and embedding-space metrics (COMET and BERTscore). Based on the automatic metrics, the best model is selected and compared against the 2 best commercial translators using the human evaluation framework HOPES. The best model proposed in this investigation (NLLB-mBART) with model switch mechanisms has reached comparable and even better automatic evaluation scores against State-of-the-art commercial models (Bing and Baidu Translators), with a SacreBLEU score of 16.8 on our test set. Furthermore, an open-source web application has been developed to allow users to translate between Cantonese and English, with the different trained models available for effective comparisons between models from this investigation and users. CANTONMT is available at https://github.com/kenrickkung/CantoneseTranslation","sentences":["This paper investigates the development and evaluation of machine translation models from Cantonese to English, where we propose a novel approach to tackle low-resource language translations.","The main objectives of the study are to develop a model that can effectively translate Cantonese to English and evaluate it against state-of-the-art commercial models.","To achieve this, a new parallel corpus has been created by combining different available corpora online with preprocessing and cleaning.","In addition, a monolingual Cantonese dataset has been created through web scraping to aid the synthetic parallel corpus generation.","Following the data collection process, several approaches, including fine-tuning models, back-translation, and model switch, have been used.","The translation quality of models has been evaluated with multiple quality metrics, including lexicon-based metrics (SacreBLEU and hLEPOR) and embedding-space metrics (COMET and BERTscore).","Based on the automatic metrics, the best model is selected and compared against the 2 best commercial translators using the human evaluation framework HOPES.","The best model proposed in this investigation (NLLB-mBART) with model switch mechanisms has reached comparable and even better automatic evaluation scores against State-of-the-art commercial models (Bing and Baidu Translators), with a SacreBLEU score of 16.8 on our test set.","Furthermore, an open-source web application has been developed to allow users to translate between Cantonese and English, with the different trained models available for effective comparisons between models from this investigation and users.","CANTONMT is available at https://github.com/kenrickkung/CantoneseTranslation"],"url":"http://arxiv.org/abs/2405.08172v1","category":"cs.CL"}
{"created":"2024-05-13 20:26:10","title":"Rethinking Histology Slide Digitization Workflows for Low-Resource Settings","abstract":"Histology slide digitization is becoming essential for telepathology (remote consultation), knowledge sharing (education), and using the state-of-the-art artificial intelligence algorithms (augmented/automated end-to-end clinical workflows). However, the cumulative costs of digital multi-slide high-speed brightfield scanners, cloud/on-premises storage, and personnel (IT and technicians) make the current slide digitization workflows out-of-reach for limited-resource settings, further widening the health equity gap; even single-slide manual scanning commercial solutions are costly due to hardware requirements (high-resolution cameras, high-spec PC/workstation, and support for only high-end microscopes). In this work, we present a new cloud slide digitization workflow for creating scanner-quality whole-slide images (WSIs) from uploaded low-quality videos, acquired from cheap and inexpensive microscopes with built-in cameras. Specifically, we present a pipeline to create stitched WSIs while automatically deblurring out-of-focus regions, upsampling input 10X images to 40X resolution, and reducing brightness/contrast and light-source illumination variations. We demonstrate the WSI creation efficacy from our workflow on World Health Organization-declared neglected tropical disease, Cutaneous Leishmaniasis (prevalent only in the poorest regions of the world and only diagnosed by sub-specialist dermatopathologists, rare in poor countries), as well as other common pathologies on core biopsies of breast, liver, duodenum, stomach and lymph node. The code and pretrained models will be accessible via our GitHub (https://github.com/nadeemlab/DeepLIIF), and the cloud platform will be available at https://deepliif.org for uploading microscope videos and downloading/viewing WSIs with shareable links (no sign-in required) for telepathology and knowledge sharing.","sentences":["Histology slide digitization is becoming essential for telepathology (remote consultation), knowledge sharing (education), and using the state-of-the-art artificial intelligence algorithms (augmented/automated end-to-end clinical workflows).","However, the cumulative costs of digital multi-slide high-speed brightfield scanners, cloud/on-premises storage, and personnel (IT and technicians) make the current slide digitization workflows out-of-reach for limited-resource settings, further widening the health equity gap; even single-slide manual scanning commercial solutions are costly due to hardware requirements (high-resolution cameras, high-spec PC/workstation, and support for only high-end microscopes).","In this work, we present a new cloud slide digitization workflow for creating scanner-quality whole-slide images (WSIs) from uploaded low-quality videos, acquired from cheap and inexpensive microscopes with built-in cameras.","Specifically, we present a pipeline to create stitched WSIs while automatically deblurring out-of-focus regions, upsampling input 10X images to 40X resolution, and reducing brightness/contrast and light-source illumination variations.","We demonstrate the WSI creation efficacy from our workflow on World Health Organization-declared neglected tropical disease, Cutaneous Leishmaniasis (prevalent only in the poorest regions of the world and only diagnosed by sub-specialist dermatopathologists, rare in poor countries), as well as other common pathologies on core biopsies of breast, liver, duodenum, stomach and lymph node.","The code and pretrained models will be accessible via our GitHub (https://github.com/nadeemlab/DeepLIIF), and the cloud platform will be available at https://deepliif.org for uploading microscope videos and downloading/viewing WSIs with shareable links (no sign-in required) for telepathology and knowledge sharing."],"url":"http://arxiv.org/abs/2405.08169v1","category":"eess.IV"}
{"created":"2024-05-13 20:21:23","title":"Lithium, rotation and metallicity in the open cluster M35","abstract":"Lithium (Li) abundance is an age indicator for G, K, and M stellar types, as its abundance decreases over time for these spectral types. However, despite the observational efforts made over the past few decades, the role of rotation, activity, and metallicity in the depletion of Li is still unclear. We have investigated how Li depletion is affected by rotation and metallicity in G and K members of the roughly Pleiades-aged open cluster M35. To do so, we have collected a sample of 165 candidate members observed with the WIYN/Hydra spectrograph. In addition, we have taken advantage of three previous spectroscopic studies of Li in M35. As a result, we have collected a final sample of 396 stars which we have classified as members and non-members of the cluster. We have measured iron abundances, Li equivalent widths, and Li abundances for the 110 M35 members added to the existing sample by this study. Finally, rotation periods for cluster members have been obtained from the literature or derived from Zwicky Transient Facility light curves. As a result, we have confirmed that fast G and K rotators are Li-rich in comparison with slow rotators of similar effective temperature. Furthermore, while we derived subsolar metallicity for M35 from our spectra, the distribution of Li in this cluster is similar to those observed for the Pleiades and M34, which have solar metallicity and slightly different ages. In addition, we have shown that an empirical relationship proposed to remove the contribution of the Fe I line at 670.75 nm to the blended feature at 670.78 nm overestimates the contribution of this iron line for M35 members. We conclude that a 0.2-0.3 dex difference in metallicity makes little difference in the Li distributions of open clusters with ages between 100 and 250 Myr.","sentences":["Lithium (Li) abundance is an age indicator for G, K, and M stellar types, as its abundance decreases over time for these spectral types.","However, despite the observational efforts made over the past few decades, the role of rotation, activity, and metallicity in the depletion of Li is still unclear.","We have investigated how Li depletion is affected by rotation and metallicity in G and K members of the roughly Pleiades-aged open cluster M35.","To do so, we have collected a sample of 165 candidate members observed with the WIYN/Hydra spectrograph.","In addition, we have taken advantage of three previous spectroscopic studies of Li in M35.","As a result, we have collected a final sample of 396 stars which we have classified as members and non-members of the cluster.","We have measured iron abundances, Li equivalent widths, and Li abundances for the 110 M35 members added to the existing sample by this study.","Finally, rotation periods for cluster members have been obtained from the literature or derived from Zwicky Transient Facility light curves.","As a result, we have confirmed that fast G and K rotators are Li-rich in comparison with slow rotators of similar effective temperature.","Furthermore, while we derived subsolar metallicity for M35 from our spectra, the distribution of Li in this cluster is similar to those observed for the Pleiades and M34, which have solar metallicity and slightly different ages.","In addition, we have shown that an empirical relationship proposed to remove the contribution of the Fe I line at 670.75 nm to the blended feature at 670.78 nm overestimates the contribution of this iron line for M35 members.","We conclude that a 0.2-0.3 dex difference in metallicity makes little difference in the Li distributions of open clusters with ages between 100 and 250 Myr."],"url":"http://arxiv.org/abs/2405.08166v1","category":"astro-ph.SR"}
{"created":"2024-05-13 19:52:16","title":"LLM Theory of Mind and Alignment: Opportunities and Risks","abstract":"Large language models (LLMs) are transforming human-computer interaction and conceptions of artificial intelligence (AI) with their impressive capacities for conversing and reasoning in natural language. There is growing interest in whether LLMs have theory of mind (ToM); the ability to reason about the mental and emotional states of others that is core to human social intelligence. As LLMs are integrated into the fabric of our personal, professional and social lives and given greater agency to make decisions with real-world consequences, there is a critical need to understand how they can be aligned with human values. ToM seems to be a promising direction of inquiry in this regard. Following the literature on the role and impacts of human ToM, this paper identifies key areas in which LLM ToM will show up in human:LLM interactions at individual and group levels, and what opportunities and risks for alignment are raised in each. On the individual level, the paper considers how LLM ToM might manifest in goal specification, conversational adaptation, empathy and anthropomorphism. On the group level, it considers how LLM ToM might facilitate collective alignment, cooperation or competition, and moral judgement-making. The paper lays out a broad spectrum of potential implications and suggests the most pressing areas for future research.","sentences":["Large language models (LLMs) are transforming human-computer interaction and conceptions of artificial intelligence (AI) with their impressive capacities for conversing and reasoning in natural language.","There is growing interest in whether LLMs have theory of mind (ToM); the ability to reason about the mental and emotional states of others that is core to human social intelligence.","As LLMs are integrated into the fabric of our personal, professional and social lives and given greater agency to make decisions with real-world consequences, there is a critical need to understand how they can be aligned with human values.","ToM seems to be a promising direction of inquiry in this regard.","Following the literature on the role and impacts of human ToM, this paper identifies key areas in which LLM ToM will show up in human:LLM interactions at individual and group levels, and what opportunities and risks for alignment are raised in each.","On the individual level, the paper considers how LLM ToM might manifest in goal specification, conversational adaptation, empathy and anthropomorphism.","On the group level, it considers how LLM ToM might facilitate collective alignment, cooperation or competition, and moral judgement-making.","The paper lays out a broad spectrum of potential implications and suggests the most pressing areas for future research."],"url":"http://arxiv.org/abs/2405.08154v1","category":"cs.HC"}
{"created":"2024-05-13 19:16:28","title":"When factorization meets argumentation: towards argumentative explanations","abstract":"Factorization-based models have gained popularity since the Netflix challenge {(2007)}. Since that, various factorization-based models have been developed and these models have been proven to be efficient in predicting users' ratings towards items. A major concern is that explaining the recommendations generated by such methods is non-trivial because the explicit meaning of the latent factors they learn are not always clear. In response, we propose a novel model that combines factorization-based methods with argumentation frameworks (AFs). The integration of AFs provides clear meaning at each stage of the model, enabling it to produce easily understandable explanations for its recommendations. In this model, for every user-item interaction, an AF is defined in which the features of items are considered as arguments, and the users' ratings towards these features determine the strength and polarity of these arguments. This perspective allows our model to treat feature attribution as a structured argumentation procedure, where each calculation is marked with explicit meaning, enhancing its inherent interpretability. Additionally, our framework seamlessly incorporates side information, such as user contexts, leading to more accurate predictions. We anticipate at least three practical applications for our model: creating explanation templates, providing interactive explanations, and generating contrastive explanations. Through testing on real-world datasets, we have found that our model, along with its variants, not only surpasses existing argumentation-based methods but also competes effectively with current context-free and context-aware methods.","sentences":["Factorization-based models have gained popularity since the Netflix challenge {(2007)}.","Since that, various factorization-based models have been developed and these models have been proven to be efficient in predicting users' ratings towards items.","A major concern is that explaining the recommendations generated by such methods is non-trivial because the explicit meaning of the latent factors they learn are not always clear.","In response, we propose a novel model that combines factorization-based methods with argumentation frameworks (AFs).","The integration of AFs provides clear meaning at each stage of the model, enabling it to produce easily understandable explanations for its recommendations.","In this model, for every user-item interaction, an AF is defined in which the features of items are considered as arguments, and the users' ratings towards these features determine the strength and polarity of these arguments.","This perspective allows our model to treat feature attribution as a structured argumentation procedure, where each calculation is marked with explicit meaning, enhancing its inherent interpretability.","Additionally, our framework seamlessly incorporates side information, such as user contexts, leading to more accurate predictions.","We anticipate at least three practical applications for our model: creating explanation templates, providing interactive explanations, and generating contrastive explanations.","Through testing on real-world datasets, we have found that our model, along with its variants, not only surpasses existing argumentation-based methods but also competes effectively with current context-free and context-aware methods."],"url":"http://arxiv.org/abs/2405.08131v1","category":"cs.AI"}
{"created":"2024-05-13 19:09:50","title":"AI-Cybersecurity Education Through Designing AI-based Cyberharassment Detection Lab","abstract":"Cyberharassment is a critical, socially relevant cybersecurity problem because of the adverse effects it can have on targeted groups or individuals. While progress has been made in understanding cyber-harassment, its detection, attacks on artificial intelligence (AI) based cyberharassment systems, and the social problems in cyberharassment detectors, little has been done in designing experiential learning educational materials that engage students in this emerging social cybersecurity in the era of AI. Experiential learning opportunities are usually provided through capstone projects and engineering design courses in STEM programs such as computer science. While capstone projects are an excellent example of experiential learning, given the interdisciplinary nature of this emerging social cybersecurity problem, it can be challenging to use them to engage non-computing students without prior knowledge of AI. Because of this, we were motivated to develop a hands-on lab platform that provided experiential learning experiences to non-computing students with little or no background knowledge in AI and discussed the lessons learned in developing this lab. In this lab used by social science students at North Carolina A&T State University across two semesters (spring and fall) in 2022, students are given a detailed lab manual and are to complete a set of well-detailed tasks. Through this process, students learn AI concepts and the application of AI for cyberharassment detection. Using pre- and post-surveys, we asked students to rate their knowledge or skills in AI and their understanding of the concepts learned. The results revealed that the students moderately understood the concepts of AI and cyberharassment.","sentences":["Cyberharassment is a critical, socially relevant cybersecurity problem because of the adverse effects it can have on targeted groups or individuals.","While progress has been made in understanding cyber-harassment, its detection, attacks on artificial intelligence (AI) based cyberharassment systems, and the social problems in cyberharassment detectors, little has been done in designing experiential learning educational materials that engage students in this emerging social cybersecurity in the era of AI.","Experiential learning opportunities are usually provided through capstone projects and engineering design courses in STEM programs such as computer science.","While capstone projects are an excellent example of experiential learning, given the interdisciplinary nature of this emerging social cybersecurity problem, it can be challenging to use them to engage non-computing students without prior knowledge of AI.","Because of this, we were motivated to develop a hands-on lab platform that provided experiential learning experiences to non-computing students with little or no background knowledge in AI and discussed the lessons learned in developing this lab.","In this lab used by social science students at North Carolina A&T State University across two semesters (spring and fall) in 2022, students are given a detailed lab manual and are to complete a set of well-detailed tasks.","Through this process, students learn AI concepts and the application of AI for cyberharassment detection.","Using pre- and post-surveys, we asked students to rate their knowledge or skills in AI and their understanding of the concepts learned.","The results revealed that the students moderately understood the concepts of AI and cyberharassment."],"url":"http://arxiv.org/abs/2405.08125v1","category":"cs.CY"}
{"created":"2024-05-13 19:05:42","title":"From Questions to Insightful Answers: Building an Informed Chatbot for University Resources","abstract":"This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings.The objective of BARKPLUG V.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion. Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks. We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS). Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS). Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and experience, as validated by usability assessments.","sentences":["This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings.","The objective of BARKPLUG V.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion.","Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks.","We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS).","Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS).","Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and experience, as validated by usability assessments."],"url":"http://arxiv.org/abs/2405.08120v1","category":"cs.ET"}
{"created":"2024-05-13 19:01:08","title":"Secret Sharing with Certified Deletion","abstract":"Secret sharing allows a user to split a secret into many shares so that the secret can be recovered if, and only if, an authorized set of shares is collected. Although secret sharing typically does not require any computational hardness assumptions, its security does require that an adversary cannot collect an authorized set of shares. Over long periods of time where an adversary can benefit from multiple data breaches, this may become an unrealistic assumption.   We initiate the systematic study of secret sharing with certified deletion in order to achieve security even against an adversary that eventually collects an authorized set of shares. In secret sharing with certified deletion, a (classical) secret is split into quantum shares which can be verifiably destroyed. We define two natural notions of security: no-signaling security and adaptive security.   Next, we show how to construct (i) a secret sharing scheme with no-signaling certified deletion for any monotone access structure, and (ii) a threshold secret sharing scheme with adaptive certified deletion. Our first construction uses Bartusek and Khurana's (CRYPTO 2023) 2-out-of-2 secret sharing scheme with certified deletion as a building block, while our second construction is built from scratch and requires several new technical ideas. For example, we significantly generalize the ``XOR extractor'' of Agarwal, Bartusek, Khurana, and Kumar (EUROCRYPT 2023) in order to obtain high rate seedless extraction from certain quantum sources of entropy.","sentences":["Secret sharing allows a user to split a secret into many shares so that the secret can be recovered if, and only if, an authorized set of shares is collected.","Although secret sharing typically does not require any computational hardness assumptions, its security does require that an adversary cannot collect an authorized set of shares.","Over long periods of time where an adversary can benefit from multiple data breaches, this may become an unrealistic assumption.   ","We initiate the systematic study of secret sharing with certified deletion in order to achieve security even against an adversary that eventually collects an authorized set of shares.","In secret sharing with certified deletion, a (classical) secret is split into quantum shares which can be verifiably destroyed.","We define two natural notions of security: no-signaling security and adaptive security.   ","Next, we show how to construct (i) a secret sharing scheme with no-signaling certified deletion for any monotone access structure, and (ii) a threshold secret sharing scheme with adaptive certified deletion.","Our first construction uses Bartusek and Khurana's (CRYPTO 2023) 2-out-of-2 secret sharing scheme with certified deletion as a building block, while our second construction is built from scratch and requires several new technical ideas.","For example, we significantly generalize the ``XOR extractor'' of Agarwal, Bartusek, Khurana, and Kumar (EUROCRYPT 2023) in order to obtain high rate seedless extraction from certain quantum sources of entropy."],"url":"http://arxiv.org/abs/2405.08117v1","category":"cs.CR"}
{"created":"2024-05-13 18:22:02","title":"Semantic MIMO Systems for Speech-to-Text Transmission","abstract":"Semantic communications have been utilized to execute numerous intelligent tasks by transmitting task-related semantic information instead of bits. In this article, we propose a semantic-aware speech-to-text transmission system for the single-user multiple-input multiple-output (MIMO) and multi-user MIMO communication scenarios, named SAC-ST. Particularly, a semantic communication system to serve the speech-to-text task at the receiver is first designed, which compresses the semantic information and generates the low-dimensional semantic features by leveraging the transformer module. In addition, a novel semantic-aware network is proposed to facilitate the transmission with high semantic fidelity to identify the critical semantic information and guarantee it is recovered accurately. Furthermore, we extend the SAC-ST with a neural network-enabled channel estimation network to mitigate the dependence on accurate channel state information and validate the feasibility of SAC-ST in practical communication environments. Simulation results will show that the proposed SAC-ST outperforms the communication framework without the semantic-aware network for speech-to-text transmission over the MIMO channels in terms of the speech-to-text metrics, especially in the low signal-to-noise regime. Moreover, the SAC-ST with the developed channel estimation network is comparable to the SAC-ST with perfect channel state information.","sentences":["Semantic communications have been utilized to execute numerous intelligent tasks by transmitting task-related semantic information instead of bits.","In this article, we propose a semantic-aware speech-to-text transmission system for the single-user multiple-input multiple-output (MIMO) and multi-user MIMO communication scenarios, named SAC-ST.","Particularly, a semantic communication system to serve the speech-to-text task at the receiver is first designed, which compresses the semantic information and generates the low-dimensional semantic features by leveraging the transformer module.","In addition, a novel semantic-aware network is proposed to facilitate the transmission with high semantic fidelity to identify the critical semantic information and guarantee it is recovered accurately.","Furthermore, we extend the SAC-ST with a neural network-enabled channel estimation network to mitigate the dependence on accurate channel state information and validate the feasibility of SAC-ST in practical communication environments.","Simulation results will show that the proposed SAC-ST outperforms the communication framework without the semantic-aware network for speech-to-text transmission over the MIMO channels in terms of the speech-to-text metrics, especially in the low signal-to-noise regime.","Moreover, the SAC-ST with the developed channel estimation network is comparable to the SAC-ST with perfect channel state information."],"url":"http://arxiv.org/abs/2405.08096v1","category":"eess.AS"}
{"created":"2024-05-13 18:00:05","title":"Show Me the Way: Real-Time Tracking of Wireless Mobile Users with UWB-Enabled RIS","abstract":"The integration of Reconfigurable Intelligent Surfaces (RIS) in 6G wireless networks offers unprecedented control over communication environments. However, identifying optimal configurations within practical constraints remains a significant challenge. This becomes especially pronounced, when the user is mobile and the configurations need to be deployed in real time. Leveraging Ultra-Wideband (UWB) as localization technique, we capture and analyze real-time movements of a user within the RIS-enabled indoor environment. Given this information about the system's geometry, a model-based optimization is utilized, which enables real-time beam steering of the RIS towards the user. However, practical limitations of UWB modules lead to fluctuating UWB estimates, causing the RIS beam to occasionally miss the tracked user. The methodologies proposed in this work aim to increase the compatibility between these two systems. To this end, we provide two key solutions: beam splitting for obtaining more robust RIS configurations and UWB estimation correction for reducing the variations in the UWB data. Through comprehensive theoretical and experimental evaluations in both stationary and mobile scenarios, the effectiveness of the proposed techniques is demonstrated. When combined, the proposed methods improve worst-case tracking performance by a significant 17.5dB compared to the conventional approach.","sentences":["The integration of Reconfigurable Intelligent Surfaces (RIS) in 6G wireless networks offers unprecedented control over communication environments.","However, identifying optimal configurations within practical constraints remains a significant challenge.","This becomes especially pronounced, when the user is mobile and the configurations need to be deployed in real time.","Leveraging Ultra-Wideband (UWB) as localization technique, we capture and analyze real-time movements of a user within the RIS-enabled indoor environment.","Given this information about the system's geometry, a model-based optimization is utilized, which enables real-time beam steering of the RIS towards the user.","However, practical limitations of UWB modules lead to fluctuating UWB estimates, causing the RIS beam to occasionally miss the tracked user.","The methodologies proposed in this work aim to increase the compatibility between these two systems.","To this end, we provide two key solutions: beam splitting for obtaining more robust RIS configurations and UWB estimation correction for reducing the variations in the UWB data.","Through comprehensive theoretical and experimental evaluations in both stationary and mobile scenarios, the effectiveness of the proposed techniques is demonstrated.","When combined, the proposed methods improve worst-case tracking performance by a significant 17.5dB compared to the conventional approach."],"url":"http://arxiv.org/abs/2405.08076v1","category":"eess.SP"}
{"created":"2024-05-13 18:00:05","title":"Methods and stability tests associated with the sterile neutrino search using improved high-energy $\u03bd_\u03bc$ event reconstruction in IceCube","abstract":"We provide supporting details for the search for a 3+1 sterile neutrino using data collected over eleven years at the IceCube Neutrino Observatory. The analysis uses atmospheric muon-flavored neutrinos from 0.5 to 100\\, TeV that traverse the Earth to reach the IceCube detector, and finds a best-fit point at $\\sin^2(2\\theta_{24}) = 0.16$ and $\\Delta m^{2}_{41} = 3.5$ eV$^2$ with a goodness-of-fit p-value of 12\\% and consistency with the null hypothesis of no oscillations to sterile neutrinos with a p-value of 3.1\\%. Several improvements were made over past analyses, which are reviewed in this article, including upgrades to the reconstruction and the study of sources of systematic uncertainty. We provide details of the fit quality and discuss stability tests that split the data for separate samples, comparing results. We find that the fits are consistent between split data sets.","sentences":["We provide supporting details for the search for a 3+1 sterile neutrino using data collected over eleven years at the IceCube Neutrino Observatory.","The analysis uses atmospheric muon-flavored neutrinos from 0.5 to 100\\, TeV that traverse the Earth to reach the IceCube detector, and finds a best-fit point at $\\sin^2(2\\theta_{24})","= 0.16$ and $\\Delta m^{2}_{41} = 3.5$ eV$^2$ with a goodness-of-fit p-value of 12\\% and consistency with the null hypothesis of no oscillations to sterile neutrinos with a p-value of 3.1\\%.","Several improvements were made over past analyses, which are reviewed in this article, including upgrades to the reconstruction and the study of sources of systematic uncertainty.","We provide details of the fit quality and discuss stability tests that split the data for separate samples, comparing results.","We find that the fits are consistent between split data sets."],"url":"http://arxiv.org/abs/2405.08077v1","category":"hep-ex"}
{"created":"2024-05-14 18:14:05","title":"Quantum oscillations in the hole-doped cuprates and the confinement of spinons","abstract":"A long standing problem in the study of the under-hole-doped cuprates has been the description of the Fermi surfaces underlying the high magnetic field quantum oscillations. Harrison and Sebastian (arXiv:1103.4181) proposed that the higher temperature pseudogap 'Fermi arcs' are reconstructed into an electron pocket by field-induced charge density wave order. But computations on such a model (Zhang and Mei, arXiv:1411.2098) show an unobserved additional oscillation frequency from a Fermi surface arising from the backsides of the hole pockets completing the Fermi arcs. We describe a transition from a fractionalized Fermi liquid (FL*) model of the pseudogap metal, to a metal with bi-directional charge density wave order without fractionalization. We show that the confinement of the fermionic spinon excitations of the FL* across this transition can eliminate the unobserved oscillation frequency, and also account for the excess observed linear-in-temperature specific heat (C. Girod et al., arXiv:2004.03650).","sentences":["A long standing problem in the study of the under-hole-doped cuprates has been the description of the Fermi surfaces underlying the high magnetic field quantum oscillations.","Harrison and Sebastian (arXiv:1103.4181) proposed that the higher temperature pseudogap 'Fermi arcs' are reconstructed into an electron pocket by field-induced charge density wave order.","But computations on such a model (Zhang and Mei, arXiv:1411.2098) show an unobserved additional oscillation frequency from a Fermi surface arising from the backsides of the hole pockets completing the Fermi arcs.","We describe a transition from a fractionalized Fermi liquid (FL*) model of the pseudogap metal, to a metal with bi-directional charge density wave order without fractionalization.","We show that the confinement of the fermionic spinon excitations of the FL* across this transition can eliminate the unobserved oscillation frequency, and also account for the excess observed linear-in-temperature specific heat (C. Girod et al., arXiv:2004.03650)."],"url":"http://arxiv.org/abs/2405.08817v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 17:59:57","title":"The RoboDrive Challenge: Drive Anytime Anywhere in Any Condition","abstract":"In the realm of autonomous driving, robust perception under out-of-distribution conditions is paramount for the safe deployment of vehicles. Challenges such as adverse weather, sensor malfunctions, and environmental unpredictability can severely impact the performance of autonomous systems. The 2024 RoboDrive Challenge was crafted to propel the development of driving perception technologies that can withstand and adapt to these real-world variabilities. Focusing on four pivotal tasks -- BEV detection, map segmentation, semantic occupancy prediction, and multi-view depth estimation -- the competition laid down a gauntlet to innovate and enhance system resilience against typical and atypical disturbances. This year's challenge consisted of five distinct tracks and attracted 140 registered teams from 93 institutes across 11 countries, resulting in nearly one thousand submissions evaluated through our servers. The competition culminated in 15 top-performing solutions, which introduced a range of innovative approaches including advanced data augmentation, multi-sensor fusion, self-supervised learning for error correction, and new algorithmic strategies to enhance sensor robustness. These contributions significantly advanced the state of the art, particularly in handling sensor inconsistencies and environmental variability. Participants, through collaborative efforts, pushed the boundaries of current technologies, showcasing their potential in real-world scenarios. Extensive evaluations and analyses provided insights into the effectiveness of these solutions, highlighting key trends and successful strategies for improving the resilience of driving perception systems. This challenge has set a new benchmark in the field, providing a rich repository of techniques expected to guide future research in this field.","sentences":["In the realm of autonomous driving, robust perception under out-of-distribution conditions is paramount for the safe deployment of vehicles.","Challenges such as adverse weather, sensor malfunctions, and environmental unpredictability can severely impact the performance of autonomous systems.","The 2024 RoboDrive Challenge was crafted to propel the development of driving perception technologies that can withstand and adapt to these real-world variabilities.","Focusing on four pivotal tasks -- BEV detection, map segmentation, semantic occupancy prediction, and multi-view depth estimation -- the competition laid down a gauntlet to innovate and enhance system resilience against typical and atypical disturbances.","This year's challenge consisted of five distinct tracks and attracted 140 registered teams from 93 institutes across 11 countries, resulting in nearly one thousand submissions evaluated through our servers.","The competition culminated in 15 top-performing solutions, which introduced a range of innovative approaches including advanced data augmentation, multi-sensor fusion, self-supervised learning for error correction, and new algorithmic strategies to enhance sensor robustness.","These contributions significantly advanced the state of the art, particularly in handling sensor inconsistencies and environmental variability.","Participants, through collaborative efforts, pushed the boundaries of current technologies, showcasing their potential in real-world scenarios.","Extensive evaluations and analyses provided insights into the effectiveness of these solutions, highlighting key trends and successful strategies for improving the resilience of driving perception systems.","This challenge has set a new benchmark in the field, providing a rich repository of techniques expected to guide future research in this field."],"url":"http://arxiv.org/abs/2405.08816v1","category":"cs.CV"}
{"created":"2024-05-14 17:59:29","title":"Performance of wave function and Green's functions based methods for non equilibrium many-body dynamics","abstract":"Theoretical descriptions of non equilibrium dynamics of quantum many-body systems essentially employ either (i) explicit treatments, relying on truncation of the expansion of the many-body wave function, (ii) compressed representations of the many-body wave function, or (iii) evolution of an effective (downfolded) representation through Green's functions. In this work, we select representative cases of each of the methods and address how these complementary approaches capture the dynamics driven by intense field perturbations to non equilibrium states. Under strong driving, the systems are characterized by strong entanglement of the single particle density matrix and natural populations approaching those of a strongly interacting equilibrium system. We generate a representative set of results that are numerically exact and form a basis for critical comparison of the distinct families of methods. We demonstrate that the compressed formulation based on similarity transformed Hamiltonians (coupled cluster approach) is practically exact in weak fields and, hence, weakly or moderately correlated systems. Coupled cluster, however, struggles for strong driving fields, under which the system exhibits strongly correlated behavior, as measured by the von Neumann entropy of the single particle density matrix. The dynamics predicted by Green's functions in the (widely popular) GW approximation are less accurate by improve significantly upon the mean-field results in the strongly driven regime.","sentences":["Theoretical descriptions of non equilibrium dynamics of quantum many-body systems essentially employ either (i) explicit treatments, relying on truncation of the expansion of the many-body wave function, (ii) compressed representations of the many-body wave function, or (iii) evolution of an effective (downfolded) representation through Green's functions.","In this work, we select representative cases of each of the methods and address how these complementary approaches capture the dynamics driven by intense field perturbations to non equilibrium states.","Under strong driving, the systems are characterized by strong entanglement of the single particle density matrix and natural populations approaching those of a strongly interacting equilibrium system.","We generate a representative set of results that are numerically exact and form a basis for critical comparison of the distinct families of methods.","We demonstrate that the compressed formulation based on similarity transformed Hamiltonians (coupled cluster approach) is practically exact in weak fields and, hence, weakly or moderately correlated systems.","Coupled cluster, however, struggles for strong driving fields, under which the system exhibits strongly correlated behavior, as measured by the von Neumann entropy of the single particle density matrix.","The dynamics predicted by Green's functions in the (widely popular) GW approximation are less accurate by improve significantly upon the mean-field results in the strongly driven regime."],"url":"http://arxiv.org/abs/2405.08814v1","category":"physics.comp-ph"}
{"created":"2024-05-14 17:58:11","title":"Slow-growing counterexamples to the strong Eremenko Conjecture","abstract":"Let $f\\colon\\mathbb{C} \\to\\mathbb{C}$ be a transcendental entire function. In 1989, Eremenko asked the following question concerning the set $I(f)$ of points that tend to infinity under iteration: can every point of $I(f)$ be joined to $\\infty$ by a curve in $I(f)$? This is known as the strong Eremenko conjecture and was disproved in 2011 by Rottenfu{\\ss}er, R\\\"uckert, Rempe and Schleicher. The function has relatively small infinite order: it can be chosen such that $\\log \\log \\,\\lvert f(z)\\rvert = (\\log \\lvert z\\rvert)^{1+o(1)}$ as $f(z)\\to \\infty$. Moreover, $f$ belongs to the \\emph{Eremenko--Lyubich class $\\mathcal{B}$}. Rottenfu{\\ss}er et al also show that the strong Eremenko conjecture does hold for any $f\\in\\mathcal{B}$ of finite order. We consider how slow a counterexample $f\\in\\mathcal{B}$ can grow. Suppose that $\\Theta\\colon [t_0,\\infty)\\to [0,\\infty)$ satisfies $\\Theta(t) \\to 0$ and \\[ (\\log t)^{-\\beta \\Theta(\\log t)}/\\Theta(t) \\to 0 \\quad\\text{ as $t\\to \\infty$} \\] for some $0<\\beta<1$, along with a certain regularity assumption. Then there exists a counterexample $f\\in\\mathcal{B}$ as above such that \\[ \\log \\log \\vert f(z)\\vert = O ( (\\log \\vert z \\vert)^{1 + \\Theta( \\log \\vert z \\vert )}) \\] as $\\vert f(z)\\vert \\to\\infty$. The hypotheses are satisfied, in particular, for $\\Theta(t) = 1/(\\log \\log t)^{\\alpha}$, for any $\\alpha>0$.","sentences":["Let $f\\colon\\mathbb{C} \\to\\mathbb{C}$ be a transcendental entire function.","In 1989, Eremenko asked the following question concerning the set $I(f)$ of points that tend to infinity under iteration: can every point of $I(f)$ be joined to $\\infty$ by a curve in $I(f)$?","This is known as the strong Eremenko conjecture and was disproved in 2011 by Rottenfu{\\ss}er, R\\\"uckert, Rempe and Schleicher.","The function has relatively small infinite order: it can be chosen such that $\\log \\log \\,\\lvert f(z)\\rvert = (\\log \\lvert z\\rvert)^{1+o(1)}$ as $f(z)\\to \\infty$.","Moreover, $f$ belongs to the \\emph{Eremenko--Lyubich class $\\mathcal{B}$}.","Rottenfu{\\ss}er et al also show that the strong Eremenko conjecture does hold for any $f\\in\\mathcal{B}$ of finite order.","We consider how slow a counterexample $f\\in\\mathcal{B}$ can grow.","Suppose that $\\Theta\\colon [t_0,\\infty)\\to [0,\\infty)$ satisfies $\\Theta(t) \\to 0$ and \\[ (\\log t)^{-\\beta \\Theta(\\log t)}/\\Theta(t)","\\to 0","\\quad\\text{ as $t\\to \\infty$} \\] for some $0<\\beta<1$, along with a certain regularity assumption.","Then there exists a counterexample $f\\in\\mathcal{B}$ as above such that \\[ \\log \\log \\vert f(z)\\vert = O ( (\\log \\vert z \\vert)^{1 + \\Theta( \\log \\vert z \\vert )})","\\] as $\\vert f(z)\\vert \\to\\infty$. The hypotheses are satisfied, in particular, for $\\Theta(t) = 1/(\\log \\log t)^{\\alpha}$, for any $\\alpha>0$."],"url":"http://arxiv.org/abs/2405.08811v1","category":"math.DS"}
{"created":"2024-05-14 17:55:32","title":"Quantum computing with Qiskit","abstract":"We describe Qiskit, a software development kit for quantum information science. We discuss the key design decisions that have shaped its development, and examine the software architecture and its core components. We demonstrate an end-to-end workflow for solving a problem in condensed matter physics on a quantum computer that serves to highlight some of Qiskit's capabilities, for example the representation and optimization of circuits at various abstraction levels, its scalability and retargetability to new gates, and the use of quantum-classical computations via dynamic circuits. Lastly, we discuss some of the ecosystem of tools and plugins that extend Qiskit for various tasks, and the future ahead.","sentences":["We describe Qiskit, a software development kit for quantum information science.","We discuss the key design decisions that have shaped its development, and examine the software architecture and its core components.","We demonstrate an end-to-end workflow for solving a problem in condensed matter physics on a quantum computer that serves to highlight some of Qiskit's capabilities, for example the representation and optimization of circuits at various abstraction levels, its scalability and retargetability to new gates, and the use of quantum-classical computations via dynamic circuits.","Lastly, we discuss some of the ecosystem of tools and plugins that extend Qiskit for various tasks, and the future ahead."],"url":"http://arxiv.org/abs/2405.08810v2","category":"quant-ph"}
{"created":"2024-05-14 17:54:54","title":"Mock-local energy density of gravitational waves","abstract":"We propose a new set of BMS charges at null infinity, characterized by a super-translation flux that contains only the `hard' term. This is achieved with a specific corner improvement of the symplectic 2-form, and we spell the conditions under which it is unique. The charges are associated to a Wald-Zoupas symplectic potential, and satisfy all standard criteria: they are covariant, provide a center-less realization of the symmetry algebra, have vanishing flux in non-radiative spacetimes, and vanish in Minkowski. We use them to define a certain notion of localized energy density of gravitational waves. They have potential applications to the generalized second law and to soft theorems.","sentences":["We propose a new set of BMS charges at null infinity, characterized by a super-translation flux that contains only the `hard' term.","This is achieved with a specific corner improvement of the symplectic 2-form, and we spell the conditions under which it is unique.","The charges are associated to a Wald-Zoupas symplectic potential, and satisfy all standard criteria: they are covariant, provide a center-less realization of the symmetry algebra, have vanishing flux in non-radiative spacetimes, and vanish in Minkowski.","We use them to define a certain notion of localized energy density of gravitational waves.","They have potential applications to the generalized second law and to soft theorems."],"url":"http://arxiv.org/abs/2405.08808v1","category":"gr-qc"}
{"created":"2024-05-14 17:53:08","title":"Bounds on the Distribution of a Sum of Two Random Variables: Revisiting a problem of Kolmogorov with application to Individual Treatment Effects","abstract":"We revisit the following problem, proposed by Kolmogorov: given prescribed marginal distributions $F$ and $G$ for random variables $X,Y$ respectively, characterize the set of compatible distribution functions for the sum $Z=X+Y$. Bounds on the distribution function for $Z$ were given by Markarov (1982), and Frank et al. (1987), the latter using copula theory. However, though they obtain the same bounds, they make different assertions concerning their sharpness. In addition, their solutions leave some open problems in the case when the given marginal distribution functions are discontinuous. These issues have led to some confusion and erroneous statements in subsequent literature, which we correct.   Kolmogorov's problem is closely related to inferring possible distributions for individual treatment effects $Y_1 - Y_0$ given the marginal distributions of $Y_1$ and $Y_0$; the latter being identified from a randomized experiment. We use our new insights to sharpen and correct results due to Fan and Park (2010) concerning individual treatment effects, and to fill some other logical gaps.","sentences":["We revisit the following problem, proposed by Kolmogorov: given prescribed marginal distributions $F$ and $G$ for random variables $X,Y$ respectively, characterize the set of compatible distribution functions for the sum $Z=X+Y$. Bounds on the distribution function for $Z$ were given by Markarov (1982), and Frank et al. (1987), the latter using copula theory.","However, though they obtain the same bounds, they make different assertions concerning their sharpness.","In addition, their solutions leave some open problems in the case when the given marginal distribution functions are discontinuous.","These issues have led to some confusion and erroneous statements in subsequent literature, which we correct.   ","Kolmogorov's problem is closely related to inferring possible distributions for individual treatment effects $Y_1 - Y_0$ given the marginal distributions of $Y_1$ and $Y_0$; the latter being identified from a randomized experiment.","We use our new insights to sharpen and correct results due to Fan and Park (2010) concerning individual treatment effects, and to fill some other logical gaps."],"url":"http://arxiv.org/abs/2405.08806v1","category":"math.ST"}
{"created":"2024-05-14 17:52:06","title":"Special potentials for relativistic Laplacians I: Fractional Rollnik-class","abstract":"We propose a counterpart of the classical Rollnik-class of potentials for fractional and massive relativistic Laplacians, and describe this space in terms of appropriate Riesz potentials. These definitions rely on precise resolvent estimates. We show that Coulomb-type potentials are elements of fractional Rollnik-class up to but not including the critical singularity of the Hardy potential. For the operators with fractional exponent $\\alpha = 1$ there exists no fractional Rollnik potential, however, in low dimensions we make sense of these classes as limiting cases by using $\\Gamma$-convergence. In a second part of the paper we derive detailed results on the self-adjointness and spectral properties of relativistic Schr\\\"odinger operators obtained under perturbations by fractional Rollnik potentials. We also define an extended fractional Rollnik-class which is the maximal space for the Hilbert-Schmidt property of the related Birman-Schwinger operators.","sentences":["We propose a counterpart of the classical Rollnik-class of potentials for fractional and massive relativistic Laplacians, and describe this space in terms of appropriate Riesz potentials.","These definitions rely on precise resolvent estimates.","We show that Coulomb-type potentials are elements of fractional Rollnik-class up to but not including the critical singularity of the Hardy potential.","For the operators with fractional exponent $\\alpha = 1$ there exists no fractional Rollnik potential, however, in low dimensions we make sense of these classes as limiting cases by using $\\Gamma$-convergence.","In a second part of the paper we derive detailed results on the self-adjointness and spectral properties of relativistic Schr\\\"odinger operators obtained under perturbations by fractional Rollnik potentials.","We also define an extended fractional Rollnik-class which is the maximal space for the Hilbert-Schmidt property of the related Birman-Schwinger operators."],"url":"http://arxiv.org/abs/2405.08805v1","category":"math.FA"}
{"created":"2024-05-14 17:50:58","title":"Temperature-dependent Structural Evolution of Ruddlesden-Popper Bilayer Nickelate La$_3$Ni$_2$O$_7$","abstract":"A recent $J. Am. Chem. Soc.$ Article (DOI: 10.1021/jacs.3c13094) details a pressure-temperature ($P$-$T$) phase diagram for the Ruddlesden-Popper bilayer nickelate La$_3$Ni$_2$O$_7$ (LNO-2222) using synchrotron X-ray diffraction. This study identifies a phase transition from $Amam$ (#63) to $Fmmm$ (#69) within the temperature range of 104 K to 120 K under initial pressure and attributes the $I\\rm{4/}$$mmm$ (#139) space group to the structure responsible for the superconductivity of LNO-2222. Herein, we examine the temperature-dependent structural evolution of LNO-2222 single crystals at ambient pressure. Contrary to symmetry increase and the established $Amam$-$Fmmm$ phase boundary, we observe an enhancement in the $Cmcm$ reflections as temperature decreases. This work not only establishes a benchmark method for single crystal structure studies of LNO-2222 using laboratory X-rays, but also enhances the understanding of the complex crystallographic behavior of this system, contributing insights to further experimental and theoretical explorations.","sentences":["A recent $J. Am.","Chem.","Soc.$ Article (DOI: 10.1021/jacs.3c13094) details a pressure-temperature ($P$-$T$) phase diagram for the Ruddlesden-Popper bilayer nickelate La$_3$Ni$_2$O$_7$ (LNO-2222) using synchrotron X-ray diffraction.","This study identifies a phase transition from $Amam$ (#63) to $Fmmm$ (#69) within the temperature range of 104 K to 120 K under initial pressure and attributes the $I\\rm{4/}$$mmm$ (#139) space group to the structure responsible for the superconductivity of LNO-2222.","Herein, we examine the temperature-dependent structural evolution of LNO-2222 single crystals at ambient pressure.","Contrary to symmetry increase and the established $Amam$-$Fmmm$ phase boundary, we observe an enhancement in the $Cmcm$ reflections as temperature decreases.","This work not only establishes a benchmark method for single crystal structure studies of LNO-2222 using laboratory X-rays, but also enhances the understanding of the complex crystallographic behavior of this system, contributing insights to further experimental and theoretical explorations."],"url":"http://arxiv.org/abs/2405.08802v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 17:48:44","title":"Estimation of Participation Factors for Power System Oscillation from Measurements","abstract":"In a power system, when the participation factors of generators are computed to rank their participations into an oscillatory mode, a model-based approach is conventionally used on the linearized system model by means of the corresponding right and left eigenvectors. This paper proposes a new approach for estimating participation factors directly from measurement data on generator responses under selected disturbances. The approach computes extended participation factors that coincide with accurate model-based participation factors when the measured responses satisfy an ideally symmetric condition. This paper relaxes this symmetric condition with the original measurement space by identifying and utilizing a coordinate transformation to a new space optimally recovering the symmetry. Thus, the optimal estimates of participation factors solely from measurements are achieved, and the accuracy and influencing factors are discussed. The proposed approach is first demonstrated in detail on a two-area system and then tested on an NPCC 48-machine power system. The penetration of inverter-based resources is also considered.","sentences":["In a power system, when the participation factors of generators are computed to rank their participations into an oscillatory mode, a model-based approach is conventionally used on the linearized system model by means of the corresponding right and left eigenvectors.","This paper proposes a new approach for estimating participation factors directly from measurement data on generator responses under selected disturbances.","The approach computes extended participation factors that coincide with accurate model-based participation factors when the measured responses satisfy an ideally symmetric condition.","This paper relaxes this symmetric condition with the original measurement space by identifying and utilizing a coordinate transformation to a new space optimally recovering the symmetry.","Thus, the optimal estimates of participation factors solely from measurements are achieved, and the accuracy and influencing factors are discussed.","The proposed approach is first demonstrated in detail on a two-area system and then tested on an NPCC 48-machine power system.","The penetration of inverter-based resources is also considered."],"url":"http://arxiv.org/abs/2405.08800v1","category":"eess.SY"}
{"created":"2024-05-14 17:44:34","title":"Ambiguous Annotations: When is a Pedestrian not a Pedestrian?","abstract":"Datasets labelled by human annotators are widely used in the training and testing of machine learning models. In recent years, researchers are increasingly paying attention to label quality. However, it is not always possible to objectively determine whether an assigned label is correct or not. The present work investigates this ambiguity in the annotation of autonomous driving datasets as an important dimension of data quality. Our experiments show that excluding highly ambiguous data from the training improves model performance of a state-of-the-art pedestrian detector in terms of LAMR, precision and F1 score, thereby saving training time and annotation costs. Furthermore, we demonstrate that, in order to safely remove ambiguous instances and ensure the retained representativeness of the training data, an understanding of the properties of the dataset and class under investigation is crucial.","sentences":["Datasets labelled by human annotators are widely used in the training and testing of machine learning models.","In recent years, researchers are increasingly paying attention to label quality.","However, it is not always possible to objectively determine whether an assigned label is correct or not.","The present work investigates this ambiguity in the annotation of autonomous driving datasets as an important dimension of data quality.","Our experiments show that excluding highly ambiguous data from the training improves model performance of a state-of-the-art pedestrian detector in terms of LAMR, precision and F1 score, thereby saving training time and annotation costs.","Furthermore, we demonstrate that, in order to safely remove ambiguous instances and ensure the retained representativeness of the training data, an understanding of the properties of the dataset and class under investigation is crucial."],"url":"http://arxiv.org/abs/2405.08794v1","category":"cs.CV"}
{"created":"2024-05-14 17:25:37","title":"The Developing Human Connectome Project: A Fast Deep Learning-based Pipeline for Neonatal Cortical Surface Reconstruction","abstract":"The Developing Human Connectome Project (dHCP) aims to explore developmental patterns of the human brain during the perinatal period. An automated processing pipeline has been developed to extract high-quality cortical surfaces from structural brain magnetic resonance (MR) images for the dHCP neonatal dataset. However, the current implementation of the pipeline requires more than 6.5 hours to process a single MRI scan, making it expensive for large-scale neuroimaging studies. In this paper, we propose a fast deep learning (DL) based pipeline for dHCP neonatal cortical surface reconstruction, incorporating DL-based brain extraction, cortical surface reconstruction and spherical projection, as well as GPU-accelerated cortical surface inflation and cortical feature estimation. We introduce a multiscale deformation network to learn diffeomorphic cortical surface reconstruction end-to-end from T2-weighted brain MRI. A fast unsupervised spherical mapping approach is integrated to minimize metric distortions between cortical surfaces and projected spheres. The entire workflow of our DL-based dHCP pipeline completes within only 24 seconds on a modern GPU, which is nearly 1000 times faster than the original dHCP pipeline. Manual quality control demonstrates that for 82.5% of the test samples, our DL-based pipeline produces superior (54.2%) or equal quality (28.3%) cortical surfaces compared to the original dHCP pipeline.","sentences":["The Developing Human Connectome Project (dHCP) aims to explore developmental patterns of the human brain during the perinatal period.","An automated processing pipeline has been developed to extract high-quality cortical surfaces from structural brain magnetic resonance (MR) images for the dHCP neonatal dataset.","However, the current implementation of the pipeline requires more than 6.5 hours to process a single MRI scan, making it expensive for large-scale neuroimaging studies.","In this paper, we propose a fast deep learning (DL) based pipeline for dHCP neonatal cortical surface reconstruction, incorporating DL-based brain extraction, cortical surface reconstruction and spherical projection, as well as GPU-accelerated cortical surface inflation and cortical feature estimation.","We introduce a multiscale deformation network to learn diffeomorphic cortical surface reconstruction end-to-end from T2-weighted brain MRI.","A fast unsupervised spherical mapping approach is integrated to minimize metric distortions between cortical surfaces and projected spheres.","The entire workflow of our DL-based dHCP pipeline completes within only 24 seconds on a modern GPU, which is nearly 1000 times faster than the original dHCP pipeline.","Manual quality control demonstrates that for 82.5% of the test samples, our DL-based pipeline produces superior (54.2%) or equal quality (28.3%) cortical surfaces compared to the original dHCP pipeline."],"url":"http://arxiv.org/abs/2405.08783v1","category":"eess.IV"}
{"created":"2024-05-14 17:11:33","title":"FolkTalent: Enhancing Classification and Tagging of Indian Folk Paintings","abstract":"Indian folk paintings have a rich mosaic of symbols, colors, textures, and stories making them an invaluable repository of cultural legacy. The paper presents a novel approach to classifying these paintings into distinct art forms and tagging them with their unique salient features. A custom dataset named FolkTalent, comprising 2279 digital images of paintings across 12 different forms, has been prepared using websites that are direct outlets of Indian folk paintings. Tags covering a wide range of attributes like color, theme, artistic style, and patterns are generated using GPT4, and verified by an expert for each painting. Classification is performed employing the RandomForest ensemble technique on fine-tuned Convolutional Neural Network (CNN) models to classify Indian folk paintings, achieving an accuracy of 91.83%. Tagging is accomplished via the prominent fine-tuned CNN-based backbones with a custom classifier attached to its top to perform multi-label image classification. The generated tags offer a deeper insight into the painting, enabling an enhanced search experience based on theme and visual attributes. The proposed hybrid model sets a new benchmark in folk painting classification and tagging, significantly contributing to cataloging India's folk-art heritage.","sentences":["Indian folk paintings have a rich mosaic of symbols, colors, textures, and stories making them an invaluable repository of cultural legacy.","The paper presents a novel approach to classifying these paintings into distinct art forms and tagging them with their unique salient features.","A custom dataset named FolkTalent, comprising 2279 digital images of paintings across 12 different forms, has been prepared using websites that are direct outlets of Indian folk paintings.","Tags covering a wide range of attributes like color, theme, artistic style, and patterns are generated using GPT4, and verified by an expert for each painting.","Classification is performed employing the RandomForest ensemble technique on fine-tuned Convolutional Neural Network (CNN) models to classify Indian folk paintings, achieving an accuracy of 91.83%.","Tagging is accomplished via the prominent fine-tuned CNN-based backbones with a custom classifier attached to its top to perform multi-label image classification.","The generated tags offer a deeper insight into the painting, enabling an enhanced search experience based on theme and visual attributes.","The proposed hybrid model sets a new benchmark in folk painting classification and tagging, significantly contributing to cataloging India's folk-art heritage."],"url":"http://arxiv.org/abs/2405.08776v1","category":"cs.CV"}
{"created":"2024-05-14 17:08:19","title":"Evolution of ferroelectric properties in SmxBi1-xFeO3 via automated Piezoresponse Force Microscopy across combinatorial spread libraries","abstract":"Combinatorial spread libraries offer a unique approach to explore evolution of materials properties over the broad concentration, temperature, and growth parameter spaces. However, the traditional limitation of this approach is the requirement for the read-out of functional properties across the library. Here we demonstrate the application of automated Piezoresponse Force Microscopy (PFM) for the exploration of the physics in the SmxBi1-xFeO3 system with the ferroelectric-antiferroelectric morphotropic phase boundary. This approach relies on the synergy of the quantitative nature of PFM and the implementation of automated experiments that allows PFM-based gird sampling over macroscopic samples. The concentration dependence of pertinent ferroelectric parameters has been determined and used to develop the mathematical framework based on Ginzburg-Landau theory describing the evolution of these properties across the concentration space. We pose that combination of automated scanning probe microscope and combinatorial spread library approach will emerge as an efficient research paradigm to close the characterization gap in the high-throughput materials discovery. We make the data sets open to the community and hope that will stimulate other efforts to interpret and understand the physics of these systems.","sentences":["Combinatorial spread libraries offer a unique approach to explore evolution of materials properties over the broad concentration, temperature, and growth parameter spaces.","However, the traditional limitation of this approach is the requirement for the read-out of functional properties across the library.","Here we demonstrate the application of automated Piezoresponse Force Microscopy (PFM) for the exploration of the physics in the SmxBi1-xFeO3 system with the ferroelectric-antiferroelectric morphotropic phase boundary.","This approach relies on the synergy of the quantitative nature of PFM and the implementation of automated experiments that allows PFM-based gird sampling over macroscopic samples.","The concentration dependence of pertinent ferroelectric parameters has been determined and used to develop the mathematical framework based on Ginzburg-Landau theory describing the evolution of these properties across the concentration space.","We pose that combination of automated scanning probe microscope and combinatorial spread library approach will emerge as an efficient research paradigm to close the characterization gap in the high-throughput materials discovery.","We make the data sets open to the community and hope that will stimulate other efforts to interpret and understand the physics of these systems."],"url":"http://arxiv.org/abs/2405.08773v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 16:59:20","title":"Energy-based Hopfield Boosting for Out-of-Distribution Detection","abstract":"Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100.","sentences":["Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world.","Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies.","We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data.","Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data.","Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100."],"url":"http://arxiv.org/abs/2405.08766v1","category":"cs.LG"}
{"created":"2024-05-14 16:53:14","title":"S3C2 Summit 2024-03: Industry Secure Supply Chain Summit","abstract":"Supply chain security has become a very important vector to consider when defending against adversary attacks. Due to this, more and more developers are keen on improving their supply chains to make them more robust against future threats. On March 7th, 2024 researchers from the Secure Software Supply Chain Center (S3C2) gathered 14 industry leaders, developers and consumers of the open source ecosystem to discuss the state of supply chain security. The goal of the summit is to share insights between companies and developers alike to foster new collaborations and ideas moving forward. Through this meeting, participants were questions on best practices and thoughts how to improve things for the future. In this paper we summarize the responses and discussions of the summit. The panel questions can be found in the appendix.","sentences":["Supply chain security has become a very important vector to consider when defending against adversary attacks.","Due to this, more and more developers are keen on improving their supply chains to make them more robust against future threats.","On March 7th, 2024 researchers from the Secure Software Supply Chain Center (S3C2) gathered 14 industry leaders, developers and consumers of the open source ecosystem to discuss the state of supply chain security.","The goal of the summit is to share insights between companies and developers alike to foster new collaborations and ideas moving forward.","Through this meeting, participants were questions on best practices and thoughts how to improve things for the future.","In this paper we summarize the responses and discussions of the summit.","The panel questions can be found in the appendix."],"url":"http://arxiv.org/abs/2405.08762v1","category":"cs.CR"}
{"created":"2024-05-14 16:49:26","title":"Effect of injection conditions on the non-linear behavior of the ECDI and related turbulent transport","abstract":"The electron-cyclotron drift instability (ECDI) has been proposed as one of the main actors behind the anomalous transport of electrons in Hall thruster devices. In this work, we revisit the theory and perform two-dimensional PIC simulations under several conditions to analyze the non-linear behavior and the induced transport under several boundary conditions. Simulation results with fully-periodic boundaries and conditions faithful to the linear theory show the growth of ECDI modes, ion-wave trapping vortexes and agree with the existing literature in early times. In the long term, however, we observe very mild oscillations and null anomalous current. The evolution towards this new equilibrium is coherent to what can be expected from energy conservation. The quenching of the oscillations seem to be highly related with the distortion of ion-trapping vortexes in phase space after a long-term interaction of ion particles with the electrostatic wave. This result suggests that sustained oscillations and turbulent current could benefit from the renewal of ions by, e.g., removing and injecting particles through axial boundaries instead of applying periodicity. This second type of simulations shows that injection conditions highly impact the late simulation behavior of ECDI oscillations, where we identify several regimes depending on the value of the ion residence time compared to the characteristic saturation time in the fully periodic case. The intermediate regime, where these two times are close, is the only one providing sustained oscillations and electron transport and seems to be the relevant one in Hall devices.","sentences":["The electron-cyclotron drift instability (ECDI) has been proposed as one of the main actors behind the anomalous transport of electrons in Hall thruster devices.","In this work, we revisit the theory and perform two-dimensional PIC simulations under several conditions to analyze the non-linear behavior and the induced transport under several boundary conditions.","Simulation results with fully-periodic boundaries and conditions faithful to the linear theory show the growth of ECDI modes, ion-wave trapping vortexes and agree with the existing literature in early times.","In the long term, however, we observe very mild oscillations and null anomalous current.","The evolution towards this new equilibrium is coherent to what can be expected from energy conservation.","The quenching of the oscillations seem to be highly related with the distortion of ion-trapping vortexes in phase space after a long-term interaction of ion particles with the electrostatic wave.","This result suggests that sustained oscillations and turbulent current could benefit from the renewal of ions by, e.g., removing and injecting particles through axial boundaries instead of applying periodicity.","This second type of simulations shows that injection conditions highly impact the late simulation behavior of ECDI oscillations, where we identify several regimes depending on the value of the ion residence time compared to the characteristic saturation time in the fully periodic case.","The intermediate regime, where these two times are close, is the only one providing sustained oscillations and electron transport and seems to be the relevant one in Hall devices."],"url":"http://arxiv.org/abs/2405.08761v1","category":"physics.plasm-ph"}
{"created":"2024-05-14 16:28:00","title":"CATEcor: an Open Science, Shaded-Truss, Externally-Occulted Coronagraph","abstract":"We present the design of a portable coronagraph, CATEcor, that incorporates a novel \"shaded truss\" style of external occultation and serves as a proof-of-concept for that family of coronagraphs. The shaded truss design style has the potential for broad application in various scientific settings. We conceived CATEcor itself as a simple instrument to observe the corona during the darker skies available during a partial solar eclipse, or for students or interested amateurs to detect the corona under ideal non-eclipsed conditions. CATEcor is therefore optimized for simplicity and accessibility to the public. It is implemented using an existing dioptric telescope and an adapter rig that mounts in front of the objective lens, restricting the telescope aperture and providing external occultation. The adapter rig, including occulter, is fabricated using fusion deposition modeling (FDM; colloquially \"3D printing\"), greatly reducing cost. The structure is designed to be integrated with moderate care and may be replicated in a university or amateur setting. While CATEcor is a simple demonstration unit, the design concept, process, and trades are useful for other more sophisticated coronagraphs in the same general family, which might operate under normal daytime skies outside the annular-eclipse conditions used for CATEcor.","sentences":["We present the design of a portable coronagraph, CATEcor, that incorporates a novel \"shaded truss\" style of external occultation and serves as a proof-of-concept for that family of coronagraphs.","The shaded truss design style has the potential for broad application in various scientific settings.","We conceived CATEcor itself as a simple instrument to observe the corona during the darker skies available during a partial solar eclipse, or for students or interested amateurs to detect the corona under ideal non-eclipsed conditions.","CATEcor is therefore optimized for simplicity and accessibility to the public.","It is implemented using an existing dioptric telescope and an adapter rig that mounts in front of the objective lens, restricting the telescope aperture and providing external occultation.","The adapter rig, including occulter, is fabricated using fusion deposition modeling (FDM; colloquially \"3D printing\"), greatly reducing cost.","The structure is designed to be integrated with moderate care and may be replicated in a university or amateur setting.","While CATEcor is a simple demonstration unit, the design concept, process, and trades are useful for other more sophisticated coronagraphs in the same general family, which might operate under normal daytime skies outside the annular-eclipse conditions used for CATEcor."],"url":"http://arxiv.org/abs/2405.08739v1","category":"astro-ph.IM"}
{"created":"2024-05-14 16:24:56","title":"Calibrated sensitivity models","abstract":"In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses. However, the sensitivity parameter in these models -- which quantifies the degree of unmeasured confounding -- is often difficult to interpret. For this reason, researchers will sometimes compare the magnitude of the sensitivity parameter to an estimate for measured confounding. This is known as calibration. We propose novel calibrated sensitivity models, which directly incorporate measured confounding, and bound the degree of unmeasured confounding by a multiple of measured confounding. We illustrate how to construct calibrated sensitivity models via several examples. We also demonstrate their advantages over standard sensitivity analyses and calibration; in particular, the calibrated sensitivity parameter is an intuitive unit-less ratio of unmeasured divided by measured confounding, unlike standard sensitivity parameters, and one can correctly incorporate uncertainty due to estimating measured confounding, which standard calibration methods fail to do. By incorporating uncertainty due to measured confounding, we observe that causal analyses can be less robust or more robust to unmeasured confounding than would have been shown with standard approaches. We develop efficient estimators and methods for inference for bounds on the average treatment effect with three calibrated sensitivity models, and establish that our estimators are doubly robust and attain parametric efficiency and asymptotic normality under nonparametric conditions on their nuisance function estimators. We illustrate our methods with data analyses on the effect of exposure to violence on attitudes towards peace in Darfur and the effect of mothers' smoking on infant birthweight.","sentences":["In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses.","However, the sensitivity parameter in these models -- which quantifies the degree of unmeasured confounding -- is often difficult to interpret.","For this reason, researchers will sometimes compare the magnitude of the sensitivity parameter to an estimate for measured confounding.","This is known as calibration.","We propose novel calibrated sensitivity models, which directly incorporate measured confounding, and bound the degree of unmeasured confounding by a multiple of measured confounding.","We illustrate how to construct calibrated sensitivity models via several examples.","We also demonstrate their advantages over standard sensitivity analyses and calibration; in particular, the calibrated sensitivity parameter is an intuitive unit-less ratio of unmeasured divided by measured confounding, unlike standard sensitivity parameters, and one can correctly incorporate uncertainty due to estimating measured confounding, which standard calibration methods fail to do.","By incorporating uncertainty due to measured confounding, we observe that causal analyses can be less robust or more robust to unmeasured confounding than would have been shown with standard approaches.","We develop efficient estimators and methods for inference for bounds on the average treatment effect with three calibrated sensitivity models, and establish that our estimators are doubly robust and attain parametric efficiency and asymptotic normality under nonparametric conditions on their nuisance function estimators.","We illustrate our methods with data analyses on the effect of exposure to violence on attitudes towards peace in Darfur and the effect of mothers' smoking on infant birthweight."],"url":"http://arxiv.org/abs/2405.08738v1","category":"stat.ME"}
{"created":"2024-05-14 16:21:58","title":"Competition in the nutrient-driven self-cycling fermentation process","abstract":"Self-cycling fermentation is an automated process used for culturing microorganisms. We consider a model of $n$ distinct species competing for a single non-reproducing nutrient in a self-cycling fermentor in which the nutrient level is used as the decanting condition. The model is formulated in terms of impulsive ordinary differential equations. We prove that two species are able to coexist in the fermentor under certain conditions. We also provide numerical simulations that suggest coexistence of three species is possible and that competitor-mediated coexistence can occur in this case. These results are in contrast to the chemostat, the continuous analogue, where multiple species cannot coexist on a single nonreproducing nutrient.","sentences":["Self-cycling fermentation is an automated process used for culturing microorganisms.","We consider a model of $n$ distinct species competing for a single non-reproducing nutrient in a self-cycling fermentor in which the nutrient level is used as the decanting condition.","The model is formulated in terms of impulsive ordinary differential equations.","We prove that two species are able to coexist in the fermentor under certain conditions.","We also provide numerical simulations that suggest coexistence of three species is possible and that competitor-mediated coexistence can occur in this case.","These results are in contrast to the chemostat, the continuous analogue, where multiple species cannot coexist on a single nonreproducing nutrient."],"url":"http://arxiv.org/abs/2405.08735v1","category":"q-bio.PE"}
{"created":"2024-05-14 16:19:13","title":"A Simple Approach to Differentiable Rendering of SDFs","abstract":"We present a simple algorithm for differentiable rendering of surfaces represented by Signed Distance Fields (SDF), which makes it easy to integrate rendering into gradient-based optimization pipelines. To tackle visibility-related derivatives that make rendering non-differentiable, existing physically based differentiable rendering methods often rely on elaborate guiding data structures or reparameterization with a global impact on variance. In this article, we investigate an alternative that embraces nonzero bias in exchange for low variance and architectural simplicity. Our method expands the lower-dimensional boundary integral into a thin band that is easy to sample when the underlying surface is represented by an SDF. We demonstrate the performance and robustness of our formulation in end-to-end inverse rendering tasks, where it obtains results that are competitive with or superior to existing work.","sentences":["We present a simple algorithm for differentiable rendering of surfaces represented by Signed Distance Fields (SDF), which makes it easy to integrate rendering into gradient-based optimization pipelines.","To tackle visibility-related derivatives that make rendering non-differentiable, existing physically based differentiable rendering methods often rely on elaborate guiding data structures or reparameterization with a global impact on variance.","In this article, we investigate an alternative that embraces nonzero bias in exchange for low variance and architectural simplicity.","Our method expands the lower-dimensional boundary integral into a thin band that is easy to sample when the underlying surface is represented by an SDF.","We demonstrate the performance and robustness of our formulation in end-to-end inverse rendering tasks, where it obtains results that are competitive with or superior to existing work."],"url":"http://arxiv.org/abs/2405.08733v1","category":"cs.GR"}
{"created":"2024-05-14 16:16:29","title":"A Generalized Difference-in-Differences Estimator for Unbiased Estimation of Desired Estimands from Staggered Adoption and Stepped-Wedge Settings","abstract":"Staggered treatment adoption arises in the evaluation of policy impact and implementation in a variety of settings. This occurs in both randomized stepped-wedge trials and non-randomized quasi-experimental designs using causal inference methods based on difference-in-differences analysis. In both settings, it is crucial to carefully consider the target estimand and possible treatment effect heterogeneities in order to estimate the effect without bias and in an interpretable fashion. This paper proposes a novel non-parametric approach to this estimation for either setting. By constructing an estimator using two-by-two difference-in-difference comparisons as building blocks with arbitrary weights, the investigator can select weights to target the desired estimand in an unbiased manner under assumed treatment effect homogeneity, and minimize the variance under an assumed working covariance structure. This provides desirable bias properties with a relatively small sacrifice in variance and power by using the comparisons efficiently. The method is demonstrated on toy examples to show the process, as well as in the re-analysis of a stepped wedge trial on the impact of novel tuberculosis diagnostic tools. A full algorithm with R code is provided to implement this method. The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff.","sentences":["Staggered treatment adoption arises in the evaluation of policy impact and implementation in a variety of settings.","This occurs in both randomized stepped-wedge trials and non-randomized quasi-experimental designs using causal inference methods based on difference-in-differences analysis.","In both settings, it is crucial to carefully consider the target estimand and possible treatment effect heterogeneities in order to estimate the effect without bias and in an interpretable fashion.","This paper proposes a novel non-parametric approach to this estimation for either setting.","By constructing an estimator using two-by-two difference-in-difference comparisons as building blocks with arbitrary weights, the investigator can select weights to target the desired estimand in an unbiased manner under assumed treatment effect homogeneity, and minimize the variance under an assumed working covariance structure.","This provides desirable bias properties with a relatively small sacrifice in variance and power by using the comparisons efficiently.","The method is demonstrated on toy examples to show the process, as well as in the re-analysis of a stepped wedge trial on the impact of novel tuberculosis diagnostic tools.","A full algorithm with R code is provided to implement this method.","The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff."],"url":"http://arxiv.org/abs/2405.08730v1","category":"stat.ME"}
{"created":"2024-05-14 16:12:47","title":"Intervention effects based on potential benefit","abstract":"Optimal treatment rules are mappings from individual patient characteristics to tailored treatment assignments that maximize mean outcomes. In this work, we introduce a conditional potential benefit (CPB) metric that measures the expected improvement under an optimally chosen treatment compared to the status quo, within covariate strata. The potential benefit combines (i) the magnitude of the treatment effect, and (ii) the propensity for subjects to naturally select a suboptimal treatment. As a consequence, heterogeneity in the CPB can provide key insights into the mechanism by which a treatment acts and/or highlight potential barriers to treatment access or adverse effects. Moreover, we demonstrate that CPB is the natural prioritization score for individualized treatment policies when intervention capacity is constrained. That is, in the resource-limited setting where treatment options are freely accessible, but the ability to intervene on a portion of the target population is constrained (e.g., if the population is large, and follow-up and encouragement of treatment uptake is labor-intensive), targeting subjects with highest CPB maximizes the mean outcome. Focusing on this resource-limited setting, we derive formulas for optimal constrained treatment rules, and for any given budget, quantify the loss compared to the optimal unconstrained rule. We describe sufficient identification assumptions, and propose nonparametric, robust, and efficient estimators of the proposed quantities emerging from our framework.","sentences":["Optimal treatment rules are mappings from individual patient characteristics to tailored treatment assignments that maximize mean outcomes.","In this work, we introduce a conditional potential benefit (CPB) metric that measures the expected improvement under an optimally chosen treatment compared to the status quo, within covariate strata.","The potential benefit combines (i) the magnitude of the treatment effect, and (ii) the propensity for subjects to naturally select a suboptimal treatment.","As a consequence, heterogeneity in the CPB can provide key insights into the mechanism by which a treatment acts and/or highlight potential barriers to treatment access or adverse effects.","Moreover, we demonstrate that CPB is the natural prioritization score for individualized treatment policies when intervention capacity is constrained.","That is, in the resource-limited setting where treatment options are freely accessible, but the ability to intervene on a portion of the target population is constrained (e.g., if the population is large, and follow-up and encouragement of treatment uptake is labor-intensive), targeting subjects with highest CPB maximizes the mean outcome.","Focusing on this resource-limited setting, we derive formulas for optimal constrained treatment rules, and for any given budget, quantify the loss compared to the optimal unconstrained rule.","We describe sufficient identification assumptions, and propose nonparametric, robust, and efficient estimators of the proposed quantities emerging from our framework."],"url":"http://arxiv.org/abs/2405.08727v1","category":"stat.ME"}
{"created":"2024-05-14 16:04:39","title":"Addressing Misspecification in Simulation-based Inference through Data-driven Calibration","abstract":"Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI's reliability. This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations. Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator. Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals.","sentences":["Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators.","However, recent work has demonstrated that model misspecification can harm SBI's reliability.","This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements.","We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations.","Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator.","Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals."],"url":"http://arxiv.org/abs/2405.08719v1","category":"stat.ML"}
{"created":"2024-05-14 17:35:27","title":"Incorporating Clinical Guidelines through Adapting Multi-modal Large Language Model for Prostate Cancer PI-RADS Scoring","abstract":"The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in the diagnosis of clinically significant prostate cancer through MRI imaging. Current deep learning-based PI-RADS scoring methods often lack the incorporation of essential PI-RADS clinical guidelines~(PICG) utilized by radiologists, potentially compromising scoring accuracy. This paper introduces a novel approach that adapts a multi-modal large language model (MLLM) to incorporate PICG into PI-RADS scoring without additional annotations and network parameters. We present a two-stage fine-tuning process aimed at adapting MLLMs originally trained on natural images to the MRI data domain while effectively integrating the PICG. In the first stage, we develop a domain adapter layer specifically tailored for processing 3D MRI image inputs and design the MLLM instructions to differentiate MRI modalities effectively. In the second stage, we translate PICG into guiding instructions for the model to generate PICG-guided image features. Through feature distillation, we align scoring network features with the PICG-guided image feature, enabling the scoring network to effectively incorporate the PICG information. We develop our model on a public dataset and evaluate it in a real-world challenging in-house dataset. Experimental results demonstrate that our approach improves the performance of current scoring networks.","sentences":["The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in the diagnosis of clinically significant prostate cancer through MRI imaging.","Current deep learning-based PI-RADS scoring methods often lack the incorporation of essential PI-RADS clinical guidelines~(PICG) utilized by radiologists, potentially compromising scoring accuracy.","This paper introduces a novel approach that adapts a multi-modal large language model (MLLM) to incorporate PICG into PI-RADS scoring without additional annotations and network parameters.","We present a two-stage fine-tuning process aimed at adapting MLLMs originally trained on natural images to the MRI data domain while effectively integrating the PICG.","In the first stage, we develop a domain adapter layer specifically tailored for processing 3D MRI image inputs and design the MLLM instructions to differentiate MRI modalities effectively.","In the second stage, we translate PICG into guiding instructions for the model to generate PICG-guided image features.","Through feature distillation, we align scoring network features with the PICG-guided image feature, enabling the scoring network to effectively incorporate the PICG information.","We develop our model on a public dataset and evaluate it in a real-world challenging in-house dataset.","Experimental results demonstrate that our approach improves the performance of current scoring networks."],"url":"http://arxiv.org/abs/2405.08786v1","category":"cs.CV"}
{"created":"2024-05-14 16:23:25","title":"Adaptive Time Stepping for a Two-Time Integro-Differential Equation in Non-Equilibrium Quantum Dynamics","abstract":"The non-equilibrium Green's function gives access to one-body observables for quantum systems. Of particular interest are quantities such as density, currents, and absorption spectra which are important for interpreting experimental results in quantum transport and spectroscopy. We present an integration scheme for the Green's function's equations of motion, the Kadanoff-Baym equations (KBE), which is both adaptive in the time integrator step size and method order as well as the history integration order. We analyze the importance of solving the KBE self-consistently and show that adapting the order of history integral evaluation is important for obtaining accurate results. To examine the efficiency of our method, we compare runtimes to a state of the art fixed time step integrator for several test systems and show an order of magnitude speedup at similar levels of accuracy.","sentences":["The non-equilibrium Green's function gives access to one-body observables for quantum systems.","Of particular interest are quantities such as density, currents, and absorption spectra which are important for interpreting experimental results in quantum transport and spectroscopy.","We present an integration scheme for the Green's function's equations of motion, the Kadanoff-Baym equations (KBE), which is both adaptive in the time integrator step size and method order as well as the history integration order.","We analyze the importance of solving the KBE self-consistently and show that adapting the order of history integral evaluation is important for obtaining accurate results.","To examine the efficiency of our method, we compare runtimes to a state of the art fixed time step integrator for several test systems and show an order of magnitude speedup at similar levels of accuracy."],"url":"http://arxiv.org/abs/2405.08737v1","category":"physics.comp-ph"}
{"created":"2024-05-14 16:16:43","title":"Dynamic On-Palm Manipulation via Controlled Sliding","abstract":"Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure. Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding. However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation. In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes. We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task. We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks. Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task. Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach. To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions.","sentences":["Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure.","Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding.","However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation.","In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes.","We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task.","We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks.","Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task.","Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach.","To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions."],"url":"http://arxiv.org/abs/2405.08731v1","category":"cs.RO"}
{"created":"2024-05-14 15:51:52","title":"Data-driven Force Observer for Human-Robot Interaction with Series Elastic Actuators using Gaussian Processes","abstract":"Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction. Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components. While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments. In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression. By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate. Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications. We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario.","sentences":["Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction.","Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components.","While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments.","In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression.","By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate.","Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications.","We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario."],"url":"http://arxiv.org/abs/2405.08711v1","category":"cs.RO"}
{"created":"2024-05-14 15:47:31","title":"Design and Analysis of Resilient Vehicular Platoon Systems over Wireless Networks","abstract":"Connected vehicular platoons provide a promising solution to improve traffic efficiency and ensure road safety. Vehicles in a platoon utilize on-board sensors and wireless vehicle-to-vehicle (V2V) links to share traffic information for cooperative adaptive cruise control. To process real-time control and alert information, there is a need to ensure clock synchronization among the platoon's vehicles. However, adversaries can jeopardize the operation of the platoon by attacking the local clocks of vehicles, leading to clock offsets with the platoon's reference clock. In this paper, a novel framework is proposed for analyzing the resilience of vehicular platoons that are connected using V2V links. In particular, a resilient design based on a diffusion protocol is proposed to re-synchronize the attacked vehicle through wireless V2V links thereby mitigating the impact of variance of the transmission delay during recovery. Then, a novel metric named temporal conditional mean exceedance is defined and analyzed in order to characterize the resilience of the platoon. Subsequently, the conditions pertaining to the V2V links and recovery time needed for a resilient design are derived. Numerical results show that the proposed resilient design is feasible in face of a nine-fold increase in the variance of transmission delay compared to a baseline designed for reliability. Moreover, the proposed approach improves the reliability, defined as the probability of meeting a desired clock offset error requirement, by 45% compared to the baseline.","sentences":["Connected vehicular platoons provide a promising solution to improve traffic efficiency and ensure road safety.","Vehicles in a platoon utilize on-board sensors and wireless vehicle-to-vehicle (V2V) links to share traffic information for cooperative adaptive cruise control.","To process real-time control and alert information, there is a need to ensure clock synchronization among the platoon's vehicles.","However, adversaries can jeopardize the operation of the platoon by attacking the local clocks of vehicles, leading to clock offsets with the platoon's reference clock.","In this paper, a novel framework is proposed for analyzing the resilience of vehicular platoons that are connected using V2V links.","In particular, a resilient design based on a diffusion protocol is proposed to re-synchronize the attacked vehicle through wireless V2V links thereby mitigating the impact of variance of the transmission delay during recovery.","Then, a novel metric named temporal conditional mean exceedance is defined and analyzed in order to characterize the resilience of the platoon.","Subsequently, the conditions pertaining to the V2V links and recovery time needed for a resilient design are derived.","Numerical results show that the proposed resilient design is feasible in face of a nine-fold increase in the variance of transmission delay compared to a baseline designed for reliability.","Moreover, the proposed approach improves the reliability, defined as the probability of meeting a desired clock offset error requirement, by 45% compared to the baseline."],"url":"http://arxiv.org/abs/2405.08706v1","category":"eess.SY"}
{"created":"2024-05-14 15:28:37","title":"Calculating response functions of coupled oscillators using quantum phase estimation","abstract":"We study the problem of estimating frequency response functions of systems of coupled, classical harmonic oscillators using a quantum computer. The functional form of these response functions can be mapped to a corresponding eigenproblem of a Hermitian matrix $H$, thus suggesting the use of quantum phase estimation. Our proposed quantum algorithm operates in the standard $s$-sparse, oracle-based query access model. For a network of $N$ oscillators with maximum norm $\\lVert H \\rVert_{\\mathrm{max}}$, and when the eigenvalue tolerance $\\varepsilon$ is much smaller than the minimum eigenvalue gap, we use $\\mathcal{O}(\\log(N s \\lVert H \\rVert_{\\mathrm{max}}/\\varepsilon)$ algorithmic qubits and obtain a rigorous worst-case query complexity upper bound $\\mathcal{O}(s \\lVert H \\rVert_{\\mathrm{max}}/(\\delta^2 \\varepsilon) )$ up to logarithmic factors, where $\\delta$ denotes the desired precision on the coefficients appearing in the response functions. Crucially, our proposal does not suffer from the infamous state preparation bottleneck and can as such potentially achieve large quantum speedups compared to relevant classical methods. As a proof-of-principle of exponential quantum speedup, we show that a simple adaptation of our algorithm solves the random glued-trees problem in polynomial time. We discuss practical limitations as well as potential improvements for quantifying finite size, end-to-end complexities for application to relevant instances.","sentences":["We study the problem of estimating frequency response functions of systems of coupled, classical harmonic oscillators using a quantum computer.","The functional form of these response functions can be mapped to a corresponding eigenproblem of a Hermitian matrix $H$, thus suggesting the use of quantum phase estimation.","Our proposed quantum algorithm operates in the standard $s$-sparse, oracle-based query access model.","For a network of $N$ oscillators with maximum norm $\\lVert H \\rVert_{\\mathrm{max}}$, and when the eigenvalue tolerance $\\varepsilon$ is much smaller than the minimum eigenvalue gap, we use $\\mathcal{O}(\\log(N s \\lVert H \\rVert_{\\mathrm{max}}/\\varepsilon)$ algorithmic qubits and obtain a rigorous worst-case query complexity upper bound $\\mathcal{O}(s \\lVert H \\rVert_{\\mathrm{max}}/(\\delta^2 \\varepsilon) )$ up to logarithmic factors, where $\\delta$ denotes the desired precision on the coefficients appearing in the response functions.","Crucially, our proposal does not suffer from the infamous state preparation bottleneck and can as such potentially achieve large quantum speedups compared to relevant classical methods.","As a proof-of-principle of exponential quantum speedup, we show that a simple adaptation of our algorithm solves the random glued-trees problem in polynomial time.","We discuss practical limitations as well as potential improvements for quantifying finite size, end-to-end complexities for application to relevant instances."],"url":"http://arxiv.org/abs/2405.08694v1","category":"quant-ph"}
{"created":"2024-05-14 15:26:42","title":"Extending Non-Perturbative Simulation Techniques for Open-Quantum Systems to Excited-State Proton Transfer and Ultrafast Non-Adiabatic Dynamics","abstract":"Excited state proton transfer is an ubiquitous phenomenon in biology and chemistry, spanning from the ultrafast reactions of photo-bases and acids to light-driven, enzymatic catalysis and photosynthesis. However, the simulation of such dynamics involves multiple challenges, since high-dimensional, out-of-equilibrium vibronic states play a crucial role, while a fully quantum description of the proton's dissipative, real-space dynamics is also required. In this work, we extend the powerful Matrix Product State approach to open quantum systems (TEDOPA) to study these demanding dynamics, and also more general non-adiabatic processes that can appear in complex photochemistry subject to strong laser driving. As an illustration, we initially consider an open model of a four-level electronic system interacting with hundreds of intramolecular vibrations that drive ultrafast excited state proton transfer, as well as an explicit photonic environment that allows us to directly monitor the resulting dual fluorescence in this system. We then demonstrate how to include a continuous 'reaction coordinate' of the proton transfer that allows numerically exact simulations that can be understood, visualized and interpreted in the familiar language of diabatic and adiabatic dynamics on potential surfaces, while also retaining an exact quantum treatment of dissipation and driving effects that could be used to study diverse problems in ultrafast photochemistry.","sentences":["Excited state proton transfer is an ubiquitous phenomenon in biology and chemistry, spanning from the ultrafast reactions of photo-bases and acids to light-driven, enzymatic catalysis and photosynthesis.","However, the simulation of such dynamics involves multiple challenges, since high-dimensional, out-of-equilibrium vibronic states play a crucial role, while a fully quantum description of the proton's dissipative, real-space dynamics is also required.","In this work, we extend the powerful Matrix Product State approach to open quantum systems (TEDOPA) to study these demanding dynamics, and also more general non-adiabatic processes that can appear in complex photochemistry subject to strong laser driving.","As an illustration, we initially consider an open model of a four-level electronic system interacting with hundreds of intramolecular vibrations that drive ultrafast excited state proton transfer, as well as an explicit photonic environment that allows us to directly monitor the resulting dual fluorescence in this system.","We then demonstrate how to include a continuous 'reaction coordinate' of the proton transfer that allows numerically exact simulations that can be understood, visualized and interpreted in the familiar language of diabatic and adiabatic dynamics on potential surfaces, while also retaining an exact quantum treatment of dissipation and driving effects that could be used to study diverse problems in ultrafast photochemistry."],"url":"http://arxiv.org/abs/2405.08693v1","category":"physics.chem-ph"}
{"created":"2024-05-14 14:55:15","title":"EndoDAC: Efficient Adapting Foundation Model for Self-Supervised Depth Estimation from Any Endoscopic Camera","abstract":"Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization. Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance. This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation. We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes. Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters. Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder. Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs. Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics. Code is available at https://github.com/BeileiCui/EndoDAC.","sentences":["Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization.","Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance.","This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation.","We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes.","Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters.","Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder.","Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs.","Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics.","Code is available at https://github.com/BeileiCui/EndoDAC."],"url":"http://arxiv.org/abs/2405.08672v1","category":"eess.IV"}
{"created":"2024-05-14 14:10:48","title":"A Fast and Scalable Pathwise-Solver for Group Lasso and Elastic Net Penalized Regression via Block-Coordinate Descent","abstract":"We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path. Special attention is given when the loss is the usual least squares loss (Gaussian loss). We show that each block-coordinate update can be solved efficiently using Newton's method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate. Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets. Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet.","sentences":["We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path.","Special attention is given when the loss is the usual least squares loss (Gaussian loss).","We show that each block-coordinate update can be solved efficiently using Newton's method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate.","Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets.","Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet."],"url":"http://arxiv.org/abs/2405.08631v1","category":"stat.CO"}
{"created":"2024-05-14 14:08:55","title":"Beyond Quantum Annealing: Optimal control solutions to MaxCut problems","abstract":"Quantum Annealing (QA) relies on mixing two Hamiltonian terms, a simple driver and a complex problem Hamiltonian, in a linear combination. The time-dependent schedule for this mixing is often taken to be linear in time: improving on this linear choice is known to be essential and has proven to be difficult. Here, we present different techniques for improving on the linear-schedule QA along two directions, conceptually distinct but leading to similar outcomes: 1) the first approach consists of constructing a Trotter-digitized QA (dQA) with schedules parameterized in terms of Fourier modes or Chebyshev polynomials, inspired by the Chopped Random Basis algorithm (CRAB) for optimal control in continuous time; 2) the second approach is technically a Quantum Approximate Optimization Algorithm (QAOA), whose solutions are found iteratively using linear interpolation or expansion in Fourier modes. Both approaches emphasize finding smooth optimal schedule parameters, ultimately leading to hybrid quantum-classical variational algorithms of the alternating Hamiltonian Ansatz type. We apply these techniques to MaxCut problems on weighted 3-regular graphs with N = 14 sites, focusing on hard instances that exhibit a small spectral gap, for which a standard linear-schedule QA performs poorly. We characterize the physics behind the optimal protocols for both the dQA and QAOA approaches, discovering shortcuts to adiabaticity-like dynamics. Furthermore, we study the transferability of such smooth solutions among hard instances of MaxCut at different circuit depths. Finally, we show that the smoothness pattern of these protocols obtained in a digital setting enables us to adapt them to continuous-time evolution, contrarily to generic non-smooth solutions. This procedure results in an optimized quantum annealing schedule that is implementable on analog devices.","sentences":["Quantum Annealing (QA) relies on mixing two Hamiltonian terms, a simple driver and a complex problem Hamiltonian, in a linear combination.","The time-dependent schedule for this mixing is often taken to be linear in time: improving on this linear choice is known to be essential and has proven to be difficult.","Here, we present different techniques for improving on the linear-schedule QA along two directions, conceptually distinct but leading to similar outcomes: 1) the first approach consists of constructing a Trotter-digitized QA (dQA) with schedules parameterized in terms of Fourier modes or Chebyshev polynomials, inspired by the Chopped Random Basis algorithm (CRAB) for optimal control in continuous time; 2) the second approach is technically a Quantum Approximate Optimization Algorithm (QAOA), whose solutions are found iteratively using linear interpolation or expansion in Fourier modes.","Both approaches emphasize finding smooth optimal schedule parameters, ultimately leading to hybrid quantum-classical variational algorithms of the alternating Hamiltonian Ansatz type.","We apply these techniques to MaxCut problems on weighted 3-regular graphs with N = 14 sites, focusing on hard instances that exhibit a small spectral gap, for which a standard linear-schedule QA performs poorly.","We characterize the physics behind the optimal protocols for both the dQA and QAOA approaches, discovering shortcuts to adiabaticity-like dynamics.","Furthermore, we study the transferability of such smooth solutions among hard instances of MaxCut at different circuit depths.","Finally, we show that the smoothness pattern of these protocols obtained in a digital setting enables us to adapt them to continuous-time evolution, contrarily to generic non-smooth solutions.","This procedure results in an optimized quantum annealing schedule that is implementable on analog devices."],"url":"http://arxiv.org/abs/2405.08630v1","category":"quant-ph"}
{"created":"2024-05-14 13:37:13","title":"EVDA: Evolving Deepfake Audio Detection Continual Learning Benchmark","abstract":"The rise of advanced large language models such as GPT-4, GPT-4o, and the Claude family has made fake audio detection increasingly challenging. Traditional fine-tuning methods struggle to keep pace with the evolving landscape of synthetic speech, necessitating continual learning approaches that can adapt to new audio while retaining the ability to detect older types. Continual learning, which acts as an effective tool for detecting newly emerged deepfake audio while maintaining performance on older types, lacks a well-constructed and user-friendly evaluation framework. To address this gap, we introduce EVDA, a benchmark for evaluating continual learning methods in deepfake audio detection. EVDA includes classic datasets from the Anti-Spoofing Voice series, Chinese fake audio detection series, and newly generated deepfake audio from models like GPT-4 and GPT-4o. It supports various continual learning techniques, such as Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), and recent methods like Regularized Adaptive Weight Modification (RAWM) and Radian Weight Modification (RWM). Additionally, EVDA facilitates the development of robust algorithms by providing an open interface for integrating new continual learning methods","sentences":["The rise of advanced large language models such as GPT-4, GPT-4o, and the Claude family has made fake audio detection increasingly challenging.","Traditional fine-tuning methods struggle to keep pace with the evolving landscape of synthetic speech, necessitating continual learning approaches that can adapt to new audio while retaining the ability to detect older types.","Continual learning, which acts as an effective tool for detecting newly emerged deepfake audio while maintaining performance on older types, lacks a well-constructed and user-friendly evaluation framework.","To address this gap, we introduce EVDA, a benchmark for evaluating continual learning methods in deepfake audio detection.","EVDA includes classic datasets from the Anti-Spoofing Voice series, Chinese fake audio detection series, and newly generated deepfake audio from models like GPT-4 and GPT-4o.","It supports various continual learning techniques, such as Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), and recent methods like Regularized Adaptive Weight Modification (RAWM) and Radian Weight Modification (RWM).","Additionally, EVDA facilitates the development of robust algorithms by providing an open interface for integrating new continual learning methods"],"url":"http://arxiv.org/abs/2405.08596v2","category":"cs.SD"}
{"created":"2024-05-14 13:05:16","title":"Rethinking the adaptive relationship between Encoder Layers and Decoder Layers","abstract":"This article explores the adaptive relationship between Encoder Layers and Decoder Layers using the SOTA model Helsinki-NLP/opus-mt-de-en, which translates German to English. The specific method involves introducing a bias-free fully connected layer between the Encoder and Decoder, with different initializations of the layer's weights, and observing the outcomes of fine-tuning versus retraining. Four experiments were conducted in total. The results suggest that directly modifying the pre-trained model structure for fine-tuning yields suboptimal performance. However, upon observing the outcomes of the experiments with retraining, this structural adjustment shows significant potential.","sentences":["This article explores the adaptive relationship between Encoder Layers and Decoder Layers using the SOTA model Helsinki-NLP/opus-mt-de-en, which translates German to English.","The specific method involves introducing a bias-free fully connected layer between the Encoder and Decoder, with different initializations of the layer's weights, and observing the outcomes of fine-tuning versus retraining.","Four experiments were conducted in total.","The results suggest that directly modifying the pre-trained model structure for fine-tuning yields suboptimal performance.","However, upon observing the outcomes of the experiments with retraining, this structural adjustment shows significant potential."],"url":"http://arxiv.org/abs/2405.08570v1","category":"cs.CL"}
{"created":"2024-05-14 12:24:52","title":"Self-Distillation Improves DNA Sequence Inference","abstract":"Self-supervised pretraining (SSP) has been recognized as a method to enhance prediction accuracy in various downstream tasks. However, its efficacy for DNA sequences remains somewhat constrained. This limitation stems primarily from the fact that most existing SSP approaches in genomics focus on masked language modeling of individual sequences, neglecting the crucial aspect of encoding statistics across multiple sequences. To overcome this challenge, we introduce an innovative deep neural network model, which incorporates collaborative learning between a `student' and a `teacher' subnetwork. In this model, the student subnetwork employs masked learning on nucleotides and progressively adapts its parameters to the teacher subnetwork through an exponential moving average approach. Concurrently, both subnetworks engage in contrastive learning, deriving insights from two augmented representations of the input sequences. This self-distillation process enables our model to effectively assimilate both contextual information from individual sequences and distributional data across the sequence population. We validated our approach with preliminary pretraining using the human reference genome, followed by applying it to 20 downstream inference tasks. The empirical results from these experiments demonstrate that our novel method significantly boosts inference performance across the majority of these tasks. Our code is available at https://github.com/wiedersehne/FinDNA.","sentences":["Self-supervised pretraining (SSP) has been recognized as a method to enhance prediction accuracy in various downstream tasks.","However, its efficacy for DNA sequences remains somewhat constrained.","This limitation stems primarily from the fact that most existing SSP approaches in genomics focus on masked language modeling of individual sequences, neglecting the crucial aspect of encoding statistics across multiple sequences.","To overcome this challenge, we introduce an innovative deep neural network model, which incorporates collaborative learning between a `student' and a `teacher' subnetwork.","In this model, the student subnetwork employs masked learning on nucleotides and progressively adapts its parameters to the teacher subnetwork through an exponential moving average approach.","Concurrently, both subnetworks engage in contrastive learning, deriving insights from two augmented representations of the input sequences.","This self-distillation process enables our model to effectively assimilate both contextual information from individual sequences and distributional data across the sequence population.","We validated our approach with preliminary pretraining using the human reference genome, followed by applying it to 20 downstream inference tasks.","The empirical results from these experiments demonstrate that our novel method significantly boosts inference performance across the majority of these tasks.","Our code is available at https://github.com/wiedersehne/FinDNA."],"url":"http://arxiv.org/abs/2405.08538v1","category":"cs.LG"}
{"created":"2024-05-14 12:14:58","title":"Parameter-Efficient Instance-Adaptive Neural Video Compression","abstract":"Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to the standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines. However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data. The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video. However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training. In this work, we propose a parameter-efficient instance-adaptive video compression framework. Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences. The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms. Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings. We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR improvements) and BD rates compared to the baselines NVC.","sentences":["Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to the standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines.","However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data.","The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video.","However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training.","In this work, we propose a parameter-efficient instance-adaptive video compression framework.","Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences.","The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms.","Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings.","We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR improvements) and BD rates compared to the baselines NVC."],"url":"http://arxiv.org/abs/2405.08530v1","category":"eess.IV"}
{"created":"2024-05-14 11:37:26","title":"Falcon 7b for Software Mention Detection in Scholarly Documents","abstract":"This paper aims to tackle the challenge posed by the increasing integration of software tools in research across various disciplines by investigating the application of Falcon-7b for the detection and classification of software mentions within scholarly texts. Specifically, the study focuses on solving Subtask I of the Software Mention Detection in Scholarly Publications (SOMD), which entails identifying and categorizing software mentions from academic literature. Through comprehensive experimentation, the paper explores different training strategies, including a dual-classifier approach, adaptive sampling, and weighted loss scaling, to enhance detection accuracy while overcoming the complexities of class imbalance and the nuanced syntax of scholarly writing. The findings highlight the benefits of selective labelling and adaptive sampling in improving the model's performance. However, they also indicate that integrating multiple strategies does not necessarily result in cumulative improvements. This research offers insights into the effective application of large language models for specific tasks such as SOMD, underlining the importance of tailored approaches to address the unique challenges presented by academic text analysis.","sentences":["This paper aims to tackle the challenge posed by the increasing integration of software tools in research across various disciplines by investigating the application of Falcon-7b for the detection and classification of software mentions within scholarly texts.","Specifically, the study focuses on solving Subtask I of the Software Mention Detection in Scholarly Publications (SOMD), which entails identifying and categorizing software mentions from academic literature.","Through comprehensive experimentation, the paper explores different training strategies, including a dual-classifier approach, adaptive sampling, and weighted loss scaling, to enhance detection accuracy while overcoming the complexities of class imbalance and the nuanced syntax of scholarly writing.","The findings highlight the benefits of selective labelling and adaptive sampling in improving the model's performance.","However, they also indicate that integrating multiple strategies does not necessarily result in cumulative improvements.","This research offers insights into the effective application of large language models for specific tasks such as SOMD, underlining the importance of tailored approaches to address the unique challenges presented by academic text analysis."],"url":"http://arxiv.org/abs/2405.08514v1","category":"cs.LG"}
{"created":"2024-05-14 10:51:48","title":"A randomly generated Majorana neutrino mass matrix using Adaptive Monte Carlo method","abstract":"A randomly generated complex symmetric matrix using Adaptive Monte Carlo method, is taken as a general form of Majorana neutrino mass matrix, which is diagonalized by the use of eigenvectors. We extract all the neutrino oscillation parameters i.e. two mass-squared differences ($\\Delta m_{21}^2$ and $\\Delta m_{32}^2$ ), three mixing angles ($\\theta_{12}$, $\\theta_{13}$, $\\theta_{23}$) and three phases i.e. one Dirac CP violating phase ($\\delta_{CP}$) and two Majorana phases ($\\alpha$ and $\\beta$). The charge-parity (CP) violating phases are extracted from the mixing matrix constructed with the eigenvectors of the Hermitian matrix formed by the complex symmetric matrix. All the neutrino oscillation parameters within 3$\\sigma$ bound are allowed in both normal hierarchy (NH) and inverted hierarchy (IH) consistent with the latest Planck cosmological upper bound, $\\sum\\vert m_i\\vert<0.12$ eV. This latest cosmological upper bound is allowed only in three cases of zero texture for $m_{11}=0$; $m_{11},m_{12}=0$ and $m_{11},m_{13}=0$ in normal hierarchy whereas none of zero texture is allowed in inverted hierarchy. We also study effective neutrino masses $m_{\\beta}$ in tritium beta decay and $m_{\\beta\\beta}$ in neutrinoless double beta decay.","sentences":["A randomly generated complex symmetric matrix using Adaptive Monte Carlo method, is taken as a general form of Majorana neutrino mass matrix, which is diagonalized by the use of eigenvectors.","We extract all the neutrino oscillation parameters i.e. two mass-squared differences ($\\Delta m_{21}^2$ and $\\Delta m_{32}^2$ ), three mixing angles ($\\theta_{12}$, $\\theta_{13}$, $\\theta_{23}$) and three phases i.e. one Dirac CP violating phase ($\\delta_{CP}$) and two Majorana phases ($\\alpha$ and $\\beta$).","The charge-parity (CP) violating phases are extracted from the mixing matrix constructed with the eigenvectors of the Hermitian matrix formed by the complex symmetric matrix.","All the neutrino oscillation parameters within 3$\\sigma$ bound are allowed in both normal hierarchy (NH) and inverted hierarchy (IH) consistent with the latest Planck cosmological upper bound, $\\sum\\vert m_i\\vert<0.12$ eV. This latest cosmological upper bound is allowed only in three cases of zero texture for $m_{11}=0$; $m_{11},m_{12}=0$ and $m_{11},m_{13}=0$ in normal hierarchy whereas none of zero texture is allowed in inverted hierarchy.","We also study effective neutrino masses $m_{\\beta}$ in tritium beta decay and $m_{\\beta\\beta}$ in neutrinoless double beta decay."],"url":"http://arxiv.org/abs/2405.08495v1","category":"hep-ph"}
{"created":"2024-05-14 08:35:39","title":"Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More","abstract":"Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable. Karalias & Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization. Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization. However, each component confronts unique challenges. First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial. Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding. In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO. First, we concretize the targets for objective construction and derandomization with theoretical justification. Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets. Finally, we apply the derivations to various CO problems. Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t. both optimization quality and speed.","sentences":["Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable.","Karalias & Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization.","Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization.","However, each component confronts unique challenges.","First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial.","Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding.","In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO.","First, we concretize the targets for objective construction and derandomization with theoretical justification.","Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets.","Finally, we apply the derivations to various CO problems.","Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t.","both optimization quality and speed."],"url":"http://arxiv.org/abs/2405.08424v1","category":"cs.LG"}
{"created":"2024-05-14 08:18:38","title":"ICO learning as a measure of transient chaos in PT-symmetric Li\u00e9nard systems","abstract":"In this article, we investigate the implications of the unsupervised learning rule known as Input-Correlations (ICO) learning in the nonlinear dynamics of two linearly coupled PT-symmetric Li\\'enard oscillators. The fixed points of the oscillator have been evaluated analytically and the Jacobian linearization is employed to study their stability. We find that on increasing the amplitude of the external periodic drive, the system exhibits period-doubling cascade to chaos within a specific parametric regime wherein we observe emergent chaotic dynamics. We further notice that the system indicates an intermittency route to chaos in the chaotic regime. Finally, in the period-4 regime of our bifurcation analysis, we predict the emergence of transient chaos which eventually settles down to a period-2 oscillator response which has been further validated by both the maximal Finite-Time Lyapunov Exponent (FTLE) using the well-known Gram-Schmidt orthogonalization technique and the Hilbert Transform of the time-series. In the transiently chaotic regime, we deploy the ICO learning to analyze the time-series from which we identify that when the chaotic evolution transforms into periodic dynamics, the synaptic weight associated with the time-series of the loss oscillator exhibits stationary temporal evolution. This signifies that in the periodic regime, there is no overlap between the filtered signals obtained from the time-series of the coupled PT-symmetric oscillators. In addition, the temporal evolution of the weight associated with the stimulus mimics the behaviour of the Hilbert transform of the time-series.","sentences":["In this article, we investigate the implications of the unsupervised learning rule known as Input-Correlations (ICO) learning in the nonlinear dynamics of two linearly coupled PT-symmetric Li\\'enard oscillators.","The fixed points of the oscillator have been evaluated analytically and the Jacobian linearization is employed to study their stability.","We find that on increasing the amplitude of the external periodic drive, the system exhibits period-doubling cascade to chaos within a specific parametric regime wherein we observe emergent chaotic dynamics.","We further notice that the system indicates an intermittency route to chaos in the chaotic regime.","Finally, in the period-4 regime of our bifurcation analysis, we predict the emergence of transient chaos which eventually settles down to a period-2 oscillator response which has been further validated by both the maximal Finite-Time Lyapunov Exponent (FTLE) using the well-known Gram-Schmidt orthogonalization technique and the Hilbert Transform of the time-series.","In the transiently chaotic regime, we deploy the ICO learning to analyze the time-series from which we identify that when the chaotic evolution transforms into periodic dynamics, the synaptic weight associated with the time-series of the loss oscillator exhibits stationary temporal evolution.","This signifies that in the periodic regime, there is no overlap between the filtered signals obtained from the time-series of the coupled PT-symmetric oscillators.","In addition, the temporal evolution of the weight associated with the stimulus mimics the behaviour of the Hilbert transform of the time-series."],"url":"http://arxiv.org/abs/2405.08414v1","category":"nlin.AO"}
{"created":"2024-05-14 08:02:12","title":"Bifurcation analysis of a two-neuron central pattern generator model for both oscillatory and convergent neuronal activities","abstract":"The neural oscillator model proposed by Matsuoka is a piecewise affine system, which exhibits distinctive periodic solutions. Although such typical oscillation patterns have been widely studied, little is understood about the dynamics of convergence to certain fixed points and bifurcations between the periodic orbits and fixed points in this model. We performed fixed point analysis on a two-neuron version of the Matsuoka oscillator model, the result of which explains the mechanism of oscillation and the discontinuity-induced bifurcations such as subcritical/supercritical Hopf-like, homoclinic-like, and grazing bifurcations. Furthermore, it provided theoretical predictions concerning a logarithmic oscillation-period scaling law and noise-induced oscillations, which are both observed around those bifurcations. These results are expected to underpin further investigations into both oscillatory and transient neuronal activities with respect to central pattern generators.","sentences":["The neural oscillator model proposed by Matsuoka is a piecewise affine system, which exhibits distinctive periodic solutions.","Although such typical oscillation patterns have been widely studied, little is understood about the dynamics of convergence to certain fixed points and bifurcations between the periodic orbits and fixed points in this model.","We performed fixed point analysis on a two-neuron version of the Matsuoka oscillator model, the result of which explains the mechanism of oscillation and the discontinuity-induced bifurcations such as subcritical/supercritical Hopf-like, homoclinic-like, and grazing bifurcations.","Furthermore, it provided theoretical predictions concerning a logarithmic oscillation-period scaling law and noise-induced oscillations, which are both observed around those bifurcations.","These results are expected to underpin further investigations into both oscillatory and transient neuronal activities with respect to central pattern generators."],"url":"http://arxiv.org/abs/2405.08409v1","category":"nlin.AO"}
{"created":"2024-05-14 07:40:57","title":"Cerebralization of mathematical quantities and physical features in neural science: a critical evaluation","abstract":"At the turn of the 20th century, Henri Poincar{\\'e} explained that geometry is a convention and that the properties of space and time are the properties of our measuring instruments. Intriguingly, numerous contemporary authors argue that space, time and even number are ''encoded'' within the brain, as a consequence of evolution, adaptation and natural selection. In the neuroscientific study of movement generation, the activity of neurons would ''encode'' kinematic parameters: when they emit action potentials, neurons would ''speak'' a language carrying notions of classical mechanics. In this article, we shall explain that the movement of a body segment is the ultimate product of a measurement, a filtered numerical outcome of multiple processes taking place in parallel in the central nervous system and converging on the groups of neurons responsible for muscle contractions. The fact that notions of classical mechanics efficiently describe movements does not imply their implementation in the inner workings of the brain. Their relevance to the question how the brain activity enables one to produce accurate movements is questioned within the framework of the neurophysiology of orienting gaze movements toward a visual target.","sentences":["At the turn of the 20th century, Henri Poincar{\\'e} explained that geometry is a convention and that the properties of space and time are the properties of our measuring instruments.","Intriguingly, numerous contemporary authors argue that space, time and even number are ''encoded'' within the brain, as a consequence of evolution, adaptation and natural selection.","In the neuroscientific study of movement generation, the activity of neurons would ''encode'' kinematic parameters: when they emit action potentials, neurons would ''speak'' a language carrying notions of classical mechanics.","In this article, we shall explain that the movement of a body segment is the ultimate product of a measurement, a filtered numerical outcome of multiple processes taking place in parallel in the central nervous system and converging on the groups of neurons responsible for muscle contractions.","The fact that notions of classical mechanics efficiently describe movements does not imply their implementation in the inner workings of the brain.","Their relevance to the question how the brain activity enables one to produce accurate movements is questioned within the framework of the neurophysiology of orienting gaze movements toward a visual target."],"url":"http://arxiv.org/abs/2405.08391v1","category":"q-bio.NC"}
{"created":"2024-05-14 04:18:33","title":"Online Test-time Adaptation for Interatomic Potentials","abstract":"Machine learning interatomic potentials (MLIPs) enable more efficient molecular dynamics (MD) simulations with ab initio accuracy, which have been used in various domains of physical science. However, distribution shift between training and test data causes deterioration of the test performance of MLIPs, and even leads to collapse of MD simulations. In this work, we propose an online Test-time Adaptation Interatomic Potential (TAIP) framework to improve the generalization on test data. Specifically, we design a dual-level self-supervised learning approach that leverages global structure and atomic local environment information to align the model with the test data. Extensive experiments demonstrate TAIP's capability to bridge the domain gap between training and test dataset without additional data. TAIP enhances the test performance on various benchmarks, from small molecule datasets to complex periodic molecular systems with various types of elements. Remarkably, it also enables stable MD simulations where the corresponding baseline models collapse.","sentences":["Machine learning interatomic potentials (MLIPs) enable more efficient molecular dynamics (MD) simulations with ab initio accuracy, which have been used in various domains of physical science.","However, distribution shift between training and test data causes deterioration of the test performance of MLIPs, and even leads to collapse of MD simulations.","In this work, we propose an online Test-time Adaptation Interatomic Potential (TAIP) framework to improve the generalization on test data.","Specifically, we design a dual-level self-supervised learning approach that leverages global structure and atomic local environment information to align the model with the test data.","Extensive experiments demonstrate TAIP's capability to bridge the domain gap between training and test dataset without additional data.","TAIP enhances the test performance on various benchmarks, from small molecule datasets to complex periodic molecular systems with various types of elements.","Remarkably, it also enables stable MD simulations where the corresponding baseline models collapse."],"url":"http://arxiv.org/abs/2405.08308v1","category":"physics.comp-ph"}
{"created":"2024-05-14 03:07:54","title":"Orthogonal Delay-Doppler Division Multiplexing Modulation with Tomlinson-Harashima Precoding","abstract":"The orthogonal delay-Doppler (DD) division multiplexing(ODDM) modulation has been recently proposed as a promising modulation scheme for next-generation communication systems with high mobility. Despite its benefits, ODDM modulation and other DD domain modulation schemes face the challenge of excessive equalization complexity. To address this challenge, we propose time domain Tomlinson-Harashima precoding (THP) for the ODDM transmitter, to make the DD domain single-tap equalizer feasible, thereby reducing the equalization complexity. In our design, we first pre-cancel the inter-symbolinterference (ISI) using the linear time-varying (LTV) channel information. Second, different from classical THP designs, we introduce a modified modulo operation with an adaptive modulus, by which the joint DD domain data multiplexing and timedomain ISI pre-cancellation can be realized without excessively increasing the bit errors. We then analytically study the losses encountered in this design, namely the power loss, the modulo noise loss, and the modulo signal loss. Based on this analysis, BER lower bounds of the ODDM system with time domain THP are derived when 4-QAM or 16-QAM modulations are adopted for symbol mapping in the DD domain. Finally, through numerical results, we validate our analysis and then demonstrate that the ODDM system with time domain THP is a promising solution to realize better BER performance over LTV channels compared to orthogonal frequency division multiplexing systems with single-tap equalizer and ODDM systems with maximum ratio combining.","sentences":["The orthogonal delay-Doppler (DD) division multiplexing(ODDM) modulation has been recently proposed as a promising modulation scheme for next-generation communication systems with high mobility.","Despite its benefits, ODDM modulation and other DD domain modulation schemes face the challenge of excessive equalization complexity.","To address this challenge, we propose time domain Tomlinson-Harashima precoding (THP) for the ODDM transmitter, to make the DD domain single-tap equalizer feasible, thereby reducing the equalization complexity.","In our design, we first pre-cancel the inter-symbolinterference (ISI) using the linear time-varying (LTV) channel information.","Second, different from classical THP designs, we introduce a modified modulo operation with an adaptive modulus, by which the joint DD domain data multiplexing and timedomain ISI pre-cancellation can be realized without excessively increasing the bit errors.","We then analytically study the losses encountered in this design, namely the power loss, the modulo noise loss, and the modulo signal loss.","Based on this analysis, BER lower bounds of the ODDM system with time domain THP are derived when 4-QAM or 16-QAM modulations are adopted for symbol mapping in the DD domain.","Finally, through numerical results, we validate our analysis and then demonstrate that the ODDM system with time domain THP is a promising solution to realize better BER performance over LTV channels compared to orthogonal frequency division multiplexing systems with single-tap equalizer and ODDM systems with maximum ratio combining."],"url":"http://arxiv.org/abs/2405.08288v1","category":"eess.SP"}
{"created":"2024-05-14 02:13:11","title":"AI-driven, Model-Free Current Control: A Deep Symbolic Approach for Optimal Induction Machine Performance","abstract":"This paper proposed a straightforward and efficient current control solution for induction machines employing deep symbolic regression (DSR). The proposed DSR-based control design offers a simple yet highly effective approach by creating an optimal control model through training and fitting, resulting in an analytical dynamic numerical expression that characterizes the data. Notably, this approach not only produces an understandable model but also demonstrates the capacity to extrapolate and estimate data points outside its training dataset, showcasing its adaptability and resilience. In contrast to conventional state-of-the-art proportional-integral (PI) current controllers, which heavily rely on specific system models, the proposed DSR-based approach stands out for its model independence. Simulation and experimental tests validate its effectiveness, highlighting its superior extrapolation capabilities compared to conventional methods. These findings pave the way for the integration of deep learning methods in power conversion applications, promising improved performance and adaptability in the control of induction machines. The simulation and experimental test results are provided with a 3.7 kw induction machine to verify the efficacy of the proposed control solution.","sentences":["This paper proposed a straightforward and efficient current control solution for induction machines employing deep symbolic regression (DSR).","The proposed DSR-based control design offers a simple yet highly effective approach by creating an optimal control model through training and fitting, resulting in an analytical dynamic numerical expression that characterizes the data.","Notably, this approach not only produces an understandable model but also demonstrates the capacity to extrapolate and estimate data points outside its training dataset, showcasing its adaptability and resilience.","In contrast to conventional state-of-the-art proportional-integral (PI) current controllers, which heavily rely on specific system models, the proposed DSR-based approach stands out for its model independence.","Simulation and experimental tests validate its effectiveness, highlighting its superior extrapolation capabilities compared to conventional methods.","These findings pave the way for the integration of deep learning methods in power conversion applications, promising improved performance and adaptability in the control of induction machines.","The simulation and experimental test results are provided with a 3.7 kw induction machine to verify the efficacy of the proposed control solution."],"url":"http://arxiv.org/abs/2405.08277v1","category":"eess.SY"}
{"created":"2024-05-14 02:05:18","title":"Cloud Dissipation and Disk Wind in the Late Phase of Star Formation","abstract":"We perform a long-term simulation of star and disk formation using three-dimensional non-ideal magnetohydrodynamics. The simulation starts from a prestellar cloud and proceeds through the long-term evolution of the circumstellar disk until $\\sim 1.5\\times10^5$ yr after protostar formation. The disk has size $\\lesssim 50$ au and little substructure in the main accretion phase because of the action of magnetic braking and the magnetically-driven outflow to remove angular momentum. The main accretion phase ends when the outflow breaks out of the cloud, causing the envelope mass to decrease rapidly. The outflow subsequently weakens as the mass accretion rate also weakens. While the envelope-to-disk accretion continues, the disk grows gradually and develops transient spiral structures due to gravitational instability. When the envelope-to-disk accretion ends, the disk becomes stable and reaches a size $\\gtrsim 300$ au. In addition, about 30% of the initial cloud mass has been ejected by the outflow. A significant finding of this work is that after the envelope dissipates, a revitalization of the wind occurs, and there is mass ejection from the disk surface that lasts until the end of the simulation. This mass ejection (or disk wind) is generated since the magnetic pressure significantly dominates both the ram pressure and thermal pressure above and below the disk at this stage. Using the angular momentum flux and mass loss rate estimated from the disk wind, the disk dissipation timescale is estimated to be $\\sim10^6$ yr.","sentences":["We perform a long-term simulation of star and disk formation using three-dimensional non-ideal magnetohydrodynamics.","The simulation starts from a prestellar cloud and proceeds through the long-term evolution of the circumstellar disk until $\\sim 1.5\\times10^5$ yr after protostar formation.","The disk has size $\\lesssim 50$ au and little substructure in the main accretion phase because of the action of magnetic braking and the magnetically-driven outflow to remove angular momentum.","The main accretion phase ends when the outflow breaks out of the cloud, causing the envelope mass to decrease rapidly.","The outflow subsequently weakens as the mass accretion rate also weakens.","While the envelope-to-disk accretion continues, the disk grows gradually and develops transient spiral structures due to gravitational instability.","When the envelope-to-disk accretion ends, the disk becomes stable and reaches a size $\\gtrsim 300$ au.","In addition, about 30% of the initial cloud mass has been ejected by the outflow.","A significant finding of this work is that after the envelope dissipates, a revitalization of the wind occurs, and there is mass ejection from the disk surface that lasts until the end of the simulation.","This mass ejection (or disk wind) is generated since the magnetic pressure significantly dominates both the ram pressure and thermal pressure above and below the disk at this stage.","Using the angular momentum flux and mass loss rate estimated from the disk wind, the disk dissipation timescale is estimated to be $\\sim10^6$ yr."],"url":"http://arxiv.org/abs/2405.08271v1","category":"astro-ph.SR"}
{"created":"2024-05-14 02:02:15","title":"Towards Clinician-Preferred Segmentation: Leveraging Human-in-the-Loop for Test Time Adaptation in Medical Image Segmentation","abstract":"Deep learning-based medical image segmentation models often face performance degradation when deployed across various medical centers, largely due to the discrepancies in data distribution. Test Time Adaptation (TTA) methods, which adapt pre-trained models to test data, have been employed to mitigate such discrepancies. However, existing TTA methods primarily focus on manipulating Batch Normalization (BN) layers or employing prompt and adversarial learning, which may not effectively rectify the inconsistencies arising from divergent data distributions. In this paper, we propose a novel Human-in-the-loop TTA (HiTTA) framework that stands out in two significant ways. First, it capitalizes on the largely overlooked potential of clinician-corrected predictions, integrating these corrections into the TTA process to steer the model towards predictions that coincide more closely with clinical annotation preferences. Second, our framework conceives a divergence loss, designed specifically to diminish the prediction divergence instigated by domain disparities, through the careful calibration of BN parameters. Our HiTTA is distinguished by its dual-faceted capability to acclimatize to the distribution of test data whilst ensuring the model's predictions align with clinical expectations, thereby enhancing its relevance in a medical context. Extensive experiments on a public dataset underscore the superiority of our HiTTA over existing TTA methods, emphasizing the advantages of integrating human feedback and our divergence loss in enhancing the model's performance and adaptability across diverse medical centers.","sentences":["Deep learning-based medical image segmentation models often face performance degradation when deployed across various medical centers, largely due to the discrepancies in data distribution.","Test Time Adaptation (TTA) methods, which adapt pre-trained models to test data, have been employed to mitigate such discrepancies.","However, existing TTA methods primarily focus on manipulating Batch Normalization (BN) layers or employing prompt and adversarial learning, which may not effectively rectify the inconsistencies arising from divergent data distributions.","In this paper, we propose a novel Human-in-the-loop TTA (HiTTA) framework that stands out in two significant ways.","First, it capitalizes on the largely overlooked potential of clinician-corrected predictions, integrating these corrections into the TTA process to steer the model towards predictions that coincide more closely with clinical annotation preferences.","Second, our framework conceives a divergence loss, designed specifically to diminish the prediction divergence instigated by domain disparities, through the careful calibration of BN parameters.","Our HiTTA is distinguished by its dual-faceted capability to acclimatize to the distribution of test data whilst ensuring the model's predictions align with clinical expectations, thereby enhancing its relevance in a medical context.","Extensive experiments on a public dataset underscore the superiority of our HiTTA over existing TTA methods, emphasizing the advantages of integrating human feedback and our divergence loss in enhancing the model's performance and adaptability across diverse medical centers."],"url":"http://arxiv.org/abs/2405.08270v1","category":"cs.CV"}
{"created":"2024-05-13 22:45:44","title":"SeNMo: A Self-Normalizing Deep Learning Model for Enhanced Multi-Omics Data Analysis in Oncology","abstract":"Multi-omics research has enhanced our understanding of cancer heterogeneity and progression. Investigating molecular data through multi-omics approaches is crucial for unraveling the complex biological mechanisms underlying cancer, thereby enabling effective diagnosis, treatment, and prevention strategies. However, predicting patient outcomes through integration of all available multi-omics data is an under-study research direction. Here, we present SeNMo (Self-normalizing Network for Multi-omics), a deep neural network trained on multi-omics data across 33 cancer types. SeNMo is efficient in handling multi-omics data characterized by high-width (many features) and low-length (fewer samples) attributes. We trained SeNMo for the task of overall survival using pan-cancer data involving 33 cancer sites from Genomics Data Commons (GDC). The training data includes gene expression, DNA methylation, miRNA expression, DNA mutations, protein expression modalities, and clinical data. We evaluated the model's performance in predicting overall survival using concordance index (C-Index). SeNMo performed consistently well in training regime, with the validation C-Index of 0.76 on GDC's public data. In the testing regime, SeNMo performed with a C-Index of 0.758 on a held-out test set. The model showed an average accuracy of 99.8% on the task of classifying the primary cancer type on the pan-cancer test cohort. SeNMo proved to be a mini-foundation model for multi-omics oncology data because it demonstrated robust performance, and adaptability not only across molecular data types but also on the classification task of predicting the primary cancer type of patients. SeNMo can be further scaled to any cancer site and molecular data type. We believe SeNMo and similar models are poised to transform the oncology landscape, offering hope for more effective, efficient, and patient-centric cancer care.","sentences":["Multi-omics research has enhanced our understanding of cancer heterogeneity and progression.","Investigating molecular data through multi-omics approaches is crucial for unraveling the complex biological mechanisms underlying cancer, thereby enabling effective diagnosis, treatment, and prevention strategies.","However, predicting patient outcomes through integration of all available multi-omics data is an under-study research direction.","Here, we present SeNMo (Self-normalizing Network for Multi-omics), a deep neural network trained on multi-omics data across 33 cancer types.","SeNMo is efficient in handling multi-omics data characterized by high-width (many features) and low-length (fewer samples) attributes.","We trained SeNMo for the task of overall survival using pan-cancer data involving 33 cancer sites from Genomics Data Commons (GDC).","The training data includes gene expression, DNA methylation, miRNA expression, DNA mutations, protein expression modalities, and clinical data.","We evaluated the model's performance in predicting overall survival using concordance index (C-Index).","SeNMo performed consistently well in training regime, with the validation C-Index of 0.76 on GDC's public data.","In the testing regime, SeNMo performed with a C-Index of 0.758 on a held-out test set.","The model showed an average accuracy of 99.8% on the task of classifying the primary cancer type on the pan-cancer test cohort.","SeNMo proved to be a mini-foundation model for multi-omics oncology data because it demonstrated robust performance, and adaptability not only across molecular data types but also on the classification task of predicting the primary cancer type of patients.","SeNMo can be further scaled to any cancer site and molecular data type.","We believe SeNMo and similar models are poised to transform the oncology landscape, offering hope for more effective, efficient, and patient-centric cancer care."],"url":"http://arxiv.org/abs/2405.08226v1","category":"cs.LG"}
{"created":"2024-05-13 22:04:10","title":"Toward Automated Programming for Robotic Assembly Using ChatGPT","abstract":"Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code. This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code. In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry. We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions. We outline the architecture of this system and strategies for task decomposition and code generation. Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project.","sentences":["Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code.","This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code.","In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry.","We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions.","We outline the architecture of this system and strategies for task decomposition and code generation.","Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project."],"url":"http://arxiv.org/abs/2405.08216v1","category":"cs.RO"}
{"created":"2024-05-13 21:33:58","title":"Interactive Lab Notebooks for Robotics Researchers","abstract":"Interactive notebooks, such as Jupyter, have revolutionized the field of data science by providing an integrated environment for data, code, and documentation. However, their adoption by robotics researchers and model developers has been limited. This study investigates the logging and record-keeping practices of robotics researchers, drawing parallels to the pre-interactive notebook era of data science. Through interviews with robotics researchers, we identified the reliance on diverse and often incompatible tools for managing experimental data, leading to challenges in reproducibility and data traceability. Our findings reveal that robotics researchers can benefit from a specialized version of interactive notebooks that supports comprehensive data entry, continuous context capture, and agile data staging. We propose extending interactive notebooks to better serve the needs of robotics researchers by integrating features akin to traditional lab notebooks. This adaptation aims to enhance the organization, analysis, and reproducibility of experimental data in robotics, fostering a more streamlined and efficient research workflow.","sentences":["Interactive notebooks, such as Jupyter, have revolutionized the field of data science by providing an integrated environment for data, code, and documentation.","However, their adoption by robotics researchers and model developers has been limited.","This study investigates the logging and record-keeping practices of robotics researchers, drawing parallels to the pre-interactive notebook era of data science.","Through interviews with robotics researchers, we identified the reliance on diverse and often incompatible tools for managing experimental data, leading to challenges in reproducibility and data traceability.","Our findings reveal that robotics researchers can benefit from a specialized version of interactive notebooks that supports comprehensive data entry, continuous context capture, and agile data staging.","We propose extending interactive notebooks to better serve the needs of robotics researchers by integrating features akin to traditional lab notebooks.","This adaptation aims to enhance the organization, analysis, and reproducibility of experimental data in robotics, fostering a more streamlined and efficient research workflow."],"url":"http://arxiv.org/abs/2405.08200v1","category":"cs.CE"}
{"created":"2024-05-13 20:58:13","title":"An adaptive enrichment design using Bayesian model averaging for selection and threshold-identification of tailoring variables","abstract":"Precision medicine stands as a transformative approach in healthcare, offering tailored treatments that can enhance patient outcomes and reduce healthcare costs. As understanding of complex disease improves, clinical trials are being designed to detect subgroups of patients with enhanced treatment effects. Biomarker-driven adaptive enrichment designs, which enroll a general population initially and later restrict accrual to treatment-sensitive patients, are gaining popularity. Current practice often assumes either pre-trial knowledge of biomarkers defining treatment-sensitive subpopulations or a simple, linear relationship between continuous markers and treatment effectiveness. Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design which identifies important tailoring variables out of a larger set of candidate biomarkers. Our proposed design is equipped with a flexible modelling framework where the effects of continuous biomarkers are introduced using free knot B-splines. The parameters of interest are then estimated by marginalizing over the space of all possible variable combinations using Bayesian model averaging. At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination due to efficacy or futility and restricting future enrollment to treatment-sensitive patients. We consider pre-categorized and continuous biomarkers, the latter of which may have complex, nonlinear relationships to the outcome and treatment effect. Using simulations, we derive the operating characteristics of our design and compare its performance to two existing approaches.","sentences":["Precision medicine stands as a transformative approach in healthcare, offering tailored treatments that can enhance patient outcomes and reduce healthcare costs.","As understanding of complex disease improves, clinical trials are being designed to detect subgroups of patients with enhanced treatment effects.","Biomarker-driven adaptive enrichment designs, which enroll a general population initially and later restrict accrual to treatment-sensitive patients, are gaining popularity.","Current practice often assumes either pre-trial knowledge of biomarkers defining treatment-sensitive subpopulations or a simple, linear relationship between continuous markers and treatment effectiveness.","Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design which identifies important tailoring variables out of a larger set of candidate biomarkers.","Our proposed design is equipped with a flexible modelling framework where the effects of continuous biomarkers are introduced using free knot B-splines.","The parameters of interest are then estimated by marginalizing over the space of all possible variable combinations using Bayesian model averaging.","At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination due to efficacy or futility and restricting future enrollment to treatment-sensitive patients.","We consider pre-categorized and continuous biomarkers, the latter of which may have complex, nonlinear relationships to the outcome and treatment effect.","Using simulations, we derive the operating characteristics of our design and compare its performance to two existing approaches."],"url":"http://arxiv.org/abs/2405.08180v1","category":"stat.ME"}
{"created":"2024-05-13 20:56:49","title":"A Theoretical Framework for Self-Gravitating k-Form Boson Stars with Internal Symmetries","abstract":"Current boson star models are largely restricted to global symmetries and lower spin fields. In this work, we generalize these systems of self-gravitating bosonic fields to allow for arbitrary totally antisymmetric tensor fields and arbitrary internal gauge symmetries. We construct a generalized formalism for Yang-Mills-like theories, which allows for arbitrary k-form fields, instead of just vector fields. The k-form fields have gauge symmetries described by semisimple, compact Lie groups. We further derive equations of motion for the k-form fields and connection coefficients of the Lie group. Extensions and applications are also discussed. We present a novel way to fix the group connection using a spacetime connection. As an example, we derive explicitly the connection coefficients for SU(2) in a spherically symmetric spacetime using rectangular vielbeins. The combination of methods presented leads to a powerful, adaptable and practical framework. As a proof of concept, we derive ordinary differential equations for a 0-form field with a SU(2) symmetry. Our framework can be used to model self-gravitating (multi) particle states with internal symmetries, such as pion condensates or dark matter. It is also suited as a tool to approach open problems in modified gravity and string theory.","sentences":["Current boson star models are largely restricted to global symmetries and lower spin fields.","In this work, we generalize these systems of self-gravitating bosonic fields to allow for arbitrary totally antisymmetric tensor fields and arbitrary internal gauge symmetries.","We construct a generalized formalism for Yang-Mills-like theories, which allows for arbitrary k-form fields, instead of just vector fields.","The k-form fields have gauge symmetries described by semisimple, compact Lie groups.","We further derive equations of motion for the k-form fields and connection coefficients of the Lie group.","Extensions and applications are also discussed.","We present a novel way to fix the group connection using a spacetime connection.","As an example, we derive explicitly the connection coefficients for SU(2) in a spherically symmetric spacetime using rectangular vielbeins.","The combination of methods presented leads to a powerful, adaptable and practical framework.","As a proof of concept, we derive ordinary differential equations for a 0-form field with a SU(2) symmetry.","Our framework can be used to model self-gravitating (multi) particle states with internal symmetries, such as pion condensates or dark matter.","It is also suited as a tool to approach open problems in modified gravity and string theory."],"url":"http://arxiv.org/abs/2405.08178v1","category":"gr-qc"}
{"created":"2024-05-13 20:05:52","title":"Efficient Spin-Adapted Implementation of Multireference Algebraic Diagrammatic Construction Theory. I. Core-Ionized States and X-Ray Photoelectron Spectra","abstract":"We present an efficient implementation of multireference algebraic diagrammatic construction theory (MR-ADC) for simulating core-ionized states and X-ray photoelectron spectra (XPS). Taking advantage of spin adaptation, automatic code generation, and density fitting, our implementation can perform calculations for molecules with more than 1500 molecular orbitals, incorporating static and dynamic correlation in the ground and excited electronic states. We demonstrate the capabilities of MR-ADC methods by simulating the XPS spectra of substituted ferrocene complexes and azobenzene isomers. For the ground electronic states of these molecules, the XPS spectra computed using the extended second-order MR-ADC method (MR-ADC(2)-X) are in excellent agreement with available experimental results. We further show that MR-ADC can be used as a tool for interpreting or predicting the results of time-resolved XPS measurements by simulating the core ionization spectra of azobenzene along its photoisomerization, including the XPS signatures of excited states and the minimum energy conical intersection. This work is the first in a series of publications reporting the efficient implementations of MR-ADC methods.","sentences":["We present an efficient implementation of multireference algebraic diagrammatic construction theory (MR-ADC) for simulating core-ionized states and X-ray photoelectron spectra (XPS).","Taking advantage of spin adaptation, automatic code generation, and density fitting, our implementation can perform calculations for molecules with more than 1500 molecular orbitals, incorporating static and dynamic correlation in the ground and excited electronic states.","We demonstrate the capabilities of MR-ADC methods by simulating the XPS spectra of substituted ferrocene complexes and azobenzene isomers.","For the ground electronic states of these molecules, the XPS spectra computed using the extended second-order MR-ADC method (MR-ADC(2)-X) are in excellent agreement with available experimental results.","We further show that MR-ADC can be used as a tool for interpreting or predicting the results of time-resolved XPS measurements by simulating the core ionization spectra of azobenzene along its photoisomerization, including the XPS signatures of excited states and the minimum energy conical intersection.","This work is the first in a series of publications reporting the efficient implementations of MR-ADC methods."],"url":"http://arxiv.org/abs/2405.08161v1","category":"physics.chem-ph"}
{"created":"2024-05-13 19:55:05","title":"Mechanical memories in solids, from disorder to design","abstract":"Solids are rigid, which means that when left undisturbed, their structures are nearly static. It follows that these structures depend on history -- but it is surprising that they hold readable memories of past events. Here we review the research that has recently flourished around mechanical memory formation, beginning with amorphous solids' various memories of deformation and mesoscopic models based on particle rearrangements. We describe how these concepts apply to a much wider range of solids and glassy matter -- and how they are a bridge to memory and physical computing in mechanical metamaterials. An understanding of memory in all these solids can potentially be the basis for designing or training functionality into materials. Just as important is memory's value for understanding matter whenever it is complex, frustrated, and out of equilibrium.","sentences":["Solids are rigid, which means that when left undisturbed, their structures are nearly static.","It follows that these structures depend on history -- but it is surprising that they hold readable memories of past events.","Here we review the research that has recently flourished around mechanical memory formation, beginning with amorphous solids' various memories of deformation and mesoscopic models based on particle rearrangements.","We describe how these concepts apply to a much wider range of solids and glassy matter -- and how they are a bridge to memory and physical computing in mechanical metamaterials.","An understanding of memory in all these solids can potentially be the basis for designing or training functionality into materials.","Just as important is memory's value for understanding matter whenever it is complex, frustrated, and out of equilibrium."],"url":"http://arxiv.org/abs/2405.08158v1","category":"cond-mat.soft"}
{"created":"2024-05-13 19:51:20","title":"Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness","abstract":"Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.","sentences":["Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks.","However, LLM is sensitive to the selection of demonstrations.","To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database.","Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks.","This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain.","Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain.","However, such knowledge is common in the real world.","Finally, exploring the self-awareness ability is also crucial for the RAL system.","So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference).","We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness.","To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities.","Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets."],"url":"http://arxiv.org/abs/2405.08151v1","category":"cs.CL"}
{"created":"2024-05-13 18:00:01","title":"Direct and Efficient Detection of Quantum Superposition","abstract":"One of the most striking quantum phenomena is superposition, where one particle simultaneously inhabits different states. Most methods to verify coherent superposition are indirect, in that they require the distinct states to be recombined. Here, we adapt an XOR game, in which separated parties measure different parts of a superposed particle, and use it to verify superpositions with \\textit{local measurements} and a second independent particle. We then turn this game into a resource-efficient verification scheme, obtaining a confidence that the particle is superposed which approaches unity exponentially fast. We demonstrate our scheme using a single photon, obtaining a 99\\% confidence that the particle is superposed with only 37 copies. Our work shows the utility of XOR games to verify quantum resources, allowing us to efficiently detect quantum superposition without reinterfering the superposed states.","sentences":["One of the most striking quantum phenomena is superposition, where one particle simultaneously inhabits different states.","Most methods to verify coherent superposition are indirect, in that they require the distinct states to be recombined.","Here, we adapt an XOR game, in which separated parties measure different parts of a superposed particle, and use it to verify superpositions with \\textit{local measurements} and a second independent particle.","We then turn this game into a resource-efficient verification scheme, obtaining a confidence that the particle is superposed which approaches unity exponentially fast.","We demonstrate our scheme using a single photon, obtaining a 99\\% confidence that the particle is superposed with only 37 copies.","Our work shows the utility of XOR games to verify quantum resources, allowing us to efficiently detect quantum superposition without reinterfering the superposed states."],"url":"http://arxiv.org/abs/2405.08065v1","category":"quant-ph"}
{"created":"2024-05-13 17:56:13","title":"Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning","abstract":"As humans, we aspire to create media content that is both freely willed and readily controlled. Thanks to the prominent development of generative techniques, we now can easily utilize 2D diffusion methods to synthesize images controlled by raw sketch or designated human poses, and even progressively edit/regenerate local regions with masked inpainting. However, similar workflows in 3D modeling tasks are still unavailable due to the lack of controllability and efficiency in 3D generation. In this paper, we present a novel controllable and interactive 3D assets modeling framework, named Coin3D. Coin3D allows users to control the 3D generation using a coarse geometry proxy assembled from basic shapes, and introduces an interactive generation workflow to support seamless local part editing while delivering responsive 3D object previewing within a few seconds. To this end, we develop several techniques, including the 3D adapter that applies volumetric coarse shape control to the diffusion model, proxy-bounded editing strategy for precise part editing, progressive volume cache to support responsive preview, and volume-SDS to ensure consistent mesh reconstruction. Extensive experiments of interactive generation and editing on diverse shape proxies demonstrate that our method achieves superior controllability and flexibility in the 3D assets generation task.","sentences":["As humans, we aspire to create media content that is both freely willed and readily controlled.","Thanks to the prominent development of generative techniques, we now can easily utilize 2D diffusion methods to synthesize images controlled by raw sketch or designated human poses, and even progressively edit/regenerate local regions with masked inpainting.","However, similar workflows in 3D modeling tasks are still unavailable due to the lack of controllability and efficiency in 3D generation.","In this paper, we present a novel controllable and interactive 3D assets modeling framework, named Coin3D. Coin3D allows users to control the 3D generation using a coarse geometry proxy assembled from basic shapes, and introduces an interactive generation workflow to support seamless local part editing while delivering responsive 3D object previewing within a few seconds.","To this end, we develop several techniques, including the 3D adapter that applies volumetric coarse shape control to the diffusion model, proxy-bounded editing strategy for precise part editing, progressive volume cache to support responsive preview, and volume-SDS to ensure consistent mesh reconstruction.","Extensive experiments of interactive generation and editing on diverse shape proxies demonstrate that our method achieves superior controllability and flexibility in the 3D assets generation task."],"url":"http://arxiv.org/abs/2405.08054v1","category":"cs.GR"}
{"created":"2024-05-14 17:13:50","title":"Jacobian Regularizer-based Neural Granger Causality","abstract":"With the advancement of neural networks, diverse methods for neural Granger causality have emerged, which demonstrate proficiency in handling complex data, and nonlinear relationships. However, the existing framework of neural Granger causality has several limitations. It requires the construction of separate predictive models for each target variable, and the relationship depends on the sparsity on the weights of the first layer, resulting in challenges in effectively modeling complex relationships between variables as well as unsatisfied estimation accuracy of Granger causality. Moreover, most of them cannot grasp full-time Granger causality. To address these drawbacks, we propose a Jacobian Regularizer-based Neural Granger Causality (JRNGC) approach, a straightforward yet highly effective method for learning multivariate summary Granger causality and full-time Granger causality by constructing a single model for all target variables. Specifically, our method eliminates the sparsity constraints of weights by leveraging an input-output Jacobian matrix regularizer, which can be subsequently represented as the weighted causal matrix in the post-hoc analysis. Extensive experiments show that our proposed approach achieves competitive performance with the state-of-the-art methods for learning summary Granger causality and full-time Granger causality while maintaining lower model complexity and high scalability.","sentences":["With the advancement of neural networks, diverse methods for neural Granger causality have emerged, which demonstrate proficiency in handling complex data, and nonlinear relationships.","However, the existing framework of neural Granger causality has several limitations.","It requires the construction of separate predictive models for each target variable, and the relationship depends on the sparsity on the weights of the first layer, resulting in challenges in effectively modeling complex relationships between variables as well as unsatisfied estimation accuracy of Granger causality.","Moreover, most of them cannot grasp full-time Granger causality.","To address these drawbacks, we propose a Jacobian Regularizer-based Neural Granger Causality (JRNGC) approach, a straightforward yet highly effective method for learning multivariate summary Granger causality and full-time Granger causality by constructing a single model for all target variables.","Specifically, our method eliminates the sparsity constraints of weights by leveraging an input-output Jacobian matrix regularizer, which can be subsequently represented as the weighted causal matrix in the post-hoc analysis.","Extensive experiments show that our proposed approach achieves competitive performance with the state-of-the-art methods for learning summary Granger causality and full-time Granger causality while maintaining lower model complexity and high scalability."],"url":"http://arxiv.org/abs/2405.08779v1","category":"cs.LG"}
{"created":"2024-05-14 17:12:17","title":"Quantum Integrable Systems arising from Separation of Variables on S3","abstract":"We study the family of quantum integrable systems that arise from separating the Schr\\\"odinger equation in all 6 separable orthogonal coordinates on the 3 sphere: ellipsoidal, prolate, oblate, Lam\\'{e}, spherical and cylindrical. On the one hand each separating coordinate system gives rise to a quantum integrable system on S2 x S2, on the other hand it also leads to families of harmonic polynomials in R4. We show that separation in ellipsoidal coordinates yields a generalised Lam\\'{e} equation - a Fuchsian ODE with 5 regular singular points. We seek polynomial solutions so that the eigenfunctions are analytic at all finite singularities. We classify eigenfunctions by their discrete symmetry and compute the joint spectrum for each symmetry class. The latter 5 separable coordinate systems are all degenerations of the ellipsoidal coordinates. We perform similar analyses on these systems and show how the ODEs degenerate in a fashion akin to their respective coordinates. For the prolate system we show that there exists a defect in the joint spectrum which prohibits a global assignment of quantum numbers: the system has quantum monodromy. This is a companion paper to our previous work where the respective classical systems were studied.","sentences":["We study the family of quantum integrable systems that arise from separating the Schr\\\"odinger equation in all 6 separable orthogonal coordinates on the 3 sphere: ellipsoidal, prolate, oblate, Lam\\'{e}, spherical and cylindrical.","On the one hand each separating coordinate system gives rise to a quantum integrable system on S2 x S2, on the other hand it also leads to families of harmonic polynomials in R4.","We show that separation in ellipsoidal coordinates yields a generalised Lam\\'{e} equation - a Fuchsian ODE with 5 regular singular points.","We seek polynomial solutions so that the eigenfunctions are analytic at all finite singularities.","We classify eigenfunctions by their discrete symmetry and compute the joint spectrum for each symmetry class.","The latter 5 separable coordinate systems are all degenerations of the ellipsoidal coordinates.","We perform similar analyses on these systems and show how the ODEs degenerate in a fashion akin to their respective coordinates.","For the prolate system we show that there exists a defect in the joint spectrum which prohibits a global assignment of quantum numbers: the system has quantum monodromy.","This is a companion paper to our previous work where the respective classical systems were studied."],"url":"http://arxiv.org/abs/2405.08778v1","category":"math-ph"}
{"created":"2024-05-14 17:12:11","title":"Daydreaming Hopfield Networks and their surprising effectiveness on correlated data","abstract":"To improve the storage capacity of the Hopfield model, we develop a version of the dreaming algorithm that perpetually reinforces the patterns to be stored (as in the Hebb rule), and erases the spurious memories (as in dreaming algorithms). For this reason, we called it Daydreaming. Daydreaming is not destructive and it converges asymptotically to stationary retrieval maps. When trained on random uncorrelated examples, the model shows optimal performance in terms of the size of the basins of attraction of stored examples and the quality of reconstruction. We also train the Daydreaming algorithm on correlated data obtained via the random-features model and argue that it spontaneously exploits the correlations thus increasing even further the storage capacity and the size of the basins of attraction. Moreover, the Daydreaming algorithm is also able to stabilize the features hidden in the data. Finally, we test Daydreaming on the MNIST dataset and show that it still works surprisingly well, producing attractors that are close to unseen examples and class prototypes.","sentences":["To improve the storage capacity of the Hopfield model, we develop a version of the dreaming algorithm that perpetually reinforces the patterns to be stored (as in the Hebb rule), and erases the spurious memories (as in dreaming algorithms).","For this reason, we called it Daydreaming.","Daydreaming is not destructive and it converges asymptotically to stationary retrieval maps.","When trained on random uncorrelated examples, the model shows optimal performance in terms of the size of the basins of attraction of stored examples and the quality of reconstruction.","We also train the Daydreaming algorithm on correlated data obtained via the random-features model and argue that it spontaneously exploits the correlations thus increasing even further the storage capacity and the size of the basins of attraction.","Moreover, the Daydreaming algorithm is also able to stabilize the features hidden in the data.","Finally, we test Daydreaming on the MNIST dataset and show that it still works surprisingly well, producing attractors that are close to unseen examples and class prototypes."],"url":"http://arxiv.org/abs/2405.08777v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-14 17:08:55","title":"Generating 4-dimensional Wormholes with Yang-Mills Casimir Sources","abstract":"This work presents a new wormhole solution in General Relativity supported by the quantum vacuum fluctuations of the Casimir effect between perfect chromometallic mirrors in $(3+1)$ dimensions, which was recently fitted using first-principle numerical simulations. Initially, we employ a perturbative approach for $x = m r \\ll 1$, where $m$ represents the Casimir mass. This approach has proven to be a reasonable approximation when compared with the exact case in this regime. To find well-behaved redshift functions, we impose constraints on the free parameters. As expected, this solution recovers the electromagnetic-like Casimir solution for $m = 0$. Analyzing the traversability conditions, we graphically find that all will be satisfied for $ 0 \\leq m \\leq 0.20$. On the other hand, all the energy conditions are violated, as usual in this context. Stability from Tolman-Oppenheimer-Volkov (TOV) equation is guaranteed for all $r$ and from the speed of sound for $0.16 \\le m \\le 0.18$. Therefore, for $0.16 \\leq m \\leq 0.18$, we will have a stable solution that satisfies all traversability conditions.","sentences":["This work presents a new wormhole solution in General Relativity supported by the quantum vacuum fluctuations of the Casimir effect between perfect chromometallic mirrors in $(3+1)$ dimensions, which was recently fitted using first-principle numerical simulations.","Initially, we employ a perturbative approach for $x = m r \\ll 1$, where $m$ represents the Casimir mass.","This approach has proven to be a reasonable approximation when compared with the exact case in this regime.","To find well-behaved redshift functions, we impose constraints on the free parameters.","As expected, this solution recovers the electromagnetic-like Casimir solution for $m = 0$.","Analyzing the traversability conditions, we graphically find that all will be satisfied for $ 0 \\leq","m \\leq 0.20$.","On the other hand, all the energy conditions are violated, as usual in this context.","Stability from Tolman-Oppenheimer-Volkov (TOV) equation is guaranteed for all $r$ and from the speed of sound for $0.16 \\le m \\le 0.18$.","Therefore, for $0.16 \\leq m \\leq 0.18$, we will have a stable solution that satisfies all traversability conditions."],"url":"http://arxiv.org/abs/2405.08774v1","category":"gr-qc"}
{"created":"2024-05-14 17:02:10","title":"Scalar Field Perturbation of Hairy Black Holes in EsGB theory","abstract":"We investigate scalar field perturbations of the hairy black holes involved with spontaneous symmetry breaking of the global U(1) symmetry in Einstein-scalar-Gauss-Bonnet theory for asymptotically flat spacetimes. We consider the mechanism that black holes without hairs become unstable at the critical point of the coupling constant and undergo a phase transition to hairy black holes in the symmetry-broken phase driven by spontaneous symmetry breaking. This transition occurs near the black hole horizon due to the diminishing influence of the Gauss-Bonnet term at infinity. To examine such process, we introduce a scalar field perturbation on the newly formed background spacetime. We solve the linearized perturbation equation using Green's function method. We begin by solving the Green's function, incorporating the branch cut contribution. This allows us to analytically investigate the late-time behavior of the perturbation at both spatial and null infinity. We found that the late-time behavior only differs from the Schwarzschild black hole by a mass term. We then proceed to calculate the quasinormal modes (QNMs) numerically, which arise from the presence of poles in the Green's function. Our primary interest lies in utilizing QNMs to investigate the stability of the black hole solutions both the symmetric and symmetry-broken phases. Consistent with the prior study, our analysis shows that hairy black holes in the symmetric phase become unstable when the quadratic coupling constant exceeds a critical value for a fixed value of the quartic coupling constant. In contrast, hairy black holes in the symmetry-broken phase are always stable at the critical value. These numerical results provide strong evidence for a dynamical process that unstable black holes without hairs transition into stable hairy black holes in the symmetry-broken phase through the spontaneous symmetry breaking.","sentences":["We investigate scalar field perturbations of the hairy black holes involved with spontaneous symmetry breaking of the global U(1) symmetry in Einstein-scalar-Gauss-Bonnet theory for asymptotically flat spacetimes.","We consider the mechanism that black holes without hairs become unstable at the critical point of the coupling constant and undergo a phase transition to hairy black holes in the symmetry-broken phase driven by spontaneous symmetry breaking.","This transition occurs near the black hole horizon due to the diminishing influence of the Gauss-Bonnet term at infinity.","To examine such process, we introduce a scalar field perturbation on the newly formed background spacetime.","We solve the linearized perturbation equation using Green's function method.","We begin by solving the Green's function, incorporating the branch cut contribution.","This allows us to analytically investigate the late-time behavior of the perturbation at both spatial and null infinity.","We found that the late-time behavior only differs from the Schwarzschild black hole by a mass term.","We then proceed to calculate the quasinormal modes (QNMs) numerically, which arise from the presence of poles in the Green's function.","Our primary interest lies in utilizing QNMs to investigate the stability of the black hole solutions both the symmetric and symmetry-broken phases.","Consistent with the prior study, our analysis shows that hairy black holes in the symmetric phase become unstable when the quadratic coupling constant exceeds a critical value for a fixed value of the quartic coupling constant.","In contrast, hairy black holes in the symmetry-broken phase are always stable at the critical value.","These numerical results provide strong evidence for a dynamical process that unstable black holes without hairs transition into stable hairy black holes in the symmetry-broken phase through the spontaneous symmetry breaking."],"url":"http://arxiv.org/abs/2405.08769v1","category":"hep-th"}
{"created":"2024-05-14 16:56:49","title":"A Generalized Curvilinear Coordinate system-based Patch Dynamics Scheme in Equation-free Multiscale Modelling","abstract":"The patch dynamics scheme in equation-free multiscale modelling can efficiently predict the macroscopic behaviours by simulating the microscale problem in a fraction of the space-time domain. The patch dynamics schemes developed so far, are mainly on rectangular domains with uniform grids and uniform rectangular patches. In real-life problems where the geometry of the domain is not regular or simple, rectangular and uniform grids or patches may not be useful. To address this kind of complexity, the concept of a generalized curvilinear coordinate system is used. An explicit representation of a patch dynamics scheme on a generalized curvilinear coordinate system in a two-dimensional domain is proposed for evolution equations. It has been applied to unsteady convection-diffusion-reaction (CDR) problems. The robustness of the scheme on the generalized curvilinear coordinate system is assessed through numerical test cases. Firstly, a convection-dominated CDR equation is considered, featuring high gradient regions in some part of the domain, for which stretched grids with non-uniform patch sizes are employed. Secondly, a non-axisymmetric diffusion equation is examined in an annulus region, where the patches have non-rectangular shapes. The results obtained demonstrate excellent agreement with the analytical solution or existing numerical solutions.","sentences":["The patch dynamics scheme in equation-free multiscale modelling can efficiently predict the macroscopic behaviours by simulating the microscale problem in a fraction of the space-time domain.","The patch dynamics schemes developed so far, are mainly on rectangular domains with uniform grids and uniform rectangular patches.","In real-life problems where the geometry of the domain is not regular or simple, rectangular and uniform grids or patches may not be useful.","To address this kind of complexity, the concept of a generalized curvilinear coordinate system is used.","An explicit representation of a patch dynamics scheme on a generalized curvilinear coordinate system in a two-dimensional domain is proposed for evolution equations.","It has been applied to unsteady convection-diffusion-reaction (CDR) problems.","The robustness of the scheme on the generalized curvilinear coordinate system is assessed through numerical test cases.","Firstly, a convection-dominated CDR equation is considered, featuring high gradient regions in some part of the domain, for which stretched grids with non-uniform patch sizes are employed.","Secondly, a non-axisymmetric diffusion equation is examined in an annulus region, where the patches have non-rectangular shapes.","The results obtained demonstrate excellent agreement with the analytical solution or existing numerical solutions."],"url":"http://arxiv.org/abs/2405.08764v1","category":"math.NA"}
{"created":"2024-05-14 16:44:06","title":"Local well-posedness and regularity properties for an initial-boundary value problem associated to the fifth order Korteweg-de Vries equation","abstract":"In this work we prove that the initial-boundary value problem (IBVP) for the fifth order Korteweg-de Vries equation \\begin{align*} \\left. \\begin{array}{rlr} u_t+\\partial_x^5 u+u\\partial_x u&\\hspace{-2mm}=0,&\\quad x\\in\\mathbb R^+,\\; t\\in\\mathbb R^+,\\\\ u(x,0)&\\hspace{-2mm}=g(x),&\\\\ u(0,t)=h_1(t),\\, \\partial_x u(0,t)&\\hspace{-2mm}=h_2(t),\\,\\partial_x^2 u(0,t)=h_3(t), \\end{array} \\right\\} \\end{align*} is locally well posed, when the data $g$, $h_1$, $h_2$, $h_3$ are taken in such a way that $g\\in H^s(\\mathbb R_x^+)$, and $h_{j+1}\\in H^{\\frac{s+2-j}5}(\\mathbb R_t^+)$, $j=0,1,2$, $s\\in [0,\\frac{11}4)\\setminus \\{\\frac12,\\frac32,\\frac52\\}$, and satisfy the following compatibility conditions: \\begin{align*} g(0)=h_1(0) \\text{ if } \\frac12<s<\\frac32;\\\\ g(0)=h_1(0),\\; g'(0)=h_2(0) \\text{ if } \\frac32<s<\\frac52;\\\\ g(0)=h_1(0), \\; g'(0)=h_2(0),\\; g''(0)=h_3(0) \\text{ if } \\frac52<s<\\frac{11}4. \\end{align*} Besides, we prove that the nonlinear part of the solution is smoother than the initial datum $g$.","sentences":["In this work we prove that the initial-boundary value problem (IBVP) for the fifth order Korteweg-de Vries equation \\begin{align*} \\left.","\\begin{array}{rlr} u_t+\\partial_x^5 u+u\\partial_x u&\\hspace{-2mm}=0,&\\quad","x\\in\\mathbb R^+,\\; t\\in\\mathbb R^+,\\\\ u(x,0)&\\hspace{-2mm}=g(x),&\\\\ u(0,t)=h_1(t),\\, \\partial_x u(0,t)&\\hspace{-2mm}=h_2(t),\\,\\partial_x^2 u(0,t)=h_3(t), \\end{array} \\right\\} \\end{align*} is locally well posed, when the data $g$, $h_1$, $h_2$, $h_3$ are taken in such a way that $g\\in H^s(\\mathbb R_x^+)$, and $h_{j+1}\\in H^{\\frac{s+2-j}5}(\\mathbb R_t^+)$, $j=0,1,2$, $s\\in [0,\\frac{11}4)\\setminus \\{\\frac12,\\frac32,\\frac52\\}$, and satisfy the following compatibility conditions: \\begin{align*} g(0)=h_1(0) \\text{ if } \\frac12<s<\\frac32;\\\\ g(0)=h_1(0),\\; g'(0)=h_2(0)","\\text{ if } \\frac32<s<\\frac52;\\\\ g(0)=h_1(0), \\; g'(0)=h_2(0),\\; g''(0)=h_3(0)","\\text{ if } \\frac52<s<\\frac{11}4.","\\end{align*} Besides, we prove that the nonlinear part of the solution is smoother than the initial datum $g$."],"url":"http://arxiv.org/abs/2405.08757v1","category":"math.AP"}
{"created":"2024-05-14 16:23:04","title":"Polytropic Dynamical Systems with Time Singularity","abstract":"In this paper we consider a class of second order singular homogeneous differential equations called the Lane-Emden-type with time singularity in the drift coefficient. Lane-Emden equations are singular initial value problems that model phenomena in astrophysics such as stellar structure and are governed by polytropics with applications in isothermal gas spheres. A hybrid method is proposed to approximate the solution of this type of dynamic equations.","sentences":["In this paper we consider a class of second order singular homogeneous differential equations called the Lane-Emden-type with time singularity in the drift coefficient.","Lane-Emden equations are singular initial value problems that model phenomena in astrophysics such as stellar structure and are governed by polytropics with applications in isothermal gas spheres.","A hybrid method is proposed to approximate the solution of this type of dynamic equations."],"url":"http://arxiv.org/abs/2405.08736v1","category":"math.DS"}
{"created":"2024-05-14 15:42:27","title":"Using autoencoders and deep transfer learning to determine the stellar parameters of 286 CARMENES M dwarfs","abstract":"Deep learning (DL) techniques are a promising approach among the set of methods used in the ever-challenging determination of stellar parameters in M dwarfs. In this context, transfer learning could play an important role in mitigating uncertainties in the results due to the synthetic gap (i.e. difference in feature distributions between observed and synthetic data). We propose a feature-based deep transfer learning (DTL) approach based on autoencoders to determine stellar parameters from high-resolution spectra. Using this methodology, we provide new estimations for the effective temperature, surface gravity, metallicity, and projected rotational velocity for 286 M dwarfs observed by the CARMENES survey. Using autoencoder architectures, we projected synthetic PHOENIX-ACES spectra and observed CARMENES spectra onto a new feature space of lower dimensionality in which the differences between the two domains are reduced. We used this low-dimensional new feature space as input for a convolutional neural network to obtain the stellar parameter determinations. We performed an extensive analysis of our estimated stellar parameters, ranging from 3050 to 4300 K, 4.7 to 5.1 dex, and -0.53 to 0.25 dex for Teff, logg, and [Fe/H], respectively. Our results are broadly consistent with those of recent studies using CARMENES data, with a systematic deviation in our Teff scale towards hotter values for estimations above 3750 K. Furthermore, our methodology mitigates the deviations in metallicity found in previous DL techniques due to the synthetic gap. We consolidated a DTL-based methodology to determine stellar parameters in M dwarfs from synthetic spectra, with no need for high-quality measurements involved in the knowledge transfer. These results suggest the great potential of DTL to mitigate the differences in feature distributions between the observations and the PHOENIX-ACES spectra.","sentences":["Deep learning (DL) techniques are a promising approach among the set of methods used in the ever-challenging determination of stellar parameters in M dwarfs.","In this context, transfer learning could play an important role in mitigating uncertainties in the results due to the synthetic gap (i.e. difference in feature distributions between observed and synthetic data).","We propose a feature-based deep transfer learning (DTL) approach based on autoencoders to determine stellar parameters from high-resolution spectra.","Using this methodology, we provide new estimations for the effective temperature, surface gravity, metallicity, and projected rotational velocity for 286 M dwarfs observed by the CARMENES survey.","Using autoencoder architectures, we projected synthetic PHOENIX-ACES spectra and observed CARMENES spectra onto a new feature space of lower dimensionality in which the differences between the two domains are reduced.","We used this low-dimensional new feature space as input for a convolutional neural network to obtain the stellar parameter determinations.","We performed an extensive analysis of our estimated stellar parameters, ranging from 3050 to 4300 K, 4.7 to 5.1 dex, and -0.53 to 0.25 dex for Teff, logg, and [Fe/H], respectively.","Our results are broadly consistent with those of recent studies using CARMENES data, with a systematic deviation in our Teff scale towards hotter values for estimations above 3750 K. Furthermore, our methodology mitigates the deviations in metallicity found in previous DL techniques due to the synthetic gap.","We consolidated a DTL-based methodology to determine stellar parameters in M dwarfs from synthetic spectra, with no need for high-quality measurements involved in the knowledge transfer.","These results suggest the great potential of DTL to mitigate the differences in feature distributions between the observations and the PHOENIX-ACES spectra."],"url":"http://arxiv.org/abs/2405.08703v1","category":"astro-ph.SR"}
{"created":"2024-05-14 15:40:09","title":"Structure and dynamics of electron-phonon coupled systems using neural quantum states","abstract":"In this work, we use neural quantum states (NQS) to describe the high-dimensional wave functions of electron-phonon coupled systems. We demonstrate that NQS can accurately and systematically learn the underlying physics of such problems through a variational Monte Carlo optimization of the energy with minimal incorporation of physical information even in highly challenging cases. We assess the ability of our approach across various lattice model examples featuring different types of couplings. The flexibility of our NQS formulation is demonstrated via application to ab initio models parametrized by density functional perturbation theory consisting of electron or hole bands coupled linearly to dispersive phonons. We compute accurate real-frequency spectral properties of electron-phonon systems via a novel formalism based on NQS. Our work establishes a general framework for exploring diverse ground state and dynamical phenomena arising in electron-phonon systems, including the non-perturbative interplay of correlated electronic and electron-phonon effects in systems ranging from simple lattice models to realistic models of materials parametrized by ab initio calculations.","sentences":["In this work, we use neural quantum states (NQS) to describe the high-dimensional wave functions of electron-phonon coupled systems.","We demonstrate that NQS can accurately and systematically learn the underlying physics of such problems through a variational Monte Carlo optimization of the energy with minimal incorporation of physical information even in highly challenging cases.","We assess the ability of our approach across various lattice model examples featuring different types of couplings.","The flexibility of our NQS formulation is demonstrated via application to ab initio models parametrized by density functional perturbation theory consisting of electron or hole bands coupled linearly to dispersive phonons.","We compute accurate real-frequency spectral properties of electron-phonon systems via a novel formalism based on NQS.","Our work establishes a general framework for exploring diverse ground state and dynamical phenomena arising in electron-phonon systems, including the non-perturbative interplay of correlated electronic and electron-phonon effects in systems ranging from simple lattice models to realistic models of materials parametrized by ab initio calculations."],"url":"http://arxiv.org/abs/2405.08701v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 15:20:57","title":"Double-activation neural network for solving parabolic equations with time delay","abstract":"This paper presents the double-activation neural network (DANN), a novel network architecture designed for solving parabolic equations with time delay. In DANN, each neuron is equipped with two activation functions to augment the network's nonlinear expressive capacity. Additionally, a new parameter is introduced for the construction of the quadratic terms in one of two activation functions, which further enhances the network's ability to capture complex nonlinear relationships. To address the issue of low fitting accuracy caused by the discontinuity of solution's derivative, a piecewise fitting approach is proposed by dividing the global solving domain into several subdomains. The convergence of the loss function is proven. Numerical results are presented to demonstrate the superior accuracy and faster convergence of DANN compared to the traditional physics-informed neural network (PINN).","sentences":["This paper presents the double-activation neural network (DANN), a novel network architecture designed for solving parabolic equations with time delay.","In DANN, each neuron is equipped with two activation functions to augment the network's nonlinear expressive capacity.","Additionally, a new parameter is introduced for the construction of the quadratic terms in one of two activation functions, which further enhances the network's ability to capture complex nonlinear relationships.","To address the issue of low fitting accuracy caused by the discontinuity of solution's derivative, a piecewise fitting approach is proposed by dividing the global solving domain into several subdomains.","The convergence of the loss function is proven.","Numerical results are presented to demonstrate the superior accuracy and faster convergence of DANN compared to the traditional physics-informed neural network (PINN)."],"url":"http://arxiv.org/abs/2405.08690v1","category":"math.NA"}
{"created":"2024-05-14 15:03:06","title":"Generalized uncertainty principle distorted quintessence dynamics","abstract":"In this paper, we invoke a generalized uncertainty principle (GUP) in the symmetry-reduced cosmological Hamiltonian for a universe driven by a quintessence scalar field with potential. Our study focuses on semi-classical regime. In particular, we derive the GUP-distorted Friedmann, Raychaudhuri, and the Klein-Gordon equation. This is followed by a systematic analysis of the qualitative dynamics for the choice of potential $V(\\phi)= V_0 \\sinh^{-n}{(\\mu \\phi)}$. This involves constructing an autonomous dynamical system of equations by choosing appropriate dynamical variables, followed by a qualitative study using linear stability theory. Our analysis shows that incorporating GUP significantly changes the existing fixed points compared to the limiting case without quantum effects by switching off the GUP.","sentences":["In this paper, we invoke a generalized uncertainty principle (GUP) in the symmetry-reduced cosmological Hamiltonian for a universe driven by a quintessence scalar field with potential.","Our study focuses on semi-classical regime.","In particular, we derive the GUP-distorted Friedmann, Raychaudhuri, and the Klein-Gordon equation.","This is followed by a systematic analysis of the qualitative dynamics for the choice of potential $V(\\phi)= V_0 \\sinh^{-n}{(\\mu \\phi)}$.","This involves constructing an autonomous dynamical system of equations by choosing appropriate dynamical variables, followed by a qualitative study using linear stability theory.","Our analysis shows that incorporating GUP significantly changes the existing fixed points compared to the limiting case without quantum effects by switching off the GUP."],"url":"http://arxiv.org/abs/2405.08680v1","category":"gr-qc"}
{"created":"2024-05-14 14:56:54","title":"Simplifying Debiased Inference via Automatic Differentiation and Probabilistic Programming","abstract":"We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience. 'Dimple' takes as input computer code representing a parameter of interest and outputs an efficient estimator. Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function. Dimple avoids this task by applying automatic differentiation to the statistical functional of interest. Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition. Dimple also uses this composition to determine the nuisances it must estimate. In software, primitives can be implemented independently of one another and reused across different estimation problems. We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code.","sentences":["We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience.","'Dimple' takes as input computer code representing a parameter of interest and outputs an efficient estimator.","Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function.","Dimple avoids this task by applying automatic differentiation to the statistical functional of interest.","Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition.","Dimple also uses this composition to determine the nuisances it must estimate.","In software, primitives can be implemented independently of one another and reused across different estimation problems.","We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code."],"url":"http://arxiv.org/abs/2405.08675v1","category":"stat.ME"}
{"created":"2024-05-14 14:51:15","title":"Quantum Circuit Model for Lattice Boltzmann Fluid Flow Simulations","abstract":"In the present contribution, we propose a quantum computational algorithm for the Lattice Boltzmann Method (LBM) to solve fluid flow equations in the low Reynolds number ($Re$) regime. Firstly, we express the LBM collision and streaming operators in matrix form. Since quantum logic gates are typically expressed as unitary matrices, we first decompose LBM operations as a product of unitaries. The particle distribution functions (PDFs) of LBM are encoded as probability amplitudes of the quantum state. We have observed that the amplitudes in the state vector (SV) can be affected: (i) by the choice of encoding the PDFs during the quantum state preparation or (ii) when collision is followed by streaming, as in classical LBM implementation. In the first case, we show that the ancilla qubit must be in superposition with the compute qubits during the quantum state preparation. The superposition allows the SV to utilize the increased Hilbert space offered by the ancilla qubit rather than placing the ancilla in a separate register, which restricts the space of possible outcomes. Next, we show that the second issue can be resolved by having an intermediate Hadamard gate before the streaming operation. The proposed algorithm has been tested through typical benchmark problems like advection-diffusion of a Gaussian hill, Poiseuille flow, Couette flow, and the lid-driven cavity problem. The results are validated with the respective analytic or reference solutions. Translating the unitaries into quantum gates (circuit synthesis) presents a primary challenge, as a unitary matrix can be decomposed in multiple ways. We report on the CNOT and U gate counts obtained for the test cases with the range of qubits from 9 to 12. Although the gate count closely agrees with the theoretical limit, the number of two qubit gates is in the $O(10^7)$ prompts special attention to circuit synthesis.","sentences":["In the present contribution, we propose a quantum computational algorithm for the Lattice Boltzmann Method (LBM) to solve fluid flow equations in the low Reynolds number ($Re$) regime.","Firstly, we express the LBM collision and streaming operators in matrix form.","Since quantum logic gates are typically expressed as unitary matrices, we first decompose LBM operations as a product of unitaries.","The particle distribution functions (PDFs) of LBM are encoded as probability amplitudes of the quantum state.","We have observed that the amplitudes in the state vector (SV) can be affected: (i) by the choice of encoding the PDFs during the quantum state preparation or (ii) when collision is followed by streaming, as in classical LBM implementation.","In the first case, we show that the ancilla qubit must be in superposition with the compute qubits during the quantum state preparation.","The superposition allows the SV to utilize the increased Hilbert space offered by the ancilla qubit rather than placing the ancilla in a separate register, which restricts the space of possible outcomes.","Next, we show that the second issue can be resolved by having an intermediate Hadamard gate before the streaming operation.","The proposed algorithm has been tested through typical benchmark problems like advection-diffusion of a Gaussian hill, Poiseuille flow, Couette flow, and the lid-driven cavity problem.","The results are validated with the respective analytic or reference solutions.","Translating the unitaries into quantum gates (circuit synthesis) presents a primary challenge, as a unitary matrix can be decomposed in multiple ways.","We report on the CNOT and U gate counts obtained for the test cases with the range of qubits from 9 to 12.","Although the gate count closely agrees with the theoretical limit, the number of two qubit gates is in the $O(10^7)$ prompts special attention to circuit synthesis."],"url":"http://arxiv.org/abs/2405.08669v1","category":"quant-ph"}
{"created":"2024-05-14 14:36:27","title":"Conformal scattering of the wave equation in the Vaidya spacetime","abstract":"We construct the conformal scattering operator for the scalar wave equation on the Vaidya spacetime using vector field methods. The spacetime we consider is Schwarzschild, near both past and future timelike infinities, in order to use existing decay results for the scalar field, ensuring our energy estimates. These estimates guarantee the injectivity of the trace operator and the closure of its range. Finally, we solve a Goursat problem for the scalar waves on null infinities, demonstrating that the range of the trace operator is dense. Consequently, this implies that the scattering operator is an isomorphism.","sentences":["We construct the conformal scattering operator for the scalar wave equation on the Vaidya spacetime using vector field methods.","The spacetime we consider is Schwarzschild, near both past and future timelike infinities, in order to use existing decay results for the scalar field, ensuring our energy estimates.","These estimates guarantee the injectivity of the trace operator and the closure of its range.","Finally, we solve a Goursat problem for the scalar waves on null infinities, demonstrating that the range of the trace operator is dense.","Consequently, this implies that the scattering operator is an isomorphism."],"url":"http://arxiv.org/abs/2405.08659v1","category":"gr-qc"}
{"created":"2024-05-14 14:35:21","title":"Self-supervised learning improves robustness of deep learning lung tumor segmentation to CT imaging differences","abstract":"Self-supervised learning (SSL) is an approach to extract useful feature representations from unlabeled data, and enable fine-tuning on downstream tasks with limited labeled examples. Self-pretraining is a SSL approach that uses the curated task dataset for both pretraining the networks and fine-tuning them. Availability of large, diverse, and uncurated public medical image sets provides the opportunity to apply SSL in the \"wild\" and potentially extract features robust to imaging variations. However, the benefit of wild- vs self-pretraining has not been studied for medical image analysis. In this paper, we compare robustness of wild versus self-pretrained transformer (vision transformer [ViT] and hierarchical shifted window [Swin]) models to computed tomography (CT) imaging differences for non-small cell lung cancer (NSCLC) segmentation. Wild-pretrained Swin models outperformed self-pretrained Swin for the various imaging acquisitions. ViT resulted in similar accuracy for both wild- and self-pretrained models. Masked image prediction pretext task that forces networks to learn the local structure resulted in higher accuracy compared to contrastive task that models global image information. Wild-pretrained models resulted in higher feature reuse at the lower level layers and feature differentiation close to output layer after fine-tuning. Hence, we conclude: Wild-pretrained networks were more robust to analyzed CT imaging differences for lung tumor segmentation than self-pretrained methods. Swin architecture benefited from such pretraining more than ViT.","sentences":["Self-supervised learning (SSL) is an approach to extract useful feature representations from unlabeled data, and enable fine-tuning on downstream tasks with limited labeled examples.","Self-pretraining is a SSL approach that uses the curated task dataset for both pretraining the networks and fine-tuning them.","Availability of large, diverse, and uncurated public medical image sets provides the opportunity to apply SSL in the \"wild\" and potentially extract features robust to imaging variations.","However, the benefit of wild- vs self-pretraining has not been studied for medical image analysis.","In this paper, we compare robustness of wild versus self-pretrained transformer (vision transformer [ViT] and hierarchical shifted window [Swin]) models to computed tomography (CT) imaging differences for non-small cell lung cancer (NSCLC) segmentation.","Wild-pretrained Swin models outperformed self-pretrained Swin for the various imaging acquisitions.","ViT resulted in similar accuracy for both wild- and self-pretrained models.","Masked image prediction pretext task that forces networks to learn the local structure resulted in higher accuracy compared to contrastive task that models global image information.","Wild-pretrained models resulted in higher feature reuse at the lower level layers and feature differentiation close to output layer after fine-tuning.","Hence, we conclude: Wild-pretrained networks were more robust to analyzed CT imaging differences for lung tumor segmentation than self-pretrained methods.","Swin architecture benefited from such pretraining more than ViT."],"url":"http://arxiv.org/abs/2405.08657v1","category":"eess.IV"}
{"created":"2024-05-14 14:35:12","title":"Unit-Constrained Data-Driven Turbulence Modeling for Separated Flows Using Symbolic Regression","abstract":"The Reynolds-averaged Navier-Stokes (RANS) method is an essential tool for turbulence research in engineering. The RANS method relies on turbulence models to close the governing equations, with Linear Eddy Viscosity Models (LEVMs) based on the Boussinesq hypothesis being prevalently utilized across engineering applications. However, the intrinsic limitations of the Boussinesq assumption render LEVMs less accurate for large separated turbulent flows, prompting the development of numerous modified LEVM variants. Recently, machine learning approaches, notably neural networks, have been employed to enhance LEVMs. Nonetheless, the \"black box\" nature of such machine learning techniques poses significant interpretability challenges in refining turbulence models. This paper introduces a novel unit-constrained turbulence modeling framework using symbolic regression to overcome these hurdles. This framework amends the constitutive equation of original LEVMs by establishing explicit equations between the Reynolds stress deviation and mean flow quantities, with unit consistency constraints bolstering the efficiency of the symbolic regression learning process. The framework's effectiveness is demonstrated through its application to the separated flow over 2D periodic hills. Numerical simulations are conducted to validate the accuracy and generalization capabilities of the learned turbulence model. Compared to the standard k-{\\epsilon} model, the learned model exhibits significantly enhanced predictive accuracy for anisotropic Reynolds stresses and flow velocities, more precisely delineates flow separation, and shows promising generalization potential.","sentences":["The Reynolds-averaged Navier-Stokes (RANS) method is an essential tool for turbulence research in engineering.","The RANS method relies on turbulence models to close the governing equations, with Linear Eddy Viscosity Models (LEVMs) based on the Boussinesq hypothesis being prevalently utilized across engineering applications.","However, the intrinsic limitations of the Boussinesq assumption render LEVMs less accurate for large separated turbulent flows, prompting the development of numerous modified LEVM variants.","Recently, machine learning approaches, notably neural networks, have been employed to enhance LEVMs.","Nonetheless, the \"black box\" nature of such machine learning techniques poses significant interpretability challenges in refining turbulence models.","This paper introduces a novel unit-constrained turbulence modeling framework using symbolic regression to overcome these hurdles.","This framework amends the constitutive equation of original LEVMs by establishing explicit equations between the Reynolds stress deviation and mean flow quantities, with unit consistency constraints bolstering the efficiency of the symbolic regression learning process.","The framework's effectiveness is demonstrated through its application to the separated flow over 2D periodic hills.","Numerical simulations are conducted to validate the accuracy and generalization capabilities of the learned turbulence model.","Compared to the standard k-{\\epsilon} model, the learned model exhibits significantly enhanced predictive accuracy for anisotropic Reynolds stresses and flow velocities, more precisely delineates flow separation, and shows promising generalization potential."],"url":"http://arxiv.org/abs/2405.08656v1","category":"physics.flu-dyn"}
{"created":"2024-05-14 14:30:32","title":"Non-local parabolic equations with singular (Morrey) time-inhomogeneous drift","abstract":"We obtain Sobolev regularity estimates for solutions of non-local parabolic equations with locally unbounded drift satisfying some minimal assumptions. These results yield Krylov bound for the corresponding Feller stable process as well as some a priori regularity estimates on solutions of McKean-Vlasov equations. A key element of our arguments is a parabolic operator norm inequality that we prove using some ideas of Adams and Krylov.","sentences":["We obtain Sobolev regularity estimates for solutions of non-local parabolic equations with locally unbounded drift satisfying some minimal assumptions.","These results yield Krylov bound for the corresponding Feller stable process as well as some a priori regularity estimates on solutions of McKean-Vlasov equations.","A key element of our arguments is a parabolic operator norm inequality that we prove using some ideas of Adams and Krylov."],"url":"http://arxiv.org/abs/2405.08652v1","category":"math.AP"}
{"created":"2024-05-14 14:21:55","title":"Certifying Robustness of Graph Convolutional Networks for Node Perturbation with Polyhedra Abstract Interpretation","abstract":"Graph convolutional neural networks (GCNs) are powerful tools for learning graph-based knowledge representations from training data. However, they are vulnerable to small perturbations in the input graph, which makes them susceptible to input faults or adversarial attacks. This poses a significant problem for GCNs intended to be used in critical applications, which need to provide certifiably robust services even in the presence of adversarial perturbations. We propose an improved GCN robustness certification technique for node classification in the presence of node feature perturbations. We introduce a novel polyhedra-based abstract interpretation approach to tackle specific challenges of graph data and provide tight upper and lower bounds for the robustness of the GCN. Experiments show that our approach simultaneously improves the tightness of robustness bounds as well as the runtime performance of certification. Moreover, our method can be used during training to further improve the robustness of GCNs.","sentences":["Graph convolutional neural networks (GCNs) are powerful tools for learning graph-based knowledge representations from training data.","However, they are vulnerable to small perturbations in the input graph, which makes them susceptible to input faults or adversarial attacks.","This poses a significant problem for GCNs intended to be used in critical applications, which need to provide certifiably robust services even in the presence of adversarial perturbations.","We propose an improved GCN robustness certification technique for node classification in the presence of node feature perturbations.","We introduce a novel polyhedra-based abstract interpretation approach to tackle specific challenges of graph data and provide tight upper and lower bounds for the robustness of the GCN.","Experiments show that our approach simultaneously improves the tightness of robustness bounds as well as the runtime performance of certification.","Moreover, our method can be used during training to further improve the robustness of GCNs."],"url":"http://arxiv.org/abs/2405.08645v1","category":"cs.LG"}
{"created":"2024-05-14 14:13:57","title":"Approaches to iterative algorithms for solving nonlinear equations with an application in tomographic absorption spectroscopy","abstract":"In this paper we propose an approach for solving systems of nonlinear equations without computing function derivatives. Motivated by the application area of tomographic absorption spectroscopy, which is a highly-nonlinear problem with variables coupling, we consider a situation where straightforward translation to a fixed point problem is not possible because the operators that represent the relevant systems of nonlinear equations are not self-mappings, i.e., they operate between spaces of different dimensions. To overcome this difficulty we suggest an \"alternating common fixed points algorithm\" that acts alternatingly on the different vector variables. This approach translates the original problem to a common fixed point problem for which iterative algorithms are abound and exhibits a viable alternative to translation to an optimization problem, which usually requires derivatives information. However, to apply any of these iterative algorithms requires to ascertain the conditions that appear in their convergence theorems. To circumvent the need to verify conditions for convergence, we propose and motivate a derivative-free algorithm that better suits the tomographic absorption spectroscopy problem at hand and is even further improved by applying to it the superiorization approach. This is presented along with experimental results that demonstrate our approach.","sentences":["In this paper we propose an approach for solving systems of nonlinear equations without computing function derivatives.","Motivated by the application area of tomographic absorption spectroscopy, which is a highly-nonlinear problem with variables coupling, we consider a situation where straightforward translation to a fixed point problem is not possible because the operators that represent the relevant systems of nonlinear equations are not self-mappings, i.e., they operate between spaces of different dimensions.","To overcome this difficulty we suggest an \"alternating common fixed points algorithm\" that acts alternatingly on the different vector variables.","This approach translates the original problem to a common fixed point problem for which iterative algorithms are abound and exhibits a viable alternative to translation to an optimization problem, which usually requires derivatives information.","However, to apply any of these iterative algorithms requires to ascertain the conditions that appear in their convergence theorems.","To circumvent the need to verify conditions for convergence, we propose and motivate a derivative-free algorithm that better suits the tomographic absorption spectroscopy problem at hand and is even further improved by applying to it the superiorization approach.","This is presented along with experimental results that demonstrate our approach."],"url":"http://arxiv.org/abs/2405.08635v1","category":"math.OC"}
{"created":"2024-05-14 14:13:10","title":"Stabilization of Integral Delay Equations by solving Fredholm equations","abstract":"In this paper, we design a stabilizing state-feedback control law for a system represented by a general class of integral delay equations subject to a pointwise and distributed input delay. The proposed controller is defined in terms of integrals of the state and input history over a fixed-length time window. We show that the closed-loop stability is guaranteed, provided the controller integral kernels are solutions to a set of Fredholm equations. The existence of solutions is guaranteed under an appropriate spectral controllability assumption, resulting in an implementable stabilizing control law. The proposed methodology appears simpler and more general compared to existing results in the literature. In particular, under additional regularity assumptions, the proposed approach can be expanded to address the degenerate case where only a distributed control term is present.","sentences":["In this paper, we design a stabilizing state-feedback control law for a system represented by a general class of integral delay equations subject to a pointwise and distributed input delay.","The proposed controller is defined in terms of integrals of the state and input history over a fixed-length time window.","We show that the closed-loop stability is guaranteed, provided the controller integral kernels are solutions to a set of Fredholm equations.","The existence of solutions is guaranteed under an appropriate spectral controllability assumption, resulting in an implementable stabilizing control law.","The proposed methodology appears simpler and more general compared to existing results in the literature.","In particular, under additional regularity assumptions, the proposed approach can be expanded to address the degenerate case where only a distributed control term is present."],"url":"http://arxiv.org/abs/2405.08634v1","category":"math.DS"}
{"created":"2024-05-14 14:11:25","title":"Instantaneous Bandwidth Estimation from Level-Crossing Samples via LSTM-based Encoder-Decoder Architecture","abstract":"This paper presents an approach for instantaneous bandwidth estimation from level-crossing samples using a long short-term memory (LSTM) encoder-decoder architecture. Level-crossing sampling is a nonuniform sampling technique that is particularly useful for energy-efficient acquisition of signals with sparse spectra. Especially in combination with fully analog wireless sensor nodes, level-crossing sampling offers a viable alternative to traditional sampling methods. However, due to the nonuniform distribution of samples, reconstructing the original signal is a challenging task. One promising reconstruction approach is time-warping, where the local signal spectrum is taken into account. However, this requires an accurate estimate of the instantaneous bandwidth of the signal. In this paper, we show that applying neural networks (NNs) to the problem of estimating instantaneous bandwidth from level-crossing samples can improve the overall reconstruction accuracy. We conduct a comprehensive numerical analysis of the proposed approach and compare it to an intensity-based bandwidth estimation method from literature.","sentences":["This paper presents an approach for instantaneous bandwidth estimation from level-crossing samples using a long short-term memory (LSTM) encoder-decoder architecture.","Level-crossing sampling is a nonuniform sampling technique that is particularly useful for energy-efficient acquisition of signals with sparse spectra.","Especially in combination with fully analog wireless sensor nodes, level-crossing sampling offers a viable alternative to traditional sampling methods.","However, due to the nonuniform distribution of samples, reconstructing the original signal is a challenging task.","One promising reconstruction approach is time-warping, where the local signal spectrum is taken into account.","However, this requires an accurate estimate of the instantaneous bandwidth of the signal.","In this paper, we show that applying neural networks (NNs) to the problem of estimating instantaneous bandwidth from level-crossing samples can improve the overall reconstruction accuracy.","We conduct a comprehensive numerical analysis of the proposed approach and compare it to an intensity-based bandwidth estimation method from literature."],"url":"http://arxiv.org/abs/2405.08632v1","category":"eess.SP"}
{"created":"2024-05-14 14:06:36","title":"Chemical-motif characterization of short-range order with E(3)-equivariant graph neural networks","abstract":"Crystalline materials have atomic-scale fluctuations in their chemical composition that modulate various mesoscale properties. Establishing chemistry-microstructure relationships in such materials requires proper characterization of these chemical fluctuations. Yet, current characterization approaches (e.g., Warren-Cowley parameters) make only partial use of the complete chemical and structural information contained in local chemical motifs. Here we introduce a framework based on E(3)-equivariant graph neural networks that is capable of completely identifying chemical motifs in arbitrary crystalline structures with any number of chemical elements. This approach naturally leads to a proper information-theoretic measure for quantifying chemical short-range order (SRO) in chemically complex materials, and a reduced - but complete - representation of the chemical space. Our framework enables the correlation of any per-atom property with their corresponding local chemical motif, thereby offering novel avenues to explore structure-property relationships in chemically-complex materials. Using the MoTaNbTi high-entropy alloy as a test system, we demonstrate the versatility of this approach by evaluating the lattice strain associated with each chemical motif, and computing the temperature dependence of chemical-fluctuations length scale.","sentences":["Crystalline materials have atomic-scale fluctuations in their chemical composition that modulate various mesoscale properties.","Establishing chemistry-microstructure relationships in such materials requires proper characterization of these chemical fluctuations.","Yet, current characterization approaches (e.g., Warren-Cowley parameters) make only partial use of the complete chemical and structural information contained in local chemical motifs.","Here we introduce a framework based on E(3)-equivariant graph neural networks that is capable of completely identifying chemical motifs in arbitrary crystalline structures with any number of chemical elements.","This approach naturally leads to a proper information-theoretic measure for quantifying chemical short-range order (SRO) in chemically complex materials, and a reduced - but complete - representation of the chemical space.","Our framework enables the correlation of any per-atom property with their corresponding local chemical motif, thereby offering novel avenues to explore structure-property relationships in chemically-complex materials.","Using the MoTaNbTi high-entropy alloy as a test system, we demonstrate the versatility of this approach by evaluating the lattice strain associated with each chemical motif, and computing the temperature dependence of chemical-fluctuations length scale."],"url":"http://arxiv.org/abs/2405.08628v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 13:55:40","title":"Unconventional surface phase transitions in a (1+1)D $SU(2)_1$ CFT edge coupled to a (2+1)D $Z_2$ bulk","abstract":"We design a (2+1)D quantum spin model in which spin-1/2 ladders are coupled through antiferromagnetic Ising interactions. The model hosts a quantum phase transition in the (2+1)D $Z_2$ universality class from the Haldane phase to the antiferromagnetic Ising ordered phase. We focus on studying the surface properties of three different surface configurations when the Ising couplings are tuned. Different behaviors are found on different surfaces. We find ordinary and two different extraordinary surface critical behaviors (SCBs) at the bulk critical point. The ordinary SCBs belong to the surface universality class of the classical 3D Ising bulk transition. One extraordinary SCBs is induced by the topological properties of the Haldane phase. Another extraordinary SCBs at the bulk critical point is induced by an unconventional surface phase transition where the surface develops an Ising order before the bulk. This surface transition is realized by coupling a (1+1)D $SU(2)_1$ CFT boundary to a (2+1)D bulk with $Z_2$ symmetry. We find that the transition is neither a (1+1)D $Z_2$ transition, expected based on symmetry consideration, nor a Kosterlitz-Thouless-like transition, violating the previous theoretical prediction. This new surface phase transition and related extraordinary SCBs deserve further analytical and numerical exploration.","sentences":["We design a (2+1)D quantum spin model in which spin-1/2 ladders are coupled through antiferromagnetic Ising interactions.","The model hosts a quantum phase transition in the (2+1)D $Z_2$ universality class from the Haldane phase to the antiferromagnetic Ising ordered phase.","We focus on studying the surface properties of three different surface configurations when the Ising couplings are tuned.","Different behaviors are found on different surfaces.","We find ordinary and two different extraordinary surface critical behaviors (SCBs) at the bulk critical point.","The ordinary SCBs belong to the surface universality class of the classical 3D Ising bulk transition.","One extraordinary SCBs is induced by the topological properties of the Haldane phase.","Another extraordinary SCBs at the bulk critical point is induced by an unconventional surface phase transition where the surface develops an Ising order before the bulk.","This surface transition is realized by coupling a (1+1)D $SU(2)_1$ CFT boundary to a (2+1)D bulk with $Z_2$ symmetry.","We find that the transition is neither a (1+1)D $Z_2$ transition, expected based on symmetry consideration, nor a Kosterlitz-Thouless-like transition, violating the previous theoretical prediction.","This new surface phase transition and related extraordinary SCBs deserve further analytical and numerical exploration."],"url":"http://arxiv.org/abs/2405.08612v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 13:49:31","title":"Dynamic NeRF: A Review","abstract":"Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D reconstruction and representation with a high resolution. After the first research of NeRF is proposed, NeRF has gained a robust developing power and is booming in the 3D modeling, representation and reconstruction areas. However the first and most of the followed research projects based on NeRF is static, which are weak in the practical applications. Therefore, more researcher are interested and focused on the study of dynamic NeRF that is more feasible and useful in practical applications or situations. Compared with the static NeRF, implementing the Dynamic NeRF is more difficult and complex. But Dynamic is more potential in the future even is the basic of Editable NeRF. In this review, we made a detailed and abundant statement for the development and important implementation principles of Dynamci NeRF. The analysis of main principle and development of Dynamic NeRF is from 2021 to 2023, including the most of the Dynamic NeRF projects. What is more, with colorful and novel special designed figures and table, We also made a detailed comparison and analysis of different features of various of Dynamic. Besides, we analyzed and discussed the key methods to implement a Dynamic NeRF. The volume of the reference papers is large. The statements and comparisons are multidimensional. With a reading of this review, the whole development history and most of the main design method or principles of Dynamic NeRF can be easy understood and gained.","sentences":["Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D reconstruction and representation with a high resolution.","After the first research of NeRF is proposed, NeRF has gained a robust developing power and is booming in the 3D modeling, representation and reconstruction areas.","However the first and most of the followed research projects based on NeRF is static, which are weak in the practical applications.","Therefore, more researcher are interested and focused on the study of dynamic NeRF that is more feasible and useful in practical applications or situations.","Compared with the static NeRF, implementing the Dynamic NeRF is more difficult and complex.","But Dynamic is more potential in the future even is the basic of Editable NeRF.","In this review, we made a detailed and abundant statement for the development and important implementation principles of Dynamci NeRF.","The analysis of main principle and development of Dynamic NeRF is from 2021 to 2023, including the most of the Dynamic NeRF projects.","What is more, with colorful and novel special designed figures and table, We also made a detailed comparison and analysis of different features of various of Dynamic.","Besides, we analyzed and discussed the key methods to implement a Dynamic NeRF.","The volume of the reference papers is large.","The statements and comparisons are multidimensional.","With a reading of this review, the whole development history and most of the main design method or principles of Dynamic NeRF can be easy understood and gained."],"url":"http://arxiv.org/abs/2405.08609v1","category":"cs.CV"}
{"created":"2024-05-14 13:41:44","title":"Optimizing Deep Reinforcement Learning for American Put Option Hedging","abstract":"This paper contributes to the existing literature on hedging American options with Deep Reinforcement Learning (DRL). The study first investigates hyperparameter impact on hedging performance, considering learning rates, training episodes, neural network architectures, training steps, and transaction cost penalty functions. Results highlight the importance of avoiding certain combinations, such as high learning rates with a high number of training episodes or low learning rates with few training episodes and emphasize the significance of utilizing moderate values for optimal outcomes. Additionally, the paper warns against excessive training steps to prevent instability and demonstrates the superiority of a quadratic transaction cost penalty function over a linear version. This study then expands upon the work of Pickard et al. (2024), who utilize a Chebyshev interpolation option pricing method to train DRL agents with market calibrated stochastic volatility models. While the results of Pickard et al. (2024) showed that these DRL agents achieve satisfactory performance on empirical asset paths, this study introduces a novel approach where new agents at weekly intervals to newly calibrated stochastic volatility models. Results show DRL agents re-trained using weekly market data surpass the performance of those trained solely on the sale date. Furthermore, the paper demonstrates that both single-train and weekly-train DRL agents outperform the Black-Scholes Delta method at transaction costs of 1% and 3%. This practical relevance suggests that practitioners can leverage readily available market data to train DRL agents for effective hedging of options in their portfolios.","sentences":["This paper contributes to the existing literature on hedging American options with Deep Reinforcement Learning (DRL).","The study first investigates hyperparameter impact on hedging performance, considering learning rates, training episodes, neural network architectures, training steps, and transaction cost penalty functions.","Results highlight the importance of avoiding certain combinations, such as high learning rates with a high number of training episodes or low learning rates with few training episodes and emphasize the significance of utilizing moderate values for optimal outcomes.","Additionally, the paper warns against excessive training steps to prevent instability and demonstrates the superiority of a quadratic transaction cost penalty function over a linear version.","This study then expands upon the work of Pickard et al. (2024), who utilize a Chebyshev interpolation option pricing method to train DRL agents with market calibrated stochastic volatility models.","While the results of Pickard et al. (2024) showed that these DRL agents achieve satisfactory performance on empirical asset paths, this study introduces a novel approach where new agents at weekly intervals to newly calibrated stochastic volatility models.","Results show DRL agents re-trained using weekly market data surpass the performance of those trained solely on the sale date.","Furthermore, the paper demonstrates that both single-train and weekly-train DRL agents outperform the Black-Scholes Delta method at transaction costs of 1% and 3%.","This practical relevance suggests that practitioners can leverage readily available market data to train DRL agents for effective hedging of options in their portfolios."],"url":"http://arxiv.org/abs/2405.08602v1","category":"q-fin.RM"}
{"created":"2024-05-14 13:41:39","title":"The Requirement for Cognition, in an Equation","abstract":"A model of the evolution of cognition is used to derive a Requirement Equation (RE), which defines what computations the fittest possible brain must make, or must choose actions as if it had made those computations. The terms in the RE depend on factors outside an animals brain, which can be modelled without making assumptions about how the brain works, from knowledge of the animals habitat and biology. In simple domains where the choices of actions have small information content, it may not be necessary to build internal models of reality; short cut computations may be just as good at choosing actions. In complex domains such as 3D spatial cognition, which underpins many complex choices of action, the RE implies that brains build Bayesian internal models of the animals surroundings; and that the models are constrained to be true to external reality.","sentences":["A model of the evolution of cognition is used to derive a Requirement Equation (RE), which defines what computations the fittest possible brain must make, or must choose actions as if it had made those computations.","The terms in the RE depend on factors outside an animals brain, which can be modelled without making assumptions about how the brain works, from knowledge of the animals habitat and biology.","In simple domains where the choices of actions have small information content, it may not be necessary to build internal models of reality; short cut computations may be just as good at choosing actions.","In complex domains such as 3D spatial cognition, which underpins many complex choices of action, the RE implies that brains build Bayesian internal models of the animals surroundings; and that the models are constrained to be true to external reality."],"url":"http://arxiv.org/abs/2405.08601v1","category":"q-bio.NC"}
{"created":"2024-05-14 13:41:35","title":"Stabilization and Optimal Control of Interconnected SDE - Scalar PDE System","abstract":"In this paper, we design a controller for an interconnected system consisting of a linear Stochastic Differential Equation (SDE) actuated through a linear hyperbolic Partial Differential Equation (PDE). Our approach aims to minimize the variance of the state of the SDE component. We leverage a backstepping technique to transform the original PDE into an uncoupled stochastic PDE. As such, we reformulate our initial problem as the control of a delayed SDE with a non-deterministic drift. Under standard controllability assumptions, we design a controller steering the mean of the states to zero while keeping its covariance bounded. As final step, we address the optimal control of the delayed SDE employing Artstein's transformation and Linear Quadratic stochastic control techniques.","sentences":["In this paper, we design a controller for an interconnected system consisting of a linear Stochastic Differential Equation (SDE) actuated through a linear hyperbolic Partial Differential Equation (PDE).","Our approach aims to minimize the variance of the state of the SDE component.","We leverage a backstepping technique to transform the original PDE into an uncoupled stochastic PDE.","As such, we reformulate our initial problem as the control of a delayed SDE with a non-deterministic drift.","Under standard controllability assumptions, we design a controller steering the mean of the states to zero while keeping its covariance bounded.","As final step, we address the optimal control of the delayed SDE employing Artstein's transformation and Linear Quadratic stochastic control techniques."],"url":"http://arxiv.org/abs/2405.08600v1","category":"math.OC"}
{"created":"2024-05-14 13:38:15","title":"Spectral approximation of convolution operators of Fredholm type","abstract":"We have developed a method for constructing spectral approximations for convolution operators of Fredholm type. The algorithm we propose is numerically stable and takes advantage of the recurrence relations satisfied by the entries of such a matrix approximation. When used for computing the Fredholm convolution of two given functions, such approximations produce the convolution more rapidly than the state-of-the-art methods. The proposed approximation also leads to a spectral method for solving the Fredholm convolution integral equations and enables the computation of eigenvalues and pseudospectra of Fredholm convolution operators, which is otherwise intractable with existing techniques.","sentences":["We have developed a method for constructing spectral approximations for convolution operators of Fredholm type.","The algorithm we propose is numerically stable and takes advantage of the recurrence relations satisfied by the entries of such a matrix approximation.","When used for computing the Fredholm convolution of two given functions, such approximations produce the convolution more rapidly than the state-of-the-art methods.","The proposed approximation also leads to a spectral method for solving the Fredholm convolution integral equations and enables the computation of eigenvalues and pseudospectra of Fredholm convolution operators, which is otherwise intractable with existing techniques."],"url":"http://arxiv.org/abs/2405.08598v1","category":"math.NA"}
{"created":"2024-05-14 13:35:39","title":"Open-Vocabulary Object Detection via Neighboring Region Attention Alignment","abstract":"The nature of diversity in real-world environments necessitates neural network models to expand from closed category settings to accommodate novel emerging categories. In this paper, we study the open-vocabulary object detection (OVD), which facilitates the detection of novel object classes under the supervision of only base annotations and open-vocabulary knowledge. However, we find that the inadequacy of neighboring relationships between regions during the alignment process inevitably constrains the performance on recent distillation-based OVD strategies. To this end, we propose Neighboring Region Attention Alignment (NRAA), which performs alignment within the attention mechanism of a set of neighboring regions to boost the open-vocabulary inference. Specifically, for a given proposal region, we randomly explore the neighboring boxes and conduct our proposed neighboring region attention (NRA) mechanism to extract relationship information. Then, this interaction information is seamlessly provided into the distillation procedure to assist the alignment between the detector and the pre-trained vision-language models (VLMs). Extensive experiments validate that our proposed model exhibits superior performance on open-vocabulary benchmarks.","sentences":["The nature of diversity in real-world environments necessitates neural network models to expand from closed category settings to accommodate novel emerging categories.","In this paper, we study the open-vocabulary object detection (OVD), which facilitates the detection of novel object classes under the supervision of only base annotations and open-vocabulary knowledge.","However, we find that the inadequacy of neighboring relationships between regions during the alignment process inevitably constrains the performance on recent distillation-based OVD strategies.","To this end, we propose Neighboring Region Attention Alignment (NRAA), which performs alignment within the attention mechanism of a set of neighboring regions to boost the open-vocabulary inference.","Specifically, for a given proposal region, we randomly explore the neighboring boxes and conduct our proposed neighboring region attention (NRA) mechanism to extract relationship information.","Then, this interaction information is seamlessly provided into the distillation procedure to assist the alignment between the detector and the pre-trained vision-language models (VLMs).","Extensive experiments validate that our proposed model exhibits superior performance on open-vocabulary benchmarks."],"url":"http://arxiv.org/abs/2405.08593v1","category":"cs.CV"}
{"created":"2024-05-14 13:23:54","title":"On the $\u03c3$-balancing property of multivariate generalized quasi-arithmetic means","abstract":"The aim of this paper is to characterize the so-called $\\sigma$-balancing property in the class of generalized quasi-arithmetic means. In general, the question is whether those elements of a given family of means that possess this property are quasi-arithmetic.   The first result in the latter direction is due to G. Aumann who showed that a balanced complex mean is necessariliy quasi-arithmetic provided that it is analytic. Then Aumann characterized quasi-arithmetic means among Cauchy means in terms of the balancing property. These results date back to the 1930s. In 2015, Lucio R. Berrone, generalizing balancedness, concluded that a mean having that more general property is quasi-arithmetic if it is symmetric, strict and continuously differentiable. A common feature of these results is that they assume a certain order of differentiability of the mean whether or not it is a natural condition.   In 2020, the balancing property was characterized in the family of generalized quasi-arithmetic means of two variables under only natural conditions, namely continuity and strict monotonicity of their generating functions. Here we extend the corresponding result for multivariate generalized quasi-arithmetic means by relaxing the conditions on the generating functions and considering the more general $\\sigma$-balancing property.","sentences":["The aim of this paper is to characterize the so-called $\\sigma$-balancing property in the class of generalized quasi-arithmetic means.","In general, the question is whether those elements of a given family of means that possess this property are quasi-arithmetic.   ","The first result in the latter direction is due to G. Aumann who showed that a balanced complex mean is necessariliy quasi-arithmetic provided that it is analytic.","Then Aumann characterized quasi-arithmetic means among Cauchy means in terms of the balancing property.","These results date back to the 1930s.","In 2015, Lucio R. Berrone, generalizing balancedness, concluded that a mean having that more general property is quasi-arithmetic if it is symmetric, strict and continuously differentiable.","A common feature of these results is that they assume a certain order of differentiability of the mean whether or not it is a natural condition.   ","In 2020, the balancing property was characterized in the family of generalized quasi-arithmetic means of two variables under only natural conditions, namely continuity and strict monotonicity of their generating functions.","Here we extend the corresponding result for multivariate generalized quasi-arithmetic means by relaxing the conditions on the generating functions and considering the more general $\\sigma$-balancing property."],"url":"http://arxiv.org/abs/2405.08583v1","category":"math.CA"}
{"created":"2024-05-14 13:20:26","title":"On Lax representations under the gauge equivalence relation and Miura-type transformations for lattice equations","abstract":"We study matrix Lax representations (MLRs) for differential-difference (lattice) equations. For a given equation, two MLRs are said to be gauge equivalent if one of them can be obtained from the other by means of a matrix gauge transformation.   We present results on the following questions:   1. When is a given MLR gauge equivalent to an MLR suitable for constructing differential-difference Miura-type transformations by the method of [G. Berkeley, S. Igonin, J. Phys. A (2016), arXiv:1512.09123]?   2. When is a given MLR gauge equivalent to a trivial MLR?   Furthermore, we present new examples of integrable differential-difference equations with Miura-type transformations.","sentences":["We study matrix Lax representations (MLRs) for differential-difference (lattice) equations.","For a given equation, two MLRs are said to be gauge equivalent if one of them can be obtained from the other by means of a matrix gauge transformation.   ","We present results on the following questions:   1.","When is a given MLR gauge equivalent to an MLR suitable for constructing differential-difference Miura-type transformations by the method of [G. Berkeley, S. Igonin, J. Phys.","A (2016), arXiv:1512.09123]?   ","2.","When is a given MLR gauge equivalent to a trivial MLR?   ","Furthermore, we present new examples of integrable differential-difference equations with Miura-type transformations."],"url":"http://arxiv.org/abs/2405.08579v1","category":"nlin.SI"}
{"created":"2024-05-14 13:07:49","title":"Mean-field theory of first-order quantum superconductor-insulator transition","abstract":"Recent experimental studies on strongly disordered indium oxide films have revealed an unusual first-order quantum phase transition between the superconducting and insulating states (SIT). This transition is characterized by a discontinuous jump from non-zero to zero values of superfluid stiffness at the critical point, contradicting the conventional ``scaling scenario'' typically associated with SIT. In this paper, we present a theoretical framework for understanding this first-order transition. Our approach is based on the concept of competition between two fundamentally distinct ground states that arise from electron pairs initially localized by strong disorder: the superconducting state and the Coulomb glass insulator. These ground states are distinguished by two crucially different order parameters, suggesting a natural expectation of a discontinuous transition between them at $T=0$. This transition occurs when the magnitudes of the superconducting gap $\\Delta$ and the Coulomb gap $E_C$ become comparable. Additionally, we extend our analysis to low non-zero temperatures and provide a mean-field ``phase diagram'' in the plane of $(T/\\Delta,E_C/\\Delta)$. Our results reveal the existence of a natural upper bound for the kinetic inductance of strongly disordered superconductors.","sentences":["Recent experimental studies on strongly disordered indium oxide films have revealed an unusual first-order quantum phase transition between the superconducting and insulating states (SIT).","This transition is characterized by a discontinuous jump from non-zero to zero values of superfluid stiffness at the critical point, contradicting the conventional ``scaling scenario'' typically associated with SIT.","In this paper, we present a theoretical framework for understanding this first-order transition.","Our approach is based on the concept of competition between two fundamentally distinct ground states that arise from electron pairs initially localized by strong disorder: the superconducting state and the Coulomb glass insulator.","These ground states are distinguished by two crucially different order parameters, suggesting a natural expectation of a discontinuous transition between them at $T=0$. This transition occurs when the magnitudes of the superconducting gap $\\Delta$ and the Coulomb gap $E_C$ become comparable.","Additionally, we extend our analysis to low non-zero temperatures and provide a mean-field ``phase diagram'' in the plane of $(T/\\Delta,E_C/\\Delta)$. Our results reveal the existence of a natural upper bound for the kinetic inductance of strongly disordered superconductors."],"url":"http://arxiv.org/abs/2405.08571v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 12:56:56","title":"Space-time stochastic Galerkin boundary elements for acoustic scattering problems","abstract":"Acoustic emission or scattering problems naturally involve uncertainties about the sound sources or boundary conditions. This article initiates the study of time domain boundary elements for such stochastic boundary problems for the acoustic wave equation. We present a space-time stochastic Galerkin boundary element method which is applied to sound-hard, sound-soft and absorbing scatterers. Uncertainties in both the sources and the boundary conditions are considered using a polynomial chaos expansion. The numerical experiments illustrate the performance and convergence of the proposed method in model problems and present an application to a problem from traffic noise.","sentences":["Acoustic emission or scattering problems naturally involve uncertainties about the sound sources or boundary conditions.","This article initiates the study of time domain boundary elements for such stochastic boundary problems for the acoustic wave equation.","We present a space-time stochastic Galerkin boundary element method which is applied to sound-hard, sound-soft and absorbing scatterers.","Uncertainties in both the sources and the boundary conditions are considered using a polynomial chaos expansion.","The numerical experiments illustrate the performance and convergence of the proposed method in model problems and present an application to a problem from traffic noise."],"url":"http://arxiv.org/abs/2405.08565v1","category":"math.NA"}
{"created":"2024-05-14 12:46:12","title":"PTPI-DL-ROMs: pre-trained physics-informed deep learning-based reduced order models for nonlinear parametrized PDEs","abstract":"The coupling of Proper Orthogonal Decomposition (POD) and deep learning-based ROMs (DL-ROMs) has proved to be a successful strategy to construct non-intrusive, highly accurate, surrogates for the real time solution of parametric nonlinear time-dependent PDEs. Inexpensive to evaluate, POD-DL-ROMs are also relatively fast to train, thanks to their limited complexity. However, POD-DL-ROMs account for the physical laws governing the problem at hand only through the training data, that are usually obtained through a full order model (FOM) relying on a high-fidelity discretization of the underlying equations. Moreover, the accuracy of POD-DL-ROMs strongly depends on the amount of available data. In this paper, we consider a major extension of POD-DL-ROMs by enforcing the fulfillment of the governing physical laws in the training process -- that is, by making them physics-informed -- to compensate for possible scarce and/or unavailable data and improve the overall reliability. To do that, we first complement POD-DL-ROMs with a trunk net architecture, endowing them with the ability to compute the problem's solution at every point in the spatial domain, and ultimately enabling a seamless computation of the physics-based loss by means of the strong continuous formulation. Then, we introduce an efficient training strategy that limits the notorious computational burden entailed by a physics-informed training phase. In particular, we take advantage of the few available data to develop a low-cost pre-training procedure; then, we fine-tune the architecture in order to further improve the prediction reliability. Accuracy and efficiency of the resulting pre-trained physics-informed DL-ROMs (PTPI-DL-ROMs) are then assessed on a set of test cases ranging from non-affinely parametrized advection-diffusion-reaction equations, to nonlinear problems like the Navier-Stokes equations for fluid flows.","sentences":["The coupling of Proper Orthogonal Decomposition (POD) and deep learning-based ROMs (DL-ROMs) has proved to be a successful strategy to construct non-intrusive, highly accurate, surrogates for the real time solution of parametric nonlinear time-dependent PDEs.","Inexpensive to evaluate, POD-DL-ROMs are also relatively fast to train, thanks to their limited complexity.","However, POD-DL-ROMs account for the physical laws governing the problem at hand only through the training data, that are usually obtained through a full order model (FOM) relying on a high-fidelity discretization of the underlying equations.","Moreover, the accuracy of POD-DL-ROMs strongly depends on the amount of available data.","In this paper, we consider a major extension of POD-DL-ROMs by enforcing the fulfillment of the governing physical laws in the training process -- that is, by making them physics-informed -- to compensate for possible scarce and/or unavailable data and improve the overall reliability.","To do that, we first complement POD-DL-ROMs with a trunk net architecture, endowing them with the ability to compute the problem's solution at every point in the spatial domain, and ultimately enabling a seamless computation of the physics-based loss by means of the strong continuous formulation.","Then, we introduce an efficient training strategy that limits the notorious computational burden entailed by a physics-informed training phase.","In particular, we take advantage of the few available data to develop a low-cost pre-training procedure; then, we fine-tune the architecture in order to further improve the prediction reliability.","Accuracy and efficiency of the resulting pre-trained physics-informed DL-ROMs (PTPI-DL-ROMs) are then assessed on a set of test cases ranging from non-affinely parametrized advection-diffusion-reaction equations, to nonlinear problems like the Navier-Stokes equations for fluid flows."],"url":"http://arxiv.org/abs/2405.08558v1","category":"math.NA"}
{"created":"2024-05-14 12:38:34","title":"A Well-Balanced Method for an Unstaggered Central Scheme, the one-space Dimensional Case","abstract":"In this paper, we propose a new MUSCL scheme by combining the ideas of the Kurganov and Tadmor scheme and the so-called Deviation method which results in a well-balanced finite volume method for the hyperbolic balance laws, by evolving the difference between the exact solution and a given stationary solution. After that, we derive a semi-discrete scheme from this new scheme and it can be shown to be essentially TVD when applied to a scalar conservation law. In the end, we apply and validate the developed methods by numerical experiments and solve classical problems featuring Euler equations with gravitational source term.","sentences":["In this paper, we propose a new MUSCL scheme by combining the ideas of the Kurganov and Tadmor scheme and the so-called Deviation method which results in a well-balanced finite volume method for the hyperbolic balance laws, by evolving the difference between the exact solution and a given stationary solution.","After that, we derive a semi-discrete scheme from this new scheme and it can be shown to be essentially TVD when applied to a scalar conservation law.","In the end, we apply and validate the developed methods by numerical experiments and solve classical problems featuring Euler equations with gravitational source term."],"url":"http://arxiv.org/abs/2405.08549v1","category":"math.NA"}
{"created":"2024-05-14 12:31:10","title":"On the characterization of Riemannian warped product Einstein metrics","abstract":"We present a series of results, including local characterizations of $(\\lambda,m+n)$-Einstein metrics in the context of warped product Einstein spaces. Using these local properties, we restate already known global characterizations of $(\\lambda,m+n)$-Einstein manifolds from He, Petersen and Wylie.","sentences":["We present a series of results, including local characterizations of $(\\lambda,m+n)$-Einstein metrics in the context of warped product Einstein spaces.","Using these local properties, we restate already known global characterizations of $(\\lambda,m+n)$-Einstein manifolds from He, Petersen and Wylie."],"url":"http://arxiv.org/abs/2405.08544v1","category":"math.DG"}
{"created":"2024-05-14 12:21:08","title":"GS-PINN: Greedy Sampling for Parameter Estimation in Partial Differential Equations","abstract":"Partial differential equation parameter estimation is a mathematical and computational process used to estimate the unknown parameters in a partial differential equation model from observational data. This paper employs a greedy sampling approach based on the Discrete Empirical Interpolation Method to identify the most informative samples in a dataset associated with a partial differential equation to estimate its parameters. Greedy samples are used to train a physics-informed neural network architecture which maps the nonlinear relation between spatio-temporal data and the measured values. To prove the impact of greedy samples on the training of the physics-informed neural network for parameter estimation of a partial differential equation, their performance is compared with random samples taken from the given dataset. Our simulation results show that for all considered partial differential equations, greedy samples outperform random samples, i.e., we can estimate parameters with a significantly lower number of samples while simultaneously reducing the relative estimation error. A Python package is also prepared to support different phases of the proposed algorithm, including data prepossessing, greedy sampling, neural network training, and comparison.","sentences":["Partial differential equation parameter estimation is a mathematical and computational process used to estimate the unknown parameters in a partial differential equation model from observational data.","This paper employs a greedy sampling approach based on the Discrete Empirical Interpolation Method to identify the most informative samples in a dataset associated with a partial differential equation to estimate its parameters.","Greedy samples are used to train a physics-informed neural network architecture which maps the nonlinear relation between spatio-temporal data and the measured values.","To prove the impact of greedy samples on the training of the physics-informed neural network for parameter estimation of a partial differential equation, their performance is compared with random samples taken from the given dataset.","Our simulation results show that for all considered partial differential equations, greedy samples outperform random samples, i.e., we can estimate parameters with a significantly lower number of samples while simultaneously reducing the relative estimation error.","A Python package is also prepared to support different phases of the proposed algorithm, including data prepossessing, greedy sampling, neural network training, and comparison."],"url":"http://arxiv.org/abs/2405.08537v1","category":"math.DS"}
{"created":"2024-05-14 12:06:44","title":"EEG-Features for Generalized Deepfake Detection","abstract":"Since the advent of Deepfakes in digital media, the development of robust and reliable detection mechanism is urgently called for. In this study, we explore a novel approach to Deepfake detection by utilizing electroencephalography (EEG) measured from the neural processing of a human participant who viewed and categorized Deepfake stimuli from the FaceForensics++ datset. These measurements serve as input features to a binary support vector classifier, trained to discriminate between real and manipulated facial images. We examine whether EEG data can inform Deepfake detection and also if it can provide a generalized representation capable of identifying Deepfakes beyond the training domain. Our preliminary results indicate that human neural processing signals can be successfully integrated into Deepfake detection frameworks and hint at the potential for a generalized neural representation of artifacts in computer generated faces. Moreover, our study provides next steps towards the understanding of how digital realism is embedded in the human cognitive system, possibly enabling the development of more realistic digital avatars in the future.","sentences":["Since the advent of Deepfakes in digital media, the development of robust and reliable detection mechanism is urgently called for.","In this study, we explore a novel approach to Deepfake detection by utilizing electroencephalography (EEG) measured from the neural processing of a human participant who viewed and categorized Deepfake stimuli from the FaceForensics++ datset.","These measurements serve as input features to a binary support vector classifier, trained to discriminate between real and manipulated facial images.","We examine whether EEG data can inform Deepfake detection and also if it can provide a generalized representation capable of identifying Deepfakes beyond the training domain.","Our preliminary results indicate that human neural processing signals can be successfully integrated into Deepfake detection frameworks and hint at the potential for a generalized neural representation of artifacts in computer generated faces.","Moreover, our study provides next steps towards the understanding of how digital realism is embedded in the human cognitive system, possibly enabling the development of more realistic digital avatars in the future."],"url":"http://arxiv.org/abs/2405.08527v1","category":"cs.LG"}
{"created":"2024-05-14 11:40:56","title":"Magnetization dynamics in skyrmions due to high-speed carrier injections from Dirac half-metals","abstract":"Recent developments in the magnetization dynamics in spin textures, particularly skyrmions, offer promising new directions for magnetic storage technologies and spintronics. Skyrmions, characterized by their topological protection and efficient mobility at low current density, are increasingly recognized for their potential applications in next-generation logic and memory devices. This study investigates the dynamics of skyrmion magnetization, focusing on the manipulation of their topological states as a basis for bitwise data storage through a modified Landau-Lifshitz-Gilbert equation (LLG). We introduce spin-polarized electrons from a topological ferromagnet that induce an electric dipole moment that interacts with the electric gauge field within the skyrmion domain. This interaction creates an effective magnetic field that results in a torque that can dynamically change the topological state of the skyrmion. In particular, we show that these torques can selectively destroy and create skyrmions, effectively writing and erasing bits, highlighting the potential of using controlled electron injection for robust and scalable skyrmion-based data storage solutions.","sentences":["Recent developments in the magnetization dynamics in spin textures, particularly skyrmions, offer promising new directions for magnetic storage technologies and spintronics.","Skyrmions, characterized by their topological protection and efficient mobility at low current density, are increasingly recognized for their potential applications in next-generation logic and memory devices.","This study investigates the dynamics of skyrmion magnetization, focusing on the manipulation of their topological states as a basis for bitwise data storage through a modified Landau-Lifshitz-Gilbert equation (LLG).","We introduce spin-polarized electrons from a topological ferromagnet that induce an electric dipole moment that interacts with the electric gauge field within the skyrmion domain.","This interaction creates an effective magnetic field that results in a torque that can dynamically change the topological state of the skyrmion.","In particular, we show that these torques can selectively destroy and create skyrmions, effectively writing and erasing bits, highlighting the potential of using controlled electron injection for robust and scalable skyrmion-based data storage solutions."],"url":"http://arxiv.org/abs/2405.08516v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 11:37:14","title":"Subspace method based on neural networks for solving the partial differential equation in weak form","abstract":"We present a subspace method based on neural networks for solving the partial differential equation in weak form with high accuracy. The basic idea of our method is to use some functions based on neural networks as base functions to span a subspace, then find an approximate solution in this subspace. Training base functions and finding an approximate solution can be separated, that is different methods can be used to train these base functions, and different methods can also be used to find an approximate solution. In this paper, we find an approximate solution of the partial differential equation in the weak form. Our method can achieve high accuracy with low cost of training. Numerical examples show that the cost of training these base functions is low, and only one hundred to two thousand epochs are needed for most tests. The error of our method can fall below the level of $10^{-7}$ for some tests. The proposed method has the better performance in terms of the accuracy and computational cost.","sentences":["We present a subspace method based on neural networks for solving the partial differential equation in weak form with high accuracy.","The basic idea of our method is to use some functions based on neural networks as base functions to span a subspace, then find an approximate solution in this subspace.","Training base functions and finding an approximate solution can be separated, that is different methods can be used to train these base functions, and different methods can also be used to find an approximate solution.","In this paper, we find an approximate solution of the partial differential equation in the weak form.","Our method can achieve high accuracy with low cost of training.","Numerical examples show that the cost of training these base functions is low, and only one hundred to two thousand epochs are needed for most tests.","The error of our method can fall below the level of $10^{-7}$ for some tests.","The proposed method has the better performance in terms of the accuracy and computational cost."],"url":"http://arxiv.org/abs/2405.08513v1","category":"math.NA"}
{"created":"2024-05-14 11:19:03","title":"Markoff-Fibonacci m-triples","abstract":"We classify all solution triples with Fibonacci components to the equation $a^2+b^2+c^2=3abc+m,$ for positive $m$. We show that for $m=2$ they are precisely $(1,F(b),F(b+2))$, with even $b$; for $m=21$, there exist exactly two Fibonacci solutions $(1,2,8)$ and $(2,2,13)$ and for any other $m$ there exists at most one Fibonacci solution, which, in case it exists, is always minimal (i.e. it is a root of a Markoff tree). Moreover, we show that there is an infinite number of values of $m$ admitting exactly one such solution.","sentences":["We classify all solution triples with Fibonacci components to the equation $a^2+b^2+c^2=3abc+m,$ for positive $m$. We show that for $m=2$ they are precisely $(1,F(b),F(b+2))$, with even $b$; for $m=21$, there exist exactly two Fibonacci solutions $(1,2,8)$ and $(2,2,13)$ and for any other $m$ there exists at most one Fibonacci solution, which, in case it exists, is always minimal (i.e. it is a root of a Markoff tree).","Moreover, we show that there is an infinite number of values of $m$ admitting exactly one such solution."],"url":"http://arxiv.org/abs/2405.08509v1","category":"math.NT"}
{"created":"2024-05-14 11:19:00","title":"Tau polarization in neutrino-nucleus interactions at the LHC energy range","abstract":"Considering that the study of neutrino-nucleus interactions with incident neutrino energy ranges in the GeV-TeV range is feasible at the Large Hadron Collider, we investigate in this paper the degree of polarization ${\\cal{P}}$ of the (anti) tau lepton produced in (anti) tau neutrino - tungsten interactions. We estimate the differential cross-sections and the longitudinal and transverse components of the tau lepton polarization as a function of the tau lepton energy and distinct values of the scattering angle, assuming different values for the energy of the incoming (anti) tau neutrino. Different models for the treatment of the nuclear effects in the parton distribution functions are assumed as input in the calculations. Our results indicate that $ {\\cal{P}} < 1$ for the neutrino energies reached at the LHC and are almost insensitive to the nuclear effects.","sentences":["Considering that the study of neutrino-nucleus interactions with incident neutrino energy ranges in the GeV-TeV range is feasible at the Large Hadron Collider, we investigate in this paper the degree of polarization ${\\cal{P}}$ of the (anti) tau lepton produced in (anti) tau neutrino - tungsten interactions.","We estimate the differential cross-sections and the longitudinal and transverse components of the tau lepton polarization as a function of the tau lepton energy and distinct values of the scattering angle, assuming different values for the energy of the incoming (anti) tau neutrino.","Different models for the treatment of the nuclear effects in the parton distribution functions are assumed as input in the calculations.","Our results indicate that $ {\\cal{P}} < 1$ for the neutrino energies reached at the LHC and are almost insensitive to the nuclear effects."],"url":"http://arxiv.org/abs/2405.08508v1","category":"hep-ph"}
{"created":"2024-05-14 10:55:04","title":"Learning Decision Policies with Instrumental Variables through Double Machine Learning","abstract":"A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders. Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables. Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect. Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator. We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies. We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework. The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments.","sentences":["A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders.","Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables.","Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect.","Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator.","We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies.","We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework.","The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded.","DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments."],"url":"http://arxiv.org/abs/2405.08498v2","category":"cs.LG"}
{"created":"2024-05-14 10:36:56","title":"Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery: An Experimental Study","abstract":"Deep learning methods, especially Convolutional Neural Networks (CNN) and Vision Transformer (ViT), are frequently employed to perform semantic segmentation of high-resolution remotely sensed images. However, CNNs are constrained by their restricted receptive fields, while ViTs face challenges due to their quadratic complexity. Recently, the Mamba model, featuring linear complexity and a global receptive field, has gained extensive attention for vision tasks. In such tasks, images need to be serialized to form sequences compatible with the Mamba model. Numerous research efforts have explored scanning strategies to serialize images, aiming to enhance the Mamba model's understanding of images. However, the effectiveness of these scanning strategies remains uncertain. In this research, we conduct a comprehensive experimental investigation on the impact of mainstream scanning directions and their combinations on semantic segmentation of remotely sensed images. Through extensive experiments on the LoveDA, ISPRS Potsdam, and ISPRS Vaihingen datasets, we demonstrate that no single scanning strategy outperforms others, regardless of their complexity or the number of scanning directions involved. A simple, single scanning direction is deemed sufficient for semantic segmentation of high-resolution remotely sensed images. Relevant directions for future research are also recommended.","sentences":["Deep learning methods, especially Convolutional Neural Networks (CNN) and Vision Transformer (ViT), are frequently employed to perform semantic segmentation of high-resolution remotely sensed images.","However, CNNs are constrained by their restricted receptive fields, while ViTs face challenges due to their quadratic complexity.","Recently, the Mamba model, featuring linear complexity and a global receptive field, has gained extensive attention for vision tasks.","In such tasks, images need to be serialized to form sequences compatible with the Mamba model.","Numerous research efforts have explored scanning strategies to serialize images, aiming to enhance the Mamba model's understanding of images.","However, the effectiveness of these scanning strategies remains uncertain.","In this research, we conduct a comprehensive experimental investigation on the impact of mainstream scanning directions and their combinations on semantic segmentation of remotely sensed images.","Through extensive experiments on the LoveDA, ISPRS Potsdam, and ISPRS Vaihingen datasets, we demonstrate that no single scanning strategy outperforms others, regardless of their complexity or the number of scanning directions involved.","A simple, single scanning direction is deemed sufficient for semantic segmentation of high-resolution remotely sensed images.","Relevant directions for future research are also recommended."],"url":"http://arxiv.org/abs/2405.08493v1","category":"cs.CV"}
{"created":"2024-05-14 10:29:57","title":"The ring of differential operators on a monomial curve is a Hopf algebroid","abstract":"This article considers cuspidal curves whose coordinate rings are numerical semigroup algebras. Their rings of differential operators are shown to be cocommutative and conilpotent left Hopf algebroids. If the semigroups are symmetric so that the curves are Gorenstein, they are full Hopf algebroids (admit an antipode).","sentences":["This article considers cuspidal curves whose coordinate rings are numerical semigroup algebras.","Their rings of differential operators are shown to be cocommutative and conilpotent left Hopf algebroids.","If the semigroups are symmetric so that the curves are Gorenstein, they are full Hopf algebroids (admit an antipode)."],"url":"http://arxiv.org/abs/2405.08490v1","category":"math.QA"}
{"created":"2024-05-14 10:18:38","title":"Doubly relaxed forward-Douglas--Rachford splitting for the sum of two nonconvex and a DC function","abstract":"In this paper, we consider a class of structured nonconvex nonsmooth optimization problems whose objective function is the sum of three nonconvex functions, one of which is expressed in a difference-of-convex (DC) form. This problem class covers several important structures in the literature including the sum of three functions and the general DC program. We propose a splitting algorithm and prove the subsequential convergence to a stationary point of the problem. The full sequential convergence, along with convergence rates for both the iterates and objective function values, is then established without requiring differentiability of the concave part. Our analysis not only extends but also unifies and improves recent convergence analyses in nonconvex settings. We benchmark our proposed algorithm with notable algorithms in the literature to show its competitiveness on both synthetic data and real power system load data.","sentences":["In this paper, we consider a class of structured nonconvex nonsmooth optimization problems whose objective function is the sum of three nonconvex functions, one of which is expressed in a difference-of-convex (DC) form.","This problem class covers several important structures in the literature including the sum of three functions and the general DC program.","We propose a splitting algorithm and prove the subsequential convergence to a stationary point of the problem.","The full sequential convergence, along with convergence rates for both the iterates and objective function values, is then established without requiring differentiability of the concave part.","Our analysis not only extends but also unifies and improves recent convergence analyses in nonconvex settings.","We benchmark our proposed algorithm with notable algorithms in the literature to show its competitiveness on both synthetic data and real power system load data."],"url":"http://arxiv.org/abs/2405.08485v1","category":"math.OC"}
{"created":"2024-05-14 09:54:52","title":"Impurity Parallel Velocity Gradient instability","abstract":"In magnetized plasmas, a radial gradient of parallel velocity, where parallel refers to the direction of magnetic field, can destabilise an electrostatic mode called as Parallel Velocity Gradient (PVG). The theory of PVG has been mainly developed assuming a single species of ions. Here, the role of impurities is investigated based on a linear, local analysis, in a homogeneous, constant magnetic field. To further simplify the analysis, the plasma is assumed to contain only two ion species - main ions and one impurity species - while our methodology can be straightforwardly extended to more species. In the cold-ion limit, retaining polarization drift for both main ions and impurity ions, and assuming Boltzmanian electrons, the system is described by 4 fluid equations closed by quasineutrality. The linearized equations can be reduced to 2 coupled equations: one for the electric potential, and one for the effective parallel velocity fluctuations, which is a linear combination of main ion and impurity parallel velocity fluctuations. This reduced system can be understood as a generalisation of the Hasegawa-Mima model. With finite radial gradient of impurity parallel flow, the linear dispersion relation then describes a new instability: the impurity PVG (i-PVG). Instability condition is described in terms of either the main ion flow shear, or equivalently, an effective flow shear, which combines main ion and impurity flow shears. Impurities can have a stabilising or destabilising role, depending on the parameters, and in particular the direction of main flow shear against impurity flow shear. Assuming a reasonable value of perpendicular wavenumber, the maximum growthrate is estimated, depending on impurity mass, charge, and concentration.","sentences":["In magnetized plasmas, a radial gradient of parallel velocity, where parallel refers to the direction of magnetic field, can destabilise an electrostatic mode called as Parallel Velocity Gradient (PVG).","The theory of PVG has been mainly developed assuming a single species of ions.","Here, the role of impurities is investigated based on a linear, local analysis, in a homogeneous, constant magnetic field.","To further simplify the analysis, the plasma is assumed to contain only two ion species - main ions and one impurity species - while our methodology can be straightforwardly extended to more species.","In the cold-ion limit, retaining polarization drift for both main ions and impurity ions, and assuming Boltzmanian electrons, the system is described by 4 fluid equations closed by quasineutrality.","The linearized equations can be reduced to 2 coupled equations: one for the electric potential, and one for the effective parallel velocity fluctuations, which is a linear combination of main ion and impurity parallel velocity fluctuations.","This reduced system can be understood as a generalisation of the Hasegawa-Mima model.","With finite radial gradient of impurity parallel flow, the linear dispersion relation then describes a new instability: the impurity PVG (i-PVG).","Instability condition is described in terms of either the main ion flow shear, or equivalently, an effective flow shear, which combines main ion and impurity flow shears.","Impurities can have a stabilising or destabilising role, depending on the parameters, and in particular the direction of main flow shear against impurity flow shear.","Assuming a reasonable value of perpendicular wavenumber, the maximum growthrate is estimated, depending on impurity mass, charge, and concentration."],"url":"http://arxiv.org/abs/2405.08472v1","category":"physics.plasm-ph"}
{"created":"2024-05-14 09:32:19","title":"Velocity-vorticity geometric constraints for the energy conservation of 3D ideal incompressible fluids","abstract":"In this paper we consider the 3D Euler equations and we first prove a criterion for energy conservation for weak solutions with velocity satisfying additional assumptions in fractional Sobolev spaces with respect to the space variables, balanced by proper integrability with respect to time. Next, we apply the criterion to study the energy conservation of solution of the Beltrami type, carefully applying properties of products in (fractional and possibly negative) Sobolev spaces and employing a suitable bootstrap argument.","sentences":["In this paper we consider the 3D Euler equations and we first prove a criterion for energy conservation for weak solutions with velocity satisfying additional assumptions in fractional Sobolev spaces with respect to the space variables, balanced by proper integrability with respect to time.","Next, we apply the criterion to study the energy conservation of solution of the Beltrami type, carefully applying properties of products in (fractional and possibly negative)","Sobolev spaces and employing a suitable bootstrap argument."],"url":"http://arxiv.org/abs/2405.08461v1","category":"math.AP"}
{"created":"2024-05-14 09:11:17","title":"A mean curvature type flow with capillary boundary in a horoball in hyperbolic space","abstract":"In this paper, we study a mean curvature type flow with capillary boundary in a horoball in hyperbolic space. Our flow preserves the volume of the bounded domain enclosed by the hypersurface and monotonically decreases the energy functional. We show that it has the long time existence and converges to a truncated umbilical hypersurface in hyperbolic space. As an application, we solve an isoperimetric type problem for hypersurfaces with capillary boundary in a horoball.","sentences":["In this paper, we study a mean curvature type flow with capillary boundary in a horoball in hyperbolic space.","Our flow preserves the volume of the bounded domain enclosed by the hypersurface and monotonically decreases the energy functional.","We show that it has the long time existence and converges to a truncated umbilical hypersurface in hyperbolic space.","As an application, we solve an isoperimetric type problem for hypersurfaces with capillary boundary in a horoball."],"url":"http://arxiv.org/abs/2405.08446v1","category":"math.DG"}
{"created":"2024-05-14 08:45:46","title":"Conformal product structures on compact K\u00e4hler manifolds","abstract":"A conformal product structure on a Riemannian manifold is a Weyl connection with reducible holonomy. We give the geometric description of all compact K\\\"ahler manifolds admitting conformal product structures","sentences":["A conformal product structure on a Riemannian manifold is a Weyl connection with reducible holonomy.","We give the geometric description of all compact K\\\"ahler manifolds admitting conformal product structures"],"url":"http://arxiv.org/abs/2405.08430v1","category":"math.DG"}
{"created":"2024-05-14 08:45:34","title":"TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection","abstract":"Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.","sentences":["Robust road surface estimation is required for autonomous ground vehicles to navigate safely.","Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments.","In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface.","Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures.","Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset.","Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface.","The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications."],"url":"http://arxiv.org/abs/2405.08429v1","category":"cs.CV"}
{"created":"2024-05-14 17:59:40","title":"Efficient Vision-Language Pre-training by Cluster Masking","abstract":"We propose a simple strategy for masking image patches during visual-language contrastive learning that improves the quality of the learned representations and the training speed. During each iteration of training, we randomly mask clusters of visually similar image patches, as measured by their raw pixel intensities. This provides an extra learning signal, beyond the contrastive training itself, since it forces a model to predict words for masked visual structures solely from context. It also speeds up training by reducing the amount of data used in each image. We evaluate the effectiveness of our model by pre-training on a number of benchmarks, finding that it outperforms other masking strategies, such as FLIP, on the quality of the learned representation.","sentences":["We propose a simple strategy for masking image patches during visual-language contrastive learning that improves the quality of the learned representations and the training speed.","During each iteration of training, we randomly mask clusters of visually similar image patches, as measured by their raw pixel intensities.","This provides an extra learning signal, beyond the contrastive training itself, since it forces a model to predict words for masked visual structures solely from context.","It also speeds up training by reducing the amount of data used in each image.","We evaluate the effectiveness of our model by pre-training on a number of benchmarks, finding that it outperforms other masking strategies, such as FLIP, on the quality of the learned representation."],"url":"http://arxiv.org/abs/2405.08815v1","category":"cs.CV"}
{"created":"2024-05-14 17:59:02","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","abstract":"Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video. To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding. This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data. Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene. Additionally, we evaluate recent video-centric LLMs, both open-source and proprietary, on the test split of our dataset. The findings reveal that even state-of-the-art video-centric LLMs significantly lag behind human performance in these tasks, highlighting the complexity and challenge inherent in video understanding. The dataset is available at https://hf.co/datasets/tomg-group-umd/cinepile","sentences":["Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video.","To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding.","This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data.","Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene.","Additionally, we evaluate recent video-centric LLMs, both open-source and proprietary, on the test split of our dataset.","The findings reveal that even state-of-the-art video-centric LLMs significantly lag behind human performance in these tasks, highlighting the complexity and challenge inherent in video understanding.","The dataset is available at https://hf.co/datasets/tomg-group-umd/cinepile"],"url":"http://arxiv.org/abs/2405.08813v1","category":"cs.CV"}
{"created":"2024-05-14 17:49:18","title":"Prospects of Privacy Advantage in Quantum Machine Learning","abstract":"Ensuring data privacy in machine learning models is critical, particularly in distributed settings where model gradients are typically shared among multiple parties to allow collaborative learning. Motivated by the increasing success of recovering input data from the gradients of classical models, this study addresses a central question: How hard is it to recover the input data from the gradients of quantum machine learning models? Focusing on variational quantum circuits (VQC) as learning models, we uncover the crucial role played by the dynamical Lie algebra (DLA) of the VQC ansatz in determining privacy vulnerabilities. While the DLA has previously been linked to the classical simulatability and trainability of VQC models, this work, for the first time, establishes its connection to the privacy of VQC models. In particular, we show that properties conducive to the trainability of VQCs, such as a polynomial-sized DLA, also facilitate the extraction of detailed snapshots of the input. We term this a weak privacy breach, as the snapshots enable training VQC models for distinct learning tasks without direct access to the original input. Further, we investigate the conditions for a strong privacy breach where the original input data can be recovered from these snapshots by classical or quantum-assisted polynomial time methods. We establish conditions on the encoding map such as classical simulatability, overlap with DLA basis, and its Fourier frequency characteristics that enable such a privacy breach of VQC models. Our findings thus play a crucial role in detailing the prospects of quantum privacy advantage by guiding the requirements for designing quantum machine learning models that balance trainability with robust privacy protection.","sentences":["Ensuring data privacy in machine learning models is critical, particularly in distributed settings where model gradients are typically shared among multiple parties to allow collaborative learning.","Motivated by the increasing success of recovering input data from the gradients of classical models, this study addresses a central question: How hard is it to recover the input data from the gradients of quantum machine learning models?","Focusing on variational quantum circuits (VQC) as learning models, we uncover the crucial role played by the dynamical Lie algebra (DLA) of the VQC ansatz in determining privacy vulnerabilities.","While the DLA has previously been linked to the classical simulatability and trainability of VQC models, this work, for the first time, establishes its connection to the privacy of VQC models.","In particular, we show that properties conducive to the trainability of VQCs, such as a polynomial-sized DLA, also facilitate the extraction of detailed snapshots of the input.","We term this a weak privacy breach, as the snapshots enable training VQC models for distinct learning tasks without direct access to the original input.","Further, we investigate the conditions for a strong privacy breach where the original input data can be recovered from these snapshots by classical or quantum-assisted polynomial time methods.","We establish conditions on the encoding map such as classical simulatability, overlap with DLA basis, and its Fourier frequency characteristics that enable such a privacy breach of VQC models.","Our findings thus play a crucial role in detailing the prospects of quantum privacy advantage by guiding the requirements for designing quantum machine learning models that balance trainability with robust privacy protection."],"url":"http://arxiv.org/abs/2405.08801v2","category":"quant-ph"}
{"created":"2024-05-14 17:41:55","title":"A Brief Introduction to Causal Inference in Machine Learning","abstract":"This is a lecture note produced for DS-GA 3001.003 \"Special Topics in DS - Causal Inference in Machine Learning\" at the Center for Data Science, New York University in Spring, 2024. This course was created to target master's and PhD level students with basic background in machine learning but who were not exposed to causal inference or causal reasoning in general previously. In particular, this course focuses on introducing such students to expand their view and knowledge of machine learning to incorporate causal reasoning, as this aspect is at the core of so-called out-of-distribution generalization (or lack thereof.)","sentences":["This is a lecture note produced for DS-GA 3001.003 \"Special Topics in DS - Causal Inference in Machine Learning\" at the Center for Data Science, New York University in Spring, 2024.","This course was created to target master's and PhD level students with basic background in machine learning but who were not exposed to causal inference or causal reasoning in general previously.","In particular, this course focuses on introducing such students to expand their view and knowledge of machine learning to incorporate causal reasoning, as this aspect is at the core of so-called out-of-distribution generalization (or lack thereof.)"],"url":"http://arxiv.org/abs/2405.08793v1","category":"cs.LG"}
{"created":"2024-05-14 16:40:45","title":"Stable Inverse Reinforcement Learning: Policies from Control Lyapunov Landscapes","abstract":"Learning from expert demonstrations to flexibly program an autonomous system with complex behaviors or to predict an agent's behavior is a powerful tool, especially in collaborative control settings. A common method to solve this problem is inverse reinforcement learning (IRL), where the observed agent, e.g., a human demonstrator, is assumed to behave according to the optimization of an intrinsic cost function that reflects its intent and informs its control actions. While the framework is expressive, it is also computationally demanding and generally lacks convergence guarantees. We therefore propose a novel, stability-certified IRL approach by reformulating the cost function inference problem to learning control Lyapunov functions (CLF) from demonstrations data. By additionally exploiting closed-form expressions for associated control policies, we are able to efficiently search the space of CLFs by observing the attractor landscape of the induced dynamics. For the construction of the inverse optimal CLFs, we use a Sum of Squares and formulate a convex optimization problem. We present a theoretical analysis of the optimality properties provided by the CLF and evaluate our approach using both simulated and real-world data.","sentences":["Learning from expert demonstrations to flexibly program an autonomous system with complex behaviors or to predict an agent's behavior is a powerful tool, especially in collaborative control settings.","A common method to solve this problem is inverse reinforcement learning (IRL), where the observed agent, e.g., a human demonstrator, is assumed to behave according to the optimization of an intrinsic cost function that reflects its intent and informs its control actions.","While the framework is expressive, it is also computationally demanding and generally lacks convergence guarantees.","We therefore propose a novel, stability-certified IRL approach by reformulating the cost function inference problem to learning control Lyapunov functions (CLF) from demonstrations data.","By additionally exploiting closed-form expressions for associated control policies, we are able to efficiently search the space of CLFs by observing the attractor landscape of the induced dynamics.","For the construction of the inverse optimal CLFs, we use a Sum of Squares and formulate a convex optimization problem.","We present a theoretical analysis of the optimality properties provided by the CLF and evaluate our approach using both simulated and real-world data."],"url":"http://arxiv.org/abs/2405.08756v1","category":"eess.SY"}
{"created":"2024-05-14 16:40:06","title":"Hierarchical Resource Partitioning on Modern GPUs: A Reinforcement Learning Approach","abstract":"GPU-based heterogeneous architectures are now commonly used in HPC clusters. Due to their architectural simplicity specialized for data-level parallelism, GPUs can offer much higher computational throughput and memory bandwidth than CPUs in the same generation do. However, as the available resources in GPUs have increased exponentially over the past decades, it has become increasingly difficult for a single program to fully utilize them. As a consequence, the industry has started supporting several resource partitioning features in order to improve the resource utilization by co-scheduling multiple programs on the same GPU die at the same time. Driven by the technological trend, this paper focuses on hierarchical resource partitioning on modern GPUs, and as an example, we utilize a combination of two different features available on recent NVIDIA GPUs in a hierarchical manner: MPS (Multi-Process Service), a finer-grained logical partitioning; and MIG (Multi-Instance GPU), a coarse-grained physical partitioning. We propose a method for comprehensively co-optimizing the setup of hierarchical partitioning and the selection of co-scheduling groups from a given set of jobs, based on reinforcement learning using their profiles. Our thorough experimental results demonstrate that our approach can successfully set up job concurrency, partitioning, and co-scheduling group selections simultaneously. This results in a maximum throughput improvement by a factor of 1.87 compared to the time-sharing scheduling.","sentences":["GPU-based heterogeneous architectures are now commonly used in HPC clusters.","Due to their architectural simplicity specialized for data-level parallelism, GPUs can offer much higher computational throughput and memory bandwidth than CPUs in the same generation do.","However, as the available resources in GPUs have increased exponentially over the past decades, it has become increasingly difficult for a single program to fully utilize them.","As a consequence, the industry has started supporting several resource partitioning features in order to improve the resource utilization by co-scheduling multiple programs on the same GPU die at the same time.","Driven by the technological trend, this paper focuses on hierarchical resource partitioning on modern GPUs, and as an example, we utilize a combination of two different features available on recent NVIDIA GPUs in a hierarchical manner: MPS (Multi-Process Service), a finer-grained logical partitioning; and MIG (Multi-Instance GPU), a coarse-grained physical partitioning.","We propose a method for comprehensively co-optimizing the setup of hierarchical partitioning and the selection of co-scheduling groups from a given set of jobs, based on reinforcement learning using their profiles.","Our thorough experimental results demonstrate that our approach can successfully set up job concurrency, partitioning, and co-scheduling group selections simultaneously.","This results in a maximum throughput improvement by a factor of 1.87 compared to the time-sharing scheduling."],"url":"http://arxiv.org/abs/2405.08754v1","category":"cs.DC"}
{"created":"2024-05-14 16:30:03","title":"Reinformer: Max-Return Sequence Modeling for offline RL","abstract":"As a data-driven paradigm, offline reinforcement learning (RL) has been formulated as sequence modeling that conditions on the hindsight information including returns, goal or future trajectory. Although promising, this supervised paradigm overlooks the core objective of RL that maximizes the return. This overlook directly leads to the lack of trajectory stitching capability that affects the sequence model learning from sub-optimal data. In this work, we introduce the concept of max-return sequence modeling which integrates the goal of maximizing returns into existing sequence models. We propose Reinforced Transformer (Reinformer), indicating the sequence model is reinforced by the RL objective. Reinformer additionally incorporates the objective of maximizing returns in the training phase, aiming to predict the maximum future return within the distribution. During inference, this in-distribution maximum return will guide the selection of optimal actions. Empirically, Reinformer is competitive with classical RL methods on the D4RL benchmark and outperforms state-of-the-art sequence model particularly in trajectory stitching ability. Code is public at \\url{https://github.com/Dragon-Zhuang/Reinformer}.","sentences":["As a data-driven paradigm, offline reinforcement learning (RL) has been formulated as sequence modeling that conditions on the hindsight information including returns, goal or future trajectory.","Although promising, this supervised paradigm overlooks the core objective of RL that maximizes the return.","This overlook directly leads to the lack of trajectory stitching capability that affects the sequence model learning from sub-optimal data.","In this work, we introduce the concept of max-return sequence modeling which integrates the goal of maximizing returns into existing sequence models.","We propose Reinforced Transformer (Reinformer), indicating the sequence model is reinforced by the RL objective.","Reinformer additionally incorporates the objective of maximizing returns in the training phase, aiming to predict the maximum future return within the distribution.","During inference, this in-distribution maximum return will guide the selection of optimal actions.","Empirically, Reinformer is competitive with classical RL methods on the D4RL benchmark and outperforms state-of-the-art sequence model particularly in trajectory stitching ability.","Code is public at \\url{https://github.com/Dragon-Zhuang/Reinformer}."],"url":"http://arxiv.org/abs/2405.08740v1","category":"cs.LG"}
{"created":"2024-05-14 15:48:36","title":"Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory","abstract":"Increasing the size of a Transformer model does not always lead to enhanced performance. This phenomenon cannot be explained by the empirical scaling laws. Furthermore, improved generalization ability occurs as the model memorizes the training samples. We present a theoretical framework that sheds light on the memorization process and performance dynamics of transformer-based language models. We model the behavior of Transformers with associative memories using Hopfield networks, such that each transformer block effectively conducts an approximate nearest-neighbor search. Based on this, we design an energy function analogous to that in the modern continuous Hopfield network which provides an insightful explanation for the attention mechanism. Using the majorization-minimization technique, we construct a global energy function that captures the layered architecture of the Transformer. Under specific conditions, we show that the minimum achievable cross-entropy loss is bounded from below by a constant approximately equal to 1. We substantiate our theoretical results by conducting experiments with GPT-2 on various data sizes, as well as training vanilla Transformers on a dataset of 2M tokens.","sentences":["Increasing the size of a Transformer model does not always lead to enhanced performance.","This phenomenon cannot be explained by the empirical scaling laws.","Furthermore, improved generalization ability occurs as the model memorizes the training samples.","We present a theoretical framework that sheds light on the memorization process and performance dynamics of transformer-based language models.","We model the behavior of Transformers with associative memories using Hopfield networks, such that each transformer block effectively conducts an approximate nearest-neighbor search.","Based on this, we design an energy function analogous to that in the modern continuous Hopfield network which provides an insightful explanation for the attention mechanism.","Using the majorization-minimization technique, we construct a global energy function that captures the layered architecture of the Transformer.","Under specific conditions, we show that the minimum achievable cross-entropy loss is bounded from below by a constant approximately equal to 1.","We substantiate our theoretical results by conducting experiments with GPT-2 on various data sizes, as well as training vanilla Transformers on a dataset of 2M tokens."],"url":"http://arxiv.org/abs/2405.08707v1","category":"cs.LG"}
{"created":"2024-05-14 15:42:55","title":"Full Line Code Completion: Bringing AI to Desktop","abstract":"In recent years, several industrial solutions for the problem of multi-token code completion have appeared, each making a great advance in the area but mostly focusing on cloud-based runtime and avoiding working on the end user's device.   In this work, we describe our approach for building a multi-token code completion feature for the JetBrains' IntelliJ Platform, which we call Full Line Code Completion. The feature suggests only syntactically correct code and works fully locally, i.e., data querying and the generation of suggestions happens on the end user's machine. We share important time and memory-consumption restrictions, as well as design principles that a code completion engine should satisfy. Working entirely on the end user's device, our code completion engine enriches user experience while being not only fast and compact but also secure. We share a number of useful techniques to meet the stated development constraints and also describe offline and online evaluation pipelines that allowed us to make better decisions.   Our online evaluation shows that the usage of the tool leads to 1.5 times more code in the IDE being produced by code completion. The described solution was initially started with the help of researchers and was bundled into two JetBrains' IDEs - PyCharm Pro and DataSpell - at the end of 2023, so we believe that this work is useful for bridging academia and industry, providing researchers with the knowledge of what happens when complex research-based solutions are integrated into real products.","sentences":["In recent years, several industrial solutions for the problem of multi-token code completion have appeared, each making a great advance in the area but mostly focusing on cloud-based runtime and avoiding working on the end user's device.   ","In this work, we describe our approach for building a multi-token code completion feature for the JetBrains' IntelliJ Platform, which we call Full Line Code Completion.","The feature suggests only syntactically correct code and works fully locally, i.e., data querying and the generation of suggestions happens on the end user's machine.","We share important time and memory-consumption restrictions, as well as design principles that a code completion engine should satisfy.","Working entirely on the end user's device, our code completion engine enriches user experience while being not only fast and compact but also secure.","We share a number of useful techniques to meet the stated development constraints and also describe offline and online evaluation pipelines that allowed us to make better decisions.   ","Our online evaluation shows that the usage of the tool leads to 1.5 times more code in the IDE being produced by code completion.","The described solution was initially started with the help of researchers and was bundled into two JetBrains' IDEs - PyCharm Pro and DataSpell - at the end of 2023, so we believe that this work is useful for bridging academia and industry, providing researchers with the knowledge of what happens when complex research-based solutions are integrated into real products."],"url":"http://arxiv.org/abs/2405.08704v1","category":"cs.SE"}
{"created":"2024-05-14 15:39:22","title":"Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity","abstract":"Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems. However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes. To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL. KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints. This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery. It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios. In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data. Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency. For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data. In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise.","sentences":["Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems.","However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes.","To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL.","KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints.","This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery.","It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios.","In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data.","Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency.","For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data.","In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise."],"url":"http://arxiv.org/abs/2405.08699v1","category":"stat.ML"}
{"created":"2024-05-14 15:37:56","title":"Byzantine-Resilient Secure Aggregation for Federated Learning Without Privacy Compromises","abstract":"Federated learning (FL) shows great promise in large scale machine learning, but brings new risks in terms of privacy and security. We propose ByITFL, a novel scheme for FL that provides resilience against Byzantine users while keeping the users' data private from the federator and private from other users. The scheme builds on the preexisting non-private FLTrust scheme, which tolerates malicious users through trust scores (TS) that attenuate or amplify the users' gradients. The trust scores are based on the ReLU function, which we approximate by a polynomial. The distributed and privacy-preserving computation in ByITFL is designed using a combination of Lagrange coded computing, verifiable secret sharing and re-randomization steps. ByITFL is the first Byzantine resilient scheme for FL with full information-theoretic privacy.","sentences":["Federated learning (FL) shows great promise in large scale machine learning, but brings new risks in terms of privacy and security.","We propose ByITFL, a novel scheme for FL that provides resilience against Byzantine users while keeping the users' data private from the federator and private from other users.","The scheme builds on the preexisting non-private FLTrust scheme, which tolerates malicious users through trust scores (TS) that attenuate or amplify the users' gradients.","The trust scores are based on the ReLU function, which we approximate by a polynomial.","The distributed and privacy-preserving computation in ByITFL is designed using a combination of Lagrange coded computing, verifiable secret sharing and re-randomization steps.","ByITFL is the first Byzantine resilient scheme for FL with full information-theoretic privacy."],"url":"http://arxiv.org/abs/2405.08698v1","category":"cs.IT"}
{"created":"2024-05-14 15:28:48","title":"The impact of Compositionality in Zero-shot Multi-label action recognition for Object-based tasks","abstract":"Addressing multi-label action recognition in videos represents a significant challenge for robotic applications in dynamic environments, especially when the robot is required to cooperate with humans in tasks that involve objects. Existing methods still struggle to recognize unseen actions or require extensive training data. To overcome these problems, we propose Dual-VCLIP, a unified approach for zero-shot multi-label action recognition. Dual-VCLIP enhances VCLIP, a zero-shot action recognition method, with the DualCoOp method for multi-label image classification. The strength of our method is that at training time it only learns two prompts, and it is therefore much simpler than other methods. We validate our method on the Charades dataset that includes a majority of object-based actions, demonstrating that -- despite its simplicity -- our method performs favorably with respect to existing methods on the complete dataset, and promising performance when tested on unseen actions. Our contribution emphasizes the impact of verb-object class-splits during robots' training for new cooperative tasks, highlighting the influence on the performance and giving insights into mitigating biases.","sentences":["Addressing multi-label action recognition in videos represents a significant challenge for robotic applications in dynamic environments, especially when the robot is required to cooperate with humans in tasks that involve objects.","Existing methods still struggle to recognize unseen actions or require extensive training data.","To overcome these problems, we propose Dual-VCLIP, a unified approach for zero-shot multi-label action recognition.","Dual-VCLIP enhances VCLIP, a zero-shot action recognition method, with the DualCoOp method for multi-label image classification.","The strength of our method is that at training time it only learns two prompts, and it is therefore much simpler than other methods.","We validate our method on the Charades dataset that includes a majority of object-based actions, demonstrating that -- despite its simplicity -- our method performs favorably with respect to existing methods on the complete dataset, and promising performance when tested on unseen actions.","Our contribution emphasizes the impact of verb-object class-splits during robots' training for new cooperative tasks, highlighting the influence on the performance and giving insights into mitigating biases."],"url":"http://arxiv.org/abs/2405.08695v1","category":"cs.CV"}
{"created":"2024-05-14 15:24:52","title":"Enhancing Reinforcement Learning in Sensor Fusion: A Comparative Analysis of Cubature and Sampling-based Integration Methods for Rover Search Planning","abstract":"This study investigates the computational speed and accuracy of two numerical integration methods, cubature and sampling-based, for integrating an integrand over a 2D polygon. Using a group of rovers searching the Martian surface with a limited sensor footprint as a test bed, the relative error and computational time are compared as the area was subdivided to improve accuracy in the sampling-based approach. The results show that the sampling-based approach exhibits a $14.75\\%$ deviation in relative error compared to cubature when it matches the computational performance at $100\\%$. Furthermore, achieving a relative error below $1\\%$ necessitates a $10000\\%$ increase in relative time to calculate due to the $\\mathcal{O}(N^2)$ complexity of the sampling-based method. It is concluded that for enhancing reinforcement learning capabilities and other high iteration algorithms, the cubature method is preferred over the sampling-based method.","sentences":["This study investigates the computational speed and accuracy of two numerical integration methods, cubature and sampling-based, for integrating an integrand over a 2D polygon.","Using a group of rovers searching the Martian surface with a limited sensor footprint as a test bed, the relative error and computational time are compared as the area was subdivided to improve accuracy in the sampling-based approach.","The results show that the sampling-based approach exhibits a $14.75\\%$ deviation in relative error compared to cubature when it matches the computational performance at $100\\%$. Furthermore, achieving a relative error below $1\\%$ necessitates a $10000\\%$ increase in relative time to calculate due to the $\\mathcal{O}(N^2)$ complexity of the sampling-based method.","It is concluded that for enhancing reinforcement learning capabilities and other high iteration algorithms, the cubature method is preferred over the sampling-based method."],"url":"http://arxiv.org/abs/2405.08691v2","category":"cs.RO"}
{"created":"2024-05-14 15:11:39","title":"Learning How to Dynamically Decouple","abstract":"Current quantum computers suffer from noise that stems from interactions between the quantum system that constitutes the quantum device and its environment. These interactions can be suppressed through dynamical decoupling to reduce computational errors. However, the performance of dynamical decoupling depends on the type of the system-environment interactions that are present, which often lack an accurate model in quantum devices. We show that the performance of dynamical decoupling can be improved by optimizing its rotational gates to tailor them to the quantum hardware. We find that compared to canonical decoupling sequences, such as CPMG and XY4, the optimized dynamical decoupling sequences yield the best performance in suppressing noise in superconducting qubits. Our work thus enhances existing error suppression methods which helps increase circuit depth and result quality on noisy hardware.","sentences":["Current quantum computers suffer from noise that stems from interactions between the quantum system that constitutes the quantum device and its environment.","These interactions can be suppressed through dynamical decoupling to reduce computational errors.","However, the performance of dynamical decoupling depends on the type of the system-environment interactions that are present, which often lack an accurate model in quantum devices.","We show that the performance of dynamical decoupling can be improved by optimizing its rotational gates to tailor them to the quantum hardware.","We find that compared to canonical decoupling sequences, such as CPMG and XY4, the optimized dynamical decoupling sequences yield the best performance in suppressing noise in superconducting qubits.","Our work thus enhances existing error suppression methods which helps increase circuit depth and result quality on noisy hardware."],"url":"http://arxiv.org/abs/2405.08689v1","category":"quant-ph"}
{"created":"2024-05-14 14:22:14","title":"Output-decomposed Learning of Mealy Machines","abstract":"We present an active automata learning algorithm which learns a decomposition of a finite state machine, based on projecting onto individual outputs. This is dual to a recent compositional learning algorithm by Labbaf et al. (2023). When projecting the outputs to a smaller set, the model itself is reduced in size. By having several such projections, we do not lose any information and the full system can be reconstructed. Depending on the structure of the system this reduces the number of queries drastically, as shown by a preliminary evaluation of the algorithm.","sentences":["We present an active automata learning algorithm which learns a decomposition of a finite state machine, based on projecting onto individual outputs.","This is dual to a recent compositional learning algorithm by Labbaf et al. (2023).","When projecting the outputs to a smaller set, the model itself is reduced in size.","By having several such projections, we do not lose any information and the full system can be reconstructed.","Depending on the structure of the system this reduces the number of queries drastically, as shown by a preliminary evaluation of the algorithm."],"url":"http://arxiv.org/abs/2405.08647v1","category":"cs.LO"}
{"created":"2024-05-14 14:18:25","title":"vMFER: Von Mises-Fisher Experience Resampling Based on Uncertainty of Gradient Directions for Policy Improvement","abstract":"Reinforcement Learning (RL) is a widely employed technique in decision-making problems, encompassing two fundamental operations -- policy evaluation and policy improvement. Enhancing learning efficiency remains a key challenge in RL, with many efforts focused on using ensemble critics to boost policy evaluation efficiency. However, when using multiple critics, the actor in the policy improvement process can obtain different gradients. Previous studies have combined these gradients without considering their disagreements. Therefore, optimizing the policy improvement process is crucial to enhance learning efficiency. This study focuses on investigating the impact of gradient disagreements caused by ensemble critics on policy improvement. We introduce the concept of uncertainty of gradient directions as a means to measure the disagreement among gradients utilized in the policy improvement process. Through measuring the disagreement among gradients, we find that transitions with lower uncertainty of gradient directions are more reliable in the policy improvement process. Building on this analysis, we propose a method called von Mises-Fisher Experience Resampling (vMFER), which optimizes the policy improvement process by resampling transitions and assigning higher confidence to transitions with lower uncertainty of gradient directions. Our experiments demonstrate that vMFER significantly outperforms the benchmark and is particularly well-suited for ensemble structures in RL.","sentences":["Reinforcement Learning (RL) is a widely employed technique in decision-making problems, encompassing two fundamental operations -- policy evaluation and policy improvement.","Enhancing learning efficiency remains a key challenge in RL, with many efforts focused on using ensemble critics to boost policy evaluation efficiency.","However, when using multiple critics, the actor in the policy improvement process can obtain different gradients.","Previous studies have combined these gradients without considering their disagreements.","Therefore, optimizing the policy improvement process is crucial to enhance learning efficiency.","This study focuses on investigating the impact of gradient disagreements caused by ensemble critics on policy improvement.","We introduce the concept of uncertainty of gradient directions as a means to measure the disagreement among gradients utilized in the policy improvement process.","Through measuring the disagreement among gradients, we find that transitions with lower uncertainty of gradient directions are more reliable in the policy improvement process.","Building on this analysis, we propose a method called von Mises-Fisher Experience Resampling (vMFER), which optimizes the policy improvement process by resampling transitions and assigning higher confidence to transitions with lower uncertainty of gradient directions.","Our experiments demonstrate that vMFER significantly outperforms the benchmark and is particularly well-suited for ensemble structures in RL."],"url":"http://arxiv.org/abs/2405.08638v1","category":"cs.LG"}
{"created":"2024-05-14 14:15:31","title":"Drift Detection: Introducing Gaussian Split Detector","abstract":"Recent research yielded a wide array of drift detectors. However, in order to achieve remarkable performance, the true class labels must be available during the drift detection phase. This paper targets at detecting drift when the ground truth is unknown during the detection phase. To that end, we introduce Gaussian Split Detector (GSD) a novel drift detector that works in batch mode. GSD is designed to work when the data follow a normal distribution and makes use of Gaussian mixture models to monitor changes in the decision boundary. The algorithm is designed to handle multi-dimension data streams and to work without the ground truth labels during the inference phase making it pertinent for real world use. In an extensive experimental study on real and synthetic datasets, we evaluate our detector against the state of the art. We show that our detector outperforms the state of the art in detecting real drift and in ignoring virtual drift which is key to avoid false alarms.","sentences":["Recent research yielded a wide array of drift detectors.","However, in order to achieve remarkable performance, the true class labels must be available during the drift detection phase.","This paper targets at detecting drift when the ground truth is unknown during the detection phase.","To that end, we introduce Gaussian Split Detector (GSD) a novel drift detector that works in batch mode.","GSD is designed to work when the data follow a normal distribution and makes use of Gaussian mixture models to monitor changes in the decision boundary.","The algorithm is designed to handle multi-dimension data streams and to work without the ground truth labels during the inference phase making it pertinent for real world use.","In an extensive experimental study on real and synthetic datasets, we evaluate our detector against the state of the art.","We show that our detector outperforms the state of the art in detecting real drift and in ignoring virtual drift which is key to avoid false alarms."],"url":"http://arxiv.org/abs/2405.08637v1","category":"cs.DC"}
{"created":"2024-05-14 14:14:23","title":"Optimal design of experiments in the context of machine-learning inter-atomic potentials: improving the efficiency and transferability of kernel based methods","abstract":"Data-driven, machine learning (ML) models of atomistic interactions are often based on flexible and non-physical functions that can relate nuanced aspects of atomic arrangements into predictions of energies and forces. As a result, these potentials are as good as the training data (usually results of so-called ab initio simulations) and we need to make sure that we have enough information for a model to become sufficiently accurate, reliable and transferable. The main challenge stems from the fact that descriptors of chemical environments are often sparse high-dimensional objects without a well-defined continuous metric. Therefore, it is rather unlikely that any ad hoc method of choosing training examples will be indiscriminate, and it will be easy to fall into the trap of confirmation bias, where the same narrow and biased sampling is used to generate train- and test- sets. We will demonstrate that classical concepts of statistical planning of experiments and optimal design can help to mitigate such problems at a relatively low computational cost. The key feature of the method we will investigate is that they allow us to assess the informativeness of data (how much we can improve the model by adding/swapping a training example) and verify if the training is feasible with the current set before obtaining any reference energies and forces -- a so-called off-line approach. In other words, we are focusing on an approach that is easy to implement and doesn't require sophisticated frameworks that involve automated access to high-performance computational (HPC).","sentences":["Data-driven, machine learning (ML) models of atomistic interactions are often based on flexible and non-physical functions that can relate nuanced aspects of atomic arrangements into predictions of energies and forces.","As a result, these potentials are as good as the training data (usually results of so-called ab initio simulations) and we need to make sure that we have enough information for a model to become sufficiently accurate, reliable and transferable.","The main challenge stems from the fact that descriptors of chemical environments are often sparse high-dimensional objects without a well-defined continuous metric.","Therefore, it is rather unlikely that any ad hoc method of choosing training examples will be indiscriminate, and it will be easy to fall into the trap of confirmation bias, where the same narrow and biased sampling is used to generate train- and test- sets.","We will demonstrate that classical concepts of statistical planning of experiments and optimal design can help to mitigate such problems at a relatively low computational cost.","The key feature of the method we will investigate is that they allow us to assess the informativeness of data (how much we can improve the model by adding/swapping a training example) and verify if the training is feasible with the current set before obtaining any reference energies and forces -- a so-called off-line approach.","In other words, we are focusing on an approach that is easy to implement and doesn't require sophisticated frameworks that involve automated access to high-performance computational (HPC)."],"url":"http://arxiv.org/abs/2405.08636v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 14:01:15","title":"RMT-BVQA: Recurrent Memory Transformer-based Blind Video Quality Assessment for Enhanced Video Content","abstract":"With recent advances in deep learning, numerous algorithms have been developed to enhance video quality, reduce visual artefacts and improve perceptual quality. However, little research has been reported on the quality assessment of enhanced content - the evaluation of enhancement methods is often based on quality metrics that were designed for compression applications. In this paper, we propose a novel blind deep video quality assessment (VQA) method specifically for enhanced video content. It employs a new Recurrent Memory Transformer (RMT) based network architecture to obtain video quality representations, which is optimised through a novel content-quality-aware contrastive learning strategy based on a new database containing 13K training patches with enhanced content. The extracted quality representations are then combined through linear regression to generate video-level quality indices. The proposed method, RMT-BVQA, has been evaluated on the VDPVE (VQA Dataset for Perceptual Video Enhancement) database through a five-fold cross validation. The results show its superior correlation performance when compared to ten existing no-reference quality metrics.","sentences":["With recent advances in deep learning, numerous algorithms have been developed to enhance video quality, reduce visual artefacts and improve perceptual quality.","However, little research has been reported on the quality assessment of enhanced content - the evaluation of enhancement methods is often based on quality metrics that were designed for compression applications.","In this paper, we propose a novel blind deep video quality assessment (VQA) method specifically for enhanced video content.","It employs a new Recurrent Memory Transformer (RMT) based network architecture to obtain video quality representations, which is optimised through a novel content-quality-aware contrastive learning strategy based on a new database containing 13K training patches with enhanced content.","The extracted quality representations are then combined through linear regression to generate video-level quality indices.","The proposed method, RMT-BVQA, has been evaluated on the VDPVE (VQA Dataset for Perceptual Video Enhancement) database through a five-fold cross validation.","The results show its superior correlation performance when compared to ten existing no-reference quality metrics."],"url":"http://arxiv.org/abs/2405.08621v2","category":"eess.IV"}
{"created":"2024-05-14 13:37:36","title":"Risks and Opportunities of Open-Source Generative AI","abstract":"Applications of Generative AI (Gen AI) are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about the potential risks of the technology, and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. This regulation is likely to put at risk the budding field of open-source generative AI. Using a three-stage framework for Gen AI development (near, mid and long-term), we analyze the risks and opportunities of open-source generative AI models with similar capabilities to the ones currently available (near to mid-term) and with greater capabilities (long-term). We argue that, overall, the benefits of open-source Gen AI outweigh its risks. As such, we encourage the open sourcing of models, training and evaluation data, and provide a set of recommendations and best practices for managing risks associated with open-source generative AI.","sentences":["Applications of Generative AI (Gen AI) are expected to revolutionize a number of different areas, ranging from science & medicine to education.","The potential for these seismic changes has triggered a lively debate about the potential risks of the technology, and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development.","This regulation is likely to put at risk the budding field of open-source generative AI.","Using a three-stage framework for Gen AI development (near, mid and long-term), we analyze the risks and opportunities of open-source generative AI models with similar capabilities to the ones currently available (near to mid-term) and with greater capabilities (long-term).","We argue that, overall, the benefits of open-source Gen AI outweigh its risks.","As such, we encourage the open sourcing of models, training and evaluation data, and provide a set of recommendations and best practices for managing risks associated with open-source generative AI."],"url":"http://arxiv.org/abs/2405.08597v1","category":"cs.LG"}
{"created":"2024-05-14 13:22:33","title":"Treatment Effect Estimation for User Interest Exploration on Recommender Systems","abstract":"Recommender systems learn personalized user preferences from user feedback like clicks. However, user feedback is usually biased towards partially observed interests, leaving many users' hidden interests unexplored. Existing approaches typically mitigate the bias, increase recommendation diversity, or use bandit algorithms to balance exploration-exploitation trade-offs. Nevertheless, they fail to consider the potential rewards of recommending different categories of items and lack the global scheduling of allocating top-N recommendations to categories, leading to suboptimal exploration. In this work, we propose an Uplift model-based Recommender (UpliftRec) framework, which regards top-N recommendation as a treatment optimization problem. UpliftRec estimates the treatment effects, i.e., the click-through rate (CTR) under different category exposure ratios, by using observational user feedback. UpliftRec calculates group-level treatment effects to discover users' hidden interests with high CTR rewards and leverages inverse propensity weighting to alleviate confounder bias. Thereafter, UpliftRec adopts a dynamic programming method to calculate the optimal treatment for overall CTR maximization. We implement UpliftRec on different backend models and conduct extensive experiments on three datasets. The empirical results validate the effectiveness of UpliftRec in discovering users' hidden interests while achieving superior recommendation accuracy.","sentences":["Recommender systems learn personalized user preferences from user feedback like clicks.","However, user feedback is usually biased towards partially observed interests, leaving many users' hidden interests unexplored.","Existing approaches typically mitigate the bias, increase recommendation diversity, or use bandit algorithms to balance exploration-exploitation trade-offs.","Nevertheless, they fail to consider the potential rewards of recommending different categories of items and lack the global scheduling of allocating top-N recommendations to categories, leading to suboptimal exploration.","In this work, we propose an Uplift model-based Recommender (UpliftRec) framework, which regards top-N recommendation as a treatment optimization problem.","UpliftRec estimates the treatment effects, i.e., the click-through rate (CTR) under different category exposure ratios, by using observational user feedback.","UpliftRec calculates group-level treatment effects to discover users' hidden interests with high CTR rewards and leverages inverse propensity weighting to alleviate confounder bias.","Thereafter, UpliftRec adopts a dynamic programming method to calculate the optimal treatment for overall CTR maximization.","We implement UpliftRec on different backend models and conduct extensive experiments on three datasets.","The empirical results validate the effectiveness of UpliftRec in discovering users' hidden interests while achieving superior recommendation accuracy."],"url":"http://arxiv.org/abs/2405.08582v1","category":"cs.IR"}
{"created":"2024-05-14 17:36:22","title":"Explicit Orthogonal Arrays and Universal Hashing with Arbitrary Parameters","abstract":"Orthogonal arrays are a type of combinatorial design that were developed in the 1940s in the design of statistical experiments. In 1947, Rao proved a lower bound on the size of any orthogonal array, and raised the problem of constructing arrays of minimum size. Kuperberg, Lovett and Peled (2017) gave a non-constructive existence proof of orthogonal arrays whose size is near-optimal (i.e., within a polynomial of Rao's lower bound), leaving open the question of an algorithmic construction. We give the first explicit, deterministic, algorithmic construction of orthogonal arrays achieving near-optimal size for all parameters. Our construction uses algebraic geometry codes.   In pseudorandomness, the notions of $t$-independent generators or $t$-independent hash functions are equivalent to orthogonal arrays. Classical constructions of $t$-independent hash functions are known when the size of the codomain is a prime power, but very few constructions are known for an arbitrary codomain. Our construction yields algorithmically efficient $t$-independent hash functions for arbitrary domain and codomain.","sentences":["Orthogonal arrays are a type of combinatorial design that were developed in the 1940s in the design of statistical experiments.","In 1947, Rao proved a lower bound on the size of any orthogonal array, and raised the problem of constructing arrays of minimum size.","Kuperberg, Lovett and Peled (2017) gave a non-constructive existence proof of orthogonal arrays whose size is near-optimal (i.e., within a polynomial of Rao's lower bound), leaving open the question of an algorithmic construction.","We give the first explicit, deterministic, algorithmic construction of orthogonal arrays achieving near-optimal size for all parameters.","Our construction uses algebraic geometry codes.   ","In pseudorandomness, the notions of $t$-independent generators or $t$-independent hash functions are equivalent to orthogonal arrays.","Classical constructions of $t$-independent hash functions are known when the size of the codomain is a prime power, but very few constructions are known for an arbitrary codomain.","Our construction yields algorithmically efficient $t$-independent hash functions for arbitrary domain and codomain."],"url":"http://arxiv.org/abs/2405.08787v1","category":"cs.DS"}
{"created":"2024-05-14 17:06:47","title":"Multi-objective SINDy for parameterized model discovery from single transient trajectory data","abstract":"The sparse identification of nonlinear dynamics (SINDy) has been established as an effective technique to produce interpretable models of dynamical systems from time-resolved state data via sparse regression. However, to model parameterized systems, SINDy requires data from transient trajectories for various parameter values over the range of interest, which are typically difficult to acquire experimentally. In this work, we extend SINDy to be able to leverage data on fixed points and/or limit cycles to reduce the number of transient trajectories needed for successful system identification. To achieve this, we incorporate the data on these attractors at various parameter values as constraints in the optimization problem. First, we show that enforcing these as hard constraints leads to an ill-conditioned regression problem due to the large number of constraints. Instead, we implement soft constraints by modifying the cost function to be minimized. This leads to the formulation of a multi-objective sparse regression problem where we simultaneously seek to minimize the error of the fit to the transients trajectories and to the data on attractors, while penalizing the number of terms in the model. Our extension, demonstrated on several numerical examples, is more robust to noisy measurements and requires substantially less training data than the original SINDy method to correctly identify a parameterized dynamical system.","sentences":["The sparse identification of nonlinear dynamics (SINDy) has been established as an effective technique to produce interpretable models of dynamical systems from time-resolved state data via sparse regression.","However, to model parameterized systems, SINDy requires data from transient trajectories for various parameter values over the range of interest, which are typically difficult to acquire experimentally.","In this work, we extend SINDy to be able to leverage data on fixed points and/or limit cycles to reduce the number of transient trajectories needed for successful system identification.","To achieve this, we incorporate the data on these attractors at various parameter values as constraints in the optimization problem.","First, we show that enforcing these as hard constraints leads to an ill-conditioned regression problem due to the large number of constraints.","Instead, we implement soft constraints by modifying the cost function to be minimized.","This leads to the formulation of a multi-objective sparse regression problem where we simultaneously seek to minimize the error of the fit to the transients trajectories and to the data on attractors, while penalizing the number of terms in the model.","Our extension, demonstrated on several numerical examples, is more robust to noisy measurements and requires substantially less training data than the original SINDy method to correctly identify a parameterized dynamical system."],"url":"http://arxiv.org/abs/2405.08771v1","category":"math.DS"}
{"created":"2024-05-14 17:06:06","title":"An optimization-based construction procedure for function space based summation-by-parts operators on arbitrary grids","abstract":"We introduce a novel construction procedure for one-dimensional summation-by-parts (SBP) operators. Existing construction procedures for FSBP operators of the form $D = P^{-1} Q$ proceed as follows: Given a boundary operator $B$, the norm matrix $P$ is first determined and then in a second step the complementary matrix $Q$ is calculated to finally get the FSBP operator $D$. In contrast, the approach proposed here determines the norm and complementary matrices, $P$ and $Q$, simultaneously by solving an optimization problem. The proposed construction procedure applies to classical SBP operators based on polynomial approximation and the broader class of function space SBP (FSBP) operators. According to our experiments, the presented approach yields a numerically stable construction procedure and FSBP operators with higher accuracy for diagonal norm difference operators at the boundaries than the traditional approach. Through numerical simulations, we highlight the advantages of our proposed technique.","sentences":["We introduce a novel construction procedure for one-dimensional summation-by-parts (SBP) operators.","Existing construction procedures for FSBP operators of the form $D = P^{-1} Q$ proceed as follows:","Given a boundary operator $B$, the norm matrix $P$ is first determined and then in a second step the complementary matrix $Q$ is calculated to finally get the FSBP operator $D$. In contrast, the approach proposed here determines the norm and complementary matrices, $P$ and $Q$, simultaneously by solving an optimization problem.","The proposed construction procedure applies to classical SBP operators based on polynomial approximation and the broader class of function space SBP (FSBP) operators.","According to our experiments, the presented approach yields a numerically stable construction procedure and FSBP operators with higher accuracy for diagonal norm difference operators at the boundaries than the traditional approach.","Through numerical simulations, we highlight the advantages of our proposed technique."],"url":"http://arxiv.org/abs/2405.08770v1","category":"math.NA"}
{"created":"2024-05-14 16:33:25","title":"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding","abstract":"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT","sentences":["We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese.","To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding.","We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization.","For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images.","Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context.","Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.","Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT"],"url":"http://arxiv.org/abs/2405.08748v1","category":"cs.CV"}
{"created":"2024-05-14 16:33:22","title":"Minimax optimal seriation in polynomial time","abstract":"We consider the statistical seriation problem, where the statistician seeks to recover a hidden ordering from a noisy observation of a permuted Robinson matrix. In this paper, we tightly characterize the minimax rate for this problem of matrix reordering when the Robinson matrix is bi-Lipschitz, and we also provide a polynomial time algorithm achieving this rate; thereby answering two open questions of [Giraud et al., 2021]. Our analysis further extends to broader classes of similarity matrices.","sentences":["We consider the statistical seriation problem, where the statistician seeks to recover a hidden ordering from a noisy observation of a permuted Robinson matrix.","In this paper, we tightly characterize the minimax rate for this problem of matrix reordering when the Robinson matrix is bi-Lipschitz, and we also provide a polynomial time algorithm achieving this rate; thereby answering two open questions of [Giraud et al., 2021].","Our analysis further extends to broader classes of similarity matrices."],"url":"http://arxiv.org/abs/2405.08747v1","category":"math.ST"}
{"created":"2024-05-14 16:31:06","title":"Robust self-testing of Bell inequalities tilted for maximal loophole-free nonlocality","abstract":"The degree of experimentally attainable nonlocality, as gauged by the amount of loophole-free violation of Bell inequalities, remains severely limited due to inefficient detectors. We address an experimentally motivated question: Which quantum strategies attain the maximal loophole-free nonlocality in the presence of inefficient detectors? For any Bell inequality and any specification of detection efficiencies, the optimal strategies are those that maximally violate a tilted version of the Bell inequality in ideal conditions. In the simplest scenario, we demonstrate that the quantum strategies that maximally violate the tilted versions of Clauser-Horne-Shimony-Holt inequality are unique up to local isometries. However, self-testing via the standard sum of squares decomposition method turns out to be analytically intractable since even high levels of the Navascu\\'es--Pironio--Ac\\'in hierarchy are insufficient to saturate the maximum quantum violation of these inequalities. Instead, we utilize a novel Jordan's lemma-based proof technique to obtain robust analytical self-testing statements for the entire family of tilted-Bell inequalities. These results allow us to unveil intriguing aspects of the effect of inefficient detectors and the complexity of characterizing the set of quantum correlations, in the simplest Bell scenario.","sentences":["The degree of experimentally attainable nonlocality, as gauged by the amount of loophole-free violation of Bell inequalities, remains severely limited due to inefficient detectors.","We address an experimentally motivated question: Which quantum strategies attain the maximal loophole-free nonlocality in the presence of inefficient detectors?","For any Bell inequality and any specification of detection efficiencies, the optimal strategies are those that maximally violate a tilted version of the Bell inequality in ideal conditions.","In the simplest scenario, we demonstrate that the quantum strategies that maximally violate the tilted versions of Clauser-Horne-Shimony-Holt inequality are unique up to local isometries.","However, self-testing via the standard sum of squares decomposition method turns out to be analytically intractable since even high levels of the Navascu\\'es--Pironio--Ac\\'in hierarchy are insufficient to saturate the maximum quantum violation of these inequalities.","Instead, we utilize a novel Jordan's lemma-based proof technique to obtain robust analytical self-testing statements for the entire family of tilted-Bell inequalities.","These results allow us to unveil intriguing aspects of the effect of inefficient detectors and the complexity of characterizing the set of quantum correlations, in the simplest Bell scenario."],"url":"http://arxiv.org/abs/2405.08743v1","category":"quant-ph"}
{"created":"2024-05-14 16:13:13","title":"Dimensionality reduction in bulk-boundary reaction-diffusion systems","abstract":"Intracellular protein patterns regulate many vital cellular functions, such as the processing of spatiotemporal information or the control of shape deformations. To do so, pattern-forming systems can be sensitive to the cell geometry by means of coupling the protein dynamics on the cell membrane to dynamics in the cytosol. Recent studies demonstrated that modeling the cytosolic dynamics in terms of an averaged protein pool disregards possibly crucial aspects of the pattern formation, most importantly concentration gradients normal to the membrane. At the same time, the coupling of two domains (surface and volume) with different dimensions renders many standard tools for the numerical analysis of self-organizing systems inefficient. Here, we present a generic framework for projecting the cytosolic dynamics onto the lower-dimensional surface that respects the influence of cytosolic concentration gradients in static and evolving geometries. This method uses a priori physical information about the system to approximate the cytosolic dynamics by a small number of dominant characteristic concentration profiles (basis), akin to basis transformations of finite element methods. As a proof of concept, we apply our framework to a toy model for volume-dependent interrupted coarsening, evaluate the accuracy of the results for various basis choices, and discuss the optimal basis choice for biologically relevant systems. Our analysis presents an efficient yet accurate method for analysing pattern formation with surface-volume coupling in evolving geometries.","sentences":["Intracellular protein patterns regulate many vital cellular functions, such as the processing of spatiotemporal information or the control of shape deformations.","To do so, pattern-forming systems can be sensitive to the cell geometry by means of coupling the protein dynamics on the cell membrane to dynamics in the cytosol.","Recent studies demonstrated that modeling the cytosolic dynamics in terms of an averaged protein pool disregards possibly crucial aspects of the pattern formation, most importantly concentration gradients normal to the membrane.","At the same time, the coupling of two domains (surface and volume) with different dimensions renders many standard tools for the numerical analysis of self-organizing systems inefficient.","Here, we present a generic framework for projecting the cytosolic dynamics onto the lower-dimensional surface that respects the influence of cytosolic concentration gradients in static and evolving geometries.","This method uses a priori physical information about the system to approximate the cytosolic dynamics by a small number of dominant characteristic concentration profiles (basis), akin to basis transformations of finite element methods.","As a proof of concept, we apply our framework to a toy model for volume-dependent interrupted coarsening, evaluate the accuracy of the results for various basis choices, and discuss the optimal basis choice for biologically relevant systems.","Our analysis presents an efficient yet accurate method for analysing pattern formation with surface-volume coupling in evolving geometries."],"url":"http://arxiv.org/abs/2405.08728v1","category":"physics.bio-ph"}
{"created":"2024-05-14 15:05:49","title":"A library of meteoroid environments encountered by spacecraft in the inner solar system","abstract":"NASA's Meteoroid Engineering Model (MEM) is designed to provide aerospace engineers with an accurate description of potentially hazardous meteoroids. It accepts a spacecraft trajectory as input and its output files describe the flux, speed, directionality, and density of microgram- to gram-sized meteoroids relative to the provided trajectory. MEM provides this information at a fairly fine level of detail in order to support detailed risk calculations. However, engineers and scientists in the very early planning stages of a mission may not yet have developed a trajectory or acquired the tools to analyze environment data. Therefore, we have developed an online library of sample MEM runs that allow new users or overloaded mission planners to get a quick feel for the characteristics of the meteoroid environment. This library provides both visualizations of these runs and input files that allow users to replicate them exactly. We also discuss the number of state vectors needed to obtain an accurate representation of the environment encountered along our sample trajectories, and outline a process for verifying that any given trajectory is adequately sampled.","sentences":["NASA's Meteoroid Engineering Model (MEM) is designed to provide aerospace engineers with an accurate description of potentially hazardous meteoroids.","It accepts a spacecraft trajectory as input and its output files describe the flux, speed, directionality, and density of microgram- to gram-sized meteoroids relative to the provided trajectory.","MEM provides this information at a fairly fine level of detail in order to support detailed risk calculations.","However, engineers and scientists in the very early planning stages of a mission may not yet have developed a trajectory or acquired the tools to analyze environment data.","Therefore, we have developed an online library of sample MEM runs that allow new users or overloaded mission planners to get a quick feel for the characteristics of the meteoroid environment.","This library provides both visualizations of these runs and input files that allow users to replicate them exactly.","We also discuss the number of state vectors needed to obtain an accurate representation of the environment encountered along our sample trajectories, and outline a process for verifying that any given trajectory is adequately sampled."],"url":"http://arxiv.org/abs/2405.08685v1","category":"astro-ph.EP"}
{"created":"2024-05-14 14:04:03","title":"Optimal Almost-Balanced Sequences","abstract":"This paper presents a novel approach to address the constrained coding challenge of generating almost-balanced sequences. While strictly balanced sequences have been well studied in the past, the problem of designing efficient algorithms with small redundancy, preferably constant or even a single bit, for almost balanced sequences has remained unsolved. A sequence is $\\varepsilon(n)$-almost balanced if its Hamming weight is between $0.5n\\pm \\varepsilon(n)$. It is known that for any algorithm with a constant number of bits, $\\varepsilon(n)$ has to be in the order of $\\Theta(\\sqrt{n})$, with $O(n)$ average time complexity. However, prior solutions with a single redundancy bit required $\\varepsilon(n)$ to be a linear shift from $n/2$. Employing an iterative method and arithmetic coding, our emphasis lies in constructing almost balanced codes with a single redundancy bit. Notably, our method surpasses previous approaches by achieving the optimal balanced order of $\\Theta(\\sqrt{n})$. Additionally, we extend our method to the non-binary case considering $q$-ary almost polarity-balanced sequences for even $q$, and almost symbol-balanced for $q=4$. Our work marks the first asymptotically optimal solutions for almost-balanced sequences, for both, binary and non-binary alphabet.","sentences":["This paper presents a novel approach to address the constrained coding challenge of generating almost-balanced sequences.","While strictly balanced sequences have been well studied in the past, the problem of designing efficient algorithms with small redundancy, preferably constant or even a single bit, for almost balanced sequences has remained unsolved.","A sequence is $\\varepsilon(n)$-almost balanced if its Hamming weight is between $0.5n\\pm \\varepsilon(n)$. It is known that for any algorithm with a constant number of bits, $\\varepsilon(n)$ has to be in the order of $\\Theta(\\sqrt{n})$, with $O(n)$ average time complexity.","However, prior solutions with a single redundancy bit required $\\varepsilon(n)$ to be a linear shift from $n/2$. Employing an iterative method and arithmetic coding, our emphasis lies in constructing almost balanced codes with a single redundancy bit.","Notably, our method surpasses previous approaches by achieving the optimal balanced order of $\\Theta(\\sqrt{n})$. Additionally, we extend our method to the non-binary case considering $q$-ary almost polarity-balanced sequences for even $q$, and almost symbol-balanced for $q=4$. Our work marks the first asymptotically optimal solutions for almost-balanced sequences, for both, binary and non-binary alphabet."],"url":"http://arxiv.org/abs/2405.08625v1","category":"cs.IT"}
{"created":"2024-05-14 13:57:23","title":"FDD Massive MIMO: How to Optimally Combine UL Pilot and Limited DL CSI Feedback?","abstract":"In frequency-division duplexing (FDD) multiple-input multiple-output (MIMO) systems, obtaining accurate downlink channel state information (CSI) for precoding is vastly challenging due to the tremendous feedback overhead with the growing number of antennas. Utilizing uplink pilots for downlink CSI estimation is a promising approach that can eliminate CSI feedback. However, the downlink CSI estimation accuracy diminishes significantly as the number of channel paths increases, resulting in reduced spectral efficiency. In this paper, we demonstrate that achieving downlink spectral efficiency comparable to perfect CSI is feasible by combining uplink CSI with limited downlink CSI feedback information. Our proposed downlink CSI feedback strategy transmits quantized phase information of downlink channel paths, deviating from conventional limited methods. We put forth a mean square error (MSE)-optimal downlink channel reconstruction method by jointly exploiting the uplink CSI and the limited downlink CSI. Armed with the MSE-optimal estimator, we derive the MSE as a function of the number of feedback bits for phase quantization. Subsequently, we present an optimal feedback bit allocation method for minimizing the MSE in the reconstructed channel through phase quantization. Utilizing a robust downlink precoding technique, we establish that the proposed downlink channel reconstruction method is sufficient for attaining a sum-spectral efficiency comparable to perfect CSI.","sentences":["In frequency-division duplexing (FDD) multiple-input multiple-output (MIMO) systems, obtaining accurate downlink channel state information (CSI) for precoding is vastly challenging due to the tremendous feedback overhead with the growing number of antennas.","Utilizing uplink pilots for downlink CSI estimation is a promising approach that can eliminate CSI feedback.","However, the downlink CSI estimation accuracy diminishes significantly as the number of channel paths increases, resulting in reduced spectral efficiency.","In this paper, we demonstrate that achieving downlink spectral efficiency comparable to perfect CSI is feasible by combining uplink CSI with limited downlink CSI feedback information.","Our proposed downlink CSI feedback strategy transmits quantized phase information of downlink channel paths, deviating from conventional limited methods.","We put forth a mean square error (MSE)-optimal downlink channel reconstruction method by jointly exploiting the uplink CSI and the limited downlink CSI.","Armed with the MSE-optimal estimator, we derive the MSE as a function of the number of feedback bits for phase quantization.","Subsequently, we present an optimal feedback bit allocation method for minimizing the MSE in the reconstructed channel through phase quantization.","Utilizing a robust downlink precoding technique, we establish that the proposed downlink channel reconstruction method is sufficient for attaining a sum-spectral efficiency comparable to perfect CSI."],"url":"http://arxiv.org/abs/2405.08614v1","category":"eess.SP"}
{"created":"2024-05-14 13:39:19","title":"The distributed biased min-consensus protocol revisited: pre-specified finite time control strategies and small-gain based analysis","abstract":"Unlike the classical distributed consensus protocols enabling the group of agents as a whole to reach an agreement regarding a certain quantity of interest in a distributed fashion, the distributed biased min-consensus protocol (DBMC) has been proven to generate advanced complexity pertaining to solving the shortest path problem. As such a protocol is commonly incorporated as the first step of a hierarchical architecture in real applications, e.g., robots path planning, management of dispersed computing services, an impedance limiting the application potential of DBMC lies in, the lack of results regarding to its convergence within a user-assigned time. In this paper, we first propose two control strategies ensuring the state error of DBMC decrease exactly to zero or a desired level manipulated by the user, respectively. To compensate the high feedback gains incurred by these two control strategies, this paper further investigates the nominal DBMC itself. By leveraging small gain based stability tools, this paper also proves the global exponential input-to-state stability of DBMC, outperforming its current stability results. Simulations have been provided to validate the efficacy of our theoretical result.","sentences":["Unlike the classical distributed consensus protocols enabling the group of agents as a whole to reach an agreement regarding a certain quantity of interest in a distributed fashion, the distributed biased min-consensus protocol (DBMC) has been proven to generate advanced complexity pertaining to solving the shortest path problem.","As such a protocol is commonly incorporated as the first step of a hierarchical architecture in real applications, e.g., robots path planning, management of dispersed computing services, an impedance limiting the application potential of DBMC lies in, the lack of results regarding to its convergence within a user-assigned time.","In this paper, we first propose two control strategies ensuring the state error of DBMC decrease exactly to zero or a desired level manipulated by the user, respectively.","To compensate the high feedback gains incurred by these two control strategies, this paper further investigates the nominal DBMC itself.","By leveraging small gain based stability tools, this paper also proves the global exponential input-to-state stability of DBMC, outperforming its current stability results.","Simulations have been provided to validate the efficacy of our theoretical result."],"url":"http://arxiv.org/abs/2405.08599v1","category":"eess.SY"}
{"created":"2024-05-14 13:29:34","title":"Accelerated Alternating Direction Method of Multipliers Gradient Tracking for Distributed Optimization","abstract":"This paper presents a novel accelerated distributed algorithm for unconstrained consensus optimization over static undirected networks. The proposed algorithm combines the benefits of acceleration from momentum, the robustness of the alternating direction method of multipliers, and the computational efficiency of gradient tracking to surpass existing state-of-the-art methods in convergence speed, while preserving their computational and communication cost. First, we prove that, by applying momentum on the average dynamic consensus protocol over the estimates and gradient, we can study the algorithm as an interconnection of two singularly perturbed systems: the outer system connects the consensus variables and the optimization variables, and the inner system connects the estimates of the optimum and the auxiliary optimization variables. Next, we prove that, by adding momentum to the auxiliary dynamics, our algorithm always achieves faster convergence than the achievable linear convergence rate for the non-accelerated alternating direction method of multipliers gradient tracking algorithm case. Through simulations, we numerically show that our accelerated algorithm surpasses the existing accelerated and non-accelerated distributed consensus first-order optimization protocols in convergence speed.","sentences":["This paper presents a novel accelerated distributed algorithm for unconstrained consensus optimization over static undirected networks.","The proposed algorithm combines the benefits of acceleration from momentum, the robustness of the alternating direction method of multipliers, and the computational efficiency of gradient tracking to surpass existing state-of-the-art methods in convergence speed, while preserving their computational and communication cost.","First, we prove that, by applying momentum on the average dynamic consensus protocol over the estimates and gradient, we can study the algorithm as an interconnection of two singularly perturbed systems: the outer system connects the consensus variables and the optimization variables, and the inner system connects the estimates of the optimum and the auxiliary optimization variables.","Next, we prove that, by adding momentum to the auxiliary dynamics, our algorithm always achieves faster convergence than the achievable linear convergence rate for the non-accelerated alternating direction method of multipliers gradient tracking algorithm case.","Through simulations, we numerically show that our accelerated algorithm surpasses the existing accelerated and non-accelerated distributed consensus first-order optimization protocols in convergence speed."],"url":"http://arxiv.org/abs/2405.08590v1","category":"math.OC"}
{"created":"2024-05-14 13:28:57","title":"Variable Substitution and Bilinear Programming for Aligning Partially Overlapping Point Sets","abstract":"In many applications, the demand arises for algorithms capable of aligning partially overlapping point sets while remaining invariant to the corresponding transformations. This research presents a method designed to meet such requirements through minimization of the objective function of the robust point matching (RPM) algorithm. First, we show that the RPM objective is a cubic polynomial. Then, through variable substitution, we transform the RPM objective to a quadratic function. Leveraging the convex envelope of bilinear monomials, we proceed to relax the resulting objective function, thus obtaining a lower bound problem that can be conveniently decomposed into distinct linear assignment and low-dimensional convex quadratic program components, both amenable to efficient optimization. Furthermore, a branch-and-bound (BnB) algorithm is devised, which solely branches over the transformation parameters, thereby boosting convergence rate. Empirical evaluations demonstrate better robustness of the proposed methodology against non-rigid deformation, positional noise, and outliers, particularly in scenarios where outliers remain distinct from inliers, when compared with prevailing state-of-the-art approaches.","sentences":["In many applications, the demand arises for algorithms capable of aligning partially overlapping point sets while remaining invariant to the corresponding transformations.","This research presents a method designed to meet such requirements through minimization of the objective function of the robust point matching (RPM) algorithm.","First, we show that the RPM objective is a cubic polynomial.","Then, through variable substitution, we transform the RPM objective to a quadratic function.","Leveraging the convex envelope of bilinear monomials, we proceed to relax the resulting objective function, thus obtaining a lower bound problem that can be conveniently decomposed into distinct linear assignment and low-dimensional convex quadratic program components, both amenable to efficient optimization.","Furthermore, a branch-and-bound (BnB) algorithm is devised, which solely branches over the transformation parameters, thereby boosting convergence rate.","Empirical evaluations demonstrate better robustness of the proposed methodology against non-rigid deformation, positional noise, and outliers, particularly in scenarios where outliers remain distinct from inliers, when compared with prevailing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2405.08589v1","category":"cs.CV"}
{"created":"2024-05-14 13:21:00","title":"Importance of hyper-parameter optimization during training of physics-informed deep learning networks","abstract":"Incorporating scientific knowledge into deep learning (DL) models for materials-based simulations can constrain the network's predictions to be within the boundaries of the material system. Altering loss functions or adding physics-based regularization (PBR) terms to reflect material properties informs a network about the physical constraints the simulation should obey. The training and tuning process of a DL network greatly affects the quality of the model, but how this process differs when using physics-based loss functions or regularization terms is not commonly discussed. In this manuscript, several PBR methods are implemented to enforce stress equilibrium on a network predicting the stress fields of a high elastic contrast composite. Models with PBR enforced the equilibrium constraint more accurately than a model without PBR, and the stress equilibrium converged more quickly. More importantly it was observed that independently fine-tuning each implementation resulted in more accurate models. More specifically each loss formulation and dataset required different learning rates and loss weights for the best performance. This result has important implications on assessing the relative effectiveness of different DL models and highlights important considerations when making a comparison between DL methods.","sentences":["Incorporating scientific knowledge into deep learning (DL) models for materials-based simulations can constrain the network's predictions to be within the boundaries of the material system.","Altering loss functions or adding physics-based regularization (PBR) terms to reflect material properties informs a network about the physical constraints the simulation should obey.","The training and tuning process of a DL network greatly affects the quality of the model, but how this process differs when using physics-based loss functions or regularization terms is not commonly discussed.","In this manuscript, several PBR methods are implemented to enforce stress equilibrium on a network predicting the stress fields of a high elastic contrast composite.","Models with PBR enforced the equilibrium constraint more accurately than a model without PBR, and the stress equilibrium converged more quickly.","More importantly it was observed that independently fine-tuning each implementation resulted in more accurate models.","More specifically each loss formulation and dataset required different learning rates and loss weights for the best performance.","This result has important implications on assessing the relative effectiveness of different DL models and highlights important considerations when making a comparison between DL methods."],"url":"http://arxiv.org/abs/2405.08580v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 12:37:05","title":"Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph","abstract":"In visual tasks, large teacher models capture essential features and deep information, enhancing performance. However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations. To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model. We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network. This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model. Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective. Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability.","sentences":["In visual tasks, large teacher models capture essential features and deep information, enhancing performance.","However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations.","To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model.","We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network.","This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model.","Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective.","Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability."],"url":"http://arxiv.org/abs/2405.08547v1","category":"cs.CV"}
{"created":"2024-05-14 12:17:19","title":"Dynamic Feature Learning and Matching for Class-Incremental Learning","abstract":"Class-incremental learning (CIL) has emerged as a means to learn new classes incrementally without catastrophic forgetting of previous classes. Recently, CIL has undergone a paradigm shift towards dynamic architectures due to their superior performance. However, these models are still limited by the following aspects: (i) Data augmentation (DA), which are tightly coupled with CIL, remains under-explored in dynamic architecture scenarios. (ii) Feature representation. The discriminativeness of dynamic feature are sub-optimal and possess potential for refinement. (iii) Classifier. The misalignment between dynamic feature and classifier constrains the capabilities of the model. To tackle the aforementioned drawbacks, we propose the Dynamic Feature Learning and Matching (DFLM) model in this paper from above three perspectives. Specifically, we firstly introduce class weight information and non-stationary functions to extend the mix DA method for dynamically adjusting the focus on memory during training. Then, von Mises-Fisher (vMF) classifier is employed to effectively model the dynamic feature distribution and implicitly learn their discriminative properties. Finally, the matching loss is proposed to facilitate the alignment between the learned dynamic features and the classifier by minimizing the distribution distance. Extensive experiments on CIL benchmarks validate that our proposed model achieves significant performance improvements over existing methods.","sentences":["Class-incremental learning (CIL) has emerged as a means to learn new classes incrementally without catastrophic forgetting of previous classes.","Recently, CIL has undergone a paradigm shift towards dynamic architectures due to their superior performance.","However, these models are still limited by the following aspects: (i) Data augmentation (DA), which are tightly coupled with CIL, remains under-explored in dynamic architecture scenarios.","(ii) Feature representation.","The discriminativeness of dynamic feature are sub-optimal and possess potential for refinement.","(iii) Classifier.","The misalignment between dynamic feature and classifier constrains the capabilities of the model.","To tackle the aforementioned drawbacks, we propose the Dynamic Feature Learning and Matching (DFLM) model in this paper from above three perspectives.","Specifically, we firstly introduce class weight information and non-stationary functions to extend the mix DA method for dynamically adjusting the focus on memory during training.","Then, von Mises-Fisher (vMF) classifier is employed to effectively model the dynamic feature distribution and implicitly learn their discriminative properties.","Finally, the matching loss is proposed to facilitate the alignment between the learned dynamic features and the classifier by minimizing the distribution distance.","Extensive experiments on CIL benchmarks validate that our proposed model achieves significant performance improvements over existing methods."],"url":"http://arxiv.org/abs/2405.08533v1","category":"cs.CV"}
{"created":"2024-05-14 12:16:43","title":"A dynamical view of Tijdeman's solution of the chairman assignment problem","abstract":"In 1980, R. Tijdeman provided an on-line algorithm that generates sequences over a finite alphabet with minimal discrepancy, that is, such that the occurrence of each letter optimally tracks its frequency. In this article, we define discrete dynamical systems generating these sequences. The dynamical systems are defined as exchanges of polytopal pieces, yielding cut and project schemes, and they code tilings of the line whose sets of vertices form model sets. We prove that these sequences of low discrepancy are natural codings of toral translations with respect to polytopal atoms, and that they generate a minimal and uniquely ergodic subshift with purely discrete spectrum. Finally, we show that the factor complexity of these sequences is of polynomial growth order $n^{d-1}$, where $d$ is the cardinality of the alphabet.","sentences":["In 1980, R. Tijdeman provided an on-line algorithm that generates sequences over a finite alphabet with minimal discrepancy, that is, such that the occurrence of each letter optimally tracks its frequency.","In this article, we define discrete dynamical systems generating these sequences.","The dynamical systems are defined as exchanges of polytopal pieces, yielding cut and project schemes, and they code tilings of the line whose sets of vertices form model sets.","We prove that these sequences of low discrepancy are natural codings of toral translations with respect to polytopal atoms, and that they generate a minimal and uniquely ergodic subshift with purely discrete spectrum.","Finally, we show that the factor complexity of these sequences is of polynomial growth order $n^{d-1}$, where $d$ is the cardinality of the alphabet."],"url":"http://arxiv.org/abs/2405.08532v1","category":"math.DS"}
{"created":"2024-05-14 12:02:28","title":"Doubly-robust inference and optimality in structure-agnostic models with smoothness","abstract":"We study the problem of constructing an estimator of the average treatment effect (ATE) that exhibits doubly-robust asymptotic linearity (DRAL). This is a stronger requirement than doubly-robust consistency. A DRAL estimator can yield asymptotically valid Wald-type confidence intervals even when the propensity score or the outcome model is inconsistently estimated. On the contrary, the celebrated doubly-robust, augmented-IPW (AIPW) estimator generally requires consistent estimation of both nuisance functions for standard root-n inference. We make three main contributions. First, we propose a new hybrid class of distributions that consists of the structure-agnostic class introduced in Balakrishnan et al (2023) with additional smoothness constraints. While DRAL is generally not possible in the pure structure-agnostic class, we show that it can be attained in the new hybrid one. Second, we calculate minimax lower bounds for estimating the ATE in the new class, as well as in the pure structure-agnostic one. Third, building upon the literature on doubly-robust inference (van der Laan, 2014, Benkeser et al, 2017, Dukes et al 2021), we propose a new estimator of the ATE that enjoys DRAL. Under certain conditions, we show that its rate of convergence in the new class can be much faster than that achieved by the AIPW estimator and, in particular, matches the minimax lower bound rate, thereby establishing its optimality. Finally, we clarify the connection between DRAL estimators and those based on higher-order influence functions (Robins et al, 2017) and complement our theoretical findings with simulations.","sentences":["We study the problem of constructing an estimator of the average treatment effect (ATE) that exhibits doubly-robust asymptotic linearity (DRAL).","This is a stronger requirement than doubly-robust consistency.","A DRAL estimator can yield asymptotically valid Wald-type confidence intervals even when the propensity score or the outcome model is inconsistently estimated.","On the contrary, the celebrated doubly-robust, augmented-IPW (AIPW) estimator generally requires consistent estimation of both nuisance functions for standard root-n inference.","We make three main contributions.","First, we propose a new hybrid class of distributions that consists of the structure-agnostic class introduced in Balakrishnan et al (2023) with additional smoothness constraints.","While DRAL is generally not possible in the pure structure-agnostic class, we show that it can be attained in the new hybrid one.","Second, we calculate minimax lower bounds for estimating the ATE in the new class, as well as in the pure structure-agnostic one.","Third, building upon the literature on doubly-robust inference (van der Laan, 2014, Benkeser et al, 2017, Dukes et al 2021), we propose a new estimator of the ATE that enjoys DRAL.","Under certain conditions, we show that its rate of convergence in the new class can be much faster than that achieved by the AIPW estimator and, in particular, matches the minimax lower bound rate, thereby establishing its optimality.","Finally, we clarify the connection between DRAL estimators and those based on higher-order influence functions (Robins et al, 2017) and complement our theoretical findings with simulations."],"url":"http://arxiv.org/abs/2405.08525v1","category":"stat.ME"}
{"created":"2024-05-14 11:56:06","title":"Cooperative Sensing of Side Lobes Interference for mmWave Blockages Localization and Mapping","abstract":"Radio localization and sensing are anticipated to play a crucial role in enhancing radio resource management in future networks. In this work, we focus on millimeter-wave communications, which are highly vulnerable to blockages, leading to severe attenuation and performance degradation. In a previous work, we proposed a novel mechanism that senses the radio environment to estimate the angular position of a moving blocker with respect to the sensing node. Building upon this foundation, this paper investigates the benefits of cooperation between different entities in the network by sharing sensed data to jointly locate the moving blocker while mapping the interference profile to probe the radio environment. Numerical evaluations demonstrate that cooperative sensing can achieve a more precise location estimation of the blocker as it further allows accurate estimation of its distance rather than its relative angular position only, leading to effective assessment of the blocker direction, trajectory and possibly, its speed, and size.","sentences":["Radio localization and sensing are anticipated to play a crucial role in enhancing radio resource management in future networks.","In this work, we focus on millimeter-wave communications, which are highly vulnerable to blockages, leading to severe attenuation and performance degradation.","In a previous work, we proposed a novel mechanism that senses the radio environment to estimate the angular position of a moving blocker with respect to the sensing node.","Building upon this foundation, this paper investigates the benefits of cooperation between different entities in the network by sharing sensed data to jointly locate the moving blocker while mapping the interference profile to probe the radio environment.","Numerical evaluations demonstrate that cooperative sensing can achieve a more precise location estimation of the blocker as it further allows accurate estimation of its distance rather than its relative angular position only, leading to effective assessment of the blocker direction, trajectory and possibly, its speed, and size."],"url":"http://arxiv.org/abs/2405.08521v1","category":"eess.SP"}
{"created":"2024-05-14 11:48:59","title":"Cryptography-Based Privacy-Preserving Method for Distributed Optimization over Time-Varying Directed Graphs with Enhanced Efficiency","abstract":"In this paper, we study the privacy-preserving distributed optimization problem, aiming to prevent attackers from stealing the private information of agents. For this purpose, we propose a novel privacy-preserving algorithm based on the Advanced Encryption Standard (AES), which is both secure and computationally efficient. By appropriately constructing the underlying weight matrices, our algorithm can be applied to time-varying directed networks. We show that the proposed algorithm can protect an agent's privacy if the agent has at least one legitimate neighbor at the initial iteration. Under the assumption that the objective function is strongly convex and Lipschitz smooth, we rigorously prove that the proposed algorithm has a linear convergence rate. Finally, the effectiveness of the proposed algorithm is demonstrated by numerical simulations of the canonical sensor fusion problem.","sentences":["In this paper, we study the privacy-preserving distributed optimization problem, aiming to prevent attackers from stealing the private information of agents.","For this purpose, we propose a novel privacy-preserving algorithm based on the Advanced Encryption Standard (AES), which is both secure and computationally efficient.","By appropriately constructing the underlying weight matrices, our algorithm can be applied to time-varying directed networks.","We show that the proposed algorithm can protect an agent's privacy if the agent has at least one legitimate neighbor at the initial iteration.","Under the assumption that the objective function is strongly convex and Lipschitz smooth, we rigorously prove that the proposed algorithm has a linear convergence rate.","Finally, the effectiveness of the proposed algorithm is demonstrated by numerical simulations of the canonical sensor fusion problem."],"url":"http://arxiv.org/abs/2405.08518v1","category":"math.OC"}
{"created":"2024-05-14 11:17:40","title":"From linear programming to colliding particles","abstract":"Although simplices are trivial from a linear optimization standpoint, the simplex algorithm can exhibit quite complex behavior. In this paper we study the behavior of max-slope pivot rules on (products of) simplices and describe the associated pivot rule polytopes. For simplices, the pivot rule polytopes are combinatorially isomorphic to associahedra. To prove this correspondence, we interpret max-slope pivot rules in terms of the combinatorics of colliding particles on a line. For prisms over simplices, we recover Stasheff's multiplihedra. For products of two simplices we get new realizations of constrainahedra, that capture the combinatorics of certain particle systems in the plane.","sentences":["Although simplices are trivial from a linear optimization standpoint, the simplex algorithm can exhibit quite complex behavior.","In this paper we study the behavior of max-slope pivot rules on (products of) simplices and describe the associated pivot rule polytopes.","For simplices, the pivot rule polytopes are combinatorially isomorphic to associahedra.","To prove this correspondence, we interpret max-slope pivot rules in terms of the combinatorics of colliding particles on a line.","For prisms over simplices, we recover Stasheff's multiplihedra.","For products of two simplices we get new realizations of constrainahedra, that capture the combinatorics of certain particle systems in the plane."],"url":"http://arxiv.org/abs/2405.08506v1","category":"math.CO"}
{"created":"2024-05-14 11:06:12","title":"IPC: Incremental Probabilistic Consensus-based Consistent Set Maximization for SLAM Backends","abstract":"In SLAM (Simultaneous localization and mapping) problems, Pose Graph Optimization (PGO) is a technique to refine an initial estimate of a set of poses (positions and orientations) from a set of pairwise relative measurements. The optimization procedure can be negatively affected even by a single outlier measurement, with possible catastrophic and meaningless results. Although recent works on robust optimization aim to mitigate the presence of outlier measurements, robust solutions capable of handling large numbers of outliers are yet to come. This paper presents IPC, acronym for Incremental Probabilistic Consensus, a method that approximates the solution to the combinatorial problem of finding the maximally consistent set of measurements in an incremental fashion. It evaluates the consistency of each loop closure measurement through a consensus-based procedure, possibly applied to a subset of the global problem, where all previously integrated inlier measurements have veto power. We evaluated IPC on standard benchmarks against several state-of-the-art methods. Although it is simple and relatively easy to implement, IPC competes with or outperforms the other tested methods in handling outliers while providing online performances. We release with this paper an open-source implementation of the proposed method.","sentences":["In SLAM (Simultaneous localization and mapping) problems, Pose Graph Optimization (PGO) is a technique to refine an initial estimate of a set of poses (positions and orientations) from a set of pairwise relative measurements.","The optimization procedure can be negatively affected even by a single outlier measurement, with possible catastrophic and meaningless results.","Although recent works on robust optimization aim to mitigate the presence of outlier measurements, robust solutions capable of handling large numbers of outliers are yet to come.","This paper presents IPC, acronym for Incremental Probabilistic Consensus, a method that approximates the solution to the combinatorial problem of finding the maximally consistent set of measurements in an incremental fashion.","It evaluates the consistency of each loop closure measurement through a consensus-based procedure, possibly applied to a subset of the global problem, where all previously integrated inlier measurements have veto power.","We evaluated IPC on standard benchmarks against several state-of-the-art methods.","Although it is simple and relatively easy to implement, IPC competes with or outperforms the other tested methods in handling outliers while providing online performances.","We release with this paper an open-source implementation of the proposed method."],"url":"http://arxiv.org/abs/2405.08503v1","category":"cs.RO"}
{"created":"2024-05-14 10:54:20","title":"Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models","abstract":"Compositionality in language models presents a problem when processing idiomatic expressions, as their meaning often cannot be directly derived from their individual parts. Although fine-tuning and other optimization strategies can be used to improve representations of idiomatic expressions, this depends on the availability of relevant data. We present the Noun Compound Synonym Substitution in Books - NCSSB - datasets, which are created by substitution of synonyms of potentially idiomatic English noun compounds in public domain book texts. We explore the trade-off between data quantity and quality when training models for idiomaticity detection, in conjunction with contextual information obtained locally (from the surrounding sentences) or externally (through language resources). Performance on an idiomaticity detection task indicates that dataset quality is a stronger factor for context-enriched models, but that quantity also plays a role in models without context inclusion strategies.","sentences":["Compositionality in language models presents a problem when processing idiomatic expressions, as their meaning often cannot be directly derived from their individual parts.","Although fine-tuning and other optimization strategies can be used to improve representations of idiomatic expressions, this depends on the availability of relevant data.","We present the Noun Compound Synonym Substitution in Books - NCSSB - datasets, which are created by substitution of synonyms of potentially idiomatic English noun compounds in public domain book texts.","We explore the trade-off between data quantity and quality when training models for idiomaticity detection, in conjunction with contextual information obtained locally (from the surrounding sentences) or externally (through language resources).","Performance on an idiomaticity detection task indicates that dataset quality is a stronger factor for context-enriched models, but that quantity also plays a role in models without context inclusion strategies."],"url":"http://arxiv.org/abs/2405.08497v1","category":"cs.CL"}
{"created":"2024-05-14 10:01:51","title":"Representing Information on DNA using Patterns Induced by Enzymatic Labeling","abstract":"Enzymatic DNA labeling is a powerful tool with applications in biochemistry, molecular biology, biotechnology, medical science, and genomic research. This paper contributes to the evolving field of DNA-based data storage by presenting a formal framework for modeling DNA labeling in strings, specifically tailored for data storage purposes. Our approach involves a known DNA molecule as a template for labeling, employing patterns induced by a set of designed labels to represent information. One hypothetical implementation can use CRISPR-Cas9 and gRNA reagents for labeling. Various aspects of the general labeling channel, including fixed-length labels, are explored, and upper bounds on the maximal size of the corresponding codes are given. The study includes the development of an efficient encoder-decoder pair that is proven optimal in terms of maximum code size under specific conditions.","sentences":["Enzymatic DNA labeling is a powerful tool with applications in biochemistry, molecular biology, biotechnology, medical science, and genomic research.","This paper contributes to the evolving field of DNA-based data storage by presenting a formal framework for modeling DNA labeling in strings, specifically tailored for data storage purposes.","Our approach involves a known DNA molecule as a template for labeling, employing patterns induced by a set of designed labels to represent information.","One hypothetical implementation can use CRISPR-Cas9 and gRNA reagents for labeling.","Various aspects of the general labeling channel, including fixed-length labels, are explored, and upper bounds on the maximal size of the corresponding codes are given.","The study includes the development of an efficient encoder-decoder pair that is proven optimal in terms of maximum code size under specific conditions."],"url":"http://arxiv.org/abs/2405.08475v1","category":"cs.IT"}
{"created":"2024-05-14 09:51:27","title":"Sparse MTTKRP Acceleration for Tensor Decomposition on GPU","abstract":"Sparse Matricized Tensor Times Khatri-Rao Product (spMTTKRP) is the bottleneck kernel of sparse tensor decomposition. In this work, we propose a GPU-based algorithm design to address the key challenges in accelerating spMTTKRP computation, including (1) eliminating global atomic operations across GPU thread blocks, (2) avoiding the intermediate values being communicated between GPU thread blocks and GPU global memory, and (3) ensuring a balanced distribution of workloads across GPU thread blocks. Our approach also supports dynamic tensor remapping, enabling the above optimizations in all the modes of the input tensor. Our approach achieves a geometric mean speedup of 1.5x, 2.0x, and 21.7x in total execution time across widely used datasets compared with the state-of-the-art GPU implementations. Our work is the only GPU implementation that can support tensors with modes greater than 4 since the state-of-the-art works have implementation constraints for tensors with a large number of modes.","sentences":["Sparse Matricized Tensor Times Khatri-Rao Product (spMTTKRP) is the bottleneck kernel of sparse tensor decomposition.","In this work, we propose a GPU-based algorithm design to address the key challenges in accelerating spMTTKRP computation, including (1) eliminating global atomic operations across GPU thread blocks, (2) avoiding the intermediate values being communicated between GPU thread blocks and GPU global memory, and (3) ensuring a balanced distribution of workloads across GPU thread blocks.","Our approach also supports dynamic tensor remapping, enabling the above optimizations in all the modes of the input tensor.","Our approach achieves a geometric mean speedup of 1.5x, 2.0x, and 21.7x in total execution time across widely used datasets compared with the state-of-the-art GPU implementations.","Our work is the only GPU implementation that can support tensors with modes greater than 4 since the state-of-the-art works have implementation constraints for tensors with a large number of modes."],"url":"http://arxiv.org/abs/2405.08470v1","category":"cs.DC"}
{"created":"2024-05-14 09:13:48","title":"Effective Front-Descent Algorithms with Convergence Guarantees","abstract":"In this manuscript, we address continuous unconstrained optimization problems and we discuss descent type methods for the reconstruction of the Pareto set. Specifically, we analyze the class of Front Descent methods, which generalizes the Front Steepest Descent algorithm allowing the employment of suitable, effective search directions (e.g., Newton, Quasi-Newton, Barzilai-Borwein). We provide a deep characterization of the behavior and the mechanisms of the algorithmic framework, and we prove that, under reasonable assumptions, standard convergence results and some complexity bounds hold for the generalized approach. Moreover, we prove that popular search directions can indeed be soundly used within the framework. Then, we provide a completely novel type of convergence results, concerning the sequence of sets produced by the procedure. In particular, iterate sets are shown to asymptotically approach stationarity for all of their points; additionally, in finite precision settings, the sets are shown to only be enriched through exploration steps in later iterations, and suitable stopping conditions can be devised. Finally, the results from a large experimental benchmark show that the proposed class of approaches far outperforms state-of-the-art methodologies.","sentences":["In this manuscript, we address continuous unconstrained optimization problems and we discuss descent type methods for the reconstruction of the Pareto set.","Specifically, we analyze the class of Front Descent methods, which generalizes the Front Steepest Descent algorithm allowing the employment of suitable, effective search directions (e.g., Newton, Quasi-Newton, Barzilai-Borwein).","We provide a deep characterization of the behavior and the mechanisms of the algorithmic framework, and we prove that, under reasonable assumptions, standard convergence results and some complexity bounds hold for the generalized approach.","Moreover, we prove that popular search directions can indeed be soundly used within the framework.","Then, we provide a completely novel type of convergence results, concerning the sequence of sets produced by the procedure.","In particular, iterate sets are shown to asymptotically approach stationarity for all of their points; additionally, in finite precision settings, the sets are shown to only be enriched through exploration steps in later iterations, and suitable stopping conditions can be devised.","Finally, the results from a large experimental benchmark show that the proposed class of approaches far outperforms state-of-the-art methodologies."],"url":"http://arxiv.org/abs/2405.08450v1","category":"math.OC"}
{"created":"2024-05-14 09:03:00","title":"Safety Constrained Multi-Agent Reinforcement Learning for Active Voltage Control","abstract":"Active voltage control presents a promising avenue for relieving power congestion and enhancing voltage quality, taking advantage of the distributed controllable generators in the power network, such as roof-top photovoltaics. While Multi-Agent Reinforcement Learning (MARL) has emerged as a compelling approach to address this challenge, existing MARL approaches tend to overlook the constrained optimization nature of this problem, failing in guaranteeing safety constraints. In this paper, we formalize the active voltage control problem as a constrained Markov game and propose a safety-constrained MARL algorithm. We expand the primal-dual optimization RL method to multi-agent settings, and augment it with a novel approach of double safety estimation to learn the policy and to update the Lagrange-multiplier. In addition, we proposed different cost functions and investigated their influences on the behavior of our constrained MARL method. We evaluate our approach in the power distribution network simulation environment with real-world scale scenarios. Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art MARL methods.","sentences":["Active voltage control presents a promising avenue for relieving power congestion and enhancing voltage quality, taking advantage of the distributed controllable generators in the power network, such as roof-top photovoltaics.","While Multi-Agent Reinforcement Learning (MARL) has emerged as a compelling approach to address this challenge, existing MARL approaches tend to overlook the constrained optimization nature of this problem, failing in guaranteeing safety constraints.","In this paper, we formalize the active voltage control problem as a constrained Markov game and propose a safety-constrained MARL algorithm.","We expand the primal-dual optimization RL method to multi-agent settings, and augment it with a novel approach of double safety estimation to learn the policy and to update the Lagrange-multiplier.","In addition, we proposed different cost functions and investigated their influences on the behavior of our constrained MARL method.","We evaluate our approach in the power distribution network simulation environment with real-world scale scenarios.","Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art MARL methods."],"url":"http://arxiv.org/abs/2405.08443v1","category":"cs.LG"}
{"created":"2024-05-14 09:01:41","title":"Unveiling quantum phase transitions from traps in variational quantum algorithms","abstract":"Understanding quantum phase transitions in physical systems is fundamental to characterize their behaviour at small temperatures. Achieving this requires both accessing good approximations to the ground state and identifying order parameters to distinguish different phases. Addressing these challenges, our work introduces a hybrid algorithm that combines quantum optimization with classical machine learning. This approach leverages the capability of near-term quantum computers to prepare locally trapped states through finite optimization. Specifically, we utilize LASSO for identifying conventional phase transitions and the Transformer model for topological transitions, applying these with a sliding window of Hamiltonian parameters to learn appropriate order parameters and estimate the critical points accurately. We verified the effectiveness of our method with numerical simulation and real-hardware experiments on Rigetti's Ankaa 9Q-1 quantum computer. Our protocol not only provides a robust framework for investigating quantum phase transitions using shallow quantum circuits but also significantly enhances efficiency and precision, opening new avenues in the integration of quantum computing and machine learning.","sentences":["Understanding quantum phase transitions in physical systems is fundamental to characterize their behaviour at small temperatures.","Achieving this requires both accessing good approximations to the ground state and identifying order parameters to distinguish different phases.","Addressing these challenges, our work introduces a hybrid algorithm that combines quantum optimization with classical machine learning.","This approach leverages the capability of near-term quantum computers to prepare locally trapped states through finite optimization.","Specifically, we utilize LASSO for identifying conventional phase transitions and the Transformer model for topological transitions, applying these with a sliding window of Hamiltonian parameters to learn appropriate order parameters and estimate the critical points accurately.","We verified the effectiveness of our method with numerical simulation and real-hardware experiments on Rigetti's Ankaa 9Q-1 quantum computer.","Our protocol not only provides a robust framework for investigating quantum phase transitions using shallow quantum circuits but also significantly enhances efficiency and precision, opening new avenues in the integration of quantum computing and machine learning."],"url":"http://arxiv.org/abs/2405.08441v1","category":"quant-ph"}
{"created":"2024-05-14 08:31:29","title":"Hereditary undecidability of fragments of some elementary theories","abstract":"It is well known that whenever a class of structures $\\mathcal{K}_1$ is interpretable in a class of structures $\\mathcal{K}_2$, then the hereditary undecidability of (a fragment of) the theory of $\\mathcal{K}_1$ implies the hereditary undecidability of (a suitable fragment of) the theory of $\\mathcal{K}_2$. In the present paper, we construct a $\\Sigma_1$-interpretation of the class of all finite bipartite graphs in the class of all pairs of equivalence relations on the same finite domain; from this we obtain the hereditary undecidability of the $\\Sigma_2$-theory of the second class. Next, we construct a $\\Sigma_1$-interpretation of the class of all pairs of equivalence relations on the same finite domain in the class of all pairs consisting of a linear ordering and an equivalence relation on the same finite domain; this gives us the hereditary undecidability of the $\\Sigma_2$-theory of the second class. The corresponding results are, in a sense, optimal, since the $\\Pi_2$-theories of the classes under consideration are decidable.   Keywords: undecidability, elementary theories, prefix fragments","sentences":["It is well known that whenever a class of structures $\\mathcal{K}_1$ is interpretable in a class of structures $\\mathcal{K}_2$, then the hereditary undecidability of (a fragment of) the theory of $\\mathcal{K}_1$ implies the hereditary undecidability of (a suitable fragment of) the theory of $\\mathcal{K}_2$. In the present paper, we construct a $\\Sigma_1$-interpretation of the class of all finite bipartite graphs in the class of all pairs of equivalence relations on the same finite domain; from this we obtain the hereditary undecidability of the $\\Sigma_2$-theory of the second class.","Next, we construct a $\\Sigma_1$-interpretation of the class of all pairs of equivalence relations on the same finite domain in the class of all pairs consisting of a linear ordering and an equivalence relation on the same finite domain; this gives us the hereditary undecidability of the $\\Sigma_2$-theory of the second class.","The corresponding results are, in a sense, optimal, since the $\\Pi_2$-theories of the classes under consideration are decidable.   ","Keywords: undecidability, elementary theories, prefix fragments"],"url":"http://arxiv.org/abs/2405.08422v1","category":"math.LO"}
{"created":"2024-05-14 07:57:42","title":"A constraint-based approach to function interpolation, with application to performance estimation for weakly convex optimisation","abstract":"We propose a novel approach to obtain interpolation constraints for a wide range of function classes, i.e. necessary and sufficient constraints that a set of points, functions values and (sub)gradients must satisfy to ensure the existence of a global function of the class considered, consistent with this set. The derivation of such constraints is crucial for instance in the performance analysis of optimization methods, since obtaining a priori tight performance guarantees requires using a tight description of function classes of interest. Our method allows setting aside all analytic properties of the function class to work only at an algebraic level, and to easily obtain counterexamples when a condition characterizing a function class cannot serve as an interpolation constraint. As an illustration, we provide interpolation constraints for a class of non convex non smooth functions: weakly convex functions with bounded subgradients, and rely on these new interpolation constraints to outperform state of the art bounds on the performance of the subgradient method on this class.","sentences":["We propose a novel approach to obtain interpolation constraints for a wide range of function classes, i.e. necessary and sufficient constraints that a set of points, functions values and (sub)gradients must satisfy to ensure the existence of a global function of the class considered, consistent with this set.","The derivation of such constraints is crucial for instance in the performance analysis of optimization methods, since obtaining a priori tight performance guarantees requires using a tight description of function classes of interest.","Our method allows setting aside all analytic properties of the function class to work only at an algebraic level, and to easily obtain counterexamples when a condition characterizing a function class cannot serve as an interpolation constraint.","As an illustration, we provide interpolation constraints for a class of non convex non smooth functions: weakly convex functions with bounded subgradients, and rely on these new interpolation constraints to outperform state of the art bounds on the performance of the subgradient method on this class."],"url":"http://arxiv.org/abs/2405.08405v1","category":"math.OC"}
{"created":"2024-05-14 07:49:33","title":"Coupled-Band ESSFM for Low-Complexity DBP","abstract":"We propose a novel digital backpropagation (DBP) technique that combines perturbation theory, subband processing, and splitting ratio optimization. We obtain 0.23 dB, 0.47 dB, or 0.91 dB gains w.r.t. dispersion compensation with only 74, 161, or 681 real multiplications/2D-symbol, improving significantly on existing DBP techniques.","sentences":["We propose a novel digital backpropagation (DBP) technique that combines perturbation theory, subband processing, and splitting ratio optimization.","We obtain 0.23 dB, 0.47 dB, or 0.91 dB gains w.r.t.","dispersion compensation with only 74, 161, or 681 real multiplications/2D-symbol, improving significantly on existing DBP techniques."],"url":"http://arxiv.org/abs/2405.08396v1","category":"cs.IT"}
{"created":"2024-05-14 07:34:41","title":"Quest for an efficient mathematical and computational method to explore optimal extreme weather modification","abstract":"It is a grand challenge to find a feasible weather modification method to mitigate the impact of extreme weather events such as tropical cyclones. Previous works have proposed potentially effective actuators and assessed their capabilities to achieve weather modification objectives through numerical simulations. However, few studies have explored efficient mathematical and computational methods to inversely determine optimal actuators from specific modification goals. Here I demonstrate the utility of the ensemble Kalman filter (EnKF)-based control method, referred to as ensemble Kalman control (EnKC). The series of numerical experiments with the Lorenz 96 model indicates that EnKC efficiently identifies local, small, and intermittent control perturbations that can mitigate extreme events. The existing techniques of EnKF, such as background error covariance localization and observation error covariance inflation, can improve the sparsity and efficiency of the control. This work paves the way toward the real-world applications of EnKC to explore the controllability of extreme atmospheric events.","sentences":["It is a grand challenge to find a feasible weather modification method to mitigate the impact of extreme weather events such as tropical cyclones.","Previous works have proposed potentially effective actuators and assessed their capabilities to achieve weather modification objectives through numerical simulations.","However, few studies have explored efficient mathematical and computational methods to inversely determine optimal actuators from specific modification goals.","Here I demonstrate the utility of the ensemble Kalman filter (EnKF)-based control method, referred to as ensemble Kalman control (EnKC).","The series of numerical experiments with the Lorenz 96 model indicates that EnKC efficiently identifies local, small, and intermittent control perturbations that can mitigate extreme events.","The existing techniques of EnKF, such as background error covariance localization and observation error covariance inflation, can improve the sparsity and efficiency of the control.","This work paves the way toward the real-world applications of EnKC to explore the controllability of extreme atmospheric events."],"url":"http://arxiv.org/abs/2405.08387v1","category":"stat.AP"}
{"created":"2024-05-14 07:25:47","title":"On Instability Properties of the Fractional Calder\u00f3n Problem","abstract":"We prove exponential instability properties for the fractional Calder\\'on problem and the conductivity formulation of the fractional Calder\\'on problem in the regime of fractional powers $s\\in (0,1)$. We particularly focus on two settings: First, we discuss instability properties in general domain geometries with scaling critical $L^{\\frac{n}{2s}}$ potentials and constant background metrics. Secondly, we investigate instability properties in general geometries with $L^{\\frac{n}{2s}}$ potentials and low regularity, variable coefficient, possibly anisotropic background metrics. In both settings we make use of the methods introduced in \\cite{KRS21} and we deduce strong compression estimates for the forward problem. In the first setting this is based on analytic smoothing estimates for a suitable comparison operator while in the second setting involving low regularity metrics this is based on an iterated compression gain. We thus generalize the results from \\cite{RS18} to generic geometries and variable coefficients and further also discuss the setting of fractional conductivity equations. In particular, this proves that the logarithmic stability estimates for the fractional Calder\\'on problem from \\cite{RS20} are optimal.","sentences":["We prove exponential instability properties for the fractional Calder\\'on problem and the conductivity formulation of the fractional Calder\\'on problem in the regime of fractional powers $s\\in (0,1)$. We particularly focus on two settings: First, we discuss instability properties in general domain geometries with scaling critical $L^{\\frac{n}{2s}}$ potentials and constant background metrics.","Secondly, we investigate instability properties in general geometries with $L^{\\frac{n}{2s}}$ potentials and low regularity, variable coefficient, possibly anisotropic background metrics.","In both settings we make use of the methods introduced in \\cite{KRS21} and we deduce strong compression estimates for the forward problem.","In the first setting this is based on analytic smoothing estimates for a suitable comparison operator while in the second setting involving low regularity metrics this is based on an iterated compression gain.","We thus generalize the results from \\cite{RS18} to generic geometries and variable coefficients and further also discuss the setting of fractional conductivity equations.","In particular, this proves that the logarithmic stability estimates for the fractional Calder\\'on problem from \\cite{RS20} are optimal."],"url":"http://arxiv.org/abs/2405.08381v1","category":"math.AP"}
{"created":"2024-05-14 07:21:27","title":"Detecting and Handling Reflection Symmetries in Mixed-Integer (Nonlinear) Programming","abstract":"Symmetries in mixed-integer (nonlinear) programs (MINLP), if not handled appropriately, are known to negatively impact the performance of (spatial) branch-and-bound algorithms. Usually one thus tries to remove symmetries from the problem formulation or is relying on a solver that automatically detects and handles symmetries. While modelers of a problem can handle various kinds of symmetries, automatic symmetry detection and handling is mostly restricted to permutation symmetries. This article therefore develops techniques such that also black-box solvers can automatically detect and handle a broader class of symmetries.   Inspired from geometric packing problems such as the kissing number problem, we focus on reflection symmetries of MINLPs. We develop a generic and easily applicable framework that allows to automatically detect reflection symmetries for MINLPs. To handle this broader class of symmetries, we discuss generalizations of state-of-the-art methods for permutation symmetries, and develop dedicated symmetry handling methods for special reflection symmetry groups. Our symmetry detection framework has been implemented in the open-source solver SCIP and we provide a comprehensive discussion of the implementation. The article concludes with a detailed numerical evaluation of our symmetry handling methods when solving MINLPs.","sentences":["Symmetries in mixed-integer (nonlinear) programs (MINLP), if not handled appropriately, are known to negatively impact the performance of (spatial) branch-and-bound algorithms.","Usually one thus tries to remove symmetries from the problem formulation or is relying on a solver that automatically detects and handles symmetries.","While modelers of a problem can handle various kinds of symmetries, automatic symmetry detection and handling is mostly restricted to permutation symmetries.","This article therefore develops techniques such that also black-box solvers can automatically detect and handle a broader class of symmetries.   ","Inspired from geometric packing problems such as the kissing number problem, we focus on reflection symmetries of MINLPs.","We develop a generic and easily applicable framework that allows to automatically detect reflection symmetries for MINLPs.","To handle this broader class of symmetries, we discuss generalizations of state-of-the-art methods for permutation symmetries, and develop dedicated symmetry handling methods for special reflection symmetry groups.","Our symmetry detection framework has been implemented in the open-source solver SCIP and we provide a comprehensive discussion of the implementation.","The article concludes with a detailed numerical evaluation of our symmetry handling methods when solving MINLPs."],"url":"http://arxiv.org/abs/2405.08379v1","category":"math.OC"}
{"created":"2024-05-14 07:05:37","title":"A Riemannian Proximal Newton-CG Method","abstract":"Recently, a Riemannian proximal Newton method has been developed for optimizing problems in the form of $\\min_{x\\in\\mathcal{M}} f(x) + \\mu \\|x\\|_1$, where $\\mathcal{M}$ is a compact embedded submanifold and $f(x)$ is smooth. Although this method converges superlinearly locally, global convergence is not guaranteed. The existing remedy relies on a hybrid approach: running a Riemannian proximal gradient method until the iterate is sufficiently accurate and switching to the Riemannian proximal Newton method. This existing approach is sensitive to the switching parameter. This paper proposes a Riemannian proximal Newton-CG method that merges the truncated conjugate gradient method with the Riemannian proximal Newton method. The global convergence and local superlinear convergence are proven. Numerical experiments show that the proposed method outperforms other state-of-the-art methods.","sentences":["Recently, a Riemannian proximal Newton method has been developed for optimizing problems in the form of $\\min_{x\\in\\mathcal{M}} f(x)","+ \\mu \\|x\\|_1$, where $\\mathcal{M}$ is a compact embedded submanifold and $f(x)$ is smooth.","Although this method converges superlinearly locally, global convergence is not guaranteed.","The existing remedy relies on a hybrid approach: running a Riemannian proximal gradient method until the iterate is sufficiently accurate and switching to the Riemannian proximal Newton method.","This existing approach is sensitive to the switching parameter.","This paper proposes a Riemannian proximal Newton-CG method that merges the truncated conjugate gradient method with the Riemannian proximal Newton method.","The global convergence and local superlinear convergence are proven.","Numerical experiments show that the proposed method outperforms other state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.08365v1","category":"math.OC"}
{"created":"2024-05-14 07:05:18","title":"UnMarker: A Universal Attack on Defensive Watermarking","abstract":"Reports regarding the misuse of $\\textit{Generative AI}$ ($\\textit{GenAI}$) to create harmful deepfakes are emerging daily. Recently, defensive watermarking, which enables $\\textit{GenAI}$ providers to hide fingerprints in their images to later use for deepfake detection, has been on the rise. Yet, its potential has not been fully explored. We present $\\textit{UnMarker}$ -- the first practical $\\textit{universal}$ attack on defensive watermarking. Unlike existing attacks, $\\textit{UnMarker}$ requires no detector feedback, no unrealistic knowledge of the scheme or similar models, and no advanced denoising pipelines that may not be available. Instead, being the product of an in-depth analysis of the watermarking paradigm revealing that robust schemes must construct their watermarks in the spectral amplitudes, $\\textit{UnMarker}$ employs two novel adversarial optimizations to disrupt the spectra of watermarked images, erasing the watermarks. Evaluations against the $\\textit{SOTA}$ prove its effectiveness, not only defeating traditional schemes while retaining superior quality compared to existing attacks but also breaking $\\textit{semantic}$ watermarks that alter the image's structure, reducing the best detection rate to $43\\%$ and rendering them useless. To our knowledge, $\\textit{UnMarker}$ is the first practical attack on $\\textit{semantic}$ watermarks, which have been deemed the future of robust watermarking. $\\textit{UnMarker}$ casts doubts on the very penitential of this countermeasure and exposes its paradoxical nature as designing schemes for robustness inevitably compromises other robustness aspects.","sentences":["Reports regarding the misuse of $\\textit{Generative AI}$ ($\\textit{GenAI}$) to create harmful deepfakes are emerging daily.","Recently, defensive watermarking, which enables $\\textit{GenAI}$ providers to hide fingerprints in their images to later use for deepfake detection, has been on the rise.","Yet, its potential has not been fully explored.","We present $\\textit{UnMarker}$ -- the first practical $\\textit{universal}$ attack on defensive watermarking.","Unlike existing attacks, $\\textit{UnMarker}$ requires no detector feedback, no unrealistic knowledge of the scheme or similar models, and no advanced denoising pipelines that may not be available.","Instead, being the product of an in-depth analysis of the watermarking paradigm revealing that robust schemes must construct their watermarks in the spectral amplitudes, $\\textit{UnMarker}$ employs two novel adversarial optimizations to disrupt the spectra of watermarked images, erasing the watermarks.","Evaluations against the $\\textit{SOTA}$ prove its effectiveness, not only defeating traditional schemes while retaining superior quality compared to existing attacks but also breaking $\\textit{semantic}$ watermarks that alter the image's structure, reducing the best detection rate to $43\\%$ and rendering them useless.","To our knowledge, $\\textit{UnMarker}$ is the first practical attack on $\\textit{semantic}$ watermarks, which have been deemed the future of robust watermarking.","$\\textit{UnMarker}$ casts doubts on the very penitential of this countermeasure and exposes its paradoxical nature as designing schemes for robustness inevitably compromises other robustness aspects."],"url":"http://arxiv.org/abs/2405.08363v1","category":"cs.CR"}
{"created":"2024-05-14 06:31:42","title":"Accuracy Evaluation of a Lightweight Analytic Vehicle Dynamics Model for Maneuver Planning","abstract":"Models for vehicle dynamics play an important role in maneuver planning for automated driving. They are used to derive trajectories from given control inputs, or to evaluate a given trajectory in terms of constraint violation or optimality criteria such as safety, comfort or ecology. Depending on the computation process, models with different assumptions and levels of detail are used; since maneuver planning usually has strong requirements for computation speed at a potentially high number of trajectory evaluations per planning cycle, most of the applied models aim to reduce complexity by implicitly or explicitly introducing simplifying assumptions. While evaluations show that these assumptions may be sufficiently valid under typical conditions, their effect has yet to be studied conclusively.   We propose a model for vehicle dynamics that is convenient for maneuver planning by supporting both an analytic approach of extracting parameters from a given trajectory, and a generative approach of establishing a trajectory from given control inputs. Both applications of the model are evaluated in real-world test drives under dynamic conditions, both on a closed-off test track and on public roads, and effects arising from the simplifying assumptions are analyzed.","sentences":["Models for vehicle dynamics play an important role in maneuver planning for automated driving.","They are used to derive trajectories from given control inputs, or to evaluate a given trajectory in terms of constraint violation or optimality criteria such as safety, comfort or ecology.","Depending on the computation process, models with different assumptions and levels of detail are used; since maneuver planning usually has strong requirements for computation speed at a potentially high number of trajectory evaluations per planning cycle, most of the applied models aim to reduce complexity by implicitly or explicitly introducing simplifying assumptions.","While evaluations show that these assumptions may be sufficiently valid under typical conditions, their effect has yet to be studied conclusively.   ","We propose a model for vehicle dynamics that is convenient for maneuver planning by supporting both an analytic approach of extracting parameters from a given trajectory, and a generative approach of establishing a trajectory from given control inputs.","Both applications of the model are evaluated in real-world test drives under dynamic conditions, both on a closed-off test track and on public roads, and effects arising from the simplifying assumptions are analyzed."],"url":"http://arxiv.org/abs/2405.08343v1","category":"cs.RO"}
{"created":"2024-05-14 05:44:07","title":"Sufficient conditions, lower bounds and trade-off relations for quantumness in Kirkwood-Dirac quasiprobability","abstract":"Kirkwood-Dirac (KD) quasiprobability is a quantum analog of classical phase space probability. It offers an informationally complete representation of quantum state wherein the quantumness associated with quantum noncommutativity manifests in its nonclassical values, i.e., the nonreal and/or negative values of the real part. This naturally raises a question: how does such form of quantumness comply with the uncertainty principle which also arise from quantum noncommutativity? Here, first, we obtain sufficient conditions for the KD quasiprobability defined relative to a pair of PVM (projection-valued measure) bases to have nonclassical values. Using these nonclassical values, we then introduce two quantities which capture the amount of KD quantumness in a quantum state relative to a single PVM basis. They are defined respectively as the nonreality, and the classicality which captures both the nonreality and negativity, of the associated KD quasiprobability over the PVM basis of interest, and another PVM basis, and maximized over all possible choices of the latter. We obtain their lower bounds, and derive trade-off relations respectively reminiscent of the Robertson and Robertson-Schr\\\"odinger uncertainty relations but with lower bounds maximized over the convex sets of Hermitian operators whose complete sets of eigenprojectors are given by the PVM bases. We discuss their measurement using weak value measurement and classical optimization, and suggest information theoretical and operational interpretations in terms of optimal estimation of the PVM basis and state disturbance.","sentences":["Kirkwood-Dirac (KD) quasiprobability is a quantum analog of classical phase space probability.","It offers an informationally complete representation of quantum state wherein the quantumness associated with quantum noncommutativity manifests in its nonclassical values, i.e., the nonreal and/or negative values of the real part.","This naturally raises a question: how does such form of quantumness comply with the uncertainty principle which also arise from quantum noncommutativity?","Here, first, we obtain sufficient conditions for the KD quasiprobability defined relative to a pair of PVM (projection-valued measure) bases to have nonclassical values.","Using these nonclassical values, we then introduce two quantities which capture the amount of KD quantumness in a quantum state relative to a single PVM basis.","They are defined respectively as the nonreality, and the classicality which captures both the nonreality and negativity, of the associated KD quasiprobability over the PVM basis of interest, and another PVM basis, and maximized over all possible choices of the latter.","We obtain their lower bounds, and derive trade-off relations respectively reminiscent of the Robertson and Robertson-Schr\\\"odinger uncertainty relations but with lower bounds maximized over the convex sets of Hermitian operators whose complete sets of eigenprojectors are given by the PVM bases.","We discuss their measurement using weak value measurement and classical optimization, and suggest information theoretical and operational interpretations in terms of optimal estimation of the PVM basis and state disturbance."],"url":"http://arxiv.org/abs/2405.08324v1","category":"quant-ph"}
{"created":"2024-05-14 05:41:59","title":"StraightPCF: Straight Point Cloud Filtering","abstract":"Point cloud filtering is a fundamental 3D vision task, which aims to remove noise while recovering the underlying clean surfaces. State-of-the-art methods remove noise by moving noisy points along stochastic trajectories to the clean surfaces. These methods often require regularization within the training objective and/or during post-processing, to ensure fidelity. In this paper, we introduce StraightPCF, a new deep learning based method for point cloud filtering. It works by moving noisy points along straight paths, thus reducing discretization errors while ensuring faster convergence to the clean surfaces. We model noisy patches as intermediate states between high noise patch variants and their clean counterparts, and design the VelocityModule to infer a constant flow velocity from the former to the latter. This constant flow leads to straight filtering trajectories. In addition, we introduce a DistanceModule that scales the straight trajectory using an estimated distance scalar to attain convergence near the clean surface. Our network is lightweight and only has $\\sim530K$ parameters, being 17% of IterativePFN (a most recent point cloud filtering network). Extensive experiments on both synthetic and real-world data show our method achieves state-of-the-art results. Our method also demonstrates nice distributions of filtered points without the need for regularization. The implementation code can be found at: https://github.com/ddsediri/StraightPCF.","sentences":["Point cloud filtering is a fundamental 3D vision task, which aims to remove noise while recovering the underlying clean surfaces.","State-of-the-art methods remove noise by moving noisy points along stochastic trajectories to the clean surfaces.","These methods often require regularization within the training objective and/or during post-processing, to ensure fidelity.","In this paper, we introduce StraightPCF, a new deep learning based method for point cloud filtering.","It works by moving noisy points along straight paths, thus reducing discretization errors while ensuring faster convergence to the clean surfaces.","We model noisy patches as intermediate states between high noise patch variants and their clean counterparts, and design the VelocityModule to infer a constant flow velocity from the former to the latter.","This constant flow leads to straight filtering trajectories.","In addition, we introduce a DistanceModule that scales the straight trajectory using an estimated distance scalar to attain convergence near the clean surface.","Our network is lightweight and only has $\\sim530K$ parameters, being 17% of IterativePFN (a most recent point cloud filtering network).","Extensive experiments on both synthetic and real-world data show our method achieves state-of-the-art results.","Our method also demonstrates nice distributions of filtered points without the need for regularization.","The implementation code can be found at: https://github.com/ddsediri/StraightPCF."],"url":"http://arxiv.org/abs/2405.08322v1","category":"cs.CV"}
{"created":"2024-05-14 05:34:54","title":"Strain-induced long-range charge-density wave order in the optimally doped Bi$_2$Sr$_{2-x}$La$_x$CuO$_{6}$ superconductor","abstract":"The mechanism of high-temperature superconductivity in copper oxides (cuprate) remains elusive, with the pseudogap phase considered a potential factor. Recent attention has focused on a long-range symmetry-broken charge-density wave (CDW) order in the underdoped regime, induced by strong magnetic fields. Here by $^{63,65}$Cu-nuclear magnetic resonance, we report the discovery of a long-range CDW order in the optimally doped Bi$_2$Sr$_{2-x}$La$_x$CuO$_6$ superconductor, induced by in-plane strain exceeding $|$$\\varepsilon$$|$ = 0.15 %, which deliberately breaks the crystal symmetry of the CuO$_2$ plane. We find that compressive/tensile strains reduce superconductivity but enhance CDW, leaving superconductivity to coexist with CDW. The findings show that a long-range CDW order is an underlying hidden order in the pseudogap state, not limited to the underdoped regime, becoming apparent under strain. Our result sheds light on the intertwining of various orders in the cuprates.","sentences":["The mechanism of high-temperature superconductivity in copper oxides (cuprate) remains elusive, with the pseudogap phase considered a potential factor.","Recent attention has focused on a long-range symmetry-broken charge-density wave (CDW) order in the underdoped regime, induced by strong magnetic fields.","Here by $^{63,65}$Cu-nuclear magnetic resonance, we report the discovery of a long-range CDW order in the optimally doped Bi$_2$Sr$_{2-x}$La$_x$CuO$_6$ superconductor, induced by in-plane strain exceeding $|$$\\varepsilon$$|$ = 0.15 %, which deliberately breaks the crystal symmetry of the CuO$_2$ plane.","We find that compressive/tensile strains reduce superconductivity but enhance CDW, leaving superconductivity to coexist with CDW.","The findings show that a long-range CDW order is an underlying hidden order in the pseudogap state, not limited to the underdoped regime, becoming apparent under strain.","Our result sheds light on the intertwining of various orders in the cuprates."],"url":"http://arxiv.org/abs/2405.08320v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 05:17:01","title":"Measurement-based quantum machine learning","abstract":"A quantum neural network (QNN) is an object that extends the notion of a classical neural network to quantum models for quantum data. We can create a QNN by parametrizing a quantum process and then using it to model unknown relations between quantum states. In this paper, we explore how to use measurement-based quantum computation for quantum machine learning problems and propose a universal QNN in this framework which we call the multiple-triangle ansatz (MuTA). Using the proposed QNN, we solve several tasks, including learning a universal set of gates, optimizing measurement with post-processing, learning a quantum instrument, and the classification of classical data. Finally, we discuss how to train an ansatz under the hardware constraints imposed by photonic Gottesman-Kitaev-Preskill qubits. Our work demonstrates the feasibility of using measurement-based quantum computation as a framework for quantum machine learning algorithms.","sentences":["A quantum neural network (QNN) is an object that extends the notion of a classical neural network to quantum models for quantum data.","We can create a QNN by parametrizing a quantum process and then using it to model unknown relations between quantum states.","In this paper, we explore how to use measurement-based quantum computation for quantum machine learning problems and propose a universal QNN in this framework which we call the multiple-triangle ansatz (MuTA).","Using the proposed QNN, we solve several tasks, including learning a universal set of gates, optimizing measurement with post-processing, learning a quantum instrument, and the classification of classical data.","Finally, we discuss how to train an ansatz under the hardware constraints imposed by photonic Gottesman-Kitaev-Preskill qubits.","Our work demonstrates the feasibility of using measurement-based quantum computation as a framework for quantum machine learning algorithms."],"url":"http://arxiv.org/abs/2405.08319v1","category":"quant-ph"}
{"created":"2024-05-14 04:12:11","title":"Flight Path Optimization with Optimal Control Method","abstract":"This paper is based on a crucial issue in the aviation world: how to optimize the trajectory and controls given to the aircraft in order to optimize flight time and fuel consumption. This study aims to provide elements of a response to this problem and to define, under certain simplifying assumptions, an optimal response, using Constrained Finite Time Optimal Control(CFTOC). The first step is to define the dynamic model of the aircraft in accordance with the controllable inputs and wind disturbances. Then we will identify a precise objective in terms of optimization and implement an optimization program to solve it under the circumstances of simulated real flight situation. Finally, the optimization result is validated and discussed by different scenarios.","sentences":["This paper is based on a crucial issue in the aviation world: how to optimize the trajectory and controls given to the aircraft in order to optimize flight time and fuel consumption.","This study aims to provide elements of a response to this problem and to define, under certain simplifying assumptions, an optimal response, using Constrained Finite Time Optimal Control(CFTOC).","The first step is to define the dynamic model of the aircraft in accordance with the controllable inputs and wind disturbances.","Then we will identify a precise objective in terms of optimization and implement an optimization program to solve it under the circumstances of simulated real flight situation.","Finally, the optimization result is validated and discussed by different scenarios."],"url":"http://arxiv.org/abs/2405.08306v1","category":"math.OC"}
{"created":"2024-05-14 04:01:32","title":"Collateral Portfolio Optimization in Crypto-Backed Stablecoins","abstract":"Stablecoins - crypto tokens whose value is pegged to a real-world asset such as the US Dollar - are an important component of the DeFi ecosystem as they mitigate the impact of token price volatility. In crypto-backed stablecoins, the peg is founded on the guarantee that in case of system shutdown, each stablecoin can be exchanged for a basket of other crypto tokens worth approximately its nominal value. However, price fluctuations that affect the collateral tokens may cause this guarantee to be invalidated. In this work, we investigate the impact of the collateral portfolio's composition on the resilience to this type of catastrophic event. For stablecoins whose developers maintain a significant portion of the collateral (e.g., MakerDAO's Dai), we propose two portfolio optimization methods, based on convex optimization and (semi)variance minimization, that account for the correlation between the various token prices. We compare the optimal portfolios to the historical evolution of Dai's collateral portfolio, and to aid reproducibility, we have made our data and code publicly available.","sentences":["Stablecoins - crypto tokens whose value is pegged to a real-world asset such as the US Dollar - are an important component of the DeFi ecosystem as they mitigate the impact of token price volatility.","In crypto-backed stablecoins, the peg is founded on the guarantee that in case of system shutdown, each stablecoin can be exchanged for a basket of other crypto tokens worth approximately its nominal value.","However, price fluctuations that affect the collateral tokens may cause this guarantee to be invalidated.","In this work, we investigate the impact of the collateral portfolio's composition on the resilience to this type of catastrophic event.","For stablecoins whose developers maintain a significant portion of the collateral (e.g., MakerDAO's Dai), we propose two portfolio optimization methods, based on convex optimization and (semi)variance minimization, that account for the correlation between the various token prices.","We compare the optimal portfolios to the historical evolution of Dai's collateral portfolio, and to aid reproducibility, we have made our data and code publicly available."],"url":"http://arxiv.org/abs/2405.08305v1","category":"cs.CR"}
{"created":"2024-05-14 03:53:17","title":"Coded Downlink Massive Random Access and a Finite de Finetti Theorem","abstract":"This paper considers a massive connectivity setting in which a base-station (BS) aims to communicate sources $(X_1,\\cdots,X_k)$ to a randomly activated subset of $k$ users, among a large pool of $n$ users, via a common downlink message. Although the identities of the $k$ active users are assumed to be known at the BS, each active user only knows whether itself is active and does not know the identities of the other active users. A naive coding strategy is to transmit the sources alongside the identities of the users for which the source information is intended, which would require $H(X_1,\\cdots,X_k) + k\\log(n)$ bits, because the cost of specifying the identity of a user is $\\log(n)$ bits. For large $n$, this overhead can be significant. This paper shows that it is possible to develop coding techniques that eliminate the dependency of the overhead on $n$, if the source distribution follows certain symmetry. Specifically, if the source distribution is independent and identically distributed (i.i.d.) then the overhead can be reduced to at most $O(\\log(k))$ bits, and in case of uniform i.i.d. sources, the overhead can be further reduced to $O(1)$ bits. For sources that follow a more general exchangeable distribution, the overhead is at most $O(k)$ bits, and in case of finite-alphabet exchangeable sources, the overhead can be further reduced to $O(\\log(k))$ bits. The downlink massive random access problem is closely connected to the study of finite exchangeable sequences. The proposed coding strategy allows bounds on the relative entropy distance between finite exchangeable distributions and i.i.d. mixture distributions to be developed, and gives a new relative entropy version of the finite de Finetti theorem which is scaling optimal.","sentences":["This paper considers a massive connectivity setting in which a base-station (BS) aims to communicate sources $(X_1,\\cdots,X_k)$ to a randomly activated subset of $k$ users, among a large pool of $n$ users, via a common downlink message.","Although the identities of the $k$ active users are assumed to be known at the BS, each active user only knows whether itself is active and does not know the identities of the other active users.","A naive coding strategy is to transmit the sources alongside the identities of the users for which the source information is intended, which would require $H(X_1,\\cdots,X_k)","+ k\\log(n)$ bits, because the cost of specifying the identity of a user is $\\log(n)$ bits.","For large $n$, this overhead can be significant.","This paper shows that it is possible to develop coding techniques that eliminate the dependency of the overhead on $n$, if the source distribution follows certain symmetry.","Specifically, if the source distribution is independent and identically distributed (i.i.d.)","then the overhead can be reduced to at most $O(\\log(k))$ bits, and in case of uniform i.i.d. sources, the overhead can be further reduced to $O(1)$ bits.","For sources that follow a more general exchangeable distribution, the overhead is at most $O(k)$ bits, and in case of finite-alphabet exchangeable sources, the overhead can be further reduced to $O(\\log(k))$ bits.","The downlink massive random access problem is closely connected to the study of finite exchangeable sequences.","The proposed coding strategy allows bounds on the relative entropy distance between finite exchangeable distributions and i.i.d. mixture distributions to be developed, and gives a new relative entropy version of the finite de Finetti theorem which is scaling optimal."],"url":"http://arxiv.org/abs/2405.08301v1","category":"cs.IT"}
{"created":"2024-05-14 03:48:45","title":"Deep Reinforcement Learning for Real-Time Ground Delay Program Revision and Corresponding Flight Delay Assignments","abstract":"This paper explores the optimization of Ground Delay Programs (GDP), a prevalent Traffic Management Initiative used in Air Traffic Management (ATM) to reconcile capacity and demand discrepancies at airports. Employing Reinforcement Learning (RL) to manage the inherent uncertainties in the national airspace system-such as weather variability, fluctuating flight demands, and airport arrival rates-we developed two RL models: Behavioral Cloning (BC) and Conservative Q-Learning (CQL). These models are designed to enhance GDP efficiency by utilizing a sophisticated reward function that integrates ground and airborne delays and terminal area congestion. We constructed a simulated single-airport environment, SAGDP_ENV, which incorporates real operational data along with predicted uncertainties to facilitate realistic decision-making scenarios. Utilizing the whole year 2019 data from Newark Liberty International Airport (EWR), our models aimed to preemptively set airport program rates. Despite thorough modeling and simulation, initial outcomes indicated that the models struggled to learn effectively, attributed potentially to oversimplified environmental assumptions. This paper discusses the challenges encountered, evaluates the models' performance against actual operational data, and outlines future directions to refine RL applications in ATM.","sentences":["This paper explores the optimization of Ground Delay Programs (GDP), a prevalent Traffic Management Initiative used in Air Traffic Management (ATM) to reconcile capacity and demand discrepancies at airports.","Employing Reinforcement Learning (RL) to manage the inherent uncertainties in the national airspace system-such as weather variability, fluctuating flight demands, and airport arrival rates-we developed two RL models: Behavioral Cloning (BC) and Conservative Q-Learning (CQL).","These models are designed to enhance GDP efficiency by utilizing a sophisticated reward function that integrates ground and airborne delays and terminal area congestion.","We constructed a simulated single-airport environment, SAGDP_ENV, which incorporates real operational data along with predicted uncertainties to facilitate realistic decision-making scenarios.","Utilizing the whole year 2019 data from Newark Liberty International Airport (EWR), our models aimed to preemptively set airport program rates.","Despite thorough modeling and simulation, initial outcomes indicated that the models struggled to learn effectively, attributed potentially to oversimplified environmental assumptions.","This paper discusses the challenges encountered, evaluates the models' performance against actual operational data, and outlines future directions to refine RL applications in ATM."],"url":"http://arxiv.org/abs/2405.08298v1","category":"cs.LG"}
{"created":"2024-05-14 03:33:31","title":"SpeechVerse: A Large-scale Generalizable Audio Language Model","abstract":"Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.","sentences":["Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions.","Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation.","We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training.","The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions.","We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks.","Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks.","Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks."],"url":"http://arxiv.org/abs/2405.08295v1","category":"cs.CL"}
{"created":"2024-05-14 03:13:55","title":"MCMC using $\\textit{bouncy}$ Hamiltonian dynamics: A unifying framework for Hamiltonian Monte Carlo and piecewise deterministic Markov process samplers","abstract":"Piecewise-deterministic Markov process (PDMP) samplers constitute a state of the art Markov chain Monte Carlo (MCMC) paradigm in Bayesian computation, with examples including the zig-zag and bouncy particle sampler (BPS). Recent work on the zig-zag has indicated its connection to Hamiltonian Monte Carlo, a version of the Metropolis algorithm that exploits Hamiltonian dynamics. Here we establish that, in fact, the connection between the paradigms extends far beyond the specific instance. The key lies in (1) the fact that any time-reversible deterministic dynamics provides a valid Metropolis proposal and (2) how PDMPs' characteristic velocity changes constitute an alternative to the usual acceptance-rejection. We turn this observation into a rigorous framework for constructing rejection-free Metropolis proposals based on bouncy Hamiltonian dynamics which simultaneously possess Hamiltonian-like properties and generate discontinuous trajectories similar in appearance to PDMPs. When combined with periodic refreshment of the inertia, the dynamics converge strongly to PDMP equivalents in the limit of increasingly frequent refreshment. We demonstrate the practical implications of this new paradigm, with a sampler based on a bouncy Hamiltonian dynamics closely related to the BPS. The resulting sampler exhibits competitive performance on challenging real-data posteriors involving tens of thousands of parameters.","sentences":["Piecewise-deterministic Markov process (PDMP) samplers constitute a state of the art Markov chain Monte Carlo (MCMC) paradigm in Bayesian computation, with examples including the zig-zag and bouncy particle sampler (BPS).","Recent work on the zig-zag has indicated its connection to Hamiltonian Monte Carlo, a version of the Metropolis algorithm that exploits Hamiltonian dynamics.","Here we establish that, in fact, the connection between the paradigms extends far beyond the specific instance.","The key lies in (1) the fact that any time-reversible deterministic dynamics provides a valid Metropolis proposal and (2) how PDMPs' characteristic velocity changes constitute an alternative to the usual acceptance-rejection.","We turn this observation into a rigorous framework for constructing rejection-free Metropolis proposals based on bouncy Hamiltonian dynamics which simultaneously possess Hamiltonian-like properties and generate discontinuous trajectories similar in appearance to PDMPs.","When combined with periodic refreshment of the inertia, the dynamics converge strongly to PDMP equivalents in the limit of increasingly frequent refreshment.","We demonstrate the practical implications of this new paradigm, with a sampler based on a bouncy Hamiltonian dynamics closely related to the BPS.","The resulting sampler exhibits competitive performance on challenging real-data posteriors involving tens of thousands of parameters."],"url":"http://arxiv.org/abs/2405.08290v1","category":"stat.CO"}
{"created":"2024-05-14 03:11:55","title":"Exploring Equilibrium Strategies in Network Games with Generative AI","abstract":"Game theory offers a powerful framework for analyzing strategic interactions among decision-makers, providing tools to model, analyze, and predict their behavior. However, implementing game theory can be challenging due to difficulties in deriving solutions, understanding interactions, and ensuring optimal performance. Traditional non-AI and discriminative AI approaches have made valuable contributions but struggle with limitations in handling large-scale games and dynamic scenarios. In this context, generative AI emerges as a promising solution because of its superior data analysis and generation capabilities. This paper comprehensively summarizes the challenges, solutions, and outlooks of combining generative AI with game theory. We start with reviewing the limitations of traditional non-AI and discriminative AI approaches in employing game theory, and then highlight the necessity and advantages of integrating generative AI. Next, we explore the applications of generative AI in various stages of the game theory lifecycle, including model formulation, solution derivation, and strategy improvement. Additionally, from game theory viewpoint, we propose a generative AI-enabled framework for optimizing machine learning model performance against false data injection attacks, supported by a case study to demonstrate its effectiveness. Finally, we outline future research directions for generative AI-enabled game theory, paving the way for its further advancements and development.","sentences":["Game theory offers a powerful framework for analyzing strategic interactions among decision-makers, providing tools to model, analyze, and predict their behavior.","However, implementing game theory can be challenging due to difficulties in deriving solutions, understanding interactions, and ensuring optimal performance.","Traditional non-AI and discriminative AI approaches have made valuable contributions but struggle with limitations in handling large-scale games and dynamic scenarios.","In this context, generative AI emerges as a promising solution because of its superior data analysis and generation capabilities.","This paper comprehensively summarizes the challenges, solutions, and outlooks of combining generative AI with game theory.","We start with reviewing the limitations of traditional non-AI and discriminative AI approaches in employing game theory, and then highlight the necessity and advantages of integrating generative AI.","Next, we explore the applications of generative AI in various stages of the game theory lifecycle, including model formulation, solution derivation, and strategy improvement.","Additionally, from game theory viewpoint, we propose a generative AI-enabled framework for optimizing machine learning model performance against false data injection attacks, supported by a case study to demonstrate its effectiveness.","Finally, we outline future research directions for generative AI-enabled game theory, paving the way for its further advancements and development."],"url":"http://arxiv.org/abs/2405.08289v1","category":"cs.GT"}
{"created":"2024-05-14 02:51:35","title":"Future Trends in the Design of Memetic Algorithms: the Case of the Linear Ordering Problem","abstract":"The way heuristic optimizers are designed has evolved over the decades, as computing power has increased. Initially, trajectory metaheuristics used to shape the state of the art in many problems, whereas today, population-based mechanisms tend to be more effective.Such has been the case for the Linear Ordering Problem (LOP), a field in which strategies such as Iterated Local Search and Variable Neighborhood Search led the way during the 1990s, but which have now been surpassed by evolutionary and memetic schemes. This paper focuses on understanding how the design of LOP optimizers will change in the future, as computing power continues to increase, yielding two main contributions. On the one hand, a metaheuristic was designed that is capable of effectively exploiting a large amount of computational resources, specifically, computing power equivalent to what a recent core can output during runs lasting over four months. Our analysis of this aspect relied on parallelization, and allowed us to conclude that as the power of the computational resources increases, it will be necessary to boost the capacities of the intensification methods applied in the memetic algorithms to keep the population from stagnating. And on the other, the best-known results for today's most challenging set of instances (xLOLIB2) were significantly outperformed. Instances with sizes ranging from 300 to 1000 were analyzed, and new bounds were established that provide a frame of reference for future research.","sentences":["The way heuristic optimizers are designed has evolved over the decades, as computing power has increased.","Initially, trajectory metaheuristics used to shape the state of the art in many problems, whereas today, population-based mechanisms tend to be more effective.","Such has been the case for the Linear Ordering Problem (LOP), a field in which strategies such as Iterated Local Search and Variable Neighborhood Search led the way during the 1990s, but which have now been surpassed by evolutionary and memetic schemes.","This paper focuses on understanding how the design of LOP optimizers will change in the future, as computing power continues to increase, yielding two main contributions.","On the one hand, a metaheuristic was designed that is capable of effectively exploiting a large amount of computational resources, specifically, computing power equivalent to what a recent core can output during runs lasting over four months.","Our analysis of this aspect relied on parallelization, and allowed us to conclude that as the power of the computational resources increases, it will be necessary to boost the capacities of the intensification methods applied in the memetic algorithms to keep the population from stagnating.","And on the other, the best-known results for today's most challenging set of instances (xLOLIB2) were significantly outperformed.","Instances with sizes ranging from 300 to 1000 were analyzed, and new bounds were established that provide a frame of reference for future research."],"url":"http://arxiv.org/abs/2405.08285v1","category":"cs.NE"}
{"created":"2024-05-14 02:46:54","title":"Vector Field-Guided Learning Predictive Control for Motion Planning of Mobile Robots with Unknown Dynamics","abstract":"Safe maneuvering capability is critical for mobile robots in complex environments. However, robotic system dynamics are often time-varying, uncertain, or even unknown during the motion planning and control process. Therefore, many existing model-based reinforcement learning (RL) methods could not achieve satisfactory reliability in guaranteeing safety. To address this challenge, we propose a two-level Vector Field-guided Learning Predictive Control (VF-LPC) approach that guarantees safe maneuverability. The first level, the guiding level, generates safe desired trajectories using the designed kinodynamic guiding vector field, enabling safe motion in obstacle-dense environments. The second level, the Integrated Motion Planning and Control (IMPC) level, first uses the deep Koopman operator to learn a nominal dynamics model offline and then updates the model uncertainties online using sparse Gaussian processes (GPs). The learned dynamics and game-based safe barrier function are then incorporated into the learning predictive control framework to generate near-optimal control sequences. We conducted tests to compare the performance of VF-LPC with existing advanced planning methods in an obstacle-dense environment. The simulation results show that it can generate feasible trajectories quickly. Then, VF-LPC is evaluated against motion planning methods that employ model predictive control (MPC) and RL in high-fidelity CarSim software. The results show that VF-LPC outperforms them under metrics of completion time, route length, and average solution time. We also carried out path-tracking control tests on a racing road to validate the model uncertainties learning capability. Finally, we conducted real-world experiments on a Hongqi E-HS3 vehicle, further validating the VF-LPC approach's effectiveness.","sentences":["Safe maneuvering capability is critical for mobile robots in complex environments.","However, robotic system dynamics are often time-varying, uncertain, or even unknown during the motion planning and control process.","Therefore, many existing model-based reinforcement learning (RL) methods could not achieve satisfactory reliability in guaranteeing safety.","To address this challenge, we propose a two-level Vector Field-guided Learning Predictive Control (VF-LPC) approach that guarantees safe maneuverability.","The first level, the guiding level, generates safe desired trajectories using the designed kinodynamic guiding vector field, enabling safe motion in obstacle-dense environments.","The second level, the Integrated Motion Planning and Control (IMPC) level, first uses the deep Koopman operator to learn a nominal dynamics model offline and then updates the model uncertainties online using sparse Gaussian processes (GPs).","The learned dynamics and game-based safe barrier function are then incorporated into the learning predictive control framework to generate near-optimal control sequences.","We conducted tests to compare the performance of VF-LPC with existing advanced planning methods in an obstacle-dense environment.","The simulation results show that it can generate feasible trajectories quickly.","Then, VF-LPC is evaluated against motion planning methods that employ model predictive control (MPC) and RL in high-fidelity CarSim software.","The results show that VF-LPC outperforms them under metrics of completion time, route length, and average solution time.","We also carried out path-tracking control tests on a racing road to validate the model uncertainties learning capability.","Finally, we conducted real-world experiments on a Hongqi E-HS3 vehicle, further validating the VF-LPC approach's effectiveness."],"url":"http://arxiv.org/abs/2405.08283v1","category":"cs.RO"}
{"created":"2024-05-14 02:32:30","title":"Parallel-in-Time Iterative Methods for Pricing American Options","abstract":"For pricing American options, %after suitable discretization in space and time, a sequence of discrete linear complementarity problems (LCPs) or equivalently Hamilton-Jacobi-Bellman (HJB) equations need to be solved in a sequential time-stepping manner. In each time step, the policy iteration or its penalty variant is often applied due to their fast convergence rates. In this paper, we aim to solve for all time steps simultaneously, by applying the policy iteration to an ``all-at-once form\" of the HJB equations, where two different parallel-in-time preconditioners are proposed to accelerate the solution of the linear systems within the policy iteration. Our proposed methods are generally applicable for such all-at-once forms of the HJB equation, arising from option pricing problems with optimal stopping and nontrivial underlying asset models. Numerical examples are presented to show the feasibility and robust convergence behavior of the proposed methodology.","sentences":["For pricing American options, %after suitable discretization in space and time, a sequence of discrete linear complementarity problems (LCPs) or equivalently Hamilton-Jacobi-Bellman (HJB) equations need to be solved in a sequential time-stepping manner.","In each time step, the policy iteration or its penalty variant is often applied due to their fast convergence rates.","In this paper, we aim to solve for all time steps simultaneously, by applying the policy iteration to an ``all-at-once form\" of the HJB equations, where two different parallel-in-time preconditioners are proposed to accelerate the solution of the linear systems within the policy iteration.","Our proposed methods are generally applicable for such all-at-once forms of the HJB equation, arising from option pricing problems with optimal stopping and nontrivial underlying asset models.","Numerical examples are presented to show the feasibility and robust convergence behavior of the proposed methodology."],"url":"http://arxiv.org/abs/2405.08280v1","category":"math.NA"}
{"created":"2024-05-14 02:06:53","title":"Power of $\\ell_1$-Norm Regularized Kaczmarz Algorithms for High-Order Tensor Recovery","abstract":"Tensors serve as a crucial tool in the representation and analysis of complex, multi-dimensional data. As data volumes continue to expand, there is an increasing demand for developing optimization algorithms that can directly operate on tensors to deliver fast and effective computations. Many problems in real-world applications can be formulated as the task of recovering high-order tensors characterized by sparse and/or low-rank structures. In this work, we propose novel Kaczmarz algorithms with a power of the $\\ell_1$-norm regularization for reconstructing high-order tensors by exploiting sparsity and/or low-rankness of tensor data. In addition, we develop both a block and an accelerated variant, along with a thorough convergence analysis of these algorithms. A variety of numerical experiments on both synthetic and real-world datasets demonstrate the effectiveness and significant potential of the proposed methods in image and video processing tasks, such as image sequence destriping and video deconvolution.","sentences":["Tensors serve as a crucial tool in the representation and analysis of complex, multi-dimensional data.","As data volumes continue to expand, there is an increasing demand for developing optimization algorithms that can directly operate on tensors to deliver fast and effective computations.","Many problems in real-world applications can be formulated as the task of recovering high-order tensors characterized by sparse and/or low-rank structures.","In this work, we propose novel Kaczmarz algorithms with a power of the $\\ell_1$-norm regularization for reconstructing high-order tensors by exploiting sparsity and/or low-rankness of tensor data.","In addition, we develop both a block and an accelerated variant, along with a thorough convergence analysis of these algorithms.","A variety of numerical experiments on both synthetic and real-world datasets demonstrate the effectiveness and significant potential of the proposed methods in image and video processing tasks, such as image sequence destriping and video deconvolution."],"url":"http://arxiv.org/abs/2405.08275v1","category":"math.OC"}
{"created":"2024-05-14 01:59:29","title":"On saturation of the discrepancy principle for nonlinear Tikhonov regularization in Hilbert spaces","abstract":"In this paper we revisit the discrepancy principle for Tikhonov regularization of nonlinear ill-posed problems in Hilbert spaces and provide some new and improved saturation results under less restrictive conditions, comparing with the existing results in the literature.","sentences":["In this paper we revisit the discrepancy principle for Tikhonov regularization of nonlinear ill-posed problems in Hilbert spaces and provide some new and improved saturation results under less restrictive conditions, comparing with the existing results in the literature."],"url":"http://arxiv.org/abs/2405.08269v1","category":"math.NA"}
{"created":"2024-05-14 01:26:49","title":"Multi-Agent Combinatorial Contracts","abstract":"Combinatorial contracts are emerging as a key paradigm in algorithmic contract design, paralleling the role of combinatorial auctions in algorithmic mechanism design. In this paper we study natural combinatorial contract settings involving teams of agents, each capable of performing multiple actions. This scenario extends two fundamental special cases previously examined in the literature, namely the single-agent combinatorial action model of [Duetting et al., 2021] and the multi-agent binary-action model of [Babaioff et al., 2012, Duetting et al., 2023].   We study the algorithmic and computational aspects of these settings, highlighting the unique challenges posed by the absence of certain monotonicity properties essential for analyzing the previous special cases. To navigate these complexities, we introduce a broad set of novel tools that deepen our understanding of combinatorial contracts environments and yield good approximation guarantees.   Our main result is a constant-factor approximation for submodular multi-agent multi-action problems with value and demand oracles access. This result is tight: we show that this problem admits no PTAS (even under binary actions). As a side product of our main result, we devise an FPTAS, with value and demand oracles, for single-agent combinatorial action scenarios with general reward functions, which is of independent interest. We also provide bounds on the gap between the optimal welfare and the principal's utility. We show that, for subadditive rewards, perhaps surprisingly, this gap scales only logarithmically (rather than linearly) in the size of the action space.","sentences":["Combinatorial contracts are emerging as a key paradigm in algorithmic contract design, paralleling the role of combinatorial auctions in algorithmic mechanism design.","In this paper we study natural combinatorial contract settings involving teams of agents, each capable of performing multiple actions.","This scenario extends two fundamental special cases previously examined in the literature, namely the single-agent combinatorial action model of [Duetting et al., 2021] and the multi-agent binary-action model of [Babaioff et al., 2012, Duetting et al., 2023].   ","We study the algorithmic and computational aspects of these settings, highlighting the unique challenges posed by the absence of certain monotonicity properties essential for analyzing the previous special cases.","To navigate these complexities, we introduce a broad set of novel tools that deepen our understanding of combinatorial contracts environments and yield good approximation guarantees.   ","Our main result is a constant-factor approximation for submodular multi-agent multi-action problems with value and demand oracles access.","This result is tight: we show that this problem admits no PTAS (even under binary actions).","As a side product of our main result, we devise an FPTAS, with value and demand oracles, for single-agent combinatorial action scenarios with general reward functions, which is of independent interest.","We also provide bounds on the gap between the optimal welfare and the principal's utility.","We show that, for subadditive rewards, perhaps surprisingly, this gap scales only logarithmically (rather than linearly) in the size of the action space."],"url":"http://arxiv.org/abs/2405.08260v1","category":"cs.GT"}
{"created":"2024-05-14 01:12:48","title":"On special properties of solutions to Camassa-Holm equation and related models","abstract":"We study unique continuation properties of solutions to the b-family of equations. This includes the Camassa-Holm and the Degasperi-Procesi models. We prove that for both, the initial value problem and the periodic boundary value problem, the unique continuation results found in \\cite{LiPo} are optimal. More precisely, the result established there for the constant $c_0=0$ fails for any constant $c_0\\neq 0$.","sentences":["We study unique continuation properties of solutions to the b-family of equations.","This includes the Camassa-Holm and the Degasperi-Procesi models.","We prove that for both, the initial value problem and the periodic boundary value problem, the unique continuation results found in \\cite{LiPo} are optimal.","More precisely, the result established there for the constant $c_0=0$ fails for any constant $c_0\\neq 0$."],"url":"http://arxiv.org/abs/2405.08258v1","category":"math.AP"}
{"created":"2024-05-14 01:01:05","title":"Thompson Sampling for Infinite-Horizon Discounted Decision Processes","abstract":"We model a Markov decision process, parametrized by an unknown parameter, and study the asymptotic behavior of a sampling-based algorithm, called Thompson sampling. The standard definition of regret is not always suitable to evaluate a policy, especially when the underlying chain structure is general. We show that the standard (expected) regret can grow (super-)linearly and fails to capture the notion of learning in realistic settings with non-trivial state evolution. By decomposing the standard (expected) regret, we develop a new metric, called the expected residual regret, which forgets the immutable consequences of past actions. Instead, it measures regret against the optimal reward moving forward from the current period. We show that the expected residual regret of the Thompson sampling algorithm is upper bounded by a term which converges exponentially fast to 0. We present conditions under which the posterior sampling error of Thompson sampling converges to 0 almost surely. We then introduce the probabilistic version of the expected residual regret and present conditions under which it converges to 0 almost surely. Thus, we provide a viable concept of learning for sampling algorithms which will serve useful in broader settings than had been considered previously.","sentences":["We model a Markov decision process, parametrized by an unknown parameter, and study the asymptotic behavior of a sampling-based algorithm, called Thompson sampling.","The standard definition of regret is not always suitable to evaluate a policy, especially when the underlying chain structure is general.","We show that the standard (expected) regret can grow (super-)linearly and fails to capture the notion of learning in realistic settings with non-trivial state evolution.","By decomposing the standard (expected) regret, we develop a new metric, called the expected residual regret, which forgets the immutable consequences of past actions.","Instead, it measures regret against the optimal reward moving forward from the current period.","We show that the expected residual regret of the Thompson sampling algorithm is upper bounded by a term which converges exponentially fast to 0.","We present conditions under which the posterior sampling error of Thompson sampling converges to 0 almost surely.","We then introduce the probabilistic version of the expected residual regret and present conditions under which it converges to 0 almost surely.","Thus, we provide a viable concept of learning for sampling algorithms which will serve useful in broader settings than had been considered previously."],"url":"http://arxiv.org/abs/2405.08253v1","category":"stat.ML"}
