{"created":"2024-05-14 17:59:29","title":"Performance of wave function and Green's functions based methods for non equilibrium many-body dynamics","abstract":"Theoretical descriptions of non equilibrium dynamics of quantum many-body systems essentially employ either (i) explicit treatments, relying on truncation of the expansion of the many-body wave function, (ii) compressed representations of the many-body wave function, or (iii) evolution of an effective (downfolded) representation through Green's functions. In this work, we select representative cases of each of the methods and address how these complementary approaches capture the dynamics driven by intense field perturbations to non equilibrium states. Under strong driving, the systems are characterized by strong entanglement of the single particle density matrix and natural populations approaching those of a strongly interacting equilibrium system. We generate a representative set of results that are numerically exact and form a basis for critical comparison of the distinct families of methods. We demonstrate that the compressed formulation based on similarity transformed Hamiltonians (coupled cluster approach) is practically exact in weak fields and, hence, weakly or moderately correlated systems. Coupled cluster, however, struggles for strong driving fields, under which the system exhibits strongly correlated behavior, as measured by the von Neumann entropy of the single particle density matrix. The dynamics predicted by Green's functions in the (widely popular) GW approximation are less accurate by improve significantly upon the mean-field results in the strongly driven regime.","sentences":["Theoretical descriptions of non equilibrium dynamics of quantum many-body systems essentially employ either (i) explicit treatments, relying on truncation of the expansion of the many-body wave function, (ii) compressed representations of the many-body wave function, or (iii) evolution of an effective (downfolded) representation through Green's functions.","In this work, we select representative cases of each of the methods and address how these complementary approaches capture the dynamics driven by intense field perturbations to non equilibrium states.","Under strong driving, the systems are characterized by strong entanglement of the single particle density matrix and natural populations approaching those of a strongly interacting equilibrium system.","We generate a representative set of results that are numerically exact and form a basis for critical comparison of the distinct families of methods.","We demonstrate that the compressed formulation based on similarity transformed Hamiltonians (coupled cluster approach) is practically exact in weak fields and, hence, weakly or moderately correlated systems.","Coupled cluster, however, struggles for strong driving fields, under which the system exhibits strongly correlated behavior, as measured by the von Neumann entropy of the single particle density matrix.","The dynamics predicted by Green's functions in the (widely popular) GW approximation are less accurate by improve significantly upon the mean-field results in the strongly driven regime."],"url":"http://arxiv.org/abs/2405.08814v1","category":"physics.comp-ph"}
{"created":"2024-05-14 17:59:02","title":"CinePile: A Long Video Question Answering Dataset and Benchmark","abstract":"Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video. To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding. This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data. Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene. Additionally, we evaluate recent video-centric LLMs, both open-source and proprietary, on the test split of our dataset. The findings reveal that even state-of-the-art video-centric LLMs significantly lag behind human performance in these tasks, highlighting the complexity and challenge inherent in video understanding. The dataset is available at https://hf.co/datasets/tomg-group-umd/cinepile","sentences":["Current datasets for long-form video understanding often fall short of providing genuine long-form comprehension challenges, as many tasks derived from these datasets can be successfully tackled by analyzing just one or a few random frames from a video.","To address this issue, we present a novel dataset and benchmark, CinePile, specifically designed for authentic long-form video understanding.","This paper details our innovative approach for creating a question-answer dataset, utilizing advanced LLMs with human-in-the-loop and building upon human-generated raw data.","Our comprehensive dataset comprises 305,000 multiple-choice questions (MCQs), covering various visual and multimodal aspects, including temporal comprehension, understanding human-object interactions, and reasoning about events or actions within a scene.","Additionally, we evaluate recent video-centric LLMs, both open-source and proprietary, on the test split of our dataset.","The findings reveal that even state-of-the-art video-centric LLMs significantly lag behind human performance in these tasks, highlighting the complexity and challenge inherent in video understanding.","The dataset is available at https://hf.co/datasets/tomg-group-umd/cinepile"],"url":"http://arxiv.org/abs/2405.08813v1","category":"cs.CV"}
{"created":"2024-05-14 17:54:54","title":"Mock-local energy density of gravitational waves","abstract":"We propose a new set of BMS charges at null infinity, characterized by a super-translation flux that contains only the `hard' term. This is achieved with a specific corner improvement of the symplectic 2-form, and we spell the conditions under which it is unique. The charges are associated to a Wald-Zoupas symplectic potential, and satisfy all standard criteria: they are covariant, provide a center-less realization of the symmetry algebra, have vanishing flux in non-radiative spacetimes, and vanish in Minkowski. We use them to define a certain notion of localized energy density of gravitational waves. They have potential applications to the generalized second law and to soft theorems.","sentences":["We propose a new set of BMS charges at null infinity, characterized by a super-translation flux that contains only the `hard' term.","This is achieved with a specific corner improvement of the symplectic 2-form, and we spell the conditions under which it is unique.","The charges are associated to a Wald-Zoupas symplectic potential, and satisfy all standard criteria: they are covariant, provide a center-less realization of the symmetry algebra, have vanishing flux in non-radiative spacetimes, and vanish in Minkowski.","We use them to define a certain notion of localized energy density of gravitational waves.","They have potential applications to the generalized second law and to soft theorems."],"url":"http://arxiv.org/abs/2405.08808v1","category":"gr-qc"}
{"created":"2024-05-14 17:51:34","title":"Photon Ring Interferometric Signatures Beyond The Universal Regime","abstract":"We calculate the interferometric signatures of black hole photon rings beyond the universal regime by perturbatively including the effects of finite ring width. Our approach first slices a thick ring into a series of thin rings, each of which falls within the universal regime. We thus calculate the visibility of the thick ring by aggregating the contributions from each thin ring, and then perturbatively expand the result into polynomials of the baseline length $u$. We show that the visibility amplitude of a thick ring depends on its \"center-of-light\" diameter; it also includes additional higher-order corrections due to the width of the ring, with the leading correction terms proportional to $u^2$ for the envelope and $u^3$ for the phase. We apply our method to images ray traced from general-relativistic magnetohydrodynamic (GRMHD) simulations and demonstrate that incorporating the higher-order corrections is crucial for accurately modeling the visibility of the first photon ring around M87*.","sentences":["We calculate the interferometric signatures of black hole photon rings beyond the universal regime by perturbatively including the effects of finite ring width.","Our approach first slices a thick ring into a series of thin rings, each of which falls within the universal regime.","We thus calculate the visibility of the thick ring by aggregating the contributions from each thin ring, and then perturbatively expand the result into polynomials of the baseline length $u$. We show that the visibility amplitude of a thick ring depends on its \"center-of-light\" diameter; it also includes additional higher-order corrections due to the width of the ring, with the leading correction terms proportional to $u^2$ for the envelope and $u^3$ for the phase.","We apply our method to images ray traced from general-relativistic magnetohydrodynamic (GRMHD) simulations and demonstrate that incorporating the higher-order corrections is crucial for accurately modeling the visibility of the first photon ring around M87*."],"url":"http://arxiv.org/abs/2405.08804v1","category":"astro-ph.HE"}
{"created":"2024-05-14 17:48:44","title":"Estimation of Participation Factors for Power System Oscillation from Measurements","abstract":"In a power system, when the participation factors of generators are computed to rank their participations into an oscillatory mode, a model-based approach is conventionally used on the linearized system model by means of the corresponding right and left eigenvectors. This paper proposes a new approach for estimating participation factors directly from measurement data on generator responses under selected disturbances. The approach computes extended participation factors that coincide with accurate model-based participation factors when the measured responses satisfy an ideally symmetric condition. This paper relaxes this symmetric condition with the original measurement space by identifying and utilizing a coordinate transformation to a new space optimally recovering the symmetry. Thus, the optimal estimates of participation factors solely from measurements are achieved, and the accuracy and influencing factors are discussed. The proposed approach is first demonstrated in detail on a two-area system and then tested on an NPCC 48-machine power system. The penetration of inverter-based resources is also considered.","sentences":["In a power system, when the participation factors of generators are computed to rank their participations into an oscillatory mode, a model-based approach is conventionally used on the linearized system model by means of the corresponding right and left eigenvectors.","This paper proposes a new approach for estimating participation factors directly from measurement data on generator responses under selected disturbances.","The approach computes extended participation factors that coincide with accurate model-based participation factors when the measured responses satisfy an ideally symmetric condition.","This paper relaxes this symmetric condition with the original measurement space by identifying and utilizing a coordinate transformation to a new space optimally recovering the symmetry.","Thus, the optimal estimates of participation factors solely from measurements are achieved, and the accuracy and influencing factors are discussed.","The proposed approach is first demonstrated in detail on a two-area system and then tested on an NPCC 48-machine power system.","The penetration of inverter-based resources is also considered."],"url":"http://arxiv.org/abs/2405.08800v1","category":"eess.SY"}
{"created":"2024-05-14 17:48:08","title":"The Flux Hypothesis for Odd Transport Phenomena","abstract":"Onsager's regression hypothesis makes a fundamental connection between macroscopic transport phenomena and the average relaxation of spontaneous microscopic fluctuations. This relaxation, however, is agnostic to odd transport phenomena, in which fluxes run orthogonal to the gradients driving them. To account for odd transport, we generalize the regression hypothesis, postulating that macroscopic linear constitutive laws are, on average, obeyed by microscopic fluctuations, whether they contribute to relaxation or not. From this \"flux hypothesis,\" Green-Kubo and reciprocal relations follow, elucidating the separate roles of broken time-reversal and parity symmetries underlying various odd transport coefficients. As an application, we derive and verify the Green-Kubo relation for odd collective diffusion in chiral active matter, first in an analytically-tractable model and subsequently through molecular dynamics simulations of concentrated active spinners.","sentences":["Onsager's regression hypothesis makes a fundamental connection between macroscopic transport phenomena and the average relaxation of spontaneous microscopic fluctuations.","This relaxation, however, is agnostic to odd transport phenomena, in which fluxes run orthogonal to the gradients driving them.","To account for odd transport, we generalize the regression hypothesis, postulating that macroscopic linear constitutive laws are, on average, obeyed by microscopic fluctuations, whether they contribute to relaxation or not.","From this \"flux hypothesis,\" Green-Kubo and reciprocal relations follow, elucidating the separate roles of broken time-reversal and parity symmetries underlying various odd transport coefficients.","As an application, we derive and verify the Green-Kubo relation for odd collective diffusion in chiral active matter, first in an analytically-tractable model and subsequently through molecular dynamics simulations of concentrated active spinners."],"url":"http://arxiv.org/abs/2405.08798v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-14 17:41:55","title":"A Brief Introduction to Causal Inference in Machine Learning","abstract":"This is a lecture note produced for DS-GA 3001.003 \"Special Topics in DS - Causal Inference in Machine Learning\" at the Center for Data Science, New York University in Spring, 2024. This course was created to target master's and PhD level students with basic background in machine learning but who were not exposed to causal inference or causal reasoning in general previously. In particular, this course focuses on introducing such students to expand their view and knowledge of machine learning to incorporate causal reasoning, as this aspect is at the core of so-called out-of-distribution generalization (or lack thereof.)","sentences":["This is a lecture note produced for DS-GA 3001.003 \"Special Topics in DS - Causal Inference in Machine Learning\" at the Center for Data Science, New York University in Spring, 2024.","This course was created to target master's and PhD level students with basic background in machine learning but who were not exposed to causal inference or causal reasoning in general previously.","In particular, this course focuses on introducing such students to expand their view and knowledge of machine learning to incorporate causal reasoning, as this aspect is at the core of so-called out-of-distribution generalization (or lack thereof.)"],"url":"http://arxiv.org/abs/2405.08793v1","category":"cs.LG"}
{"created":"2024-05-14 17:41:07","title":"Towards Enhanced RAC Accessibility: Leveraging Datasets and LLMs","abstract":"This paper explores the potential of large language models (LLMs) to make the Aeronautical Regulations of Colombia (RAC) more accessible. Given the complexity and extensive technicality of the RAC, this study introduces a novel approach to simplifying these regulations for broader understanding. By developing the first-ever RAC database, which contains 24,478 expertly labeled question-and-answer pairs, and fine-tuning LLMs specifically for RAC applications, the paper outlines the methodology for dataset assembly, expert-led annotation, and model training. Utilizing the Gemma1.1 2b model along with advanced techniques like Unsloth for efficient VRAM usage and flash attention mechanisms, the research aims to expedite training processes. This initiative establishes a foundation to enhance the comprehensibility and accessibility of RAC, potentially benefiting novices and reducing dependence on expert consultations for navigating the aviation industry's regulatory landscape.   You can visit the dataset (https://huggingface.co/somosnlp/gemma-1.1-2b-it_ColombiaRAC_FullyCurated_format_chatML_V1) and the model (https://huggingface.co/datasets/somosnlp/ColombiaRAC_FullyCurated) here.","sentences":["This paper explores the potential of large language models (LLMs) to make the Aeronautical Regulations of Colombia (RAC) more accessible.","Given the complexity and extensive technicality of the RAC, this study introduces a novel approach to simplifying these regulations for broader understanding.","By developing the first-ever RAC database, which contains 24,478 expertly labeled question-and-answer pairs, and fine-tuning LLMs specifically for RAC applications, the paper outlines the methodology for dataset assembly, expert-led annotation, and model training.","Utilizing the Gemma1.1 2b model along with advanced techniques like Unsloth for efficient VRAM usage and flash attention mechanisms, the research aims to expedite training processes.","This initiative establishes a foundation to enhance the comprehensibility and accessibility of RAC, potentially benefiting novices and reducing dependence on expert consultations for navigating the aviation industry's regulatory landscape.   ","You can visit the dataset (https://huggingface.co/somosnlp/gemma-1.1-2b-it_ColombiaRAC_FullyCurated_format_chatML_V1) and the model (https://huggingface.co/datasets/somosnlp/ColombiaRAC_FullyCurated) here."],"url":"http://arxiv.org/abs/2405.08792v1","category":"cs.LG"}
{"created":"2024-05-14 17:39:47","title":"On the basin of attraction of a critical three-cycle of a model for the secant map","abstract":"We consider the secant method $S_p$ applied to a real polynomial $p$ of degree $d+1$ as a discrete dynamical system on $\\mathbb R^2$. If the polynomial $p$ has a local extremum at a point $\\alpha$ then the discrete dynamical system generated by the iterates of the secant map exhibits a critical periodic orbit of period 3 or three-cycle at the point $(\\alpha,\\alpha)$. We propose a simple model map $T_{a,d}$ having a unique fixed point at the origin which encodes the dynamical behaviour of $S_p^3$ at the critical three-cycle. The main goal of the paper is to describe the geometry and topology of the basin of attraction of the origin of $T_{a,d}$ as well as its boundary. Our results concern global, rather than local, dynamical behaviour. They include that the boundary of the basin of attraction is the stable manifold of a fixed point or contains the stable manifold of a two-cycle, depending on the values of the parameters of $d$ (even or odd) and $a\\in \\mathbb R$ (positive or negative).","sentences":["We consider the secant method $S_p$ applied to a real polynomial $p$ of degree $d+1$ as a discrete dynamical system on $\\mathbb R^2$.","If the polynomial $p$ has a local extremum at a point $\\alpha$ then the discrete dynamical system generated by the iterates of the secant map exhibits a critical periodic orbit of period 3 or three-cycle at the point $(\\alpha,\\alpha)$. We propose a simple model map $T_{a,d}$ having a unique fixed point at the origin which encodes the dynamical behaviour of $S_p^3$ at the critical three-cycle.","The main goal of the paper is to describe the geometry and topology of the basin of attraction of the origin of $T_{a,d}$ as well as its boundary.","Our results concern global, rather than local, dynamical behaviour.","They include that the boundary of the basin of attraction is the stable manifold of a fixed point or contains the stable manifold of a two-cycle, depending on the values of the parameters of $d$ (even or odd) and $a\\in \\mathbb R$ (positive or negative)."],"url":"http://arxiv.org/abs/2405.08791v1","category":"math.DS"}
{"created":"2024-05-14 17:38:17","title":"Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","abstract":"This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling. Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically. We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters. We also provide an ablation study of KAN-specific parameters impact on performance. The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics.","sentences":["This paper introduces a novel application of Kolmogorov-Arnold Networks (KANs) to time series forecasting, leveraging their adaptive activation functions for enhanced predictive modeling.","Inspired by the Kolmogorov-Arnold representation theorem, KANs replace traditional linear weights with spline-parametrized univariate functions, allowing them to learn activation patterns dynamically.","We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters.","We also provide an ablation study of KAN-specific parameters impact on performance.","The proposed approach opens new avenues for adaptive forecasting models, emphasizing the potential of KANs as a powerful tool in predictive analytics."],"url":"http://arxiv.org/abs/2405.08790v1","category":"eess.SP"}
{"created":"2024-05-14 17:36:22","title":"Explicit Orthogonal Arrays and Universal Hashing with Arbitrary Parameters","abstract":"Orthogonal arrays are a type of combinatorial design that were developed in the 1940s in the design of statistical experiments. In 1947, Rao proved a lower bound on the size of any orthogonal array, and raised the problem of constructing arrays of minimum size. Kuperberg, Lovett and Peled (2017) gave a non-constructive existence proof of orthogonal arrays whose size is near-optimal (i.e., within a polynomial of Rao's lower bound), leaving open the question of an algorithmic construction. We give the first explicit, deterministic, algorithmic construction of orthogonal arrays achieving near-optimal size for all parameters. Our construction uses algebraic geometry codes.   In pseudorandomness, the notions of $t$-independent generators or $t$-independent hash functions are equivalent to orthogonal arrays. Classical constructions of $t$-independent hash functions are known when the size of the codomain is a prime power, but very few constructions are known for an arbitrary codomain. Our construction yields algorithmically efficient $t$-independent hash functions for arbitrary domain and codomain.","sentences":["Orthogonal arrays are a type of combinatorial design that were developed in the 1940s in the design of statistical experiments.","In 1947, Rao proved a lower bound on the size of any orthogonal array, and raised the problem of constructing arrays of minimum size.","Kuperberg, Lovett and Peled (2017) gave a non-constructive existence proof of orthogonal arrays whose size is near-optimal (i.e., within a polynomial of Rao's lower bound), leaving open the question of an algorithmic construction.","We give the first explicit, deterministic, algorithmic construction of orthogonal arrays achieving near-optimal size for all parameters.","Our construction uses algebraic geometry codes.   ","In pseudorandomness, the notions of $t$-independent generators or $t$-independent hash functions are equivalent to orthogonal arrays.","Classical constructions of $t$-independent hash functions are known when the size of the codomain is a prime power, but very few constructions are known for an arbitrary codomain.","Our construction yields algorithmically efficient $t$-independent hash functions for arbitrary domain and codomain."],"url":"http://arxiv.org/abs/2405.08787v1","category":"cs.DS"}
{"created":"2024-05-14 17:35:27","title":"Incorporating Clinical Guidelines through Adapting Multi-modal Large Language Model for Prostate Cancer PI-RADS Scoring","abstract":"The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in the diagnosis of clinically significant prostate cancer through MRI imaging. Current deep learning-based PI-RADS scoring methods often lack the incorporation of essential PI-RADS clinical guidelines~(PICG) utilized by radiologists, potentially compromising scoring accuracy. This paper introduces a novel approach that adapts a multi-modal large language model (MLLM) to incorporate PICG into PI-RADS scoring without additional annotations and network parameters. We present a two-stage fine-tuning process aimed at adapting MLLMs originally trained on natural images to the MRI data domain while effectively integrating the PICG. In the first stage, we develop a domain adapter layer specifically tailored for processing 3D MRI image inputs and design the MLLM instructions to differentiate MRI modalities effectively. In the second stage, we translate PICG into guiding instructions for the model to generate PICG-guided image features. Through feature distillation, we align scoring network features with the PICG-guided image feature, enabling the scoring network to effectively incorporate the PICG information. We develop our model on a public dataset and evaluate it in a real-world challenging in-house dataset. Experimental results demonstrate that our approach improves the performance of current scoring networks.","sentences":["The Prostate Imaging Reporting and Data System (PI-RADS) is pivotal in the diagnosis of clinically significant prostate cancer through MRI imaging.","Current deep learning-based PI-RADS scoring methods often lack the incorporation of essential PI-RADS clinical guidelines~(PICG) utilized by radiologists, potentially compromising scoring accuracy.","This paper introduces a novel approach that adapts a multi-modal large language model (MLLM) to incorporate PICG into PI-RADS scoring without additional annotations and network parameters.","We present a two-stage fine-tuning process aimed at adapting MLLMs originally trained on natural images to the MRI data domain while effectively integrating the PICG.","In the first stage, we develop a domain adapter layer specifically tailored for processing 3D MRI image inputs and design the MLLM instructions to differentiate MRI modalities effectively.","In the second stage, we translate PICG into guiding instructions for the model to generate PICG-guided image features.","Through feature distillation, we align scoring network features with the PICG-guided image feature, enabling the scoring network to effectively incorporate the PICG information.","We develop our model on a public dataset and evaluate it in a real-world challenging in-house dataset.","Experimental results demonstrate that our approach improves the performance of current scoring networks."],"url":"http://arxiv.org/abs/2405.08786v1","category":"cs.CV"}
{"created":"2024-05-14 17:15:28","title":"Harnessing the power of longitudinal medical imaging for eye disease prognosis using Transformer-based sequence modeling","abstract":"Deep learning has enabled breakthroughs in automated diagnosis from medical imaging, with many successful applications in ophthalmology. However, standard medical image classification approaches only assess disease presence at the time of acquisition, neglecting the common clinical setting of longitudinal imaging. For slow, progressive eye diseases like age-related macular degeneration (AMD) and primary open-angle glaucoma (POAG), patients undergo repeated imaging over time to track disease progression and forecasting the future risk of developing disease is critical to properly plan treatment. Our proposed Longitudinal Transformer for Survival Analysis (LTSA) enables dynamic disease prognosis from longitudinal medical imaging, modeling the time to disease from sequences of fundus photography images captured over long, irregular time periods. Using longitudinal imaging data from the Age-Related Eye Disease Study (AREDS) and Ocular Hypertension Treatment Study (OHTS), LTSA significantly outperformed a single-image baseline in 19/20 head-to-head comparisons on late AMD prognosis and 18/20 comparisons on POAG prognosis. A temporal attention analysis also suggested that, while the most recent image is typically the most influential, prior imaging still provides additional prognostic value.","sentences":["Deep learning has enabled breakthroughs in automated diagnosis from medical imaging, with many successful applications in ophthalmology.","However, standard medical image classification approaches only assess disease presence at the time of acquisition, neglecting the common clinical setting of longitudinal imaging.","For slow, progressive eye diseases like age-related macular degeneration (AMD) and primary open-angle glaucoma (POAG), patients undergo repeated imaging over time to track disease progression and forecasting the future risk of developing disease is critical to properly plan treatment.","Our proposed Longitudinal Transformer for Survival Analysis (LTSA) enables dynamic disease prognosis from longitudinal medical imaging, modeling the time to disease from sequences of fundus photography images captured over long, irregular time periods.","Using longitudinal imaging data from the Age-Related Eye Disease Study (AREDS) and Ocular Hypertension Treatment Study (OHTS), LTSA significantly outperformed a single-image baseline in 19/20 head-to-head comparisons on late AMD prognosis and 18/20 comparisons on POAG prognosis.","A temporal attention analysis also suggested that, while the most recent image is typically the most influential, prior imaging still provides additional prognostic value."],"url":"http://arxiv.org/abs/2405.08780v1","category":"cs.CV"}
{"created":"2024-05-14 17:11:33","title":"FolkTalent: Enhancing Classification and Tagging of Indian Folk Paintings","abstract":"Indian folk paintings have a rich mosaic of symbols, colors, textures, and stories making them an invaluable repository of cultural legacy. The paper presents a novel approach to classifying these paintings into distinct art forms and tagging them with their unique salient features. A custom dataset named FolkTalent, comprising 2279 digital images of paintings across 12 different forms, has been prepared using websites that are direct outlets of Indian folk paintings. Tags covering a wide range of attributes like color, theme, artistic style, and patterns are generated using GPT4, and verified by an expert for each painting. Classification is performed employing the RandomForest ensemble technique on fine-tuned Convolutional Neural Network (CNN) models to classify Indian folk paintings, achieving an accuracy of 91.83%. Tagging is accomplished via the prominent fine-tuned CNN-based backbones with a custom classifier attached to its top to perform multi-label image classification. The generated tags offer a deeper insight into the painting, enabling an enhanced search experience based on theme and visual attributes. The proposed hybrid model sets a new benchmark in folk painting classification and tagging, significantly contributing to cataloging India's folk-art heritage.","sentences":["Indian folk paintings have a rich mosaic of symbols, colors, textures, and stories making them an invaluable repository of cultural legacy.","The paper presents a novel approach to classifying these paintings into distinct art forms and tagging them with their unique salient features.","A custom dataset named FolkTalent, comprising 2279 digital images of paintings across 12 different forms, has been prepared using websites that are direct outlets of Indian folk paintings.","Tags covering a wide range of attributes like color, theme, artistic style, and patterns are generated using GPT4, and verified by an expert for each painting.","Classification is performed employing the RandomForest ensemble technique on fine-tuned Convolutional Neural Network (CNN) models to classify Indian folk paintings, achieving an accuracy of 91.83%.","Tagging is accomplished via the prominent fine-tuned CNN-based backbones with a custom classifier attached to its top to perform multi-label image classification.","The generated tags offer a deeper insight into the painting, enabling an enhanced search experience based on theme and visual attributes.","The proposed hybrid model sets a new benchmark in folk painting classification and tagging, significantly contributing to cataloging India's folk-art heritage."],"url":"http://arxiv.org/abs/2405.08776v1","category":"cs.CV"}
{"created":"2024-05-14 17:08:55","title":"Generating 4-dimensional Wormholes with Yang-Mills Casimir Sources","abstract":"This work presents a new wormhole solution in General Relativity supported by the quantum vacuum fluctuations of the Casimir effect between perfect chromometallic mirrors in $(3+1)$ dimensions, which was recently fitted using first-principle numerical simulations. Initially, we employ a perturbative approach for $x = m r \\ll 1$, where $m$ represents the Casimir mass. This approach has proven to be a reasonable approximation when compared with the exact case in this regime. To find well-behaved redshift functions, we impose constraints on the free parameters. As expected, this solution recovers the electromagnetic-like Casimir solution for $m = 0$. Analyzing the traversability conditions, we graphically find that all will be satisfied for $ 0 \\leq m \\leq 0.20$. On the other hand, all the energy conditions are violated, as usual in this context. Stability from Tolman-Oppenheimer-Volkov (TOV) equation is guaranteed for all $r$ and from the speed of sound for $0.16 \\le m \\le 0.18$. Therefore, for $0.16 \\leq m \\leq 0.18$, we will have a stable solution that satisfies all traversability conditions.","sentences":["This work presents a new wormhole solution in General Relativity supported by the quantum vacuum fluctuations of the Casimir effect between perfect chromometallic mirrors in $(3+1)$ dimensions, which was recently fitted using first-principle numerical simulations.","Initially, we employ a perturbative approach for $x = m r \\ll 1$, where $m$ represents the Casimir mass.","This approach has proven to be a reasonable approximation when compared with the exact case in this regime.","To find well-behaved redshift functions, we impose constraints on the free parameters.","As expected, this solution recovers the electromagnetic-like Casimir solution for $m = 0$.","Analyzing the traversability conditions, we graphically find that all will be satisfied for $ 0 \\leq","m \\leq 0.20$.","On the other hand, all the energy conditions are violated, as usual in this context.","Stability from Tolman-Oppenheimer-Volkov (TOV) equation is guaranteed for all $r$ and from the speed of sound for $0.16 \\le m \\le 0.18$.","Therefore, for $0.16 \\leq m \\leq 0.18$, we will have a stable solution that satisfies all traversability conditions."],"url":"http://arxiv.org/abs/2405.08774v1","category":"gr-qc"}
{"created":"2024-05-14 17:06:49","title":"Direct bounds on Left-Right gauge boson masses","abstract":"While the third run of the Large Hadron Collider (LHC) is ongoing, the underlying theory that extends the Standard Model remains so far unknown. Left-Right Models (LRMs) introduce a new gauge sector, and can restore parity symmetry at high enough energies. If LRMs are indeed realized in nature, the mediators of the new weak force can be searched for in colliders via their direct production. We recast existing experimental limits from the LHC Run 2 and derive generic bounds on the masses of the heavy LRM gauge bosons. As a novelty, we discuss the dependence of the $W_R$ and $Z_R$ total decay width on the LRM scalar content, obtaining model-independent bounds within the specific realizations of the LRM scalar sectors analysed here. These bounds avoid the need to detail the spectrum of the scalar sector, and apply in the general case where no discrete symmetry is enforced. Moreover, we emphasize the impact on the $W_R$ production at LHC of general textures of the right-handed quark mixing matrix without manifest left-right symmetry. We find that the $W_R$ and $Z_R$ masses are constrained to lie above $2$ TeV and $4$ TeV, respectively.","sentences":["While the third run of the Large Hadron Collider (LHC) is ongoing, the underlying theory that extends the Standard Model remains so far unknown.","Left-Right Models (LRMs) introduce a new gauge sector, and can restore parity symmetry at high enough energies.","If LRMs are indeed realized in nature, the mediators of the new weak force can be searched for in colliders via their direct production.","We recast existing experimental limits from the LHC Run 2 and derive generic bounds on the masses of the heavy LRM gauge bosons.","As a novelty, we discuss the dependence of the $W_R$ and $Z_R$ total decay width on the LRM scalar content, obtaining model-independent bounds within the specific realizations of the LRM scalar sectors analysed here.","These bounds avoid the need to detail the spectrum of the scalar sector, and apply in the general case where no discrete symmetry is enforced.","Moreover, we emphasize the impact on the $W_R$ production at LHC of general textures of the right-handed quark mixing matrix without manifest left-right symmetry.","We find that the $W_R$ and $Z_R$ masses are constrained to lie above $2$ TeV and $4$ TeV, respectively."],"url":"http://arxiv.org/abs/2405.08772v1","category":"hep-ph"}
{"created":"2024-05-14 17:02:10","title":"Scalar Field Perturbation of Hairy Black Holes in EsGB theory","abstract":"We investigate scalar field perturbations of the hairy black holes involved with spontaneous symmetry breaking of the global U(1) symmetry in Einstein-scalar-Gauss-Bonnet theory for asymptotically flat spacetimes. We consider the mechanism that black holes without hairs become unstable at the critical point of the coupling constant and undergo a phase transition to hairy black holes in the symmetry-broken phase driven by spontaneous symmetry breaking. This transition occurs near the black hole horizon due to the diminishing influence of the Gauss-Bonnet term at infinity. To examine such process, we introduce a scalar field perturbation on the newly formed background spacetime. We solve the linearized perturbation equation using Green's function method. We begin by solving the Green's function, incorporating the branch cut contribution. This allows us to analytically investigate the late-time behavior of the perturbation at both spatial and null infinity. We found that the late-time behavior only differs from the Schwarzschild black hole by a mass term. We then proceed to calculate the quasinormal modes (QNMs) numerically, which arise from the presence of poles in the Green's function. Our primary interest lies in utilizing QNMs to investigate the stability of the black hole solutions both the symmetric and symmetry-broken phases. Consistent with the prior study, our analysis shows that hairy black holes in the symmetric phase become unstable when the quadratic coupling constant exceeds a critical value for a fixed value of the quartic coupling constant. In contrast, hairy black holes in the symmetry-broken phase are always stable at the critical value. These numerical results provide strong evidence for a dynamical process that unstable black holes without hairs transition into stable hairy black holes in the symmetry-broken phase through the spontaneous symmetry breaking.","sentences":["We investigate scalar field perturbations of the hairy black holes involved with spontaneous symmetry breaking of the global U(1) symmetry in Einstein-scalar-Gauss-Bonnet theory for asymptotically flat spacetimes.","We consider the mechanism that black holes without hairs become unstable at the critical point of the coupling constant and undergo a phase transition to hairy black holes in the symmetry-broken phase driven by spontaneous symmetry breaking.","This transition occurs near the black hole horizon due to the diminishing influence of the Gauss-Bonnet term at infinity.","To examine such process, we introduce a scalar field perturbation on the newly formed background spacetime.","We solve the linearized perturbation equation using Green's function method.","We begin by solving the Green's function, incorporating the branch cut contribution.","This allows us to analytically investigate the late-time behavior of the perturbation at both spatial and null infinity.","We found that the late-time behavior only differs from the Schwarzschild black hole by a mass term.","We then proceed to calculate the quasinormal modes (QNMs) numerically, which arise from the presence of poles in the Green's function.","Our primary interest lies in utilizing QNMs to investigate the stability of the black hole solutions both the symmetric and symmetry-broken phases.","Consistent with the prior study, our analysis shows that hairy black holes in the symmetric phase become unstable when the quadratic coupling constant exceeds a critical value for a fixed value of the quartic coupling constant.","In contrast, hairy black holes in the symmetry-broken phase are always stable at the critical value.","These numerical results provide strong evidence for a dynamical process that unstable black holes without hairs transition into stable hairy black holes in the symmetry-broken phase through the spontaneous symmetry breaking."],"url":"http://arxiv.org/abs/2405.08769v1","category":"hep-th"}
{"created":"2024-05-14 17:00:43","title":"EfficientTrain++: Generalized Curriculum Learning for Efficient Visual Backbone Training","abstract":"The superior performance of modern visual backbones usually comes with a costly training procedure. We contribute to this issue by generalizing the idea of curriculum learning beyond its original formulation, i.e., training models using easier-to-harder data. Specifically, we reformulate the training curriculum as a soft-selection function, which uncovers progressively more difficult patterns within each example during training, instead of performing easier-to-harder sample selection. Our work is inspired by an intriguing observation on the learning dynamics of visual backbones: during the earlier stages of training, the model predominantly learns to recognize some 'easier-to-learn' discriminative patterns in the data. These patterns, when observed through frequency and spatial domains, incorporate lower-frequency components, and the natural image contents without distortion or data augmentation. Motivated by these findings, we propose a curriculum where the model always leverages all the training data at every learning stage, yet the exposure to the 'easier-to-learn' patterns of each example is initiated first, with harder patterns gradually introduced as training progresses. To implement this idea in a computationally efficient way, we introduce a cropping operation in the Fourier spectrum of the inputs, enabling the model to learn from only the lower-frequency components. Then we show that exposing the contents of natural images can be readily achieved by modulating the intensity of data augmentation. Finally, we integrate these aspects and design curriculum schedules with tailored search algorithms. The resulting method, EfficientTrain++, is simple, general, yet surprisingly effective. It reduces the training time of a wide variety of popular models by 1.5-3.0x on ImageNet-1K/22K without sacrificing accuracy. It also demonstrates efficacy in self-supervised learning (e.g., MAE).","sentences":["The superior performance of modern visual backbones usually comes with a costly training procedure.","We contribute to this issue by generalizing the idea of curriculum learning beyond its original formulation, i.e., training models using easier-to-harder data.","Specifically, we reformulate the training curriculum as a soft-selection function, which uncovers progressively more difficult patterns within each example during training, instead of performing easier-to-harder sample selection.","Our work is inspired by an intriguing observation on the learning dynamics of visual backbones: during the earlier stages of training, the model predominantly learns to recognize some 'easier-to-learn' discriminative patterns in the data.","These patterns, when observed through frequency and spatial domains, incorporate lower-frequency components, and the natural image contents without distortion or data augmentation.","Motivated by these findings, we propose a curriculum where the model always leverages all the training data at every learning stage, yet the exposure to the 'easier-to-learn' patterns of each example is initiated first, with harder patterns gradually introduced as training progresses.","To implement this idea in a computationally efficient way, we introduce a cropping operation in the Fourier spectrum of the inputs, enabling the model to learn from only the lower-frequency components.","Then we show that exposing the contents of natural images can be readily achieved by modulating the intensity of data augmentation.","Finally, we integrate these aspects and design curriculum schedules with tailored search algorithms.","The resulting method, EfficientTrain++, is simple, general, yet surprisingly effective.","It reduces the training time of a wide variety of popular models by 1.5-3.0x on ImageNet-1K/22K without sacrificing accuracy.","It also demonstrates efficacy in self-supervised learning (e.g., MAE)."],"url":"http://arxiv.org/abs/2405.08768v1","category":"cs.CV"}
{"created":"2024-05-14 16:58:37","title":"Image to Pseudo-Episode: Boosting Few-Shot Segmentation by Unlabeled Data","abstract":"Few-shot segmentation (FSS) aims to train a model which can segment the object from novel classes with a few labeled samples. The insufficient generalization ability of models leads to unsatisfactory performance when the models lack enough labeled data from the novel classes. Considering that there are abundant unlabeled data available, it is promising to improve the generalization ability by exploiting these various data. For leveraging unlabeled data, we propose a novel method, named Image to Pseudo-Episode (IPE), to generate pseudo-episodes from unlabeled data. Specifically, our method contains two modules, i.e., the pseudo-label generation module and the episode generation module. The former module generates pseudo-labels from unlabeled images by the spectral clustering algorithm, and the latter module generates pseudo-episodes from pseudo-labeled images by data augmentation methods. Extensive experiments on PASCAL-$5^i$ and COCO-$20^i$ demonstrate that our method achieves the state-of-the-art performance for FSS.","sentences":["Few-shot segmentation (FSS) aims to train a model which can segment the object from novel classes with a few labeled samples.","The insufficient generalization ability of models leads to unsatisfactory performance when the models lack enough labeled data from the novel classes.","Considering that there are abundant unlabeled data available, it is promising to improve the generalization ability by exploiting these various data.","For leveraging unlabeled data, we propose a novel method, named Image to Pseudo-Episode (IPE), to generate pseudo-episodes from unlabeled data.","Specifically, our method contains two modules, i.e., the pseudo-label generation module and the episode generation module.","The former module generates pseudo-labels from unlabeled images by the spectral clustering algorithm, and the latter module generates pseudo-episodes from pseudo-labeled images by data augmentation methods.","Extensive experiments on PASCAL-$5^i$ and COCO-$20^i$ demonstrate that our method achieves the state-of-the-art performance for FSS."],"url":"http://arxiv.org/abs/2405.08765v1","category":"cs.CV"}
{"created":"2024-05-14 16:56:49","title":"A Generalized Curvilinear Coordinate system-based Patch Dynamics Scheme in Equation-free Multiscale Modelling","abstract":"The patch dynamics scheme in equation-free multiscale modelling can efficiently predict the macroscopic behaviours by simulating the microscale problem in a fraction of the space-time domain. The patch dynamics schemes developed so far, are mainly on rectangular domains with uniform grids and uniform rectangular patches. In real-life problems where the geometry of the domain is not regular or simple, rectangular and uniform grids or patches may not be useful. To address this kind of complexity, the concept of a generalized curvilinear coordinate system is used. An explicit representation of a patch dynamics scheme on a generalized curvilinear coordinate system in a two-dimensional domain is proposed for evolution equations. It has been applied to unsteady convection-diffusion-reaction (CDR) problems. The robustness of the scheme on the generalized curvilinear coordinate system is assessed through numerical test cases. Firstly, a convection-dominated CDR equation is considered, featuring high gradient regions in some part of the domain, for which stretched grids with non-uniform patch sizes are employed. Secondly, a non-axisymmetric diffusion equation is examined in an annulus region, where the patches have non-rectangular shapes. The results obtained demonstrate excellent agreement with the analytical solution or existing numerical solutions.","sentences":["The patch dynamics scheme in equation-free multiscale modelling can efficiently predict the macroscopic behaviours by simulating the microscale problem in a fraction of the space-time domain.","The patch dynamics schemes developed so far, are mainly on rectangular domains with uniform grids and uniform rectangular patches.","In real-life problems where the geometry of the domain is not regular or simple, rectangular and uniform grids or patches may not be useful.","To address this kind of complexity, the concept of a generalized curvilinear coordinate system is used.","An explicit representation of a patch dynamics scheme on a generalized curvilinear coordinate system in a two-dimensional domain is proposed for evolution equations.","It has been applied to unsteady convection-diffusion-reaction (CDR) problems.","The robustness of the scheme on the generalized curvilinear coordinate system is assessed through numerical test cases.","Firstly, a convection-dominated CDR equation is considered, featuring high gradient regions in some part of the domain, for which stretched grids with non-uniform patch sizes are employed.","Secondly, a non-axisymmetric diffusion equation is examined in an annulus region, where the patches have non-rectangular shapes.","The results obtained demonstrate excellent agreement with the analytical solution or existing numerical solutions."],"url":"http://arxiv.org/abs/2405.08764v1","category":"math.NA"}
{"created":"2024-05-14 16:55:15","title":"Genus, Fiberedness, $\u03c4$ and $\u03b5$ of Satellite Knots with $n$-Twisted Generalized Mazur patterns","abstract":"We study a family of $(1,1)$-pattern knots that generalize the Mazur pattern, and compute the concordance invariants $\\tau$ and $\\epsilon$ of $n$-twisted satellites formed from these patterns. We show that none of the $n$-twisted patterns from this family act surjectively on the smooth or rational concordance group. We also determine when the $n$-twisted generalized Mazur patterns are fibered in the solid torus, compute their genus in $S^1 \\times D^2$, and show that $n$-twisted satellites with generalized Mazur patterns and non-trivial companions are not Floer thin.","sentences":["We study a family of $(1,1)$-pattern knots that generalize the Mazur pattern, and compute the concordance invariants $\\tau$ and $\\epsilon$ of $n$-twisted satellites formed from these patterns.","We show that none of the $n$-twisted patterns from this family act surjectively on the smooth or rational concordance group.","We also determine when the $n$-twisted generalized Mazur patterns are fibered in the solid torus, compute their genus in $S^1 \\times D^2$, and show that $n$-twisted satellites with generalized Mazur patterns and non-trivial companions are not Floer thin."],"url":"http://arxiv.org/abs/2405.08763v1","category":"math.GT"}
{"created":"2024-05-14 16:48:56","title":"Is the Pope Catholic? Yes, the Pope is Catholic. Generative Evaluation of Intent Resolution in LLMs","abstract":"Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors -- human or AI -- to understand beyond the literal meaning of words. While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances. Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation. Our findings show that LLMs struggle to generate pragmatically relevant responses to non-literal language, achieving only 50-55% accuracy on average. While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses. Using chain-of-thought to make models spell out intentions yields much smaller gains (60% for Mistral-Instruct). These findings suggest that LLMs are not yet effective pragmatic interlocutors, highlighting the need for better approaches for modeling intentions and utilizing them for pragmatic generation.","sentences":["Humans often express their communicative intents indirectly or non-literally, which requires their interlocutors -- human or AI -- to understand beyond the literal meaning of words.","While most existing work has focused on discriminative evaluations, we present a new approach to generatively evaluate large language models' (LLMs') intention understanding by examining their responses to non-literal utterances.","Ideally, an LLM should respond in line with the true intention of a non-literal utterance, not its literal interpretation.","Our findings show that LLMs struggle to generate pragmatically relevant responses to non-literal language, achieving only 50-55% accuracy on average.","While explicitly providing oracle intentions significantly improves performance (e.g., 75% for Mistral-Instruct), this still indicates challenges in leveraging given intentions to produce appropriate responses.","Using chain-of-thought to make models spell out intentions yields much smaller gains (60% for Mistral-Instruct).","These findings suggest that LLMs are not yet effective pragmatic interlocutors, highlighting the need for better approaches for modeling intentions and utilizing them for pragmatic generation."],"url":"http://arxiv.org/abs/2405.08760v1","category":"cs.CL"}
{"created":"2024-05-14 16:45:01","title":"Low Mass Naked Singularities from Dark Core Collapse","abstract":"Near-solar mass black holes (BHs) could have been involved in the two recent gravitational wave events, GW190425 and GW190814. Since such a low mass BH cannot be formed via stellar evolution, a model has been proposed based on the core collapse of a neutron star initiated by a certain number of dark matter (DM) particles. In this process, the accumulated DM particles collapse to form a tiny BH inside the neutron star, and the entire neutron star is transmuted into a BH after a certain time due to the accretion of matter by the endoparasitic BH from its host. Here, we argue that, depending on the initial conditions, a dark core collapse could give rise to either a BH or a naked singularity. For example, if the accumulated cloud of DM particles in the core of a neutron star can be modeled as an anisotropic fluid and it fulfils the criterion for collapse, an endoparasitic naked singularity could form instead of an endoparasitic BH. Immediately after its formation, the naked singularity should begin accreting matter from the host neutron star, thus eventually transmuting the entire host into a near-solar mass, relatively slowly-spinning naked singularity. We also propose a general technique to constrain the DM particle--neutron scattering cross section using the lack of pulsars near the Galactic centre and assuming that these missing pulsars have already been transmuted into BHs and/or naked singularities. Thus, the missing pulsars also indicate the existence of many such singularities near the Galactic center.","sentences":["Near-solar mass black holes (BHs) could have been involved in the two recent gravitational wave events, GW190425 and GW190814.","Since such a low mass BH cannot be formed via stellar evolution, a model has been proposed based on the core collapse of a neutron star initiated by a certain number of dark matter (DM) particles.","In this process, the accumulated DM particles collapse to form a tiny BH inside the neutron star, and the entire neutron star is transmuted into a BH after a certain time due to the accretion of matter by the endoparasitic BH from its host.","Here, we argue that, depending on the initial conditions, a dark core collapse could give rise to either a BH or a naked singularity.","For example, if the accumulated cloud of DM particles in the core of a neutron star can be modeled as an anisotropic fluid and it fulfils the criterion for collapse, an endoparasitic naked singularity could form instead of an endoparasitic BH.","Immediately after its formation, the naked singularity should begin accreting matter from the host neutron star, thus eventually transmuting the entire host into a near-solar mass, relatively slowly-spinning naked singularity.","We also propose a general technique to constrain the DM particle--neutron scattering cross section using the lack of pulsars near the Galactic centre and assuming that these missing pulsars have already been transmuted into BHs and/or naked singularities.","Thus, the missing pulsars also indicate the existence of many such singularities near the Galactic center."],"url":"http://arxiv.org/abs/2405.08758v1","category":"astro-ph.HE"}
{"created":"2024-05-14 16:40:45","title":"Stable Inverse Reinforcement Learning: Policies from Control Lyapunov Landscapes","abstract":"Learning from expert demonstrations to flexibly program an autonomous system with complex behaviors or to predict an agent's behavior is a powerful tool, especially in collaborative control settings. A common method to solve this problem is inverse reinforcement learning (IRL), where the observed agent, e.g., a human demonstrator, is assumed to behave according to the optimization of an intrinsic cost function that reflects its intent and informs its control actions. While the framework is expressive, it is also computationally demanding and generally lacks convergence guarantees. We therefore propose a novel, stability-certified IRL approach by reformulating the cost function inference problem to learning control Lyapunov functions (CLF) from demonstrations data. By additionally exploiting closed-form expressions for associated control policies, we are able to efficiently search the space of CLFs by observing the attractor landscape of the induced dynamics. For the construction of the inverse optimal CLFs, we use a Sum of Squares and formulate a convex optimization problem. We present a theoretical analysis of the optimality properties provided by the CLF and evaluate our approach using both simulated and real-world data.","sentences":["Learning from expert demonstrations to flexibly program an autonomous system with complex behaviors or to predict an agent's behavior is a powerful tool, especially in collaborative control settings.","A common method to solve this problem is inverse reinforcement learning (IRL), where the observed agent, e.g., a human demonstrator, is assumed to behave according to the optimization of an intrinsic cost function that reflects its intent and informs its control actions.","While the framework is expressive, it is also computationally demanding and generally lacks convergence guarantees.","We therefore propose a novel, stability-certified IRL approach by reformulating the cost function inference problem to learning control Lyapunov functions (CLF) from demonstrations data.","By additionally exploiting closed-form expressions for associated control policies, we are able to efficiently search the space of CLFs by observing the attractor landscape of the induced dynamics.","For the construction of the inverse optimal CLFs, we use a Sum of Squares and formulate a convex optimization problem.","We present a theoretical analysis of the optimality properties provided by the CLF and evaluate our approach using both simulated and real-world data."],"url":"http://arxiv.org/abs/2405.08756v1","category":"eess.SY"}
{"created":"2024-05-14 16:40:37","title":"Distributed Threat Intelligence at the Edge Devices: A Large Language Model-Driven Approach","abstract":"With the proliferation of edge devices, there is a significant increase in attack surface on these devices. The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of large language models (LLMs), represents a promising paradigm for enhancing cybersecurity on low-powered edge devices. This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time. Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally. LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives. Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies. The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge. Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network.","sentences":["With the proliferation of edge devices, there is a significant increase in attack surface on these devices.","The decentralized deployment of threat intelligence on edge devices, coupled with adaptive machine learning techniques such as the in-context learning feature of large language models (LLMs), represents a promising paradigm for enhancing cybersecurity on low-powered edge devices.","This approach involves the deployment of lightweight machine learning models directly onto edge devices to analyze local data streams, such as network traffic and system logs, in real-time.","Additionally, distributing computational tasks to an edge server reduces latency and improves responsiveness while also enhancing privacy by processing sensitive data locally.","LLM servers can enable these edge servers to autonomously adapt to evolving threats and attack patterns, continuously updating their models to improve detection accuracy and reduce false positives.","Furthermore, collaborative learning mechanisms facilitate peer-to-peer secure and trustworthy knowledge sharing among edge devices, enhancing the collective intelligence of the network and enabling dynamic threat mitigation measures such as device quarantine in response to detected anomalies.","The scalability and flexibility of this approach make it well-suited for diverse and evolving network environments, as edge devices only send suspicious information such as network traffic and system log changes, offering a resilient and efficient solution to combat emerging cyber threats at the network edge.","Thus, our proposed framework can improve edge computing security by providing better security in cyber threat detection and mitigation by isolating the edge devices from the network."],"url":"http://arxiv.org/abs/2405.08755v1","category":"cs.CR"}
{"created":"2024-05-14 16:40:06","title":"Hierarchical Resource Partitioning on Modern GPUs: A Reinforcement Learning Approach","abstract":"GPU-based heterogeneous architectures are now commonly used in HPC clusters. Due to their architectural simplicity specialized for data-level parallelism, GPUs can offer much higher computational throughput and memory bandwidth than CPUs in the same generation do. However, as the available resources in GPUs have increased exponentially over the past decades, it has become increasingly difficult for a single program to fully utilize them. As a consequence, the industry has started supporting several resource partitioning features in order to improve the resource utilization by co-scheduling multiple programs on the same GPU die at the same time. Driven by the technological trend, this paper focuses on hierarchical resource partitioning on modern GPUs, and as an example, we utilize a combination of two different features available on recent NVIDIA GPUs in a hierarchical manner: MPS (Multi-Process Service), a finer-grained logical partitioning; and MIG (Multi-Instance GPU), a coarse-grained physical partitioning. We propose a method for comprehensively co-optimizing the setup of hierarchical partitioning and the selection of co-scheduling groups from a given set of jobs, based on reinforcement learning using their profiles. Our thorough experimental results demonstrate that our approach can successfully set up job concurrency, partitioning, and co-scheduling group selections simultaneously. This results in a maximum throughput improvement by a factor of 1.87 compared to the time-sharing scheduling.","sentences":["GPU-based heterogeneous architectures are now commonly used in HPC clusters.","Due to their architectural simplicity specialized for data-level parallelism, GPUs can offer much higher computational throughput and memory bandwidth than CPUs in the same generation do.","However, as the available resources in GPUs have increased exponentially over the past decades, it has become increasingly difficult for a single program to fully utilize them.","As a consequence, the industry has started supporting several resource partitioning features in order to improve the resource utilization by co-scheduling multiple programs on the same GPU die at the same time.","Driven by the technological trend, this paper focuses on hierarchical resource partitioning on modern GPUs, and as an example, we utilize a combination of two different features available on recent NVIDIA GPUs in a hierarchical manner: MPS (Multi-Process Service), a finer-grained logical partitioning; and MIG (Multi-Instance GPU), a coarse-grained physical partitioning.","We propose a method for comprehensively co-optimizing the setup of hierarchical partitioning and the selection of co-scheduling groups from a given set of jobs, based on reinforcement learning using their profiles.","Our thorough experimental results demonstrate that our approach can successfully set up job concurrency, partitioning, and co-scheduling group selections simultaneously.","This results in a maximum throughput improvement by a factor of 1.87 compared to the time-sharing scheduling."],"url":"http://arxiv.org/abs/2405.08754v1","category":"cs.DC"}
{"created":"2024-05-14 16:33:25","title":"Hunyuan-DiT: A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding","abstract":"We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese. To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding. We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization. For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images. Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context. Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models. Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT","sentences":["We present Hunyuan-DiT, a text-to-image diffusion transformer with fine-grained understanding of both English and Chinese.","To construct Hunyuan-DiT, we carefully design the transformer structure, text encoder, and positional encoding.","We also build from scratch a whole data pipeline to update and evaluate data for iterative model optimization.","For fine-grained language understanding, we train a Multimodal Large Language Model to refine the captions of the images.","Finally, Hunyuan-DiT can perform multi-turn multimodal dialogue with users, generating and refining images according to the context.","Through our holistic human evaluation protocol with more than 50 professional human evaluators, Hunyuan-DiT sets a new state-of-the-art in Chinese-to-image generation compared with other open-source models.","Code and pretrained models are publicly available at github.com/Tencent/HunyuanDiT"],"url":"http://arxiv.org/abs/2405.08748v1","category":"cs.CV"}
{"created":"2024-05-14 16:32:50","title":"Decomposing geographical and universal aspects of human mobility","abstract":"Driven by access to large volumes of detailed movement data, the study of human mobility has grown rapidly over the past decade. This body of work has argued that human mobility is scale-free, has proposed models to generate scale-free moving distance distribution, and explained how the scale-free distribution arises from aggregating displacements across scales. However, the field of human mobility has not explicitly addressed how mobility is structured by geographical constraints - such as the outlines of landmasses, lakes, rivers, the placement of buildings, roadways, and cities.   Using unique datasets capturing millions of movements between precise locations, this paper shows how separating the effect of geography from mobility choices reveals a universal power law spanning five orders of magnitude (from 10 m to 1,000,000 m). We incorporate geography through the pair distribution function, a fundamental quantity from condensed matter physics that encapsulates the structure of locations on which mobility occurs. This distribution captures the constraints that geography places on human mobility across different length scales.   Our description conclusively addresses debates between distance-based and opportunity-based perspectives on human mobility. By demonstrating how the spatial distribution of human settlements shapes human mobility, we provide a novel perspective that bridges the gap between these previously opposing ideas.","sentences":["Driven by access to large volumes of detailed movement data, the study of human mobility has grown rapidly over the past decade.","This body of work has argued that human mobility is scale-free, has proposed models to generate scale-free moving distance distribution, and explained how the scale-free distribution arises from aggregating displacements across scales.","However, the field of human mobility has not explicitly addressed how mobility is structured by geographical constraints - such as the outlines of landmasses, lakes, rivers, the placement of buildings, roadways, and cities.   ","Using unique datasets capturing millions of movements between precise locations, this paper shows how separating the effect of geography from mobility choices reveals a universal power law spanning five orders of magnitude (from 10 m to 1,000,000 m).","We incorporate geography through the pair distribution function, a fundamental quantity from condensed matter physics that encapsulates the structure of locations on which mobility occurs.","This distribution captures the constraints that geography places on human mobility across different length scales.   ","Our description conclusively addresses debates between distance-based and opportunity-based perspectives on human mobility.","By demonstrating how the spatial distribution of human settlements shapes human mobility, we provide a novel perspective that bridges the gap between these previously opposing ideas."],"url":"http://arxiv.org/abs/2405.08746v1","category":"physics.soc-ph"}
{"created":"2024-05-14 16:30:32","title":"A tunable binaural audio telepresence system capable of balancing immersive and enhanced modes","abstract":"Binaural Audio Telepresence (BAT) aims to encode the acoustic scene at the far end into binaural signals for the user at the near end. BAT encompasses an immense range of applications that can vary between two extreme modes of Immersive BAT (I-BAT) and Enhanced BAT (E-BAT). With I-BAT, our goal is to preserve the full ambience as if we were at the far end, while with E-BAT, our goal is to enhance the far-end conversation with significantly improved speech quality and intelligibility. To this end, this paper presents a tunable BAT system to vary between these two AT modes with a desired application-specific balance. Microphone signals are converted into binaural signals with prescribed ambience factor. A novel Spatial COherence REpresentation (SCORE) is proposed as an input feature for model training so that the network remains robust to different array setups. Experimental results demonstrated the superior performance of the proposed BAT, even when the array configurations were not included in the training phase.","sentences":["Binaural Audio Telepresence (BAT) aims to encode the acoustic scene at the far end into binaural signals for the user at the near end.","BAT encompasses an immense range of applications that can vary between two extreme modes of Immersive BAT (I-BAT) and Enhanced BAT (E-BAT).","With I-BAT, our goal is to preserve the full ambience as if we were at the far end, while with E-BAT, our goal is to enhance the far-end conversation with significantly improved speech quality and intelligibility.","To this end, this paper presents a tunable BAT system to vary between these two AT modes with a desired application-specific balance.","Microphone signals are converted into binaural signals with prescribed ambience factor.","A novel Spatial COherence REpresentation (SCORE) is proposed as an input feature for model training so that the network remains robust to different array setups.","Experimental results demonstrated the superior performance of the proposed BAT, even when the array configurations were not included in the training phase."],"url":"http://arxiv.org/abs/2405.08742v1","category":"eess.AS"}
{"created":"2024-05-14 16:28:00","title":"CATEcor: an Open Science, Shaded-Truss, Externally-Occulted Coronagraph","abstract":"We present the design of a portable coronagraph, CATEcor, that incorporates a novel \"shaded truss\" style of external occultation and serves as a proof-of-concept for that family of coronagraphs. The shaded truss design style has the potential for broad application in various scientific settings. We conceived CATEcor itself as a simple instrument to observe the corona during the darker skies available during a partial solar eclipse, or for students or interested amateurs to detect the corona under ideal non-eclipsed conditions. CATEcor is therefore optimized for simplicity and accessibility to the public. It is implemented using an existing dioptric telescope and an adapter rig that mounts in front of the objective lens, restricting the telescope aperture and providing external occultation. The adapter rig, including occulter, is fabricated using fusion deposition modeling (FDM; colloquially \"3D printing\"), greatly reducing cost. The structure is designed to be integrated with moderate care and may be replicated in a university or amateur setting. While CATEcor is a simple demonstration unit, the design concept, process, and trades are useful for other more sophisticated coronagraphs in the same general family, which might operate under normal daytime skies outside the annular-eclipse conditions used for CATEcor.","sentences":["We present the design of a portable coronagraph, CATEcor, that incorporates a novel \"shaded truss\" style of external occultation and serves as a proof-of-concept for that family of coronagraphs.","The shaded truss design style has the potential for broad application in various scientific settings.","We conceived CATEcor itself as a simple instrument to observe the corona during the darker skies available during a partial solar eclipse, or for students or interested amateurs to detect the corona under ideal non-eclipsed conditions.","CATEcor is therefore optimized for simplicity and accessibility to the public.","It is implemented using an existing dioptric telescope and an adapter rig that mounts in front of the objective lens, restricting the telescope aperture and providing external occultation.","The adapter rig, including occulter, is fabricated using fusion deposition modeling (FDM; colloquially \"3D printing\"), greatly reducing cost.","The structure is designed to be integrated with moderate care and may be replicated in a university or amateur setting.","While CATEcor is a simple demonstration unit, the design concept, process, and trades are useful for other more sophisticated coronagraphs in the same general family, which might operate under normal daytime skies outside the annular-eclipse conditions used for CATEcor."],"url":"http://arxiv.org/abs/2405.08739v1","category":"astro-ph.IM"}
{"created":"2024-05-14 16:17:43","title":"Multi-Server Multi-Function Distributed Computation","abstract":"The work here studies the communication cost for a multi-server multi-task distributed computation framework, and does so for a broad class of functions and data statistics. Considering the framework where a user seeks the computation of multiple complex (conceivably non-linear) tasks from a set of distributed servers, we establish communication cost upper bounds for a variety of data statistics, function classes and data placements across the servers. To do so, we proceed to apply, for the first time here, K\\\"orner's characteristic graph approach -- which is known to capture the structural properties of data and functions -- to the promising framework of multi-server multi-task distributed computing. Going beyond the general expressions, and in order to offer clearer insight, we also consider the well-known scenario of cyclic dataset placement and linearly separable functions over the binary field, in which case our approach exhibits considerable gains over the state of art. Similar gains are identified for the case of multi-linear functions.","sentences":["The work here studies the communication cost for a multi-server multi-task distributed computation framework, and does so for a broad class of functions and data statistics.","Considering the framework where a user seeks the computation of multiple complex (conceivably non-linear) tasks from a set of distributed servers, we establish communication cost upper bounds for a variety of data statistics, function classes and data placements across the servers.","To do so, we proceed to apply, for the first time here, K\\\"orner's characteristic graph approach -- which is known to capture the structural properties of data and functions -- to the promising framework of multi-server multi-task distributed computing.","Going beyond the general expressions, and in order to offer clearer insight, we also consider the well-known scenario of cyclic dataset placement and linearly separable functions over the binary field, in which case our approach exhibits considerable gains over the state of art.","Similar gains are identified for the case of multi-linear functions."],"url":"http://arxiv.org/abs/2405.08732v1","category":"cs.IT"}
{"created":"2024-05-14 16:16:43","title":"Dynamic On-Palm Manipulation via Controlled Sliding","abstract":"Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure. Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding. However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation. In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes. We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task. We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks. Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task. Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach. To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions.","sentences":["Non-prehensile manipulation enables fast interactions with objects by circumventing the need to grasp and ungrasp as well as handling objects that cannot be grasped through force closure.","Current approaches to non-prehensile manipulation focus on static contacts, avoiding the underactuation that comes with sliding.","However, the ability to control sliding contact, essentially removing the no-slip constraint, opens up new possibilities in dynamic manipulation.","In this paper, we explore a challenging dynamic non-prehensile manipulation task that requires the consideration of the full spectrum of hybrid contact modes.","We leverage recent methods in contact-implicit MPC to handle the multi-modal planning aspect of the task.","We demonstrate, with careful consideration of integration between the simple model used for MPC and the low-level tracking controller, how contact-implicit MPC can be adapted to dynamic tasks.","Surprisingly, despite the known inaccuracies of frictional rigid contact models, our method is able to react to these inaccuracies while still quickly performing the task.","Moreover, we do not use common aids such as reference trajectories or motion primitives, highlighting the generality of our approach.","To the best of our knowledge, this is the first application of contact-implicit MPC to a dynamic manipulation task in three dimensions."],"url":"http://arxiv.org/abs/2405.08731v1","category":"cs.RO"}
{"created":"2024-05-14 16:16:29","title":"A Generalized Difference-in-Differences Estimator for Unbiased Estimation of Desired Estimands from Staggered Adoption and Stepped-Wedge Settings","abstract":"Staggered treatment adoption arises in the evaluation of policy impact and implementation in a variety of settings. This occurs in both randomized stepped-wedge trials and non-randomized quasi-experimental designs using causal inference methods based on difference-in-differences analysis. In both settings, it is crucial to carefully consider the target estimand and possible treatment effect heterogeneities in order to estimate the effect without bias and in an interpretable fashion. This paper proposes a novel non-parametric approach to this estimation for either setting. By constructing an estimator using two-by-two difference-in-difference comparisons as building blocks with arbitrary weights, the investigator can select weights to target the desired estimand in an unbiased manner under assumed treatment effect homogeneity, and minimize the variance under an assumed working covariance structure. This provides desirable bias properties with a relatively small sacrifice in variance and power by using the comparisons efficiently. The method is demonstrated on toy examples to show the process, as well as in the re-analysis of a stepped wedge trial on the impact of novel tuberculosis diagnostic tools. A full algorithm with R code is provided to implement this method. The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff.","sentences":["Staggered treatment adoption arises in the evaluation of policy impact and implementation in a variety of settings.","This occurs in both randomized stepped-wedge trials and non-randomized quasi-experimental designs using causal inference methods based on difference-in-differences analysis.","In both settings, it is crucial to carefully consider the target estimand and possible treatment effect heterogeneities in order to estimate the effect without bias and in an interpretable fashion.","This paper proposes a novel non-parametric approach to this estimation for either setting.","By constructing an estimator using two-by-two difference-in-difference comparisons as building blocks with arbitrary weights, the investigator can select weights to target the desired estimand in an unbiased manner under assumed treatment effect homogeneity, and minimize the variance under an assumed working covariance structure.","This provides desirable bias properties with a relatively small sacrifice in variance and power by using the comparisons efficiently.","The method is demonstrated on toy examples to show the process, as well as in the re-analysis of a stepped wedge trial on the impact of novel tuberculosis diagnostic tools.","A full algorithm with R code is provided to implement this method.","The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff."],"url":"http://arxiv.org/abs/2405.08730v1","category":"stat.ME"}
{"created":"2024-05-14 16:15:31","title":"Targeted Augmentation for Low-Resource Event Extraction","abstract":"Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples. Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance). This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence. Extensive experimental results demonstrate the effectiveness of the proposed paradigm. Furthermore, identified limitations are discussed, shedding light on areas for future improvement.","sentences":["Addressing the challenge of low-resource information extraction remains an ongoing issue due to the inherent information scarcity within limited training examples.","Existing data augmentation methods, considered potential solutions, struggle to strike a balance between weak augmentation (e.g., synonym augmentation) and drastic augmentation (e.g., conditional generation without proper guidance).","This paper introduces a novel paradigm that employs targeted augmentation and back validation to produce augmented examples with enhanced diversity, polarity, accuracy, and coherence.","Extensive experimental results demonstrate the effectiveness of the proposed paradigm.","Furthermore, identified limitations are discussed, shedding light on areas for future improvement."],"url":"http://arxiv.org/abs/2405.08729v1","category":"cs.CL"}
{"created":"2024-05-14 16:13:13","title":"Dimensionality reduction in bulk-boundary reaction-diffusion systems","abstract":"Intracellular protein patterns regulate many vital cellular functions, such as the processing of spatiotemporal information or the control of shape deformations. To do so, pattern-forming systems can be sensitive to the cell geometry by means of coupling the protein dynamics on the cell membrane to dynamics in the cytosol. Recent studies demonstrated that modeling the cytosolic dynamics in terms of an averaged protein pool disregards possibly crucial aspects of the pattern formation, most importantly concentration gradients normal to the membrane. At the same time, the coupling of two domains (surface and volume) with different dimensions renders many standard tools for the numerical analysis of self-organizing systems inefficient. Here, we present a generic framework for projecting the cytosolic dynamics onto the lower-dimensional surface that respects the influence of cytosolic concentration gradients in static and evolving geometries. This method uses a priori physical information about the system to approximate the cytosolic dynamics by a small number of dominant characteristic concentration profiles (basis), akin to basis transformations of finite element methods. As a proof of concept, we apply our framework to a toy model for volume-dependent interrupted coarsening, evaluate the accuracy of the results for various basis choices, and discuss the optimal basis choice for biologically relevant systems. Our analysis presents an efficient yet accurate method for analysing pattern formation with surface-volume coupling in evolving geometries.","sentences":["Intracellular protein patterns regulate many vital cellular functions, such as the processing of spatiotemporal information or the control of shape deformations.","To do so, pattern-forming systems can be sensitive to the cell geometry by means of coupling the protein dynamics on the cell membrane to dynamics in the cytosol.","Recent studies demonstrated that modeling the cytosolic dynamics in terms of an averaged protein pool disregards possibly crucial aspects of the pattern formation, most importantly concentration gradients normal to the membrane.","At the same time, the coupling of two domains (surface and volume) with different dimensions renders many standard tools for the numerical analysis of self-organizing systems inefficient.","Here, we present a generic framework for projecting the cytosolic dynamics onto the lower-dimensional surface that respects the influence of cytosolic concentration gradients in static and evolving geometries.","This method uses a priori physical information about the system to approximate the cytosolic dynamics by a small number of dominant characteristic concentration profiles (basis), akin to basis transformations of finite element methods.","As a proof of concept, we apply our framework to a toy model for volume-dependent interrupted coarsening, evaluate the accuracy of the results for various basis choices, and discuss the optimal basis choice for biologically relevant systems.","Our analysis presents an efficient yet accurate method for analysing pattern formation with surface-volume coupling in evolving geometries."],"url":"http://arxiv.org/abs/2405.08728v1","category":"physics.bio-ph"}
{"created":"2024-05-14 16:12:27","title":"I-CTRL: Imitation to Control Humanoid Robots Through Constrained Reinforcement Learning","abstract":"This paper addresses the critical need for refining robot motions that, despite achieving a high visual similarity through human-to-humanoid retargeting methods, fall short of practical execution in the physical realm. Existing techniques in the graphics community often prioritize visual fidelity over physics-based feasibility, posing a significant challenge for deploying bipedal systems in practical applications. Our research introduces a constrained reinforcement learning algorithm to produce physics-based high-quality motion imitation onto legged humanoid robots that enhance motion resemblance while successfully following the reference human trajectory. We name our framework: I-CTRL. By reformulating the motion imitation problem as a constrained refinement over non-physics-based retargeted motions, our framework excels in motion imitation with simple and unique rewards that generalize across four robots. Moreover, our framework can follow large-scale motion datasets with a unique RL agent. The proposed approach signifies a crucial step forward in advancing the control of bipedal robots, emphasizing the importance of aligning visual and physical realism for successful motion imitation.","sentences":["This paper addresses the critical need for refining robot motions that, despite achieving a high visual similarity through human-to-humanoid retargeting methods, fall short of practical execution in the physical realm.","Existing techniques in the graphics community often prioritize visual fidelity over physics-based feasibility, posing a significant challenge for deploying bipedal systems in practical applications.","Our research introduces a constrained reinforcement learning algorithm to produce physics-based high-quality motion imitation onto legged humanoid robots that enhance motion resemblance while successfully following the reference human trajectory.","We name our framework: I-CTRL.","By reformulating the motion imitation problem as a constrained refinement over non-physics-based retargeted motions, our framework excels in motion imitation with simple and unique rewards that generalize across four robots.","Moreover, our framework can follow large-scale motion datasets with a unique RL agent.","The proposed approach signifies a crucial step forward in advancing the control of bipedal robots, emphasizing the importance of aligning visual and physical realism for successful motion imitation."],"url":"http://arxiv.org/abs/2405.08726v1","category":"cs.RO"}
{"created":"2024-05-14 16:04:39","title":"Addressing Misspecification in Simulation-based Inference through Data-driven Calibration","abstract":"Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI's reliability. This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations. Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator. Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals.","sentences":["Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators.","However, recent work has demonstrated that model misspecification can harm SBI's reliability.","This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements.","We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations.","Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator.","Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals."],"url":"http://arxiv.org/abs/2405.08719v1","category":"stat.ML"}
{"created":"2024-05-14 15:59:58","title":"Generalized energy gap law: An open system dynamics approach to non-adiabatic phenomena in molecules","abstract":"Non-adiabatic molecular phenomena, arising from the breakdown of the Born-Oppenheimer approximation, govern the fate of virtually all photo-physical and photochemical processes and limit the quantum efficiency of molecules and other solid-state embedded quantum emitters. A simple and elegant description, the energy gap law, was derived five decades ago, predicting that the non-adiabatic coupling between the excited and ground potential landscapes lead to non-radiative decay with a quasi-exponential dependence on the energy gap. We revisit and extend this theory to account for crucial aspects such as vibrational relaxation, dephasing, and radiative loss. We find a closed analytical solution with general validity which indicates a direct proportionality of the non-radiative rate with the vibrational relaxation rate at low temperatures, and with the dephasing rate of the electronic transition at high temperatures. Our work establishes a connection between nanoscale quantum optics, open quantum system dynamics and non-adiabatic molecular physics.","sentences":["Non-adiabatic molecular phenomena, arising from the breakdown of the Born-Oppenheimer approximation, govern the fate of virtually all photo-physical and photochemical processes and limit the quantum efficiency of molecules and other solid-state embedded quantum emitters.","A simple and elegant description, the energy gap law, was derived five decades ago, predicting that the non-adiabatic coupling between the excited and ground potential landscapes lead to non-radiative decay with a quasi-exponential dependence on the energy gap.","We revisit and extend this theory to account for crucial aspects such as vibrational relaxation, dephasing, and radiative loss.","We find a closed analytical solution with general validity which indicates a direct proportionality of the non-radiative rate with the vibrational relaxation rate at low temperatures, and with the dephasing rate of the electronic transition at high temperatures.","Our work establishes a connection between nanoscale quantum optics, open quantum system dynamics and non-adiabatic molecular physics."],"url":"http://arxiv.org/abs/2405.08718v1","category":"quant-ph"}
{"created":"2024-05-14 15:58:22","title":"Commuting Clifford actions","abstract":"It shown that if a vector space carries commuting actions of two Clifford algebras, then the quadratic monomials using generators from either Clifford algebra determine a spinor representation of an orthogonal Lie algebra.   Examples of this construction have applications to high energy physics, particularly to the standard model and unification. It is shown how to use Clifford data to construct spectral triples for the Pati-Salam model that admit an action of Spin(10).","sentences":["It shown that if a vector space carries commuting actions of two Clifford algebras, then the quadratic monomials using generators from either Clifford algebra determine a spinor representation of an orthogonal Lie algebra.   ","Examples of this construction have applications to high energy physics, particularly to the standard model and unification.","It is shown how to use Clifford data to construct spectral triples for the Pati-Salam model that admit an action of Spin(10)."],"url":"http://arxiv.org/abs/2405.08716v1","category":"math-ph"}
{"created":"2024-05-14 15:48:36","title":"Beyond Scaling Laws: Understanding Transformer Performance with Associative Memory","abstract":"Increasing the size of a Transformer model does not always lead to enhanced performance. This phenomenon cannot be explained by the empirical scaling laws. Furthermore, improved generalization ability occurs as the model memorizes the training samples. We present a theoretical framework that sheds light on the memorization process and performance dynamics of transformer-based language models. We model the behavior of Transformers with associative memories using Hopfield networks, such that each transformer block effectively conducts an approximate nearest-neighbor search. Based on this, we design an energy function analogous to that in the modern continuous Hopfield network which provides an insightful explanation for the attention mechanism. Using the majorization-minimization technique, we construct a global energy function that captures the layered architecture of the Transformer. Under specific conditions, we show that the minimum achievable cross-entropy loss is bounded from below by a constant approximately equal to 1. We substantiate our theoretical results by conducting experiments with GPT-2 on various data sizes, as well as training vanilla Transformers on a dataset of 2M tokens.","sentences":["Increasing the size of a Transformer model does not always lead to enhanced performance.","This phenomenon cannot be explained by the empirical scaling laws.","Furthermore, improved generalization ability occurs as the model memorizes the training samples.","We present a theoretical framework that sheds light on the memorization process and performance dynamics of transformer-based language models.","We model the behavior of Transformers with associative memories using Hopfield networks, such that each transformer block effectively conducts an approximate nearest-neighbor search.","Based on this, we design an energy function analogous to that in the modern continuous Hopfield network which provides an insightful explanation for the attention mechanism.","Using the majorization-minimization technique, we construct a global energy function that captures the layered architecture of the Transformer.","Under specific conditions, we show that the minimum achievable cross-entropy loss is bounded from below by a constant approximately equal to 1.","We substantiate our theoretical results by conducting experiments with GPT-2 on various data sizes, as well as training vanilla Transformers on a dataset of 2M tokens."],"url":"http://arxiv.org/abs/2405.08707v1","category":"cs.LG"}
{"created":"2024-05-14 15:42:55","title":"Full Line Code Completion: Bringing AI to Desktop","abstract":"In recent years, several industrial solutions for the problem of multi-token code completion have appeared, each making a great advance in the area but mostly focusing on cloud-based runtime and avoiding working on the end user's device.   In this work, we describe our approach for building a multi-token code completion feature for the JetBrains' IntelliJ Platform, which we call Full Line Code Completion. The feature suggests only syntactically correct code and works fully locally, i.e., data querying and the generation of suggestions happens on the end user's machine. We share important time and memory-consumption restrictions, as well as design principles that a code completion engine should satisfy. Working entirely on the end user's device, our code completion engine enriches user experience while being not only fast and compact but also secure. We share a number of useful techniques to meet the stated development constraints and also describe offline and online evaluation pipelines that allowed us to make better decisions.   Our online evaluation shows that the usage of the tool leads to 1.5 times more code in the IDE being produced by code completion. The described solution was initially started with the help of researchers and was bundled into two JetBrains' IDEs - PyCharm Pro and DataSpell - at the end of 2023, so we believe that this work is useful for bridging academia and industry, providing researchers with the knowledge of what happens when complex research-based solutions are integrated into real products.","sentences":["In recent years, several industrial solutions for the problem of multi-token code completion have appeared, each making a great advance in the area but mostly focusing on cloud-based runtime and avoiding working on the end user's device.   ","In this work, we describe our approach for building a multi-token code completion feature for the JetBrains' IntelliJ Platform, which we call Full Line Code Completion.","The feature suggests only syntactically correct code and works fully locally, i.e., data querying and the generation of suggestions happens on the end user's machine.","We share important time and memory-consumption restrictions, as well as design principles that a code completion engine should satisfy.","Working entirely on the end user's device, our code completion engine enriches user experience while being not only fast and compact but also secure.","We share a number of useful techniques to meet the stated development constraints and also describe offline and online evaluation pipelines that allowed us to make better decisions.   ","Our online evaluation shows that the usage of the tool leads to 1.5 times more code in the IDE being produced by code completion.","The described solution was initially started with the help of researchers and was bundled into two JetBrains' IDEs - PyCharm Pro and DataSpell - at the end of 2023, so we believe that this work is useful for bridging academia and industry, providing researchers with the knowledge of what happens when complex research-based solutions are integrated into real products."],"url":"http://arxiv.org/abs/2405.08704v1","category":"cs.SE"}
{"created":"2024-05-14 15:40:09","title":"Structure and dynamics of electron-phonon coupled systems using neural quantum states","abstract":"In this work, we use neural quantum states (NQS) to describe the high-dimensional wave functions of electron-phonon coupled systems. We demonstrate that NQS can accurately and systematically learn the underlying physics of such problems through a variational Monte Carlo optimization of the energy with minimal incorporation of physical information even in highly challenging cases. We assess the ability of our approach across various lattice model examples featuring different types of couplings. The flexibility of our NQS formulation is demonstrated via application to ab initio models parametrized by density functional perturbation theory consisting of electron or hole bands coupled linearly to dispersive phonons. We compute accurate real-frequency spectral properties of electron-phonon systems via a novel formalism based on NQS. Our work establishes a general framework for exploring diverse ground state and dynamical phenomena arising in electron-phonon systems, including the non-perturbative interplay of correlated electronic and electron-phonon effects in systems ranging from simple lattice models to realistic models of materials parametrized by ab initio calculations.","sentences":["In this work, we use neural quantum states (NQS) to describe the high-dimensional wave functions of electron-phonon coupled systems.","We demonstrate that NQS can accurately and systematically learn the underlying physics of such problems through a variational Monte Carlo optimization of the energy with minimal incorporation of physical information even in highly challenging cases.","We assess the ability of our approach across various lattice model examples featuring different types of couplings.","The flexibility of our NQS formulation is demonstrated via application to ab initio models parametrized by density functional perturbation theory consisting of electron or hole bands coupled linearly to dispersive phonons.","We compute accurate real-frequency spectral properties of electron-phonon systems via a novel formalism based on NQS.","Our work establishes a general framework for exploring diverse ground state and dynamical phenomena arising in electron-phonon systems, including the non-perturbative interplay of correlated electronic and electron-phonon effects in systems ranging from simple lattice models to realistic models of materials parametrized by ab initio calculations."],"url":"http://arxiv.org/abs/2405.08701v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 15:40:04","title":"Ab Initio Polaron Wave Functions","abstract":"In this work we demonstrate that accurate ground state wave functions may be constructed for polarons in a fully ab initio setting across the wide range of couplings associated with both the large and small polaron limits. We present a single general unitary transformation approach which encompasses an ab initio version of the Lee-Low-Pines theory at weak coupling and the coherent state Landau-Pekar framework at strong coupling while interpolating between these limits in general cases. We show that perturbation theory around these limits may be performed in a facile manner to assess the accuracy of the approach, as well as provide an independent route to the ab initio properties of polarons. We test these ideas on the case of LiF, where the electron-polaron is expected to be large and relatively weakly coupled, while the hole-polaron is expected to be a strongly coupled small polaron.","sentences":["In this work we demonstrate that accurate ground state wave functions may be constructed for polarons in a fully ab initio setting across the wide range of couplings associated with both the large and small polaron limits.","We present a single general unitary transformation approach which encompasses an ab initio version of the Lee-Low-Pines theory at weak coupling and the coherent state Landau-Pekar framework at strong coupling while interpolating between these limits in general cases.","We show that perturbation theory around these limits may be performed in a facile manner to assess the accuracy of the approach, as well as provide an independent route to the ab initio properties of polarons.","We test these ideas on the case of LiF, where the electron-polaron is expected to be large and relatively weakly coupled, while the hole-polaron is expected to be a strongly coupled small polaron."],"url":"http://arxiv.org/abs/2405.08700v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 15:39:22","title":"Weakly-supervised causal discovery based on fuzzy knowledge and complex data complementarity","abstract":"Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems. However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes. To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL. KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints. This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery. It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios. In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data. Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency. For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data. In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise.","sentences":["Causal discovery based on observational data is important for deciphering the causal mechanism behind complex systems.","However, the effectiveness of existing causal discovery methods is limited due to inferior prior knowledge, domain inconsistencies, and the challenges of high-dimensional datasets with small sample sizes.","To address this gap, we propose a novel weakly-supervised fuzzy knowledge and data co-driven causal discovery method named KEEL.","KEEL adopts a fuzzy causal knowledge schema to encapsulate diverse types of fuzzy knowledge, and forms corresponding weakened constraints.","This schema not only lessens the dependency on expertise but also allows various types of limited and error-prone fuzzy knowledge to guide causal discovery.","It can enhance the generalization and robustness of causal discovery, especially in high-dimensional and small-sample scenarios.","In addition, we integrate the extended linear causal model (ELCM) into KEEL for dealing with the multi-distribution and incomplete data.","Extensive experiments with different datasets demonstrate the superiority of KEEL over several state-of-the-art methods in accuracy, robustness and computational efficiency.","For causal discovery in real protein signal transduction processes, KEEL outperforms the benchmark method with limited data.","In summary, KEEL is effective to tackle the causal discovery tasks with higher accuracy while alleviating the requirement for extensive domain expertise."],"url":"http://arxiv.org/abs/2405.08699v1","category":"stat.ML"}
{"created":"2024-05-14 15:26:42","title":"Extending Non-Perturbative Simulation Techniques for Open-Quantum Systems to Excited-State Proton Transfer and Ultrafast Non-Adiabatic Dynamics","abstract":"Excited state proton transfer is an ubiquitous phenomenon in biology and chemistry, spanning from the ultrafast reactions of photo-bases and acids to light-driven, enzymatic catalysis and photosynthesis. However, the simulation of such dynamics involves multiple challenges, since high-dimensional, out-of-equilibrium vibronic states play a crucial role, while a fully quantum description of the proton's dissipative, real-space dynamics is also required. In this work, we extend the powerful Matrix Product State approach to open quantum systems (TEDOPA) to study these demanding dynamics, and also more general non-adiabatic processes that can appear in complex photochemistry subject to strong laser driving. As an illustration, we initially consider an open model of a four-level electronic system interacting with hundreds of intramolecular vibrations that drive ultrafast excited state proton transfer, as well as an explicit photonic environment that allows us to directly monitor the resulting dual fluorescence in this system. We then demonstrate how to include a continuous 'reaction coordinate' of the proton transfer that allows numerically exact simulations that can be understood, visualized and interpreted in the familiar language of diabatic and adiabatic dynamics on potential surfaces, while also retaining an exact quantum treatment of dissipation and driving effects that could be used to study diverse problems in ultrafast photochemistry.","sentences":["Excited state proton transfer is an ubiquitous phenomenon in biology and chemistry, spanning from the ultrafast reactions of photo-bases and acids to light-driven, enzymatic catalysis and photosynthesis.","However, the simulation of such dynamics involves multiple challenges, since high-dimensional, out-of-equilibrium vibronic states play a crucial role, while a fully quantum description of the proton's dissipative, real-space dynamics is also required.","In this work, we extend the powerful Matrix Product State approach to open quantum systems (TEDOPA) to study these demanding dynamics, and also more general non-adiabatic processes that can appear in complex photochemistry subject to strong laser driving.","As an illustration, we initially consider an open model of a four-level electronic system interacting with hundreds of intramolecular vibrations that drive ultrafast excited state proton transfer, as well as an explicit photonic environment that allows us to directly monitor the resulting dual fluorescence in this system.","We then demonstrate how to include a continuous 'reaction coordinate' of the proton transfer that allows numerically exact simulations that can be understood, visualized and interpreted in the familiar language of diabatic and adiabatic dynamics on potential surfaces, while also retaining an exact quantum treatment of dissipation and driving effects that could be used to study diverse problems in ultrafast photochemistry."],"url":"http://arxiv.org/abs/2405.08693v1","category":"physics.chem-ph"}
{"created":"2024-05-14 15:25:53","title":"Quaternionic Cartan coverings and applications","abstract":"We present the topological foundation for solvability of Multiplicative Cousin problems formulated on an axially symmetric domain $\\Omega \\subset \\mathbb H.$ In particular, we provide a geometric construction of quaternionic Cartan coverings, which are generalizations of (complex) Cartan coverings as presented in Section 4 of [FP]. Because of the requirements of symmetry inherent to the domains of definition of quaternionic regular functions, the existence of quaternionic Cartan coverings of $\\Omega$ is not a consequence of existence of complex Cartan coverings, because for the latter there are no requirements for the symmetries with respect to the real axis. Due to the special role of the real axis, also the covering restricted to $\\Omega \\cap \\mathbb R$ has to have additional properties. All these required properties were achieved by starting from a particular symmetric tiling of the symmetric set $\\Omega \\cap (\\mathbb R + i\\mathbb R)$. Finally we provide an application of these results to prove the vanishing of 'antisymmetric' cohomology groups of planar symmetric domains for $n \\geq 2$.","sentences":["We present the topological foundation for solvability of Multiplicative Cousin problems formulated on an axially symmetric domain $\\Omega \\subset \\mathbb H.$","In particular, we provide a geometric construction of quaternionic Cartan coverings, which are generalizations of (complex) Cartan coverings as presented in Section 4 of [FP].","Because of the requirements of symmetry inherent to the domains of definition of quaternionic regular functions, the existence of quaternionic Cartan coverings of $\\Omega$ is not a consequence of existence of complex Cartan coverings, because for the latter there are no requirements for the symmetries with respect to the real axis.","Due to the special role of the real axis, also the covering restricted to $\\Omega \\cap \\mathbb R$ has to have additional properties.","All these required properties were achieved by starting from a particular symmetric tiling of the symmetric set $\\Omega \\cap (\\mathbb R +","i\\mathbb R)$. Finally we provide an application of these results to prove the vanishing of 'antisymmetric' cohomology groups of planar symmetric domains for $n \\geq 2$."],"url":"http://arxiv.org/abs/2405.08692v1","category":"math.CV"}
{"created":"2024-05-14 15:09:11","title":"Latent group structure in linear panel data models with endogenous regressors","abstract":"This paper concerns the estimation of linear panel data models with endogenous regressors and a latent group structure in the coefficients. We consider instrumental variables estimation of the group-specific coefficient vector. We show that direct application of the Kmeans algorithm to the generalized method of moments objective function does not yield unique estimates. We newly develop and theoretically justify two-stage estimation methods that apply the Kmeans algorithm to a regression of the dependent variable on predicted values of the endogenous regressors. The results of Monte Carlo simulations demonstrate that two-stage estimation with the first stage modeled using a latent group structure achieves good classification accuracy, even if the true first-stage regression is fully heterogeneous. We apply our estimation methods to revisiting the relationship between income and democracy.","sentences":["This paper concerns the estimation of linear panel data models with endogenous regressors and a latent group structure in the coefficients.","We consider instrumental variables estimation of the group-specific coefficient vector.","We show that direct application of the Kmeans algorithm to the generalized method of moments objective function does not yield unique estimates.","We newly develop and theoretically justify two-stage estimation methods that apply the Kmeans algorithm to a regression of the dependent variable on predicted values of the endogenous regressors.","The results of Monte Carlo simulations demonstrate that two-stage estimation with the first stage modeled using a latent group structure achieves good classification accuracy, even if the true first-stage regression is fully heterogeneous.","We apply our estimation methods to revisiting the relationship between income and democracy."],"url":"http://arxiv.org/abs/2405.08687v1","category":"econ.EM"}
{"created":"2024-05-14 15:08:07","title":"Antiferromagnetic Quantum Anomalous Hall Effect Modulated by Spin Flips and Flops","abstract":"The interplay between nontrivial band topology and layered antiferromagnetism in MnBi2Te4 has opened up a new avenue for exploring topological phases of matter. Representative examples include the quantum anomalous Hall effect and axion insulator state observed in odd and even number layers of MnBi2Te4, when the top and bottom surfaces have parallel and antiparallel spin alignments respectively. The rich and complex spin dynamics associated with the van der Waals antiferromagnetic order is expected to generate novel topological phases and phase transitions that are unique to MnBi2Te4. Here we fabricate a device of 7-septuple-layer MnBi2Te4 covered with AlOx capping layer, which enables the investigation of antiferromagnetic quantum anomalous Hall effect over wide parameter spaces. By tuning the gate voltage and perpendicular magnetic field, we uncover a cascade of quantum phase transitions that can be attributed to the influence of spin configurations on charge transport. Furthermore, we find that an in-plane magnetic field enhances both the coercive field and exchange gap of the surface state, in sharp contrast to that in ferromagnetic quantum anomalous Hall state. We propose that these peculiar features arise from the spin flip and flop transitions inherent to van der Waals antiferromagnet. The versatile tunability of the quantum anomalous Hall effect in MnBi2Te4 paves the way for potential applications in topological antiferromagnetic spintronics.","sentences":["The interplay between nontrivial band topology and layered antiferromagnetism in MnBi2Te4 has opened up a new avenue for exploring topological phases of matter.","Representative examples include the quantum anomalous Hall effect and axion insulator state observed in odd and even number layers of MnBi2Te4, when the top and bottom surfaces have parallel and antiparallel spin alignments respectively.","The rich and complex spin dynamics associated with the van der Waals antiferromagnetic order is expected to generate novel topological phases and phase transitions that are unique to MnBi2Te4.","Here we fabricate a device of 7-septuple-layer MnBi2Te4 covered with AlOx capping layer, which enables the investigation of antiferromagnetic quantum anomalous Hall effect over wide parameter spaces.","By tuning the gate voltage and perpendicular magnetic field, we uncover a cascade of quantum phase transitions that can be attributed to the influence of spin configurations on charge transport.","Furthermore, we find that an in-plane magnetic field enhances both the coercive field and exchange gap of the surface state, in sharp contrast to that in ferromagnetic quantum anomalous Hall state.","We propose that these peculiar features arise from the spin flip and flop transitions inherent to van der Waals antiferromagnet.","The versatile tunability of the quantum anomalous Hall effect in MnBi2Te4 paves the way for potential applications in topological antiferromagnetic spintronics."],"url":"http://arxiv.org/abs/2405.08686v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 15:04:46","title":"Achieving Fairness Through Channel Pruning for Dermatological Disease Diagnosis","abstract":"Numerous studies have revealed that deep learning-based medical image classification models may exhibit bias towards specific demographic attributes, such as race, gender, and age. Existing bias mitigation methods often achieve high level of fairness at the cost of significant accuracy degradation. In response to this challenge, we propose an innovative and adaptable Soft Nearest Neighbor Loss-based channel pruning framework, which achieves fairness through channel pruning. Traditionally, channel pruning is utilized to accelerate neural network inference. However, our work demonstrates that pruning can also be a potent tool for achieving fairness. Our key insight is that different channels in a layer contribute differently to the accuracy of different groups. By selectively pruning critical channels that lead to the accuracy difference between the privileged and unprivileged groups, we can effectively improve fairness without sacrificing accuracy significantly. Experiments conducted on two skin lesion diagnosis datasets across multiple sensitive attributes validate the effectiveness of our method in achieving state-of-the-art trade-off between accuracy and fairness. Our code is available at https://github.com/Kqp1227/Sensitive-Channel-Pruning.","sentences":["Numerous studies have revealed that deep learning-based medical image classification models may exhibit bias towards specific demographic attributes, such as race, gender, and age.","Existing bias mitigation methods often achieve high level of fairness at the cost of significant accuracy degradation.","In response to this challenge, we propose an innovative and adaptable Soft Nearest Neighbor Loss-based channel pruning framework, which achieves fairness through channel pruning.","Traditionally, channel pruning is utilized to accelerate neural network inference.","However, our work demonstrates that pruning can also be a potent tool for achieving fairness.","Our key insight is that different channels in a layer contribute differently to the accuracy of different groups.","By selectively pruning critical channels that lead to the accuracy difference between the privileged and unprivileged groups, we can effectively improve fairness without sacrificing accuracy significantly.","Experiments conducted on two skin lesion diagnosis datasets across multiple sensitive attributes validate the effectiveness of our method in achieving state-of-the-art trade-off between accuracy and fairness.","Our code is available at https://github.com/Kqp1227/Sensitive-Channel-Pruning."],"url":"http://arxiv.org/abs/2405.08681v1","category":"cs.CV"}
{"created":"2024-05-14 15:03:06","title":"Generalized uncertainty principle distorted quintessence dynamics","abstract":"In this paper, we invoke a generalized uncertainty principle (GUP) in the symmetry-reduced cosmological Hamiltonian for a universe driven by a quintessence scalar field with potential. Our study focuses on semi-classical regime. In particular, we derive the GUP-distorted Friedmann, Raychaudhuri, and the Klein-Gordon equation. This is followed by a systematic analysis of the qualitative dynamics for the choice of potential $V(\\phi)= V_0 \\sinh^{-n}{(\\mu \\phi)}$. This involves constructing an autonomous dynamical system of equations by choosing appropriate dynamical variables, followed by a qualitative study using linear stability theory. Our analysis shows that incorporating GUP significantly changes the existing fixed points compared to the limiting case without quantum effects by switching off the GUP.","sentences":["In this paper, we invoke a generalized uncertainty principle (GUP) in the symmetry-reduced cosmological Hamiltonian for a universe driven by a quintessence scalar field with potential.","Our study focuses on semi-classical regime.","In particular, we derive the GUP-distorted Friedmann, Raychaudhuri, and the Klein-Gordon equation.","This is followed by a systematic analysis of the qualitative dynamics for the choice of potential $V(\\phi)= V_0 \\sinh^{-n}{(\\mu \\phi)}$.","This involves constructing an autonomous dynamical system of equations by choosing appropriate dynamical variables, followed by a qualitative study using linear stability theory.","Our analysis shows that incorporating GUP significantly changes the existing fixed points compared to the limiting case without quantum effects by switching off the GUP."],"url":"http://arxiv.org/abs/2405.08680v1","category":"gr-qc"}
{"created":"2024-05-14 15:00:09","title":"Investigating Design Choices in Joint-Embedding Predictive Architectures for General Audio Representation Learning","abstract":"This paper addresses the problem of self-supervised general-purpose audio representation learning. We explore the use of Joint-Embedding Predictive Architectures (JEPA) for this task, which consists of splitting an input mel-spectrogram into two parts (context and target), computing neural representations for each, and training the neural network to predict the target representations from the context representations. We investigate several design choices within this framework and study their influence through extensive experiments by evaluating our models on various audio classification benchmarks, including environmental sounds, speech and music downstream tasks. We focus notably on which part of the input data is used as context or target and show experimentally that it significantly impacts the model's quality. In particular, we notice that some effective design choices in the image domain lead to poor performance on audio, thus highlighting major differences between these two modalities.","sentences":["This paper addresses the problem of self-supervised general-purpose audio representation learning.","We explore the use of Joint-Embedding Predictive Architectures (JEPA) for this task, which consists of splitting an input mel-spectrogram into two parts (context and target), computing neural representations for each, and training the neural network to predict the target representations from the context representations.","We investigate several design choices within this framework and study their influence through extensive experiments by evaluating our models on various audio classification benchmarks, including environmental sounds, speech and music downstream tasks.","We focus notably on which part of the input data is used as context or target and show experimentally that it significantly impacts the model's quality.","In particular, we notice that some effective design choices in the image domain lead to poor performance on audio, thus highlighting major differences between these two modalities."],"url":"http://arxiv.org/abs/2405.08679v1","category":"cs.SD"}
{"created":"2024-05-14 14:55:57","title":"Expensive Multi-Objective Bayesian Optimization Based on Diffusion Models","abstract":"Multi-objective Bayesian optimization (MOBO) has shown promising performance on various expensive multi-objective optimization problems (EMOPs). However, effectively modeling complex distributions of the Pareto optimal solutions is difficult with limited function evaluations. Existing Pareto set learning algorithms may exhibit considerable instability in such expensive scenarios, leading to significant deviations between the obtained solution set and the Pareto set (PS). In this paper, we propose a novel Composite Diffusion Model based Pareto Set Learning algorithm, namely CDM-PSL, for expensive MOBO. CDM-PSL includes both unconditional and conditional diffusion model for generating high-quality samples. Besides, we introduce an information entropy based weighting method to balance different objectives of EMOPs. This method is integrated with the guiding strategy, ensuring that all the objectives are appropriately balanced and given due consideration during the optimization process; Extensive experimental results on both synthetic benchmarks and real-world problems demonstrates that our proposed algorithm attains superior performance compared with various state-of-the-art MOBO algorithms.","sentences":["Multi-objective Bayesian optimization (MOBO) has shown promising performance on various expensive multi-objective optimization problems (EMOPs).","However, effectively modeling complex distributions of the Pareto optimal solutions is difficult with limited function evaluations.","Existing Pareto set learning algorithms may exhibit considerable instability in such expensive scenarios, leading to significant deviations between the obtained solution set and the Pareto set (PS).","In this paper, we propose a novel Composite Diffusion Model based Pareto Set Learning algorithm, namely CDM-PSL, for expensive MOBO.","CDM-PSL includes both unconditional and conditional diffusion model for generating high-quality samples.","Besides, we introduce an information entropy based weighting method to balance different objectives of EMOPs.","This method is integrated with the guiding strategy, ensuring that all the objectives are appropriately balanced and given due consideration during the optimization process; Extensive experimental results on both synthetic benchmarks and real-world problems demonstrates that our proposed algorithm attains superior performance compared with various state-of-the-art MOBO algorithms."],"url":"http://arxiv.org/abs/2405.08674v1","category":"cs.LG"}
{"created":"2024-05-14 14:55:17","title":"Healthy Horndeski gravities with torsion","abstract":"We show that the full Horndeski theory with both curvature and torsion can support nonsingular, stable and subluminal cosmological solutions at all times. Thus, with torsion, the usual No-Go theorem that holds in a curved spacetime is avoided. In particular, it is essential to include the nonminimal derivative couplings of the $\\mathcal{L}_{5}$ part of the Horndeski action ($G^{\\mu\\nu}\\,\\nabla_\\mu \\nabla_\\nu \\phi,$ and $(\\nabla^2 \\phi)^3$). Without the latter a No-Go already impedes the eternal subluminality of nonsingular, stable cosmologies.","sentences":["We show that the full Horndeski theory with both curvature and torsion can support nonsingular, stable and subluminal cosmological solutions at all times.","Thus, with torsion, the usual No-Go theorem that holds in a curved spacetime is avoided.","In particular, it is essential to include the nonminimal derivative couplings of the $\\mathcal{L}_{5}$ part of the Horndeski action ($G^{\\mu\\nu}\\,\\nabla_\\mu \\nabla_\\nu \\phi,$ and $(\\nabla^2 \\phi)^3$).","Without the latter a No-Go already impedes the eternal subluminality of nonsingular, stable cosmologies."],"url":"http://arxiv.org/abs/2405.08673v1","category":"gr-qc"}
{"created":"2024-05-14 14:52:39","title":"Infrared gravity and a celestial obstruction to monogamy constraints","abstract":"We argue that gravitational interactions between particles require a departure from the conventional picture of the quantum state of a multiparticle system in terms of tensor products of one-particle states. This modification is essential in order to accommodate the existence of a new boost-like relativistic angular momentum charge which pairs of particles must carry asymptotically due to long-range effects of gravity. These findings challenge conventional assumptions, prompting a reevaluation of the constraints on quantum entanglement between particle subsystems in a black hole geometry.","sentences":["We argue that gravitational interactions between particles require a departure from the conventional picture of the quantum state of a multiparticle system in terms of tensor products of one-particle states.","This modification is essential in order to accommodate the existence of a new boost-like relativistic angular momentum charge which pairs of particles must carry asymptotically due to long-range effects of gravity.","These findings challenge conventional assumptions, prompting a reevaluation of the constraints on quantum entanglement between particle subsystems in a black hole geometry."],"url":"http://arxiv.org/abs/2405.08670v1","category":"gr-qc"}
{"created":"2024-05-14 14:51:12","title":"Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research","abstract":"Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs. However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia. To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources. By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings. Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations. Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach. Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm. Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry.","sentences":["Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs.","However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia.","To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework.","GDPL facilitates the transfer of VLMs' robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources.","By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings.","Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations.","Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach.","Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm.","Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry."],"url":"http://arxiv.org/abs/2405.08668v1","category":"cs.CV"}
{"created":"2024-05-14 14:50:15","title":"Characterisation and simulation of stitched CMOS strip sensors","abstract":"In high-energy physics, there is a need to investigate alternative silicon sensor concepts that offer cost-efficient, large-area coverage. Sensors based on CMOS imaging technology present such a silicon sensor concept for tracking detectors. The CMOS Strips project investigates passive CMOS strip sensors fabricated by LFoundry in a 150nm technology. By employing the technique of stitching, two different strip sensor formats have been realised. The sensor performance is characterised based on measurements at the DESY II Test Beam Facility. The sensor response was simulated utilising Monte Carlo methods and electric fields provided by TCAD device simulations. This study shows that employing the stitching technique does not affect the hit detection efficiency. A first look at the electric field within the sensor and its impact on generated charge carriers is being discussed.","sentences":["In high-energy physics, there is a need to investigate alternative silicon sensor concepts that offer cost-efficient, large-area coverage.","Sensors based on CMOS imaging technology present such a silicon sensor concept for tracking detectors.","The CMOS Strips project investigates passive CMOS strip sensors fabricated by LFoundry in a 150nm technology.","By employing the technique of stitching, two different strip sensor formats have been realised.","The sensor performance is characterised based on measurements at the DESY II Test Beam Facility.","The sensor response was simulated utilising Monte Carlo methods and electric fields provided by TCAD device simulations.","This study shows that employing the stitching technique does not affect the hit detection efficiency.","A first look at the electric field within the sensor and its impact on generated charge carriers is being discussed."],"url":"http://arxiv.org/abs/2405.08667v1","category":"physics.ins-det"}
{"created":"2024-05-14 14:41:58","title":"Gradient Estimation and Variance Reduction in Stochastic and Deterministic Models","abstract":"It seems that in the current age, computers, computation, and data have an increasingly important role to play in scientific research and discovery. This is reflected in part by the rise of machine learning and artificial intelligence, which have become great areas of interest not just for computer science but also for many other fields of study. More generally, there have been trends moving towards the use of bigger, more complex and higher capacity models. It also seems that stochastic models, and stochastic variants of existing deterministic models, have become important research directions in various fields. For all of these types of models, gradient-based optimization remains as the dominant paradigm for model fitting, control, and more. This dissertation considers unconstrained, nonlinear optimization problems, with a focus on the gradient itself, that key quantity which enables the solution of such problems.   In chapter 1, we introduce the notion of reverse differentiation, a term which describes the body of techniques which enables the efficient computation of gradients. We cover relevant techniques both in the deterministic and stochastic cases. We present a new framework for calculating the gradient of problems which involve both deterministic and stochastic elements. In chapter 2, we analyze the properties of the gradient estimator, with a focus on those properties which are typically assumed in convergence proofs of optimization algorithms. Chapter 3 gives various examples of applying our new gradient estimator. We further explore the idea of working with piecewise continuous models, that is, models with distinct branches and if statements which define what specific branch to use.","sentences":["It seems that in the current age, computers, computation, and data have an increasingly important role to play in scientific research and discovery.","This is reflected in part by the rise of machine learning and artificial intelligence, which have become great areas of interest not just for computer science but also for many other fields of study.","More generally, there have been trends moving towards the use of bigger, more complex and higher capacity models.","It also seems that stochastic models, and stochastic variants of existing deterministic models, have become important research directions in various fields.","For all of these types of models, gradient-based optimization remains as the dominant paradigm for model fitting, control, and more.","This dissertation considers unconstrained, nonlinear optimization problems, with a focus on the gradient itself, that key quantity which enables the solution of such problems.   ","In chapter 1, we introduce the notion of reverse differentiation, a term which describes the body of techniques which enables the efficient computation of gradients.","We cover relevant techniques both in the deterministic and stochastic cases.","We present a new framework for calculating the gradient of problems which involve both deterministic and stochastic elements.","In chapter 2, we analyze the properties of the gradient estimator, with a focus on those properties which are typically assumed in convergence proofs of optimization algorithms.","Chapter 3 gives various examples of applying our new gradient estimator.","We further explore the idea of working with piecewise continuous models, that is, models with distinct branches and if statements which define what specific branch to use."],"url":"http://arxiv.org/abs/2405.08661v1","category":"cs.LG"}
{"created":"2024-05-14 14:36:27","title":"Conformal scattering of the wave equation in the Vaidya spacetime","abstract":"We construct the conformal scattering operator for the scalar wave equation on the Vaidya spacetime using vector field methods. The spacetime we consider is Schwarzschild, near both past and future timelike infinities, in order to use existing decay results for the scalar field, ensuring our energy estimates. These estimates guarantee the injectivity of the trace operator and the closure of its range. Finally, we solve a Goursat problem for the scalar waves on null infinities, demonstrating that the range of the trace operator is dense. Consequently, this implies that the scattering operator is an isomorphism.","sentences":["We construct the conformal scattering operator for the scalar wave equation on the Vaidya spacetime using vector field methods.","The spacetime we consider is Schwarzschild, near both past and future timelike infinities, in order to use existing decay results for the scalar field, ensuring our energy estimates.","These estimates guarantee the injectivity of the trace operator and the closure of its range.","Finally, we solve a Goursat problem for the scalar waves on null infinities, demonstrating that the range of the trace operator is dense.","Consequently, this implies that the scattering operator is an isomorphism."],"url":"http://arxiv.org/abs/2405.08659v1","category":"gr-qc"}
{"created":"2024-05-14 14:35:35","title":"Beyond the Black Box: Do More Complex Models Provide Superior XAI Explanations?","abstract":"The increasing complexity of Artificial Intelligence models poses challenges to interpretability, particularly in the healthcare sector. This study investigates the impact of deep learning model complexity and Explainable AI (XAI) efficacy, utilizing four ResNet architectures (ResNet-18, 34, 50, 101). Through methodical experimentation on 4,369 lung X-ray images of COVID-19-infected and healthy patients, the research evaluates models' classification performance and the relevance of corresponding XAI explanations with respect to the ground-truth disease masks. Results indicate that the increase in model complexity is associated with a decrease in classification accuracy and AUC-ROC scores (ResNet-18: 98.4%, 0.997; ResNet-101: 95.9%, 0.988). Notably, in eleven out of twelve statistical tests performed, no statistically significant differences occurred between XAI quantitative metrics - Relevance Rank Accuracy and the proposed Positive Attribution Ratio - across trained models. These results suggest that increased model complexity does not consistently lead to higher performance or relevance of explanations for models' decision-making processes.","sentences":["The increasing complexity of Artificial Intelligence models poses challenges to interpretability, particularly in the healthcare sector.","This study investigates the impact of deep learning model complexity and Explainable AI (XAI) efficacy, utilizing four ResNet architectures (ResNet-18, 34, 50, 101).","Through methodical experimentation on 4,369 lung X-ray images of COVID-19-infected and healthy patients, the research evaluates models' classification performance and the relevance of corresponding XAI explanations with respect to the ground-truth disease masks.","Results indicate that the increase in model complexity is associated with a decrease in classification accuracy and AUC-ROC scores (ResNet-18: 98.4%, 0.997; ResNet-101: 95.9%, 0.988).","Notably, in eleven out of twelve statistical tests performed, no statistically significant differences occurred between XAI quantitative metrics - Relevance Rank Accuracy and the proposed Positive Attribution Ratio - across trained models.","These results suggest that increased model complexity does not consistently lead to higher performance or relevance of explanations for models' decision-making processes."],"url":"http://arxiv.org/abs/2405.08658v1","category":"eess.IV"}
{"created":"2024-05-14 14:35:12","title":"Unit-Constrained Data-Driven Turbulence Modeling for Separated Flows Using Symbolic Regression","abstract":"The Reynolds-averaged Navier-Stokes (RANS) method is an essential tool for turbulence research in engineering. The RANS method relies on turbulence models to close the governing equations, with Linear Eddy Viscosity Models (LEVMs) based on the Boussinesq hypothesis being prevalently utilized across engineering applications. However, the intrinsic limitations of the Boussinesq assumption render LEVMs less accurate for large separated turbulent flows, prompting the development of numerous modified LEVM variants. Recently, machine learning approaches, notably neural networks, have been employed to enhance LEVMs. Nonetheless, the \"black box\" nature of such machine learning techniques poses significant interpretability challenges in refining turbulence models. This paper introduces a novel unit-constrained turbulence modeling framework using symbolic regression to overcome these hurdles. This framework amends the constitutive equation of original LEVMs by establishing explicit equations between the Reynolds stress deviation and mean flow quantities, with unit consistency constraints bolstering the efficiency of the symbolic regression learning process. The framework's effectiveness is demonstrated through its application to the separated flow over 2D periodic hills. Numerical simulations are conducted to validate the accuracy and generalization capabilities of the learned turbulence model. Compared to the standard k-{\\epsilon} model, the learned model exhibits significantly enhanced predictive accuracy for anisotropic Reynolds stresses and flow velocities, more precisely delineates flow separation, and shows promising generalization potential.","sentences":["The Reynolds-averaged Navier-Stokes (RANS) method is an essential tool for turbulence research in engineering.","The RANS method relies on turbulence models to close the governing equations, with Linear Eddy Viscosity Models (LEVMs) based on the Boussinesq hypothesis being prevalently utilized across engineering applications.","However, the intrinsic limitations of the Boussinesq assumption render LEVMs less accurate for large separated turbulent flows, prompting the development of numerous modified LEVM variants.","Recently, machine learning approaches, notably neural networks, have been employed to enhance LEVMs.","Nonetheless, the \"black box\" nature of such machine learning techniques poses significant interpretability challenges in refining turbulence models.","This paper introduces a novel unit-constrained turbulence modeling framework using symbolic regression to overcome these hurdles.","This framework amends the constitutive equation of original LEVMs by establishing explicit equations between the Reynolds stress deviation and mean flow quantities, with unit consistency constraints bolstering the efficiency of the symbolic regression learning process.","The framework's effectiveness is demonstrated through its application to the separated flow over 2D periodic hills.","Numerical simulations are conducted to validate the accuracy and generalization capabilities of the learned turbulence model.","Compared to the standard k-{\\epsilon} model, the learned model exhibits significantly enhanced predictive accuracy for anisotropic Reynolds stresses and flow velocities, more precisely delineates flow separation, and shows promising generalization potential."],"url":"http://arxiv.org/abs/2405.08656v1","category":"physics.flu-dyn"}
{"created":"2024-05-14 14:34:24","title":"A Distributed Approach to Autonomous Intersection Management via Multi-Agent Reinforcement Learning","abstract":"Autonomous intersection management (AIM) poses significant challenges due to the intricate nature of real-world traffic scenarios and the need for a highly expensive centralised server in charge of simultaneously controlling all the vehicles. This study addresses such issues by proposing a novel distributed approach to AIM utilizing multi-agent reinforcement learning (MARL). We show that by leveraging the 3D surround view technology for advanced assistance systems, autonomous vehicles can accurately navigate intersection scenarios without needing any centralised controller. The contributions of this paper thus include a MARL-based algorithm for the autonomous management of a 4-way intersection and also the introduction of a new strategy called prioritised scenario replay for improved training efficacy. We validate our approach as an innovative alternative to conventional centralised AIM techniques, ensuring the full reproducibility of our results. Specifically, experiments conducted in virtual environments using the SMARTS platform highlight its superiority over benchmarks across various metrics.","sentences":["Autonomous intersection management (AIM) poses significant challenges due to the intricate nature of real-world traffic scenarios and the need for a highly expensive centralised server in charge of simultaneously controlling all the vehicles.","This study addresses such issues by proposing a novel distributed approach to AIM utilizing multi-agent reinforcement learning (MARL).","We show that by leveraging the 3D surround view technology for advanced assistance systems, autonomous vehicles can accurately navigate intersection scenarios without needing any centralised controller.","The contributions of this paper thus include a MARL-based algorithm for the autonomous management of a 4-way intersection and also the introduction of a new strategy called prioritised scenario replay for improved training efficacy.","We validate our approach as an innovative alternative to conventional centralised AIM techniques, ensuring the full reproducibility of our results.","Specifically, experiments conducted in virtual environments using the SMARTS platform highlight its superiority over benchmarks across various metrics."],"url":"http://arxiv.org/abs/2405.08655v1","category":"cs.RO"}
{"created":"2024-05-14 14:32:58","title":"Can we Defend Against the Unknown? An Empirical Study About Threshold Selection for Neural Network Monitoring","abstract":"With the increasing use of neural networks in critical systems, runtime monitoring becomes essential to reject unsafe predictions during inference. Various techniques have emerged to establish rejection scores that maximize the separability between the distributions of safe and unsafe predictions. The efficacy of these approaches is mostly evaluated using threshold-agnostic metrics, such as the area under the receiver operating characteristic curve. However, in real-world applications, an effective monitor also requires identifying a good threshold to transform these scores into meaningful binary decisions. Despite the pivotal importance of threshold optimization, this problem has received little attention. A few studies touch upon this question, but they typically assume that the runtime data distribution mirrors the training distribution, which is a strong assumption as monitors are supposed to safeguard a system against potentially unforeseen threats. In this work, we present rigorous experiments on various image datasets to investigate: 1. The effectiveness of monitors in handling unforeseen threats, which are not available during threshold adjustments. 2. Whether integrating generic threats into the threshold optimization scheme can enhance the robustness of monitors.","sentences":["With the increasing use of neural networks in critical systems, runtime monitoring becomes essential to reject unsafe predictions during inference.","Various techniques have emerged to establish rejection scores that maximize the separability between the distributions of safe and unsafe predictions.","The efficacy of these approaches is mostly evaluated using threshold-agnostic metrics, such as the area under the receiver operating characteristic curve.","However, in real-world applications, an effective monitor also requires identifying a good threshold to transform these scores into meaningful binary decisions.","Despite the pivotal importance of threshold optimization, this problem has received little attention.","A few studies touch upon this question, but they typically assume that the runtime data distribution mirrors the training distribution, which is a strong assumption as monitors are supposed to safeguard a system against potentially unforeseen threats.","In this work, we present rigorous experiments on various image datasets to investigate: 1.","The effectiveness of monitors in handling unforeseen threats, which are not available during threshold adjustments.","2.","Whether integrating generic threats into the threshold optimization scheme can enhance the robustness of monitors."],"url":"http://arxiv.org/abs/2405.08654v1","category":"cs.LG"}
{"created":"2024-05-14 14:25:11","title":"Modeling Realistic Heating Profiles of Transition Region Hot Loops on the Sun: Evidence for Impulsive Heating and Non-equilibrium Ionization","abstract":"The study examines the heating profile of hot solar transition region loops, particularly focusing on transient brightenings observed in IRIS 1400{\\AA} slit-jaw images. The findings challenge the adequacy of simplistic, singular heating mechanisms, revealing that the heating is temporally impulsive and requires a spatially complex profile with multiple heating scales. A forward modeling code is utilized to generate synthetic IRIS emission spectra of these loops based on HYDRAD output, confirming that emitting ions are out of equilibrium. The modeling further indicates that density-dependent dielectronic recombination rates must be included to reproduce the observed line ratios. Collectively, this evidence substantiates that the loops are subject to impulsive heating and that the components of the transiently brightened plasma are driven far from thermal equilibrium. Heating events such as these are ubiquitous in the transition region and the analysis described above provides a robust observational diagnostic tool for characterizing the plasma.","sentences":["The study examines the heating profile of hot solar transition region loops, particularly focusing on transient brightenings observed in IRIS 1400{\\AA} slit-jaw images.","The findings challenge the adequacy of simplistic, singular heating mechanisms, revealing that the heating is temporally impulsive and requires a spatially complex profile with multiple heating scales.","A forward modeling code is utilized to generate synthetic IRIS emission spectra of these loops based on HYDRAD output, confirming that emitting ions are out of equilibrium.","The modeling further indicates that density-dependent dielectronic recombination rates must be included to reproduce the observed line ratios.","Collectively, this evidence substantiates that the loops are subject to impulsive heating and that the components of the transiently brightened plasma are driven far from thermal equilibrium.","Heating events such as these are ubiquitous in the transition region and the analysis described above provides a robust observational diagnostic tool for characterizing the plasma."],"url":"http://arxiv.org/abs/2405.08648v1","category":"astro-ph.SR"}
{"created":"2024-05-14 14:21:43","title":"Thinking Tokens for Language Modeling","abstract":"How much is 56 times 37? Language models often make mistakes in these types of difficult calculations. This is usually explained by their inability to perform complex reasoning. Since language models rely on large training sets and great memorization capability, naturally they are not equipped to run complex calculations. However, one can argue that humans also cannot perform this calculation immediately and require a considerable amount of time to construct the solution. In order to enhance the generalization capability of language models, and as a parallel to human behavior, we propose to use special 'thinking tokens' which allow the model to perform much more calculations whenever a complex problem is encountered.","sentences":["How much is 56 times 37?","Language models often make mistakes in these types of difficult calculations.","This is usually explained by their inability to perform complex reasoning.","Since language models rely on large training sets and great memorization capability, naturally they are not equipped to run complex calculations.","However, one can argue that humans also cannot perform this calculation immediately and require a considerable amount of time to construct the solution.","In order to enhance the generalization capability of language models, and as a parallel to human behavior, we propose to use special 'thinking tokens' which allow the model to perform much more calculations whenever a complex problem is encountered."],"url":"http://arxiv.org/abs/2405.08644v1","category":"cs.CL"}
{"created":"2024-05-14 14:18:38","title":"Upwards homogeneity in iterated symmetric extensions","abstract":"It is sometimes desirable in choiceless constructions of set theory that one iteratively extends some ground model without adding new sets of ordinals after the first extension. Pushing this further, one may wish to have models $V \\subseteq M \\subseteq N$ of $\\mathsf{ZF}$ such that $N$ contains no subsets of $V$ that do not already appear in $M$. We isolate, in the case that $M$ and $N$ are symmetric extensions (particular inner models of a generic extension of $V$), the exact conditions that cause this behaviour and show how it can broadly be applied to many known constructions. We call this behaviour upwards homogeneity.","sentences":["It is sometimes desirable in choiceless constructions of set theory that one iteratively extends some ground model without adding new sets of ordinals after the first extension.","Pushing this further, one may wish to have models $V \\subseteq M","\\subseteq N$ of $\\mathsf{ZF}$ such that $N$ contains no subsets of $V$ that do not already appear in $M$. We isolate, in the case that $M$ and $N$ are symmetric extensions (particular inner models of a generic extension of $V$), the exact conditions that cause this behaviour and show how it can broadly be applied to many known constructions.","We call this behaviour upwards homogeneity."],"url":"http://arxiv.org/abs/2405.08639v1","category":"math.LO"}
{"created":"2024-05-14 14:14:23","title":"Optimal design of experiments in the context of machine-learning inter-atomic potentials: improving the efficiency and transferability of kernel based methods","abstract":"Data-driven, machine learning (ML) models of atomistic interactions are often based on flexible and non-physical functions that can relate nuanced aspects of atomic arrangements into predictions of energies and forces. As a result, these potentials are as good as the training data (usually results of so-called ab initio simulations) and we need to make sure that we have enough information for a model to become sufficiently accurate, reliable and transferable. The main challenge stems from the fact that descriptors of chemical environments are often sparse high-dimensional objects without a well-defined continuous metric. Therefore, it is rather unlikely that any ad hoc method of choosing training examples will be indiscriminate, and it will be easy to fall into the trap of confirmation bias, where the same narrow and biased sampling is used to generate train- and test- sets. We will demonstrate that classical concepts of statistical planning of experiments and optimal design can help to mitigate such problems at a relatively low computational cost. The key feature of the method we will investigate is that they allow us to assess the informativeness of data (how much we can improve the model by adding/swapping a training example) and verify if the training is feasible with the current set before obtaining any reference energies and forces -- a so-called off-line approach. In other words, we are focusing on an approach that is easy to implement and doesn't require sophisticated frameworks that involve automated access to high-performance computational (HPC).","sentences":["Data-driven, machine learning (ML) models of atomistic interactions are often based on flexible and non-physical functions that can relate nuanced aspects of atomic arrangements into predictions of energies and forces.","As a result, these potentials are as good as the training data (usually results of so-called ab initio simulations) and we need to make sure that we have enough information for a model to become sufficiently accurate, reliable and transferable.","The main challenge stems from the fact that descriptors of chemical environments are often sparse high-dimensional objects without a well-defined continuous metric.","Therefore, it is rather unlikely that any ad hoc method of choosing training examples will be indiscriminate, and it will be easy to fall into the trap of confirmation bias, where the same narrow and biased sampling is used to generate train- and test- sets.","We will demonstrate that classical concepts of statistical planning of experiments and optimal design can help to mitigate such problems at a relatively low computational cost.","The key feature of the method we will investigate is that they allow us to assess the informativeness of data (how much we can improve the model by adding/swapping a training example) and verify if the training is feasible with the current set before obtaining any reference energies and forces -- a so-called off-line approach.","In other words, we are focusing on an approach that is easy to implement and doesn't require sophisticated frameworks that involve automated access to high-performance computational (HPC)."],"url":"http://arxiv.org/abs/2405.08636v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 14:13:10","title":"Stabilization of Integral Delay Equations by solving Fredholm equations","abstract":"In this paper, we design a stabilizing state-feedback control law for a system represented by a general class of integral delay equations subject to a pointwise and distributed input delay. The proposed controller is defined in terms of integrals of the state and input history over a fixed-length time window. We show that the closed-loop stability is guaranteed, provided the controller integral kernels are solutions to a set of Fredholm equations. The existence of solutions is guaranteed under an appropriate spectral controllability assumption, resulting in an implementable stabilizing control law. The proposed methodology appears simpler and more general compared to existing results in the literature. In particular, under additional regularity assumptions, the proposed approach can be expanded to address the degenerate case where only a distributed control term is present.","sentences":["In this paper, we design a stabilizing state-feedback control law for a system represented by a general class of integral delay equations subject to a pointwise and distributed input delay.","The proposed controller is defined in terms of integrals of the state and input history over a fixed-length time window.","We show that the closed-loop stability is guaranteed, provided the controller integral kernels are solutions to a set of Fredholm equations.","The existence of solutions is guaranteed under an appropriate spectral controllability assumption, resulting in an implementable stabilizing control law.","The proposed methodology appears simpler and more general compared to existing results in the literature.","In particular, under additional regularity assumptions, the proposed approach can be expanded to address the degenerate case where only a distributed control term is present."],"url":"http://arxiv.org/abs/2405.08634v1","category":"math.DS"}
{"created":"2024-05-14 14:10:48","title":"A Fast and Scalable Pathwise-Solver for Group Lasso and Elastic Net Penalized Regression via Block-Coordinate Descent","abstract":"We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path. Special attention is given when the loss is the usual least squares loss (Gaussian loss). We show that each block-coordinate update can be solved efficiently using Newton's method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate. Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets. Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet.","sentences":["We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path.","Special attention is given when the loss is the usual least squares loss (Gaussian loss).","We show that each block-coordinate update can be solved efficiently using Newton's method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate.","Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets.","Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet."],"url":"http://arxiv.org/abs/2405.08631v1","category":"stat.CO"}
{"created":"2024-05-14 14:08:55","title":"Beyond Quantum Annealing: Optimal control solutions to MaxCut problems","abstract":"Quantum Annealing (QA) relies on mixing two Hamiltonian terms, a simple driver and a complex problem Hamiltonian, in a linear combination. The time-dependent schedule for this mixing is often taken to be linear in time: improving on this linear choice is known to be essential and has proven to be difficult. Here, we present different techniques for improving on the linear-schedule QA along two directions, conceptually distinct but leading to similar outcomes: 1) the first approach consists of constructing a Trotter-digitized QA (dQA) with schedules parameterized in terms of Fourier modes or Chebyshev polynomials, inspired by the Chopped Random Basis algorithm (CRAB) for optimal control in continuous time; 2) the second approach is technically a Quantum Approximate Optimization Algorithm (QAOA), whose solutions are found iteratively using linear interpolation or expansion in Fourier modes. Both approaches emphasize finding smooth optimal schedule parameters, ultimately leading to hybrid quantum-classical variational algorithms of the alternating Hamiltonian Ansatz type. We apply these techniques to MaxCut problems on weighted 3-regular graphs with N = 14 sites, focusing on hard instances that exhibit a small spectral gap, for which a standard linear-schedule QA performs poorly. We characterize the physics behind the optimal protocols for both the dQA and QAOA approaches, discovering shortcuts to adiabaticity-like dynamics. Furthermore, we study the transferability of such smooth solutions among hard instances of MaxCut at different circuit depths. Finally, we show that the smoothness pattern of these protocols obtained in a digital setting enables us to adapt them to continuous-time evolution, contrarily to generic non-smooth solutions. This procedure results in an optimized quantum annealing schedule that is implementable on analog devices.","sentences":["Quantum Annealing (QA) relies on mixing two Hamiltonian terms, a simple driver and a complex problem Hamiltonian, in a linear combination.","The time-dependent schedule for this mixing is often taken to be linear in time: improving on this linear choice is known to be essential and has proven to be difficult.","Here, we present different techniques for improving on the linear-schedule QA along two directions, conceptually distinct but leading to similar outcomes: 1) the first approach consists of constructing a Trotter-digitized QA (dQA) with schedules parameterized in terms of Fourier modes or Chebyshev polynomials, inspired by the Chopped Random Basis algorithm (CRAB) for optimal control in continuous time; 2) the second approach is technically a Quantum Approximate Optimization Algorithm (QAOA), whose solutions are found iteratively using linear interpolation or expansion in Fourier modes.","Both approaches emphasize finding smooth optimal schedule parameters, ultimately leading to hybrid quantum-classical variational algorithms of the alternating Hamiltonian Ansatz type.","We apply these techniques to MaxCut problems on weighted 3-regular graphs with N = 14 sites, focusing on hard instances that exhibit a small spectral gap, for which a standard linear-schedule QA performs poorly.","We characterize the physics behind the optimal protocols for both the dQA and QAOA approaches, discovering shortcuts to adiabaticity-like dynamics.","Furthermore, we study the transferability of such smooth solutions among hard instances of MaxCut at different circuit depths.","Finally, we show that the smoothness pattern of these protocols obtained in a digital setting enables us to adapt them to continuous-time evolution, contrarily to generic non-smooth solutions.","This procedure results in an optimized quantum annealing schedule that is implementable on analog devices."],"url":"http://arxiv.org/abs/2405.08630v1","category":"quant-ph"}
{"created":"2024-05-14 14:07:49","title":"Improving pulsar timing precision through superior Time-of-Arrival creation","abstract":"The measurement of pulsar pulse times-of-arrival (ToAs) is a crucial step in detecting low-frequency gravitational waves. To determine ToAs, we can use template-matching to compare each observed pulse profile with a standard template. However, using different combinations of templates and template-matching methods (TMMs) without careful consideration may lead to inconsistent results. In pulsar timing array (PTA) experiments, distinct ToAs from the same observations can be obtained, due to the use of diverse templates and TMMs. In other words, employing diverse approaches can yield different timing results and would thus have a significant impact on subsequent gravitational wave searches. In this paper, we examine several commonly used combinations to analyze their effect on pulse ToAs. we evaluate the potential impact of template and TMM selection on thirteen typical millisecond pulsars within the European PTA. We employ pulsar timing methods, specifically the root mean square and reduced chi-square $\\chi_r^2$ of the residuals of the best timing solution to assess the outcomes. Additionally, we evaluate the system-limited noise floor (SLNF) for each pulsar at various telescopes operating around 1.4~GHz using frequency-resolved templates.   Our findings suggest that utilizing data-derived and smoothed templates in conjunction with the Fourier-domain with Markov-chain Monte Carlo (FDM) TMM is generally the most effective approach, though there may be exceptions that require further attention. Furthermore, we determine that pulse phase jitter noise does not significantly limit the current precision of the European PTA's timing, as jitter levels derived from other studies are much smaller than the SLNF.","sentences":["The measurement of pulsar pulse times-of-arrival (ToAs) is a crucial step in detecting low-frequency gravitational waves.","To determine ToAs, we can use template-matching to compare each observed pulse profile with a standard template.","However, using different combinations of templates and template-matching methods (TMMs) without careful consideration may lead to inconsistent results.","In pulsar timing array (PTA) experiments, distinct ToAs from the same observations can be obtained, due to the use of diverse templates and TMMs.","In other words, employing diverse approaches can yield different timing results and would thus have a significant impact on subsequent gravitational wave searches.","In this paper, we examine several commonly used combinations to analyze their effect on pulse ToAs.","we evaluate the potential impact of template and TMM selection on thirteen typical millisecond pulsars within the European PTA.","We employ pulsar timing methods, specifically the root mean square and reduced chi-square $\\chi_r^2$ of the residuals of the best timing solution to assess the outcomes.","Additionally, we evaluate the system-limited noise floor (SLNF) for each pulsar at various telescopes operating around 1.4~GHz using frequency-resolved templates.   ","Our findings suggest that utilizing data-derived and smoothed templates in conjunction with the Fourier-domain with Markov-chain Monte Carlo (FDM) TMM is generally the most effective approach, though there may be exceptions that require further attention.","Furthermore, we determine that pulse phase jitter noise does not significantly limit the current precision of the European PTA's timing, as jitter levels derived from other studies are much smaller than the SLNF."],"url":"http://arxiv.org/abs/2405.08629v1","category":"astro-ph.IM"}
{"created":"2024-05-14 14:06:26","title":"Literature Review on Maneuver-Based Scenario Description for Automated Driving Simulations","abstract":"The increasing complexity of automated driving functions and their growing operational design domains imply more demanding requirements on their validation. Classical methods such as field tests or formal analyses are not sufficient anymore and need to be complemented by simulations. For simulations, the standard approach is scenario-based testing, as opposed to distance-based testing primarily performed in field tests. Currently, the time evolution of specific scenarios is mainly described using trajectories, which limit or at least hamper generalizations towards variations. As an alternative, maneuver-based approaches have been proposed. We shed light on the state of the art and available foundations for this new method through a literature review of early and recent works related to maneuver-based scenario description. It includes related modeling approaches originally developed for other applications. Current limitations and research gaps are identified.","sentences":["The increasing complexity of automated driving functions and their growing operational design domains imply more demanding requirements on their validation.","Classical methods such as field tests or formal analyses are not sufficient anymore and need to be complemented by simulations.","For simulations, the standard approach is scenario-based testing, as opposed to distance-based testing primarily performed in field tests.","Currently, the time evolution of specific scenarios is mainly described using trajectories, which limit or at least hamper generalizations towards variations.","As an alternative, maneuver-based approaches have been proposed.","We shed light on the state of the art and available foundations for this new method through a literature review of early and recent works related to maneuver-based scenario description.","It includes related modeling approaches originally developed for other applications.","Current limitations and research gaps are identified."],"url":"http://arxiv.org/abs/2405.08626v1","category":"cs.RO"}
{"created":"2024-05-14 14:04:03","title":"Optimal Almost-Balanced Sequences","abstract":"This paper presents a novel approach to address the constrained coding challenge of generating almost-balanced sequences. While strictly balanced sequences have been well studied in the past, the problem of designing efficient algorithms with small redundancy, preferably constant or even a single bit, for almost balanced sequences has remained unsolved. A sequence is $\\varepsilon(n)$-almost balanced if its Hamming weight is between $0.5n\\pm \\varepsilon(n)$. It is known that for any algorithm with a constant number of bits, $\\varepsilon(n)$ has to be in the order of $\\Theta(\\sqrt{n})$, with $O(n)$ average time complexity. However, prior solutions with a single redundancy bit required $\\varepsilon(n)$ to be a linear shift from $n/2$. Employing an iterative method and arithmetic coding, our emphasis lies in constructing almost balanced codes with a single redundancy bit. Notably, our method surpasses previous approaches by achieving the optimal balanced order of $\\Theta(\\sqrt{n})$. Additionally, we extend our method to the non-binary case considering $q$-ary almost polarity-balanced sequences for even $q$, and almost symbol-balanced for $q=4$. Our work marks the first asymptotically optimal solutions for almost-balanced sequences, for both, binary and non-binary alphabet.","sentences":["This paper presents a novel approach to address the constrained coding challenge of generating almost-balanced sequences.","While strictly balanced sequences have been well studied in the past, the problem of designing efficient algorithms with small redundancy, preferably constant or even a single bit, for almost balanced sequences has remained unsolved.","A sequence is $\\varepsilon(n)$-almost balanced if its Hamming weight is between $0.5n\\pm \\varepsilon(n)$. It is known that for any algorithm with a constant number of bits, $\\varepsilon(n)$ has to be in the order of $\\Theta(\\sqrt{n})$, with $O(n)$ average time complexity.","However, prior solutions with a single redundancy bit required $\\varepsilon(n)$ to be a linear shift from $n/2$. Employing an iterative method and arithmetic coding, our emphasis lies in constructing almost balanced codes with a single redundancy bit.","Notably, our method surpasses previous approaches by achieving the optimal balanced order of $\\Theta(\\sqrt{n})$. Additionally, we extend our method to the non-binary case considering $q$-ary almost polarity-balanced sequences for even $q$, and almost symbol-balanced for $q=4$. Our work marks the first asymptotically optimal solutions for almost-balanced sequences, for both, binary and non-binary alphabet."],"url":"http://arxiv.org/abs/2405.08625v1","category":"cs.IT"}
{"created":"2024-05-14 14:03:00","title":"Primordial black holes, a small review","abstract":"With the direct discovery of gravitational waves, black holes have regain interest in the recent years. In particular primordial black holes (PBHs), which originate from the very early Universe, may constitute (at least in part) dark matter. The possibility that dark matter is made of black holes is particularly appealing, and multi-messenger searches are important to probe this hypothesis. In this paper I will discuss the concept of primordial black holes, their origins, their characteristics and the current constraints. In addition I will explain that the study of black holes is of utmost interest since they may constitute portals to new physics and to quantum gravity.","sentences":["With the direct discovery of gravitational waves, black holes have regain interest in the recent years.","In particular primordial black holes (PBHs), which originate from the very early Universe, may constitute (at least in part) dark matter.","The possibility that dark matter is made of black holes is particularly appealing, and multi-messenger searches are important to probe this hypothesis.","In this paper I will discuss the concept of primordial black holes, their origins, their characteristics and the current constraints.","In addition I will explain that the study of black holes is of utmost interest since they may constitute portals to new physics and to quantum gravity."],"url":"http://arxiv.org/abs/2405.08624v1","category":"gr-qc"}
{"created":"2024-05-14 14:01:15","title":"RMT-BVQA: Recurrent Memory Transformer-based Blind Video Quality Assessment for Enhanced Video Content","abstract":"With recent advances in deep learning, numerous algorithms have been developed to enhance video quality, reduce visual artefacts and improve perceptual quality. However, little research has been reported on the quality assessment of enhanced content - the evaluation of enhancement methods is often based on quality metrics that were designed for compression applications. In this paper, we propose a novel blind deep video quality assessment (VQA) method specifically for enhanced video content. It employs a new Recurrent Memory Transformer (RMT) based network architecture to obtain video quality representations, which is optimised through a novel content-quality-aware contrastive learning strategy based on a new database containing 13K training patches with enhanced content. The extracted quality representations are then combined through linear regression to generate video-level quality indices. The proposed method, RMT-BVQA, has been evaluated on the VDPVE (VQA Dataset for Perceptual Video Enhancement) database through a five-fold cross validation. The results show its superior correlation performance when compared to ten existing no-reference quality metrics.","sentences":["With recent advances in deep learning, numerous algorithms have been developed to enhance video quality, reduce visual artefacts and improve perceptual quality.","However, little research has been reported on the quality assessment of enhanced content - the evaluation of enhancement methods is often based on quality metrics that were designed for compression applications.","In this paper, we propose a novel blind deep video quality assessment (VQA) method specifically for enhanced video content.","It employs a new Recurrent Memory Transformer (RMT) based network architecture to obtain video quality representations, which is optimised through a novel content-quality-aware contrastive learning strategy based on a new database containing 13K training patches with enhanced content.","The extracted quality representations are then combined through linear regression to generate video-level quality indices.","The proposed method, RMT-BVQA, has been evaluated on the VDPVE (VQA Dataset for Perceptual Video Enhancement) database through a five-fold cross validation.","The results show its superior correlation performance when compared to ten existing no-reference quality metrics."],"url":"http://arxiv.org/abs/2405.08621v1","category":"eess.IV"}
{"created":"2024-05-14 14:00:53","title":"Ruijsenaars duality for B, C, D Toda chains","abstract":"We use the Hamiltonian reduction method to construct the Ruijsenaars dual systems to generalized Toda chains associated with the classical Lie algebras of types $B, C, D$. The dual systems turn out to be the $B, C$ and $D$ analogues of the rational Goldfish model, which is, as in the type $A$ case, the strong coupling limit of rational Ruijsenaars systems. We explain how both types of systems emerge in the reduction of the cotangent bundle of a Lie group and provide the formulae for dual Hamiltonians. We compute explicitly the higher Hamiltonians of Goldfish models using the Cauchy--Binet theorem.","sentences":["We use the Hamiltonian reduction method to construct the Ruijsenaars dual systems to generalized Toda chains associated with the classical Lie algebras of types $B, C, D$.","The dual systems turn out to be the $B, C$ and $D$ analogues of the rational Goldfish model, which is, as in the type $A$ case, the strong coupling limit of rational Ruijsenaars systems.","We explain how both types of systems emerge in the reduction of the cotangent bundle of a Lie group and provide the formulae for dual Hamiltonians.","We compute explicitly the higher Hamiltonians of Goldfish models using the Cauchy--Binet theorem."],"url":"http://arxiv.org/abs/2405.08620v1","category":"math-ph"}
{"created":"2024-05-14 13:59:24","title":"ALMol: Aligned Language-Molecule Translation LLMs through Offline Preference Contrastive Optimisation","abstract":"The field of chemistry and Artificial Intelligence (AI) intersection is an area of active research that aims to accelerate scientific discovery. The integration of large language models (LLMs) with scientific modalities has shown significant promise in this endeavour. However, challenges persist in effectively addressing training efficacy and the out-of-distribution problem, particularly as existing approaches rely on larger models and datasets. In this context, we focus on machine language-molecule translation and deploy a novel training approach called contrastive preference optimisation, which avoids generating translations that are merely adequate but not perfect. To ensure generalisability and mitigate memorisation effects, we conduct experiments using only 10\\% of the data. Our results demonstrate that our models achieve up to a 32\\% improvement compared to counterpart models. We also introduce a scalable fine-grained evaluation methodology that accommodates responsibility.","sentences":["The field of chemistry and Artificial Intelligence (AI) intersection is an area of active research that aims to accelerate scientific discovery.","The integration of large language models (LLMs) with scientific modalities has shown significant promise in this endeavour.","However, challenges persist in effectively addressing training efficacy and the out-of-distribution problem, particularly as existing approaches rely on larger models and datasets.","In this context, we focus on machine language-molecule translation and deploy a novel training approach called contrastive preference optimisation, which avoids generating translations that are merely adequate but not perfect.","To ensure generalisability and mitigate memorisation effects, we conduct experiments using only 10\\% of the data.","Our results demonstrate that our models achieve up to a 32\\% improvement compared to counterpart models.","We also introduce a scalable fine-grained evaluation methodology that accommodates responsibility."],"url":"http://arxiv.org/abs/2405.08619v2","category":"cs.CL"}
{"created":"2024-05-14 13:49:46","title":"Gamma-protocol for secure transmission of information","abstract":"Secure communication that allows only the sender and intended recipient of a message to view its content has a long history. Quantum objects, such as single photons are ideal carriers for secure information transmission because, according to the no-cloning theorem [1], it is impossible to create an identical and independent copy of an arbitrary quantum state while its detection leads to the information distortion. BB84 [2,3] is the first quantum cryptography protocol for a quantum key generation and distribution, based on single photon sources. This quantum key is used for coding and decoding of classical information. We propose completely different protocol based on a stochastic decay of an ensemble of radioactive nuclei randomly emitting a stream of gamma-photons. We experimentally demonstrate a method how to transmit classical information containing binary bits (0 or 1) with the help of this stream. Transmission is organized such that eavesdropping is impossible since the presence of information in the stream of randomly emitted gamma-photons can be hidden. Reading of this information needs precise knowledge of the repetition rate of its sending in advance. It is unrealistic for the eavesdropper to disclose this rate, and without knowledge of this parameter it is impossible to make the transmitted information visible.","sentences":["Secure communication that allows only the sender and intended recipient of a message to view its content has a long history.","Quantum objects, such as single photons are ideal carriers for secure information transmission because, according to the no-cloning theorem [1], it is impossible to create an identical and independent copy of an arbitrary quantum state while its detection leads to the information distortion.","BB84 [2,3] is the first quantum cryptography protocol for a quantum key generation and distribution, based on single photon sources.","This quantum key is used for coding and decoding of classical information.","We propose completely different protocol based on a stochastic decay of an ensemble of radioactive nuclei randomly emitting a stream of gamma-photons.","We experimentally demonstrate a method how to transmit classical information containing binary bits (0 or 1) with the help of this stream.","Transmission is organized such that eavesdropping is impossible since the presence of information in the stream of randomly emitted gamma-photons can be hidden.","Reading of this information needs precise knowledge of the repetition rate of its sending in advance.","It is unrealistic for the eavesdropper to disclose this rate, and without knowledge of this parameter it is impossible to make the transmitted information visible."],"url":"http://arxiv.org/abs/2405.08610v1","category":"quant-ph"}
{"created":"2024-05-14 13:42:47","title":"On gradient estimates of the heat semigroups on step-two Carnot groups","abstract":"In this work, we give a sufficient condition for a step-two Carnot group to satisfy the quasi Bakry-\\'Emery curvature condition. As an application, we establish the gradient estimate for the heat semigroup on the free step-two Carnot group with three generators $N_{3,2}$. Moreover, high order gradient estimates and the Riemannian counterparts are also deduced under an extra condition.","sentences":["In this work, we give a sufficient condition for a step-two Carnot group to satisfy the quasi Bakry-\\'Emery curvature condition.","As an application, we establish the gradient estimate for the heat semigroup on the free step-two Carnot group with three generators $N_{3,2}$.","Moreover, high order gradient estimates and the Riemannian counterparts are also deduced under an extra condition."],"url":"http://arxiv.org/abs/2405.08605v1","category":"math.AP"}
{"created":"2024-05-14 13:42:19","title":"Towards Geometry-Aware Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization","abstract":"Multi-objective combinatorial optimization (MOCO) problems are prevalent in various real-world applications. Most existing neural methods for MOCO problems rely solely on decomposition and utilize precise hypervolume to enhance diversity. However, these methods often approximate only limited regions of the Pareto front and spend excessive time on diversity enhancement because of ambiguous decomposition and time-consuming hypervolume calculation. To address these limitations, we design a Geometry-Aware Pareto set Learning algorithm named GAPL, which provides a novel geometric perspective for neural MOCO via a Pareto attention model based on hypervolume expectation maximization. In addition, we propose a hypervolume residual update strategy to enable the Pareto attention model to capture both local and non-local information of the Pareto set/front. We also design a novel inference approach to further improve quality of the solution set and speed up hypervolume calculation and local subset selection. Experimental results on three classic MOCO problems demonstrate that our GAPL outperforms state-of-the-art neural baselines via superior decomposition and efficient diversity enhancement.","sentences":["Multi-objective combinatorial optimization (MOCO) problems are prevalent in various real-world applications.","Most existing neural methods for MOCO problems rely solely on decomposition and utilize precise hypervolume to enhance diversity.","However, these methods often approximate only limited regions of the Pareto front and spend excessive time on diversity enhancement because of ambiguous decomposition and time-consuming hypervolume calculation.","To address these limitations, we design a Geometry-Aware Pareto set Learning algorithm named GAPL, which provides a novel geometric perspective for neural MOCO via a Pareto attention model based on hypervolume expectation maximization.","In addition, we propose a hypervolume residual update strategy to enable the Pareto attention model to capture both local and non-local information of the Pareto set/front.","We also design a novel inference approach to further improve quality of the solution set and speed up hypervolume calculation and local subset selection.","Experimental results on three classic MOCO problems demonstrate that our GAPL outperforms state-of-the-art neural baselines via superior decomposition and efficient diversity enhancement."],"url":"http://arxiv.org/abs/2405.08604v1","category":"cs.LG"}
{"created":"2024-05-14 13:42:05","title":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine","abstract":"Since the release of ChatGPT and GPT-4, large language models (LLMs) and multimodal large language models (MLLMs) have garnered significant attention due to their powerful and general capabilities in understanding, reasoning, and generation, thereby offering new paradigms for the integration of artificial intelligence with medicine. This survey comprehensively overviews the development background and principles of LLMs and MLLMs, as well as explores their application scenarios, challenges, and future directions in medicine. Specifically, this survey begins by focusing on the paradigm shift, tracing the evolution from traditional models to LLMs and MLLMs, summarizing the model structures to provide detailed foundational knowledge. Subsequently, the survey details the entire process from constructing and evaluating to using LLMs and MLLMs with a clear logic. Following this, to emphasize the significant value of LLMs and MLLMs in healthcare, we survey and summarize 6 promising applications in healthcare. Finally, the survey discusses the challenges faced by medical LLMs and MLLMs and proposes a feasible approach and direction for the subsequent integration of artificial intelligence with medicine. Thus, this survey aims to provide researchers with a valuable and comprehensive reference guide from the perspectives of the background, principles, and clinical applications of LLMs and MLLMs.","sentences":["Since the release of ChatGPT and GPT-4, large language models (LLMs) and multimodal large language models (MLLMs) have garnered significant attention due to their powerful and general capabilities in understanding, reasoning, and generation, thereby offering new paradigms for the integration of artificial intelligence with medicine.","This survey comprehensively overviews the development background and principles of LLMs and MLLMs, as well as explores their application scenarios, challenges, and future directions in medicine.","Specifically, this survey begins by focusing on the paradigm shift, tracing the evolution from traditional models to LLMs and MLLMs, summarizing the model structures to provide detailed foundational knowledge.","Subsequently, the survey details the entire process from constructing and evaluating to using LLMs and MLLMs with a clear logic.","Following this, to emphasize the significant value of LLMs and MLLMs in healthcare, we survey and summarize 6 promising applications in healthcare.","Finally, the survey discusses the challenges faced by medical LLMs and MLLMs and proposes a feasible approach and direction for the subsequent integration of artificial intelligence with medicine.","Thus, this survey aims to provide researchers with a valuable and comprehensive reference guide from the perspectives of the background, principles, and clinical applications of LLMs and MLLMs."],"url":"http://arxiv.org/abs/2405.08603v1","category":"cs.CL"}
{"created":"2024-05-14 13:39:19","title":"The distributed biased min-consensus protocol revisited: pre-specified finite time control strategies and small-gain based analysis","abstract":"Unlike the classical distributed consensus protocols enabling the group of agents as a whole to reach an agreement regarding a certain quantity of interest in a distributed fashion, the distributed biased min-consensus protocol (DBMC) has been proven to generate advanced complexity pertaining to solving the shortest path problem. As such a protocol is commonly incorporated as the first step of a hierarchical architecture in real applications, e.g., robots path planning, management of dispersed computing services, an impedance limiting the application potential of DBMC lies in, the lack of results regarding to its convergence within a user-assigned time. In this paper, we first propose two control strategies ensuring the state error of DBMC decrease exactly to zero or a desired level manipulated by the user, respectively. To compensate the high feedback gains incurred by these two control strategies, this paper further investigates the nominal DBMC itself. By leveraging small gain based stability tools, this paper also proves the global exponential input-to-state stability of DBMC, outperforming its current stability results. Simulations have been provided to validate the efficacy of our theoretical result.","sentences":["Unlike the classical distributed consensus protocols enabling the group of agents as a whole to reach an agreement regarding a certain quantity of interest in a distributed fashion, the distributed biased min-consensus protocol (DBMC) has been proven to generate advanced complexity pertaining to solving the shortest path problem.","As such a protocol is commonly incorporated as the first step of a hierarchical architecture in real applications, e.g., robots path planning, management of dispersed computing services, an impedance limiting the application potential of DBMC lies in, the lack of results regarding to its convergence within a user-assigned time.","In this paper, we first propose two control strategies ensuring the state error of DBMC decrease exactly to zero or a desired level manipulated by the user, respectively.","To compensate the high feedback gains incurred by these two control strategies, this paper further investigates the nominal DBMC itself.","By leveraging small gain based stability tools, this paper also proves the global exponential input-to-state stability of DBMC, outperforming its current stability results.","Simulations have been provided to validate the efficacy of our theoretical result."],"url":"http://arxiv.org/abs/2405.08599v1","category":"eess.SY"}
{"created":"2024-05-14 13:37:36","title":"Risks and Opportunities of Open-Source Generative AI","abstract":"Applications of Generative AI (Gen AI) are expected to revolutionize a number of different areas, ranging from science & medicine to education. The potential for these seismic changes has triggered a lively debate about the potential risks of the technology, and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development. This regulation is likely to put at risk the budding field of open-source generative AI. Using a three-stage framework for Gen AI development (near, mid and long-term), we analyze the risks and opportunities of open-source generative AI models with similar capabilities to the ones currently available (near to mid-term) and with greater capabilities (long-term). We argue that, overall, the benefits of open-source Gen AI outweigh its risks. As such, we encourage the open sourcing of models, training and evaluation data, and provide a set of recommendations and best practices for managing risks associated with open-source generative AI.","sentences":["Applications of Generative AI (Gen AI) are expected to revolutionize a number of different areas, ranging from science & medicine to education.","The potential for these seismic changes has triggered a lively debate about the potential risks of the technology, and resulted in calls for tighter regulation, in particular from some of the major tech companies who are leading in AI development.","This regulation is likely to put at risk the budding field of open-source generative AI.","Using a three-stage framework for Gen AI development (near, mid and long-term), we analyze the risks and opportunities of open-source generative AI models with similar capabilities to the ones currently available (near to mid-term) and with greater capabilities (long-term).","We argue that, overall, the benefits of open-source Gen AI outweigh its risks.","As such, we encourage the open sourcing of models, training and evaluation data, and provide a set of recommendations and best practices for managing risks associated with open-source generative AI."],"url":"http://arxiv.org/abs/2405.08597v1","category":"cs.LG"}
{"created":"2024-05-14 13:37:13","title":"EVDA: Evolving Deepfake Audio Detection Continual Learning Benchmark","abstract":"The rise of advanced large language models such as GPT-4, GPT-4o, and the Claude family has made fake audio detection increasingly challenging. Traditional fine-tuning methods struggle to keep pace with the evolving landscape of synthetic speech, necessitating continual learning approaches that can adapt to new audio while retaining the ability to detect older types. Continual learning, which acts as an effective tool for detecting newly emerged deepfake audio while maintaining performance on older types, lacks a well-constructed and user-friendly evaluation framework. To address this gap, we introduce EVDA, a benchmark for evaluating continual learning methods in deepfake audio detection. EVDA includes classic datasets from the Anti-Spoofing Voice series, Chinese fake audio detection series, and newly generated deepfake audio from models like GPT-4 and GPT-4o. It supports various continual learning techniques, such as Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), and recent methods like Regularized Adaptive Weight Modification (RAWM) and Radian Weight Modification (RWM). Additionally, EVDA facilitates the development of robust algorithms by providing an open interface for integrating new continual learning methods","sentences":["The rise of advanced large language models such as GPT-4, GPT-4o, and the Claude family has made fake audio detection increasingly challenging.","Traditional fine-tuning methods struggle to keep pace with the evolving landscape of synthetic speech, necessitating continual learning approaches that can adapt to new audio while retaining the ability to detect older types.","Continual learning, which acts as an effective tool for detecting newly emerged deepfake audio while maintaining performance on older types, lacks a well-constructed and user-friendly evaluation framework.","To address this gap, we introduce EVDA, a benchmark for evaluating continual learning methods in deepfake audio detection.","EVDA includes classic datasets from the Anti-Spoofing Voice series, Chinese fake audio detection series, and newly generated deepfake audio from models like GPT-4 and GPT-4o.","It supports various continual learning techniques, such as Elastic Weight Consolidation (EWC), Learning without Forgetting (LwF), and recent methods like Regularized Adaptive Weight Modification (RAWM) and Radian Weight Modification (RWM).","Additionally, EVDA facilitates the development of robust algorithms by providing an open interface for integrating new continual learning methods"],"url":"http://arxiv.org/abs/2405.08596v2","category":"cs.SD"}
{"created":"2024-05-14 13:36:51","title":"Online busy time scheduling with flexible jobs","abstract":"We present several competitive ratios for the online busy time scheduling problem with flexible jobs. The busy time scheduling problem is a fundamental scheduling problem motivated by energy efficiency with the goal of minimizing the total time that machines with multiple processors are enabled. In the busy time scheduling problem, an unbounded number of machines is given, where each machine has $g$ processors. No more than $g$ jobs can be scheduled simultaneously on each machine. A machine consumes energy whenever at least one job is scheduled at any time on the machine. Scheduling a single job at some time $t$ consumes the same amount of energy as scheduling $g$ jobs at time $t$. In the online setting, jobs are revealed when they are released.   We consider the cases where $g$ is unbounded and bounded. In this paper, we revisit the bounds of the unbounded general setting from the literature and tighten it significantly. We also consider agreeable jobs. For the bounded setting, we show a tightened upper bound. Furthermore, we show the first constant competitive ratio in the bounded setting that does not require lookahead.","sentences":["We present several competitive ratios for the online busy time scheduling problem with flexible jobs.","The busy time scheduling problem is a fundamental scheduling problem motivated by energy efficiency with the goal of minimizing the total time that machines with multiple processors are enabled.","In the busy time scheduling problem, an unbounded number of machines is given, where each machine has $g$ processors.","No more than $g$ jobs can be scheduled simultaneously on each machine.","A machine consumes energy whenever at least one job is scheduled at any time on the machine.","Scheduling a single job at some time $t$ consumes the same amount of energy as scheduling $g$ jobs at time $t$. In the online setting, jobs are revealed when they are released.   ","We consider the cases where $g$ is unbounded and bounded.","In this paper, we revisit the bounds of the unbounded general setting from the literature and tighten it significantly.","We also consider agreeable jobs.","For the bounded setting, we show a tightened upper bound.","Furthermore, we show the first constant competitive ratio in the bounded setting that does not require lookahead."],"url":"http://arxiv.org/abs/2405.08595v1","category":"cs.DS"}
{"created":"2024-05-14 13:34:34","title":"Horocycle flows on abelian covers of surfaces of negative curvature","abstract":"We consider the unit speed parametrization of the horocycle flow on infinite Abelian covers of compact surfaces of negative curvature. We prove an asymptotic result for the ergodic integrals of sufficiently regular functions. In the case of constant curvature, where the unit speed and the uniformly contracting parametrizations of horocycles coincide, we recover a result by Ledrappier and Sarig. Our method, which does not use symbolic dynamics, is based on a general Fourier decomposition for Abelian covers and on the study of spectral theory of weighted (and twisted) transfer operators for the geodesic flow acting on appropriate anisotropic Banach spaces.","sentences":["We consider the unit speed parametrization of the horocycle flow on infinite Abelian covers of compact surfaces of negative curvature.","We prove an asymptotic result for the ergodic integrals of sufficiently regular functions.","In the case of constant curvature, where the unit speed and the uniformly contracting parametrizations of horocycles coincide, we recover a result by Ledrappier and Sarig.","Our method, which does not use symbolic dynamics, is based on a general Fourier decomposition for Abelian covers and on the study of spectral theory of weighted (and twisted) transfer operators for the geodesic flow acting on appropriate anisotropic Banach spaces."],"url":"http://arxiv.org/abs/2405.08592v1","category":"math.DS"}
{"created":"2024-05-14 13:24:51","title":"EchoTracker: Advancing Myocardial Point Tracking in Echocardiography","abstract":"Tissue tracking in echocardiography is challenging due to the complex cardiac motion and the inherent nature of ultrasound acquisitions. Although optical flow methods are considered state-of-the-art (SOTA), they struggle with long-range tracking, noise occlusions, and drift throughout the cardiac cycle. Recently, novel learning-based point tracking techniques have been introduced to tackle some of these issues. In this paper, we build upon these techniques and introduce EchoTracker, a two-fold coarse-to-fine model that facilitates the tracking of queried points on a tissue surface across ultrasound image sequences. The architecture contains a preliminary coarse initialization of the trajectories, followed by reinforcement iterations based on fine-grained appearance changes. It is efficient, light, and can run on mid-range GPUs. Experiments demonstrate that the model outperforms SOTA methods, with an average position accuracy of 67% and a median trajectory error of 2.86 pixels. Furthermore, we show a relative improvement of 25% when using our model to calculate the global longitudinal strain (GLS) in a clinical test-retest dataset compared to other methods. This implies that learning-based point tracking can potentially improve performance and yield a higher diagnostic and prognostic value for clinical measurements than current techniques. Our source code is available at: https://github.com/riponazad/echotracker/.","sentences":["Tissue tracking in echocardiography is challenging due to the complex cardiac motion and the inherent nature of ultrasound acquisitions.","Although optical flow methods are considered state-of-the-art (SOTA), they struggle with long-range tracking, noise occlusions, and drift throughout the cardiac cycle.","Recently, novel learning-based point tracking techniques have been introduced to tackle some of these issues.","In this paper, we build upon these techniques and introduce EchoTracker, a two-fold coarse-to-fine model that facilitates the tracking of queried points on a tissue surface across ultrasound image sequences.","The architecture contains a preliminary coarse initialization of the trajectories, followed by reinforcement iterations based on fine-grained appearance changes.","It is efficient, light, and can run on mid-range GPUs.","Experiments demonstrate that the model outperforms SOTA methods, with an average position accuracy of 67% and a median trajectory error of 2.86 pixels.","Furthermore, we show a relative improvement of 25% when using our model to calculate the global longitudinal strain (GLS) in a clinical test-retest dataset compared to other methods.","This implies that learning-based point tracking can potentially improve performance and yield a higher diagnostic and prognostic value for clinical measurements than current techniques.","Our source code is available at: https://github.com/riponazad/echotracker/."],"url":"http://arxiv.org/abs/2405.08587v1","category":"cs.CV"}
{"created":"2024-05-14 13:24:19","title":"Cross-Domain Feature Augmentation for Domain Generalization","abstract":"Domain generalization aims to develop models that are robust to distribution shifts. Existing methods focus on learning invariance across domains to enhance model robustness, and data augmentation has been widely used to learn invariant predictors, with most methods performing augmentation in the input space. However, augmentation in the input space has limited diversity whereas in the feature space is more versatile and has shown promising results. Nonetheless, feature semantics is seldom considered and existing feature augmentation methods suffer from a limited variety of augmented features. We decompose features into class-generic, class-specific, domain-generic, and domain-specific components. We propose a cross-domain feature augmentation method named XDomainMix that enables us to increase sample diversity while emphasizing the learning of invariant representations to achieve domain generalization. Experiments on widely used benchmark datasets demonstrate that our proposed method is able to achieve state-of-the-art performance. Quantitative analysis indicates that our feature augmentation approach facilitates the learning of effective models that are invariant across different domains.","sentences":["Domain generalization aims to develop models that are robust to distribution shifts.","Existing methods focus on learning invariance across domains to enhance model robustness, and data augmentation has been widely used to learn invariant predictors, with most methods performing augmentation in the input space.","However, augmentation in the input space has limited diversity whereas in the feature space is more versatile and has shown promising results.","Nonetheless, feature semantics is seldom considered and existing feature augmentation methods suffer from a limited variety of augmented features.","We decompose features into class-generic, class-specific, domain-generic, and domain-specific components.","We propose a cross-domain feature augmentation method named XDomainMix that enables us to increase sample diversity while emphasizing the learning of invariant representations to achieve domain generalization.","Experiments on widely used benchmark datasets demonstrate that our proposed method is able to achieve state-of-the-art performance.","Quantitative analysis indicates that our feature augmentation approach facilitates the learning of effective models that are invariant across different domains."],"url":"http://arxiv.org/abs/2405.08586v1","category":"cs.CV"}
{"created":"2024-05-14 13:24:14","title":"Design of a Multi-User RIS-Aided System with Statistical Channel Knowledge","abstract":"Reconfigurable intelligent surface (RIS) is a promising technology to enhance the spectral and energy efficiency in a wireless communication system. The design of the phase shifts of an RIS in every channel coherence interval demands a huge training overhead, making its deployment practically infeasible. The design complexity can be significantly reduced by exploiting the second-order statistics of the channels. This paper is the extension of our previous work to the design of an RIS for the multi-user setup, where we employ maximisation of the lower bound of the achievable sum-rate of the users. Unlike for the single-user case, obtaining a closed-form expression for the update of the filters and phase shifts is more challenging in the multi-user case. We resort to the fractional programming (FP) approach and the non-convex block coordinate descent (BCD) method to solve the optimisation problem. As the phase shifts of the RIS obtained by the proposed algorithms are based on the statistical channel knowledge, they do not need to be updated in every channel coherence interval.","sentences":["Reconfigurable intelligent surface (RIS) is a promising technology to enhance the spectral and energy efficiency in a wireless communication system.","The design of the phase shifts of an RIS in every channel coherence interval demands a huge training overhead, making its deployment practically infeasible.","The design complexity can be significantly reduced by exploiting the second-order statistics of the channels.","This paper is the extension of our previous work to the design of an RIS for the multi-user setup, where we employ maximisation of the lower bound of the achievable sum-rate of the users.","Unlike for the single-user case, obtaining a closed-form expression for the update of the filters and phase shifts is more challenging in the multi-user case.","We resort to the fractional programming (FP) approach and the non-convex block coordinate descent (BCD) method to solve the optimisation problem.","As the phase shifts of the RIS obtained by the proposed algorithms are based on the statistical channel knowledge, they do not need to be updated in every channel coherence interval."],"url":"http://arxiv.org/abs/2405.08585v1","category":"eess.SP"}
{"created":"2024-05-14 13:23:54","title":"On the $\u03c3$-balancing property of multivariate generalized quasi-arithmetic means","abstract":"The aim of this paper is to characterize the so-called $\\sigma$-balancing property in the class of generalized quasi-arithmetic means. In general, the question is whether those elements of a given family of means that possess this property are quasi-arithmetic.   The first result in the latter direction is due to G. Aumann who showed that a balanced complex mean is necessariliy quasi-arithmetic provided that it is analytic. Then Aumann characterized quasi-arithmetic means among Cauchy means in terms of the balancing property. These results date back to the 1930s. In 2015, Lucio R. Berrone, generalizing balancedness, concluded that a mean having that more general property is quasi-arithmetic if it is symmetric, strict and continuously differentiable. A common feature of these results is that they assume a certain order of differentiability of the mean whether or not it is a natural condition.   In 2020, the balancing property was characterized in the family of generalized quasi-arithmetic means of two variables under only natural conditions, namely continuity and strict monotonicity of their generating functions. Here we extend the corresponding result for multivariate generalized quasi-arithmetic means by relaxing the conditions on the generating functions and considering the more general $\\sigma$-balancing property.","sentences":["The aim of this paper is to characterize the so-called $\\sigma$-balancing property in the class of generalized quasi-arithmetic means.","In general, the question is whether those elements of a given family of means that possess this property are quasi-arithmetic.   ","The first result in the latter direction is due to G. Aumann who showed that a balanced complex mean is necessariliy quasi-arithmetic provided that it is analytic.","Then Aumann characterized quasi-arithmetic means among Cauchy means in terms of the balancing property.","These results date back to the 1930s.","In 2015, Lucio R. Berrone, generalizing balancedness, concluded that a mean having that more general property is quasi-arithmetic if it is symmetric, strict and continuously differentiable.","A common feature of these results is that they assume a certain order of differentiability of the mean whether or not it is a natural condition.   ","In 2020, the balancing property was characterized in the family of generalized quasi-arithmetic means of two variables under only natural conditions, namely continuity and strict monotonicity of their generating functions.","Here we extend the corresponding result for multivariate generalized quasi-arithmetic means by relaxing the conditions on the generating functions and considering the more general $\\sigma$-balancing property."],"url":"http://arxiv.org/abs/2405.08583v1","category":"math.CA"}
{"created":"2024-05-14 13:18:28","title":"Intelligent Control in 6G Open RAN: Security Risk or Opportunity?","abstract":"The Open Radio Access Network (Open RAN) framework, emerging as the cornerstone for Artificial Intelligence (AI)-enabled Sixth-Generation (6G) mobile networks, heralds a transformative shift in radio access network architecture. As the adoption of Open RAN accelerates, ensuring its security becomes critical. The RAN Intelligent Controller (RIC) plays a central role in Open RAN by improving network efficiency and flexibility. Nevertheless, it also brings about potential security risks that need careful scrutiny. Therefore, it is imperative to evaluate the current state of RIC security comprehensively. This assessment is essential to gain a profound understanding of the security considerations associated with RIC. This survey combines a comprehensive analysis of RAN security, tracing its evolution from 2G to 5G, with an in-depth exploration of RIC security, marking the first comprehensive examination of its kind in the literature. Real-world security incidents involving RIC are vividly illustrated, providing practical insights. The study evaluates the security implications of the RIC within the 6G Open RAN context, addressing security vulnerabilities, mitigation strategies, and potential enhancements. It aims to guide stakeholders in the telecom industry toward a secure and dependable telecommunications infrastructure. The article serves as a valuable reference, shedding light on the RIC's crucial role within the broader network infrastructure and emphasizing security's paramount importance. This survey also explores the promising security opportunities that the RIC presents for enhancing network security and resilience in the context of 6G mobile networks. It outlines open issues, lessons learned, and future research directions in the domain of intelligent control in 6G open RAN, facilitating a comprehensive understanding of this dynamic landscape.","sentences":["The Open Radio Access Network (Open RAN) framework, emerging as the cornerstone for Artificial Intelligence (AI)-enabled Sixth-Generation (6G) mobile networks, heralds a transformative shift in radio access network architecture.","As the adoption of Open RAN accelerates, ensuring its security becomes critical.","The RAN Intelligent Controller (RIC) plays a central role in Open RAN by improving network efficiency and flexibility.","Nevertheless, it also brings about potential security risks that need careful scrutiny.","Therefore, it is imperative to evaluate the current state of RIC security comprehensively.","This assessment is essential to gain a profound understanding of the security considerations associated with RIC.","This survey combines a comprehensive analysis of RAN security, tracing its evolution from 2G to 5G, with an in-depth exploration of RIC security, marking the first comprehensive examination of its kind in the literature.","Real-world security incidents involving RIC are vividly illustrated, providing practical insights.","The study evaluates the security implications of the RIC within the 6G Open RAN context, addressing security vulnerabilities, mitigation strategies, and potential enhancements.","It aims to guide stakeholders in the telecom industry toward a secure and dependable telecommunications infrastructure.","The article serves as a valuable reference, shedding light on the RIC's crucial role within the broader network infrastructure and emphasizing security's paramount importance.","This survey also explores the promising security opportunities that the RIC presents for enhancing network security and resilience in the context of 6G mobile networks.","It outlines open issues, lessons learned, and future research directions in the domain of intelligent control in 6G open RAN, facilitating a comprehensive understanding of this dynamic landscape."],"url":"http://arxiv.org/abs/2405.08577v1","category":"cs.NI"}
{"created":"2024-05-14 13:16:46","title":"Hearing Touch: Audio-Visual Pretraining for Contact-Rich Manipulation","abstract":"Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch. In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing. Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications. In this paper, we address this gap by using contact microphones as an alternative tactile sensor. Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation. To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation. For supplementary information including videos of real robot experiments, please see https://sites.google.com/view/hearing-touch.","sentences":["Although pre-training on a large amount of data is beneficial for robot learning, current paradigms only perform large-scale pretraining for visual representations, whereas representations for other modalities are trained from scratch.","In contrast to the abundance of visual data, it is unclear what relevant internet-scale data may be used for pretraining other modalities such as tactile sensing.","Such pretraining becomes increasingly crucial in the low-data regimes common in robotics applications.","In this paper, we address this gap by using contact microphones as an alternative tactile sensor.","Our key insight is that contact microphones capture inherently audio-based information, allowing us to leverage large-scale audio-visual pretraining to obtain representations that boost the performance of robotic manipulation.","To the best of our knowledge, our method is the first approach leveraging large-scale multisensory pre-training for robotic manipulation.","For supplementary information including videos of real robot experiments, please see https://sites.google.com/view/hearing-touch."],"url":"http://arxiv.org/abs/2405.08576v1","category":"cs.RO"}
{"created":"2024-05-14 13:05:01","title":"Can 3GPP New Radio Non-Terrestrial Networks Meet the IMT-2020 Requirements for Satellite Radio Interface Technology?","abstract":"The International Telecommunication Union defined the requirements for 5G in the International Mobile Telecommunications 2020 (IMT-2020) standard in 2017. Since then, advances in technology and standardization have made the ubiquitous deployment of 5G via satellite a practical possibility, for example, in locations where terrestrial networks (TNs) are not available. However, it may be difficult for satellite networks to achieve the same performance as TNs. To address this, the IMT-2020 requirements for satellite radio interface technology have recently been established. In this paper, these requirements are evaluated through system simulations for the 3rd Generation Partnership Project New Radio non-terrestrial networks with a low Earth orbit satellite. The focus is on the throughput, area traffic capacity, and spectral efficiency requirements. It is observed that the downlink (DL) requirements can be met for user equipment with 2 receive antenna elements. The results also reveal that frequency reuse factor 1 (FRF1) may outperform FRF3 in DL with a dual-antenna setup, which is a surprising finding since FRF3 is typically considered to outperform FRF1 due to better interference reduction. For uplink (UL), 1 transmit antenna is sufficient to meet the requirements by a relatively large margin - a promising result given that UL is generally more demanding.","sentences":["The International Telecommunication Union defined the requirements for 5G in the International Mobile Telecommunications 2020 (IMT-2020) standard in 2017.","Since then, advances in technology and standardization have made the ubiquitous deployment of 5G via satellite a practical possibility, for example, in locations where terrestrial networks (TNs) are not available.","However, it may be difficult for satellite networks to achieve the same performance as TNs.","To address this, the IMT-2020 requirements for satellite radio interface technology have recently been established.","In this paper, these requirements are evaluated through system simulations for the 3rd Generation Partnership Project New Radio non-terrestrial networks with a low Earth orbit satellite.","The focus is on the throughput, area traffic capacity, and spectral efficiency requirements.","It is observed that the downlink (DL) requirements can be met for user equipment with 2 receive antenna elements.","The results also reveal that frequency reuse factor 1 (FRF1) may outperform FRF3 in DL with a dual-antenna setup, which is a surprising finding since FRF3 is typically considered to outperform FRF1 due to better interference reduction.","For uplink (UL), 1 transmit antenna is sufficient to meet the requirements by a relatively large margin - a promising result given that UL is generally more demanding."],"url":"http://arxiv.org/abs/2405.08569v1","category":"cs.NI"}
{"created":"2024-05-14 13:02:05","title":"Generating quantum dissonance via local operations","abstract":"Correlations may arise in quantum systems through various means, of which the most remarkable one is quantum entanglement. Additionally, there are systems that exhibit non-classical correlations even in the absence of entanglement. Quantum dissonance refers to how quantum discord (QD) -- the difference between the total correlation and the classical correlation in a given quantum state -- appears as a non-classical correlation in a system without entanglement. It could be said that QD has the potential to provide a more inclusive viewpoint for discerning the non-classical correlations. In this work, we address the problem of manipulating the QD between two subsystems through local operations. We propose two explicit procedures for obtaining separable Werner states, a type of mixed state with nonzero QD. Both approaches involve performing local operations on classically correlated states and offers a step-by-step method for obtaining separable Werner states with nonzero discord, providing an alternative (explicit and user-friendly) to existing methods.","sentences":["Correlations may arise in quantum systems through various means, of which the most remarkable one is quantum entanglement.","Additionally, there are systems that exhibit non-classical correlations even in the absence of entanglement.","Quantum dissonance refers to how quantum discord (QD) -- the difference between the total correlation and the classical correlation in a given quantum state -- appears as a non-classical correlation in a system without entanglement.","It could be said that QD has the potential to provide a more inclusive viewpoint for discerning the non-classical correlations.","In this work, we address the problem of manipulating the QD between two subsystems through local operations.","We propose two explicit procedures for obtaining separable Werner states, a type of mixed state with nonzero QD.","Both approaches involve performing local operations on classically correlated states and offers a step-by-step method for obtaining separable Werner states with nonzero discord, providing an alternative (explicit and user-friendly) to existing methods."],"url":"http://arxiv.org/abs/2405.08568v1","category":"quant-ph"}
{"created":"2024-05-14 13:01:04","title":"Python-Based Reinforcement Learning on Simulink Models","abstract":"This paper proposes a framework for training Reinforcement Learning agents using Python in conjunction with Simulink models. Leveraging Python's superior customization options and popular libraries like Stable Baselines3, we aim to bridge the gap between the established Simulink environment and the flexibility of Python for training bleeding edge agents. Our approach is demonstrated on the Quanser Aero 2, a versatile dual-rotor helicopter. We show that policies trained on Simulink models can be seamlessly transferred to the real system, enabling efficient development and deployment of Reinforcement Learning agents for control tasks. Through systematic integration steps, including C-code generation from Simulink, DLL compilation, and Python interface development, we establish a robust framework for training agents on Simulink models. Experimental results demonstrate the effectiveness of our approach, surpassing previous efforts and highlighting the potential of combining Simulink with Python for Reinforcement Learning research and applications.","sentences":["This paper proposes a framework for training Reinforcement Learning agents using Python in conjunction with Simulink models.","Leveraging Python's superior customization options and popular libraries like Stable Baselines3, we aim to bridge the gap between the established Simulink environment and the flexibility of Python for training bleeding edge agents.","Our approach is demonstrated on the Quanser Aero 2, a versatile dual-rotor helicopter.","We show that policies trained on Simulink models can be seamlessly transferred to the real system, enabling efficient development and deployment of Reinforcement Learning agents for control tasks.","Through systematic integration steps, including C-code generation from Simulink, DLL compilation, and Python interface development, we establish a robust framework for training agents on Simulink models.","Experimental results demonstrate the effectiveness of our approach, surpassing previous efforts and highlighting the potential of combining Simulink with Python for Reinforcement Learning research and applications."],"url":"http://arxiv.org/abs/2405.08567v1","category":"cs.LG"}
{"created":"2024-05-14 12:52:42","title":"Anytime Sorting Algorithms (Extended Version)","abstract":"This paper addresses the anytime sorting problem, aiming to develop algorithms providing tentative estimates of the sorted list at each execution step. Comparisons are treated as steps, and the Spearman's footrule metric evaluates estimation accuracy. We propose a general approach for making any sorting algorithm anytime and introduce two new algorithms: multizip sort and Corsort. Simulations showcase the superior performance of both algorithms compared to existing methods. Multizip sort keeps a low global complexity, while Corsort produces intermediate estimates surpassing previous algorithms.","sentences":["This paper addresses the anytime sorting problem, aiming to develop algorithms providing tentative estimates of the sorted list at each execution step.","Comparisons are treated as steps, and the Spearman's footrule metric evaluates estimation accuracy.","We propose a general approach for making any sorting algorithm anytime and introduce two new algorithms: multizip sort and Corsort.","Simulations showcase the superior performance of both algorithms compared to existing methods.","Multizip sort keeps a low global complexity, while Corsort produces intermediate estimates surpassing previous algorithms."],"url":"http://arxiv.org/abs/2405.08564v1","category":"cs.DS"}
{"created":"2024-05-14 12:51:40","title":"Primordial black holes generated by fast-roll mechanism in non-canonical natural inflation","abstract":"In this work, a new fast-roll (FR) mechanism to generate primordial black holes (PBHs) and their coeval gravitational waves (GWs) in generalized non-canonical natural inflation is introduced. In this model, choosing a suitable function for non-canonical mass scale parameter $M(\\phi)$ gives rise to produce a cliff-like region in field evolution path. When inflaton rolls down the steep cliff, its kinetic energy during the FR stage is amplified in comparison with slow-roll (SR) stage. Hence, seeds of PBHs production are born in this transient FR stage. Depending on the position of the cliff, appropriate cases of PBHs for explaining total dark matter (DM), microlensing effects, LIGO-VIRGO events and NANOGrav 15 year data can be formed. The density spectrum of GWs related to one case of the model lies in the NANOGrav 15 year domain and behaves like $\\Omega_{\\rm GW_0}\\sim f^{5-\\gamma}$. The spectral index $\\gamma=3.42$ for this case satisfies the NANOGrav 15 year constraint. Moreover, regarding reheating considerations, it is demonstrated that PBHs are born in radiation-dominated (RD) era. Furthermore, viability of the model in light of theoretical swampland criteria and observational constraints on cosmic microwave background (CMB) scales are illustrated.","sentences":["In this work, a new fast-roll (FR) mechanism to generate primordial black holes (PBHs) and their coeval gravitational waves (GWs) in generalized non-canonical natural inflation is introduced.","In this model, choosing a suitable function for non-canonical mass scale parameter $M(\\phi)$ gives rise to produce a cliff-like region in field evolution path.","When inflaton rolls down the steep cliff, its kinetic energy during the FR stage is amplified in comparison with slow-roll (SR) stage.","Hence, seeds of PBHs production are born in this transient FR stage.","Depending on the position of the cliff, appropriate cases of PBHs for explaining total dark matter (DM), microlensing effects, LIGO-VIRGO events and NANOGrav 15 year data can be formed.","The density spectrum of GWs related to one case of the model lies in the NANOGrav 15 year domain and behaves like $\\Omega_{\\rm GW_0}\\sim f^{5-\\gamma}$. The spectral index $\\gamma=3.42$ for this case satisfies the NANOGrav 15 year constraint.","Moreover, regarding reheating considerations, it is demonstrated that PBHs are born in radiation-dominated (RD) era.","Furthermore, viability of the model in light of theoretical swampland criteria and observational constraints on cosmic microwave background (CMB) scales are illustrated."],"url":"http://arxiv.org/abs/2405.08563v1","category":"gr-qc"}
{"created":"2024-05-14 12:48:32","title":"Minimax and maximin problems for sums of translates on the real axis","abstract":"Sums of translates generalize logarithms of weighted algebraic polynomials. The paper presents the solution to the minimax and maximin problems on the real axis for sums of translates. We prove that there is a unique function that is extremal in both problems. The key in our proof is a reduction to the problem on a segment. For this, we work out an analogue of the Mhaskar-Rakhmanov-Saff theorem, too.","sentences":["Sums of translates generalize logarithms of weighted algebraic polynomials.","The paper presents the solution to the minimax and maximin problems on the real axis for sums of translates.","We prove that there is a unique function that is extremal in both problems.","The key in our proof is a reduction to the problem on a segment.","For this, we work out an analogue of the Mhaskar-Rakhmanov-Saff theorem, too."],"url":"http://arxiv.org/abs/2405.08561v1","category":"math.CA"}
{"created":"2024-05-14 12:45:49","title":"Shape-aware synthesis of pathological lung CT scans using CycleGAN for enhanced semi-supervised lung segmentation","abstract":"This paper addresses the problem of pathological lung segmentation, a significant challenge in medical image analysis, particularly pronounced in cases of peripheral opacities (severe fibrosis and consolidation) because of the textural similarity between lung tissue and surrounding areas. To overcome these challenges, this paper emphasizes the use of CycleGAN for unpaired image-to-image translation, in order to provide an augmentation method able to generate fake pathological images matching an existing ground truth. Although previous studies have employed CycleGAN, they often neglect the challenge of shape deformation, which is crucial for accurate medical image segmentation. Our work introduces an innovative strategy that incorporates additional loss functions. Specifically, it proposes an L1 loss based on the lung surrounding which shape is constrained to remain unchanged at the transition from the healthy to pathological domains. The lung surrounding is derived based on ground truth lung masks available in the healthy domain. Furthermore, preprocessing steps, such as cropping based on ribs/vertebra locations, are applied to refine the input for the CycleGAN, ensuring that the network focus on the lung region. This is essential to avoid extraneous biases, such as the zoom effect bias, which can divert attention from the main task. The method is applied to enhance in semi-supervised manner the lung segmentation process by employing a U-Net model trained with on-the-fly data augmentation incorporating synthetic pathological tissues generated by the CycleGAN model. Preliminary results from this research demonstrate significant qualitative and quantitative improvements, setting a new benchmark in the field of pathological lung segmentation. Our code is available at https://github.com/noureddinekhiati/Semi-supervised-lung-segmentation","sentences":["This paper addresses the problem of pathological lung segmentation, a significant challenge in medical image analysis, particularly pronounced in cases of peripheral opacities (severe fibrosis and consolidation) because of the textural similarity between lung tissue and surrounding areas.","To overcome these challenges, this paper emphasizes the use of CycleGAN for unpaired image-to-image translation, in order to provide an augmentation method able to generate fake pathological images matching an existing ground truth.","Although previous studies have employed CycleGAN, they often neglect the challenge of shape deformation, which is crucial for accurate medical image segmentation.","Our work introduces an innovative strategy that incorporates additional loss functions.","Specifically, it proposes an L1 loss based on the lung surrounding which shape is constrained to remain unchanged at the transition from the healthy to pathological domains.","The lung surrounding is derived based on ground truth lung masks available in the healthy domain.","Furthermore, preprocessing steps, such as cropping based on ribs/vertebra locations, are applied to refine the input for the CycleGAN, ensuring that the network focus on the lung region.","This is essential to avoid extraneous biases, such as the zoom effect bias, which can divert attention from the main task.","The method is applied to enhance in semi-supervised manner the lung segmentation process by employing a U-Net model trained with on-the-fly data augmentation incorporating synthetic pathological tissues generated by the CycleGAN model.","Preliminary results from this research demonstrate significant qualitative and quantitative improvements, setting a new benchmark in the field of pathological lung segmentation.","Our code is available at https://github.com/noureddinekhiati/Semi-supervised-lung-segmentation"],"url":"http://arxiv.org/abs/2405.08556v1","category":"eess.IV"}
{"created":"2024-05-14 12:40:25","title":"Learning Multi-Agent Communication from Graph Modeling Perspective","abstract":"In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives. To enhance coordination among these agents, a distributed communication framework is often employed. However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts. In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph. We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process. Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner. Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents.","sentences":["In numerous artificial intelligence applications, the collaborative efforts of multiple intelligent agents are imperative for the successful attainment of target objectives.","To enhance coordination among these agents, a distributed communication framework is often employed.","However, information sharing among all agents proves to be resource-intensive, while the adoption of a manually pre-defined communication architecture imposes limitations on inter-agent communication, thereby constraining the potential for collaborative efforts.","In this study, we introduce a novel approach wherein we conceptualize the communication architecture among agents as a learnable graph.","We formulate this problem as the task of determining the communication graph while enabling the architecture parameters to update normally, thus necessitating a bi-level optimization process.","Utilizing continuous relaxation of the graph representation and incorporating attention units, our proposed approach, CommFormer, efficiently optimizes the communication graph and concurrently refines architectural parameters through gradient descent in an end-to-end manner.","Extensive experiments on a variety of cooperative tasks substantiate the robustness of our model across diverse cooperative scenarios, where agents are able to develop more coordinated and sophisticated strategies regardless of changes in the number of agents."],"url":"http://arxiv.org/abs/2405.08550v1","category":"cs.LG"}
{"created":"2024-05-14 12:37:19","title":"Strict Self-Assembly of Discrete Self-Similar Fractal Shapes","abstract":"This paper gives a (polynomial time) algorithm to decide whether a given Discrete Self-Similar Fractal Shape can be assembled in the aTAM model.In the positive case, the construction relies on a Self-Assembling System in the aTAM which strictly assembles a particular self-similar fractal shape, namely a variant $K^\\infty$ of the Sierpinski Carpet. We prove that the aTAM we propose is correct through a novel device, \\emph{self-describing circuits} which are generally useful for rigorous yet readable proofs of the behaviour of aTAMs.We then discuss which self-similar fractals can or cannot be strictly self-assembled in the aTAM. It turns out that the ability of iterates of the generator to pass information is crucial: either this \\emph{bandwidth} is eventually sufficient in both cardinal directions and $K^\\infty$ appears within the fractal pattern after some finite number of iterations, or that bandwidth remains ever insufficient in one direction and any aTAM trying to self-assemble the shape will end up either bounded with an ultimately periodic pattern covering arbitrarily large squares. This is established thanks to a new characterization of the productions of systems whose productions have a uniformly bounded treewidth.","sentences":["This paper gives a (polynomial time) algorithm to decide whether a given Discrete Self-Similar Fractal Shape can be assembled in the aTAM model.","In the positive case, the construction relies on a Self-Assembling System in the aTAM which strictly assembles a particular self-similar fractal shape, namely a variant $K^\\infty$ of the Sierpinski Carpet.","We prove that the aTAM we propose is correct through a novel device, \\emph{self-describing circuits} which are generally useful for rigorous yet readable proofs of the behaviour of aTAMs.","We then discuss which self-similar fractals can or cannot be strictly self-assembled in the aTAM.","It turns out that the ability of iterates of the generator to pass information is crucial: either this \\emph{bandwidth} is eventually sufficient in both cardinal directions and $K^\\infty$ appears within the fractal pattern after some finite number of iterations, or that bandwidth remains ever insufficient in one direction and any aTAM trying to self-assemble the shape will end up either bounded with an ultimately periodic pattern covering arbitrarily large squares.","This is established thanks to a new characterization of the productions of systems whose productions have a uniformly bounded treewidth."],"url":"http://arxiv.org/abs/2405.08548v1","category":"cs.DM"}
{"created":"2024-05-14 12:34:25","title":"Analysing Cross-Speaker Convergence in Face-to-Face Dialogue through the Lens of Automatically Detected Shared Linguistic Constructions","abstract":"Conversation requires a substantial amount of coordination between dialogue participants, from managing turn taking to negotiating mutual understanding. Part of this coordination effort surfaces as the reuse of linguistic behaviour across speakers, a process often referred to as alignment. While the presence of linguistic alignment is well documented in the literature, several questions remain open, including the extent to which patterns of reuse across speakers have an impact on the emergence of labelling conventions for novel referents. In this study, we put forward a methodology for automatically detecting shared lemmatised constructions -- expressions with a common lexical core used by both speakers within a dialogue -- and apply it to a referential communication corpus where participants aim to identify novel objects for which no established labels exist. Our analyses uncover the usage patterns of shared constructions in interaction and reveal that features such as their frequency and the amount of different constructions used for a referent are associated with the degree of object labelling convergence the participants exhibit after social interaction. More generally, the present study shows that automatically detected shared constructions offer a useful level of analysis to investigate the dynamics of reference negotiation in dialogue.","sentences":["Conversation requires a substantial amount of coordination between dialogue participants, from managing turn taking to negotiating mutual understanding.","Part of this coordination effort surfaces as the reuse of linguistic behaviour across speakers, a process often referred to as alignment.","While the presence of linguistic alignment is well documented in the literature, several questions remain open, including the extent to which patterns of reuse across speakers have an impact on the emergence of labelling conventions for novel referents.","In this study, we put forward a methodology for automatically detecting shared lemmatised constructions -- expressions with a common lexical core used by both speakers within a dialogue -- and apply it to a referential communication corpus where participants aim to identify novel objects for which no established labels exist.","Our analyses uncover the usage patterns of shared constructions in interaction and reveal that features such as their frequency and the amount of different constructions used for a referent are associated with the degree of object labelling convergence the participants exhibit after social interaction.","More generally, the present study shows that automatically detected shared constructions offer a useful level of analysis to investigate the dynamics of reference negotiation in dialogue."],"url":"http://arxiv.org/abs/2405.08546v1","category":"cs.CL"}
{"created":"2024-05-14 12:30:33","title":"Distinguishing bounce and inflation via quantum signatures from cosmic microwave background","abstract":"Cosmological inflation is a popular paradigm for understanding Cosmic Microwave Background Radiation (CMBR); however, it faces many conceptual challenges. An alternative mechanism to inflation for generating an almost scale-invariant spectrum of perturbations is a \\emph{bouncing cosmology} with an initial matter-dominated contraction phase, during which the modes corresponding to currently observed scales exited the Hubble radius. Bouncing cosmology avoids the initial singularity but has fine-tuning problems. Taking an \\emph{agnostic view} of the two early-universe paradigms, we propose a quantum measure -- Dynamical Fidelity Susceptibility (DFS) of CMBR -- that distinguishes the two scenarios. Taking two simple models with the same power-spectrum, we explicitly show that DFS behaves differently for the two scenarios. We discuss the possibility of using DFS as a distinguisher in the upcoming space missions.","sentences":["Cosmological inflation is a popular paradigm for understanding Cosmic Microwave Background Radiation (CMBR); however, it faces many conceptual challenges.","An alternative mechanism to inflation for generating an almost scale-invariant spectrum of perturbations is a \\emph{bouncing cosmology} with an initial matter-dominated contraction phase, during which the modes corresponding to currently observed scales exited the Hubble radius.","Bouncing cosmology avoids the initial singularity but has fine-tuning problems.","Taking an \\emph{agnostic view} of the two early-universe paradigms, we propose a quantum measure -- Dynamical Fidelity Susceptibility (DFS) of CMBR -- that distinguishes the two scenarios.","Taking two simple models with the same power-spectrum, we explicitly show that DFS behaves differently for the two scenarios.","We discuss the possibility of using DFS as a distinguisher in the upcoming space missions."],"url":"http://arxiv.org/abs/2405.08543v1","category":"astro-ph.CO"}
{"created":"2024-05-14 12:30:04","title":"Industrial Metaverse: Enabling Technologies, Open Problems, and Future Trends","abstract":"As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks. This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing. However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse. This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector. Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse. Specifically, we first analyze the advantages of the Metaverse for industrial production. Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production. Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints. Furthermore, we investigate the extant solutions devised to address them. Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse.","sentences":["As an emerging technology that enables seamless integration between the physical and virtual worlds, the Metaverse has great potential to be deployed in the industrial production field with the development of extended reality (XR) and next-generation communication networks.","This deployment, called the Industrial Metaverse, is used for product design, production operations, industrial quality inspection, and product testing.","However, there lacks of in-depth understanding of the enabling technologies associated with the Industrial Metaverse.","This encompasses both the precise industrial scenarios targeted by each technology and the potential migration of technologies developed in other domains to the industrial sector.","Driven by this issue, in this article, we conduct a comprehensive survey of the state-of-the-art literature on the Industrial Metaverse.","Specifically, we first analyze the advantages of the Metaverse for industrial production.","Then, we review a collection of key enabling technologies of the Industrial Metaverse, including blockchain (BC), digital twin (DT), 6G, XR, and artificial intelligence (AI), and analyze how these technologies can support different aspects of industrial production.","Subsequently, we present numerous formidable challenges encountered within the Industrial Metaverse, including confidentiality and security concerns, resource limitations, and interoperability constraints.","Furthermore, we investigate the extant solutions devised to address them.","Finally, we briefly outline several open issues and future research directions of the Industrial Metaverse."],"url":"http://arxiv.org/abs/2405.08542v1","category":"cs.CE"}
{"created":"2024-05-14 12:26:19","title":"Generalizing Knowledge Graph Embedding with Universal Orthogonal Parameterization","abstract":"Recent advances in knowledge graph embedding (KGE) rely on Euclidean/hyperbolic orthogonal relation transformations to model intrinsic logical patterns and topological structures. However, existing approaches are confined to rigid relational orthogonalization with restricted dimension and homogeneous geometry, leading to deficient modeling capability. In this work, we move beyond these approaches in terms of both dimension and geometry by introducing a powerful framework named GoldE, which features a universal orthogonal parameterization based on a generalized form of Householder reflection. Such parameterization can naturally achieve dimensional extension and geometric unification with theoretical guarantees, enabling our framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity of knowledge graphs. Empirically, GoldE achieves state-of-the-art performance on three standard benchmarks. Codes are available at https://github.com/xxrep/GoldE.","sentences":["Recent advances in knowledge graph embedding (KGE) rely on Euclidean/hyperbolic orthogonal relation transformations to model intrinsic logical patterns and topological structures.","However, existing approaches are confined to rigid relational orthogonalization with restricted dimension and homogeneous geometry, leading to deficient modeling capability.","In this work, we move beyond these approaches in terms of both dimension and geometry by introducing a powerful framework named GoldE, which features a universal orthogonal parameterization based on a generalized form of Householder reflection.","Such parameterization can naturally achieve dimensional extension and geometric unification with theoretical guarantees, enabling our framework to simultaneously capture crucial logical patterns and inherent topological heterogeneity of knowledge graphs.","Empirically, GoldE achieves state-of-the-art performance on three standard benchmarks.","Codes are available at https://github.com/xxrep/GoldE."],"url":"http://arxiv.org/abs/2405.08540v1","category":"cs.LG"}
{"created":"2024-05-14 12:20:14","title":"Gauge invariance of the Aharonov-Bohm effect in a quantum electrodynamics framework","abstract":"The gauge invariance of the Aharonov-Bohm (AB) effect with a quantum treatment for the electromagnetic field is demonstrated. We provide an exact solution for the electromagnetic ground energy due to the interaction of the quantum electromagnetic field with the classical charges and currents that act as sources of the potentials in a classical description, in the Lorenz gauge. Then, we use first-order perturbation theory to compute an extra change on the electromagnetic ground energy due to the presence of a quantum charged particle with known wave function in the system. This energy in general depends on the quantum particle path in an interferometer, what results in an AB phase difference between the paths. The gauge invariance of this AB phase difference is then shown for the magnetic, electric, and the recently proposed electrodynamic versions of the AB effect. However, the AB phase difference could depend on the gauge for nonclosed paths, what reinforces the view that it only can be measured in closed paths.","sentences":["The gauge invariance of the Aharonov-Bohm (AB) effect with a quantum treatment for the electromagnetic field is demonstrated.","We provide an exact solution for the electromagnetic ground energy due to the interaction of the quantum electromagnetic field with the classical charges and currents that act as sources of the potentials in a classical description, in the Lorenz gauge.","Then, we use first-order perturbation theory to compute an extra change on the electromagnetic ground energy due to the presence of a quantum charged particle with known wave function in the system.","This energy in general depends on the quantum particle path in an interferometer, what results in an AB phase difference between the paths.","The gauge invariance of this AB phase difference is then shown for the magnetic, electric, and the recently proposed electrodynamic versions of the AB effect.","However, the AB phase difference could depend on the gauge for nonclosed paths, what reinforces the view that it only can be measured in closed paths."],"url":"http://arxiv.org/abs/2405.08536v1","category":"quant-ph"}
{"created":"2024-05-14 12:16:43","title":"A dynamical view of Tijdeman's solution of the chairman assignment problem","abstract":"In 1980, R. Tijdeman provided an on-line algorithm that generates sequences over a finite alphabet with minimal discrepancy, that is, such that the occurrence of each letter optimally tracks its frequency. In this article, we define discrete dynamical systems generating these sequences. The dynamical systems are defined as exchanges of polytopal pieces, yielding cut and project schemes, and they code tilings of the line whose sets of vertices form model sets. We prove that these sequences of low discrepancy are natural codings of toral translations with respect to polytopal atoms, and that they generate a minimal and uniquely ergodic subshift with purely discrete spectrum. Finally, we show that the factor complexity of these sequences is of polynomial growth order $n^{d-1}$, where $d$ is the cardinality of the alphabet.","sentences":["In 1980, R. Tijdeman provided an on-line algorithm that generates sequences over a finite alphabet with minimal discrepancy, that is, such that the occurrence of each letter optimally tracks its frequency.","In this article, we define discrete dynamical systems generating these sequences.","The dynamical systems are defined as exchanges of polytopal pieces, yielding cut and project schemes, and they code tilings of the line whose sets of vertices form model sets.","We prove that these sequences of low discrepancy are natural codings of toral translations with respect to polytopal atoms, and that they generate a minimal and uniquely ergodic subshift with purely discrete spectrum.","Finally, we show that the factor complexity of these sequences is of polynomial growth order $n^{d-1}$, where $d$ is the cardinality of the alphabet."],"url":"http://arxiv.org/abs/2405.08532v1","category":"math.DS"}
{"created":"2024-05-14 12:14:58","title":"Parameter-Efficient Instance-Adaptive Neural Video Compression","abstract":"Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to the standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines. However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data. The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video. However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training. In this work, we propose a parameter-efficient instance-adaptive video compression framework. Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences. The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms. Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings. We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR improvements) and BD rates compared to the baselines NVC.","sentences":["Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to the standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines.","However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data.","The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video.","However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training.","In this work, we propose a parameter-efficient instance-adaptive video compression framework.","Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences.","The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms.","Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings.","We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR improvements) and BD rates compared to the baselines NVC."],"url":"http://arxiv.org/abs/2405.08530v1","category":"eess.IV"}
{"created":"2024-05-14 12:10:52","title":"Quark and gluon momentum fractions in the pion and in the kaon","abstract":"We present the full decomposition of the momentum fraction carried by quarks and gluons in the pion and the kaon. We employ three gauge ensembles generated with $N_f=2+1+1$ Wilson twisted-mass clover-improved fermions at the physical quark masses. For both mesons we perform a continuum extrapolation directly at the physical pion mass, which allows us to determine for the first time the momentum decomposition at the physical point. We find that the total momentum fraction carried by quarks is 0.532(56) and 0.618(32) and by gluons 0.388(49) and 0.408(61) in the pion and in the kaon, respectively, in the $\\overline{\\mathrm{MS}}$ scheme and at the renormalization scale of 2 GeV.   Having computed both the quark and gluon contributions in the continuum limit, we find that the momentum sum is 0.926(68) for the pion and 1.046(90) for the kaon, verifying the momentum sum rule.","sentences":["We present the full decomposition of the momentum fraction carried by quarks and gluons in the pion and the kaon.","We employ three gauge ensembles generated with $N_f=2+1+1$ Wilson twisted-mass clover-improved fermions at the physical quark masses.","For both mesons we perform a continuum extrapolation directly at the physical pion mass, which allows us to determine for the first time the momentum decomposition at the physical point.","We find that the total momentum fraction carried by quarks is 0.532(56) and 0.618(32) and by gluons 0.388(49) and 0.408(61) in the pion and in the kaon, respectively, in the $\\overline{\\mathrm{MS}}$ scheme and at the renormalization scale of 2 GeV.   Having computed both the quark and gluon contributions in the continuum limit, we find that the momentum sum is 0.926(68) for the pion and 1.046(90) for the kaon, verifying the momentum sum rule."],"url":"http://arxiv.org/abs/2405.08529v1","category":"hep-lat"}
{"created":"2024-05-14 12:07:07","title":"From Internet of Things Data to Business Processes: Challenges and a Framework","abstract":"The IoT and Business Process Management (BPM) communities co-exist in many shared application domains, such as manufacturing and healthcare. The IoT community has a strong focus on hardware, connectivity and data; the BPM community focuses mainly on finding, controlling, and enhancing the structured interactions among the IoT devices in processes. While the field of Process Mining deals with the extraction of process models and process analytics from process event logs, the data produced by IoT sensors often is at a lower granularity than these process-level events. The fundamental questions about extracting and abstracting process-related data from streams of IoT sensor values are: (1) Which sensor values can be clustered together as part of process events?, (2) Which sensor values signify the start and end of such events?, (3) Which sensor values are related but not essential? This work proposes a framework to semi-automatically perform a set of structured steps to convert low-level IoT sensor data into higher-level process events that are suitable for process mining. The framework is meant to provide a generic sequence of abstract steps to guide the event extraction, abstraction, and correlation, with variation points for plugging in specific analysis techniques and algorithms for each step. To assess the completeness of the framework, we present a set of challenges, how they can be tackled through the framework, and an example on how to instantiate the framework in a real-world demonstration from the field of smart manufacturing. Based on this framework, future research can be conducted in a structured manner through refining and improving individual steps.","sentences":["The IoT and Business Process Management (BPM) communities co-exist in many shared application domains, such as manufacturing and healthcare.","The IoT community has a strong focus on hardware, connectivity and data; the BPM community focuses mainly on finding, controlling, and enhancing the structured interactions among the IoT devices in processes.","While the field of Process Mining deals with the extraction of process models and process analytics from process event logs, the data produced by IoT sensors often is at a lower granularity than these process-level events.","The fundamental questions about extracting and abstracting process-related data from streams of IoT sensor values are: (1) Which sensor values can be clustered together as part of process events?, (2) Which sensor values signify the start and end of such events?, (3) Which sensor values are related but not essential?","This work proposes a framework to semi-automatically perform a set of structured steps to convert low-level IoT sensor data into higher-level process events that are suitable for process mining.","The framework is meant to provide a generic sequence of abstract steps to guide the event extraction, abstraction, and correlation, with variation points for plugging in specific analysis techniques and algorithms for each step.","To assess the completeness of the framework, we present a set of challenges, how they can be tackled through the framework, and an example on how to instantiate the framework in a real-world demonstration from the field of smart manufacturing.","Based on this framework, future research can be conducted in a structured manner through refining and improving individual steps."],"url":"http://arxiv.org/abs/2405.08528v1","category":"cs.SE"}
{"created":"2024-05-14 12:06:44","title":"EEG-Features for Generalized Deepfake Detection","abstract":"Since the advent of Deepfakes in digital media, the development of robust and reliable detection mechanism is urgently called for. In this study, we explore a novel approach to Deepfake detection by utilizing electroencephalography (EEG) measured from the neural processing of a human participant who viewed and categorized Deepfake stimuli from the FaceForensics++ datset. These measurements serve as input features to a binary support vector classifier, trained to discriminate between real and manipulated facial images. We examine whether EEG data can inform Deepfake detection and also if it can provide a generalized representation capable of identifying Deepfakes beyond the training domain. Our preliminary results indicate that human neural processing signals can be successfully integrated into Deepfake detection frameworks and hint at the potential for a generalized neural representation of artifacts in computer generated faces. Moreover, our study provides next steps towards the understanding of how digital realism is embedded in the human cognitive system, possibly enabling the development of more realistic digital avatars in the future.","sentences":["Since the advent of Deepfakes in digital media, the development of robust and reliable detection mechanism is urgently called for.","In this study, we explore a novel approach to Deepfake detection by utilizing electroencephalography (EEG) measured from the neural processing of a human participant who viewed and categorized Deepfake stimuli from the FaceForensics++ datset.","These measurements serve as input features to a binary support vector classifier, trained to discriminate between real and manipulated facial images.","We examine whether EEG data can inform Deepfake detection and also if it can provide a generalized representation capable of identifying Deepfakes beyond the training domain.","Our preliminary results indicate that human neural processing signals can be successfully integrated into Deepfake detection frameworks and hint at the potential for a generalized neural representation of artifacts in computer generated faces.","Moreover, our study provides next steps towards the understanding of how digital realism is embedded in the human cognitive system, possibly enabling the development of more realistic digital avatars in the future."],"url":"http://arxiv.org/abs/2405.08527v1","category":"cs.LG"}
{"created":"2024-05-14 12:02:28","title":"Doubly-robust inference and optimality in structure-agnostic models with smoothness","abstract":"We study the problem of constructing an estimator of the average treatment effect (ATE) that exhibits doubly-robust asymptotic linearity (DRAL). This is a stronger requirement than doubly-robust consistency. A DRAL estimator can yield asymptotically valid Wald-type confidence intervals even when the propensity score or the outcome model is inconsistently estimated. On the contrary, the celebrated doubly-robust, augmented-IPW (AIPW) estimator generally requires consistent estimation of both nuisance functions for standard root-n inference. We make three main contributions. First, we propose a new hybrid class of distributions that consists of the structure-agnostic class introduced in Balakrishnan et al (2023) with additional smoothness constraints. While DRAL is generally not possible in the pure structure-agnostic class, we show that it can be attained in the new hybrid one. Second, we calculate minimax lower bounds for estimating the ATE in the new class, as well as in the pure structure-agnostic one. Third, building upon the literature on doubly-robust inference (van der Laan, 2014, Benkeser et al, 2017, Dukes et al 2021), we propose a new estimator of the ATE that enjoys DRAL. Under certain conditions, we show that its rate of convergence in the new class can be much faster than that achieved by the AIPW estimator and, in particular, matches the minimax lower bound rate, thereby establishing its optimality. Finally, we clarify the connection between DRAL estimators and those based on higher-order influence functions (Robins et al, 2017) and complement our theoretical findings with simulations.","sentences":["We study the problem of constructing an estimator of the average treatment effect (ATE) that exhibits doubly-robust asymptotic linearity (DRAL).","This is a stronger requirement than doubly-robust consistency.","A DRAL estimator can yield asymptotically valid Wald-type confidence intervals even when the propensity score or the outcome model is inconsistently estimated.","On the contrary, the celebrated doubly-robust, augmented-IPW (AIPW) estimator generally requires consistent estimation of both nuisance functions for standard root-n inference.","We make three main contributions.","First, we propose a new hybrid class of distributions that consists of the structure-agnostic class introduced in Balakrishnan et al (2023) with additional smoothness constraints.","While DRAL is generally not possible in the pure structure-agnostic class, we show that it can be attained in the new hybrid one.","Second, we calculate minimax lower bounds for estimating the ATE in the new class, as well as in the pure structure-agnostic one.","Third, building upon the literature on doubly-robust inference (van der Laan, 2014, Benkeser et al, 2017, Dukes et al 2021), we propose a new estimator of the ATE that enjoys DRAL.","Under certain conditions, we show that its rate of convergence in the new class can be much faster than that achieved by the AIPW estimator and, in particular, matches the minimax lower bound rate, thereby establishing its optimality.","Finally, we clarify the connection between DRAL estimators and those based on higher-order influence functions (Robins et al, 2017) and complement our theoretical findings with simulations."],"url":"http://arxiv.org/abs/2405.08525v1","category":"stat.ME"}
{"created":"2024-05-14 12:01:23","title":"The Asymptotic Properties of the Extreme Eigenvectors of High-dimensional Generalized Spiked Covariance Model","abstract":"In this paper, we investigate the asymptotic behaviors of the extreme eigenvectors in a general spiked covariance matrix, where the dimension and sample size increase proportionally. We eliminate the restrictive assumption of the block diagonal structure in the population covariance matrix. Moreover, there is no requirement for the spiked eigenvalues and the 4th moment to be bounded. Specifically, we apply random matrix theory to derive the convergence and limiting distributions of certain projections of the extreme eigenvectors in a large sample covariance matrix within a generalized spiked population model. Furthermore, our techniques are robust and effective, even when spiked eigenvalues differ significantly in magnitude from nonspiked ones. Finally, we propose a powerful statistic for hypothesis testing for the eigenspaces of covariance matrices.","sentences":["In this paper, we investigate the asymptotic behaviors of the extreme eigenvectors in a general spiked covariance matrix, where the dimension and sample size increase proportionally.","We eliminate the restrictive assumption of the block diagonal structure in the population covariance matrix.","Moreover, there is no requirement for the spiked eigenvalues and the 4th moment to be bounded.","Specifically, we apply random matrix theory to derive the convergence and limiting distributions of certain projections of the extreme eigenvectors in a large sample covariance matrix within a generalized spiked population model.","Furthermore, our techniques are robust and effective, even when spiked eigenvalues differ significantly in magnitude from nonspiked ones.","Finally, we propose a powerful statistic for hypothesis testing for the eigenspaces of covariance matrices."],"url":"http://arxiv.org/abs/2405.08524v1","category":"math.ST"}
{"created":"2024-05-14 11:56:26","title":"In Search of the Biggest Bangs since the Big Bang","abstract":"Many galaxies contain supermassive black holes (SMBHs), whose formation and history raise many puzzles. Pulsar timing arrays have recently discovered a low-frequency cosmological \"hum\" of gravitational waves that may be emitted by SMBH binary systems, and the JWST and other telescopes have discovered an unexpectedly large population of high-redshift SMBHs. We argue that these two discoveries may be linked, and that they may enhance the prospects for measuring gravitational waves emitted during the mergers of massive black holes, thereby opening the way towards resolving many puzzles about SMBHs as well as providing new opportunities to probe general relativity.","sentences":["Many galaxies contain supermassive black holes (SMBHs), whose formation and history raise many puzzles.","Pulsar timing arrays have recently discovered a low-frequency cosmological \"hum\" of gravitational waves that may be emitted by SMBH binary systems, and the JWST and other telescopes have discovered an unexpectedly large population of high-redshift SMBHs.","We argue that these two discoveries may be linked, and that they may enhance the prospects for measuring gravitational waves emitted during the mergers of massive black holes, thereby opening the way towards resolving many puzzles about SMBHs as well as providing new opportunities to probe general relativity."],"url":"http://arxiv.org/abs/2405.08522v1","category":"gr-qc"}
{"created":"2024-05-14 11:54:46","title":"Empowering Programmable Wireless Environments with Optical Anchor-based Positioning","abstract":"The evolution toward sixth-generation (6G) wireless networks has introduced programmable wireless environments (PWEs) and reconfigurable intelligent surfaces (RISs) as transformative elements for achieving near-deterministic wireless communications. However, the enhanced capabilities of RISs within PWEs, especially as we move toward more complex electromagnetic functions by increasing the number of reflecting elements, underscore the need for high-precision user localization, since inaccurate localization could lead to erroneous configuration of RISs, which would then compromise the effectiveness of PWEs. In this direction, this paper investigates the integration of RISs and optical anchors within PWEs, emphasizing the crucial role of ultra-precise localization in unlocking advanced electromagnetic functionalities. Specifically, we present an in-depth analysis of various localization techniques, both RISbased and RIS-independent, while introducing the concept of empowering PWEs with optical anchors for enhanced localization precision. Our findings highlight that accurate localization is essential to fully exploit the capabilities of RISs, paving the way for future applications. Through this exploration, we contribute to the advancement of PWEs in line with the ambitious goals of the 6G standards and improve the quality of service in next generation wireless networks.","sentences":["The evolution toward sixth-generation (6G) wireless networks has introduced programmable wireless environments (PWEs) and reconfigurable intelligent surfaces (RISs) as transformative elements for achieving near-deterministic wireless communications.","However, the enhanced capabilities of RISs within PWEs, especially as we move toward more complex electromagnetic functions by increasing the number of reflecting elements, underscore the need for high-precision user localization, since inaccurate localization could lead to erroneous configuration of RISs, which would then compromise the effectiveness of PWEs.","In this direction, this paper investigates the integration of RISs and optical anchors within PWEs, emphasizing the crucial role of ultra-precise localization in unlocking advanced electromagnetic functionalities.","Specifically, we present an in-depth analysis of various localization techniques, both RISbased and RIS-independent, while introducing the concept of empowering PWEs with optical anchors for enhanced localization precision.","Our findings highlight that accurate localization is essential to fully exploit the capabilities of RISs, paving the way for future applications.","Through this exploration, we contribute to the advancement of PWEs in line with the ambitious goals of the 6G standards and improve the quality of service in next generation wireless networks."],"url":"http://arxiv.org/abs/2405.08520v1","category":"cs.IT"}
{"created":"2024-05-14 11:52:02","title":"Spin Hall magnetoresistance in Pt/(Ga,Mn)N devices","abstract":"Diluted magnetic semiconductors (DMS) have attracted significant attention for their potential in spintronic applications. Particularly, magnetically-doped GaN is highly attractive due to its high relevance for the CMOS industry and the possibility of developing advanced spintronic devices which are fully compatible with the current industrial procedures. Despite this interest, there remains a need to investigate the spintronic parameters that characterize interfaces within these systems. Here, we perform spin Hall magnetoresistance (SMR) measurements to evaluate the spin transfer at a Pt/(Ga,Mn)N interface. We determine the transparency of the interface through the estimation of the real part of the spin mixing conductance finding $G_r = 2.6\\times 10^{14} \\, \\Omega^{-1} m^{-2}$, comparable to state-of-the-art yttrium iron garnet (YIG)/Pt interfaces. Moreover, the magnetic ordering probed by SMR above the (Ga,Mn)N Curie temperature TC provides a broader temperature range for the efficient generation and detection of spin currents, relaxing the conditions for this material to be applied in new spintronic devices.","sentences":["Diluted magnetic semiconductors (DMS) have attracted significant attention for their potential in spintronic applications.","Particularly, magnetically-doped GaN is highly attractive due to its high relevance for the CMOS industry and the possibility of developing advanced spintronic devices which are fully compatible with the current industrial procedures.","Despite this interest, there remains a need to investigate the spintronic parameters that characterize interfaces within these systems.","Here, we perform spin Hall magnetoresistance (SMR) measurements to evaluate the spin transfer at a Pt/(Ga,Mn)N interface.","We determine the transparency of the interface through the estimation of the real part of the spin mixing conductance finding $G_r = 2.6\\times 10^{14} \\, \\Omega^{-1} m^{-2}$, comparable to state-of-the-art yttrium iron garnet (YIG)/Pt interfaces.","Moreover, the magnetic ordering probed by SMR above the (Ga,Mn)N Curie temperature TC provides a broader temperature range for the efficient generation and detection of spin currents, relaxing the conditions for this material to be applied in new spintronic devices."],"url":"http://arxiv.org/abs/2405.08519v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-14 11:40:56","title":"Magnetization dynamics in skyrmions due to high-speed carrier injections from Dirac half-metals","abstract":"Recent developments in the magnetization dynamics in spin textures, particularly skyrmions, offer promising new directions for magnetic storage technologies and spintronics. Skyrmions, characterized by their topological protection and efficient mobility at low current density, are increasingly recognized for their potential applications in next-generation logic and memory devices. This study investigates the dynamics of skyrmion magnetization, focusing on the manipulation of their topological states as a basis for bitwise data storage through a modified Landau-Lifshitz-Gilbert equation (LLG). We introduce spin-polarized electrons from a topological ferromagnet that induce an electric dipole moment that interacts with the electric gauge field within the skyrmion domain. This interaction creates an effective magnetic field that results in a torque that can dynamically change the topological state of the skyrmion. In particular, we show that these torques can selectively destroy and create skyrmions, effectively writing and erasing bits, highlighting the potential of using controlled electron injection for robust and scalable skyrmion-based data storage solutions.","sentences":["Recent developments in the magnetization dynamics in spin textures, particularly skyrmions, offer promising new directions for magnetic storage technologies and spintronics.","Skyrmions, characterized by their topological protection and efficient mobility at low current density, are increasingly recognized for their potential applications in next-generation logic and memory devices.","This study investigates the dynamics of skyrmion magnetization, focusing on the manipulation of their topological states as a basis for bitwise data storage through a modified Landau-Lifshitz-Gilbert equation (LLG).","We introduce spin-polarized electrons from a topological ferromagnet that induce an electric dipole moment that interacts with the electric gauge field within the skyrmion domain.","This interaction creates an effective magnetic field that results in a torque that can dynamically change the topological state of the skyrmion.","In particular, we show that these torques can selectively destroy and create skyrmions, effectively writing and erasing bits, highlighting the potential of using controlled electron injection for robust and scalable skyrmion-based data storage solutions."],"url":"http://arxiv.org/abs/2405.08516v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 11:21:52","title":"Growing Artificial Neural Networks for Control: the Role of Neuronal Diversity","abstract":"In biological evolution complex neural structures grow from a handful of cellular ingredients. As genomes in nature are bounded in size, this complexity is achieved by a growth process where cells communicate locally to decide whether to differentiate, proliferate and connect with other cells. This self-organisation is hypothesized to play an important part in the generalisation, and robustness of biological neural networks. Artificial neural networks (ANNs), on the other hand, are traditionally optimized in the space of weights. Thus, the benefits and challenges of growing artificial neural networks remain understudied. Building on the previously introduced Neural Developmental Programs (NDP), in this work we present an algorithm for growing ANNs that solve reinforcement learning tasks. We identify a key challenge: ensuring phenotypic complexity requires maintaining neuronal diversity, but this diversity comes at the cost of optimization stability. To address this, we introduce two mechanisms: (a) equipping neurons with an intrinsic state inherited upon neurogenesis; (b) lateral inhibition, a mechanism inspired by biological growth, which controlls the pace of growth, helping diversity persist. We show that both mechanisms contribute to neuronal diversity and that, equipped with them, NDPs achieve comparable results to existing direct and developmental encodings in complex locomotion tasks","sentences":["In biological evolution complex neural structures grow from a handful of cellular ingredients.","As genomes in nature are bounded in size, this complexity is achieved by a growth process where cells communicate locally to decide whether to differentiate, proliferate and connect with other cells.","This self-organisation is hypothesized to play an important part in the generalisation, and robustness of biological neural networks.","Artificial neural networks (ANNs), on the other hand, are traditionally optimized in the space of weights.","Thus, the benefits and challenges of growing artificial neural networks remain understudied.","Building on the previously introduced Neural Developmental Programs (NDP), in this work we present an algorithm for growing ANNs that solve reinforcement learning tasks.","We identify a key challenge: ensuring phenotypic complexity requires maintaining neuronal diversity, but this diversity comes at the cost of optimization stability.","To address this, we introduce two mechanisms: (a) equipping neurons with an intrinsic state inherited upon neurogenesis; (b) lateral inhibition, a mechanism inspired by biological growth, which controlls the pace of growth, helping diversity persist.","We show that both mechanisms contribute to neuronal diversity and that, equipped with them, NDPs achieve comparable results to existing direct and developmental encodings in complex locomotion tasks"],"url":"http://arxiv.org/abs/2405.08510v1","category":"cs.NE"}
{"created":"2024-05-14 11:12:55","title":"Imaging Localized Variable Capacitance During Switching Processes in Silicon Diodes by Time-Resolved Electron Holography","abstract":"Interference Gating or iGate is a unique method for ultrafast time-resolved electron holography in a transmission electron microscope enabling a spatiotemporal resolution in the nm and ns regime with a minimal technological effort. Here, iGate is used for the first image-based investigation of the local dynamics of the projected electric potential in the area of the space charge region of two different general purpose silicon diodes during switching between unbiased and reverse biased condition with a temporal resolution of 25ns at a repetition rate of 3MHz. The obtained results for a focus-ion-beam-prepared ultrafast UG1A rectifier diode, which shows a decreasing capacitance with increasing reverse bias are in good agreement with an electric characterization of the macroscopic device as well as with theoretical expectations. For a severely modified 1N4007 device, however, time-resolved electron holography revealed a MOSCAP-like behavior with a rising capacitance in the area of the space charge region during the switching into reverse biased condition. Remarkably, a different behavior, dominated by the effective capacitance of the electrical setup, can be observed in the vacuum region outside both devices within the same measurements, clearly showing the benefits of localized dynamic potentiometry.","sentences":["Interference Gating or iGate is a unique method for ultrafast time-resolved electron holography in a transmission electron microscope enabling a spatiotemporal resolution in the nm and ns regime with a minimal technological effort.","Here, iGate is used for the first image-based investigation of the local dynamics of the projected electric potential in the area of the space charge region of two different general purpose silicon diodes during switching between unbiased and reverse biased condition with a temporal resolution of 25ns at a repetition rate of 3MHz.","The obtained results for a focus-ion-beam-prepared ultrafast UG1A rectifier diode, which shows a decreasing capacitance with increasing reverse bias are in good agreement with an electric characterization of the macroscopic device as well as with theoretical expectations.","For a severely modified 1N4007 device, however, time-resolved electron holography revealed a MOSCAP-like behavior with a rising capacitance in the area of the space charge region during the switching into reverse biased condition.","Remarkably, a different behavior, dominated by the effective capacitance of the electrical setup, can be observed in the vacuum region outside both devices within the same measurements, clearly showing the benefits of localized dynamic potentiometry."],"url":"http://arxiv.org/abs/2405.08505v2","category":"physics.app-ph"}
{"created":"2024-05-14 11:11:47","title":"Vacuum currents in curved tubes","abstract":"We investigate the combined effects of spatial curvature and topology on the properties of the vacuum state for a charged scalar field localized on rotationally symmetric 2D curved tubes. For a general spatial geometry and for quasiperiodicity condition with a general phase, the representation of the Hadamard function is provided where the topological contribution is explicitly extracted. As an important local characteristic of the vacuum state the expectation value of the current density is studied. The vacuum current is a periodic function of the magnetic flux enclosed by the tube with the period of flux quantum. The general formula is specified for constant radius and conical tubes. As another application, we consider the Hadamard function and the vacuum current density for a scalar field on the Beltrami pseudosphere. Several representations are provided for the corresponding expectation value. For small values of the proper radius of the tube, compared with the curvature radius, the effect of spatial curvature on the vacuum current is weak and the leading term in the corresponding expansion coincides with the current density on a constant radius tube. The effect of curvature is essential for proper radii of the tube larger than the radius of spatial curvature. In this limit the fall-off of the current density, as a function of the proper radius, follows a power-law for both massless and massive fields. This behavior is in clear contrast to the one for a constant radius tube with exponential decay for massive fields. We also compare the vacuum currents on the Beltrami pseudosphere and on locally de Sitter and anti-de Sitter 2D tubes.","sentences":["We investigate the combined effects of spatial curvature and topology on the properties of the vacuum state for a charged scalar field localized on rotationally symmetric 2D curved tubes.","For a general spatial geometry and for quasiperiodicity condition with a general phase, the representation of the Hadamard function is provided where the topological contribution is explicitly extracted.","As an important local characteristic of the vacuum state the expectation value of the current density is studied.","The vacuum current is a periodic function of the magnetic flux enclosed by the tube with the period of flux quantum.","The general formula is specified for constant radius and conical tubes.","As another application, we consider the Hadamard function and the vacuum current density for a scalar field on the Beltrami pseudosphere.","Several representations are provided for the corresponding expectation value.","For small values of the proper radius of the tube, compared with the curvature radius, the effect of spatial curvature on the vacuum current is weak and the leading term in the corresponding expansion coincides with the current density on a constant radius tube.","The effect of curvature is essential for proper radii of the tube larger than the radius of spatial curvature.","In this limit the fall-off of the current density, as a function of the proper radius, follows a power-law for both massless and massive fields.","This behavior is in clear contrast to the one for a constant radius tube with exponential decay for massive fields.","We also compare the vacuum currents on the Beltrami pseudosphere and on locally de Sitter and anti-de Sitter 2D tubes."],"url":"http://arxiv.org/abs/2405.08504v1","category":"hep-th"}
{"created":"2024-05-14 11:04:16","title":"Archimedes-AUEB at SemEval-2024 Task 5: LLM explains Civil Procedure","abstract":"The SemEval task on Argument Reasoning in Civil Procedure is challenging in that it requires understanding legal concepts and inferring complex arguments. Currently, most Large Language Models (LLM) excelling in the legal realm are principally purposed for classification tasks, hence their reasoning rationale is subject to contention. The approach we advocate involves using a powerful teacher-LLM (ChatGPT) to extend the training dataset with explanations and generate synthetic data. The resulting data are then leveraged to fine-tune a small student-LLM. Contrary to previous work, our explanations are not directly derived from the teacher's internal knowledge. Instead they are grounded in authentic human analyses, therefore delivering a superior reasoning signal. Additionally, a new `mutation' method generates artificial data instances inspired from existing ones. We are publicly releasing the explanations as an extension to the original dataset, along with the synthetic dataset and the prompts that were used to generate both. Our system ranked 15th in the SemEval competition. It outperforms its own teacher and can produce explanations aligned with the original human analyses, as verified by legal experts.","sentences":["The SemEval task on Argument Reasoning in Civil Procedure is challenging in that it requires understanding legal concepts and inferring complex arguments.","Currently, most Large Language Models (LLM) excelling in the legal realm are principally purposed for classification tasks, hence their reasoning rationale is subject to contention.","The approach we advocate involves using a powerful teacher-LLM (ChatGPT) to extend the training dataset with explanations and generate synthetic data.","The resulting data are then leveraged to fine-tune a small student-LLM.","Contrary to previous work, our explanations are not directly derived from the teacher's internal knowledge.","Instead they are grounded in authentic human analyses, therefore delivering a superior reasoning signal.","Additionally, a new `mutation' method generates artificial data instances inspired from existing ones.","We are publicly releasing the explanations as an extension to the original dataset, along with the synthetic dataset and the prompts that were used to generate both.","Our system ranked 15th in the SemEval competition.","It outperforms its own teacher and can produce explanations aligned with the original human analyses, as verified by legal experts."],"url":"http://arxiv.org/abs/2405.08502v1","category":"cs.CL"}
{"created":"2024-05-14 10:55:09","title":"$Z_{2}$ order fractionalization, topological phase transition, and odd frequency pairing in an exactly solvable spin-charge ladder","abstract":"Motivated by the order fractionalization in Kitaev-Kondo model, we propose an exactly solvable spin-charge ladder model to study the order fractionalization with discrete symmetry. The spin-charge ladder is composed of a spin chain and a superconducting wire coupled via an Ising-type interaction, and we obtain the exact solution in the flat band limit. The exact solution reveals the $Z_{2}$ order fractionalization with dual symmetry breaking and intertwined order parameters. We investigate the topological phase transition of the spin-charge ladder via the spectral chiral index, and identify the correlated topological superconductor (TSC*) phase with gapped $Z_{2}$ Kondo flux excitations. We demonstrate Majorana spinons generated odd frequency pairing in the superconducting wire. We also discuss the order fractionalization in the perspective of $Z_{2}$ lattice gauge theory.","sentences":["Motivated by the order fractionalization in Kitaev-Kondo model, we propose an exactly solvable spin-charge ladder model to study the order fractionalization with discrete symmetry.","The spin-charge ladder is composed of a spin chain and a superconducting wire coupled via an Ising-type interaction, and we obtain the exact solution in the flat band limit.","The exact solution reveals the $Z_{2}$ order fractionalization with dual symmetry breaking and intertwined order parameters.","We investigate the topological phase transition of the spin-charge ladder via the spectral chiral index, and identify the correlated topological superconductor (TSC*) phase with gapped $Z_{2}$ Kondo flux excitations.","We demonstrate Majorana spinons generated odd frequency pairing in the superconducting wire.","We also discuss the order fractionalization in the perspective of $Z_{2}$ lattice gauge theory."],"url":"http://arxiv.org/abs/2405.08499v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 10:52:18","title":"Influence of Organic Molecules and Phosphate Ions on the Formation of Biosilica Patterns in Diatoms","abstract":"The formation of regularly structured silica valves in diatoms is a captivating process in biomineralization. Specific organic molecules, including long-chain polyamines, silaffins, and silacidins, have been found to be essential in this process. The molecular structure of synthesized polyamines greatly affects the quantity, size, and shape of silica precipitates. Experimental findings indicate that silica precipitation occurs at specific concentrations of phosphate ions, with higher concentrations resulting in larger aggregates of organic molecules that serve as templates for silica formation.   Our study focuses on the hypothesis that pattern formation in diatom valve structures is driven by the phase separation of species-specific organic molecules, with the evolving organic structures acting as templates for silica deposition. We specifically investigate the role of phosphate ions in the self-assembly of long-chain polyamines and analyze how their reaction with organic molecules impacts the morphology of the organic template. By varying the model parameters, including degree of dissociation and initial concentrations of reacting components, we show that the resulting geometric features of the patterns are highly dependent on these factors.   Furthermore, we explore the scenario where an initial array of organic droplets serves as a static template for silica deposition, considering the effects of \"prepatterning\" by the silica costae in the base layer. We obtain numerical solutions that generate a diverse range of two-dimensional patterns closely resembling the valve structures observed experimentally.","sentences":["The formation of regularly structured silica valves in diatoms is a captivating process in biomineralization.","Specific organic molecules, including long-chain polyamines, silaffins, and silacidins, have been found to be essential in this process.","The molecular structure of synthesized polyamines greatly affects the quantity, size, and shape of silica precipitates.","Experimental findings indicate that silica precipitation occurs at specific concentrations of phosphate ions, with higher concentrations resulting in larger aggregates of organic molecules that serve as templates for silica formation.   ","Our study focuses on the hypothesis that pattern formation in diatom valve structures is driven by the phase separation of species-specific organic molecules, with the evolving organic structures acting as templates for silica deposition.","We specifically investigate the role of phosphate ions in the self-assembly of long-chain polyamines and analyze how their reaction with organic molecules impacts the morphology of the organic template.","By varying the model parameters, including degree of dissociation and initial concentrations of reacting components, we show that the resulting geometric features of the patterns are highly dependent on these factors.   ","Furthermore, we explore the scenario where an initial array of organic droplets serves as a static template for silica deposition, considering the effects of \"prepatterning\" by the silica costae in the base layer.","We obtain numerical solutions that generate a diverse range of two-dimensional patterns closely resembling the valve structures observed experimentally."],"url":"http://arxiv.org/abs/2405.08496v1","category":"physics.bio-ph"}
{"created":"2024-05-14 10:51:48","title":"A randomly generated Majorana neutrino mass matrix using Adaptive Monte Carlo method","abstract":"A randomly generated complex symmetric matrix using Adaptive Monte Carlo method, is taken as a general form of Majorana neutrino mass matrix, which is diagonalized by the use of eigenvectors. We extract all the neutrino oscillation parameters i.e. two mass-squared differences ($\\Delta m_{21}^2$ and $\\Delta m_{32}^2$ ), three mixing angles ($\\theta_{12}$, $\\theta_{13}$, $\\theta_{23}$) and three phases i.e. one Dirac CP violating phase ($\\delta_{CP}$) and two Majorana phases ($\\alpha$ and $\\beta$). The charge-parity (CP) violating phases are extracted from the mixing matrix constructed with the eigenvectors of the Hermitian matrix formed by the complex symmetric matrix. All the neutrino oscillation parameters within 3$\\sigma$ bound are allowed in both normal hierarchy (NH) and inverted hierarchy (IH) consistent with the latest Planck cosmological upper bound, $\\sum\\vert m_i\\vert<0.12$ eV. This latest cosmological upper bound is allowed only in three cases of zero texture for $m_{11}=0$; $m_{11},m_{12}=0$ and $m_{11},m_{13}=0$ in normal hierarchy whereas none of zero texture is allowed in inverted hierarchy. We also study effective neutrino masses $m_{\\beta}$ in tritium beta decay and $m_{\\beta\\beta}$ in neutrinoless double beta decay.","sentences":["A randomly generated complex symmetric matrix using Adaptive Monte Carlo method, is taken as a general form of Majorana neutrino mass matrix, which is diagonalized by the use of eigenvectors.","We extract all the neutrino oscillation parameters i.e. two mass-squared differences ($\\Delta m_{21}^2$ and $\\Delta m_{32}^2$ ), three mixing angles ($\\theta_{12}$, $\\theta_{13}$, $\\theta_{23}$) and three phases i.e. one Dirac CP violating phase ($\\delta_{CP}$) and two Majorana phases ($\\alpha$ and $\\beta$).","The charge-parity (CP) violating phases are extracted from the mixing matrix constructed with the eigenvectors of the Hermitian matrix formed by the complex symmetric matrix.","All the neutrino oscillation parameters within 3$\\sigma$ bound are allowed in both normal hierarchy (NH) and inverted hierarchy (IH) consistent with the latest Planck cosmological upper bound, $\\sum\\vert m_i\\vert<0.12$ eV. This latest cosmological upper bound is allowed only in three cases of zero texture for $m_{11}=0$; $m_{11},m_{12}=0$ and $m_{11},m_{13}=0$ in normal hierarchy whereas none of zero texture is allowed in inverted hierarchy.","We also study effective neutrino masses $m_{\\beta}$ in tritium beta decay and $m_{\\beta\\beta}$ in neutrinoless double beta decay."],"url":"http://arxiv.org/abs/2405.08495v1","category":"hep-ph"}
{"created":"2024-05-14 10:29:20","title":"Effects of skull properties on transcranial focused ultrasound transmission","abstract":"Transcranial focused ultrasound can deliver energy to the brain in a minimally invasive manner for a variety of clinical applications. However, acoustic inhomogeneities within the skull cause significant wave interactions, leading to difficulties in predicting the energy delivered to the target. We present a comprehensive examination of intracranial acoustic fields generated by focused ultrasound transducers and assess the characteristics of cranial bone that affect acoustic transmission. Acoustic field maps were generated at 88 regions of interest across 10 historical and 2 Thiel-embalmed human skull specimens with sonication at frequencies of 220 kHz, 650 kHz, and 1000 kHz. The average peak pressure attenuation was $3.1 \\pm 1.4$ dB, $9.0 \\pm 1.7$ dB, and $14.6 \\pm 4.1$ dB, respectively. The average power attenuation was $5.0 \\pm 2.4$ dB, $14.9 \\pm 3.2$ dB, and $24.1 \\pm 6.3$ dB, respectively. The effect of skull thickness, skull density ratio, and skull curvature on intracranial peak pressure and power was investigated and linear fits produced. Our results demonstrate that Thiel-embalmed samples fall within the confidence intervals of fits for historical samples. The effects of angulation and spacing between the transducer and the skull were also investigated. Results indicate that wave superposition resulting from skull and transducer spacing could lead to a 30-40% uncertainty in peak recorded intracranial pressure.","sentences":["Transcranial focused ultrasound can deliver energy to the brain in a minimally invasive manner for a variety of clinical applications.","However, acoustic inhomogeneities within the skull cause significant wave interactions, leading to difficulties in predicting the energy delivered to the target.","We present a comprehensive examination of intracranial acoustic fields generated by focused ultrasound transducers and assess the characteristics of cranial bone that affect acoustic transmission.","Acoustic field maps were generated at 88 regions of interest across 10 historical and 2 Thiel-embalmed human skull specimens with sonication at frequencies of 220 kHz, 650 kHz, and 1000 kHz.","The average peak pressure attenuation was $3.1 \\pm 1.4$ dB, $9.0 \\pm 1.7$ dB, and $14.6 \\pm 4.1$ dB, respectively.","The average power attenuation was $5.0 \\pm 2.4$ dB, $14.9 \\pm 3.2$ dB, and $24.1 \\pm 6.3$ dB, respectively.","The effect of skull thickness, skull density ratio, and skull curvature on intracranial peak pressure and power was investigated and linear fits produced.","Our results demonstrate that Thiel-embalmed samples fall within the confidence intervals of fits for historical samples.","The effects of angulation and spacing between the transducer and the skull were also investigated.","Results indicate that wave superposition resulting from skull and transducer spacing could lead to a 30-40% uncertainty in peak recorded intracranial pressure."],"url":"http://arxiv.org/abs/2405.08489v1","category":"physics.med-ph"}
{"created":"2024-05-14 10:29:04","title":"Metastable hierarchy in abstract low-temperature lattice models: an application to Kawasaki dynamics for Ising lattice gas with macrscopic number of particles","abstract":"This article is divided into two parts. In the first part, we study the hierarchical phenomenon of metastability in low-temperature lattice models in the most general setting. Given an abstract dynamical system governed by a Hamiltonian function, we prove that there exists a hierarchical decomposition of the collection of stable plateaux in the system into multiple $\\mathfrak{m}$ levels, such that at each level there exist tunneling metastable transitions between the stable plateaux, which can be characterized by convergence to a simple Markov chain as the inverse temperature $\\beta$ tends to infinity. In the second part, as an application, we characterize the $3$-level metastable hierarchy in Kawasaki dynamics for Ising lattice gas with macroscopic number of particles. We prove that the ground states in this model are those in which the particles line up and form a one-dimensional strip, and identify the full structure relevant to the tunneling transitions between these ground states. In particular, the results differ from the previous work [5] in that the particles in the ground states are likely to form a strip rather than a square droplet. The main tool is the resolvent approach to metastability, recently developed in [24]. Along with the analysis, we present a theorem on the sharp asymptotics of the exit distribution from cycles, which to the author's knowledge is not known in the community and therefore may be of independent interest.","sentences":["This article is divided into two parts.","In the first part, we study the hierarchical phenomenon of metastability in low-temperature lattice models in the most general setting.","Given an abstract dynamical system governed by a Hamiltonian function, we prove that there exists a hierarchical decomposition of the collection of stable plateaux in the system into multiple $\\mathfrak{m}$ levels, such that at each level there exist tunneling metastable transitions between the stable plateaux, which can be characterized by convergence to a simple Markov chain as the inverse temperature $\\beta$ tends to infinity.","In the second part, as an application, we characterize the $3$-level metastable hierarchy in Kawasaki dynamics for Ising lattice gas with macroscopic number of particles.","We prove that the ground states in this model are those in which the particles line up and form a one-dimensional strip, and identify the full structure relevant to the tunneling transitions between these ground states.","In particular, the results differ from the previous work [5] in that the particles in the ground states are likely to form a strip rather than a square droplet.","The main tool is the resolvent approach to metastability, recently developed in [24].","Along with the analysis, we present a theorem on the sharp asymptotics of the exit distribution from cycles, which to the author's knowledge is not known in the community and therefore may be of independent interest."],"url":"http://arxiv.org/abs/2405.08488v1","category":"math.PR"}
{"created":"2024-05-14 10:24:19","title":"Semantic Contextualization of Face Forgery: A New Definition, Dataset, and Detection Method","abstract":"In recent years, deep learning has greatly streamlined the process of generating realistic fake face images. Aware of the dangers, researchers have developed various tools to spot these counterfeits. Yet none asked the fundamental question: What digital manipulations make a real photographic face image fake, while others do not? In this paper, we put face forgery in a semantic context and define that computational methods that alter semantic face attributes to exceed human discrimination thresholds are sources of face forgery. Guided by our new definition, we construct a large face forgery image dataset, where each image is associated with a set of labels organized in a hierarchical graph. Our dataset enables two new testing protocols to probe the generalization of face forgery detectors. Moreover, we propose a semantics-oriented face forgery detection method that captures label relations and prioritizes the primary task (\\ie, real or fake face detection). We show that the proposed dataset successfully exposes the weaknesses of current detectors as the test set and consistently improves their generalizability as the training set. Additionally, we demonstrate the superiority of our semantics-oriented method over traditional binary and multi-class classification-based detectors.","sentences":["In recent years, deep learning has greatly streamlined the process of generating realistic fake face images.","Aware of the dangers, researchers have developed various tools to spot these counterfeits.","Yet none asked the fundamental question: What digital manipulations make a real photographic face image fake, while others do not?","In this paper, we put face forgery in a semantic context and define that computational methods that alter semantic face attributes to exceed human discrimination thresholds are sources of face forgery.","Guided by our new definition, we construct a large face forgery image dataset, where each image is associated with a set of labels organized in a hierarchical graph.","Our dataset enables two new testing protocols to probe the generalization of face forgery detectors.","Moreover, we propose a semantics-oriented face forgery detection method that captures label relations and prioritizes the primary task (\\ie, real or fake face detection).","We show that the proposed dataset successfully exposes the weaknesses of current detectors as the test set and consistently improves their generalizability as the training set.","Additionally, we demonstrate the superiority of our semantics-oriented method over traditional binary and multi-class classification-based detectors."],"url":"http://arxiv.org/abs/2405.08487v1","category":"cs.CV"}
{"created":"2024-05-14 10:18:38","title":"Doubly relaxed forward-Douglas--Rachford splitting for the sum of two nonconvex and a DC function","abstract":"In this paper, we consider a class of structured nonconvex nonsmooth optimization problems whose objective function is the sum of three nonconvex functions, one of which is expressed in a difference-of-convex (DC) form. This problem class covers several important structures in the literature including the sum of three functions and the general DC program. We propose a splitting algorithm and prove the subsequential convergence to a stationary point of the problem. The full sequential convergence, along with convergence rates for both the iterates and objective function values, is then established without requiring differentiability of the concave part. Our analysis not only extends but also unifies and improves recent convergence analyses in nonconvex settings. We benchmark our proposed algorithm with notable algorithms in the literature to show its competitiveness on both synthetic data and real power system load data.","sentences":["In this paper, we consider a class of structured nonconvex nonsmooth optimization problems whose objective function is the sum of three nonconvex functions, one of which is expressed in a difference-of-convex (DC) form.","This problem class covers several important structures in the literature including the sum of three functions and the general DC program.","We propose a splitting algorithm and prove the subsequential convergence to a stationary point of the problem.","The full sequential convergence, along with convergence rates for both the iterates and objective function values, is then established without requiring differentiability of the concave part.","Our analysis not only extends but also unifies and improves recent convergence analyses in nonconvex settings.","We benchmark our proposed algorithm with notable algorithms in the literature to show its competitiveness on both synthetic data and real power system load data."],"url":"http://arxiv.org/abs/2405.08485v1","category":"math.OC"}
{"created":"2024-05-14 10:10:45","title":"RDPN6D: Residual-based Dense Point-wise Network for 6Dof Object Pose Estimation Based on RGB-D Images","abstract":"In this work, we introduce a novel method for calculating the 6DoF pose of an object using a single RGB-D image. Unlike existing methods that either directly predict objects' poses or rely on sparse keypoints for pose recovery, our approach addresses this challenging task using dense correspondence, i.e., we regress the object coordinates for each visible pixel. Our method leverages existing object detection methods. We incorporate a re-projection mechanism to adjust the camera's intrinsic matrix to accommodate cropping in RGB-D images. Moreover, we transform the 3D object coordinates into a residual representation, which can effectively reduce the output space and yield superior performance. We conducted extensive experiments to validate the efficacy of our approach for 6D pose estimation. Our approach outperforms most previous methods, especially in occlusion scenarios, and demonstrates notable improvements over the state-of-the-art methods. Our code is available on https://github.com/AI-Application-and-Integration-Lab/RDPN6D.","sentences":["In this work, we introduce a novel method for calculating the 6DoF pose of an object using a single RGB-D image.","Unlike existing methods that either directly predict objects' poses or rely on sparse keypoints for pose recovery, our approach addresses this challenging task using dense correspondence, i.e., we regress the object coordinates for each visible pixel.","Our method leverages existing object detection methods.","We incorporate a re-projection mechanism to adjust the camera's intrinsic matrix to accommodate cropping in RGB-D images.","Moreover, we transform the 3D object coordinates into a residual representation, which can effectively reduce the output space and yield superior performance.","We conducted extensive experiments to validate the efficacy of our approach for 6D pose estimation.","Our approach outperforms most previous methods, especially in occlusion scenarios, and demonstrates notable improvements over the state-of-the-art methods.","Our code is available on https://github.com/AI-Application-and-Integration-Lab/RDPN6D."],"url":"http://arxiv.org/abs/2405.08483v1","category":"cs.CV"}
{"created":"2024-05-14 10:08:46","title":"A practical transmitter device for passive state BB84 quantum key distribution","abstract":"In prepare-and-measure quantum key distribution systems, careful preparation of quantum states within the transmitter device is a significant driver of both complexity and cost. Moreover, the security guarantees of such systems rest on the correct operation of high speed quantum random number generators (QRNGs) and the high-fidelity modulation of weak optical signals by high-speed optoelectronic devices, all of which must be hardened against a variety of known side-channel attacks.   A fully passive state preparation approach elegantly resolves these problems by combining state preparation and QRNG stages into a single optical instrument. By using pairs of optical pulses from a gain-switched laser diode as ready-to-use qubits, the QKD transmitter can be radically simplified, eventually comprising a single laser and local phase tomography stage. We demonstrate our simplified transmitter by establishing a QKD link over a 10 km fiber, generating a secret key rate 110 bits/s, sufficient for practical deployment in \"last mile\" urban quantum networks. Our results show promise in making QKD simpler and more accessible, closing a critical technology gap in building a secure quantum communication infrastructure.","sentences":["In prepare-and-measure quantum key distribution systems, careful preparation of quantum states within the transmitter device is a significant driver of both complexity and cost.","Moreover, the security guarantees of such systems rest on the correct operation of high speed quantum random number generators (QRNGs) and the high-fidelity modulation of weak optical signals by high-speed optoelectronic devices, all of which must be hardened against a variety of known side-channel attacks.   ","A fully passive state preparation approach elegantly resolves these problems by combining state preparation and QRNG stages into a single optical instrument.","By using pairs of optical pulses from a gain-switched laser diode as ready-to-use qubits, the QKD transmitter can be radically simplified, eventually comprising a single laser and local phase tomography stage.","We demonstrate our simplified transmitter by establishing a QKD link over a 10 km fiber, generating a secret key rate 110 bits/s, sufficient for practical deployment in \"last mile\" urban quantum networks.","Our results show promise in making QKD simpler and more accessible, closing a critical technology gap in building a secure quantum communication infrastructure."],"url":"http://arxiv.org/abs/2405.08481v1","category":"quant-ph"}
{"created":"2024-05-14 10:02:20","title":"Cosmological constraints on mass-varying dark matter","abstract":"Light mass warm dark matter is an interesting and viable alternative to the cold dark matter paradigm. An intriguing variation of this scenario is the mass-varying dark matter model where the dark matter mass varies with time during its cosmic history. This is realized in multiple particle physics models. In this work, we study the cosmological constraints on such a model where the dark matter mass transitions from zero to a finite value in the early Universe. In this model, the matter power spectrum exhibits power suppression below a scale that depends on the epoch of transition, and the angular power spectrum of the cosmic microwave background show a distinctive phase shift. We use the latest cosmic microwave background and the weak lensing data to place lower limit on the transition redshift and ease the $S_8$ tension, unlike the warm dark matter model. This analysis also facilitates a marginal detection of the dark matter (DM) mass. Our findings reveal that while Planck data alone reduces the $S_8$ tension to approximately $2\\sigma$, it does not sufficiently constrain the DM mass. However, when combined with the $S_8$ measurement from KIDS1000+BOSS+2dfLenS, the tension significantly decreases to roughly $1.3\\sigma$, and we observe the detection of a DM mass at $41.7^{+7.81}_{-27.5}\\,\\mathrm{eV}$. Further analysis incorporating a combined data set from ACT and weak lensing results in an even more pronounced reduction in the tension to approximately $0.4\\sigma$, alongside a higher detected mass of $51.2^{+16}_{-33.5}\\,\\mathrm{eV}$. We also find a better fit to the combined data compared to the $\\Lambda$CDM model.","sentences":["Light mass warm dark matter is an interesting and viable alternative to the cold dark matter paradigm.","An intriguing variation of this scenario is the mass-varying dark matter model where the dark matter mass varies with time during its cosmic history.","This is realized in multiple particle physics models.","In this work, we study the cosmological constraints on such a model where the dark matter mass transitions from zero to a finite value in the early Universe.","In this model, the matter power spectrum exhibits power suppression below a scale that depends on the epoch of transition, and the angular power spectrum of the cosmic microwave background show a distinctive phase shift.","We use the latest cosmic microwave background and the weak lensing data to place lower limit on the transition redshift and ease the $S_8$ tension, unlike the warm dark matter model.","This analysis also facilitates a marginal detection of the dark matter (DM) mass.","Our findings reveal that while Planck data alone reduces the $S_8$ tension to approximately $2\\sigma$, it does not sufficiently constrain the DM mass.","However, when combined with the $S_8$ measurement from KIDS1000+BOSS+2dfLenS, the tension significantly decreases to roughly $1.3\\sigma$, and we observe the detection of a DM mass at $41.7^{+7.81}_{-27.5}\\,\\mathrm{eV}$. Further analysis incorporating a combined data set from ACT and weak lensing results in an even more pronounced reduction in the tension to approximately $0.4\\sigma$, alongside a higher detected mass of $51.2^{+16}_{-33.5}\\,\\mathrm{eV}$. We also find a better fit to the combined data compared to the $\\Lambda$CDM model."],"url":"http://arxiv.org/abs/2405.08476v1","category":"astro-ph.CO"}
{"created":"2024-05-14 10:01:51","title":"Representing Information on DNA using Patterns Induced by Enzymatic Labeling","abstract":"Enzymatic DNA labeling is a powerful tool with applications in biochemistry, molecular biology, biotechnology, medical science, and genomic research. This paper contributes to the evolving field of DNA-based data storage by presenting a formal framework for modeling DNA labeling in strings, specifically tailored for data storage purposes. Our approach involves a known DNA molecule as a template for labeling, employing patterns induced by a set of designed labels to represent information. One hypothetical implementation can use CRISPR-Cas9 and gRNA reagents for labeling. Various aspects of the general labeling channel, including fixed-length labels, are explored, and upper bounds on the maximal size of the corresponding codes are given. The study includes the development of an efficient encoder-decoder pair that is proven optimal in terms of maximum code size under specific conditions.","sentences":["Enzymatic DNA labeling is a powerful tool with applications in biochemistry, molecular biology, biotechnology, medical science, and genomic research.","This paper contributes to the evolving field of DNA-based data storage by presenting a formal framework for modeling DNA labeling in strings, specifically tailored for data storage purposes.","Our approach involves a known DNA molecule as a template for labeling, employing patterns induced by a set of designed labels to represent information.","One hypothetical implementation can use CRISPR-Cas9 and gRNA reagents for labeling.","Various aspects of the general labeling channel, including fixed-length labels, are explored, and upper bounds on the maximal size of the corresponding codes are given.","The study includes the development of an efficient encoder-decoder pair that is proven optimal in terms of maximum code size under specific conditions."],"url":"http://arxiv.org/abs/2405.08475v1","category":"cs.IT"}
{"created":"2024-05-14 09:55:03","title":"Improving the Real-Data Driven Network Evaluation Model for Digital Twin Networks","abstract":"With the emergence and proliferation of new forms of large-scale services such as smart homes, virtual reality/augmented reality, the increasingly complex networks are raising concerns about significant operational costs. As a result, the need for network management automation is emphasized, and Digital Twin Networks (DTN) technology is expected to become the foundation technology for autonomous networks. DTN has the advantage of being able to operate and system networks based on real-time collected data in a closed-loop system, and currently it is mainly designed for optimization scenarios. To improve network performance in optimization scenarios, it is necessary to select appropriate configurations and perform accurate performance evaluation based on real data. However, most network evaluation models currently use simulation data. Meanwhile, according to DTN standards documents, artificial intelligence (AI) models can ensure scalability, real-time performance, and accuracy in large-scale networks. Various AI research and standardization work is ongoing to optimize the use of DTN. When designing AI models, it is crucial to consider the characteristics of the data. This paper presents an autoencoder-based skip connected message passing neural network (AE-SMPN) as a network evaluation model using real network data. The model is created by utilizing graph neural network (GNN) with recurrent neural network (RNN) models to capture the spatiotemporal features of network data. Additionally, an AutoEncoder (AE) is employed to extract initial features. The neural network was trained using the real DTN dataset provided by the Barcelona Neural Networking Center (BNN-UPC), and the paper presents the analysis of the model structure along with experimental results.","sentences":["With the emergence and proliferation of new forms of large-scale services such as smart homes, virtual reality/augmented reality, the increasingly complex networks are raising concerns about significant operational costs.","As a result, the need for network management automation is emphasized, and Digital Twin Networks (DTN) technology is expected to become the foundation technology for autonomous networks.","DTN has the advantage of being able to operate and system networks based on real-time collected data in a closed-loop system, and currently it is mainly designed for optimization scenarios.","To improve network performance in optimization scenarios, it is necessary to select appropriate configurations and perform accurate performance evaluation based on real data.","However, most network evaluation models currently use simulation data.","Meanwhile, according to DTN standards documents, artificial intelligence (AI) models can ensure scalability, real-time performance, and accuracy in large-scale networks.","Various AI research and standardization work is ongoing to optimize the use of DTN.","When designing AI models, it is crucial to consider the characteristics of the data.","This paper presents an autoencoder-based skip connected message passing neural network (AE-SMPN) as a network evaluation model using real network data.","The model is created by utilizing graph neural network (GNN) with recurrent neural network (RNN) models to capture the spatiotemporal features of network data.","Additionally, an AutoEncoder (AE) is employed to extract initial features.","The neural network was trained using the real DTN dataset provided by the Barcelona Neural Networking Center (BNN-UPC), and the paper presents the analysis of the model structure along with experimental results."],"url":"http://arxiv.org/abs/2405.08473v1","category":"cs.LG"}
{"created":"2024-05-14 09:51:09","title":"GPT-3.5 for Grammatical Error Correction","abstract":"This paper investigates the application of GPT-3.5 for Grammatical Error Correction (GEC) in multiple languages in several settings: zero-shot GEC, fine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses generated by other GEC models. In the zero-shot setting, we conduct automatic evaluations of the corrections proposed by GPT-3.5 using several methods: estimating grammaticality with language models (LMs), the Scribendi test, and comparing the semantic embeddings of sentences. GPT-3.5 has a known tendency to over-correct erroneous sentences and propose alternative corrections. For several languages, such as Czech, German, Russian, Spanish, and Ukrainian, GPT-3.5 substantially alters the source sentences, including their semantics, which presents significant challenges for evaluation with reference-based metrics. For English, GPT-3.5 demonstrates high recall, generates fluent corrections, and generally preserves sentence semantics. However, human evaluation for both English and Russian reveals that, despite its strong error-detection capabilities, GPT-3.5 struggles with several error types, including punctuation mistakes, tense errors, syntactic dependencies between words, and lexical compatibility at the sentence level.","sentences":["This paper investigates the application of GPT-3.5 for Grammatical Error Correction (GEC) in multiple languages in several settings: zero-shot GEC, fine-tuning for GEC, and using GPT-3.5 to re-rank correction hypotheses generated by other GEC models.","In the zero-shot setting, we conduct automatic evaluations of the corrections proposed by GPT-3.5 using several methods: estimating grammaticality with language models (LMs), the Scribendi test, and comparing the semantic embeddings of sentences.","GPT-3.5 has a known tendency to over-correct erroneous sentences and propose alternative corrections.","For several languages, such as Czech, German, Russian, Spanish, and Ukrainian, GPT-3.5 substantially alters the source sentences, including their semantics, which presents significant challenges for evaluation with reference-based metrics.","For English, GPT-3.5 demonstrates high recall, generates fluent corrections, and generally preserves sentence semantics.","However, human evaluation for both English and Russian reveals that, despite its strong error-detection capabilities, GPT-3.5 struggles with several error types, including punctuation mistakes, tense errors, syntactic dependencies between words, and lexical compatibility at the sentence level."],"url":"http://arxiv.org/abs/2405.08469v1","category":"cs.CL"}
{"created":"2024-05-14 09:44:52","title":"Challenges and Opportunities in Text Generation Explainability","abstract":"The necessity for interpretability in natural language processing (NLP) has risen alongside the growing prominence of large language models. Among the myriad tasks within NLP, text generation stands out as a primary objective of autoregressive models. The NLP community has begun to take a keen interest in gaining a deeper understanding of text generation, leading to the development of model-agnostic explainable artificial intelligence (xAI) methods tailored to this task. The design and evaluation of explainability methods are non-trivial since they depend on many factors involved in the text generation process, e.g., the autoregressive model and its stochastic nature. This paper outlines 17 challenges categorized into three groups that arise during the development and assessment of attribution-based explainability methods. These challenges encompass issues concerning tokenization, defining explanation similarity, determining token importance and prediction change metrics, the level of human intervention required, and the creation of suitable test datasets. The paper illustrates how these challenges can be intertwined, showcasing new opportunities for the community. These include developing probabilistic word-level explainability methods and engaging humans in the explainability pipeline, from the data design to the final evaluation, to draw robust conclusions on xAI methods.","sentences":["The necessity for interpretability in natural language processing (NLP) has risen alongside the growing prominence of large language models.","Among the myriad tasks within NLP, text generation stands out as a primary objective of autoregressive models.","The NLP community has begun to take a keen interest in gaining a deeper understanding of text generation, leading to the development of model-agnostic explainable artificial intelligence (xAI) methods tailored to this task.","The design and evaluation of explainability methods are non-trivial since they depend on many factors involved in the text generation process, e.g., the autoregressive model and its stochastic nature.","This paper outlines 17 challenges categorized into three groups that arise during the development and assessment of attribution-based explainability methods.","These challenges encompass issues concerning tokenization, defining explanation similarity, determining token importance and prediction change metrics, the level of human intervention required, and the creation of suitable test datasets.","The paper illustrates how these challenges can be intertwined, showcasing new opportunities for the community.","These include developing probabilistic word-level explainability methods and engaging humans in the explainability pipeline, from the data design to the final evaluation, to draw robust conclusions on xAI methods."],"url":"http://arxiv.org/abs/2405.08468v1","category":"cs.CL"}
{"created":"2024-05-14 09:43:32","title":"Equilibrium Propagation: the Quantum and the Thermal Cases","abstract":"Equilibrium propagation is a recently introduced method to use and train artificial neural networks in which the network is at the minimum (more generally extremum) of an energy functional. Equilibrium propagation has shown good performance on a number of benchmark tasks. Here we extend equilibrium propagation in two directions. First we show that there is a natural quantum generalization of equilibrium propagation in which a quantum neural network is taken to be in the ground state (more generally any eigenstate) of the network Hamiltonian, with a similar training mechanism that exploits the fact that the mean energy is extremal on eigenstates. Second we extend the analysis of equilibrium propagation at finite temperature, showing that thermal fluctuations allow one to naturally train the network without having to clamp the output layer during training. We also study the low temperature limit of equilibrium propagation.","sentences":["Equilibrium propagation is a recently introduced method to use and train artificial neural networks in which the network is at the minimum (more generally extremum) of an energy functional.","Equilibrium propagation has shown good performance on a number of benchmark tasks.","Here we extend equilibrium propagation in two directions.","First we show that there is a natural quantum generalization of equilibrium propagation in which a quantum neural network is taken to be in the ground state (more generally any eigenstate) of the network Hamiltonian, with a similar training mechanism that exploits the fact that the mean energy is extremal on eigenstates.","Second we extend the analysis of equilibrium propagation at finite temperature, showing that thermal fluctuations allow one to naturally train the network without having to clamp the output layer during training.","We also study the low temperature limit of equilibrium propagation."],"url":"http://arxiv.org/abs/2405.08467v1","category":"quant-ph"}
{"created":"2024-05-14 09:42:21","title":"Work-in-Progress: Crash Course: Can (Under Attack) Autonomous Driving Beat Human Drivers?","abstract":"Autonomous driving is a research direction that has gained enormous traction in the last few years thanks to advancements in Artificial Intelligence (AI). Depending on the level of independence from the human driver, several studies show that Autonomous Vehicles (AVs) can reduce the number of on-road crashes and decrease overall fuel emissions by improving efficiency. However, security research on this topic is mixed and presents some gaps. On one hand, these studies often neglect the intrinsic vulnerabilities of AI algorithms, which are known to compromise the security of these systems. On the other, the most prevalent attacks towards AI rely on unrealistic assumptions, such as access to the model parameters or the training dataset. As such, it is unclear if autonomous driving can still claim several advantages over human driving in real-world applications. This paper evaluates the inherent risks in autonomous driving by examining the current landscape of AVs and establishing a pragmatic threat model. Through our analysis, we develop specific claims highlighting the delicate balance between the advantages of AVs and potential security challenges in real-world scenarios. Our evaluation serves as a foundation for providing essential takeaway messages, guiding both researchers and practitioners at various stages of the automation pipeline. In doing so, we contribute valuable insights to advance the discourse on the security and viability of autonomous driving in real-world applications.","sentences":["Autonomous driving is a research direction that has gained enormous traction in the last few years thanks to advancements in Artificial Intelligence (AI).","Depending on the level of independence from the human driver, several studies show that Autonomous Vehicles (AVs) can reduce the number of on-road crashes and decrease overall fuel emissions by improving efficiency.","However, security research on this topic is mixed and presents some gaps.","On one hand, these studies often neglect the intrinsic vulnerabilities of AI algorithms, which are known to compromise the security of these systems.","On the other, the most prevalent attacks towards AI rely on unrealistic assumptions, such as access to the model parameters or the training dataset.","As such, it is unclear if autonomous driving can still claim several advantages over human driving in real-world applications.","This paper evaluates the inherent risks in autonomous driving by examining the current landscape of AVs and establishing a pragmatic threat model.","Through our analysis, we develop specific claims highlighting the delicate balance between the advantages of AVs and potential security challenges in real-world scenarios.","Our evaluation serves as a foundation for providing essential takeaway messages, guiding both researchers and practitioners at various stages of the automation pipeline.","In doing so, we contribute valuable insights to advance the discourse on the security and viability of autonomous driving in real-world applications."],"url":"http://arxiv.org/abs/2405.08466v1","category":"cs.CR"}
{"created":"2024-05-14 09:38:44","title":"How to Surprisingly Consider Recommendations? A Knowledge-Graph-based Approach Relying on Complex Network Metrics","abstract":"Traditional recommendation proposals, including content-based and collaborative filtering, usually focus on similarity between items or users. Existing approaches lack ways of introducing unexpectedness into recommendations, prioritizing globally popular items over exposing users to unforeseen items. This investigation aims to design and evaluate a novel layer on top of recommender systems suited to incorporate relational information and suggest items with a user-defined degree of surprise. We propose a Knowledge Graph (KG) based recommender system by encoding user interactions on item catalogs. Our study explores whether network-level metrics on KGs can influence the degree of surprise in recommendations. We hypothesize that surprisingness correlates with certain network metrics, treating user profiles as subgraphs within a larger catalog KG. The achieved solution reranks recommendations based on their impact on structural graph metrics. Our research contributes to optimizing recommendations to reflect the metrics. We experimentally evaluate our approach on two datasets of LastFM listening histories and synthetic Netflix viewing profiles. We find that reranking items based on complex network metrics leads to a more unexpected and surprising composition of recommendation lists.","sentences":["Traditional recommendation proposals, including content-based and collaborative filtering, usually focus on similarity between items or users.","Existing approaches lack ways of introducing unexpectedness into recommendations, prioritizing globally popular items over exposing users to unforeseen items.","This investigation aims to design and evaluate a novel layer on top of recommender systems suited to incorporate relational information and suggest items with a user-defined degree of surprise.","We propose a Knowledge Graph (KG) based recommender system by encoding user interactions on item catalogs.","Our study explores whether network-level metrics on KGs can influence the degree of surprise in recommendations.","We hypothesize that surprisingness correlates with certain network metrics, treating user profiles as subgraphs within a larger catalog KG.","The achieved solution reranks recommendations based on their impact on structural graph metrics.","Our research contributes to optimizing recommendations to reflect the metrics.","We experimentally evaluate our approach on two datasets of LastFM listening histories and synthetic Netflix viewing profiles.","We find that reranking items based on complex network metrics leads to a more unexpected and surprising composition of recommendation lists."],"url":"http://arxiv.org/abs/2405.08465v1","category":"cs.IR"}
{"created":"2024-05-14 09:35:47","title":"Goodness-of-fit and utility estimation: what's possible and what's not","abstract":"A goodness-of-fit index measures the consistency of consumption data with a given model of utility-maximization. We show that for the class of well-behaved (i.e., continuous and increasing) utility functions there is no goodness-of-fit index that is continuous and accurate, where the latter means that a perfect score is obtained if and only if a dataset can be rationalized by a well-behaved utility function. While many standard goodness-of-fit indices are inaccurate we show that these indices are (in a sense we make precise) essentially accurate. Goodness-of-fit indices are typically generated by loss functions and we find that standard loss functions usually do not yield a best-fitting utility function when they are minimized. Nonetheless, welfare comparisons can be made by working out a robust preference relation from the data.","sentences":["A goodness-of-fit index measures the consistency of consumption data with a given model of utility-maximization.","We show that for the class of well-behaved (i.e., continuous and increasing) utility functions there is no goodness-of-fit index that is continuous and accurate, where the latter means that a perfect score is obtained if and only if a dataset can be rationalized by a well-behaved utility function.","While many standard goodness-of-fit indices are inaccurate we show that these indices are (in a sense we make precise) essentially accurate.","Goodness-of-fit indices are typically generated by loss functions and we find that standard loss functions usually do not yield a best-fitting utility function when they are minimized.","Nonetheless, welfare comparisons can be made by working out a robust preference relation from the data."],"url":"http://arxiv.org/abs/2405.08464v1","category":"econ.TH"}
{"created":"2024-05-14 09:33:04","title":"A Timely Survey on Vision Transformer for Deepfake Detection","abstract":"In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality. However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety. To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency. This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures. Furthermore, it succinctly delineates the structure and characteristics of each model. By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain.","sentences":["In recent years, the rapid advancement of deepfake technology has revolutionized content creation, lowering forgery costs while elevating quality.","However, this progress brings forth pressing concerns such as infringements on individual rights, national security threats, and risks to public safety.","To counter these challenges, various detection methodologies have emerged, with Vision Transformer (ViT)-based approaches showcasing superior performance in generality and efficiency.","This survey presents a timely overview of ViT-based deepfake detection models, categorized into standalone, sequential, and parallel architectures.","Furthermore, it succinctly delineates the structure and characteristics of each model.","By analyzing existing research and addressing future directions, this survey aims to equip researchers with a nuanced understanding of ViT's pivotal role in deepfake detection, serving as a valuable reference for both academic and practical pursuits in this domain."],"url":"http://arxiv.org/abs/2405.08463v1","category":"cs.CV"}
{"created":"2024-05-14 09:31:31","title":"Evaluating LLMs at Evaluating Temporal Generalization","abstract":"The rapid advancement of Large Language Models (LLMs) highlights the urgent need for evolving evaluation methodologies that keep pace with improvements in language comprehension and information processing. However, traditional benchmarks, which are often static, fail to capture the continually changing information landscape, leading to a disparity between the perceived and actual effectiveness of LLMs in ever-changing real-world scenarios. Furthermore, these benchmarks do not adequately measure the models' capabilities over a broader temporal range or their adaptability over time. We examine current LLMs in terms of temporal generalization and bias, revealing that various temporal biases emerge in both language likelihood and prognostic prediction. This serves as a caution for LLM practitioners to pay closer attention to mitigating temporal biases. Also, we propose an evaluation framework Freshbench for dynamically generating benchmarks from the most recent real-world prognostication prediction. Our code is available at https://github.com/FreedomIntelligence/FreshBench. The dataset will be released soon.","sentences":["The rapid advancement of Large Language Models (LLMs) highlights the urgent need for evolving evaluation methodologies that keep pace with improvements in language comprehension and information processing.","However, traditional benchmarks, which are often static, fail to capture the continually changing information landscape, leading to a disparity between the perceived and actual effectiveness of LLMs in ever-changing real-world scenarios.","Furthermore, these benchmarks do not adequately measure the models' capabilities over a broader temporal range or their adaptability over time.","We examine current LLMs in terms of temporal generalization and bias, revealing that various temporal biases emerge in both language likelihood and prognostic prediction.","This serves as a caution for LLM practitioners to pay closer attention to mitigating temporal biases.","Also, we propose an evaluation framework Freshbench for dynamically generating benchmarks from the most recent real-world prognostication prediction.","Our code is available at https://github.com/FreedomIntelligence/FreshBench.","The dataset will be released soon."],"url":"http://arxiv.org/abs/2405.08460v1","category":"cs.CL"}
{"created":"2024-05-14 09:29:54","title":"Revealed preference and revealed preference cycles: a survey","abstract":"Afriat's Theorem (1967) states that a dataset can be thought of as being generated by a consumer maximizing a continuous and increasing utility function if and only if it is free of revealed preference cycles containing a strict relation. The latter property is often known by its acronym, GARP (for generalized axiom of revealed preference). This paper surveys extensions and applications of Afriat's seminal result. We focus on those results where the consistency of a dataset with the maximization of a utility function satisfying some property can be characterized by a suitably modified version of GARP.","sentences":["Afriat's Theorem (1967) states that a dataset can be thought of as being generated by a consumer maximizing a continuous and increasing utility function if and only if it is free of revealed preference cycles containing a strict relation.","The latter property is often known by its acronym, GARP (for generalized axiom of revealed preference).","This paper surveys extensions and applications of Afriat's seminal result.","We focus on those results where the consistency of a dataset with the maximization of a utility function satisfying some property can be characterized by a suitably modified version of GARP."],"url":"http://arxiv.org/abs/2405.08459v1","category":"econ.TH"}
{"created":"2024-05-14 09:28:25","title":"Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation","abstract":"Few-shot segmentation remains challenging due to the limitations of its labeling information for unseen classes. Most previous approaches rely on extracting high-level feature maps from the frozen visual encoder to compute the pixel-wise similarity as a key prior guidance for the decoder. However, such a prior representation suffers from coarse granularity and poor generalization to new classes since these high-level feature maps have obvious category bias. In this work, we propose to replace the visual prior representation with the visual-text alignment capacity to capture more reliable guidance and enhance the model generalization. Specifically, we design two kinds of training-free prior information generation strategy that attempts to utilize the semantic alignment capability of the Contrastive Language-Image Pre-training model (CLIP) to locate the target class. Besides, to acquire more accurate prior guidance, we build a high-order relationship of attention maps and utilize it to refine the initial prior information. Experiments on both the PASCAL-5{i} and COCO-20{i} datasets show that our method obtains a clearly substantial improvement and reaches the new state-of-the-art performance.","sentences":["Few-shot segmentation remains challenging due to the limitations of its labeling information for unseen classes.","Most previous approaches rely on extracting high-level feature maps from the frozen visual encoder to compute the pixel-wise similarity as a key prior guidance for the decoder.","However, such a prior representation suffers from coarse granularity and poor generalization to new classes since these high-level feature maps have obvious category bias.","In this work, we propose to replace the visual prior representation with the visual-text alignment capacity to capture more reliable guidance and enhance the model generalization.","Specifically, we design two kinds of training-free prior information generation strategy that attempts to utilize the semantic alignment capability of the Contrastive Language-Image Pre-training model (CLIP) to locate the target class.","Besides, to acquire more accurate prior guidance, we build a high-order relationship of attention maps and utilize it to refine the initial prior information.","Experiments on both the PASCAL-5{i} and COCO-20{i} datasets show that our method obtains a clearly substantial improvement and reaches the new state-of-the-art performance."],"url":"http://arxiv.org/abs/2405.08458v1","category":"cs.CV"}
{"created":"2024-05-14 09:26:36","title":"A superresolution-based quantum spectrometer using a pair of phase-controlled spatial light modulators satisfying the Heisenberg limit","abstract":"Recently, a phase-controlled superresolution has been proposed and experimentally demonstrated using the classical light of a continuous-wave laser to overcome the shot-noise limit in classical physics as well as to solve the limited N in N00N-based quantum sensing. Here, the superresolution is applied to a quantum spectrometer using phase-controlled spatial light modulators (SLMs) in a Mach-Zehnder interferometer. For the validity of the proposed method, a general solution is analytically derived from the SLM-based projection measurements, and numerical calculations are conducted for the N-dependent phase sensitivity and resolution. Besides, a classical spectrometer is numerically compared, whose resolution is the same as the superresolution. However, the phase sensitivity of the SLM-based quantum spectrometer shows a quantum advantage due to the reduced scan range, whereas the classical one has no change.","sentences":["Recently, a phase-controlled superresolution has been proposed and experimentally demonstrated using the classical light of a continuous-wave laser to overcome the shot-noise limit in classical physics as well as to solve the limited N in N00N-based quantum sensing.","Here, the superresolution is applied to a quantum spectrometer using phase-controlled spatial light modulators (SLMs) in a Mach-Zehnder interferometer.","For the validity of the proposed method, a general solution is analytically derived from the SLM-based projection measurements, and numerical calculations are conducted for the N-dependent phase sensitivity and resolution.","Besides, a classical spectrometer is numerically compared, whose resolution is the same as the superresolution.","However, the phase sensitivity of the SLM-based quantum spectrometer shows a quantum advantage due to the reduced scan range, whereas the classical one has no change."],"url":"http://arxiv.org/abs/2405.08456v1","category":"quant-ph"}
{"created":"2024-05-14 09:19:57","title":"Outside and inside a magnetic island: different perspectives to describe the same observables","abstract":"We compare three different approaches to describe a magnetic island in a generic toroidal plasma: (i) perturbative, from the perspective of the equilibrium magnetic field and the related action in a variational principle formulation, (ii) again perturbative, based on the integrability of a system with a single resonant mode and the application of a canonical transformation onto a new island equilibrium system, and (iii) non-perturbative, making use of a full geometric description of the island considered as a stand-alone plasma domain. For the three approaches, we characterize some observables and discuss the respective limits.","sentences":["We compare three different approaches to describe a magnetic island in a generic toroidal plasma: (i) perturbative, from the perspective of the equilibrium magnetic field and the related action in a variational principle formulation, (ii) again perturbative, based on the integrability of a system with a single resonant mode and the application of a canonical transformation onto a new island equilibrium system, and (iii) non-perturbative, making use of a full geometric description of the island considered as a stand-alone plasma domain.","For the three approaches, we characterize some observables and discuss the respective limits."],"url":"http://arxiv.org/abs/2405.08453v1","category":"physics.plasm-ph"}
{"created":"2024-05-14 09:19:46","title":"Far-from-equilibrium complex landscapes","abstract":"Systems with a complex dynamics like glasses or models of biological evolution are often pictured in terms of complex landscapes, with a large number of possible collective states. We show on the example of a stochastic spin model with non-reciprocal and heterogeneous interactions how the complex landscape notion can be generalized far from equilibrium, where collective states may exhibit spontaneous oscillations, often hidden by the presence of disorder. We identify relevant observables, like the density of entropy production, to unveil the presence of oscillations, and we characterize the complex landscape of our model in terms of a configurational entropy, that counts the number of nonequilibrium collective states with a given entropy production density.","sentences":["Systems with a complex dynamics like glasses or models of biological evolution are often pictured in terms of complex landscapes, with a large number of possible collective states.","We show on the example of a stochastic spin model with non-reciprocal and heterogeneous interactions how the complex landscape notion can be generalized far from equilibrium, where collective states may exhibit spontaneous oscillations, often hidden by the presence of disorder.","We identify relevant observables, like the density of entropy production, to unveil the presence of oscillations, and we characterize the complex landscape of our model in terms of a configurational entropy, that counts the number of nonequilibrium collective states with a given entropy production density."],"url":"http://arxiv.org/abs/2405.08452v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-14 09:19:28","title":"Planet formation regulated by galactic-scale interstellar turbulence","abstract":"Planets form from protoplanetary discs of dust and gas that surround stars younger than a few million years. The properties of these discs dictate how planets grow, determining the nature, architecture and composition of the observed exoplanet population. The nature of (turbulent) angular momentum transport through the disc in particular is critical for planet growth and migration. While the dominant physical processes in disc evolution remain unknown, theoretical studies widely assume an isolated star-disc system. Here we challenge this conventional assumption by showing that, far from being isolated systems, discs in typical star forming environments are constantly replenished by capturing new material from the interstellar medium (ISM). We show that this in-fall alone explains observed disc masses, outer radii and stellar accretion rates as a function of time and stellar mass. ISM capture is also capable of driving disc turbulence corresponding to viscosity in the range $\\alpha_\\mathrm{SS} \\sim 10^{-5}{-}10^{-1}$, as observationally inferred. We find that $20{-}70$~percent of discs are composed of material captured in the most recent half of their life-time, implying their properties are not direct probes of internal disc physics. Our results suggest that planet formation is driven by the turbulent cascade of energy from galactic-scales down to the protoplanetary disc and that recent evidence of in-fall from the ISM onto mature protoplanetary discs are part of an important, general process. This represents a far-reaching shift in our understanding of every stage of planet formation, from dust aggregation to the migration of giant planets through the disc.","sentences":["Planets form from protoplanetary discs of dust and gas that surround stars younger than a few million years.","The properties of these discs dictate how planets grow, determining the nature, architecture and composition of the observed exoplanet population.","The nature of (turbulent) angular momentum transport through the disc in particular is critical for planet growth and migration.","While the dominant physical processes in disc evolution remain unknown, theoretical studies widely assume an isolated star-disc system.","Here we challenge this conventional assumption by showing that, far from being isolated systems, discs in typical star forming environments are constantly replenished by capturing new material from the interstellar medium (ISM).","We show that this in-fall alone explains observed disc masses, outer radii and stellar accretion rates as a function of time and stellar mass.","ISM capture is also capable of driving disc turbulence corresponding to viscosity in the range $\\alpha_\\mathrm{SS} \\sim 10^{-5}{-}10^{-1}$, as observationally inferred.","We find that $20{-}70$~percent of discs are composed of material captured in the most recent half of their life-time, implying their properties are not direct probes of internal disc physics.","Our results suggest that planet formation is driven by the turbulent cascade of energy from galactic-scales down to the protoplanetary disc and that recent evidence of in-fall from the ISM onto mature protoplanetary discs are part of an important, general process.","This represents a far-reaching shift in our understanding of every stage of planet formation, from dust aggregation to the migration of giant planets through the disc."],"url":"http://arxiv.org/abs/2405.08451v1","category":"astro-ph.EP"}
{"created":"2024-05-14 09:13:48","title":"Effective Front-Descent Algorithms with Convergence Guarantees","abstract":"In this manuscript, we address continuous unconstrained optimization problems and we discuss descent type methods for the reconstruction of the Pareto set. Specifically, we analyze the class of Front Descent methods, which generalizes the Front Steepest Descent algorithm allowing the employment of suitable, effective search directions (e.g., Newton, Quasi-Newton, Barzilai-Borwein). We provide a deep characterization of the behavior and the mechanisms of the algorithmic framework, and we prove that, under reasonable assumptions, standard convergence results and some complexity bounds hold for the generalized approach. Moreover, we prove that popular search directions can indeed be soundly used within the framework. Then, we provide a completely novel type of convergence results, concerning the sequence of sets produced by the procedure. In particular, iterate sets are shown to asymptotically approach stationarity for all of their points; additionally, in finite precision settings, the sets are shown to only be enriched through exploration steps in later iterations, and suitable stopping conditions can be devised. Finally, the results from a large experimental benchmark show that the proposed class of approaches far outperforms state-of-the-art methodologies.","sentences":["In this manuscript, we address continuous unconstrained optimization problems and we discuss descent type methods for the reconstruction of the Pareto set.","Specifically, we analyze the class of Front Descent methods, which generalizes the Front Steepest Descent algorithm allowing the employment of suitable, effective search directions (e.g., Newton, Quasi-Newton, Barzilai-Borwein).","We provide a deep characterization of the behavior and the mechanisms of the algorithmic framework, and we prove that, under reasonable assumptions, standard convergence results and some complexity bounds hold for the generalized approach.","Moreover, we prove that popular search directions can indeed be soundly used within the framework.","Then, we provide a completely novel type of convergence results, concerning the sequence of sets produced by the procedure.","In particular, iterate sets are shown to asymptotically approach stationarity for all of their points; additionally, in finite precision settings, the sets are shown to only be enriched through exploration steps in later iterations, and suitable stopping conditions can be devised.","Finally, the results from a large experimental benchmark show that the proposed class of approaches far outperforms state-of-the-art methodologies."],"url":"http://arxiv.org/abs/2405.08450v1","category":"math.OC"}
{"created":"2024-05-14 09:12:30","title":"AI-Resilient Interfaces","abstract":"AI is powerful, but it can make choices that result in objective errors, contextually inappropriate outputs, and disliked options. We need AI-resilient interfaces that help people be resilient to the AI choices that are not right, or not right for them. To support this goal, interfaces need to help users notice and have the context to appropriately judge those AI choices. Existing human-AI interaction guidelines recommend efficient user dismissal, modification, or otherwise efficient recovery from AI choices that a user does not like. However, in order to recover from AI choices, the user must notice them first. This can be difficult. For example, when generating summaries of long documents, a system's exclusion of a detail that is critically important to the user is hard for the user to notice. That detail can be hiding in a wall of text in the original document, and the existence of a summary may tempt the user not to read the original document as carefully. Once noticed, judging AI choices well can also be challenging. The interface may provide very little information that contextualizes the choices, and the user may fall back on assumptions when deciding whether to dismiss, modify, or otherwise recover from an AI choice. Building on prior work, this paper defines key aspects of AI-resilient interfaces, illustrated with examples. Designing interfaces for increased AI-resilience of users will improve AI safety, usability, and utility. This is especially critical where AI-powered systems are used for context- and preference-dominated open-ended AI-assisted tasks, like ideating, summarizing, searching, sensemaking, and the reading and writing of text or code.","sentences":["AI is powerful, but it can make choices that result in objective errors, contextually inappropriate outputs, and disliked options.","We need AI-resilient interfaces that help people be resilient to the AI choices that are not right, or not right for them.","To support this goal, interfaces need to help users notice and have the context to appropriately judge those AI choices.","Existing human-AI interaction guidelines recommend efficient user dismissal, modification, or otherwise efficient recovery from AI choices that a user does not like.","However, in order to recover from AI choices, the user must notice them first.","This can be difficult.","For example, when generating summaries of long documents, a system's exclusion of a detail that is critically important to the user is hard for the user to notice.","That detail can be hiding in a wall of text in the original document, and the existence of a summary may tempt the user not to read the original document as carefully.","Once noticed, judging AI choices well can also be challenging.","The interface may provide very little information that contextualizes the choices, and the user may fall back on assumptions when deciding whether to dismiss, modify, or otherwise recover from an AI choice.","Building on prior work, this paper defines key aspects of AI-resilient interfaces, illustrated with examples.","Designing interfaces for increased AI-resilience of users will improve AI safety, usability, and utility.","This is especially critical where AI-powered systems are used for context- and preference-dominated open-ended AI-assisted tasks, like ideating, summarizing, searching, sensemaking, and the reading and writing of text or code."],"url":"http://arxiv.org/abs/2405.08447v1","category":"cs.HC"}
{"created":"2024-05-14 09:12:30","title":"Understanding the performance gap between online and offline alignment algorithms","abstract":"Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment. However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF. Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods. This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations. We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference. We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification. This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process. Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks. Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms.","sentences":["Reinforcement learning from human feedback (RLHF) is the canonical framework for large language model alignment.","However, rising popularity in offline alignment algorithms challenge the need for on-policy sampling in RLHF.","Within the context of reward over-optimization, we start with an opening set of experiments that demonstrate the clear advantage of online methods over offline methods.","This prompts us to investigate the causes to the performance discrepancy through a series of carefully designed experimental ablations.","We show empirically that hypotheses such as offline data coverage and data quality by itself cannot convincingly explain the performance difference.","We also find that while offline algorithms train policy to become good at pairwise classification, it is worse at generations; in the meantime the policies trained by online algorithms are good at generations while worse at pairwise classification.","This hints at a unique interplay between discriminative and generative capabilities, which is greatly impacted by the sampling process.","Lastly, we observe that the performance discrepancy persists for both contrastive and non-contrastive loss functions, and appears not to be addressed by simply scaling up policy networks.","Taken together, our study sheds light on the pivotal role of on-policy sampling in AI alignment, and hints at certain fundamental challenges of offline alignment algorithms."],"url":"http://arxiv.org/abs/2405.08448v1","category":"cs.LG"}
{"created":"2024-05-14 09:03:26","title":"Multi-dimensional piecewise contractions are asymptotically periodic","abstract":"Piecewise contractions (PCs) are piecewise smooth maps that decrease distance between pair of points in the same domain of continuity. The dynamics of a variety of systems is described by PCs. During the last decade, a lot of effort has been devoted to proving that in parametrized families of one-dimensional PCs, the $\\omega$-limit set of a typical PC consists of finitely many periodic orbits while there exist atypical PCs with Cantor $\\omega$-limit sets. In this article, we extend these results to the multi-dimensional case. More precisely, we provide criteria to show that an arbitrary family $\\{f_{\\mu}\\}_{\\mu\\in U}$ of locally bi-Lipschitz piecewise contractions $f_\\mu:X\\to X$ defined on a compact metric space $X$ is asymptotically periodic for Lebesgue almost every parameter $\\mu$ running over an open subset $U$ of the $M$-dimensional Euclidean space $\\mathbb{R}^M$. As a corollary of our results, we prove that piecewise affine contractions of $\\mathbb{R}^d$ defined in generic polyhedral partitions are asymptotically periodic.","sentences":["Piecewise contractions (PCs) are piecewise smooth maps that decrease distance between pair of points in the same domain of continuity.","The dynamics of a variety of systems is described by PCs.","During the last decade, a lot of effort has been devoted to proving that in parametrized families of one-dimensional PCs, the $\\omega$-limit set of a typical PC consists of finitely many periodic orbits while there exist atypical PCs with Cantor $\\omega$-limit sets.","In this article, we extend these results to the multi-dimensional case.","More precisely, we provide criteria to show that an arbitrary family $\\{f_{\\mu}\\}_{\\mu\\in U}$ of locally bi-Lipschitz piecewise contractions $f_\\mu:X\\to X$ defined on a compact metric space $X$ is asymptotically periodic for Lebesgue almost every parameter $\\mu$ running over an open subset $U$ of the $M$-dimensional Euclidean space $\\mathbb{R}^M$. As a corollary of our results, we prove that piecewise affine contractions of $\\mathbb{R}^d$ defined in generic polyhedral partitions are asymptotically periodic."],"url":"http://arxiv.org/abs/2405.08444v1","category":"math.DS"}
{"created":"2024-05-14 09:03:00","title":"Safety Constrained Multi-Agent Reinforcement Learning for Active Voltage Control","abstract":"Active voltage control presents a promising avenue for relieving power congestion and enhancing voltage quality, taking advantage of the distributed controllable generators in the power network, such as roof-top photovoltaics. While Multi-Agent Reinforcement Learning (MARL) has emerged as a compelling approach to address this challenge, existing MARL approaches tend to overlook the constrained optimization nature of this problem, failing in guaranteeing safety constraints. In this paper, we formalize the active voltage control problem as a constrained Markov game and propose a safety-constrained MARL algorithm. We expand the primal-dual optimization RL method to multi-agent settings, and augment it with a novel approach of double safety estimation to learn the policy and to update the Lagrange-multiplier. In addition, we proposed different cost functions and investigated their influences on the behavior of our constrained MARL method. We evaluate our approach in the power distribution network simulation environment with real-world scale scenarios. Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art MARL methods.","sentences":["Active voltage control presents a promising avenue for relieving power congestion and enhancing voltage quality, taking advantage of the distributed controllable generators in the power network, such as roof-top photovoltaics.","While Multi-Agent Reinforcement Learning (MARL) has emerged as a compelling approach to address this challenge, existing MARL approaches tend to overlook the constrained optimization nature of this problem, failing in guaranteeing safety constraints.","In this paper, we formalize the active voltage control problem as a constrained Markov game and propose a safety-constrained MARL algorithm.","We expand the primal-dual optimization RL method to multi-agent settings, and augment it with a novel approach of double safety estimation to learn the policy and to update the Lagrange-multiplier.","In addition, we proposed different cost functions and investigated their influences on the behavior of our constrained MARL method.","We evaluate our approach in the power distribution network simulation environment with real-world scale scenarios.","Experimental results demonstrate the effectiveness of the proposed method compared with the state-of-the-art MARL methods."],"url":"http://arxiv.org/abs/2405.08443v1","category":"cs.LG"}
{"created":"2024-05-14 08:57:11","title":"General amplitude of near-threshold hadron scattering for exotic hadrons","abstract":"We discuss the general behavior of the scattering amplitude with channel couplings near the two-body threshold. It is known that the Flatt\\'{e} amplitude, which is often used in the analysis of experimental data involving exotic hadrons, has some constraint in the near-threshold energy region. While the M-matrix gives the general expression of the scattering amplitude, it is not smoothly connected to the Flatt\\'{e} amplitude, due to the property of the determinant of the amplitude in channel space. In this paper, based on the effective field theory, we propose new parametrization of the scattering amplitude which gives the general expression near the threshold and has a well-defined limit reproducing the Flatt\\'{e} amplitude. We show that the nonresonant background contribution exists in the general amplitude even in the first order in the momentum. Finally, we quantitatively evaluate the cross sections by changing the strength of the background contribution. We find that the interference with the background term may induce a dip structure of the cross section near the threshold, in addition to the peak and threshold cusp structures.","sentences":["We discuss the general behavior of the scattering amplitude with channel couplings near the two-body threshold.","It is known that the Flatt\\'{e} amplitude, which is often used in the analysis of experimental data involving exotic hadrons, has some constraint in the near-threshold energy region.","While the M-matrix gives the general expression of the scattering amplitude, it is not smoothly connected to the Flatt\\'{e} amplitude, due to the property of the determinant of the amplitude in channel space.","In this paper, based on the effective field theory, we propose new parametrization of the scattering amplitude which gives the general expression near the threshold and has a well-defined limit reproducing the Flatt\\'{e} amplitude.","We show that the nonresonant background contribution exists in the general amplitude even in the first order in the momentum.","Finally, we quantitatively evaluate the cross sections by changing the strength of the background contribution.","We find that the interference with the background term may induce a dip structure of the cross section near the threshold, in addition to the peak and threshold cusp structures."],"url":"http://arxiv.org/abs/2405.08436v1","category":"hep-ph"}
{"created":"2024-05-14 08:56:34","title":"Duality and Black Hole Evaporation","abstract":"We construct a model that describes the quantum black hole evaporation unitarily in a Hilbert space of infinite dimension. This construction generalizes Page's finite dimensional approach to infinite dimensions. The basic ingredient is the Murray-von Neumann coupling for finite type II factors. This coupling measures, at each time of the evaporation, the relative continuous dimension of the radiation and the black hole subspaces. The unitary transformation, implementing the quantum evaporation and thus determining the time dependence of the coupling, is identified with the dual modular automorphism. In the appendix we sketch, using von Neumann construction of infinite tensor products of EPR pairs of q-bits, some q-bit holographic correspondences as well as an algebraic definition of ER bridges.","sentences":["We construct a model that describes the quantum black hole evaporation unitarily in a Hilbert space of infinite dimension.","This construction generalizes Page's finite dimensional approach to infinite dimensions.","The basic ingredient is the Murray-von Neumann coupling for finite type II factors.","This coupling measures, at each time of the evaporation, the relative continuous dimension of the radiation and the black hole subspaces.","The unitary transformation, implementing the quantum evaporation and thus determining the time dependence of the coupling, is identified with the dual modular automorphism.","In the appendix we sketch, using von Neumann construction of infinite tensor products of EPR pairs of q-bits, some q-bit holographic correspondences as well as an algebraic definition of ER bridges."],"url":"http://arxiv.org/abs/2405.08435v1","category":"hep-th"}
{"created":"2024-05-14 08:52:09","title":"Hochster's type formulae","abstract":"We give an elementary proof and generalize some Hochsters's type formulae on local cohomology and Ext's of squarefree modules","sentences":["We give an elementary proof and generalize some Hochsters's type formulae on local cohomology and Ext's of squarefree modules"],"url":"http://arxiv.org/abs/2405.08432v1","category":"math.AC"}
{"created":"2024-05-14 08:51:16","title":"Similarity Metrics for MR Image-To-Image Translation","abstract":"Image-to-image translation can create large impact in medical imaging, i.e. if images of a patient can be translated to another modality, type or sequence for better diagnosis. However, these methods must be validated by human reader studies, which are costly and restricted to small samples. Automatic evaluation of large samples to pre-evaluate and continuously improve methods before human validation is needed. In this study, we give an overview of reference and non-reference metrics for image synthesis assessment and investigate the ability of nine metrics, that need a reference (SSIM, MS-SSIM, PSNR, MSE, NMSE, MAE, LPIPS, NMI and PCC) and three non-reference metrics (BLUR, MSN, MNG) to detect 11 kinds of distortions in MR images from the BraSyn dataset. In addition we test a downstream segmentation metric and the effect of three normalization methods (Minmax, cMinMax and Zscore). Although PSNR and SSIM are frequently used to evaluate generative models for image-to-image-translation tasks in the medical domain, they show very specific shortcomings. SSIM ignores blurring but is very sensitive to intensity shifts in unnormalized MR images. PSNR is even more sensitive to different normalization methods and hardly measures the degree of distortions. Further metrics, such as LPIPS, NMI and DICE can be very useful to evaluate other similarity aspects. If the images to be compared are misaligned, most metrics are flawed. By carefully selecting and reasonably combining image similarity metrics, the training and selection of generative models for MR image synthesis can be improved. Many aspects of their output can be validated before final and costly evaluation by trained radiologists is conducted.","sentences":["Image-to-image translation can create large impact in medical imaging, i.e. if images of a patient can be translated to another modality, type or sequence for better diagnosis.","However, these methods must be validated by human reader studies, which are costly and restricted to small samples.","Automatic evaluation of large samples to pre-evaluate and continuously improve methods before human validation is needed.","In this study, we give an overview of reference and non-reference metrics for image synthesis assessment and investigate the ability of nine metrics, that need a reference (SSIM, MS-SSIM, PSNR, MSE, NMSE, MAE, LPIPS, NMI and PCC) and three non-reference metrics (BLUR, MSN, MNG) to detect 11 kinds of distortions in MR images from the BraSyn dataset.","In addition we test a downstream segmentation metric and the effect of three normalization methods (Minmax, cMinMax and Zscore).","Although PSNR and SSIM are frequently used to evaluate generative models for image-to-image-translation tasks in the medical domain, they show very specific shortcomings.","SSIM ignores blurring but is very sensitive to intensity shifts in unnormalized MR images.","PSNR is even more sensitive to different normalization methods and hardly measures the degree of distortions.","Further metrics, such as LPIPS, NMI and DICE can be very useful to evaluate other similarity aspects.","If the images to be compared are misaligned, most metrics are flawed.","By carefully selecting and reasonably combining image similarity metrics, the training and selection of generative models for MR image synthesis can be improved.","Many aspects of their output can be validated before final and costly evaluation by trained radiologists is conducted."],"url":"http://arxiv.org/abs/2405.08431v1","category":"eess.IV"}
{"created":"2024-05-14 08:44:04","title":"A Low-Power Spike Detector Using In-Memory Computing for Event-based Neural Frontend","abstract":"With the sensor scaling of next-generation Brain-Machine Interface (BMI) systems, the massive A/D conversion and analog multiplexing at the neural frontend poses a challenge in terms of power and data rates for wireless and implantable BMIs. While previous works have reported the neuromorphic compression of neural signal, further compression requires integration of spike detectors on chip. In this work, we propose an efficient HRAM-based spike detector using In-memory computing for compressive event-based neural frontend. Our proposed method involves detecting spikes from event pulses without reconstructing the signal and uses a 10T hybrid in-memory computing bitcell for the accumulation and thresholding operations. We show that our method ensures a spike detection accuracy of 92-99% for neural signal inputs while consuming only 13.8 nW per channel in 65 nm CMOS.","sentences":["With the sensor scaling of next-generation Brain-Machine Interface (BMI) systems, the massive A/D conversion and analog multiplexing at the neural frontend poses a challenge in terms of power and data rates for wireless and implantable BMIs.","While previous works have reported the neuromorphic compression of neural signal, further compression requires integration of spike detectors on chip.","In this work, we propose an efficient HRAM-based spike detector using In-memory computing for compressive event-based neural frontend.","Our proposed method involves detecting spikes from event pulses without reconstructing the signal and uses a 10T hybrid in-memory computing bitcell for the accumulation and thresholding operations.","We show that our method ensures a spike detection accuracy of 92-99% for neural signal inputs while consuming only 13.8 nW per channel in 65 nm CMOS."],"url":"http://arxiv.org/abs/2405.08428v1","category":"eess.SP"}
{"created":"2024-05-14 08:42:49","title":"Impact of Stickers on Multimodal Chat Sentiment Analysis and Intent Recognition: A New Task, Dataset and Baseline","abstract":"Stickers are increasingly used in social media to express sentiment and intent. When finding typing troublesome, people often use a sticker instead. Despite the significant impact of stickers on sentiment analysis and intent recognition, little research has been conducted. To address this gap, we propose a new task: Multimodal chat Sentiment Analysis and Intent Recognition involving Stickers (MSAIRS). Additionally, we introduce a novel multimodal dataset containing Chinese chat records and stickers excerpted from several mainstream social media platforms. Our dataset includes paired data with the same text but different stickers, and various stickers consisting of the same images with different texts, allowing us to better understand the impact of stickers on chat sentiment and intent. We also propose an effective multimodal joint model, MMSAIR, for our task, which is validated on our datasets and indicates that visual information of stickers counts. Our dataset and code will be publicly available.","sentences":["Stickers are increasingly used in social media to express sentiment and intent.","When finding typing troublesome, people often use a sticker instead.","Despite the significant impact of stickers on sentiment analysis and intent recognition, little research has been conducted.","To address this gap, we propose a new task: Multimodal chat Sentiment Analysis and Intent Recognition involving Stickers (MSAIRS).","Additionally, we introduce a novel multimodal dataset containing Chinese chat records and stickers excerpted from several mainstream social media platforms.","Our dataset includes paired data with the same text but different stickers, and various stickers consisting of the same images with different texts, allowing us to better understand the impact of stickers on chat sentiment and intent.","We also propose an effective multimodal joint model, MMSAIR, for our task, which is validated on our datasets and indicates that visual information of stickers counts.","Our dataset and code will be publicly available."],"url":"http://arxiv.org/abs/2405.08427v1","category":"cs.CL"}
{"created":"2024-05-14 08:42:00","title":"On the universal and generalized orbifold Euler characteristics","abstract":"We discuss the universal orbifold Euler characteristic and generalized orbifold Euler characteristics corresponding to finitely generated groups $A$ (the $A$-Euler characteristics). We show that the collection of all $A$-Euler characteristics for $A$ of the form $A'\\times Z$ ($Z$ is the group of integers) with finite $A'$ determine the universal orbifold Euler characteristic.","sentences":["We discuss the universal orbifold Euler characteristic and generalized orbifold Euler characteristics corresponding to finitely generated groups $A$ (the $A$-Euler characteristics).","We show that the collection of all $A$-Euler characteristics for $A$ of the form $A'\\times Z$ ($Z$ is the group of integers) with finite $A'$ determine the universal orbifold Euler characteristic."],"url":"http://arxiv.org/abs/2405.08426v1","category":"math.AG"}
{"created":"2024-05-14 08:39:05","title":"Rogers-Ramanujan identities in Statistical Mechanics","abstract":"We describe the story of the Rogers-Ramanujan identities; being known for 85 years and having about 130 pure mathematics proofs, suddenly entering physics when Rodney Baxter solved the Hard Hexagon Model in Statistical Mechanics in 1980. We next cover the accompanying proofs by George E Andrews of other related Baxter identities arisen of Rogers-Ramanujan type, leading into a new flourishing partnership of Physics and Mathematics. Our narrative goes into the subsequent 44 years, explaining the progress in physics and mathematical analysis. Finally we show some related crossovers with regard to the Elliptic q-gamma function and some Vector Partition generating functional equations; the latter of which may be new. The present paper is essentially chapter 11 of a 32 chapter book to appear in June 2024.","sentences":["We describe the story of the Rogers-Ramanujan identities; being known for 85 years and having about 130 pure mathematics proofs, suddenly entering physics when Rodney Baxter solved the Hard Hexagon Model in Statistical Mechanics in 1980.","We next cover the accompanying proofs by George E Andrews of other related Baxter identities arisen of Rogers-Ramanujan type, leading into a new flourishing partnership of Physics and Mathematics.","Our narrative goes into the subsequent 44 years, explaining the progress in physics and mathematical analysis.","Finally we show some related crossovers with regard to the Elliptic q-gamma function and some Vector Partition generating functional equations; the latter of which may be new.","The present paper is essentially chapter 11 of a 32 chapter book to appear in June 2024."],"url":"http://arxiv.org/abs/2405.08425v1","category":"math-ph"}
{"created":"2024-05-14 08:23:30","title":"Simple and Efficient Quantization Techniques for Neural Speech Coding","abstract":"Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques. Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder has to be learned that allows for efficient transmission of the input audio signal. This discrete representation is typically generated by applying a quantizer to the output of the neural encoder. In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder. In this paper, we propose simple alternatives to VQ, which are based on projected Scalar Quantization (SQ). These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs. Furthermore, we propose a new causal network architecture for neural speech coding that shows good performance at very low computational complexity.","sentences":["Neural audio coding has emerged as a vivid research direction by promising good audio quality at very low bitrates unachievable by classical coding techniques.","Here, end-to-end trainable autoencoder-like models represent the state of the art, where a discrete representation in the bottleneck of the autoencoder has to be learned that allows for efficient transmission of the input audio signal.","This discrete representation is typically generated by applying a quantizer to the output of the neural encoder.","In almost all state-of-the-art neural audio coding approaches, this quantizer is realized as a Vector Quantizer (VQ) and a lot of effort has been spent to alleviate drawbacks of this quantization technique when used together with a neural audio coder.","In this paper, we propose simple alternatives to VQ, which are based on projected Scalar Quantization (SQ).","These quantization techniques do not need any additional losses, scheduling parameters or codebook storage thereby simplifying the training of neural audio codecs.","Furthermore, we propose a new causal network architecture for neural speech coding that shows good performance at very low computational complexity."],"url":"http://arxiv.org/abs/2405.08417v1","category":"eess.AS"}
{"created":"2024-05-14 08:18:57","title":"On the transcendentality condition for Gaussian Gabor frames","abstract":"We give a criterion for higher-dimensional Gaussian Gabor frames, which is a reformulation of one of the main results in a previous article by the first and last authors in more explicit terms. We also show that this density criterion for Gaussian Gabor frames is generic in a certain sense.","sentences":["We give a criterion for higher-dimensional Gaussian Gabor frames, which is a reformulation of one of the main results in a previous article by the first and last authors in more explicit terms.","We also show that this density criterion for Gaussian Gabor frames is generic in a certain sense."],"url":"http://arxiv.org/abs/2405.08415v1","category":"math.FA"}
{"created":"2024-05-14 08:12:48","title":"Bootstrapping Lagrangian Perturbation Theory for the Large Scale Structure","abstract":"We develop a model-independent approach to lagrangian perturbation theory for the large scale structure of the universe. We focus on the displacement field for dark matter particles, and derive its most general structure without assuming a specific form for the equations of motion, but implementing a set of general requirements based on symmetry principles and consistency with the perturbative approach. We present explicit results up to sixth order, and provide an algorithmic procedure for arbitrarily higher orders. The resulting displacement field is expressed as an expansion in operators built up from the linear density field, with time-dependent coefficients that can be obtained, in a specific model, by solving ordinary differential equations. The derived structure is general enough to cover a wide spectrum of models beyond $\\Lambda$CDM, including modified gravity scenarios of the Hordenski type and models with multiple dark matter species. This work is a first step towards a complete model-independent lagrangian forward model, to be employed in cosmological analyses with power spectrum and bispectrum, other summary statistics, and field-level inference.","sentences":["We develop a model-independent approach to lagrangian perturbation theory for the large scale structure of the universe.","We focus on the displacement field for dark matter particles, and derive its most general structure without assuming a specific form for the equations of motion, but implementing a set of general requirements based on symmetry principles and consistency with the perturbative approach.","We present explicit results up to sixth order, and provide an algorithmic procedure for arbitrarily higher orders.","The resulting displacement field is expressed as an expansion in operators built up from the linear density field, with time-dependent coefficients that can be obtained, in a specific model, by solving ordinary differential equations.","The derived structure is general enough to cover a wide spectrum of models beyond $\\Lambda$CDM, including modified gravity scenarios of the Hordenski type and models with multiple dark matter species.","This work is a first step towards a complete model-independent lagrangian forward model, to be employed in cosmological analyses with power spectrum and bispectrum, other summary statistics, and field-level inference."],"url":"http://arxiv.org/abs/2405.08413v1","category":"astro-ph.CO"}
{"created":"2024-05-14 08:02:12","title":"Bifurcation analysis of a two-neuron central pattern generator model for both oscillatory and convergent neuronal activities","abstract":"The neural oscillator model proposed by Matsuoka is a piecewise affine system, which exhibits distinctive periodic solutions. Although such typical oscillation patterns have been widely studied, little is understood about the dynamics of convergence to certain fixed points and bifurcations between the periodic orbits and fixed points in this model. We performed fixed point analysis on a two-neuron version of the Matsuoka oscillator model, the result of which explains the mechanism of oscillation and the discontinuity-induced bifurcations such as subcritical/supercritical Hopf-like, homoclinic-like, and grazing bifurcations. Furthermore, it provided theoretical predictions concerning a logarithmic oscillation-period scaling law and noise-induced oscillations, which are both observed around those bifurcations. These results are expected to underpin further investigations into both oscillatory and transient neuronal activities with respect to central pattern generators.","sentences":["The neural oscillator model proposed by Matsuoka is a piecewise affine system, which exhibits distinctive periodic solutions.","Although such typical oscillation patterns have been widely studied, little is understood about the dynamics of convergence to certain fixed points and bifurcations between the periodic orbits and fixed points in this model.","We performed fixed point analysis on a two-neuron version of the Matsuoka oscillator model, the result of which explains the mechanism of oscillation and the discontinuity-induced bifurcations such as subcritical/supercritical Hopf-like, homoclinic-like, and grazing bifurcations.","Furthermore, it provided theoretical predictions concerning a logarithmic oscillation-period scaling law and noise-induced oscillations, which are both observed around those bifurcations.","These results are expected to underpin further investigations into both oscillatory and transient neuronal activities with respect to central pattern generators."],"url":"http://arxiv.org/abs/2405.08409v1","category":"nlin.AO"}
{"created":"2024-05-14 07:58:57","title":"Global existence of small data weak solutions to the semilinear wave equations with time-dependent scale-invariant damping","abstract":"In this paper, we are concerned with the global existence of small data weak solutions to the $n-$dimensional semilinear wave equation $\\partial_t^2u-\\Delta u+\\frac{\\mu}{t}\\partial_tu=|u|^p$ with time-dependent scale-invariant damping, where $n\\geq 2$, $t\\geq 1$, $\\mu\\in(0,1)\\cup(1,2]$ and $p>1$. This equation can be changed into the semilinear generalized Tricomi equation $\\partial_t^2u-t^m\\Delta u=t^{\\alpha(m)}|u|^p$, where $m=m(\\mu)>0$ and $\\alpha(m)\\in\\Bbb R$ are two suitable constants. At first, for the more general semilinear Tricomi equation $\\partial_t^2v-t^m\\Delta v=t^{\\alpha}|v|^p$ with any fixed constant $m>0$ and arbitrary parameter $\\alpha\\in\\Bbb R$, we shall show that in the case of $\\alpha\\leq -2$, $n\\geq 3$ and $p>1$, the small data weak solution $v$ exists globally; in the case of $\\alpha>-2$, through determining the conformal exponent $p_{conf}(n,m,\\alpha)>1$, the global small data weak solution $v$ exists when some extra restrictions of $p\\geq p_{conf}(n,m,\\alpha)$ are given. Returning to the original equation $\\partial_t^2u-\\Delta u+\\frac{\\mu}{t}\\partial_tu=|u|^p$, the corresponding global existence results on the small data solution $u$ can be obtained.","sentences":["In this paper, we are concerned with the global existence of small data weak solutions to the $n-$dimensional semilinear wave equation $\\partial_t^2u-\\Delta u+\\frac{\\mu}{t}\\partial_tu=|u|^p$ with time-dependent scale-invariant damping, where $n\\geq 2$, $t\\geq 1$, $\\mu\\in(0,1)\\cup(1,2]$ and $p>1$. This equation can be changed into the semilinear generalized Tricomi equation $\\partial_t^2u-t^m\\Delta u=t^{\\alpha(m)}|u|^p$, where $m=m(\\mu)>0$ and $\\alpha(m)\\in\\Bbb R$ are two suitable constants.","At first, for the more general semilinear Tricomi equation $\\partial_t^2v-t^m\\Delta v=t^{\\alpha}|v|^p$ with any fixed constant $m>0$ and arbitrary parameter $\\alpha\\in\\Bbb R$, we shall show that in the case of $\\alpha\\leq -2$, $n\\geq 3$ and $p>1$, the small data weak solution $v$ exists globally; in the case of $\\alpha>-2$, through determining the conformal exponent $p_{conf}(n,m,\\alpha)>1$, the global small data weak solution $v$ exists when some extra restrictions of $p\\geq p_{conf}(n,m,\\alpha)$ are given.","Returning to the original equation $\\partial_t^2u-\\Delta u+\\frac{\\mu}{t}\\partial_tu=|u|^p$, the corresponding global existence results on the small data solution $u$ can be obtained."],"url":"http://arxiv.org/abs/2405.08407v1","category":"math.AP"}
{"created":"2024-05-14 07:55:34","title":"Realtime Global Optimization of a Fail-Safe Emergency Stop Maneuver for Arbitrary Electrical / Electronical Failures in Automated Driving","abstract":"In the event of a critical system failures in auto-mated vehicles, fail-operational or fail-safe measures provide minimum guarantees for the vehicle's performance, depending on which of its subsystems remain operational. Various such methods have been proposed which, upon failure, use different remaining sets of operational subsystems to execute maneuvers that bring the vehicle into a safe state under different environmental conditions. One particular such method proposes a fail-safe emergency stop system that requires no particular electric or electronic subsystem to be available after failure, and still provides a basic situation-dependent emergency stop maneuver. This is achieved by preemptively setting parameters to a hydraulic / mechanical system prior to failure, which after failure executes the preset maneuver \"blindly\". The focus of this paper is the particular challenge of implementing a lightweight planning algorithm that can cope with the complex uncertainties of the given task while still providing a globally optimal solution at regular intervals, based on the perceived and predicted environment of the automated vehicle.","sentences":["In the event of a critical system failures in auto-mated vehicles, fail-operational or fail-safe measures provide minimum guarantees for the vehicle's performance, depending on which of its subsystems remain operational.","Various such methods have been proposed which, upon failure, use different remaining sets of operational subsystems to execute maneuvers that bring the vehicle into a safe state under different environmental conditions.","One particular such method proposes a fail-safe emergency stop system that requires no particular electric or electronic subsystem to be available after failure, and still provides a basic situation-dependent emergency stop maneuver.","This is achieved by preemptively setting parameters to a hydraulic / mechanical system prior to failure, which after failure executes the preset maneuver \"blindly\".","The focus of this paper is the particular challenge of implementing a lightweight planning algorithm that can cope with the complex uncertainties of the given task while still providing a globally optimal solution at regular intervals, based on the perceived and predicted environment of the automated vehicle."],"url":"http://arxiv.org/abs/2405.08401v1","category":"cs.RO"}
{"created":"2024-05-14 07:54:54","title":"Stylometric Watermarks for Large Language Models","abstract":"The rapid advancement of large language models (LLMs) has made it increasingly difficult to distinguish between text written by humans and machines. Addressing this, we propose a novel method for generating watermarks that strategically alters token probabilities during generation. Unlike previous works, this method uniquely employs linguistic features such as stylometry. Concretely, we introduce acrostica and sensorimotor norms to LLMs. Further, these features are parameterized by a key, which is updated every sentence. To compute this key, we use semantic zero shot classification, which enhances resilience. In our evaluation, we find that for three or more sentences, our method achieves a false positive and false negative rate of 0.02. For the case of a cyclic translation attack, we observe similar results for seven or more sentences. This research is of particular of interest for proprietary LLMs to facilitate accountability and prevent societal harm.","sentences":["The rapid advancement of large language models (LLMs) has made it increasingly difficult to distinguish between text written by humans and machines.","Addressing this, we propose a novel method for generating watermarks that strategically alters token probabilities during generation.","Unlike previous works, this method uniquely employs linguistic features such as stylometry.","Concretely, we introduce acrostica and sensorimotor norms to LLMs.","Further, these features are parameterized by a key, which is updated every sentence.","To compute this key, we use semantic zero shot classification, which enhances resilience.","In our evaluation, we find that for three or more sentences, our method achieves a false positive and false negative rate of 0.02.","For the case of a cyclic translation attack, we observe similar results for seven or more sentences.","This research is of particular of interest for proprietary LLMs to facilitate accountability and prevent societal harm."],"url":"http://arxiv.org/abs/2405.08400v1","category":"cs.CL"}
{"created":"2024-05-14 07:40:57","title":"Cerebralization of mathematical quantities and physical features in neural science: a critical evaluation","abstract":"At the turn of the 20th century, Henri Poincar{\\'e} explained that geometry is a convention and that the properties of space and time are the properties of our measuring instruments. Intriguingly, numerous contemporary authors argue that space, time and even number are ''encoded'' within the brain, as a consequence of evolution, adaptation and natural selection. In the neuroscientific study of movement generation, the activity of neurons would ''encode'' kinematic parameters: when they emit action potentials, neurons would ''speak'' a language carrying notions of classical mechanics. In this article, we shall explain that the movement of a body segment is the ultimate product of a measurement, a filtered numerical outcome of multiple processes taking place in parallel in the central nervous system and converging on the groups of neurons responsible for muscle contractions. The fact that notions of classical mechanics efficiently describe movements does not imply their implementation in the inner workings of the brain. Their relevance to the question how the brain activity enables one to produce accurate movements is questioned within the framework of the neurophysiology of orienting gaze movements toward a visual target.","sentences":["At the turn of the 20th century, Henri Poincar{\\'e} explained that geometry is a convention and that the properties of space and time are the properties of our measuring instruments.","Intriguingly, numerous contemporary authors argue that space, time and even number are ''encoded'' within the brain, as a consequence of evolution, adaptation and natural selection.","In the neuroscientific study of movement generation, the activity of neurons would ''encode'' kinematic parameters: when they emit action potentials, neurons would ''speak'' a language carrying notions of classical mechanics.","In this article, we shall explain that the movement of a body segment is the ultimate product of a measurement, a filtered numerical outcome of multiple processes taking place in parallel in the central nervous system and converging on the groups of neurons responsible for muscle contractions.","The fact that notions of classical mechanics efficiently describe movements does not imply their implementation in the inner workings of the brain.","Their relevance to the question how the brain activity enables one to produce accurate movements is questioned within the framework of the neurophysiology of orienting gaze movements toward a visual target."],"url":"http://arxiv.org/abs/2405.08391v1","category":"q-bio.NC"}
{"created":"2024-05-14 07:33:35","title":"Group Dispersal Modelling revisited","abstract":"In this paper we revisit the notion of grouped dispersal that have been introduced by Soubeyrand and co-authors \\cite{soubeyrand2011patchy} to model the simultaneous (and hence dependent) dispersal of several propagules from a single source in a homogeneous environment. We built a time continuous measure valued process that takes into account the main feature of a grouped dispersal and derive its infinitesimal generator. To cope with the mutligeneration aspect associated to the demography we introduce two types of propagules in the description of the population which is one of the main innovations here. We also provide a rigorous description of the process and its generator. We derive as well, some large population asymptotics of the process unveilling the degenerate ultra parabolic system of PDE satisfied by the density of population. Finally, we also show that such a PDE system has a non-trivial solution which is unique in a certain functional space.","sentences":["In this paper we revisit the notion of grouped dispersal that have been introduced by Soubeyrand and co-authors \\cite{soubeyrand2011patchy} to model the simultaneous (and hence dependent) dispersal of several propagules from a single source in a homogeneous environment.","We built a time continuous measure valued process that takes into account the main feature of a grouped dispersal and derive its infinitesimal generator.","To cope with the mutligeneration aspect associated to the demography we introduce two types of propagules in the description of the population which is one of the main innovations here.","We also provide a rigorous description of the process and its generator.","We derive as well, some large population asymptotics of the process unveilling the degenerate ultra parabolic system of PDE satisfied by the density of population.","Finally, we also show that such a PDE system has a non-trivial solution which is unique in a certain functional space."],"url":"http://arxiv.org/abs/2405.08384v1","category":"math.AP"}
{"created":"2024-05-14 07:30:11","title":"Faithful Artin induction and the Chebotarev density theorem","abstract":"Given a finite group G, we prove that the vector space spanned by the faithful irreducible characters of G is generated by the monomial characters in the vector space. As a consequence, we show that in any family of G-extensions of a fixed number field F, almost all are subject to a strong effective version of the Chebotarev density theorem. We use this version of the Chebotarev density theorem to deduce several consequences for class groups in families of number fields.","sentences":["Given a finite group G, we prove that the vector space spanned by the faithful irreducible characters of G is generated by the monomial characters in the vector space.","As a consequence, we show that in any family of G-extensions of a fixed number field F, almost all are subject to a strong effective version of the Chebotarev density theorem.","We use this version of the Chebotarev density theorem to deduce several consequences for class groups in families of number fields."],"url":"http://arxiv.org/abs/2405.08383v1","category":"math.NT"}
{"created":"2024-05-14 07:25:47","title":"On Instability Properties of the Fractional Calder\u00f3n Problem","abstract":"We prove exponential instability properties for the fractional Calder\\'on problem and the conductivity formulation of the fractional Calder\\'on problem in the regime of fractional powers $s\\in (0,1)$. We particularly focus on two settings: First, we discuss instability properties in general domain geometries with scaling critical $L^{\\frac{n}{2s}}$ potentials and constant background metrics. Secondly, we investigate instability properties in general geometries with $L^{\\frac{n}{2s}}$ potentials and low regularity, variable coefficient, possibly anisotropic background metrics. In both settings we make use of the methods introduced in \\cite{KRS21} and we deduce strong compression estimates for the forward problem. In the first setting this is based on analytic smoothing estimates for a suitable comparison operator while in the second setting involving low regularity metrics this is based on an iterated compression gain. We thus generalize the results from \\cite{RS18} to generic geometries and variable coefficients and further also discuss the setting of fractional conductivity equations. In particular, this proves that the logarithmic stability estimates for the fractional Calder\\'on problem from \\cite{RS20} are optimal.","sentences":["We prove exponential instability properties for the fractional Calder\\'on problem and the conductivity formulation of the fractional Calder\\'on problem in the regime of fractional powers $s\\in (0,1)$. We particularly focus on two settings: First, we discuss instability properties in general domain geometries with scaling critical $L^{\\frac{n}{2s}}$ potentials and constant background metrics.","Secondly, we investigate instability properties in general geometries with $L^{\\frac{n}{2s}}$ potentials and low regularity, variable coefficient, possibly anisotropic background metrics.","In both settings we make use of the methods introduced in \\cite{KRS21} and we deduce strong compression estimates for the forward problem.","In the first setting this is based on analytic smoothing estimates for a suitable comparison operator while in the second setting involving low regularity metrics this is based on an iterated compression gain.","We thus generalize the results from \\cite{RS18} to generic geometries and variable coefficients and further also discuss the setting of fractional conductivity equations.","In particular, this proves that the logarithmic stability estimates for the fractional Calder\\'on problem from \\cite{RS20} are optimal."],"url":"http://arxiv.org/abs/2405.08381v1","category":"math.AP"}
{"created":"2024-05-14 07:23:10","title":"CIER: A Novel Experience Replay Approach with Causal Inference in Deep Reinforcement Learning","abstract":"In the training process of Deep Reinforcement Learning (DRL), agents require repetitive interactions with the environment. With an increase in training volume and model complexity, it is still a challenging problem to enhance data utilization and explainability of DRL training. This paper addresses these challenges by focusing on the temporal correlations within the time dimension of time series. We propose a novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences. Furthermore, the subsequences are employed for causal inference to identify fundamental causal factors that significantly impact training outcomes. We design a module to provide feedback on the causality during DRL training. Several experiments demonstrate the feasibility of our approach in common environments, confirming its ability to enhance the effectiveness of DRL training and impart a certain level of explainability to the training process. Additionally, we extended our approach with priority experience replay algorithm, and experimental results demonstrate the continued effectiveness of our approach.","sentences":["In the training process of Deep Reinforcement Learning (DRL), agents require repetitive interactions with the environment.","With an increase in training volume and model complexity, it is still a challenging problem to enhance data utilization and explainability of DRL training.","This paper addresses these challenges by focusing on the temporal correlations within the time dimension of time series.","We propose a novel approach to segment multivariate time series into meaningful subsequences and represent the time series based on these subsequences.","Furthermore, the subsequences are employed for causal inference to identify fundamental causal factors that significantly impact training outcomes.","We design a module to provide feedback on the causality during DRL training.","Several experiments demonstrate the feasibility of our approach in common environments, confirming its ability to enhance the effectiveness of DRL training and impart a certain level of explainability to the training process.","Additionally, we extended our approach with priority experience replay algorithm, and experimental results demonstrate the continued effectiveness of our approach."],"url":"http://arxiv.org/abs/2405.08380v1","category":"cs.LG"}
{"created":"2024-05-14 07:21:27","title":"Detecting and Handling Reflection Symmetries in Mixed-Integer (Nonlinear) Programming","abstract":"Symmetries in mixed-integer (nonlinear) programs (MINLP), if not handled appropriately, are known to negatively impact the performance of (spatial) branch-and-bound algorithms. Usually one thus tries to remove symmetries from the problem formulation or is relying on a solver that automatically detects and handles symmetries. While modelers of a problem can handle various kinds of symmetries, automatic symmetry detection and handling is mostly restricted to permutation symmetries. This article therefore develops techniques such that also black-box solvers can automatically detect and handle a broader class of symmetries.   Inspired from geometric packing problems such as the kissing number problem, we focus on reflection symmetries of MINLPs. We develop a generic and easily applicable framework that allows to automatically detect reflection symmetries for MINLPs. To handle this broader class of symmetries, we discuss generalizations of state-of-the-art methods for permutation symmetries, and develop dedicated symmetry handling methods for special reflection symmetry groups. Our symmetry detection framework has been implemented in the open-source solver SCIP and we provide a comprehensive discussion of the implementation. The article concludes with a detailed numerical evaluation of our symmetry handling methods when solving MINLPs.","sentences":["Symmetries in mixed-integer (nonlinear) programs (MINLP), if not handled appropriately, are known to negatively impact the performance of (spatial) branch-and-bound algorithms.","Usually one thus tries to remove symmetries from the problem formulation or is relying on a solver that automatically detects and handles symmetries.","While modelers of a problem can handle various kinds of symmetries, automatic symmetry detection and handling is mostly restricted to permutation symmetries.","This article therefore develops techniques such that also black-box solvers can automatically detect and handle a broader class of symmetries.   ","Inspired from geometric packing problems such as the kissing number problem, we focus on reflection symmetries of MINLPs.","We develop a generic and easily applicable framework that allows to automatically detect reflection symmetries for MINLPs.","To handle this broader class of symmetries, we discuss generalizations of state-of-the-art methods for permutation symmetries, and develop dedicated symmetry handling methods for special reflection symmetry groups.","Our symmetry detection framework has been implemented in the open-source solver SCIP and we provide a comprehensive discussion of the implementation.","The article concludes with a detailed numerical evaluation of our symmetry handling methods when solving MINLPs."],"url":"http://arxiv.org/abs/2405.08379v1","category":"math.OC"}
{"created":"2024-05-14 07:19:37","title":"ASP-Completeness of Hamiltonicity in Grid Graphs, with Applications to Loop Puzzles","abstract":"We prove that Hamiltonicity in maximum-degree-3 grid graphs (directed or undirected) is ASP-complete, i.e., it has a parsimonious reduction from every NP search problem (including a polynomial-time bijection between solutions). As a consequence, given k Hamiltonian cycles, it is NP-complete to find another; and counting Hamiltonian cycles is #P-complete. If we require the grid graph's vertices to form a full $m \\times n$ rectangle, then we show that Hamiltonicity remains ASP-complete if the edges are directed or if we allow removing some edges (whereas including all undirected edges is known to be easy). These results enable us to develop a stronger \"T-metacell\" framework for proving ASP-completeness of rectangular puzzles, which requires building just a single gadget representing a degree-3 grid-graph vertex. We apply this general theory to prove ASP-completeness of 38 pencil-and-paper puzzles where the goal is to draw a loop subject to given constraints: Slalom, Onsen-meguri, Mejilink, Detour, Tapa-Like Loop, Kouchoku, Icelom; Masyu, Yajilin, Nagareru, Castle Wall, Moon or Sun, Country Road, Geradeweg, Maxi Loop, Mid-loop, Balance Loop, Simple Loop, Haisu, Reflect Link, Linesweeper; Vertex/Touch Slitherlink, Dotchi-Loop, Ovotovata, Building Walk, Rail Pool, Disorderly Loop, Ant Mill, Koburin, Mukkonn Enn, Rassi Silai, (Crossing) Ichimaga, Tapa, Canal View, Aqre, and Paintarea. The last 14 of these puzzles were not even known to be NP-hard. Along the way, we prove ASP-completeness of some simple forms of Tree-Residue Vertex-Breaking (TRVB), including planar multigraphs with degree-6 breakable vertices, or with degree-4 breakable and degree-1 unbreakable vertices.","sentences":["We prove that Hamiltonicity in maximum-degree-3 grid graphs (directed or undirected) is ASP-complete, i.e., it has a parsimonious reduction from every NP search problem (including a polynomial-time bijection between solutions).","As a consequence, given k Hamiltonian cycles, it is NP-complete to find another; and counting Hamiltonian cycles is #P-complete.","If we require the grid graph's vertices to form a full $m \\times n$ rectangle, then we show that Hamiltonicity remains ASP-complete if the edges are directed or if we allow removing some edges (whereas including all undirected edges is known to be easy).","These results enable us to develop a stronger \"T-metacell\" framework for proving ASP-completeness of rectangular puzzles, which requires building just a single gadget representing a degree-3 grid-graph vertex.","We apply this general theory to prove ASP-completeness of 38 pencil-and-paper puzzles where the goal is to draw a loop subject to given constraints: Slalom, Onsen-meguri, Mejilink, Detour, Tapa-Like Loop, Kouchoku, Icelom; Masyu, Yajilin, Nagareru, Castle Wall, Moon or Sun, Country Road, Geradeweg, Maxi Loop, Mid-loop, Balance Loop, Simple Loop, Haisu, Reflect Link, Linesweeper; Vertex/Touch Slitherlink, Dotchi-Loop, Ovotovata, Building Walk, Rail Pool, Disorderly Loop, Ant Mill, Koburin, Mukkonn Enn, Rassi Silai, (Crossing) Ichimaga, Tapa, Canal View, Aqre, and Paintarea.","The last 14 of these puzzles were not even known to be NP-hard.","Along the way, we prove ASP-completeness of some simple forms of Tree-Residue Vertex-Breaking (TRVB), including planar multigraphs with degree-6 breakable vertices, or with degree-4 breakable and degree-1 unbreakable vertices."],"url":"http://arxiv.org/abs/2405.08377v1","category":"cs.CC"}
{"created":"2024-05-14 07:16:36","title":"PromptMind Team at MEDIQA-CORR 2024: Improving Clinical Text Correction with Error Categorization and LLM Ensembles","abstract":"This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals. This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it. Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information. We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy. We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge. In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance.","sentences":["This paper describes our approach to the MEDIQA-CORR shared task, which involves error detection and correction in clinical notes curated by medical professionals.","This task involves handling three subtasks: detecting the presence of errors, identifying the specific sentence containing the error, and correcting it.","Through our work, we aim to assess the capabilities of Large Language Models (LLMs) trained on a vast corpora of internet data that contain both factual and unreliable information.","We propose to comprehensively address all subtasks together, and suggest employing a unique prompt-based in-context learning strategy.","We will evaluate its efficacy in this specialized task demanding a combination of general reasoning and medical knowledge.","In medical systems where prediction errors can have grave consequences, we propose leveraging self-consistency and ensemble methods to enhance error correction and error detection performance."],"url":"http://arxiv.org/abs/2405.08373v1","category":"cs.CL"}
{"created":"2024-05-14 07:14:20","title":"Homogeneous spaces of semidirect products and finite Gelfand pairs","abstract":"Let $K\\leq H$ be two finite groups and let $C\\leq A$ be two finite abelian groups, with $H$ acting on $A$ as a group of isomorphisms admitting $C$ as a $K$-invariant subgroup. We study the homogeneous space $X\\coloneqq\\left(H\\ltimes A\\right)/\\left(K\\ltimes C\\right)$ and determine the decomposition of the permutation representation of $H\\ltimes A$ acting on $X$. We then characterize when this is multiplicity-free, that is, when $\\left(H\\ltimes A,K\\ltimes C\\right)$ is a Gelfand pair. If this is the case, we explicitly calculate the corresponding spherical functions. From our general construction and related analysis, we recover Dunkl's results on the $q$-analog of the nonbinary Johnson scheme.","sentences":["Let $K\\leq H$ be two finite groups and let $C\\leq A$ be two finite abelian groups, with $H$ acting on $A$ as a group of isomorphisms admitting $C$ as a $K$-invariant subgroup.","We study the homogeneous space $X\\coloneqq\\left(H\\ltimes A\\right)/\\left(K\\ltimes C\\right)$ and determine the decomposition of the permutation representation of $H\\ltimes A$ acting on $X$. We then characterize when this is multiplicity-free, that is, when $\\left(H\\ltimes A,K\\ltimes C\\right)$ is a Gelfand pair.","If this is the case, we explicitly calculate the corresponding spherical functions.","From our general construction and related analysis, we recover Dunkl's results on the $q$-analog of the nonbinary Johnson scheme."],"url":"http://arxiv.org/abs/2405.08371v1","category":"math.RT"}
{"created":"2024-05-14 07:12:07","title":"Orbifolded Elliptic Genera of Non-Compact Models","abstract":"We revisit the flavored elliptic genus of the N=2 superconformal cigar model and generalize the analysis of the path integral result to the case of real central charge. It gives rise to a non-holomorphic modular covariant function generalizing completed mock modular forms. We also compute the genus for angular orbifolds of the cigar and Liouville theory and decompose it in terms of discrete and continuous contributions. The orbifolded elliptic genus at fractional level is a completed mock modular form with a shadow related to U$(1)$ modular invariants at rational radius squared. We take the limit of the orbifolded genera towards a weighted ground state index and carefully interpret the contributions. We stress that the orbifold cigar and Liouville theories have a maximal and a minimal radius, respectively.","sentences":["We revisit the flavored elliptic genus of the N=2 superconformal cigar model and generalize the analysis of the path integral result to the case of real central charge.","It gives rise to a non-holomorphic modular covariant function generalizing completed mock modular forms.","We also compute the genus for angular orbifolds of the cigar and Liouville theory and decompose it in terms of discrete and continuous contributions.","The orbifolded elliptic genus at fractional level is a completed mock modular form with a shadow related to U$(1)$ modular invariants at rational radius squared.","We take the limit of the orbifolded genera towards a weighted ground state index and carefully interpret the contributions.","We stress that the orbifold cigar and Liouville theories have a maximal and a minimal radius, respectively."],"url":"http://arxiv.org/abs/2405.08370v1","category":"hep-th"}
{"created":"2024-05-14 07:10:09","title":"A generic approach to homogenization of a diffusion driven by growing incompressible drift","abstract":"We study how the resolvent-family of a diffusion behaves, as thedrift grows to infinity. The limit turns out to be a selfadjoint pseudo-resolvent.After reduction of the underlying Hilbert-space, this pseudo-resolvent becomesa resolvent to a strongly continuous semi-group of contractions. We prove thatthis semi-group is associated to some Hunt-process on some suitable state-space which is constructed from equivalence classes of the drifts trajectories.Finally, we show a distributional limit theorem for the accelerated diffusiontoward the associated Hunt process.","sentences":["We study how the resolvent-family of a diffusion behaves, as thedrift grows to infinity.","The limit turns out to be a selfadjoint pseudo-resolvent.","After reduction of the underlying Hilbert-space, this pseudo-resolvent becomesa resolvent to a strongly continuous semi-group of contractions.","We prove thatthis semi-group is associated to some Hunt-process on some suitable state-space which is constructed from equivalence classes of the drifts trajectories.","Finally, we show a distributional limit theorem for the accelerated diffusiontoward the associated Hunt process."],"url":"http://arxiv.org/abs/2405.08369v1","category":"math.PR"}
{"created":"2024-05-14 07:05:33","title":"Is addition definable from multiplication and successor?","abstract":"A map $f\\colon R\\to S$ between (associative, unital, but not necessarily commutative) rings is a \\emph{brachymorphism} if $f(x+1)=f(x)+1$ and $f(xy)=f(x)f(y)$ whenever $x,y\\in R$.We tackle the problem whether every brachymorphism is additive (i.e., $f(x+y)=f(x)+f(y)$), showing that in many contexts, including the following, the answer is positive: -- $R$ is finite (or, more generally, $R$ is left or right Artinian); -- $R$ is any ring of $2\\times2$ matrices over a commutative ring; -- $R$ is Engelian; -- every element of $R$ is a sum of $\\pi$-regular and central elements (this applies to $\\pi$-regular rings, C*-algebras, and power series rings); -- $R$ is the full matrix ring of order greater than~$1$ over any ring; -- $f$ is the power function $x\\mapsto x^n$ over any ring; -- $f$ is the determinant function over any ring $R$ of $n\\times n$ matrices, with $n\\geq3$, over a commutative ring, such that if $n>3$ then $R$ contains $n$ scalar matrices with invertible differences.We leave open the problem whether every brachymorphism is additive.","sentences":["A map $f\\colon R\\to S$ between (associative, unital, but not necessarily commutative) rings is a \\emph{brachymorphism} if $f(x+1)=f(x)+1$ and $f(xy)=f(x)f(y)$ whenever $x,y\\in R$.We tackle the problem whether every brachymorphism is additive (i.e., $f(x+y)=f(x)+f(y)$), showing that in many contexts, including the following, the answer is positive: -- $R$ is finite (or, more generally, $R$ is left or right Artinian); -- $R$ is any ring of $2\\times2$ matrices over a commutative ring; -- $R$ is Engelian; -- every element of $R$ is a sum of $\\pi$-regular and central elements (this applies to $\\pi$-regular rings, C*-algebras, and power series rings); -- $R$ is the full matrix ring of order greater than~$1$ over any ring; -- $f$ is the power function $x\\mapsto x^n$ over any ring; -- $f$ is the determinant function over any ring $R$ of $n\\times n$ matrices, with $n\\geq3$, over a commutative ring, such that if $n>3$ then $R$ contains $n$ scalar matrices with invertible differences.","We leave open the problem whether every brachymorphism is additive."],"url":"http://arxiv.org/abs/2405.08364v1","category":"math.RA"}
{"created":"2024-05-14 07:04:05","title":"Multiband strain balanced superlattice material system for third generation infrared detectors","abstract":"Recent increasing interest in strain balanced Type-II superlattices material causing close attention from industry. Tremendous investment was drawn toward establishing strain balanced superlattice (SLS) as new alternative for infrared photodetectors across the broad range of infrared spectrum. In recent years, SLS material system has shown capability in particular specifications to compete with mature and standard mercury cadmium telluride for mid-wavelength and long-wavelength infrared detection. It has been great interest in SLS material system for applications aligned with the standard of third generation of infrared detectors. In that level, photodetectors with multi-color detection capabilities based on SLS material system are highly desired. In this presentation, some recent progress in three color infrared photodetectors based on SLS material system will be presented.","sentences":["Recent increasing interest in strain balanced Type-II superlattices material causing close attention from industry.","Tremendous investment was drawn toward establishing strain balanced superlattice (SLS) as new alternative for infrared photodetectors across the broad range of infrared spectrum.","In recent years, SLS material system has shown capability in particular specifications to compete with mature and standard mercury cadmium telluride for mid-wavelength and long-wavelength infrared detection.","It has been great interest in SLS material system for applications aligned with the standard of third generation of infrared detectors.","In that level, photodetectors with multi-color detection capabilities based on SLS material system are highly desired.","In this presentation, some recent progress in three color infrared photodetectors based on SLS material system will be presented."],"url":"http://arxiv.org/abs/2405.08362v1","category":"physics.app-ph"}
{"created":"2024-05-14 07:02:33","title":"A Representability Theorem for Stacks in Derived Geometry Contexts","abstract":"The representability theorem for stacks, due to Artin in the underived setting and Lurie in the derived setting, gives conditions under which a stack is representable by an $n$-geometric stack. In recent work of Ben-Bassat, Kelly, and Kremnizer, a new theory of derived analytic geometry has been proposed as geometry relative to the $(\\infty,1)$-category of simplicial commutative Ind-Banach $R$-modules, for $R$ a Banach ring. In this paper, we prove a representability theorem which holds in a very general context, which we call a representability context, encompassing both the derived algebraic geometry context of To\\\"en and Vezzosi and these new derived analytic geometry contexts. The representability theorem gives natural and easily verifiable conditions for checking that derived stacks in these contexts are $n$-geometric, such as having an $n$-geometric truncation, being nilcomplete, and having an obstruction theory. Future work will explore representability of certain moduli stacks arising in derived analytic geometry, for example moduli stacks of Galois representations.","sentences":["The representability theorem for stacks, due to Artin in the underived setting and Lurie in the derived setting, gives conditions under which a stack is representable by an $n$-geometric stack.","In recent work of Ben-Bassat, Kelly, and Kremnizer, a new theory of derived analytic geometry has been proposed as geometry relative to the $(\\infty,1)$-category of simplicial commutative Ind-Banach $R$-modules, for $R$ a Banach ring.","In this paper, we prove a representability theorem which holds in a very general context, which we call a representability context, encompassing both the derived algebraic geometry context of To\\\"en and Vezzosi and these new derived analytic geometry contexts.","The representability theorem gives natural and easily verifiable conditions for checking that derived stacks in these contexts are $n$-geometric, such as having an $n$-geometric truncation, being nilcomplete, and having an obstruction theory.","Future work will explore representability of certain moduli stacks arising in derived analytic geometry, for example moduli stacks of Galois representations."],"url":"http://arxiv.org/abs/2405.08361v1","category":"math.AG"}
{"created":"2024-05-14 07:01:48","title":"A Local discontinuous Galerkin method for the Benajamin-Ono equation","abstract":"The main purpose of this paper is to design a local discontinuous Galerkin (LDG) method for the Benjamin-Ono equation. We analyze the stability and error estimates for the semi-discrete LDG scheme. We prove that the scheme is $L^2$-stable and it converges at a rate $\\mathcal{O}(h^{k+1/2})$ for general nonlinear flux. Furthermore, we develop a fully discrete LDG scheme using the four-stage fourth order Runge-Kutta method and ensure the devised scheme is strongly stable in case of linear flux using two-step and three-step stability approach under an appropriate time step constraint. Numerical examples are provided to validate the efficiency and accuracy of the method.","sentences":["The main purpose of this paper is to design a local discontinuous Galerkin (LDG) method for the Benjamin-Ono equation.","We analyze the stability and error estimates for the semi-discrete LDG scheme.","We prove that the scheme is $L^2$-stable and it converges at a rate $\\mathcal{O}(h^{k+1/2})$ for general nonlinear flux.","Furthermore, we develop a fully discrete LDG scheme using the four-stage fourth order Runge-Kutta method and ensure the devised scheme is strongly stable in case of linear flux using two-step and three-step stability approach under an appropriate time step constraint.","Numerical examples are provided to validate the efficiency and accuracy of the method."],"url":"http://arxiv.org/abs/2405.08360v1","category":"math.NA"}
{"created":"2024-05-14 06:50:19","title":"Seal-Tools: Self-Instruct Tool Learning Dataset for Agent Tuning and Detailed Benchmark","abstract":"This paper presents a new tool learning dataset Seal-Tools, which contains self-instruct API-like tools. Seal-Tools not only offers a large number of tools, but also includes instances which demonstrate the practical application of tools. Seeking to generate data on a large scale while ensuring reliability, we propose a self-instruct method to generate tools and instances, allowing precise control over the process. Moreover, our Seal-Tools contains hard instances that call multiple tools to complete the job, among which some are nested tool callings. For precise and comprehensive evaluation, we use strict format control and design three metrics from different dimensions. Therefore, Seal-Tools can serve as a new benchmark to evaluate the tool-calling ability of LLMs. Finally, we evaluate several prevalent LLMs and our finetuned model on Seal-Tools. The results show that current systems are far from perfect. The code, data and experiment results are available at https://github.com/fairyshine/Seal-Tools .","sentences":["This paper presents a new tool learning dataset Seal-Tools, which contains self-instruct API-like tools.","Seal-Tools not only offers a large number of tools, but also includes instances which demonstrate the practical application of tools.","Seeking to generate data on a large scale while ensuring reliability, we propose a self-instruct method to generate tools and instances, allowing precise control over the process.","Moreover, our Seal-Tools contains hard instances that call multiple tools to complete the job, among which some are nested tool callings.","For precise and comprehensive evaluation, we use strict format control and design three metrics from different dimensions.","Therefore, Seal-Tools can serve as a new benchmark to evaluate the tool-calling ability of LLMs.","Finally, we evaluate several prevalent LLMs and our finetuned model on Seal-Tools.","The results show that current systems are far from perfect.","The code, data and experiment results are available at https://github.com/fairyshine/Seal-Tools ."],"url":"http://arxiv.org/abs/2405.08355v1","category":"cs.CL"}
{"created":"2024-05-14 06:50:17","title":"Fuzzy Dark Matter Less-complex Wormhole Structures in Extended Theories of Gravity","abstract":"Fuzzy dark matter wormhole solutions coupled with anisotropic matter distribution are explored in 4D Einstein-Gauss-Bonnet and $f(R)$ gravity, where $R$ is the Ricci scalar. We derive the shape function for fuzzy wormholes and explore their possible stability. We study the embedding diagrams of the active gravitational mass associated with fuzzy dark matter wormholes by taking a certain shape function. Aiming to highlight the role of Einstein-Gauss-Bonnet and $f(R)$ gravity in the modeling of less complex fuzzy wormhole structures, we evaluate the complexity factor, the conservation equation, and null energy conditions. Our study reinforces more importance of uniformly distributed pressure effects throughout the less complex region than to the emergence of energy density homogeneity in the stability of fuzzy wormholes. It is shown that the active gravitational mass of the fuzzy wormhole structures varies inversely with the radial distance thereby suggesting the breaching of energy conditions at some arena of Einasto index. Furthermore, it is revealed that stable fuzzy dark matter wormhole structures exist in nature in the surroundings of cold dark matter halos and galactic bulges. The important physics understood from our analysis is that in both four-dimensional Einstein-Gauss-Bonnet and $f(R)$ gravity, feasible geometries of fuzzy dark matter wormholes exist naturally in the environments of different galactic haloes.","sentences":["Fuzzy dark matter wormhole solutions coupled with anisotropic matter distribution are explored in 4D Einstein-Gauss-Bonnet and $f(R)$ gravity, where $R$ is the Ricci scalar.","We derive the shape function for fuzzy wormholes and explore their possible stability.","We study the embedding diagrams of the active gravitational mass associated with fuzzy dark matter wormholes by taking a certain shape function.","Aiming to highlight the role of Einstein-Gauss-Bonnet and $f(R)$ gravity in the modeling of less complex fuzzy wormhole structures, we evaluate the complexity factor, the conservation equation, and null energy conditions.","Our study reinforces more importance of uniformly distributed pressure effects throughout the less complex region than to the emergence of energy density homogeneity in the stability of fuzzy wormholes.","It is shown that the active gravitational mass of the fuzzy wormhole structures varies inversely with the radial distance thereby suggesting the breaching of energy conditions at some arena of Einasto index.","Furthermore, it is revealed that stable fuzzy dark matter wormhole structures exist in nature in the surroundings of cold dark matter halos and galactic bulges.","The important physics understood from our analysis is that in both four-dimensional Einstein-Gauss-Bonnet and $f(R)$ gravity, feasible geometries of fuzzy dark matter wormholes exist naturally in the environments of different galactic haloes."],"url":"http://arxiv.org/abs/2405.08354v1","category":"gr-qc"}
{"created":"2024-05-14 06:45:27","title":"Sibson's $\u03b1$-Mutual Information and its Variational Representations","abstract":"Information measures can be constructed from R\\'enyi divergences much like mutual information from Kullback-Leibler divergence. One such information measure is known as Sibson's $\\alpha$-mutual information and has received renewed attention recently in several contexts: concentration of measure under dependence, statistical learning, hypothesis testing, and estimation theory. In this paper, we survey and extend the state of the art. In particular, we introduce variational representations for Sibson's $\\alpha$-mutual information and employ them in each of the contexts just described to derive novel results. Namely, we produce generalized Transportation-Cost inequalities and Fano-type inequalities. We also present an overview of known applications, spanning from learning theory and Bayesian risk to universal prediction.","sentences":["Information measures can be constructed from R\\'enyi divergences much like mutual information from Kullback-Leibler divergence.","One such information measure is known as Sibson's $\\alpha$-mutual information and has received renewed attention recently in several contexts: concentration of measure under dependence, statistical learning, hypothesis testing, and estimation theory.","In this paper, we survey and extend the state of the art.","In particular, we introduce variational representations for Sibson's $\\alpha$-mutual information and employ them in each of the contexts just described to derive novel results.","Namely, we produce generalized Transportation-Cost inequalities and Fano-type inequalities.","We also present an overview of known applications, spanning from learning theory and Bayesian risk to universal prediction."],"url":"http://arxiv.org/abs/2405.08352v1","category":"cs.IT"}
{"created":"2024-05-14 06:38:27","title":"Model-Free Unsupervised Anomaly detection framework in multivariate time-series of industrial dynamical systems","abstract":"In this paper, a new model-free anomaly detection framework is proposed for time-series induced by industrial dynamical systems. The framework lies in the category of conventional approaches which enable appealing features such as, a fast learning with reduced amount of learning data, a reduced memory, a high potential for explainability as well as easiness of incremental learning mechanism to incorporate operator feedback after an alarm is raised an analyzed. All these are crucial features towards acceptance of data-driven solution by industry but they are rarely considered in the comparisons between competing methods which generally exclusively focus on performance metrics. Moreover, the features engineering step involved in the proposed framework is inspired by the time-series being implicitly governed by physical laws as it is generally the case in industrial time-series. Two examples are given to assess the efficiency of the proposed approach.","sentences":["In this paper, a new model-free anomaly detection framework is proposed for time-series induced by industrial dynamical systems.","The framework lies in the category of conventional approaches which enable appealing features such as, a fast learning with reduced amount of learning data, a reduced memory, a high potential for explainability as well as easiness of incremental learning mechanism to incorporate operator feedback after an alarm is raised an analyzed.","All these are crucial features towards acceptance of data-driven solution by industry but they are rarely considered in the comparisons between competing methods which generally exclusively focus on performance metrics.","Moreover, the features engineering step involved in the proposed framework is inspired by the time-series being implicitly governed by physical laws as it is generally the case in industrial time-series.","Two examples are given to assess the efficiency of the proposed approach."],"url":"http://arxiv.org/abs/2405.08349v1","category":"eess.SY"}
{"created":"2024-05-14 06:31:42","title":"Accuracy Evaluation of a Lightweight Analytic Vehicle Dynamics Model for Maneuver Planning","abstract":"Models for vehicle dynamics play an important role in maneuver planning for automated driving. They are used to derive trajectories from given control inputs, or to evaluate a given trajectory in terms of constraint violation or optimality criteria such as safety, comfort or ecology. Depending on the computation process, models with different assumptions and levels of detail are used; since maneuver planning usually has strong requirements for computation speed at a potentially high number of trajectory evaluations per planning cycle, most of the applied models aim to reduce complexity by implicitly or explicitly introducing simplifying assumptions. While evaluations show that these assumptions may be sufficiently valid under typical conditions, their effect has yet to be studied conclusively.   We propose a model for vehicle dynamics that is convenient for maneuver planning by supporting both an analytic approach of extracting parameters from a given trajectory, and a generative approach of establishing a trajectory from given control inputs. Both applications of the model are evaluated in real-world test drives under dynamic conditions, both on a closed-off test track and on public roads, and effects arising from the simplifying assumptions are analyzed.","sentences":["Models for vehicle dynamics play an important role in maneuver planning for automated driving.","They are used to derive trajectories from given control inputs, or to evaluate a given trajectory in terms of constraint violation or optimality criteria such as safety, comfort or ecology.","Depending on the computation process, models with different assumptions and levels of detail are used; since maneuver planning usually has strong requirements for computation speed at a potentially high number of trajectory evaluations per planning cycle, most of the applied models aim to reduce complexity by implicitly or explicitly introducing simplifying assumptions.","While evaluations show that these assumptions may be sufficiently valid under typical conditions, their effect has yet to be studied conclusively.   ","We propose a model for vehicle dynamics that is convenient for maneuver planning by supporting both an analytic approach of extracting parameters from a given trajectory, and a generative approach of establishing a trajectory from given control inputs.","Both applications of the model are evaluated in real-world test drives under dynamic conditions, both on a closed-off test track and on public roads, and effects arising from the simplifying assumptions are analyzed."],"url":"http://arxiv.org/abs/2405.08343v1","category":"cs.RO"}
{"created":"2024-05-14 06:31:38","title":"Abnormal Respiratory Sound Identification Using Audio-Spectrogram Vision Transformer","abstract":"Respiratory disease, the third leading cause of deaths globally, is considered a high-priority ailment requiring significant research on identification and treatment. Stethoscope-recorded lung sounds and artificial intelligence-powered devices have been used to identify lung disorders and aid specialists in making accurate diagnoses. In this study, audio-spectrogram vision transformer (AS-ViT), a new approach for identifying abnormal respiration sounds, was developed. The sounds of the lungs are converted into visual representations called spectrograms using a technique called short-time Fourier transform (STFT). These images are then analyzed using a model called vision transformer to identify different types of respiratory sounds. The classification was carried out using the ICBHI 2017 database, which includes various types of lung sounds with different frequencies, noise levels, and backgrounds. The proposed AS-ViT method was evaluated using three metrics and achieved 79.1% and 59.8% for 60:40 split ratio and 86.4% and 69.3% for 80:20 split ratio in terms of unweighted average recall and overall scores respectively for respiratory sound detection, surpassing previous state-of-the-art results.","sentences":["Respiratory disease, the third leading cause of deaths globally, is considered a high-priority ailment requiring significant research on identification and treatment.","Stethoscope-recorded lung sounds and artificial intelligence-powered devices have been used to identify lung disorders and aid specialists in making accurate diagnoses.","In this study, audio-spectrogram vision transformer (AS-ViT), a new approach for identifying abnormal respiration sounds, was developed.","The sounds of the lungs are converted into visual representations called spectrograms using a technique called short-time Fourier transform (STFT).","These images are then analyzed using a model called vision transformer to identify different types of respiratory sounds.","The classification was carried out using the ICBHI 2017 database, which includes various types of lung sounds with different frequencies, noise levels, and backgrounds.","The proposed AS-ViT method was evaluated using three metrics and achieved 79.1% and 59.8% for 60:40 split ratio and 86.4% and 69.3% for 80:20 split ratio in terms of unweighted average recall and overall scores respectively for respiratory sound detection, surpassing previous state-of-the-art results."],"url":"http://arxiv.org/abs/2405.08342v1","category":"cs.SD"}
{"created":"2024-05-14 06:27:44","title":"On approximation to a real number by algebraic numbers of bounded degree","abstract":"In his seminal 1961 paper, Wirsing studied how well a given transcendental real number $\\xi$ can be approximated by algebraic numbers $\\alpha$ of degree at most $n$ for a given positive integer $n$, in terms of the so-called naive height $H(\\alpha)$ of $\\alpha$. He showed that the infimum $\\omega^*_n(\\xi)$ of all $\\omega$ for which infinitely many such $\\alpha$ have $|\\xi-\\alpha| \\le H(\\alpha)^{-\\omega-1}$ is at least $(n+1)/2$. He also asked if we could even have $\\omega^*_n(\\xi) \\ge n$ as it is generally expected. Since then, all improvements on Wirsing's lower bound were of the form $n/2+\\mathcal{O}(1)$ until Badziahin and Schleischitz showed in 2021 that $\\omega^*_n(\\xi) \\ge an$ for each $n\\ge 4$, with $a=1/\\sqrt{3}\\simeq 0.577$. In this paper, we use a different approach partly inspired by parametric geometry of numbers and show that $\\omega^*_n(\\xi) \\ge an$ for each $n\\ge 2$, with $a=1/(2-\\log 2)\\simeq 0.765$.","sentences":["In his seminal 1961 paper, Wirsing studied how well a given transcendental real number $\\xi$ can be approximated by algebraic numbers $\\alpha$ of degree at most $n$ for a given positive integer $n$, in terms of the so-called naive height $H(\\alpha)$ of $\\alpha$. He showed that the infimum $\\omega^*_n(\\xi)$ of all $\\omega$ for which infinitely many such $\\alpha$ have $|\\xi-\\alpha| \\le H(\\alpha)^{-\\omega-1}$ is at least $(n+1)/2$. He also asked if we could even have $\\omega^*_n(\\xi) \\ge n$ as it is generally expected.","Since then, all improvements on Wirsing's lower bound were of the form $n/2+\\mathcal{O}(1)$ until Badziahin and Schleischitz showed in 2021 that $\\omega^*_n(\\xi) \\ge an$ for each $n\\ge 4$, with $a=1/\\sqrt{3}\\simeq 0.577$.","In this paper, we use a different approach partly inspired by parametric geometry of numbers and show that $\\omega^*_n(\\xi) \\ge an$ for each $n\\ge 2$, with $a=1/(2-\\log 2)\\simeq 0.765$."],"url":"http://arxiv.org/abs/2405.08341v1","category":"math.NT"}
{"created":"2024-05-14 06:22:01","title":"Automatic adjustment of undulator optics for FELs","abstract":"In this paper, we describe a way to automatically adjust the quadrupole focusing along the undulator to avoid the instabilities, taking into account energy change and undulator focusing. The procedure is more generalized and applicable to any strongly focusing (planar) undulator.","sentences":["In this paper, we describe a way to automatically adjust the quadrupole focusing along the undulator to avoid the instabilities, taking into account energy change and undulator focusing.","The procedure is more generalized and applicable to any strongly focusing (planar) undulator."],"url":"http://arxiv.org/abs/2405.08339v1","category":"physics.acc-ph"}
{"created":"2024-05-14 06:16:13","title":"Perivascular space Identification Nnunet for Generalised Usage (PINGU)","abstract":"Perivascular spaces(PVSs) form a central component of the brain\\'s waste clearance system, the glymphatic system. These structures are visible on MRI images, and their morphology is associated with aging and neurological disease. Manual quantification of PVS is time consuming and subjective. Numerous deep learning methods for PVS segmentation have been developed, however the majority have been developed and evaluated on homogenous datasets and high resolution scans, perhaps limiting their applicability for the wide range of image qualities acquired in clinic and research. In this work we train a nnUNet, a top-performing biomedical image segmentation algorithm, on a heterogenous training sample of manually segmented MRI images of a range of different qualities and resolutions from 6 different datasets. These are compared to publicly available deep learning methods for 3D segmentation of PVS. The resulting model, PINGU (Perivascular space Identification Nnunet for Generalised Usage), achieved voxel and cluster level dice scores of 0.50(SD=0.15), 0.63(0.17) in the white matter(WM), and 0.54(0.11), 0.66(0.17) in the basal ganglia(BG). Performance on data from unseen sites was substantially lower for both PINGU(0.20-0.38(WM, voxel), 0.29-0.58(WM, cluster), 0.22-0.36(BG, voxel), 0.46-0.60(BG, cluster)) and the publicly available algorithms(0.18-0.30(WM, voxel), 0.29-0.38(WM cluster), 0.10-0.20(BG, voxel), 0.15-0.37(BG, cluster)), but PINGU strongly outperformed the publicly available algorithms, particularly in the BG. Finally, training PINGU on manual segmentations from a single site with homogenous scan properties gave marginally lower performances on internal cross-validation, but in some cases gave higher performance on external validation. PINGU stands out as broad-use PVS segmentation tool, with particular strength in the BG, an area of PVS related to vascular disease and pathology.","sentences":["Perivascular spaces(PVSs) form a central component of the brain\\'s waste clearance system, the glymphatic system.","These structures are visible on MRI images, and their morphology is associated with aging and neurological disease.","Manual quantification of PVS is time consuming and subjective.","Numerous deep learning methods for PVS segmentation have been developed, however the majority have been developed and evaluated on homogenous datasets and high resolution scans, perhaps limiting their applicability for the wide range of image qualities acquired in clinic and research.","In this work we train a nnUNet, a top-performing biomedical image segmentation algorithm, on a heterogenous training sample of manually segmented MRI images of a range of different qualities and resolutions from 6 different datasets.","These are compared to publicly available deep learning methods for 3D segmentation of PVS.","The resulting model, PINGU (Perivascular space Identification Nnunet for Generalised Usage), achieved voxel and cluster level dice scores of 0.50(SD=0.15), 0.63(0.17) in the white matter(WM), and 0.54(0.11), 0.66(0.17) in the basal ganglia(BG).","Performance on data from unseen sites was substantially lower for both PINGU(0.20-0.38(WM, voxel), 0.29-0.58(WM, cluster), 0.22-0.36(BG, voxel), 0.46-0.60(BG, cluster)) and the publicly available algorithms(0.18-0.30(WM, voxel), 0.29-0.38(WM cluster), 0.10-0.20(BG, voxel), 0.15-0.37(BG, cluster)), but PINGU strongly outperformed the publicly available algorithms, particularly in the BG.","Finally, training PINGU on manual segmentations from a single site with homogenous scan properties gave marginally lower performances on internal cross-validation, but in some cases gave higher performance on external validation.","PINGU stands out as broad-use PVS segmentation tool, with particular strength in the BG, an area of PVS related to vascular disease and pathology."],"url":"http://arxiv.org/abs/2405.08337v1","category":"cs.CV"}
{"created":"2024-05-14 06:13:22","title":"A Comparative Study of Shadows of Magnetized and Non-Magnetized Singularities","abstract":"The recent observations of the galactic center of the M87 galaxy have made the field of observing black holes and calculating its shadow much more intriguing. Approaching the question of calculating shadows, many approximations are made in order to simplify the equations which makes the considered case less realistic. Understanding the shadow of different singularities under the influence of magnetic field is of more importance astrophysically as the accreting matter around the singularity would generate electromagnetic fields as it would be in plasma state due to the high tidal effects. Here, we use Ernst technique to immerse spacetimes in uniform, sourceless and asymptotic magnetic field. Later, we compare the effective potential of null geodesics in magnetized and non-magnetized cases. This study would be helpful in understanding the M87 shadow and the forthcoming image of shadow of SagA*.","sentences":["The recent observations of the galactic center of the M87 galaxy have made the field of observing black holes and calculating its shadow much more intriguing.","Approaching the question of calculating shadows, many approximations are made in order to simplify the equations which makes the considered case less realistic.","Understanding the shadow of different singularities under the influence of magnetic field is of more importance astrophysically as the accreting matter around the singularity would generate electromagnetic fields as it would be in plasma state due to the high tidal effects.","Here, we use Ernst technique to immerse spacetimes in uniform, sourceless and asymptotic magnetic field.","Later, we compare the effective potential of null geodesics in magnetized and non-magnetized cases.","This study would be helpful in understanding the M87 shadow and the forthcoming image of shadow of SagA*."],"url":"http://arxiv.org/abs/2405.08335v1","category":"gr-qc"}
{"created":"2024-05-14 06:09:08","title":"Could Chemical LLMs benefit from Message Passing","abstract":"Pretrained language models (LMs) showcase significant capabilities in processing molecular text, while concurrently, message passing neural networks (MPNNs) demonstrate resilience and versatility in the domain of molecular science. Despite these advancements, we find there are limited studies investigating the bidirectional interactions between molecular structures and their corresponding textual representations. Therefore, in this paper, we propose two strategies to evaluate whether an information integration can enhance the performance: contrast learning, which involves utilizing an MPNN to supervise the training of the LM, and fusion, which exploits information from both models. Our empirical analysis reveals that the integration approaches exhibit superior performance compared to baselines when applied to smaller molecular graphs, while these integration approaches do not yield performance enhancements on large scale graphs.","sentences":["Pretrained language models (LMs) showcase significant capabilities in processing molecular text, while concurrently, message passing neural networks (MPNNs) demonstrate resilience and versatility in the domain of molecular science.","Despite these advancements, we find there are limited studies investigating the bidirectional interactions between molecular structures and their corresponding textual representations.","Therefore, in this paper, we propose two strategies to evaluate whether an information integration can enhance the performance: contrast learning, which involves utilizing an MPNN to supervise the training of the LM, and fusion, which exploits information from both models.","Our empirical analysis reveals that the integration approaches exhibit superior performance compared to baselines when applied to smaller molecular graphs, while these integration approaches do not yield performance enhancements on large scale graphs."],"url":"http://arxiv.org/abs/2405.08334v1","category":"cs.LG"}
{"created":"2024-05-14 06:01:48","title":"Parameter estimation and long-range dependence of the fractional binomial process","abstract":"In 1990, Jakeman (see \\cite{jakeman1990statistics}) defined the binomial process as a special case of the classical birth-death process, where the probability of birth is proportional to the difference between a fixed number and the number of individuals present. Later, a fractional generalization of the binomial process was studied by Cahoy and Polito (2012) (see \\cite{cahoy2012fractional}) and called it as fractional binomial process (FBP). In this paper, we study second-order properties of the FBP and the long-range behavior of the FBP and its noise process. We also estimate the parameters of the FBP using the method of moments procedure. Finally, we present the simulated sample paths and its algorithm for the FBP.","sentences":["In 1990, Jakeman (see \\cite{jakeman1990statistics}) defined the binomial process as a special case of the classical birth-death process, where the probability of birth is proportional to the difference between a fixed number and the number of individuals present.","Later, a fractional generalization of the binomial process was studied by Cahoy and Polito (2012) (see \\cite{cahoy2012fractional}) and called it as fractional binomial process (FBP).","In this paper, we study second-order properties of the FBP and the long-range behavior of the FBP and its noise process.","We also estimate the parameters of the FBP using the method of moments procedure.","Finally, we present the simulated sample paths and its algorithm for the FBP."],"url":"http://arxiv.org/abs/2405.08332v1","category":"math.ST"}
{"created":"2024-05-14 05:57:22","title":"Are Generics and Negativity about Social Groups Common on Social Media? A Comparative Analysis of Twitter (X) Data","abstract":"Generics (unquantified generalizations) are thought to be pervasive in communication and when they are about social groups, this may offend and polarize people because generics gloss over variations between individuals. Generics about social groups might be particularly common on Twitter (X). This remains unexplored, however. Using machine learning (ML) techniques, we therefore developed an automatic classifier for social generics, applied it to more than a million tweets about people, and analyzed the tweets. We found that most tweets (78%) about people contained no generics. However, tweets with social generics received more 'likes' and retweets. Furthermore, while recent psychological research may lead to the prediction that tweets with generics about political groups are more common than tweets with generics about ethnic groups, we found the opposite. However, consistent with recent claims that political animosity is less constrained by social norms than animosity against gender and ethnic groups, negative tweets with generics about political groups were significantly more prevalent and retweeted than negative tweets about ethnic groups. Our study provides the first ML-based insights into the use and impact of social generics on Twitter.","sentences":["Generics (unquantified generalizations) are thought to be pervasive in communication and when they are about social groups, this may offend and polarize people because generics gloss over variations between individuals.","Generics about social groups might be particularly common on Twitter (X).","This remains unexplored, however.","Using machine learning (ML) techniques, we therefore developed an automatic classifier for social generics, applied it to more than a million tweets about people, and analyzed the tweets.","We found that most tweets (78%) about people contained no generics.","However, tweets with social generics received more 'likes' and retweets.","Furthermore, while recent psychological research may lead to the prediction that tweets with generics about political groups are more common than tweets with generics about ethnic groups, we found the opposite.","However, consistent with recent claims that political animosity is less constrained by social norms than animosity against gender and ethnic groups, negative tweets with generics about political groups were significantly more prevalent and retweeted than negative tweets about ethnic groups.","Our study provides the first ML-based insights into the use and impact of social generics on Twitter."],"url":"http://arxiv.org/abs/2405.08331v1","category":"cs.SI"}
{"created":"2024-05-14 05:52:01","title":"Cross-Dataset Generalization For Retinal Lesions Segmentation","abstract":"Identifying lesions in fundus images is an important milestone toward an automated and interpretable diagnosis of retinal diseases. To support research in this direction, multiple datasets have been released, proposing groundtruth maps for different lesions. However, important discrepancies exist between the annotations and raise the question of generalization across datasets. This study characterizes several known datasets and compares different techniques that have been proposed to enhance the generalisation performance of a model, such as stochastic weight averaging, model soups and ensembles. Our results provide insights into how to combine coarsely labelled data with a finely-grained dataset in order to improve the lesions segmentation.","sentences":["Identifying lesions in fundus images is an important milestone toward an automated and interpretable diagnosis of retinal diseases.","To support research in this direction, multiple datasets have been released, proposing groundtruth maps for different lesions.","However, important discrepancies exist between the annotations and raise the question of generalization across datasets.","This study characterizes several known datasets and compares different techniques that have been proposed to enhance the generalisation performance of a model, such as stochastic weight averaging, model soups and ensembles.","Our results provide insights into how to combine coarsely labelled data with a finely-grained dataset in order to improve the lesions segmentation."],"url":"http://arxiv.org/abs/2405.08329v1","category":"cs.CV"}
{"created":"2024-05-14 05:50:53","title":"Towards Multi-Task Generative-AI Edge Services with an Attention-based Diffusion DRL Approach","abstract":"As an emerging paradigm of content creation, AI-Generated Content (AIGC) has been widely adopted by a large number of edge end users. However, the requests for generated content from AIGC users have obvious diversity, and there remains a notable lack of research addressing the variance in user demands for AIGC services. This gap underscores a critical need for suitable AIGC service selection mechanisms satisfying various AIGC user requirements under resource-constrained edge environments. To address this challenge, this paper proposes a novel Attention-based Diffusion Soft Actor-Critic (ADSAC) algorithm to select the appropriate AIGC model in response to heterogeneous AIGC user requests. Specifically, the ADSAC algorithm integrates a diffusion model as the policy network in the off-policy reinforcement learning (RL) framework, to capture the intricate relationships between the characteristics of AIGC tasks and the integrated edge network states. Furthermore, an attention mechanism is utilized to harness the contextual long-range dependencies present in state feature vectors, enhancing the decision-making process. Extensive experiments validate the effectiveness of our algorithm in enhancing the overall user utility and reducing the crash rate of servers. Compared to the existing methods, the proposed ADSAC algorithm outperforms existing methods, reducing the overall user utility loss and the server crash rate by at least 58.3% and 58.4%, respectively. These results demonstrate our ADSAC algorithm is a robust solution to the challenges of diverse and dynamic user requirements in edge-based AIGC application environments.","sentences":["As an emerging paradigm of content creation, AI-Generated Content (AIGC) has been widely adopted by a large number of edge end users.","However, the requests for generated content from AIGC users have obvious diversity, and there remains a notable lack of research addressing the variance in user demands for AIGC services.","This gap underscores a critical need for suitable AIGC service selection mechanisms satisfying various AIGC user requirements under resource-constrained edge environments.","To address this challenge, this paper proposes a novel Attention-based Diffusion Soft Actor-Critic (ADSAC) algorithm to select the appropriate AIGC model in response to heterogeneous AIGC user requests.","Specifically, the ADSAC algorithm integrates a diffusion model as the policy network in the off-policy reinforcement learning (RL) framework, to capture the intricate relationships between the characteristics of AIGC tasks and the integrated edge network states.","Furthermore, an attention mechanism is utilized to harness the contextual long-range dependencies present in state feature vectors, enhancing the decision-making process.","Extensive experiments validate the effectiveness of our algorithm in enhancing the overall user utility and reducing the crash rate of servers.","Compared to the existing methods, the proposed ADSAC algorithm outperforms existing methods, reducing the overall user utility loss and the server crash rate by at least 58.3% and 58.4%, respectively.","These results demonstrate our ADSAC algorithm is a robust solution to the challenges of diverse and dynamic user requirements in edge-based AIGC application environments."],"url":"http://arxiv.org/abs/2405.08328v1","category":"cs.NI"}
{"created":"2024-05-14 05:48:23","title":"Centers of Universal Enveloping Algebras","abstract":"The universal enveloping algebra $U(\\mathfrak{g} )$ of a current (super)algebra or loop (super)algebra $\\mathfrak{g} $ is considered over an algebraically closed field $\\mathbb{K} $ with characteristic $p\\ge 0$. This paper focuses on the structure of the center $Z(\\mathfrak{g} )$ of $U(\\mathfrak{g} )$. In the case of zero characteristic, $Z(\\mathfrak{g} )$ is generated by the centers of $\\mathfrak{g} $. In the case of prime characteristic, $Z(\\mathfrak{g} )$ is generated by the centers of $\\mathfrak{g} $ and the $p$-centers of $U(\\mathfrak{g} )$. We also study the structure of $Z(\\mathfrak{g} )$ in the semisimple Lie (super)algebra.","sentences":["The universal enveloping algebra $U(\\mathfrak{g} )$ of a current (super)algebra or loop (super)algebra $\\mathfrak{g} $ is considered over an algebraically closed field $\\mathbb{K} $ with characteristic $p\\ge 0$.","This paper focuses on the structure of the center $Z(\\mathfrak{g} )$ of $U(\\mathfrak{g} )$.","In the case of zero characteristic, $Z(\\mathfrak{g} )$ is generated by the centers of $\\mathfrak{g} $.","In the case of prime characteristic, $Z(\\mathfrak{g} )$ is generated by the centers of $\\mathfrak{g} $ and the $p$-centers of $U(\\mathfrak{g} )$.","We also study the structure of $Z(\\mathfrak{g} )$ in the semisimple Lie (super)algebra."],"url":"http://arxiv.org/abs/2405.08325v1","category":"math.RT"}
{"created":"2024-05-14 04:58:23","title":"No-Regret Learning of Nash Equilibrium for Black-Box Games via Gaussian Processes","abstract":"This paper investigates the challenge of learning in black-box games, where the underlying utility function is unknown to any of the agents. While there is an extensive body of literature on the theoretical analysis of algorithms for computing the Nash equilibrium with complete information about the game, studies on Nash equilibrium in black-box games are less common. In this paper, we focus on learning the Nash equilibrium when the only available information about an agent's payoff comes in the form of empirical queries. We provide a no-regret learning algorithm that utilizes Gaussian processes to identify the equilibrium in such games. Our approach not only ensures a theoretical convergence rate but also demonstrates effectiveness across a variety collection of games through experimental validation.","sentences":["This paper investigates the challenge of learning in black-box games, where the underlying utility function is unknown to any of the agents.","While there is an extensive body of literature on the theoretical analysis of algorithms for computing the Nash equilibrium with complete information about the game, studies on Nash equilibrium in black-box games are less common.","In this paper, we focus on learning the Nash equilibrium when the only available information about an agent's payoff comes in the form of empirical queries.","We provide a no-regret learning algorithm that utilizes Gaussian processes to identify the equilibrium in such games.","Our approach not only ensures a theoretical convergence rate but also demonstrates effectiveness across a variety collection of games through experimental validation."],"url":"http://arxiv.org/abs/2405.08318v1","category":"cs.LG"}
{"created":"2024-05-14 04:51:23","title":"SpeechGuard: Exploring the Adversarial Robustness of Multimodal Large Language Models","abstract":"Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lately. However, the safety and robustness of these models remains largely unclear. In this work, we investigate the potential vulnerabilities of such instruction-following speech-language models to adversarial attacks and jailbreaking. Specifically, we design algorithms that can generate adversarial examples to jailbreak SLMs in both white-box and black-box attack settings without human involvement. Additionally, we propose countermeasures to thwart such jailbreaking attacks. Our models, trained on dialog data with speech instructions, achieve state-of-the-art performance on spoken question-answering task, scoring over 80% on both safety and helpfulness metrics. Despite safety guardrails, experiments on jailbreaking demonstrate the vulnerability of SLMs to adversarial perturbations and transfer attacks, with average attack success rates of 90% and 10% respectively when evaluated on a dataset of carefully designed harmful questions spanning 12 different toxic categories. However, we demonstrate that our proposed countermeasures reduce the attack success significantly.","sentences":["Integrated Speech and Large Language Models (SLMs) that can follow speech instructions and generate relevant text responses have gained popularity lately.","However, the safety and robustness of these models remains largely unclear.","In this work, we investigate the potential vulnerabilities of such instruction-following speech-language models to adversarial attacks and jailbreaking.","Specifically, we design algorithms that can generate adversarial examples to jailbreak SLMs in both white-box and black-box attack settings without human involvement.","Additionally, we propose countermeasures to thwart such jailbreaking attacks.","Our models, trained on dialog data with speech instructions, achieve state-of-the-art performance on spoken question-answering task, scoring over 80% on both safety and helpfulness metrics.","Despite safety guardrails, experiments on jailbreaking demonstrate the vulnerability of SLMs to adversarial perturbations and transfer attacks, with average attack success rates of 90% and 10% respectively when evaluated on a dataset of carefully designed harmful questions spanning 12 different toxic categories.","However, we demonstrate that our proposed countermeasures reduce the attack success significantly."],"url":"http://arxiv.org/abs/2405.08317v1","category":"cs.CL"}
{"created":"2024-05-14 04:27:16","title":"A Decoupling and Aggregating Framework for Joint Extraction of Entities and Relations","abstract":"Named Entity Recognition and Relation Extraction are two crucial and challenging subtasks in the field of Information Extraction. Despite the successes achieved by the traditional approaches, fundamental research questions remain open. First, most recent studies use parameter sharing for a single subtask or shared features for both two subtasks, ignoring their semantic differences. Second, information interaction mainly focuses on the two subtasks, leaving the fine-grained informtion interaction among the subtask-specific features of encoding subjects, relations, and objects unexplored. Motivated by the aforementioned limitations, we propose a novel model to jointly extract entities and relations. The main novelties are as follows: (1) We propose to decouple the feature encoding process into three parts, namely encoding subjects, encoding objects, and encoding relations. Thanks to this, we are able to use fine-grained subtask-specific features. (2) We propose novel inter-aggregation and intra-aggregation strategies to enhance the information interaction and construct individual fine-grained subtask-specific features, respectively. The experimental results demonstrate that our model outperforms several previous state-of-the-art models. Extensive additional experiments further confirm the effectiveness of our model.","sentences":["Named Entity Recognition and Relation Extraction are two crucial and challenging subtasks in the field of Information Extraction.","Despite the successes achieved by the traditional approaches, fundamental research questions remain open.","First, most recent studies use parameter sharing for a single subtask or shared features for both two subtasks, ignoring their semantic differences.","Second, information interaction mainly focuses on the two subtasks, leaving the fine-grained informtion interaction among the subtask-specific features of encoding subjects, relations, and objects unexplored.","Motivated by the aforementioned limitations, we propose a novel model to jointly extract entities and relations.","The main novelties are as follows: (1) We propose to decouple the feature encoding process into three parts, namely encoding subjects, encoding objects, and encoding relations.","Thanks to this, we are able to use fine-grained subtask-specific features.","(2) We propose novel inter-aggregation and intra-aggregation strategies to enhance the information interaction and construct individual fine-grained subtask-specific features, respectively.","The experimental results demonstrate that our model outperforms several previous state-of-the-art models.","Extensive additional experiments further confirm the effectiveness of our model."],"url":"http://arxiv.org/abs/2405.08311v1","category":"cs.CL"}
{"created":"2024-05-14 17:50:58","title":"A Mimicking Theorem for processes driven by fractional Brownian motion","abstract":"In this paper, we prove a mimicking theorem for stochastic processes with an additive Gaussian noise along with some entropy and transport type estimates. As an application of these results, we prove sharp quantitative propagation of chaos result and derive a formula for the marginal dynamics of collections of locally interacting stochastic differential equations with additive Gaussian noise.","sentences":["In this paper, we prove a mimicking theorem for stochastic processes with an additive Gaussian noise along with some entropy and transport type estimates.","As an application of these results, we prove sharp quantitative propagation of chaos result and derive a formula for the marginal dynamics of collections of locally interacting stochastic differential equations with additive Gaussian noise."],"url":"http://arxiv.org/abs/2405.08803v1","category":"math.PR"}
{"created":"2024-05-14 17:45:12","title":"The fundamental martingale with applications to Markov Random Fields","abstract":"We consider collections of SDEs indexed by a graph. Each SDE is driven by an additive Gaussian noise and each drift term interacts with all other SDEs within the graph neighbourhood. We derive the fundamental martingale for a class of Gaussian processes and use this to prove a Girsanov type theorem. Further, we use this to construct a clique factorisation to prove that the law of the interacting SDEs forms a 2-Markov Random Field.","sentences":["We consider collections of SDEs indexed by a graph.","Each SDE is driven by an additive Gaussian noise and each drift term interacts with all other SDEs within the graph neighbourhood.","We derive the fundamental martingale for a class of Gaussian processes and use this to prove a Girsanov type theorem.","Further, we use this to construct a clique factorisation to prove that the law of the interacting SDEs forms a 2-Markov Random Field."],"url":"http://arxiv.org/abs/2405.08795v1","category":"math.PR"}
{"created":"2024-05-14 16:48:46","title":"Optimal Sequential Procedure for Early Detection of Multiple Side Effects","abstract":"In this paper, we propose an optimal sequential procedure for the early detection of potential side effects resulting from the administration of some treatment (e.g. a vaccine, say). The results presented here extend previous results obtained in Wang and Boukai (2024) who study the single side effect case to the case of two (or more) side effects. While the sequential procedure we employ, simultaneously monitors several of the treatment's side effects, the $(\\alpha, \\beta)$-optimal test we propose does not require any information about the inter-correlation between these potential side effects. However, in all of the subsequent analyses, including the derivations of the exact expressions of the Average Sample Number (ASN), the Power function, and the properties of the post-test (or post-detection) estimators, we accounted specifically, for the correlation between the potential side effects. In the real-life application (such as post-marketing surveillance), the number of available observations is large enough to justify asymptotic analyses of the sequential procedure (testing and post-detection estimation) properties. Accordingly, we also derive the consistency and asymptotic normality of our post-test estimators; results which enable us to also provide (asymptotic, post-detection) confidence intervals for the probabilities of various side-effects. Moreover, to compare two specific side effects, their relative risk plays an important role. We derive the distribution of the estimated relative risk in the asymptotic framework to provide appropriate inference. To illustrate the theoretical results presented, we provide two detailed examples based on the data of side effects on COVID-19 vaccine collected in Nigeria (see Nigeria (see Ilori et al. (2022)).","sentences":["In this paper, we propose an optimal sequential procedure for the early detection of potential side effects resulting from the administration of some treatment (e.g. a vaccine, say).","The results presented here extend previous results obtained in Wang and Boukai (2024) who study the single side effect case to the case of two (or more) side effects.","While the sequential procedure we employ, simultaneously monitors several of the treatment's side effects, the $(\\alpha, \\beta)$-optimal test we propose does not require any information about the inter-correlation between these potential side effects.","However, in all of the subsequent analyses, including the derivations of the exact expressions of the Average Sample Number (ASN), the Power function, and the properties of the post-test (or post-detection) estimators, we accounted specifically, for the correlation between the potential side effects.","In the real-life application (such as post-marketing surveillance), the number of available observations is large enough to justify asymptotic analyses of the sequential procedure (testing and post-detection estimation) properties.","Accordingly, we also derive the consistency and asymptotic normality of our post-test estimators; results which enable us to also provide (asymptotic, post-detection) confidence intervals for the probabilities of various side-effects.","Moreover, to compare two specific side effects, their relative risk plays an important role.","We derive the distribution of the estimated relative risk in the asymptotic framework to provide appropriate inference.","To illustrate the theoretical results presented, we provide two detailed examples based on the data of side effects on COVID-19 vaccine collected in Nigeria (see Nigeria (see Ilori et al. (2022))."],"url":"http://arxiv.org/abs/2405.08759v1","category":"stat.ME"}
{"created":"2024-05-14 16:33:29","title":"Longitudinal Structure of Quark-Gluon Plasma Unveiled Through Nuclear Deformations","abstract":"The study of quark-gluon plasma (QGP) is hindered by our limited understanding of its initial conditions, particularly its longitudinal structure. We propose a novel approach that entails analyzing collisions involving nuclei of similar masses but different deformations. This strategy allows us to vary the initial conditions and collective expansion of the QGP, while minimizing the influence of non-flow correlations. Using a dynamical transport model, we have for the first time extracted the complete longitudinal structure of elliptic flow ($v_2$). Our findings reveal that although deformation significantly enhances the overall magnitude of $v_2$, it does not alter its longitudinal profile. This approach not only enables the separation of the rapidity dependence of flow from its rapidity decorrelations but also prompts further investigation into other nuclear structural features, such as nuclear skin thickness, to advance our understanding of the QGP's initial conditions.","sentences":["The study of quark-gluon plasma (QGP) is hindered by our limited understanding of its initial conditions, particularly its longitudinal structure.","We propose a novel approach that entails analyzing collisions involving nuclei of similar masses but different deformations.","This strategy allows us to vary the initial conditions and collective expansion of the QGP, while minimizing the influence of non-flow correlations.","Using a dynamical transport model, we have for the first time extracted the complete longitudinal structure of elliptic flow ($v_2$).","Our findings reveal that although deformation significantly enhances the overall magnitude of $v_2$, it does not alter its longitudinal profile.","This approach not only enables the separation of the rapidity dependence of flow from its rapidity decorrelations but also prompts further investigation into other nuclear structural features, such as nuclear skin thickness, to advance our understanding of the QGP's initial conditions."],"url":"http://arxiv.org/abs/2405.08749v1","category":"nucl-th"}
{"created":"2024-05-14 16:09:52","title":"Revisiting Reactor Anti-Neutrino 5 MeV Bump with $^{13}$C Neutral-Current Interaction","abstract":"For the first time, we systematically investigate the potential of neutrino-nucleus neutral current interactions with $^{13}$C to identify the origin of the 5 MeV bump observed in reactor anti-neutrino spectra in the inverse beta decay process. The distinctive signal is obtained from the de-excitation of $^{13}$C$^*$ into the ground state emitting a 3.685 MeV photon in various liquid scintillator detectors. Such an interaction predominantly occurs for the reactor anti-neutrinos within the energy range coinciding with the 5 MeV bump. For a detector that has a capability of 95\\% level photon and electron separation and small thorium contamination below $5 \\times 10^{-17}$ gr/gr located in a site with an overburden of about a few hundred m.w.e, such as the location of near detectors of RENO and Daya Bay will have a great sensitivity to resolve the 5 MeV bump. In addition, we propose a novel approach to track the time evolution of reactor isotopes by analyzing our $^{13}$C signal shedding light on the contributions from $^{235}$U or $^{239}$Pu to the observed bump. This provides an extra powerful tool in both discriminating the flux models and testing any new physics possibilities for the 5 MeV bump at 3$\\sigma$ to 5$\\sigma$ level with much less systematic uncertainties and assuming 10 kt.year of data collection. Our detector requirements are realistic, aligning well with recent studies conducted for existing or forthcoming experiments.","sentences":["For the first time, we systematically investigate the potential of neutrino-nucleus neutral current interactions with $^{13}$C to identify the origin of the 5 MeV bump observed in reactor anti-neutrino spectra in the inverse beta decay process.","The distinctive signal is obtained from the de-excitation of $^{13}$C$^*$ into the ground state emitting a 3.685 MeV photon in various liquid scintillator detectors.","Such an interaction predominantly occurs for the reactor anti-neutrinos within the energy range coinciding with the 5 MeV bump.","For a detector that has a capability of 95\\% level photon and electron separation and small thorium contamination below $5 \\times 10^{-17}$ gr/gr located in a site with an overburden of about a few hundred m.w.e, such as the location of near detectors of RENO and Daya Bay will have a great sensitivity to resolve the 5 MeV bump.","In addition, we propose a novel approach to track the time evolution of reactor isotopes by analyzing our $^{13}$C signal shedding light on the contributions from $^{235}$U or $^{239}$Pu to the observed bump.","This provides an extra powerful tool in both discriminating the flux models and testing any new physics possibilities for the 5 MeV bump at 3$\\sigma$ to 5$\\sigma$ level with much less systematic uncertainties and assuming 10 kt.year of data collection.","Our detector requirements are realistic, aligning well with recent studies conducted for existing or forthcoming experiments."],"url":"http://arxiv.org/abs/2405.08724v1","category":"hep-ph"}
{"created":"2024-05-14 13:56:12","title":"GN-SINDy: Greedy Sampling Neural Network in Sparse Identification of Nonlinear Partial Differential Equations","abstract":"The sparse identification of nonlinear dynamical systems (SINDy) is a data-driven technique employed for uncovering and representing the fundamental dynamics of intricate systems based on observational data. However, a primary obstacle in the discovery of models for nonlinear partial differential equations (PDEs) lies in addressing the challenges posed by the curse of dimensionality and large datasets. Consequently, the strategic selection of the most informative samples within a given dataset plays a crucial role in reducing computational costs and enhancing the effectiveness of SINDy-based algorithms. To this aim, we employ a greedy sampling approach to the snapshot matrix of a PDE to obtain its valuable samples, which are suitable to train a deep neural network (DNN) in a SINDy framework. SINDy based algorithms often consist of a data collection unit, constructing a dictionary of basis functions, computing the time derivative, and solving a sparse identification problem which ends to regularised least squares minimization. In this paper, we extend the results of a SINDy based deep learning model discovery (DeePyMoD) approach by integrating greedy sampling technique in its data collection unit and new sparsity promoting algorithms in the least squares minimization unit. In this regard we introduce the greedy sampling neural network in sparse identification of nonlinear partial differential equations (GN-SINDy) which blends a greedy sampling method, the DNN, and the SINDy algorithm. In the implementation phase, to show the effectiveness of GN-SINDy, we compare its results with DeePyMoD by using a Python package that is prepared for this purpose on numerous PDE discovery","sentences":["The sparse identification of nonlinear dynamical systems (SINDy) is a data-driven technique employed for uncovering and representing the fundamental dynamics of intricate systems based on observational data.","However, a primary obstacle in the discovery of models for nonlinear partial differential equations (PDEs) lies in addressing the challenges posed by the curse of dimensionality and large datasets.","Consequently, the strategic selection of the most informative samples within a given dataset plays a crucial role in reducing computational costs and enhancing the effectiveness of SINDy-based algorithms.","To this aim, we employ a greedy sampling approach to the snapshot matrix of a PDE to obtain its valuable samples, which are suitable to train a deep neural network (DNN) in a SINDy framework.","SINDy based algorithms often consist of a data collection unit, constructing a dictionary of basis functions, computing the time derivative, and solving a sparse identification problem which ends to regularised least squares minimization.","In this paper, we extend the results of a SINDy based deep learning model discovery (DeePyMoD) approach by integrating greedy sampling technique in its data collection unit and new sparsity promoting algorithms in the least squares minimization unit.","In this regard we introduce the greedy sampling neural network in sparse identification of nonlinear partial differential equations (GN-SINDy) which blends a greedy sampling method, the DNN, and the SINDy algorithm.","In the implementation phase, to show the effectiveness of GN-SINDy, we compare its results with DeePyMoD by using a Python package that is prepared for this purpose on numerous PDE discovery"],"url":"http://arxiv.org/abs/2405.08613v1","category":"math.DS"}
{"created":"2024-05-14 12:16:32","title":"The TDHF code Sky3D version 1.2","abstract":"The Sky3D code has been widely used to describe nuclear ground states, collective vibrational excitations, and heavy-ion collisions. The approach is based on Skyrme forces or related energy density functionals. The static and dynamic equations are solved on a three-dimensional grid, and pairing is been implemented in the BCS approximation. This updated version of the code aims to facilitate the calculation of nuclear strength functions in the regime of linear response theory, while retaining all existing functionality and use cases. The strength functions are benchmarked against available RPA codes, and the user has the freedom of choice when selecting the nature of external excitation (from monopole to hexadecapole and more). Some utility programs are also provided that calculate the strength function from the time-dependent output of the dynamic calculations of the Sky3D code.","sentences":["The Sky3D code has been widely used to describe nuclear ground states, collective vibrational excitations, and heavy-ion collisions.","The approach is based on Skyrme forces or related energy density functionals.","The static and dynamic equations are solved on a three-dimensional grid, and pairing is been implemented in the BCS approximation.","This updated version of the code aims to facilitate the calculation of nuclear strength functions in the regime of linear response theory, while retaining all existing functionality and use cases.","The strength functions are benchmarked against available RPA codes, and the user has the freedom of choice when selecting the nature of external excitation (from monopole to hexadecapole and more).","Some utility programs are also provided that calculate the strength function from the time-dependent output of the dynamic calculations of the Sky3D code."],"url":"http://arxiv.org/abs/2405.08531v1","category":"physics.comp-ph"}
{"created":"2024-05-14 11:37:29","title":"Precarious Experiences: Citizens' Frustrations, Anxieties and Burdens of an Online Welfare Benefit System","abstract":"There is a significant overlap between people who are supported by income-related social welfare benefits, often in precarious situations, and those who experience greater digital exclusion. We report on a study of claimants using the UK's Universal Credit online welfare benefit system designed as, and still, \"digital by default\". Through data collection involving remote interviews (n=11) and online surveys (n=66), we expose claimants' own lived experiences interacting with this system. The claimants explain how digital channels can contribute to an imbalance of power and agency, at a time when their own circumstances mean they have reduced abilities, resources and capacities, and where design choices can adversely affect people's utility to leverage help from their own wider socio-technical ecosystems. We contribute eight recommendations from these accounts to inform the future design and development of digital welfare benefit systems for this population, to reduce digital barriers and harms.","sentences":["There is a significant overlap between people who are supported by income-related social welfare benefits, often in precarious situations, and those who experience greater digital exclusion.","We report on a study of claimants using the UK's Universal Credit online welfare benefit system designed as, and still, \"digital by default\".","Through data collection involving remote interviews (n=11) and online surveys (n=66), we expose claimants' own lived experiences interacting with this system.","The claimants explain how digital channels can contribute to an imbalance of power and agency, at a time when their own circumstances mean they have reduced abilities, resources and capacities, and where design choices can adversely affect people's utility to leverage help from their own wider socio-technical ecosystems.","We contribute eight recommendations from these accounts to inform the future design and development of digital welfare benefit systems for this population, to reduce digital barriers and harms."],"url":"http://arxiv.org/abs/2405.08515v1","category":"cs.HC"}
{"created":"2024-05-14 07:10:03","title":"Microscopic investigation of wobbling motion in atomic nuclei using the triaxial projected shell model approach","abstract":"A systematic investigation of the wobbling band structures observed in odd-mass nuclei of $^{161,163,165,167}$Lu, $^{167}$Ta $^{131}$Cs, $^{135}$Pr, $^{151}$Eu, $^{183}$Au, $^{133}$Ba, $^{105}$Pd, $^{133}$La, $^{187}$Au and $^{127}$Xe is performed using the triaxial projected shell model (TPSM) approach. It is demonstrated that all the studied band structures have transverse wobbling mode, except for $^{133}$La, $^{187}$Au (negative parity), $^{183}$Au (positive parity) and $^{127}$Xe nuclei where the wobbling frequency increases with spin, indicating that the collective motion has a longitudinal character. To elucidate further the wobbling nature of the band structures, electromagnetic transition probabilities have been evaluated and it is observed that inter-band transitions are dominated by $E2$ rather than $M1$ as expected for a typical signature partner band. It is shown that TPSM approach provides a reasonable description of all the measured properties of the studied nuclei.","sentences":["A systematic investigation of the wobbling band structures observed in odd-mass nuclei of $^{161,163,165,167}$Lu, $^{167}$Ta $^{131}$Cs, $^{135}$Pr, $^{151}$Eu, $^{183}$Au, $^{133}$Ba, $^{105}$Pd, $^{133}$La, $^{187}$Au and $^{127}$Xe is performed using the triaxial projected shell model (TPSM) approach.","It is demonstrated that all the studied band structures have transverse wobbling mode, except for $^{133}$La, $^{187}$Au (negative parity), $^{183}$Au (positive parity) and $^{127}$Xe nuclei where the wobbling frequency increases with spin, indicating that the collective motion has a longitudinal character.","To elucidate further the wobbling nature of the band structures, electromagnetic transition probabilities have been evaluated and it is observed that inter-band transitions are dominated by $E2$ rather than $M1$ as expected for a typical signature partner band.","It is shown that TPSM approach provides a reasonable description of all the measured properties of the studied nuclei."],"url":"http://arxiv.org/abs/2405.08368v1","category":"nucl-th"}
{"created":"2024-05-14 06:55:16","title":"GPS-IDS: An Anomaly-based GPS Spoofing Attack Detection Framework for Autonomous Vehicles","abstract":"Autonomous Vehicles (AVs) heavily rely on sensors and communication networks like Global Positioning System (GPS) to navigate autonomously. Prior research has indicated that networks like GPS are vulnerable to cyber-attacks such as spoofing and jamming, thus posing serious risks like navigation errors and system failures. These threats are expected to intensify with the widespread deployment of AVs, making it crucial to detect and mitigate such attacks. This paper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly Behavior Analysis (ABA)-based intrusion detection framework to detect GPS spoofing attacks on AVs. The framework uses a novel physics-based vehicle behavior model where a GPS navigation model is integrated into the conventional dynamic bicycle model for accurate AV behavior representation. Temporal features derived from this behavior model are analyzed using machine learning to detect normal and abnormal navigation behavior. The performance of the GPS-IDS framework is evaluated on the AV-GPS-Dataset - a real-world dataset collected by the team using an AV testbed. The dataset has been publicly released for the global research community. To the best of our knowledge, this dataset is the first of its kind and will serve as a useful resource to address such security challenges.","sentences":["Autonomous Vehicles (AVs) heavily rely on sensors and communication networks like Global Positioning System (GPS) to navigate autonomously.","Prior research has indicated that networks like GPS are vulnerable to cyber-attacks such as spoofing and jamming, thus posing serious risks like navigation errors and system failures.","These threats are expected to intensify with the widespread deployment of AVs, making it crucial to detect and mitigate such attacks.","This paper proposes GPS Intrusion Detection System, or GPS-IDS, an Anomaly Behavior Analysis (ABA)-based intrusion detection framework to detect GPS spoofing attacks on AVs.","The framework uses a novel physics-based vehicle behavior model where a GPS navigation model is integrated into the conventional dynamic bicycle model for accurate AV behavior representation.","Temporal features derived from this behavior model are analyzed using machine learning to detect normal and abnormal navigation behavior.","The performance of the GPS-IDS framework is evaluated on the AV-GPS-Dataset - a real-world dataset collected by the team using an AV testbed.","The dataset has been publicly released for the global research community.","To the best of our knowledge, this dataset is the first of its kind and will serve as a useful resource to address such security challenges."],"url":"http://arxiv.org/abs/2405.08359v1","category":"cs.CR"}
{"created":"2024-05-14 04:23:44","title":"Cross-Category Functional Grasp Tansfer","abstract":"The grasp generation of dexterous hand often requires a large number of grasping annotations. Especially for functional grasp-requiring the grasp pose to be convenient for the subsequent use of the object. However, annotating high DoF dexterous hand pose is rather challenging. This prompt us to explore how people achieve manipulations on new objects based on past grasp experiences. We find that people are adept at discovering and leveraging various similarities between objects when grasping new items, including shape, layout, and grasp type. In light of this, we analyze and collect grasp-related similarity relationships among 51 common tool-like object categories and annotate semantic grasp representation for 1768 objects. These data are organized into the form of a knowledge graph, which helps infer our proposed cross-category functional grasp synthesis. Through extensive experiments, we demonstrate that the grasp-related knowledge indeed contributed to achieving functional grasp transfer across unknown or entirely new categories of objects. We will publicly release the dataset and code to facilitate future research.","sentences":["The grasp generation of dexterous hand often requires a large number of grasping annotations.","Especially for functional grasp-requiring the grasp pose to be convenient for the subsequent use of the object.","However, annotating high DoF dexterous hand pose is rather challenging.","This prompt us to explore how people achieve manipulations on new objects based on past grasp experiences.","We find that people are adept at discovering and leveraging various similarities between objects when grasping new items, including shape, layout, and grasp type.","In light of this, we analyze and collect grasp-related similarity relationships among 51 common tool-like object categories and annotate semantic grasp representation for 1768 objects.","These data are organized into the form of a knowledge graph, which helps infer our proposed cross-category functional grasp synthesis.","Through extensive experiments, we demonstrate that the grasp-related knowledge indeed contributed to achieving functional grasp transfer across unknown or entirely new categories of objects.","We will publicly release the dataset and code to facilitate future research."],"url":"http://arxiv.org/abs/2405.08310v1","category":"cs.RO"}
{"created":"2024-05-14 03:58:19","title":"Computational Thought Experiments for a More Rigorous Philosophy and Science of the Mind","abstract":"We offer philosophical motivations for a method we call Virtual World Cognitive Science (VW CogSci), in which researchers use virtual embodied agents that are embedded in virtual worlds to explore questions in the field of Cognitive Science. We focus on questions about mental and linguistic representation and the ways that such computational modeling can add rigor to philosophical thought experiments, as well as the terminology used in the scientific study of such representations. We find that this method forces researchers to take a god's-eye view when describing dynamical relationships between entities in minds and entities in an environment in a way that eliminates the need for problematic talk of belief and concept types, such as the belief that cats are silly, and the concept CAT, while preserving belief and concept tokens in individual cognizers' minds. We conclude with some further key advantages of VW CogSci for the scientific study of mental and linguistic representation and for Cognitive Science more broadly.","sentences":["We offer philosophical motivations for a method we call Virtual World Cognitive Science (VW CogSci), in which researchers use virtual embodied agents that are embedded in virtual worlds to explore questions in the field of Cognitive Science.","We focus on questions about mental and linguistic representation and the ways that such computational modeling can add rigor to philosophical thought experiments, as well as the terminology used in the scientific study of such representations.","We find that this method forces researchers to take a god's-eye view when describing dynamical relationships between entities in minds and entities in an environment in a way that eliminates the need for problematic talk of belief and concept types, such as the belief that cats are silly, and the concept CAT, while preserving belief and concept tokens in individual cognizers' minds.","We conclude with some further key advantages of VW CogSci for the scientific study of mental and linguistic representation and for Cognitive Science more broadly."],"url":"http://arxiv.org/abs/2405.08304v2","category":"cs.CL"}
{"created":"2024-05-14 03:53:52","title":"Designing Adaptive User Interfaces for mHealth applications targeting chronic disease: A User-Centric Approach","abstract":"mHealth interventions show significant potential to help in the self-management of chronic diseases, but their under use remains a problem. Considering the substantial diversity among individuals dealing with chronic diseases, tailored strategies are essential. \\emph{Adaptive User Interfaces} (AUIs) may help address the diverse and evolving needs of this demographic. To investigate this approach, we developed an AUI prototype informed by existing literature findings. We then used this prototype as the basis for focus group discussions and interview studies with 22 participants managing various chronic diseases, and follow-up surveys of all participants. Through these investigations, we pinpointed key challenges related to the use of AUIs, strategies to improve adaptation design, and potential trade-offs between these challenges and strategies. Concurrently, a quantitative survey was conducted to extract preferences for AUIs in chronic disease-related applications with 90 further participants. This uncovered participants' preferences for various adaptations, data types, collection methods, and involvement levels. Finally, we synthesised these insights and categories, aligning them with existing guidelines and design considerations for mHealth app adaptation design. This resulted in nine guidelines that we refined by a final feedback survey conducted with 20 participants.","sentences":["mHealth interventions show significant potential to help in the self-management of chronic diseases, but their under use remains a problem.","Considering the substantial diversity among individuals dealing with chronic diseases, tailored strategies are essential.","\\emph{Adaptive User Interfaces} (AUIs) may help address the diverse and evolving needs of this demographic.","To investigate this approach, we developed an AUI prototype informed by existing literature findings.","We then used this prototype as the basis for focus group discussions and interview studies with 22 participants managing various chronic diseases, and follow-up surveys of all participants.","Through these investigations, we pinpointed key challenges related to the use of AUIs, strategies to improve adaptation design, and potential trade-offs between these challenges and strategies.","Concurrently, a quantitative survey was conducted to extract preferences for AUIs in chronic disease-related applications with 90 further participants.","This uncovered participants' preferences for various adaptations, data types, collection methods, and involvement levels.","Finally, we synthesised these insights and categories, aligning them with existing guidelines and design considerations for mHealth app adaptation design.","This resulted in nine guidelines that we refined by a final feedback survey conducted with 20 participants."],"url":"http://arxiv.org/abs/2405.08302v1","category":"cs.HC"}
{"created":"2024-05-14 03:42:33","title":"Distance-Restricted Explanations: Theoretical Underpinnings & Efficient Implementation","abstract":"The uses of machine learning (ML) have snowballed in recent years. In many cases, ML models are highly complex, and their operation is beyond the understanding of human decision-makers. Nevertheless, some uses of ML models involve high-stakes and safety-critical applications. Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding the operation of such complex ML models, thus eliciting trust in their operation. Unfortunately, the majority of past XAI work is based on informal approaches, that offer no guarantees of rigor. Unsurprisingly, there exists comprehensive experimental and theoretical evidence confirming that informal methods of XAI can provide human-decision makers with erroneous information. Logic-based XAI represents a rigorous approach to explainability; it is model-based and offers the strongest guarantees of rigor of computed explanations. However, a well-known drawback of logic-based XAI is the complexity of logic reasoning, especially for highly complex ML models. Recent work proposed distance-restricted explanations, i.e. explanations that are rigorous provided the distance to a given input is small enough. Distance-restricted explainability is tightly related with adversarial robustness, and it has been shown to scale for moderately complex ML models, but the number of inputs still represents a key limiting factor. This paper investigates novel algorithms for scaling up the performance of logic-based explainers when computing and enumerating ML model explanations with a large number of inputs.","sentences":["The uses of machine learning (ML) have snowballed in recent years.","In many cases, ML models are highly complex, and their operation is beyond the understanding of human decision-makers.","Nevertheless, some uses of ML models involve high-stakes and safety-critical applications.","Explainable artificial intelligence (XAI) aims to help human decision-makers in understanding the operation of such complex ML models, thus eliciting trust in their operation.","Unfortunately, the majority of past XAI work is based on informal approaches, that offer no guarantees of rigor.","Unsurprisingly, there exists comprehensive experimental and theoretical evidence confirming that informal methods of XAI can provide human-decision makers with erroneous information.","Logic-based XAI represents a rigorous approach to explainability; it is model-based and offers the strongest guarantees of rigor of computed explanations.","However, a well-known drawback of logic-based XAI is the complexity of logic reasoning, especially for highly complex ML models.","Recent work proposed distance-restricted explanations, i.e. explanations that are rigorous provided the distance to a given input is small enough.","Distance-restricted explainability is tightly related with adversarial robustness, and it has been shown to scale for moderately complex ML models, but the number of inputs still represents a key limiting factor.","This paper investigates novel algorithms for scaling up the performance of logic-based explainers when computing and enumerating ML model explanations with a large number of inputs."],"url":"http://arxiv.org/abs/2405.08297v1","category":"cs.LG"}
{"created":"2024-05-14 02:34:56","title":"Automatic Segmentation of the Kidneys and Cystic Renal Lesions on Non-Contrast CT Using a Convolutional Neural Network","abstract":"Objective: Automated segmentation tools are useful for calculating kidney volumes rapidly and accurately. Furthermore, these tools have the power to facilitate large-scale image-based artificial intelligence projects by generating input labels, such as for image registration algorithms. Prior automated segmentation models have largely ignored non-contrast computed tomography (CT) imaging. This work aims to implement and train a deep learning (DL) model to segment the kidneys and cystic renal lesions (CRLs) from non-contrast CT scans.   Methods: Manual segmentation of the kidneys and CRLs was performed on 150 non-contrast abdominal CT scans. The data were divided into an 80/20 train/test split and a deep learning (DL) model was trained to segment the kidneys and CRLs. Various scoring metrics were used to assess model performance, including the Dice Similarity Coefficient (DSC), Jaccard Index (JI), and absolute and percent error kidney volume and lesion volume. Bland-Altman (B-A) analysis was performed to compare manual versus DL-based kidney volumes.   Results: The DL model achieved a median kidney DSC of 0.934, median CRL DSC of 0.711, and total median study DSC of 0.823. Average volume errors were 0.9% for renal parenchyma, 37.0% for CRLs, and 2.2% overall. B-A analysis demonstrated that DL-based volumes tended to be greater than manual volumes, with a mean bias of +3.0 ml (+/- 2 SD of +/- 50.2 ml).   Conclusion: A deep learning model trained to segment kidneys and cystic renal lesions on non-contrast CT examinations was able to provide highly accurate segmentations, with a median kidney Dice Similarity Coefficient of 0.934.   Keywords: deep learning; kidney segmentation; artificial intelligence; convolutional neural networks.","sentences":["Objective: Automated segmentation tools are useful for calculating kidney volumes rapidly and accurately.","Furthermore, these tools have the power to facilitate large-scale image-based artificial intelligence projects by generating input labels, such as for image registration algorithms.","Prior automated segmentation models have largely ignored non-contrast computed tomography (CT) imaging.","This work aims to implement and train a deep learning (DL) model to segment the kidneys and cystic renal lesions (CRLs) from non-contrast CT scans.   ","Methods: Manual segmentation of the kidneys and CRLs was performed on 150 non-contrast abdominal CT scans.","The data were divided into an 80/20 train/test split and a deep learning (DL) model was trained to segment the kidneys and CRLs.","Various scoring metrics were used to assess model performance, including the Dice Similarity Coefficient (DSC), Jaccard Index (JI), and absolute and percent error kidney volume and lesion volume.","Bland-Altman (B-A) analysis was performed to compare manual versus DL-based kidney volumes.   ","Results:","The DL model achieved a median kidney DSC of 0.934, median CRL DSC of 0.711, and total median study DSC of 0.823.","Average volume errors were 0.9% for renal parenchyma, 37.0% for CRLs, and 2.2% overall.","B-A analysis demonstrated that DL-based volumes tended to be greater than manual volumes, with a mean bias of +3.0 ml (+/- 2 SD of +/- 50.2 ml).   ","Conclusion: A deep learning model trained to segment kidneys and cystic renal lesions on non-contrast CT examinations was able to provide highly accurate segmentations, with a median kidney Dice Similarity Coefficient of 0.934.   ","Keywords: deep learning; kidney segmentation; artificial intelligence; convolutional neural networks."],"url":"http://arxiv.org/abs/2405.08282v1","category":"eess.IV"}
{"created":"2024-05-14 02:05:36","title":"VS-Assistant: Versatile Surgery Assistant on the Demand of Surgeons","abstract":"The surgical intervention is crucial to patient healthcare, and many studies have developed advanced algorithms to provide understanding and decision-making assistance for surgeons. Despite great progress, these algorithms are developed for a single specific task and scenario, and in practice require the manual combination of different functions, thus limiting the applicability. Thus, an intelligent and versatile surgical assistant is expected to accurately understand the surgeon's intentions and accordingly conduct the specific tasks to support the surgical process. In this work, by leveraging advanced multimodal large language models (MLLMs), we propose a Versatile Surgery Assistant (VS-Assistant) that can accurately understand the surgeon's intention and complete a series of surgical understanding tasks, e.g., surgical scene analysis, surgical instrument detection, and segmentation on demand. Specifically, to achieve superior surgical multimodal understanding, we devise a mixture of projectors (MOP) module to align the surgical MLLM in VS-Assistant to balance the natural and surgical knowledge. Moreover, we devise a surgical Function-Calling Tuning strategy to enable the VS-Assistant to understand surgical intentions, and thus make a series of surgical function calls on demand to meet the needs of the surgeons. Extensive experiments on neurosurgery data confirm that our VS-Assistant can understand the surgeon's intention more accurately than the existing MLLM, resulting in overwhelming performance in textual analysis and visual tasks. Source code and models will be made public.","sentences":["The surgical intervention is crucial to patient healthcare, and many studies have developed advanced algorithms to provide understanding and decision-making assistance for surgeons.","Despite great progress, these algorithms are developed for a single specific task and scenario, and in practice require the manual combination of different functions, thus limiting the applicability.","Thus, an intelligent and versatile surgical assistant is expected to accurately understand the surgeon's intentions and accordingly conduct the specific tasks to support the surgical process.","In this work, by leveraging advanced multimodal large language models (MLLMs), we propose a Versatile Surgery Assistant (VS-Assistant) that can accurately understand the surgeon's intention and complete a series of surgical understanding tasks, e.g., surgical scene analysis, surgical instrument detection, and segmentation on demand.","Specifically, to achieve superior surgical multimodal understanding, we devise a mixture of projectors (MOP) module to align the surgical MLLM in VS-Assistant to balance the natural and surgical knowledge.","Moreover, we devise a surgical Function-Calling Tuning strategy to enable the VS-Assistant to understand surgical intentions, and thus make a series of surgical function calls on demand to meet the needs of the surgeons.","Extensive experiments on neurosurgery data confirm that our VS-Assistant can understand the surgeon's intention more accurately than the existing MLLM, resulting in overwhelming performance in textual analysis and visual tasks.","Source code and models will be made public."],"url":"http://arxiv.org/abs/2405.08272v1","category":"cs.CV"}
{"created":"2024-05-14 00:57:02","title":"Smart Sampling: Self-Attention and Bootstrapping for Improved Ensembled Q-Learning","abstract":"We present a novel method aimed at enhancing the sample efficiency of ensemble Q learning. Our proposed approach integrates multi-head self-attention into the ensembled Q networks while bootstrapping the state-action pairs ingested by the ensemble. This not only results in performance improvements over the original REDQ (Chen et al. 2021) and its variant DroQ (Hi-raoka et al. 2022), thereby enhancing Q predictions, but also effectively reduces both the average normalized bias and standard deviation of normalized bias within Q-function ensembles. Importantly, our method also performs well even in scenarios with a low update-to-data (UTD) ratio. Notably, the implementation of our proposed method is straightforward, requiring minimal modifications to the base model.","sentences":["We present a novel method aimed at enhancing the sample efficiency of ensemble Q learning.","Our proposed approach integrates multi-head self-attention into the ensembled Q networks while bootstrapping the state-action pairs ingested by the ensemble.","This not only results in performance improvements over the original REDQ (Chen et al. 2021) and its variant DroQ (Hi-raoka et al. 2022), thereby enhancing Q predictions, but also effectively reduces both the average normalized bias and standard deviation of normalized bias within Q-function ensembles.","Importantly, our method also performs well even in scenarios with a low update-to-data (UTD) ratio.","Notably, the implementation of our proposed method is straightforward, requiring minimal modifications to the base model."],"url":"http://arxiv.org/abs/2405.08252v1","category":"cs.LG"}
{"created":"2024-05-14 00:41:16","title":"QLingNet: An efficient and flexible modeling framework for subsonic airfoils","abstract":"Artificial intelligence techniques are considered an effective means to accelerate flow field simulations. However, current deep learning methods struggle to achieve generalization to flow field resolutions while ensuring computational efficiency. This paper presents a deep learning approach for rapid prediction of two types of subsonic flow fields with different resolutions. Unlike convolutional neural networks, the constructed feature extractor integrates features of different spatial scales along the channel dimension, reducing the sensitivity of the deep learning model to resolution while improving computational efficiency. Additionally, to ensure consistency between the input and output resolutions of the deep learning model, a memory pooling strategy is proposed, which ensures accurate reconstruction of flow fields at any resolution. By conducting extensive qualitative and quantitative analyses on a given test dataset, it is demonstrated that the proposed deep learning model can achieve a three-order-of-magnitude speedup compared to CPU-based solvers while adapting to flow fields of arbitrary resolutions. Moreover, the prediction accuracy for pressure exceeds 99\\%, laying the foundation for the development of large-scale models in the field of aerodynamics.","sentences":["Artificial intelligence techniques are considered an effective means to accelerate flow field simulations.","However, current deep learning methods struggle to achieve generalization to flow field resolutions while ensuring computational efficiency.","This paper presents a deep learning approach for rapid prediction of two types of subsonic flow fields with different resolutions.","Unlike convolutional neural networks, the constructed feature extractor integrates features of different spatial scales along the channel dimension, reducing the sensitivity of the deep learning model to resolution while improving computational efficiency.","Additionally, to ensure consistency between the input and output resolutions of the deep learning model, a memory pooling strategy is proposed, which ensures accurate reconstruction of flow fields at any resolution.","By conducting extensive qualitative and quantitative analyses on a given test dataset, it is demonstrated that the proposed deep learning model can achieve a three-order-of-magnitude speedup compared to CPU-based solvers while adapting to flow fields of arbitrary resolutions.","Moreover, the prediction accuracy for pressure exceeds 99\\%, laying the foundation for the development of large-scale models in the field of aerodynamics."],"url":"http://arxiv.org/abs/2405.08248v1","category":"physics.flu-dyn"}
{"created":"2024-05-14 00:39:21","title":"Automated classification of multi-parametric body MRI series","abstract":"Multi-parametric MRI (mpMRI) studies are widely available in clinical practice for the diagnosis of various diseases. As the volume of mpMRI exams increases yearly, there are concomitant inaccuracies that exist within the DICOM header fields of these exams. This precludes the use of the header information for the arrangement of the different series as part of the radiologist's hanging protocol, and clinician oversight is needed for correction. In this pilot work, we propose an automated framework to classify the type of 8 different series in mpMRI studies. We used 1,363 studies acquired by three Siemens scanners to train a DenseNet-121 model with 5-fold cross-validation. Then, we evaluated the performance of the DenseNet-121 ensemble on a held-out test set of 313 mpMRI studies. Our method achieved an average precision of 96.6%, sensitivity of 96.6%, specificity of 99.6%, and F1 score of 96.6% for the MRI series classification task. To the best of our knowledge, we are the first to develop a method to classify the series type in mpMRI studies acquired at the level of the chest, abdomen, and pelvis. Our method has the capability for robust automation of hanging protocols in modern radiology practice.","sentences":["Multi-parametric MRI (mpMRI) studies are widely available in clinical practice for the diagnosis of various diseases.","As the volume of mpMRI exams increases yearly, there are concomitant inaccuracies that exist within the DICOM header fields of these exams.","This precludes the use of the header information for the arrangement of the different series as part of the radiologist's hanging protocol, and clinician oversight is needed for correction.","In this pilot work, we propose an automated framework to classify the type of 8 different series in mpMRI studies.","We used 1,363 studies acquired by three Siemens scanners to train a DenseNet-121 model with 5-fold cross-validation.","Then, we evaluated the performance of the DenseNet-121 ensemble on a held-out test set of 313 mpMRI studies.","Our method achieved an average precision of 96.6%, sensitivity of 96.6%, specificity of 99.6%, and F1 score of 96.6% for the MRI series classification task.","To the best of our knowledge, we are the first to develop a method to classify the series type in mpMRI studies acquired at the level of the chest, abdomen, and pelvis.","Our method has the capability for robust automation of hanging protocols in modern radiology practice."],"url":"http://arxiv.org/abs/2405.08247v1","category":"eess.IV"}
{"created":"2024-05-14 00:22:06","title":"Compositional Text-to-Image Generation with Dense Blob Representations","abstract":"Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding inputs for better controllability. In this work, we propose to decompose a scene into visual primitives - denoted as dense blob representations - that contain fine-grained details of the scene while being modular, human-interpretable, and easy-to-construct. Based on blob representations, we develop a blob-grounded text-to-image diffusion model, termed BlobGEN, for compositional generation. Particularly, we introduce a new masked cross-attention module to disentangle the fusion between blob representations and visual features. To leverage the compositionality of large language models (LLMs), we introduce a new in-context learning approach to generate blob representations from text prompts. Our extensive experiments show that BlobGEN achieves superior zero-shot generation quality and better layout-guided controllability on MS-COCO. When augmented by LLMs, our method exhibits superior numerical and spatial correctness on compositional image generation benchmarks. Project page: https://blobgen-2d.github.io.","sentences":["Existing text-to-image models struggle to follow complex text prompts, raising the need for extra grounding inputs for better controllability.","In this work, we propose to decompose a scene into visual primitives - denoted as dense blob representations - that contain fine-grained details of the scene while being modular, human-interpretable, and easy-to-construct.","Based on blob representations, we develop a blob-grounded text-to-image diffusion model, termed BlobGEN, for compositional generation.","Particularly, we introduce a new masked cross-attention module to disentangle the fusion between blob representations and visual features.","To leverage the compositionality of large language models (LLMs), we introduce a new in-context learning approach to generate blob representations from text prompts.","Our extensive experiments show that BlobGEN achieves superior zero-shot generation quality and better layout-guided controllability on MS-COCO.","When augmented by LLMs, our method exhibits superior numerical and spatial correctness on compositional image generation benchmarks.","Project page: https://blobgen-2d.github.io."],"url":"http://arxiv.org/abs/2405.08246v1","category":"cs.CV"}
{"created":"2024-05-14 00:20:32","title":"Progressive enhancement and restoration for mural images under low-light and defected conditions based on multi-receptive field strategy","abstract":"Ancient murals are valuable cultural heritage with great archaeological value. They provide insights into ancient religions, ceremonies, folklore, among other things through their content. However, due to long-term oxidation and inadequate protection, ancient murals have suffered continuous damage, including peeling and mold etc. Additionally, since ancient murals were typically painted indoors, the light intensity in images captured by digital devices is often low. The poor visibility hampers the further restoration of damaged areas. To address the escalating damage to ancient frescoes and facilitate batch restoration at archaeological sites, we propose a two-stage restoration model which called MER(Mural Enhancement and Restoration net) for ancient murals that are damaged and have been captured in low light. Our two-stage model not only enhances the visual quality of restored images but also achieves commendable results in relevant metric evaluations compared with other competitors. Furthermore, we have launched a website dedicated to the restoration of ancient mural paintings, utilizing the proposed model. Code is available at https://gitee.com/bbfan2024/MER.git.","sentences":["Ancient murals are valuable cultural heritage with great archaeological value.","They provide insights into ancient religions, ceremonies, folklore, among other things through their content.","However, due to long-term oxidation and inadequate protection, ancient murals have suffered continuous damage, including peeling and mold etc.","Additionally, since ancient murals were typically painted indoors, the light intensity in images captured by digital devices is often low.","The poor visibility hampers the further restoration of damaged areas.","To address the escalating damage to ancient frescoes and facilitate batch restoration at archaeological sites, we propose a two-stage restoration model which called MER(Mural Enhancement and Restoration net) for ancient murals that are damaged and have been captured in low light.","Our two-stage model not only enhances the visual quality of restored images but also achieves commendable results in relevant metric evaluations compared with other competitors.","Furthermore, we have launched a website dedicated to the restoration of ancient mural paintings, utilizing the proposed model.","Code is available at https://gitee.com/bbfan2024/MER.git."],"url":"http://arxiv.org/abs/2405.08245v1","category":"cs.CV"}
{"created":"2024-05-14 00:14:21","title":"Structural fluctuations in active glasses","abstract":"The glassy dynamics of dense active matter have recently become a topic of interest due to their importance in biological processes such as wound healing and tissue development. However, while the liquid-state properties of dense active matter have been studied in relation to the glass transition of active matter, the solid-state properties of active glasses have yet to be understood. In this work, we study the structural fluctuations in the active glasses composed of self-propelled particles. We develop a formalism to describe the solid-state properties of active glasses in the harmonic approximation limit and use it to analyze the displacement fields in the active glasses. Our findings reveal that the dynamics of high-frequency normal modes become quasi-static with respect to the active forces, and consequently, excitations of these modes are significantly suppressed. This leads to a violation of the equipartition law, suppression of particle displacements, and the apparent collective motion of active glasses. Overall, our results provide a fundamental understanding of the solid-state properties of active glasses.","sentences":["The glassy dynamics of dense active matter have recently become a topic of interest due to their importance in biological processes such as wound healing and tissue development.","However, while the liquid-state properties of dense active matter have been studied in relation to the glass transition of active matter, the solid-state properties of active glasses have yet to be understood.","In this work, we study the structural fluctuations in the active glasses composed of self-propelled particles.","We develop a formalism to describe the solid-state properties of active glasses in the harmonic approximation limit and use it to analyze the displacement fields in the active glasses.","Our findings reveal that the dynamics of high-frequency normal modes become quasi-static with respect to the active forces, and consequently, excitations of these modes are significantly suppressed.","This leads to a violation of the equipartition law, suppression of particle displacements, and the apparent collective motion of active glasses.","Overall, our results provide a fundamental understanding of the solid-state properties of active glasses."],"url":"http://arxiv.org/abs/2405.08243v1","category":"cond-mat.soft"}
{"created":"2024-05-14 00:05:58","title":"No Joke: An Embodied Conversational Agent Greeting Older Adults with Humour or a Smile Unrelated to Initial Acceptance","abstract":"Embodied conversation agents (ECAs) are increasingly being developed for older adults as assistants or companions. Older adults may not be familiar with ECAs, influencing uptake and acceptability. First impressions can correlate strongly with subsequent judgments, even of computer agents, and could influence acceptance. Using the circumplex model of affect, we developed three versions of an ECA -- laughing, smiling, and neutral in expression -- to evaluate how positive first impressions affect acceptance. Results from 249 older adults indicated no statistically significant effects except for general attitudes towards technology and intelligent agents. This questions the potential of laughter, jokes, puns, and smiles as a method of initial engagement for older adults.","sentences":["Embodied conversation agents (ECAs) are increasingly being developed for older adults as assistants or companions.","Older adults may not be familiar with ECAs, influencing uptake and acceptability.","First impressions can correlate strongly with subsequent judgments, even of computer agents, and could influence acceptance.","Using the circumplex model of affect, we developed three versions of an ECA -- laughing, smiling, and neutral in expression -- to evaluate how positive first impressions affect acceptance.","Results from 249 older adults indicated no statistically significant effects except for general attitudes towards technology and intelligent agents.","This questions the potential of laughter, jokes, puns, and smiles as a method of initial engagement for older adults."],"url":"http://arxiv.org/abs/2405.08242v1","category":"cs.HC"}
{"created":"2024-05-13 23:38:50","title":"Silver-Tongued and Sundry: Exploring Intersectional Pronouns with ChatGPT","abstract":"ChatGPT is a conversational agent built on a large language model. Trained on a significant portion of human output, ChatGPT can mimic people to a degree. As such, we need to consider what social identities ChatGPT simulates (or can be designed to simulate). In this study, we explored the case of identity simulation through Japanese first-person pronouns, which are tightly connected to social identities in intersectional ways, i.e., intersectional pronouns. We conducted a controlled online experiment where people from two regions in Japan (Kanto and Kinki) witnessed interactions with ChatGPT using ten sets of first-person pronouns. We discovered that pronouns alone can evoke perceptions of social identities in ChatGPT at the intersections of gender, age, region, and formality, with caveats. This work highlights the importance of pronoun use for social identity simulation, provides a language-based methodology for culturally-sensitive persona development, and advances the potential of intersectional identities in intelligent agents.","sentences":["ChatGPT is a conversational agent built on a large language model.","Trained on a significant portion of human output, ChatGPT can mimic people to a degree.","As such, we need to consider what social identities ChatGPT simulates (or can be designed to simulate).","In this study, we explored the case of identity simulation through Japanese first-person pronouns, which are tightly connected to social identities in intersectional ways, i.e., intersectional pronouns.","We conducted a controlled online experiment where people from two regions in Japan (Kanto and Kinki) witnessed interactions with ChatGPT using ten sets of first-person pronouns.","We discovered that pronouns alone can evoke perceptions of social identities in ChatGPT at the intersections of gender, age, region, and formality, with caveats.","This work highlights the importance of pronoun use for social identity simulation, provides a language-based methodology for culturally-sensitive persona development, and advances the potential of intersectional identities in intelligent agents."],"url":"http://arxiv.org/abs/2405.08238v1","category":"cs.HC"}
{"created":"2024-05-13 22:01:03","title":"Interpreting Latent Student Knowledge Representations in Programming Assignments","abstract":"Recent advances in artificial intelligence for education leverage generative large language models, including using them to predict open-ended student responses rather than their correctness only. However, the black-box nature of these models limits the interpretability of the learned student knowledge representations. In this paper, we conduct a first exploration into interpreting latent student knowledge representations by presenting InfoOIRT, an Information regularized Open-ended Item Response Theory model, which encourages the latent student knowledge states to be interpretable while being able to generate student-written code for open-ended programming questions. InfoOIRT maximizes the mutual information between a fixed subset of latent knowledge states enforced with simple prior distributions and generated student code, which encourages the model to learn disentangled representations of salient syntactic and semantic code features including syntactic styles, mastery of programming skills, and code structures. Through experiments on a real-world programming education dataset, we show that InfoOIRT can both accurately generate student code and lead to interpretable student knowledge representations.","sentences":["Recent advances in artificial intelligence for education leverage generative large language models, including using them to predict open-ended student responses rather than their correctness only.","However, the black-box nature of these models limits the interpretability of the learned student knowledge representations.","In this paper, we conduct a first exploration into interpreting latent student knowledge representations by presenting InfoOIRT, an Information regularized Open-ended Item Response Theory model, which encourages the latent student knowledge states to be interpretable while being able to generate student-written code for open-ended programming questions.","InfoOIRT maximizes the mutual information between a fixed subset of latent knowledge states enforced with simple prior distributions and generated student code, which encourages the model to learn disentangled representations of salient syntactic and semantic code features including syntactic styles, mastery of programming skills, and code structures.","Through experiments on a real-world programming education dataset, we show that InfoOIRT can both accurately generate student code and lead to interpretable student knowledge representations."],"url":"http://arxiv.org/abs/2405.08213v1","category":"cs.CL"}
{"created":"2024-05-13 21:47:35","title":"A Semantic and Motion-Aware Spatiotemporal Transformer Network for Action Detection","abstract":"This paper presents a novel spatiotemporal transformer network that introduces several original components to detect actions in untrimmed videos. First, the multi-feature selective semantic attention model calculates the correlations between spatial and motion features to model spatiotemporal interactions between different action semantics properly. Second, the motion-aware network encodes the locations of action semantics in video frames utilizing the motion-aware 2D positional encoding algorithm. Such a motion-aware mechanism memorizes the dynamic spatiotemporal variations in action frames that current methods cannot exploit. Third, the sequence-based temporal attention model captures the heterogeneous temporal dependencies in action frames. In contrast to standard temporal attention used in natural language processing, primarily aimed at finding similarities between linguistic words, the proposed sequence-based temporal attention is designed to determine both the differences and similarities between video frames that jointly define the meaning of actions. The proposed approach outperforms the state-of-the-art solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24, and EPIC-Kitchens.","sentences":["This paper presents a novel spatiotemporal transformer network that introduces several original components to detect actions in untrimmed videos.","First, the multi-feature selective semantic attention model calculates the correlations between spatial and motion features to model spatiotemporal interactions between different action semantics properly.","Second, the motion-aware network encodes the locations of action semantics in video frames utilizing the motion-aware 2D positional encoding algorithm.","Such a motion-aware mechanism memorizes the dynamic spatiotemporal variations in action frames that current methods cannot exploit.","Third, the sequence-based temporal attention model captures the heterogeneous temporal dependencies in action frames.","In contrast to standard temporal attention used in natural language processing, primarily aimed at finding similarities between linguistic words, the proposed sequence-based temporal attention is designed to determine both the differences and similarities between video frames that jointly define the meaning of actions.","The proposed approach outperforms the state-of-the-art solutions on four spatiotemporal action datasets: AVA 2.2, AVA 2.1, UCF101-24, and EPIC-Kitchens."],"url":"http://arxiv.org/abs/2405.08204v1","category":"cs.CV"}
{"created":"2024-05-13 21:30:50","title":"Modeling of Time-varying Wireless Communication Channel with Fading and Shadowing","abstract":"The real-time quantification of the effect of a wireless channel on the transmitting signal is crucial for the analysis and the intelligent design of wireless communication systems for various services. Recent mechanisms to model channel characteristics independent of coding, modulation, signal processing, etc., using deep learning neural networks are promising solutions. However, the current approaches are neither statistically accurate nor able to adapt to the changing environment. In this paper, we propose a new approach that combines a deep learning neural network with a mixture density network model to derive the conditional probability density function (PDF) of receiving power given a communication distance in general wireless communication systems. Furthermore, a deep transfer learning scheme is designed and implemented to allow the channel model to dynamically adapt to changes in communication environments. Extensive experiments on Nakagami fading channel model and Log-normal shadowing channel model with path loss and noise show that the new approach is more statistically accurate, faster, and more robust than the previous deep learning-based channel models.","sentences":["The real-time quantification of the effect of a wireless channel on the transmitting signal is crucial for the analysis and the intelligent design of wireless communication systems for various services.","Recent mechanisms to model channel characteristics independent of coding, modulation, signal processing, etc., using deep learning neural networks are promising solutions.","However, the current approaches are neither statistically accurate nor able to adapt to the changing environment.","In this paper, we propose a new approach that combines a deep learning neural network with a mixture density network model to derive the conditional probability density function (PDF) of receiving power given a communication distance in general wireless communication systems.","Furthermore, a deep transfer learning scheme is designed and implemented to allow the channel model to dynamically adapt to changes in communication environments.","Extensive experiments on Nakagami fading channel model and Log-normal shadowing channel model with path loss and noise show that the new approach is more statistically accurate, faster, and more robust than the previous deep learning-based channel models."],"url":"http://arxiv.org/abs/2405.08199v1","category":"cs.LG"}
{"created":"2024-05-13 21:02:31","title":"Towards Energy-Aware Federated Learning via MARL: A Dual-Selection Approach for Model and Client","abstract":"Although Federated Learning (FL) is promising in knowledge sharing for heterogeneous Artificial Intelligence of Thing (AIoT) devices, their training performance and energy efficacy are severely restricted in practical battery-driven scenarios due to the ``wooden barrel effect'' caused by the mismatch between homogeneous model paradigms and heterogeneous device capability. As a result, due to various kinds of differences among devices, it is hard for existing FL methods to conduct training effectively in energy-constrained scenarios, such as the battery constraints of devices. To tackle the above issues, we propose an energy-aware FL framework named DR-FL, which considers the energy constraints in both clients and heterogeneous deep learning models to enable energy-efficient FL. Unlike Vanilla FL, DR-FL adopts our proposed Muti-Agents Reinforcement Learning (MARL)-based dual-selection method, which allows participated devices to make contributions to the global model effectively and adaptively based on their computing capabilities and energy capacities in a MARL-based manner. Experiments on various well-known datasets show that DR-FL can not only maximise knowledge sharing among heterogeneous models under the energy constraint of large-scale AIoT systems but also improve the model performance of each involved heterogeneous device.","sentences":["Although Federated Learning (FL) is promising in knowledge sharing for heterogeneous Artificial Intelligence of Thing (AIoT) devices, their training performance and energy efficacy are severely restricted in practical battery-driven scenarios due to the ``wooden barrel effect'' caused by the mismatch between homogeneous model paradigms and heterogeneous device capability.","As a result, due to various kinds of differences among devices, it is hard for existing FL methods to conduct training effectively in energy-constrained scenarios, such as the battery constraints of devices.","To tackle the above issues, we propose an energy-aware FL framework named DR-FL, which considers the energy constraints in both clients and heterogeneous deep learning models to enable energy-efficient FL.","Unlike Vanilla FL, DR-FL adopts our proposed Muti-Agents Reinforcement Learning (MARL)-based dual-selection method, which allows participated devices to make contributions to the global model effectively and adaptively based on their computing capabilities and energy capacities in a MARL-based manner.","Experiments on various well-known datasets show that DR-FL can not only maximise knowledge sharing among heterogeneous models under the energy constraint of large-scale AIoT systems but also improve the model performance of each involved heterogeneous device."],"url":"http://arxiv.org/abs/2405.08183v1","category":"cs.LG"}
{"created":"2024-05-13 20:39:27","title":"Estimating Direct and Indirect Causal Effects of Spatiotemporal Interventions in Presence of Spatial Interference","abstract":"Spatial interference (SI) occurs when the treatment at one location affects the outcomes at other locations. Accounting for spatial interference in spatiotemporal settings poses further challenges as interference violates the stable unit treatment value assumption, making it infeasible for standard causal inference methods to quantify the effects of time-varying treatment at spatially varying outcomes. In this paper, we first formalize the concept of spatial interference in case of time-varying treatment assignments by extending the potential outcome framework under the assumption of no unmeasured confounding. We then propose our deep learning based potential outcome model for spatiotemporal causal inference. We utilize latent factor modeling to reduce the bias due to time-varying confounding while leveraging the power of U-Net architecture to capture global and local spatial interference in data over time. Our causal estimators are an extension of average treatment effect (ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial interference on treated and untreated data. Being the first of its kind deep learning based spatiotemporal causal inference technique, our approach shows advantages over several baseline methods based on the experiment results on two synthetic datasets, with and without spatial interference. Our results on real-world climate dataset also align with domain knowledge, further demonstrating the effectiveness of our proposed method.","sentences":["Spatial interference (SI) occurs when the treatment at one location affects the outcomes at other locations.","Accounting for spatial interference in spatiotemporal settings poses further challenges as interference violates the stable unit treatment value assumption, making it infeasible for standard causal inference methods to quantify the effects of time-varying treatment at spatially varying outcomes.","In this paper, we first formalize the concept of spatial interference in case of time-varying treatment assignments by extending the potential outcome framework under the assumption of no unmeasured confounding.","We then propose our deep learning based potential outcome model for spatiotemporal causal inference.","We utilize latent factor modeling to reduce the bias due to time-varying confounding while leveraging the power of U-Net architecture to capture global and local spatial interference in data over time.","Our causal estimators are an extension of average treatment effect (ATE) for estimating direct (DATE) and indirect effects (IATE) of spatial interference on treated and untreated data.","Being the first of its kind deep learning based spatiotemporal causal inference technique, our approach shows advantages over several baseline methods based on the experiment results on two synthetic datasets, with and without spatial interference.","Our results on real-world climate dataset also align with domain knowledge, further demonstrating the effectiveness of our proposed method."],"url":"http://arxiv.org/abs/2405.08174v1","category":"cs.LG"}
{"created":"2024-05-13 20:37:04","title":"CANTONMT: Investigating Back-Translation and Model-Switch Mechanisms for Cantonese-English Neural Machine Translation","abstract":"This paper investigates the development and evaluation of machine translation models from Cantonese to English, where we propose a novel approach to tackle low-resource language translations. The main objectives of the study are to develop a model that can effectively translate Cantonese to English and evaluate it against state-of-the-art commercial models. To achieve this, a new parallel corpus has been created by combining different available corpora online with preprocessing and cleaning. In addition, a monolingual Cantonese dataset has been created through web scraping to aid the synthetic parallel corpus generation. Following the data collection process, several approaches, including fine-tuning models, back-translation, and model switch, have been used. The translation quality of models has been evaluated with multiple quality metrics, including lexicon-based metrics (SacreBLEU and hLEPOR) and embedding-space metrics (COMET and BERTscore). Based on the automatic metrics, the best model is selected and compared against the 2 best commercial translators using the human evaluation framework HOPES. The best model proposed in this investigation (NLLB-mBART) with model switch mechanisms has reached comparable and even better automatic evaluation scores against State-of-the-art commercial models (Bing and Baidu Translators), with a SacreBLEU score of 16.8 on our test set. Furthermore, an open-source web application has been developed to allow users to translate between Cantonese and English, with the different trained models available for effective comparisons between models from this investigation and users. CANTONMT is available at https://github.com/kenrickkung/CantoneseTranslation","sentences":["This paper investigates the development and evaluation of machine translation models from Cantonese to English, where we propose a novel approach to tackle low-resource language translations.","The main objectives of the study are to develop a model that can effectively translate Cantonese to English and evaluate it against state-of-the-art commercial models.","To achieve this, a new parallel corpus has been created by combining different available corpora online with preprocessing and cleaning.","In addition, a monolingual Cantonese dataset has been created through web scraping to aid the synthetic parallel corpus generation.","Following the data collection process, several approaches, including fine-tuning models, back-translation, and model switch, have been used.","The translation quality of models has been evaluated with multiple quality metrics, including lexicon-based metrics (SacreBLEU and hLEPOR) and embedding-space metrics (COMET and BERTscore).","Based on the automatic metrics, the best model is selected and compared against the 2 best commercial translators using the human evaluation framework HOPES.","The best model proposed in this investigation (NLLB-mBART) with model switch mechanisms has reached comparable and even better automatic evaluation scores against State-of-the-art commercial models (Bing and Baidu Translators), with a SacreBLEU score of 16.8 on our test set.","Furthermore, an open-source web application has been developed to allow users to translate between Cantonese and English, with the different trained models available for effective comparisons between models from this investigation and users.","CANTONMT is available at https://github.com/kenrickkung/CantoneseTranslation"],"url":"http://arxiv.org/abs/2405.08172v1","category":"cs.CL"}
{"created":"2024-05-13 20:26:10","title":"Rethinking Histology Slide Digitization Workflows for Low-Resource Settings","abstract":"Histology slide digitization is becoming essential for telepathology (remote consultation), knowledge sharing (education), and using the state-of-the-art artificial intelligence algorithms (augmented/automated end-to-end clinical workflows). However, the cumulative costs of digital multi-slide high-speed brightfield scanners, cloud/on-premises storage, and personnel (IT and technicians) make the current slide digitization workflows out-of-reach for limited-resource settings, further widening the health equity gap; even single-slide manual scanning commercial solutions are costly due to hardware requirements (high-resolution cameras, high-spec PC/workstation, and support for only high-end microscopes). In this work, we present a new cloud slide digitization workflow for creating scanner-quality whole-slide images (WSIs) from uploaded low-quality videos, acquired from cheap and inexpensive microscopes with built-in cameras. Specifically, we present a pipeline to create stitched WSIs while automatically deblurring out-of-focus regions, upsampling input 10X images to 40X resolution, and reducing brightness/contrast and light-source illumination variations. We demonstrate the WSI creation efficacy from our workflow on World Health Organization-declared neglected tropical disease, Cutaneous Leishmaniasis (prevalent only in the poorest regions of the world and only diagnosed by sub-specialist dermatopathologists, rare in poor countries), as well as other common pathologies on core biopsies of breast, liver, duodenum, stomach and lymph node. The code and pretrained models will be accessible via our GitHub (https://github.com/nadeemlab/DeepLIIF), and the cloud platform will be available at https://deepliif.org for uploading microscope videos and downloading/viewing WSIs with shareable links (no sign-in required) for telepathology and knowledge sharing.","sentences":["Histology slide digitization is becoming essential for telepathology (remote consultation), knowledge sharing (education), and using the state-of-the-art artificial intelligence algorithms (augmented/automated end-to-end clinical workflows).","However, the cumulative costs of digital multi-slide high-speed brightfield scanners, cloud/on-premises storage, and personnel (IT and technicians) make the current slide digitization workflows out-of-reach for limited-resource settings, further widening the health equity gap; even single-slide manual scanning commercial solutions are costly due to hardware requirements (high-resolution cameras, high-spec PC/workstation, and support for only high-end microscopes).","In this work, we present a new cloud slide digitization workflow for creating scanner-quality whole-slide images (WSIs) from uploaded low-quality videos, acquired from cheap and inexpensive microscopes with built-in cameras.","Specifically, we present a pipeline to create stitched WSIs while automatically deblurring out-of-focus regions, upsampling input 10X images to 40X resolution, and reducing brightness/contrast and light-source illumination variations.","We demonstrate the WSI creation efficacy from our workflow on World Health Organization-declared neglected tropical disease, Cutaneous Leishmaniasis (prevalent only in the poorest regions of the world and only diagnosed by sub-specialist dermatopathologists, rare in poor countries), as well as other common pathologies on core biopsies of breast, liver, duodenum, stomach and lymph node.","The code and pretrained models will be accessible via our GitHub (https://github.com/nadeemlab/DeepLIIF), and the cloud platform will be available at https://deepliif.org for uploading microscope videos and downloading/viewing WSIs with shareable links (no sign-in required) for telepathology and knowledge sharing."],"url":"http://arxiv.org/abs/2405.08169v1","category":"eess.IV"}
{"created":"2024-05-13 20:21:23","title":"Lithium, rotation and metallicity in the open cluster M35","abstract":"Lithium (Li) abundance is an age indicator for G, K, and M stellar types, as its abundance decreases over time for these spectral types. However, despite the observational efforts made over the past few decades, the role of rotation, activity, and metallicity in the depletion of Li is still unclear. We have investigated how Li depletion is affected by rotation and metallicity in G and K members of the roughly Pleiades-aged open cluster M35. To do so, we have collected a sample of 165 candidate members observed with the WIYN/Hydra spectrograph. In addition, we have taken advantage of three previous spectroscopic studies of Li in M35. As a result, we have collected a final sample of 396 stars which we have classified as members and non-members of the cluster. We have measured iron abundances, Li equivalent widths, and Li abundances for the 110 M35 members added to the existing sample by this study. Finally, rotation periods for cluster members have been obtained from the literature or derived from Zwicky Transient Facility light curves. As a result, we have confirmed that fast G and K rotators are Li-rich in comparison with slow rotators of similar effective temperature. Furthermore, while we derived subsolar metallicity for M35 from our spectra, the distribution of Li in this cluster is similar to those observed for the Pleiades and M34, which have solar metallicity and slightly different ages. In addition, we have shown that an empirical relationship proposed to remove the contribution of the Fe I line at 670.75 nm to the blended feature at 670.78 nm overestimates the contribution of this iron line for M35 members. We conclude that a 0.2-0.3 dex difference in metallicity makes little difference in the Li distributions of open clusters with ages between 100 and 250 Myr.","sentences":["Lithium (Li) abundance is an age indicator for G, K, and M stellar types, as its abundance decreases over time for these spectral types.","However, despite the observational efforts made over the past few decades, the role of rotation, activity, and metallicity in the depletion of Li is still unclear.","We have investigated how Li depletion is affected by rotation and metallicity in G and K members of the roughly Pleiades-aged open cluster M35.","To do so, we have collected a sample of 165 candidate members observed with the WIYN/Hydra spectrograph.","In addition, we have taken advantage of three previous spectroscopic studies of Li in M35.","As a result, we have collected a final sample of 396 stars which we have classified as members and non-members of the cluster.","We have measured iron abundances, Li equivalent widths, and Li abundances for the 110 M35 members added to the existing sample by this study.","Finally, rotation periods for cluster members have been obtained from the literature or derived from Zwicky Transient Facility light curves.","As a result, we have confirmed that fast G and K rotators are Li-rich in comparison with slow rotators of similar effective temperature.","Furthermore, while we derived subsolar metallicity for M35 from our spectra, the distribution of Li in this cluster is similar to those observed for the Pleiades and M34, which have solar metallicity and slightly different ages.","In addition, we have shown that an empirical relationship proposed to remove the contribution of the Fe I line at 670.75 nm to the blended feature at 670.78 nm overestimates the contribution of this iron line for M35 members.","We conclude that a 0.2-0.3 dex difference in metallicity makes little difference in the Li distributions of open clusters with ages between 100 and 250 Myr."],"url":"http://arxiv.org/abs/2405.08166v1","category":"astro-ph.SR"}
{"created":"2024-05-13 19:52:16","title":"LLM Theory of Mind and Alignment: Opportunities and Risks","abstract":"Large language models (LLMs) are transforming human-computer interaction and conceptions of artificial intelligence (AI) with their impressive capacities for conversing and reasoning in natural language. There is growing interest in whether LLMs have theory of mind (ToM); the ability to reason about the mental and emotional states of others that is core to human social intelligence. As LLMs are integrated into the fabric of our personal, professional and social lives and given greater agency to make decisions with real-world consequences, there is a critical need to understand how they can be aligned with human values. ToM seems to be a promising direction of inquiry in this regard. Following the literature on the role and impacts of human ToM, this paper identifies key areas in which LLM ToM will show up in human:LLM interactions at individual and group levels, and what opportunities and risks for alignment are raised in each. On the individual level, the paper considers how LLM ToM might manifest in goal specification, conversational adaptation, empathy and anthropomorphism. On the group level, it considers how LLM ToM might facilitate collective alignment, cooperation or competition, and moral judgement-making. The paper lays out a broad spectrum of potential implications and suggests the most pressing areas for future research.","sentences":["Large language models (LLMs) are transforming human-computer interaction and conceptions of artificial intelligence (AI) with their impressive capacities for conversing and reasoning in natural language.","There is growing interest in whether LLMs have theory of mind (ToM); the ability to reason about the mental and emotional states of others that is core to human social intelligence.","As LLMs are integrated into the fabric of our personal, professional and social lives and given greater agency to make decisions with real-world consequences, there is a critical need to understand how they can be aligned with human values.","ToM seems to be a promising direction of inquiry in this regard.","Following the literature on the role and impacts of human ToM, this paper identifies key areas in which LLM ToM will show up in human:LLM interactions at individual and group levels, and what opportunities and risks for alignment are raised in each.","On the individual level, the paper considers how LLM ToM might manifest in goal specification, conversational adaptation, empathy and anthropomorphism.","On the group level, it considers how LLM ToM might facilitate collective alignment, cooperation or competition, and moral judgement-making.","The paper lays out a broad spectrum of potential implications and suggests the most pressing areas for future research."],"url":"http://arxiv.org/abs/2405.08154v1","category":"cs.HC"}
{"created":"2024-05-13 19:16:28","title":"When factorization meets argumentation: towards argumentative explanations","abstract":"Factorization-based models have gained popularity since the Netflix challenge {(2007)}. Since that, various factorization-based models have been developed and these models have been proven to be efficient in predicting users' ratings towards items. A major concern is that explaining the recommendations generated by such methods is non-trivial because the explicit meaning of the latent factors they learn are not always clear. In response, we propose a novel model that combines factorization-based methods with argumentation frameworks (AFs). The integration of AFs provides clear meaning at each stage of the model, enabling it to produce easily understandable explanations for its recommendations. In this model, for every user-item interaction, an AF is defined in which the features of items are considered as arguments, and the users' ratings towards these features determine the strength and polarity of these arguments. This perspective allows our model to treat feature attribution as a structured argumentation procedure, where each calculation is marked with explicit meaning, enhancing its inherent interpretability. Additionally, our framework seamlessly incorporates side information, such as user contexts, leading to more accurate predictions. We anticipate at least three practical applications for our model: creating explanation templates, providing interactive explanations, and generating contrastive explanations. Through testing on real-world datasets, we have found that our model, along with its variants, not only surpasses existing argumentation-based methods but also competes effectively with current context-free and context-aware methods.","sentences":["Factorization-based models have gained popularity since the Netflix challenge {(2007)}.","Since that, various factorization-based models have been developed and these models have been proven to be efficient in predicting users' ratings towards items.","A major concern is that explaining the recommendations generated by such methods is non-trivial because the explicit meaning of the latent factors they learn are not always clear.","In response, we propose a novel model that combines factorization-based methods with argumentation frameworks (AFs).","The integration of AFs provides clear meaning at each stage of the model, enabling it to produce easily understandable explanations for its recommendations.","In this model, for every user-item interaction, an AF is defined in which the features of items are considered as arguments, and the users' ratings towards these features determine the strength and polarity of these arguments.","This perspective allows our model to treat feature attribution as a structured argumentation procedure, where each calculation is marked with explicit meaning, enhancing its inherent interpretability.","Additionally, our framework seamlessly incorporates side information, such as user contexts, leading to more accurate predictions.","We anticipate at least three practical applications for our model: creating explanation templates, providing interactive explanations, and generating contrastive explanations.","Through testing on real-world datasets, we have found that our model, along with its variants, not only surpasses existing argumentation-based methods but also competes effectively with current context-free and context-aware methods."],"url":"http://arxiv.org/abs/2405.08131v1","category":"cs.AI"}
{"created":"2024-05-13 19:09:50","title":"AI-Cybersecurity Education Through Designing AI-based Cyberharassment Detection Lab","abstract":"Cyberharassment is a critical, socially relevant cybersecurity problem because of the adverse effects it can have on targeted groups or individuals. While progress has been made in understanding cyber-harassment, its detection, attacks on artificial intelligence (AI) based cyberharassment systems, and the social problems in cyberharassment detectors, little has been done in designing experiential learning educational materials that engage students in this emerging social cybersecurity in the era of AI. Experiential learning opportunities are usually provided through capstone projects and engineering design courses in STEM programs such as computer science. While capstone projects are an excellent example of experiential learning, given the interdisciplinary nature of this emerging social cybersecurity problem, it can be challenging to use them to engage non-computing students without prior knowledge of AI. Because of this, we were motivated to develop a hands-on lab platform that provided experiential learning experiences to non-computing students with little or no background knowledge in AI and discussed the lessons learned in developing this lab. In this lab used by social science students at North Carolina A&T State University across two semesters (spring and fall) in 2022, students are given a detailed lab manual and are to complete a set of well-detailed tasks. Through this process, students learn AI concepts and the application of AI for cyberharassment detection. Using pre- and post-surveys, we asked students to rate their knowledge or skills in AI and their understanding of the concepts learned. The results revealed that the students moderately understood the concepts of AI and cyberharassment.","sentences":["Cyberharassment is a critical, socially relevant cybersecurity problem because of the adverse effects it can have on targeted groups or individuals.","While progress has been made in understanding cyber-harassment, its detection, attacks on artificial intelligence (AI) based cyberharassment systems, and the social problems in cyberharassment detectors, little has been done in designing experiential learning educational materials that engage students in this emerging social cybersecurity in the era of AI.","Experiential learning opportunities are usually provided through capstone projects and engineering design courses in STEM programs such as computer science.","While capstone projects are an excellent example of experiential learning, given the interdisciplinary nature of this emerging social cybersecurity problem, it can be challenging to use them to engage non-computing students without prior knowledge of AI.","Because of this, we were motivated to develop a hands-on lab platform that provided experiential learning experiences to non-computing students with little or no background knowledge in AI and discussed the lessons learned in developing this lab.","In this lab used by social science students at North Carolina A&T State University across two semesters (spring and fall) in 2022, students are given a detailed lab manual and are to complete a set of well-detailed tasks.","Through this process, students learn AI concepts and the application of AI for cyberharassment detection.","Using pre- and post-surveys, we asked students to rate their knowledge or skills in AI and their understanding of the concepts learned.","The results revealed that the students moderately understood the concepts of AI and cyberharassment."],"url":"http://arxiv.org/abs/2405.08125v1","category":"cs.CY"}
{"created":"2024-05-13 19:05:42","title":"From Questions to Insightful Answers: Building an Informed Chatbot for University Resources","abstract":"This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings.The objective of BARKPLUG V.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion. Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks. We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS). Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS). Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and experience, as validated by usability assessments.","sentences":["This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings.","The objective of BARKPLUG V.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion.","Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks.","We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS).","Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS).","Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and experience, as validated by usability assessments."],"url":"http://arxiv.org/abs/2405.08120v1","category":"cs.ET"}
{"created":"2024-05-13 19:01:08","title":"Secret Sharing with Certified Deletion","abstract":"Secret sharing allows a user to split a secret into many shares so that the secret can be recovered if, and only if, an authorized set of shares is collected. Although secret sharing typically does not require any computational hardness assumptions, its security does require that an adversary cannot collect an authorized set of shares. Over long periods of time where an adversary can benefit from multiple data breaches, this may become an unrealistic assumption.   We initiate the systematic study of secret sharing with certified deletion in order to achieve security even against an adversary that eventually collects an authorized set of shares. In secret sharing with certified deletion, a (classical) secret is split into quantum shares which can be verifiably destroyed. We define two natural notions of security: no-signaling security and adaptive security.   Next, we show how to construct (i) a secret sharing scheme with no-signaling certified deletion for any monotone access structure, and (ii) a threshold secret sharing scheme with adaptive certified deletion. Our first construction uses Bartusek and Khurana's (CRYPTO 2023) 2-out-of-2 secret sharing scheme with certified deletion as a building block, while our second construction is built from scratch and requires several new technical ideas. For example, we significantly generalize the ``XOR extractor'' of Agarwal, Bartusek, Khurana, and Kumar (EUROCRYPT 2023) in order to obtain high rate seedless extraction from certain quantum sources of entropy.","sentences":["Secret sharing allows a user to split a secret into many shares so that the secret can be recovered if, and only if, an authorized set of shares is collected.","Although secret sharing typically does not require any computational hardness assumptions, its security does require that an adversary cannot collect an authorized set of shares.","Over long periods of time where an adversary can benefit from multiple data breaches, this may become an unrealistic assumption.   ","We initiate the systematic study of secret sharing with certified deletion in order to achieve security even against an adversary that eventually collects an authorized set of shares.","In secret sharing with certified deletion, a (classical) secret is split into quantum shares which can be verifiably destroyed.","We define two natural notions of security: no-signaling security and adaptive security.   ","Next, we show how to construct (i) a secret sharing scheme with no-signaling certified deletion for any monotone access structure, and (ii) a threshold secret sharing scheme with adaptive certified deletion.","Our first construction uses Bartusek and Khurana's (CRYPTO 2023) 2-out-of-2 secret sharing scheme with certified deletion as a building block, while our second construction is built from scratch and requires several new technical ideas.","For example, we significantly generalize the ``XOR extractor'' of Agarwal, Bartusek, Khurana, and Kumar (EUROCRYPT 2023) in order to obtain high rate seedless extraction from certain quantum sources of entropy."],"url":"http://arxiv.org/abs/2405.08117v1","category":"cs.CR"}
{"created":"2024-05-13 18:22:02","title":"Semantic MIMO Systems for Speech-to-Text Transmission","abstract":"Semantic communications have been utilized to execute numerous intelligent tasks by transmitting task-related semantic information instead of bits. In this article, we propose a semantic-aware speech-to-text transmission system for the single-user multiple-input multiple-output (MIMO) and multi-user MIMO communication scenarios, named SAC-ST. Particularly, a semantic communication system to serve the speech-to-text task at the receiver is first designed, which compresses the semantic information and generates the low-dimensional semantic features by leveraging the transformer module. In addition, a novel semantic-aware network is proposed to facilitate the transmission with high semantic fidelity to identify the critical semantic information and guarantee it is recovered accurately. Furthermore, we extend the SAC-ST with a neural network-enabled channel estimation network to mitigate the dependence on accurate channel state information and validate the feasibility of SAC-ST in practical communication environments. Simulation results will show that the proposed SAC-ST outperforms the communication framework without the semantic-aware network for speech-to-text transmission over the MIMO channels in terms of the speech-to-text metrics, especially in the low signal-to-noise regime. Moreover, the SAC-ST with the developed channel estimation network is comparable to the SAC-ST with perfect channel state information.","sentences":["Semantic communications have been utilized to execute numerous intelligent tasks by transmitting task-related semantic information instead of bits.","In this article, we propose a semantic-aware speech-to-text transmission system for the single-user multiple-input multiple-output (MIMO) and multi-user MIMO communication scenarios, named SAC-ST.","Particularly, a semantic communication system to serve the speech-to-text task at the receiver is first designed, which compresses the semantic information and generates the low-dimensional semantic features by leveraging the transformer module.","In addition, a novel semantic-aware network is proposed to facilitate the transmission with high semantic fidelity to identify the critical semantic information and guarantee it is recovered accurately.","Furthermore, we extend the SAC-ST with a neural network-enabled channel estimation network to mitigate the dependence on accurate channel state information and validate the feasibility of SAC-ST in practical communication environments.","Simulation results will show that the proposed SAC-ST outperforms the communication framework without the semantic-aware network for speech-to-text transmission over the MIMO channels in terms of the speech-to-text metrics, especially in the low signal-to-noise regime.","Moreover, the SAC-ST with the developed channel estimation network is comparable to the SAC-ST with perfect channel state information."],"url":"http://arxiv.org/abs/2405.08096v1","category":"eess.AS"}
{"created":"2024-05-13 18:00:05","title":"Show Me the Way: Real-Time Tracking of Wireless Mobile Users with UWB-Enabled RIS","abstract":"The integration of Reconfigurable Intelligent Surfaces (RIS) in 6G wireless networks offers unprecedented control over communication environments. However, identifying optimal configurations within practical constraints remains a significant challenge. This becomes especially pronounced, when the user is mobile and the configurations need to be deployed in real time. Leveraging Ultra-Wideband (UWB) as localization technique, we capture and analyze real-time movements of a user within the RIS-enabled indoor environment. Given this information about the system's geometry, a model-based optimization is utilized, which enables real-time beam steering of the RIS towards the user. However, practical limitations of UWB modules lead to fluctuating UWB estimates, causing the RIS beam to occasionally miss the tracked user. The methodologies proposed in this work aim to increase the compatibility between these two systems. To this end, we provide two key solutions: beam splitting for obtaining more robust RIS configurations and UWB estimation correction for reducing the variations in the UWB data. Through comprehensive theoretical and experimental evaluations in both stationary and mobile scenarios, the effectiveness of the proposed techniques is demonstrated. When combined, the proposed methods improve worst-case tracking performance by a significant 17.5dB compared to the conventional approach.","sentences":["The integration of Reconfigurable Intelligent Surfaces (RIS) in 6G wireless networks offers unprecedented control over communication environments.","However, identifying optimal configurations within practical constraints remains a significant challenge.","This becomes especially pronounced, when the user is mobile and the configurations need to be deployed in real time.","Leveraging Ultra-Wideband (UWB) as localization technique, we capture and analyze real-time movements of a user within the RIS-enabled indoor environment.","Given this information about the system's geometry, a model-based optimization is utilized, which enables real-time beam steering of the RIS towards the user.","However, practical limitations of UWB modules lead to fluctuating UWB estimates, causing the RIS beam to occasionally miss the tracked user.","The methodologies proposed in this work aim to increase the compatibility between these two systems.","To this end, we provide two key solutions: beam splitting for obtaining more robust RIS configurations and UWB estimation correction for reducing the variations in the UWB data.","Through comprehensive theoretical and experimental evaluations in both stationary and mobile scenarios, the effectiveness of the proposed techniques is demonstrated.","When combined, the proposed methods improve worst-case tracking performance by a significant 17.5dB compared to the conventional approach."],"url":"http://arxiv.org/abs/2405.08076v1","category":"eess.SP"}
{"created":"2024-05-13 18:00:05","title":"Methods and stability tests associated with the sterile neutrino search using improved high-energy $\u03bd_\u03bc$ event reconstruction in IceCube","abstract":"We provide supporting details for the search for a 3+1 sterile neutrino using data collected over eleven years at the IceCube Neutrino Observatory. The analysis uses atmospheric muon-flavored neutrinos from 0.5 to 100\\, TeV that traverse the Earth to reach the IceCube detector, and finds a best-fit point at $\\sin^2(2\\theta_{24}) = 0.16$ and $\\Delta m^{2}_{41} = 3.5$ eV$^2$ with a goodness-of-fit p-value of 12\\% and consistency with the null hypothesis of no oscillations to sterile neutrinos with a p-value of 3.1\\%. Several improvements were made over past analyses, which are reviewed in this article, including upgrades to the reconstruction and the study of sources of systematic uncertainty. We provide details of the fit quality and discuss stability tests that split the data for separate samples, comparing results. We find that the fits are consistent between split data sets.","sentences":["We provide supporting details for the search for a 3+1 sterile neutrino using data collected over eleven years at the IceCube Neutrino Observatory.","The analysis uses atmospheric muon-flavored neutrinos from 0.5 to 100\\, TeV that traverse the Earth to reach the IceCube detector, and finds a best-fit point at $\\sin^2(2\\theta_{24})","= 0.16$ and $\\Delta m^{2}_{41} = 3.5$ eV$^2$ with a goodness-of-fit p-value of 12\\% and consistency with the null hypothesis of no oscillations to sterile neutrinos with a p-value of 3.1\\%.","Several improvements were made over past analyses, which are reviewed in this article, including upgrades to the reconstruction and the study of sources of systematic uncertainty.","We provide details of the fit quality and discuss stability tests that split the data for separate samples, comparing results.","We find that the fits are consistent between split data sets."],"url":"http://arxiv.org/abs/2405.08077v1","category":"hep-ex"}
{"created":"2024-05-13 16:48:16","title":"Radio Resource Management and Path Planning in Intelligent Transportation Systems via Reinforcement Learning for Environmental Sustainability","abstract":"Efficient and dynamic path planning has become an important topic for urban areas with larger density of connected vehicles (CV) which results in reduction of travel time and directly contributes to environmental sustainability through reducing energy consumption. CVs exploit the cellular wireless vehicle-to-everything (C-V2X) communication technology to disseminate the vehicle-to-infrastructure (V2I) messages to the Base-station (BS) to improve situation awareness on urban roads. In this paper, we investigate radio resource management (RRM) in such a framework to minimize the age of information (AoI) so as to enhance path planning results. We use the fact that V2I messages with lower AoI value result in less error in estimating the road capacity and more accurate path planning. Through simulations, we compare road travel times and volume over capacity (V/C) against different levels of AoI and demonstrate the promising performance of the proposed framework.","sentences":["Efficient and dynamic path planning has become an important topic for urban areas with larger density of connected vehicles (CV) which results in reduction of travel time and directly contributes to environmental sustainability through reducing energy consumption.","CVs exploit the cellular wireless vehicle-to-everything (C-V2X) communication technology to disseminate the vehicle-to-infrastructure (V2I) messages to the Base-station (BS) to improve situation awareness on urban roads.","In this paper, we investigate radio resource management (RRM) in such a framework to minimize the age of information (AoI) so as to enhance path planning results.","We use the fact that V2I messages with lower AoI value result in less error in estimating the road capacity and more accurate path planning.","Through simulations, we compare road travel times and volume over capacity (V/C) against different levels of AoI and demonstrate the promising performance of the proposed framework."],"url":"http://arxiv.org/abs/2405.08053v1","category":"eess.SP"}
{"created":"2024-05-13 14:51:02","title":"Comparative analysis of neural network architectures for short-term FOREX forecasting","abstract":"The present document delineates the analysis, design, implementation, and benchmarking of various neural network architectures within a short-term frequency prediction system for the foreign exchange market (FOREX). Our aim is to simulate the judgment of the human expert (technical analyst) using a system that responds promptly to changes in market conditions, thus enabling the optimization of short-term trading strategies. We designed and implemented a series of LSTM neural network architectures which are taken as input the exchange rate values and generate the short-term market trend forecasting signal and an ANN custom architecture based on technical analysis indicator simulators We performed a comparative analysis of the results and came to useful conclusions regarding the suitability of each architecture and the cost in terms of time and computational power to implement them. The ANN custom architecture produces better prediction quality with higher sensitivity using fewer resources and spending less time than LSTM architectures. The ANN custom architecture appears to be ideal for use in low-power computing systems and for use cases that need fast decisions with the least possible computational cost.","sentences":["The present document delineates the analysis, design, implementation, and benchmarking of various neural network architectures within a short-term frequency prediction system for the foreign exchange market (FOREX).","Our aim is to simulate the judgment of the human expert (technical analyst) using a system that responds promptly to changes in market conditions, thus enabling the optimization of short-term trading strategies.","We designed and implemented a series of LSTM neural network architectures which are taken as input the exchange rate values and generate the short-term market trend forecasting signal and an ANN custom architecture based on technical analysis indicator simulators","We performed a comparative analysis of the results and came to useful conclusions regarding the suitability of each architecture and the cost in terms of time and computational power to implement them.","The ANN custom architecture produces better prediction quality with higher sensitivity using fewer resources and spending less time than LSTM architectures.","The ANN custom architecture appears to be ideal for use in low-power computing systems and for use cases that need fast decisions with the least possible computational cost."],"url":"http://arxiv.org/abs/2405.08045v1","category":"q-fin.MF"}
{"created":"2024-05-13 13:55:34","title":"Mitigating federated learning contribution allocation instability through randomized aggregation","abstract":"Federated learning (FL) is a novel collaborative machine learning framework designed to preserve privacy while enabling the creation of robust models. This paradigm addresses a growing need for data security by allowing multiple participants to contribute to a model without exposing their individual datasets. A pivotal issue within this framework, however, concerns the fair and accurate attribution of contributions from various participants to the creation of the joint global model. Incorrect contribution distribution can erode trust among participants, result in inequitable compensation, and ultimately diminish the willingness of parties to engage or actively contribute to the federation. While several methods for remunerating participants have been proposed, little attention was given to the analysis of the stability of these methods when evaluating contributions, which is critical to ensure the long-term viability and fairness of FL systems. In this paper, we analyse this stability through the calculation of contributions by gradient-based model reconstruction techniques with Shapley values. Our investigation reveals that Shapley values fail to reflect baseline contributions, especially when employing different aggregation techniques. To address this issue, we extend on established aggregation techniques by introducing FedRandom, which is designed to sample contributions in a more equitable and distributed manner. We demonstrate that this approach not only serves as a viable aggregation technique but also significantly improves the accuracy of contribution assessment compared to traditional methods. Our results suggest that FedRandom enhances the overall fairness and stability of the federated learning system, making it a superior choice for federations with limited number of participants.","sentences":["Federated learning (FL) is a novel collaborative machine learning framework designed to preserve privacy while enabling the creation of robust models.","This paradigm addresses a growing need for data security by allowing multiple participants to contribute to a model without exposing their individual datasets.","A pivotal issue within this framework, however, concerns the fair and accurate attribution of contributions from various participants to the creation of the joint global model.","Incorrect contribution distribution can erode trust among participants, result in inequitable compensation, and ultimately diminish the willingness of parties to engage or actively contribute to the federation.","While several methods for remunerating participants have been proposed, little attention was given to the analysis of the stability of these methods when evaluating contributions, which is critical to ensure the long-term viability and fairness of FL systems.","In this paper, we analyse this stability through the calculation of contributions by gradient-based model reconstruction techniques with Shapley values.","Our investigation reveals that Shapley values fail to reflect baseline contributions, especially when employing different aggregation techniques.","To address this issue, we extend on established aggregation techniques by introducing FedRandom, which is designed to sample contributions in a more equitable and distributed manner.","We demonstrate that this approach not only serves as a viable aggregation technique but also significantly improves the accuracy of contribution assessment compared to traditional methods.","Our results suggest that FedRandom enhances the overall fairness and stability of the federated learning system, making it a superior choice for federations with limited number of participants."],"url":"http://arxiv.org/abs/2405.08044v1","category":"cs.LG"}
{"created":"2024-05-13 12:56:24","title":"HRNet: Differentially Private Hierarchical and Multi-Resolution Network for Human Mobility Data Synthesization","abstract":"Human mobility data offers valuable insights for many applications such as urban planning and pandemic response, but its use also raises privacy concerns. In this paper, we introduce the Hierarchical and Multi-Resolution Network (HRNet), a novel deep generative model specifically designed to synthesize realistic human mobility data while guaranteeing differential privacy. We first identify the key difficulties inherent in learning human mobility data under differential privacy. In response to these challenges, HRNet integrates three components: a hierarchical location encoding mechanism, multi-task learning across multiple resolutions, and private pre-training. These elements collectively enhance the model's ability under the constraints of differential privacy. Through extensive comparative experiments utilizing a real-world dataset, HRNet demonstrates a marked improvement over existing methods in balancing the utility-privacy trade-off.","sentences":["Human mobility data offers valuable insights for many applications such as urban planning and pandemic response, but its use also raises privacy concerns.","In this paper, we introduce the Hierarchical and Multi-Resolution Network (HRNet), a novel deep generative model specifically designed to synthesize realistic human mobility data while guaranteeing differential privacy.","We first identify the key difficulties inherent in learning human mobility data under differential privacy.","In response to these challenges, HRNet integrates three components: a hierarchical location encoding mechanism, multi-task learning across multiple resolutions, and private pre-training.","These elements collectively enhance the model's ability under the constraints of differential privacy.","Through extensive comparative experiments utilizing a real-world dataset, HRNet demonstrates a marked improvement over existing methods in balancing the utility-privacy trade-off."],"url":"http://arxiv.org/abs/2405.08043v1","category":"cs.CR"}
{"created":"2024-05-13 12:40:18","title":"LLAniMAtion: LLAMA Driven Gesture Animation","abstract":"Co-speech gesturing is an important modality in conversation, providing context and social cues. In character animation, appropriate and synchronised gestures add realism, and can make interactive agents more engaging. Historically, methods for automatically generating gestures were predominantly audio-driven, exploiting the prosodic and speech-related content that is encoded in the audio signal. In this paper we instead experiment with using LLM features for gesture generation that are extracted from text using LLAMA2. We compare against audio features, and explore combining the two modalities in both objective tests and a user study. Surprisingly, our results show that LLAMA2 features on their own perform significantly better than audio features and that including both modalities yields no significant difference to using LLAMA2 features in isolation. We demonstrate that the LLAMA2 based model can generate both beat and semantic gestures without any audio input, suggesting LLMs can provide rich encodings that are well suited for gesture generation.","sentences":["Co-speech gesturing is an important modality in conversation, providing context and social cues.","In character animation, appropriate and synchronised gestures add realism, and can make interactive agents more engaging.","Historically, methods for automatically generating gestures were predominantly audio-driven, exploiting the prosodic and speech-related content that is encoded in the audio signal.","In this paper we instead experiment with using LLM features for gesture generation that are extracted from text using LLAMA2.","We compare against audio features, and explore combining the two modalities in both objective tests and a user study.","Surprisingly, our results show that LLAMA2 features on their own perform significantly better than audio features and that including both modalities yields no significant difference to using LLAMA2 features in isolation.","We demonstrate that the LLAMA2 based model can generate both beat and semantic gestures without any audio input, suggesting LLMs can provide rich encodings that are well suited for gesture generation."],"url":"http://arxiv.org/abs/2405.08042v1","category":"cs.HC"}
{"created":"2024-05-13 09:41:34","title":"DeepFMEA -- A Scalable Framework Harmonizing Process Expertise and Data-Driven PHM","abstract":"Machine Learning (ML) based prognostics and health monitoring (PHM) tools provide new opportunities for manufacturers to operate and maintain their equipment in a risk-optimized manner and utilize it more sustainably along its lifecycle. Yet, in most industrial settings, data is often limited in quantity, and its quality can be inconsistent - both critical for developing and operating reliable ML models. To bridge this gap in practice, successfully industrialized PHM tools rely on the introduction of domain expertise as a prior, to enable sufficiently accurate predictions, while enhancing their interpretability.   Thus, a key challenge while developing data-driven PHM tools involves translating the experience and process knowledge of maintenance personnel, development, and service engineers into a data structure. This structure must not only capture the diversity and variability of the expertise but also render this knowledge accessible for various data-driven algorithms. This results in data models that are heavily tailored towards a specific application and the failure modes the development team aims to detect or predict. The lack of a standardized approach limits developments' extensibility to new failure modes, their transferability to new applications, and it inhibits the utilization of standard data management and MLOps tools, increasing the burden on the development team.   DeepFMEA draws inspiration from the Failure Mode and Effects Analysis (FMEA) in its structured approach to the analysis of any technical system and the resulting standardized data model, while considering aspects that are crucial to capturing process and maintenance expertise in a way that is both intuitive to domain experts and the resulting information can be introduced as priors to ML algorithms.","sentences":["Machine Learning (ML) based prognostics and health monitoring (PHM) tools provide new opportunities for manufacturers to operate and maintain their equipment in a risk-optimized manner and utilize it more sustainably along its lifecycle.","Yet, in most industrial settings, data is often limited in quantity, and its quality can be inconsistent - both critical for developing and operating reliable ML models.","To bridge this gap in practice, successfully industrialized PHM tools rely on the introduction of domain expertise as a prior, to enable sufficiently accurate predictions, while enhancing their interpretability.   ","Thus, a key challenge while developing data-driven PHM tools involves translating the experience and process knowledge of maintenance personnel, development, and service engineers into a data structure.","This structure must not only capture the diversity and variability of the expertise but also render this knowledge accessible for various data-driven algorithms.","This results in data models that are heavily tailored towards a specific application and the failure modes the development team aims to detect or predict.","The lack of a standardized approach limits developments' extensibility to new failure modes, their transferability to new applications, and it inhibits the utilization of standard data management and MLOps tools, increasing the burden on the development team.   ","DeepFMEA draws inspiration from the Failure Mode and Effects Analysis (FMEA) in its structured approach to the analysis of any technical system and the resulting standardized data model, while considering aspects that are crucial to capturing process and maintenance expertise in a way that is both intuitive to domain experts and the resulting information can be introduced as priors to ML algorithms."],"url":"http://arxiv.org/abs/2405.08041v1","category":"cs.LG"}
{"created":"2024-05-13 08:41:02","title":"Vehicles Swarm Intelligence: Cooperation in both Longitudinal and Lateral Dimensions","abstract":"Longitudinal-only platooning methods are facing great challenges on running mobility, since they may be impeded by slow-moving vehicles from time to time. To address this issue, this paper proposes a vehicles swarming method coupled both longitudinal and lateral cooperation. The proposed method bears the following contributions: i) enhancing driving mobility by swarming like a bee colony; ii) ensuring the success rate of overtaking; iii) cruising as a string of platoon to preserve sustainability. Evaluations indicate that the proposed method is capable of maneuvering a vehicle swarm to overtake slow-moving vehicles safely and successfully. The proposed method is confirmed to improve running mobility by 12.04%. Swarming safety is ensured by a safe following distance. The proposed method's influence on traffic is limited within five upstream vehicles.","sentences":["Longitudinal-only platooning methods are facing great challenges on running mobility, since they may be impeded by slow-moving vehicles from time to time.","To address this issue, this paper proposes a vehicles swarming method coupled both longitudinal and lateral cooperation.","The proposed method bears the following contributions: i) enhancing driving mobility by swarming like a bee colony; ii) ensuring the success rate of overtaking; iii) cruising as a string of platoon to preserve sustainability.","Evaluations indicate that the proposed method is capable of maneuvering a vehicle swarm to overtake slow-moving vehicles safely and successfully.","The proposed method is confirmed to improve running mobility by 12.04%.","Swarming safety is ensured by a safe following distance.","The proposed method's influence on traffic is limited within five upstream vehicles."],"url":"http://arxiv.org/abs/2405.08039v1","category":"cs.RO"}
{"created":"2024-05-13 06:57:18","title":"Feature Expansion and enhanced Compression for Class Incremental Learning","abstract":"Class incremental learning consists in training discriminative models to classify an increasing number of classes over time. However, doing so using only the newly added class data leads to the known problem of catastrophic forgetting of the previous classes. Recently, dynamic deep learning architectures have been shown to exhibit a better stability-plasticity trade-off by dynamically adding new feature extractors to the model in order to learn new classes followed by a compression step to scale the model back to its original size, thus avoiding a growing number of parameters. In this context, we propose a new algorithm that enhances the compression of previous class knowledge by cutting and mixing patches of previous class samples with the new images during compression using our Rehearsal-CutMix method. We show that this new data augmentation reduces catastrophic forgetting by specifically targeting past class information and improving its compression. Extensive experiments performed on the CIFAR and ImageNet datasets under diverse incremental learning evaluation protocols demonstrate that our approach consistently outperforms the state-of-the-art . The code will be made available upon publication of our work.","sentences":["Class incremental learning consists in training discriminative models to classify an increasing number of classes over time.","However, doing so using only the newly added class data leads to the known problem of catastrophic forgetting of the previous classes.","Recently, dynamic deep learning architectures have been shown to exhibit a better stability-plasticity trade-off by dynamically adding new feature extractors to the model in order to learn new classes followed by a compression step to scale the model back to its original size, thus avoiding a growing number of parameters.","In this context, we propose a new algorithm that enhances the compression of previous class knowledge by cutting and mixing patches of previous class samples with the new images during compression using our Rehearsal-CutMix method.","We show that this new data augmentation reduces catastrophic forgetting by specifically targeting past class information and improving its compression.","Extensive experiments performed on the CIFAR and ImageNet datasets under diverse incremental learning evaluation protocols demonstrate that our approach consistently outperforms the state-of-the-art .","The code will be made available upon publication of our work."],"url":"http://arxiv.org/abs/2405.08038v1","category":"cs.LG"}
{"created":"2024-05-14 17:59:57","title":"The RoboDrive Challenge: Drive Anytime Anywhere in Any Condition","abstract":"In the realm of autonomous driving, robust perception under out-of-distribution conditions is paramount for the safe deployment of vehicles. Challenges such as adverse weather, sensor malfunctions, and environmental unpredictability can severely impact the performance of autonomous systems. The 2024 RoboDrive Challenge was crafted to propel the development of driving perception technologies that can withstand and adapt to these real-world variabilities. Focusing on four pivotal tasks -- BEV detection, map segmentation, semantic occupancy prediction, and multi-view depth estimation -- the competition laid down a gauntlet to innovate and enhance system resilience against typical and atypical disturbances. This year's challenge consisted of five distinct tracks and attracted 140 registered teams from 93 institutes across 11 countries, resulting in nearly one thousand submissions evaluated through our servers. The competition culminated in 15 top-performing solutions, which introduced a range of innovative approaches including advanced data augmentation, multi-sensor fusion, self-supervised learning for error correction, and new algorithmic strategies to enhance sensor robustness. These contributions significantly advanced the state of the art, particularly in handling sensor inconsistencies and environmental variability. Participants, through collaborative efforts, pushed the boundaries of current technologies, showcasing their potential in real-world scenarios. Extensive evaluations and analyses provided insights into the effectiveness of these solutions, highlighting key trends and successful strategies for improving the resilience of driving perception systems. This challenge has set a new benchmark in the field, providing a rich repository of techniques expected to guide future research in this field.","sentences":["In the realm of autonomous driving, robust perception under out-of-distribution conditions is paramount for the safe deployment of vehicles.","Challenges such as adverse weather, sensor malfunctions, and environmental unpredictability can severely impact the performance of autonomous systems.","The 2024 RoboDrive Challenge was crafted to propel the development of driving perception technologies that can withstand and adapt to these real-world variabilities.","Focusing on four pivotal tasks -- BEV detection, map segmentation, semantic occupancy prediction, and multi-view depth estimation -- the competition laid down a gauntlet to innovate and enhance system resilience against typical and atypical disturbances.","This year's challenge consisted of five distinct tracks and attracted 140 registered teams from 93 institutes across 11 countries, resulting in nearly one thousand submissions evaluated through our servers.","The competition culminated in 15 top-performing solutions, which introduced a range of innovative approaches including advanced data augmentation, multi-sensor fusion, self-supervised learning for error correction, and new algorithmic strategies to enhance sensor robustness.","These contributions significantly advanced the state of the art, particularly in handling sensor inconsistencies and environmental variability.","Participants, through collaborative efforts, pushed the boundaries of current technologies, showcasing their potential in real-world scenarios.","Extensive evaluations and analyses provided insights into the effectiveness of these solutions, highlighting key trends and successful strategies for improving the resilience of driving perception systems.","This challenge has set a new benchmark in the field, providing a rich repository of techniques expected to guide future research in this field."],"url":"http://arxiv.org/abs/2405.08816v1","category":"cs.CV"}
{"created":"2024-05-14 17:58:26","title":"Chaotic dynamics at the boundary of a basin of attraction via non-transversal intersections for a non-global smooth diffeomorphism","abstract":"In this paper we give analytic proofs of the existence of transversal homoclinic points for a family of non-globally smooth diffeomorphisms having the origin as a fixed point which come out as a truncated map governing the local dynamics near a critical period three cycle associated to the Secant map. Using Moser's version of Birkhoff-Smale's Theorem, we prove that the boundary of the basin of attraction of the origin contains a Cantor-like invariant subset such that the restricted dynamics to it is conjugate to the full shift of $N$-symbols for any integer $N\\ge 2$ or infinity.","sentences":["In this paper we give analytic proofs of the existence of transversal homoclinic points for a family of non-globally smooth diffeomorphisms having the origin as a fixed point which come out as a truncated map governing the local dynamics near a critical period three cycle associated to the Secant map.","Using Moser's version of Birkhoff-Smale's Theorem, we prove that the boundary of the basin of attraction of the origin contains a Cantor-like invariant subset such that the restricted dynamics to it is conjugate to the full shift of $N$-symbols for any integer $N\\ge 2$ or infinity."],"url":"http://arxiv.org/abs/2405.08812v1","category":"math.DS"}
{"created":"2024-05-14 17:58:11","title":"Slow-growing counterexamples to the strong Eremenko Conjecture","abstract":"Let $f\\colon\\mathbb{C} \\to\\mathbb{C}$ be a transcendental entire function. In 1989, Eremenko asked the following question concerning the set $I(f)$ of points that tend to infinity under iteration: can every point of $I(f)$ be joined to $\\infty$ by a curve in $I(f)$? This is known as the strong Eremenko conjecture and was disproved in 2011 by Rottenfu{\\ss}er, R\\\"uckert, Rempe and Schleicher. The function has relatively small infinite order: it can be chosen such that $\\log \\log \\,\\lvert f(z)\\rvert = (\\log \\lvert z\\rvert)^{1+o(1)}$ as $f(z)\\to \\infty$. Moreover, $f$ belongs to the \\emph{Eremenko--Lyubich class $\\mathcal{B}$}. Rottenfu{\\ss}er et al also show that the strong Eremenko conjecture does hold for any $f\\in\\mathcal{B}$ of finite order. We consider how slow a counterexample $f\\in\\mathcal{B}$ can grow. Suppose that $\\Theta\\colon [t_0,\\infty)\\to [0,\\infty)$ satisfies $\\Theta(t) \\to 0$ and \\[ (\\log t)^{-\\beta \\Theta(\\log t)}/\\Theta(t) \\to 0 \\quad\\text{ as $t\\to \\infty$} \\] for some $0<\\beta<1$, along with a certain regularity assumption. Then there exists a counterexample $f\\in\\mathcal{B}$ as above such that \\[ \\log \\log \\vert f(z)\\vert = O ( (\\log \\vert z \\vert)^{1 + \\Theta( \\log \\vert z \\vert )}) \\] as $\\vert f(z)\\vert \\to\\infty$. The hypotheses are satisfied, in particular, for $\\Theta(t) = 1/(\\log \\log t)^{\\alpha}$, for any $\\alpha>0$.","sentences":["Let $f\\colon\\mathbb{C} \\to\\mathbb{C}$ be a transcendental entire function.","In 1989, Eremenko asked the following question concerning the set $I(f)$ of points that tend to infinity under iteration: can every point of $I(f)$ be joined to $\\infty$ by a curve in $I(f)$?","This is known as the strong Eremenko conjecture and was disproved in 2011 by Rottenfu{\\ss}er, R\\\"uckert, Rempe and Schleicher.","The function has relatively small infinite order: it can be chosen such that $\\log \\log \\,\\lvert f(z)\\rvert = (\\log \\lvert z\\rvert)^{1+o(1)}$ as $f(z)\\to \\infty$.","Moreover, $f$ belongs to the \\emph{Eremenko--Lyubich class $\\mathcal{B}$}.","Rottenfu{\\ss}er et al also show that the strong Eremenko conjecture does hold for any $f\\in\\mathcal{B}$ of finite order.","We consider how slow a counterexample $f\\in\\mathcal{B}$ can grow.","Suppose that $\\Theta\\colon [t_0,\\infty)\\to [0,\\infty)$ satisfies $\\Theta(t) \\to 0$ and \\[ (\\log t)^{-\\beta \\Theta(\\log t)}/\\Theta(t)","\\to 0","\\quad\\text{ as $t\\to \\infty$} \\] for some $0<\\beta<1$, along with a certain regularity assumption.","Then there exists a counterexample $f\\in\\mathcal{B}$ as above such that \\[ \\log \\log \\vert f(z)\\vert = O ( (\\log \\vert z \\vert)^{1 + \\Theta( \\log \\vert z \\vert )})","\\] as $\\vert f(z)\\vert \\to\\infty$. The hypotheses are satisfied, in particular, for $\\Theta(t) = 1/(\\log \\log t)^{\\alpha}$, for any $\\alpha>0$."],"url":"http://arxiv.org/abs/2405.08811v1","category":"math.DS"}
{"created":"2024-05-14 17:55:04","title":"Giving a $KO$ to 8D Gauge Anomalies","abstract":"In \\cite{Garcia-Etxebarria:2017crf}, it was found that the system of $k$ D7-branes probing an $O7^+$-plane suffers from an $\\mathfrak{sp}(k)$ gauge anomaly when $k>1$. These authors then conjectured that this 8D $\\mathcal{N}=1$ gauge theory couples to an 8D topological field theory (TFT) such that the total system is anomaly-free, thus acting as a \"topological\" Green-Schwarz mechanism. In this note, we construct such an 8D TFT and show that it indeed cancels the gauge anomaly. The key step is to engineer the relevant topological operators from D3-branes and fluxbranes placed infinitely far away from the stack of 7-branes. Such symmetry operators have topological vector bundles defined on them whose $KO/KSp$-homology classes play a role in the anomaly cancellation.","sentences":["In \\cite{Garcia-Etxebarria:2017crf}, it was found that the system of $k$ D7-branes probing an $O7^+$-plane suffers from an $\\mathfrak{sp}(k)$ gauge anomaly when $k>1$. These authors then conjectured that this 8D $\\mathcal{N}=1$ gauge theory couples to an 8D topological field theory (TFT) such that the total system is anomaly-free, thus acting as a \"topological\" Green-Schwarz mechanism.","In this note, we construct such an 8D TFT and show that it indeed cancels the gauge anomaly.","The key step is to engineer the relevant topological operators from D3-branes and fluxbranes placed infinitely far away from the stack of 7-branes.","Such symmetry operators have topological vector bundles defined on them whose $KO/KSp$-homology classes play a role in the anomaly cancellation."],"url":"http://arxiv.org/abs/2405.08809v1","category":"hep-th"}
{"created":"2024-05-14 17:54:17","title":"SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation","abstract":"Large multimodal models (LMMs) have proven flexible and generalisable across many tasks and fields. Although they have strong potential to aid scientific research, their capabilities in this domain are not well characterised. A key aspect of scientific research is the ability to understand and interpret figures, which serve as a rich, compressed source of complex information. In this work, we present SciFIBench, a scientific figure interpretation benchmark. Our main benchmark consists of a 1000-question gold set of multiple-choice questions split between two tasks across 12 categories. The questions are curated from CS arXiv paper figures and captions, using adversarial filtering to find hard negatives and human verification for quality control. We evaluate 26 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we investigate the alignment and reasoning faithfulness of the LMMs on augmented question sets from our benchmark. We release SciFIBench to encourage progress in this domain.","sentences":["Large multimodal models (LMMs) have proven flexible and generalisable across many tasks and fields.","Although they have strong potential to aid scientific research, their capabilities in this domain are not well characterised.","A key aspect of scientific research is the ability to understand and interpret figures, which serve as a rich, compressed source of complex information.","In this work, we present SciFIBench, a scientific figure interpretation benchmark.","Our main benchmark consists of a 1000-question gold set of multiple-choice questions split between two tasks across 12 categories.","The questions are curated from CS arXiv paper figures and captions, using adversarial filtering to find hard negatives and human verification for quality control.","We evaluate 26 LMMs on SciFIBench, finding it to be a challenging benchmark.","Finally, we investigate the alignment and reasoning faithfulness of the LMMs on augmented question sets from our benchmark.","We release SciFIBench to encourage progress in this domain."],"url":"http://arxiv.org/abs/2405.08807v1","category":"cs.CV"}
{"created":"2024-05-14 17:50:58","title":"Temperature-dependent Structural Evolution of Ruddlesden-Popper Bilayer Nickelate La$_3$Ni$_2$O$_7$","abstract":"A recent $J. Am. Chem. Soc.$ Article (DOI: 10.1021/jacs.3c13094) details a pressure-temperature ($P$-$T$) phase diagram for the Ruddlesden-Popper bilayer nickelate La$_3$Ni$_2$O$_7$ (LNO-2222) using synchrotron X-ray diffraction. This study identifies a phase transition from $Amam$ (#63) to $Fmmm$ (#69) within the temperature range of 104 K to 120 K under initial pressure and attributes the $I\\rm{4/}$$mmm$ (#139) space group to the structure responsible for the superconductivity of LNO-2222. Herein, we examine the temperature-dependent structural evolution of LNO-2222 single crystals at ambient pressure. Contrary to symmetry increase and the established $Amam$-$Fmmm$ phase boundary, we observe an enhancement in the $Cmcm$ reflections as temperature decreases. This work not only establishes a benchmark method for single crystal structure studies of LNO-2222 using laboratory X-rays, but also enhances the understanding of the complex crystallographic behavior of this system, contributing insights to further experimental and theoretical explorations.","sentences":["A recent $J. Am.","Chem.","Soc.$ Article (DOI: 10.1021/jacs.3c13094) details a pressure-temperature ($P$-$T$) phase diagram for the Ruddlesden-Popper bilayer nickelate La$_3$Ni$_2$O$_7$ (LNO-2222) using synchrotron X-ray diffraction.","This study identifies a phase transition from $Amam$ (#63) to $Fmmm$ (#69) within the temperature range of 104 K to 120 K under initial pressure and attributes the $I\\rm{4/}$$mmm$ (#139) space group to the structure responsible for the superconductivity of LNO-2222.","Herein, we examine the temperature-dependent structural evolution of LNO-2222 single crystals at ambient pressure.","Contrary to symmetry increase and the established $Amam$-$Fmmm$ phase boundary, we observe an enhancement in the $Cmcm$ reflections as temperature decreases.","This work not only establishes a benchmark method for single crystal structure studies of LNO-2222 using laboratory X-rays, but also enhances the understanding of the complex crystallographic behavior of this system, contributing insights to further experimental and theoretical explorations."],"url":"http://arxiv.org/abs/2405.08802v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 17:37:01","title":"Using application conditions to rank graph transformations for graph repair","abstract":"When using graphs and graph transformations to model systems, consistency is an important concern. While consistency has primarily been viewed as a binary property, i.e., a graph is consistent or inconsistent with respect to a set of constraints, recent work has presented an approach to consistency as a graduated property. This allows living with inconsistencies for a while and repairing them when necessary. When repairing inconsistencies in a graph, we use graph transformation rules with so-called impairment- and repair-indicating application conditions to understand how much repair gain certain rule applications would bring. Both types of conditions can be derived from given graph constraints. Our main theorem shows that the difference between the number of actual constraint violations before and after a graph transformation step can be characterized by the difference between the numbers of violated impairment-indicating and repair-indicating application conditions. This theory forms the basis for algorithms with look-ahead that rank graph transformations according to their potential for graph repair. An initial evaluation shows that graph repair can be well supported by rules with these new types of application conditions.","sentences":["When using graphs and graph transformations to model systems, consistency is an important concern.","While consistency has primarily been viewed as a binary property, i.e., a graph is consistent or inconsistent with respect to a set of constraints, recent work has presented an approach to consistency as a graduated property.","This allows living with inconsistencies for a while and repairing them when necessary.","When repairing inconsistencies in a graph, we use graph transformation rules with so-called impairment- and repair-indicating application conditions to understand how much repair gain certain rule applications would bring.","Both types of conditions can be derived from given graph constraints.","Our main theorem shows that the difference between the number of actual constraint violations before and after a graph transformation step can be characterized by the difference between the numbers of violated impairment-indicating and repair-indicating application conditions.","This theory forms the basis for algorithms with look-ahead that rank graph transformations according to their potential for graph repair.","An initial evaluation shows that graph repair can be well supported by rules with these new types of application conditions."],"url":"http://arxiv.org/abs/2405.08788v1","category":"cs.SE"}
{"created":"2024-05-14 17:30:58","title":"A Hot Mess: The Rich and Complex Soft Emitting Regions Surrounding the Reflection Dominated Flaring Central Engine of Mrk 1239","abstract":"Previous X-ray works on Mrk 1239 have revealed a complex Narrow Line Seyfert 1 (NLS1) that exhibits substantial absorption and strong emission from both collisional (CIE) and photoionized (PIE) plasmas. Here, we report on deep-pointed observations with $XMM{\\rm -}Newton$ and $NuSTAR$, along with $Swift$ monitoring, to understand the $0.3-30$ keV continuum emission and the central engine geometry. A strong X-ray flare, where the AGN brightens by a factor of five in $\\sim30$ ks, is captured between $4-30$ keV and can be attributed to a brightening of the primary continuum. However, the lack of any variability below $\\sim3$ keV on long- or short-time scales requires complete absorption of the AGN continuum with a neutral medium of column density $\\sim 10^{23.5}{\\rm cm}^{-2}$. The timing and spectral properties are consistent with a blurred reflection interpretation for the primary emission. The variability and presence of a Compton hump disfavours ionized partial covering. The neutral absorber, if outflowing, could be crashing into the surrounding medium and ISM to produce the low-energy continuum and CIE. Scattered emission off the inner torus could produce the PIE. The intricate scenario is demanded by the data and highlights the complexity of the environment that is normally invisible when overwhelmed by the AGN continuum. Objects like Mrk 1239 serve as important sources for unveiling the interface between the AGN and host galaxy environments.","sentences":["Previous X-ray works on Mrk 1239 have revealed a complex Narrow Line Seyfert 1 (NLS1) that exhibits substantial absorption and strong emission from both collisional (CIE) and photoionized (PIE) plasmas.","Here, we report on deep-pointed observations with $XMM{\\rm -}Newton$ and $NuSTAR$, along with $Swift$ monitoring, to understand the $0.3-30$ keV continuum emission and the central engine geometry.","A strong X-ray flare, where the AGN brightens by a factor of five in $\\sim30$ ks, is captured between $4-30$ keV and can be attributed to a brightening of the primary continuum.","However, the lack of any variability below $\\sim3$ keV on long- or short-time scales requires complete absorption of the AGN continuum with a neutral medium of column density $\\sim 10^{23.5}{\\rm cm}^{-2}$.","The timing and spectral properties are consistent with a blurred reflection interpretation for the primary emission.","The variability and presence of a Compton hump disfavours ionized partial covering.","The neutral absorber, if outflowing, could be crashing into the surrounding medium and ISM to produce the low-energy continuum and CIE.","Scattered emission off the inner torus could produce the PIE.","The intricate scenario is demanded by the data and highlights the complexity of the environment that is normally invisible when overwhelmed by the AGN continuum.","Objects like Mrk 1239 serve as important sources for unveiling the interface between the AGN and host galaxy environments."],"url":"http://arxiv.org/abs/2405.08785v1","category":"astro-ph.HE"}
{"created":"2024-05-14 17:13:50","title":"Jacobian Regularizer-based Neural Granger Causality","abstract":"With the advancement of neural networks, diverse methods for neural Granger causality have emerged, which demonstrate proficiency in handling complex data, and nonlinear relationships. However, the existing framework of neural Granger causality has several limitations. It requires the construction of separate predictive models for each target variable, and the relationship depends on the sparsity on the weights of the first layer, resulting in challenges in effectively modeling complex relationships between variables as well as unsatisfied estimation accuracy of Granger causality. Moreover, most of them cannot grasp full-time Granger causality. To address these drawbacks, we propose a Jacobian Regularizer-based Neural Granger Causality (JRNGC) approach, a straightforward yet highly effective method for learning multivariate summary Granger causality and full-time Granger causality by constructing a single model for all target variables. Specifically, our method eliminates the sparsity constraints of weights by leveraging an input-output Jacobian matrix regularizer, which can be subsequently represented as the weighted causal matrix in the post-hoc analysis. Extensive experiments show that our proposed approach achieves competitive performance with the state-of-the-art methods for learning summary Granger causality and full-time Granger causality while maintaining lower model complexity and high scalability.","sentences":["With the advancement of neural networks, diverse methods for neural Granger causality have emerged, which demonstrate proficiency in handling complex data, and nonlinear relationships.","However, the existing framework of neural Granger causality has several limitations.","It requires the construction of separate predictive models for each target variable, and the relationship depends on the sparsity on the weights of the first layer, resulting in challenges in effectively modeling complex relationships between variables as well as unsatisfied estimation accuracy of Granger causality.","Moreover, most of them cannot grasp full-time Granger causality.","To address these drawbacks, we propose a Jacobian Regularizer-based Neural Granger Causality (JRNGC) approach, a straightforward yet highly effective method for learning multivariate summary Granger causality and full-time Granger causality by constructing a single model for all target variables.","Specifically, our method eliminates the sparsity constraints of weights by leveraging an input-output Jacobian matrix regularizer, which can be subsequently represented as the weighted causal matrix in the post-hoc analysis.","Extensive experiments show that our proposed approach achieves competitive performance with the state-of-the-art methods for learning summary Granger causality and full-time Granger causality while maintaining lower model complexity and high scalability."],"url":"http://arxiv.org/abs/2405.08779v1","category":"cs.LG"}
{"created":"2024-05-14 17:12:17","title":"Quantum Integrable Systems arising from Separation of Variables on S3","abstract":"We study the family of quantum integrable systems that arise from separating the Schr\\\"odinger equation in all 6 separable orthogonal coordinates on the 3 sphere: ellipsoidal, prolate, oblate, Lam\\'{e}, spherical and cylindrical. On the one hand each separating coordinate system gives rise to a quantum integrable system on S2 x S2, on the other hand it also leads to families of harmonic polynomials in R4. We show that separation in ellipsoidal coordinates yields a generalised Lam\\'{e} equation - a Fuchsian ODE with 5 regular singular points. We seek polynomial solutions so that the eigenfunctions are analytic at all finite singularities. We classify eigenfunctions by their discrete symmetry and compute the joint spectrum for each symmetry class. The latter 5 separable coordinate systems are all degenerations of the ellipsoidal coordinates. We perform similar analyses on these systems and show how the ODEs degenerate in a fashion akin to their respective coordinates. For the prolate system we show that there exists a defect in the joint spectrum which prohibits a global assignment of quantum numbers: the system has quantum monodromy. This is a companion paper to our previous work where the respective classical systems were studied.","sentences":["We study the family of quantum integrable systems that arise from separating the Schr\\\"odinger equation in all 6 separable orthogonal coordinates on the 3 sphere: ellipsoidal, prolate, oblate, Lam\\'{e}, spherical and cylindrical.","On the one hand each separating coordinate system gives rise to a quantum integrable system on S2 x S2, on the other hand it also leads to families of harmonic polynomials in R4.","We show that separation in ellipsoidal coordinates yields a generalised Lam\\'{e} equation - a Fuchsian ODE with 5 regular singular points.","We seek polynomial solutions so that the eigenfunctions are analytic at all finite singularities.","We classify eigenfunctions by their discrete symmetry and compute the joint spectrum for each symmetry class.","The latter 5 separable coordinate systems are all degenerations of the ellipsoidal coordinates.","We perform similar analyses on these systems and show how the ODEs degenerate in a fashion akin to their respective coordinates.","For the prolate system we show that there exists a defect in the joint spectrum which prohibits a global assignment of quantum numbers: the system has quantum monodromy.","This is a companion paper to our previous work where the respective classical systems were studied."],"url":"http://arxiv.org/abs/2405.08778v1","category":"math-ph"}
{"created":"2024-05-14 17:12:11","title":"Daydreaming Hopfield Networks and their surprising effectiveness on correlated data","abstract":"To improve the storage capacity of the Hopfield model, we develop a version of the dreaming algorithm that perpetually reinforces the patterns to be stored (as in the Hebb rule), and erases the spurious memories (as in dreaming algorithms). For this reason, we called it Daydreaming. Daydreaming is not destructive and it converges asymptotically to stationary retrieval maps. When trained on random uncorrelated examples, the model shows optimal performance in terms of the size of the basins of attraction of stored examples and the quality of reconstruction. We also train the Daydreaming algorithm on correlated data obtained via the random-features model and argue that it spontaneously exploits the correlations thus increasing even further the storage capacity and the size of the basins of attraction. Moreover, the Daydreaming algorithm is also able to stabilize the features hidden in the data. Finally, we test Daydreaming on the MNIST dataset and show that it still works surprisingly well, producing attractors that are close to unseen examples and class prototypes.","sentences":["To improve the storage capacity of the Hopfield model, we develop a version of the dreaming algorithm that perpetually reinforces the patterns to be stored (as in the Hebb rule), and erases the spurious memories (as in dreaming algorithms).","For this reason, we called it Daydreaming.","Daydreaming is not destructive and it converges asymptotically to stationary retrieval maps.","When trained on random uncorrelated examples, the model shows optimal performance in terms of the size of the basins of attraction of stored examples and the quality of reconstruction.","We also train the Daydreaming algorithm on correlated data obtained via the random-features model and argue that it spontaneously exploits the correlations thus increasing even further the storage capacity and the size of the basins of attraction.","Moreover, the Daydreaming algorithm is also able to stabilize the features hidden in the data.","Finally, we test Daydreaming on the MNIST dataset and show that it still works surprisingly well, producing attractors that are close to unseen examples and class prototypes."],"url":"http://arxiv.org/abs/2405.08777v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-14 17:10:20","title":"Quantum Entanglement Through the Lens of Paraconsistent Logic","abstract":"This paper presents an alternative approach to quantum entanglement, one that effectively resolves the logical inconsistencies without leading to logical contradictions. By addressing some of the inconsistencies within quantum mechanics, such as state superposition and non-locality, that challenge classical causal explanations, our method is constructed on the principles of paraconsistent logic. Our aim is to develop a para-consistent framework that supports the features of quantum mechanics while remaining faithful to its fundamental principles. In this pursuit, we scrutinize the philosophical and mathematical foundations of quantum mechanics in relation to classical logic systems. This method is designed to untangle theoretical puzzle spaces and promote coherence in the discussion of quantum theory. Ultimately, our approach offers a potential solution for interpreting quantum mechanics in a more coherent manner.","sentences":["This paper presents an alternative approach to quantum entanglement, one that effectively resolves the logical inconsistencies without leading to logical contradictions.","By addressing some of the inconsistencies within quantum mechanics, such as state superposition and non-locality, that challenge classical causal explanations, our method is constructed on the principles of paraconsistent logic.","Our aim is to develop a para-consistent framework that supports the features of quantum mechanics while remaining faithful to its fundamental principles.","In this pursuit, we scrutinize the philosophical and mathematical foundations of quantum mechanics in relation to classical logic systems.","This method is designed to untangle theoretical puzzle spaces and promote coherence in the discussion of quantum theory.","Ultimately, our approach offers a potential solution for interpreting quantum mechanics in a more coherent manner."],"url":"http://arxiv.org/abs/2405.08775v1","category":"quant-ph"}
{"created":"2024-05-14 17:08:19","title":"Evolution of ferroelectric properties in SmxBi1-xFeO3 via automated Piezoresponse Force Microscopy across combinatorial spread libraries","abstract":"Combinatorial spread libraries offer a unique approach to explore evolution of materials properties over the broad concentration, temperature, and growth parameter spaces. However, the traditional limitation of this approach is the requirement for the read-out of functional properties across the library. Here we demonstrate the application of automated Piezoresponse Force Microscopy (PFM) for the exploration of the physics in the SmxBi1-xFeO3 system with the ferroelectric-antiferroelectric morphotropic phase boundary. This approach relies on the synergy of the quantitative nature of PFM and the implementation of automated experiments that allows PFM-based gird sampling over macroscopic samples. The concentration dependence of pertinent ferroelectric parameters has been determined and used to develop the mathematical framework based on Ginzburg-Landau theory describing the evolution of these properties across the concentration space. We pose that combination of automated scanning probe microscope and combinatorial spread library approach will emerge as an efficient research paradigm to close the characterization gap in the high-throughput materials discovery. We make the data sets open to the community and hope that will stimulate other efforts to interpret and understand the physics of these systems.","sentences":["Combinatorial spread libraries offer a unique approach to explore evolution of materials properties over the broad concentration, temperature, and growth parameter spaces.","However, the traditional limitation of this approach is the requirement for the read-out of functional properties across the library.","Here we demonstrate the application of automated Piezoresponse Force Microscopy (PFM) for the exploration of the physics in the SmxBi1-xFeO3 system with the ferroelectric-antiferroelectric morphotropic phase boundary.","This approach relies on the synergy of the quantitative nature of PFM and the implementation of automated experiments that allows PFM-based gird sampling over macroscopic samples.","The concentration dependence of pertinent ferroelectric parameters has been determined and used to develop the mathematical framework based on Ginzburg-Landau theory describing the evolution of these properties across the concentration space.","We pose that combination of automated scanning probe microscope and combinatorial spread library approach will emerge as an efficient research paradigm to close the characterization gap in the high-throughput materials discovery.","We make the data sets open to the community and hope that will stimulate other efforts to interpret and understand the physics of these systems."],"url":"http://arxiv.org/abs/2405.08773v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 17:06:47","title":"Multi-objective SINDy for parameterized model discovery from single transient trajectory data","abstract":"The sparse identification of nonlinear dynamics (SINDy) has been established as an effective technique to produce interpretable models of dynamical systems from time-resolved state data via sparse regression. However, to model parameterized systems, SINDy requires data from transient trajectories for various parameter values over the range of interest, which are typically difficult to acquire experimentally. In this work, we extend SINDy to be able to leverage data on fixed points and/or limit cycles to reduce the number of transient trajectories needed for successful system identification. To achieve this, we incorporate the data on these attractors at various parameter values as constraints in the optimization problem. First, we show that enforcing these as hard constraints leads to an ill-conditioned regression problem due to the large number of constraints. Instead, we implement soft constraints by modifying the cost function to be minimized. This leads to the formulation of a multi-objective sparse regression problem where we simultaneously seek to minimize the error of the fit to the transients trajectories and to the data on attractors, while penalizing the number of terms in the model. Our extension, demonstrated on several numerical examples, is more robust to noisy measurements and requires substantially less training data than the original SINDy method to correctly identify a parameterized dynamical system.","sentences":["The sparse identification of nonlinear dynamics (SINDy) has been established as an effective technique to produce interpretable models of dynamical systems from time-resolved state data via sparse regression.","However, to model parameterized systems, SINDy requires data from transient trajectories for various parameter values over the range of interest, which are typically difficult to acquire experimentally.","In this work, we extend SINDy to be able to leverage data on fixed points and/or limit cycles to reduce the number of transient trajectories needed for successful system identification.","To achieve this, we incorporate the data on these attractors at various parameter values as constraints in the optimization problem.","First, we show that enforcing these as hard constraints leads to an ill-conditioned regression problem due to the large number of constraints.","Instead, we implement soft constraints by modifying the cost function to be minimized.","This leads to the formulation of a multi-objective sparse regression problem where we simultaneously seek to minimize the error of the fit to the transients trajectories and to the data on attractors, while penalizing the number of terms in the model.","Our extension, demonstrated on several numerical examples, is more robust to noisy measurements and requires substantially less training data than the original SINDy method to correctly identify a parameterized dynamical system."],"url":"http://arxiv.org/abs/2405.08771v1","category":"math.DS"}
{"created":"2024-05-14 16:39:19","title":"Self-repellent Brownian Bridges in an Interacting Bose Gas","abstract":"We consider a model of $d$-dimensional interacting quantum Bose gas, expressed in terms of an ensemble of interacting Brownian bridges in a large box and undergoing the influence of all the interactions between the legs of each of the Brownian bridges. We study the thermodynamic limit of the system and give an explicit formula for the limiting free energy and a necessary and sufficient criterion for the occurrence of a condensation phase transition. For $d\\geq 5$ and sufficiently small interaction, we prove that the condensate phase is not empty. The ideas of proof rely on the similarity of the interaction to that of the self-repellent random walk, and build on a lace expansion method conducive to treating {\\it paths} undergoing mutual repellence within each bridge.","sentences":["We consider a model of $d$-dimensional interacting quantum Bose gas, expressed in terms of an ensemble of interacting Brownian bridges in a large box and undergoing the influence of all the interactions between the legs of each of the Brownian bridges.","We study the thermodynamic limit of the system and give an explicit formula for the limiting free energy and a necessary and sufficient criterion for the occurrence of a condensation phase transition.","For $d\\geq 5$ and sufficiently small interaction, we prove that the condensate phase is not empty.","The ideas of proof rely on the similarity of the interaction to that of the self-repellent random walk, and build on a lace expansion method conducive to treating {\\it paths} undergoing mutual repellence within each bridge."],"url":"http://arxiv.org/abs/2405.08753v1","category":"math.PR"}
{"created":"2024-05-14 16:37:01","title":"Effective Field Theory Framework: Construction Strategies and Soft Collinear Effective Theory (SCET)","abstract":"Effective Field Theory (EFT) stands as a cornerstone in modern theoretical physics, offering a powerful framework for describing the dynamics of physical systems across a wide range of energy scales. This article provides an in-depth exploration of EFT and its diverse applications in various branches of physics. Beginning with a foundational overview of EFT principles, including its formulation, renormalization, and power counting rules, we delve into its versatility in addressing complex phenomena beyond the reach of traditional approaches. In particle physics, EFT techniques are indispensable for precision calculations and theoretical predictions, enabling the systematic treatment of quantum field theories at energies inaccessible to direct experimentation. Moreover, EFT plays a crucial role in understanding the low-energy dynamics of hadrons, nuclei, and other strongly interacting systems. This article aims to provide a comprehensive introduction to EFT and as an example of constructing EFT from a fundamental theory like QCD we explore how to construct a soft collinear effective theory SCET.","sentences":["Effective Field Theory (EFT) stands as a cornerstone in modern theoretical physics, offering a powerful framework for describing the dynamics of physical systems across a wide range of energy scales.","This article provides an in-depth exploration of EFT and its diverse applications in various branches of physics.","Beginning with a foundational overview of EFT principles, including its formulation, renormalization, and power counting rules, we delve into its versatility in addressing complex phenomena beyond the reach of traditional approaches.","In particle physics, EFT techniques are indispensable for precision calculations and theoretical predictions, enabling the systematic treatment of quantum field theories at energies inaccessible to direct experimentation.","Moreover, EFT plays a crucial role in understanding the low-energy dynamics of hadrons, nuclei, and other strongly interacting systems.","This article aims to provide a comprehensive introduction to EFT and as an example of constructing EFT from a fundamental theory like QCD we explore how to construct a soft collinear effective theory SCET."],"url":"http://arxiv.org/abs/2405.08752v1","category":"hep-ph"}
{"created":"2024-05-14 16:35:21","title":"From Text to Context: An Entailment Approach for News Stakeholder Classification","abstract":"Navigating the complex landscape of news articles involves understanding the various actors or entities involved, referred to as news stakeholders. These stakeholders, ranging from policymakers to opposition figures, citizens, and more, play pivotal roles in shaping news narratives. Recognizing their stakeholder types, reflecting their roles, political alignments, social standing, and more, is paramount for a nuanced comprehension of news content. Despite existing works focusing on salient entity extraction, coverage variations, and political affiliations through social media data, the automated detection of stakeholder roles within news content remains an underexplored domain. In this paper, we bridge this gap by introducing an effective approach to classify stakeholder types in news articles. Our method involves transforming the stakeholder classification problem into a natural language inference task, utilizing contextual information from news articles and external knowledge to enhance the accuracy of stakeholder type detection. Moreover, our proposed model showcases efficacy in zero-shot settings, further extending its applicability to diverse news contexts.","sentences":["Navigating the complex landscape of news articles involves understanding the various actors or entities involved, referred to as news stakeholders.","These stakeholders, ranging from policymakers to opposition figures, citizens, and more, play pivotal roles in shaping news narratives.","Recognizing their stakeholder types, reflecting their roles, political alignments, social standing, and more, is paramount for a nuanced comprehension of news content.","Despite existing works focusing on salient entity extraction, coverage variations, and political affiliations through social media data, the automated detection of stakeholder roles within news content remains an underexplored domain.","In this paper, we bridge this gap by introducing an effective approach to classify stakeholder types in news articles.","Our method involves transforming the stakeholder classification problem into a natural language inference task, utilizing contextual information from news articles and external knowledge to enhance the accuracy of stakeholder type detection.","Moreover, our proposed model showcases efficacy in zero-shot settings, further extending its applicability to diverse news contexts."],"url":"http://arxiv.org/abs/2405.08751v1","category":"cs.CL"}
{"created":"2024-05-14 16:32:11","title":"Enhancing Blind Video Quality Assessment with Rich Quality-aware Features","abstract":"In this paper, we present a simple but effective method to enhance blind video quality assessment (BVQA) models for social media videos. Motivated by previous researches that leverage pre-trained features extracted from various computer vision models as the feature representation for BVQA, we further explore rich quality-aware features from pre-trained blind image quality assessment (BIQA) and BVQA models as auxiliary features to help the BVQA model to handle complex distortions and diverse content of social media videos. Specifically, we use SimpleVQA, a BVQA model that consists of a trainable Swin Transformer-B and a fixed SlowFast, as our base model. The Swin Transformer-B and SlowFast components are responsible for extracting spatial and motion features, respectively. Then, we extract three kinds of features from Q-Align, LIQE, and FAST-VQA to capture frame-level quality-aware features, frame-level quality-aware along with scene-specific features, and spatiotemporal quality-aware features, respectively. Through concatenating these features, we employ a multi-layer perceptron (MLP) network to regress them into quality scores. Experimental results demonstrate that the proposed model achieves the best performance on three public social media VQA datasets. Moreover, the proposed model won first place in the CVPR NTIRE 2024 Short-form UGC Video Quality Assessment Challenge. The code is available at \\url{https://github.com/sunwei925/RQ-VQA.git}.","sentences":["In this paper, we present a simple but effective method to enhance blind video quality assessment (BVQA) models for social media videos.","Motivated by previous researches that leverage pre-trained features extracted from various computer vision models as the feature representation for BVQA, we further explore rich quality-aware features from pre-trained blind image quality assessment (BIQA) and BVQA models as auxiliary features to help the BVQA model to handle complex distortions and diverse content of social media videos.","Specifically, we use SimpleVQA, a BVQA model that consists of a trainable Swin Transformer-B and a fixed SlowFast, as our base model.","The Swin Transformer-B and SlowFast components are responsible for extracting spatial and motion features, respectively.","Then, we extract three kinds of features from Q-Align, LIQE, and FAST-VQA to capture frame-level quality-aware features, frame-level quality-aware along with scene-specific features, and spatiotemporal quality-aware features, respectively.","Through concatenating these features, we employ a multi-layer perceptron (MLP) network to regress them into quality scores.","Experimental results demonstrate that the proposed model achieves the best performance on three public social media VQA datasets.","Moreover, the proposed model won first place in the CVPR NTIRE 2024 Short-form UGC Video Quality Assessment Challenge.","The code is available at \\url{https://github.com/sunwei925/RQ-VQA.git}."],"url":"http://arxiv.org/abs/2405.08745v1","category":"eess.IV"}
{"created":"2024-05-14 16:31:06","title":"Robust self-testing of Bell inequalities tilted for maximal loophole-free nonlocality","abstract":"The degree of experimentally attainable nonlocality, as gauged by the amount of loophole-free violation of Bell inequalities, remains severely limited due to inefficient detectors. We address an experimentally motivated question: Which quantum strategies attain the maximal loophole-free nonlocality in the presence of inefficient detectors? For any Bell inequality and any specification of detection efficiencies, the optimal strategies are those that maximally violate a tilted version of the Bell inequality in ideal conditions. In the simplest scenario, we demonstrate that the quantum strategies that maximally violate the tilted versions of Clauser-Horne-Shimony-Holt inequality are unique up to local isometries. However, self-testing via the standard sum of squares decomposition method turns out to be analytically intractable since even high levels of the Navascu\\'es--Pironio--Ac\\'in hierarchy are insufficient to saturate the maximum quantum violation of these inequalities. Instead, we utilize a novel Jordan's lemma-based proof technique to obtain robust analytical self-testing statements for the entire family of tilted-Bell inequalities. These results allow us to unveil intriguing aspects of the effect of inefficient detectors and the complexity of characterizing the set of quantum correlations, in the simplest Bell scenario.","sentences":["The degree of experimentally attainable nonlocality, as gauged by the amount of loophole-free violation of Bell inequalities, remains severely limited due to inefficient detectors.","We address an experimentally motivated question: Which quantum strategies attain the maximal loophole-free nonlocality in the presence of inefficient detectors?","For any Bell inequality and any specification of detection efficiencies, the optimal strategies are those that maximally violate a tilted version of the Bell inequality in ideal conditions.","In the simplest scenario, we demonstrate that the quantum strategies that maximally violate the tilted versions of Clauser-Horne-Shimony-Holt inequality are unique up to local isometries.","However, self-testing via the standard sum of squares decomposition method turns out to be analytically intractable since even high levels of the Navascu\\'es--Pironio--Ac\\'in hierarchy are insufficient to saturate the maximum quantum violation of these inequalities.","Instead, we utilize a novel Jordan's lemma-based proof technique to obtain robust analytical self-testing statements for the entire family of tilted-Bell inequalities.","These results allow us to unveil intriguing aspects of the effect of inefficient detectors and the complexity of characterizing the set of quantum correlations, in the simplest Bell scenario."],"url":"http://arxiv.org/abs/2405.08743v1","category":"quant-ph"}
{"created":"2024-05-14 16:23:25","title":"Adaptive Time Stepping for a Two-Time Integro-Differential Equation in Non-Equilibrium Quantum Dynamics","abstract":"The non-equilibrium Green's function gives access to one-body observables for quantum systems. Of particular interest are quantities such as density, currents, and absorption spectra which are important for interpreting experimental results in quantum transport and spectroscopy. We present an integration scheme for the Green's function's equations of motion, the Kadanoff-Baym equations (KBE), which is both adaptive in the time integrator step size and method order as well as the history integration order. We analyze the importance of solving the KBE self-consistently and show that adapting the order of history integral evaluation is important for obtaining accurate results. To examine the efficiency of our method, we compare runtimes to a state of the art fixed time step integrator for several test systems and show an order of magnitude speedup at similar levels of accuracy.","sentences":["The non-equilibrium Green's function gives access to one-body observables for quantum systems.","Of particular interest are quantities such as density, currents, and absorption spectra which are important for interpreting experimental results in quantum transport and spectroscopy.","We present an integration scheme for the Green's function's equations of motion, the Kadanoff-Baym equations (KBE), which is both adaptive in the time integrator step size and method order as well as the history integration order.","We analyze the importance of solving the KBE self-consistently and show that adapting the order of history integral evaluation is important for obtaining accurate results.","To examine the efficiency of our method, we compare runtimes to a state of the art fixed time step integrator for several test systems and show an order of magnitude speedup at similar levels of accuracy."],"url":"http://arxiv.org/abs/2405.08737v1","category":"physics.comp-ph"}
{"created":"2024-05-14 16:23:04","title":"Polytropic Dynamical Systems with Time Singularity","abstract":"In this paper we consider a class of second order singular homogeneous differential equations called the Lane-Emden-type with time singularity in the drift coefficient. Lane-Emden equations are singular initial value problems that model phenomena in astrophysics such as stellar structure and are governed by polytropics with applications in isothermal gas spheres. A hybrid method is proposed to approximate the solution of this type of dynamic equations.","sentences":["In this paper we consider a class of second order singular homogeneous differential equations called the Lane-Emden-type with time singularity in the drift coefficient.","Lane-Emden equations are singular initial value problems that model phenomena in astrophysics such as stellar structure and are governed by polytropics with applications in isothermal gas spheres.","A hybrid method is proposed to approximate the solution of this type of dynamic equations."],"url":"http://arxiv.org/abs/2405.08736v1","category":"math.DS"}
{"created":"2024-05-14 16:21:58","title":"Competition in the nutrient-driven self-cycling fermentation process","abstract":"Self-cycling fermentation is an automated process used for culturing microorganisms. We consider a model of $n$ distinct species competing for a single non-reproducing nutrient in a self-cycling fermentor in which the nutrient level is used as the decanting condition. The model is formulated in terms of impulsive ordinary differential equations. We prove that two species are able to coexist in the fermentor under certain conditions. We also provide numerical simulations that suggest coexistence of three species is possible and that competitor-mediated coexistence can occur in this case. These results are in contrast to the chemostat, the continuous analogue, where multiple species cannot coexist on a single nonreproducing nutrient.","sentences":["Self-cycling fermentation is an automated process used for culturing microorganisms.","We consider a model of $n$ distinct species competing for a single non-reproducing nutrient in a self-cycling fermentor in which the nutrient level is used as the decanting condition.","The model is formulated in terms of impulsive ordinary differential equations.","We prove that two species are able to coexist in the fermentor under certain conditions.","We also provide numerical simulations that suggest coexistence of three species is possible and that competitor-mediated coexistence can occur in this case.","These results are in contrast to the chemostat, the continuous analogue, where multiple species cannot coexist on a single nonreproducing nutrient."],"url":"http://arxiv.org/abs/2405.08735v1","category":"q-bio.PE"}
{"created":"2024-05-14 16:05:23","title":"A regularized eigenmatrix method for unstructured sparse recovery","abstract":"The recently developed data-driven eigenmatrix method shows very promising reconstruction accuracy in sparse recovery for a wide range of kernel functions and random sample locations. However, its current implementation can lead to numerical instability if the threshold tolerance is not appropriately chosen. To incorporate regularization techniques, we propose to regularize the eigenmatrix method by replacing the computation of an ill-conditioned pseudo-inverse by the solution of an ill-conditioned least square system, which can be efficiently treated by Tikhonov regularization. Extensive numerical examples confirmed the improved effectiveness of our proposed method, especially when the noise levels are relatively high.","sentences":["The recently developed data-driven eigenmatrix method shows very promising reconstruction accuracy in sparse recovery for a wide range of kernel functions and random sample locations.","However, its current implementation can lead to numerical instability if the threshold tolerance is not appropriately chosen.","To incorporate regularization techniques, we propose to regularize the eigenmatrix method by replacing the computation of an ill-conditioned pseudo-inverse by the solution of an ill-conditioned least square system, which can be efficiently treated by Tikhonov regularization.","Extensive numerical examples confirmed the improved effectiveness of our proposed method, especially when the noise levels are relatively high."],"url":"http://arxiv.org/abs/2405.08721v1","category":"math.NA"}
{"created":"2024-05-14 15:51:52","title":"Data-driven Force Observer for Human-Robot Interaction with Series Elastic Actuators using Gaussian Processes","abstract":"Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction. Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components. While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments. In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression. By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate. Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications. We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario.","sentences":["Ensuring safety and adapting to the user's behavior are of paramount importance in physical human-robot interaction.","Thus, incorporating elastic actuators in the robot's mechanical design has become popular, since it offers intrinsic compliance and additionally provide a coarse estimate for the interaction force by measuring the deformation of the elastic components.","While observer-based methods have been shown to improve these estimates, they rely on accurate models of the system, which are challenging to obtain in complex operating environments.","In this work, we overcome this issue by learning the unknown dynamics components using Gaussian process (GP) regression.","By employing the learned model in a Bayesian filtering framework, we improve the estimation accuracy and additionally obtain an observer that explicitly considers local model uncertainty in the confidence measure of the state estimate.","Furthermore, we derive guaranteed estimation error bounds, thus, facilitating the use in safety-critical applications.","We demonstrate the effectiveness of the proposed approach experimentally in a human-exoskeleton interaction scenario."],"url":"http://arxiv.org/abs/2405.08711v1","category":"cs.RO"}
{"created":"2024-05-14 15:47:31","title":"Design and Analysis of Resilient Vehicular Platoon Systems over Wireless Networks","abstract":"Connected vehicular platoons provide a promising solution to improve traffic efficiency and ensure road safety. Vehicles in a platoon utilize on-board sensors and wireless vehicle-to-vehicle (V2V) links to share traffic information for cooperative adaptive cruise control. To process real-time control and alert information, there is a need to ensure clock synchronization among the platoon's vehicles. However, adversaries can jeopardize the operation of the platoon by attacking the local clocks of vehicles, leading to clock offsets with the platoon's reference clock. In this paper, a novel framework is proposed for analyzing the resilience of vehicular platoons that are connected using V2V links. In particular, a resilient design based on a diffusion protocol is proposed to re-synchronize the attacked vehicle through wireless V2V links thereby mitigating the impact of variance of the transmission delay during recovery. Then, a novel metric named temporal conditional mean exceedance is defined and analyzed in order to characterize the resilience of the platoon. Subsequently, the conditions pertaining to the V2V links and recovery time needed for a resilient design are derived. Numerical results show that the proposed resilient design is feasible in face of a nine-fold increase in the variance of transmission delay compared to a baseline designed for reliability. Moreover, the proposed approach improves the reliability, defined as the probability of meeting a desired clock offset error requirement, by 45% compared to the baseline.","sentences":["Connected vehicular platoons provide a promising solution to improve traffic efficiency and ensure road safety.","Vehicles in a platoon utilize on-board sensors and wireless vehicle-to-vehicle (V2V) links to share traffic information for cooperative adaptive cruise control.","To process real-time control and alert information, there is a need to ensure clock synchronization among the platoon's vehicles.","However, adversaries can jeopardize the operation of the platoon by attacking the local clocks of vehicles, leading to clock offsets with the platoon's reference clock.","In this paper, a novel framework is proposed for analyzing the resilience of vehicular platoons that are connected using V2V links.","In particular, a resilient design based on a diffusion protocol is proposed to re-synchronize the attacked vehicle through wireless V2V links thereby mitigating the impact of variance of the transmission delay during recovery.","Then, a novel metric named temporal conditional mean exceedance is defined and analyzed in order to characterize the resilience of the platoon.","Subsequently, the conditions pertaining to the V2V links and recovery time needed for a resilient design are derived.","Numerical results show that the proposed resilient design is feasible in face of a nine-fold increase in the variance of transmission delay compared to a baseline designed for reliability.","Moreover, the proposed approach improves the reliability, defined as the probability of meeting a desired clock offset error requirement, by 45% compared to the baseline."],"url":"http://arxiv.org/abs/2405.08706v1","category":"eess.SY"}
{"created":"2024-05-14 15:28:37","title":"Calculating response functions of coupled oscillators using quantum phase estimation","abstract":"We study the problem of estimating frequency response functions of systems of coupled, classical harmonic oscillators using a quantum computer. The functional form of these response functions can be mapped to a corresponding eigenproblem of a Hermitian matrix $H$, thus suggesting the use of quantum phase estimation. Our proposed quantum algorithm operates in the standard $s$-sparse, oracle-based query access model. For a network of $N$ oscillators with maximum norm $\\lVert H \\rVert_{\\mathrm{max}}$, and when the eigenvalue tolerance $\\varepsilon$ is much smaller than the minimum eigenvalue gap, we use $\\mathcal{O}(\\log(N s \\lVert H \\rVert_{\\mathrm{max}}/\\varepsilon)$ algorithmic qubits and obtain a rigorous worst-case query complexity upper bound $\\mathcal{O}(s \\lVert H \\rVert_{\\mathrm{max}}/(\\delta^2 \\varepsilon) )$ up to logarithmic factors, where $\\delta$ denotes the desired precision on the coefficients appearing in the response functions. Crucially, our proposal does not suffer from the infamous state preparation bottleneck and can as such potentially achieve large quantum speedups compared to relevant classical methods. As a proof-of-principle of exponential quantum speedup, we show that a simple adaptation of our algorithm solves the random glued-trees problem in polynomial time. We discuss practical limitations as well as potential improvements for quantifying finite size, end-to-end complexities for application to relevant instances.","sentences":["We study the problem of estimating frequency response functions of systems of coupled, classical harmonic oscillators using a quantum computer.","The functional form of these response functions can be mapped to a corresponding eigenproblem of a Hermitian matrix $H$, thus suggesting the use of quantum phase estimation.","Our proposed quantum algorithm operates in the standard $s$-sparse, oracle-based query access model.","For a network of $N$ oscillators with maximum norm $\\lVert H \\rVert_{\\mathrm{max}}$, and when the eigenvalue tolerance $\\varepsilon$ is much smaller than the minimum eigenvalue gap, we use $\\mathcal{O}(\\log(N s \\lVert H \\rVert_{\\mathrm{max}}/\\varepsilon)$ algorithmic qubits and obtain a rigorous worst-case query complexity upper bound $\\mathcal{O}(s \\lVert H \\rVert_{\\mathrm{max}}/(\\delta^2 \\varepsilon) )$ up to logarithmic factors, where $\\delta$ denotes the desired precision on the coefficients appearing in the response functions.","Crucially, our proposal does not suffer from the infamous state preparation bottleneck and can as such potentially achieve large quantum speedups compared to relevant classical methods.","As a proof-of-principle of exponential quantum speedup, we show that a simple adaptation of our algorithm solves the random glued-trees problem in polynomial time.","We discuss practical limitations as well as potential improvements for quantifying finite size, end-to-end complexities for application to relevant instances."],"url":"http://arxiv.org/abs/2405.08694v1","category":"quant-ph"}
{"created":"2024-05-14 15:24:52","title":"Enhancing Reinforcement Learning in Sensor Fusion: A Comparative Analysis of Cubature and Sampling-based Integration Methods for Rover Search Planning","abstract":"This study investigates the computational speed and accuracy of two numerical integration methods, cubature and sampling-based, for integrating an integrand over a 2D polygon. Using a group of rovers searching the Martian surface with a limited sensor footprint as a test bed, the relative error and computational time are compared as the area was subdivided to improve accuracy in the sampling-based approach. The results show that the sampling-based approach exhibits a $14.75\\%$ deviation in relative error compared to cubature when it matches the computational performance at $100\\%$. Furthermore, achieving a relative error below $1\\%$ necessitates a $10000\\%$ increase in relative time to calculate due to the $\\mathcal{O}(N^2)$ complexity of the sampling-based method. It is concluded that for enhancing reinforcement learning capabilities and other high iteration algorithms, the cubature method is preferred over the sampling-based method.","sentences":["This study investigates the computational speed and accuracy of two numerical integration methods, cubature and sampling-based, for integrating an integrand over a 2D polygon.","Using a group of rovers searching the Martian surface with a limited sensor footprint as a test bed, the relative error and computational time are compared as the area was subdivided to improve accuracy in the sampling-based approach.","The results show that the sampling-based approach exhibits a $14.75\\%$ deviation in relative error compared to cubature when it matches the computational performance at $100\\%$. Furthermore, achieving a relative error below $1\\%$ necessitates a $10000\\%$ increase in relative time to calculate due to the $\\mathcal{O}(N^2)$ complexity of the sampling-based method.","It is concluded that for enhancing reinforcement learning capabilities and other high iteration algorithms, the cubature method is preferred over the sampling-based method."],"url":"http://arxiv.org/abs/2405.08691v2","category":"cs.RO"}
{"created":"2024-05-14 15:20:57","title":"Double-activation neural network for solving parabolic equations with time delay","abstract":"This paper presents the double-activation neural network (DANN), a novel network architecture designed for solving parabolic equations with time delay. In DANN, each neuron is equipped with two activation functions to augment the network's nonlinear expressive capacity. Additionally, a new parameter is introduced for the construction of the quadratic terms in one of two activation functions, which further enhances the network's ability to capture complex nonlinear relationships. To address the issue of low fitting accuracy caused by the discontinuity of solution's derivative, a piecewise fitting approach is proposed by dividing the global solving domain into several subdomains. The convergence of the loss function is proven. Numerical results are presented to demonstrate the superior accuracy and faster convergence of DANN compared to the traditional physics-informed neural network (PINN).","sentences":["This paper presents the double-activation neural network (DANN), a novel network architecture designed for solving parabolic equations with time delay.","In DANN, each neuron is equipped with two activation functions to augment the network's nonlinear expressive capacity.","Additionally, a new parameter is introduced for the construction of the quadratic terms in one of two activation functions, which further enhances the network's ability to capture complex nonlinear relationships.","To address the issue of low fitting accuracy caused by the discontinuity of solution's derivative, a piecewise fitting approach is proposed by dividing the global solving domain into several subdomains.","The convergence of the loss function is proven.","Numerical results are presented to demonstrate the superior accuracy and faster convergence of DANN compared to the traditional physics-informed neural network (PINN)."],"url":"http://arxiv.org/abs/2405.08690v1","category":"math.NA"}
{"created":"2024-05-14 15:11:39","title":"Learning How to Dynamically Decouple","abstract":"Current quantum computers suffer from noise that stems from interactions between the quantum system that constitutes the quantum device and its environment. These interactions can be suppressed through dynamical decoupling to reduce computational errors. However, the performance of dynamical decoupling depends on the type of the system-environment interactions that are present, which often lack an accurate model in quantum devices. We show that the performance of dynamical decoupling can be improved by optimizing its rotational gates to tailor them to the quantum hardware. We find that compared to canonical decoupling sequences, such as CPMG and XY4, the optimized dynamical decoupling sequences yield the best performance in suppressing noise in superconducting qubits. Our work thus enhances existing error suppression methods which helps increase circuit depth and result quality on noisy hardware.","sentences":["Current quantum computers suffer from noise that stems from interactions between the quantum system that constitutes the quantum device and its environment.","These interactions can be suppressed through dynamical decoupling to reduce computational errors.","However, the performance of dynamical decoupling depends on the type of the system-environment interactions that are present, which often lack an accurate model in quantum devices.","We show that the performance of dynamical decoupling can be improved by optimizing its rotational gates to tailor them to the quantum hardware.","We find that compared to canonical decoupling sequences, such as CPMG and XY4, the optimized dynamical decoupling sequences yield the best performance in suppressing noise in superconducting qubits.","Our work thus enhances existing error suppression methods which helps increase circuit depth and result quality on noisy hardware."],"url":"http://arxiv.org/abs/2405.08689v1","category":"quant-ph"}
{"created":"2024-05-14 15:05:49","title":"A library of meteoroid environments encountered by spacecraft in the inner solar system","abstract":"NASA's Meteoroid Engineering Model (MEM) is designed to provide aerospace engineers with an accurate description of potentially hazardous meteoroids. It accepts a spacecraft trajectory as input and its output files describe the flux, speed, directionality, and density of microgram- to gram-sized meteoroids relative to the provided trajectory. MEM provides this information at a fairly fine level of detail in order to support detailed risk calculations. However, engineers and scientists in the very early planning stages of a mission may not yet have developed a trajectory or acquired the tools to analyze environment data. Therefore, we have developed an online library of sample MEM runs that allow new users or overloaded mission planners to get a quick feel for the characteristics of the meteoroid environment. This library provides both visualizations of these runs and input files that allow users to replicate them exactly. We also discuss the number of state vectors needed to obtain an accurate representation of the environment encountered along our sample trajectories, and outline a process for verifying that any given trajectory is adequately sampled.","sentences":["NASA's Meteoroid Engineering Model (MEM) is designed to provide aerospace engineers with an accurate description of potentially hazardous meteoroids.","It accepts a spacecraft trajectory as input and its output files describe the flux, speed, directionality, and density of microgram- to gram-sized meteoroids relative to the provided trajectory.","MEM provides this information at a fairly fine level of detail in order to support detailed risk calculations.","However, engineers and scientists in the very early planning stages of a mission may not yet have developed a trajectory or acquired the tools to analyze environment data.","Therefore, we have developed an online library of sample MEM runs that allow new users or overloaded mission planners to get a quick feel for the characteristics of the meteoroid environment.","This library provides both visualizations of these runs and input files that allow users to replicate them exactly.","We also discuss the number of state vectors needed to obtain an accurate representation of the environment encountered along our sample trajectories, and outline a process for verifying that any given trajectory is adequately sampled."],"url":"http://arxiv.org/abs/2405.08685v1","category":"astro-ph.EP"}
{"created":"2024-05-14 15:05:37","title":"Superconducting and topological properties of compound Lu$_4$H$_7$N","abstract":"A recent experiment has reported a nitrogen-doped lutetium hydride acheving a remarkable Tc of 294 K at just 1 GPa, significantly reducing the required pressure for obtaining room temperature superconductivity. However, subsequent experimental and theoretical investigations have encountered difficulties in replicating these results, leaving the structure of this Lu-H-N compound shrouded in uncertainty. Here, we propose a stable structure for Lu$_4$H$_7$N employing first-principles calculations. Our calculations reveal that Lu$_4$H$_7$N has a Tc of 1.044 K, which can be substantially enhanced to 11.721 K at 150 GPa, due to the increasing electron-phonon coupling (EPC). Notably, we delve into the nontrivial Z2 band topology of Lu$_4$H$_7$N, featuring discernible surface states near the Fermi level, and we explore its spin Hall conductivity characteristics. Furthermore, we find that the electron doping can enhance the EPC strength and Tc of Lu$_4$H$_7$N, such as the Lu$_4$H$_7$O structure we predict simulating electron doping for Lu$_4$H$_7$N with an impressive Tc of 3.837 K. This work demonstrates the coexistence of superconducting and topological properties in a Lu-H-N system compound, which holds the promise of guiding the search for novel topological superconducting materials.","sentences":["A recent experiment has reported a nitrogen-doped lutetium hydride acheving a remarkable Tc of 294 K at just 1 GPa, significantly reducing the required pressure for obtaining room temperature superconductivity.","However, subsequent experimental and theoretical investigations have encountered difficulties in replicating these results, leaving the structure of this Lu-H-N compound shrouded in uncertainty.","Here, we propose a stable structure for Lu$_4$H$_7$N employing first-principles calculations.","Our calculations reveal that Lu$_4$H$_7$N has a Tc of 1.044 K, which can be substantially enhanced to 11.721 K at 150 GPa, due to the increasing electron-phonon coupling (EPC).","Notably, we delve into the nontrivial Z2 band topology of Lu$_4$H$_7$N, featuring discernible surface states near the Fermi level, and we explore its spin Hall conductivity characteristics.","Furthermore, we find that the electron doping can enhance the EPC strength and Tc of Lu$_4$H$_7$N, such as the Lu$_4$H$_7$O structure we predict simulating electron doping for Lu$_4$H$_7$N with an impressive Tc of 3.837","K. This work demonstrates the coexistence of superconducting and topological properties in a Lu-H-N system compound, which holds the promise of guiding the search for novel topological superconducting materials."],"url":"http://arxiv.org/abs/2405.08684v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 14:44:15","title":"Near critical asymptotics in the Frozen Erd\u0151s-R\u00e9nyi","abstract":"We consider a variant of the classical Erd\\H{o}s-R\\'enyi random graph, where components with surplus are slowed down to prevent the apparition of complex components. The sizes of the components of this process undergo a similar phase transition to that of the classical model, and in the critical window the scaling limit of the sizes of the components is a \"frozen\" version of Aldous' multiplicative coalescent [2]. The aim of this article is to describe the long time asymptotics in the critical window for the total number of vertices which belong to a component with surplus.","sentences":["We consider a variant of the classical Erd\\H{o}s-R\\'enyi random graph, where components with surplus are slowed down to prevent the apparition of complex components.","The sizes of the components of this process undergo a similar phase transition to that of the classical model, and in the critical window the scaling limit of the sizes of the components is a \"frozen\" version of Aldous' multiplicative coalescent [2].","The aim of this article is to describe the long time asymptotics in the critical window for the total number of vertices which belong to a component with surplus."],"url":"http://arxiv.org/abs/2405.08664v1","category":"math.PR"}
{"created":"2024-05-14 14:44:05","title":"D-CAST: Distributed Consensus Switch in Wireless Trustworthy Autonomous System","abstract":"The protocols of distributed consensus normally aim to tolerate different types of faults including crash faults and byzantine faults that occur in the distributed systems. However, the dynamic network topology and stochastic wireless channels may cause the same trustworthy system to suffer both crash fault and byzantine fault. This article proposes the concept of a distributed consensus autonomous switch mechanism in trustworthy autonomous systems (D-CAST) to reach the different fault tolerance requirements of the dynamic nodes and discusses the challenges of D-CAST while it is implemented in the wireless trustworthy system.","sentences":["The protocols of distributed consensus normally aim to tolerate different types of faults including crash faults and byzantine faults that occur in the distributed systems.","However, the dynamic network topology and stochastic wireless channels may cause the same trustworthy system to suffer both crash fault and byzantine fault.","This article proposes the concept of a distributed consensus autonomous switch mechanism in trustworthy autonomous systems (D-CAST) to reach the different fault tolerance requirements of the dynamic nodes and discusses the challenges of D-CAST while it is implemented in the wireless trustworthy system."],"url":"http://arxiv.org/abs/2405.08663v1","category":"cs.DC"}
{"created":"2024-05-14 14:30:36","title":"The Connectedness Homomorphism between Discrete Morse Complexes","abstract":"Given two discrete Morse functions on a simplicial complex, we introduce the {\\em connectedness homomorphism} between the corresponding discrete Morse complexes. This concept leads to a novel framework for studying the connectedness in discrete Morse theory at the chain complex level. In particular, we apply it to describe a discrete analogy to `cusp-degeneration' of Morse complexes. A precise comparison between smooth case and our discrete cases is also given.","sentences":["Given two discrete Morse functions on a simplicial complex, we introduce the {\\em connectedness homomorphism} between the corresponding discrete Morse complexes.","This concept leads to a novel framework for studying the connectedness in discrete Morse theory at the chain complex level.","In particular, we apply it to describe a discrete analogy to `cusp-degeneration' of Morse complexes.","A precise comparison between smooth case and our discrete cases is also given."],"url":"http://arxiv.org/abs/2405.08653v1","category":"math.CO"}
{"created":"2024-05-14 14:28:49","title":"The computational power of discrete chemical reaction networks with bounded executions","abstract":"Chemical reaction networks (CRNs) model systems where molecules interact according to a finite set of reactions such as (A + B \\to C), representing that if a molecule of (A) and (B) collide, they disappear and a molecule of (C) is produced. CRNs can compute Boolean-valued predicates (\\phi:\\mathbb{N}^d \\to \\{0,1\\}) and integer-valued functions (f:\\mathbb{N}^d \\to \\mathbb{N}); for instance (X_1 + X_2 \\to Y) computes the function (\\min(x_1,x_2)).   We study the computational power of execution bounded CRNs, in which only a finite number of reactions can occur from the initial configuration (e.g., ruling out reversible reactions such as (A \\rightleftharpoons B)). The power and composability of such CRNs depend crucially on some other modeling choices that do not affect the computational power of CRNs with unbounded executions, namely whether an initial leader is present, and whether (for predicates) all species are required to \"vote\" for the Boolean output. If the CRN starts with an initial leader, and can allow only the leader to vote, then all semilinear predicates and functions can be stably computed in (O(n \\log n)) parallel time by execution bounded CRNs.   However, if no initial leader is allowed, all species vote, and the CRN is \"noncollapsing\" (does not shrink from initially large to final (O(1)) size configurations), then execution bounded CRNs are severely limited, able to compute only eventually constant predicates. A key tool is to characterize execution bounded CRNs as precisely those with a nonnegative linear potential function that is strictly decreased by every reaction, a result that may be of independent interest.","sentences":["Chemical reaction networks (CRNs) model systems where molecules interact according to a finite set of reactions such as (A + B \\to C), representing that if a molecule of (A) and (B) collide, they disappear and a molecule of (C) is produced.","CRNs can compute Boolean-valued predicates (\\phi:\\mathbb{N}^d \\to \\{0,1\\}) and integer-valued functions (f:\\mathbb{N}^d \\to \\mathbb{N}); for instance (X_1 + X_2 \\to Y) computes the function (\\min(x_1,x_2)).   ","We study the computational power of execution bounded CRNs, in which only a finite number of reactions can occur from the initial configuration (e.g., ruling out reversible reactions such as (A \\rightleftharpoons B)).","The power and composability of such CRNs depend crucially on some other modeling choices that do not affect the computational power of CRNs with unbounded executions, namely whether an initial leader is present, and whether (for predicates) all species are required to \"vote\" for the Boolean output.","If the CRN starts with an initial leader, and can allow only the leader to vote, then all semilinear predicates and functions can be stably computed in (O(n \\log n)) parallel time by execution bounded CRNs.   ","However, if no initial leader is allowed, all species vote, and the CRN is \"noncollapsing\" (does not shrink from initially large to final (O(1))","size configurations), then execution bounded CRNs are severely limited, able to compute only eventually constant predicates.","A key tool is to characterize execution bounded CRNs as precisely those with a nonnegative linear potential function that is strictly decreased by every reaction, a result that may be of independent interest."],"url":"http://arxiv.org/abs/2405.08649v1","category":"cs.CC"}
{"created":"2024-05-14 14:22:14","title":"Output-decomposed Learning of Mealy Machines","abstract":"We present an active automata learning algorithm which learns a decomposition of a finite state machine, based on projecting onto individual outputs. This is dual to a recent compositional learning algorithm by Labbaf et al. (2023). When projecting the outputs to a smaller set, the model itself is reduced in size. By having several such projections, we do not lose any information and the full system can be reconstructed. Depending on the structure of the system this reduces the number of queries drastically, as shown by a preliminary evaluation of the algorithm.","sentences":["We present an active automata learning algorithm which learns a decomposition of a finite state machine, based on projecting onto individual outputs.","This is dual to a recent compositional learning algorithm by Labbaf et al. (2023).","When projecting the outputs to a smaller set, the model itself is reduced in size.","By having several such projections, we do not lose any information and the full system can be reconstructed.","Depending on the structure of the system this reduces the number of queries drastically, as shown by a preliminary evaluation of the algorithm."],"url":"http://arxiv.org/abs/2405.08647v1","category":"cs.LO"}
{"created":"2024-05-14 14:21:02","title":"Cavity-enhanced superconductivity via band engineering","abstract":"We consider a two-dimensional electron gas interacting with a quantized cavity mode. We find that the coupling between the electrons and the photons in the cavity enhances the superconducting gap. Crucially, all terms in the Peierls phase are kept, in contrast to more naive approaches, which may result in spurious superradiant phase transitions. We use a mean-field theory to show that the gap increases approximately linearly with the cavity coupling strength. The effect can be observed locally as an increase in the gap size via scanning tunneling microscopy (STM) measurements for a flake of a 2D material (or for a Moir\\'e system where the enhancement is expected to be more pronounced due to a large lattice constant) interacting with a locally-structured electromagnetic field formed by split-ring resonators. Our results are also relevant for quantum optics setups with cold atoms interacting with the cavity mode, where the lattice geometry and system parameters can be tuned in a vast range.","sentences":["We consider a two-dimensional electron gas interacting with a quantized cavity mode.","We find that the coupling between the electrons and the photons in the cavity enhances the superconducting gap.","Crucially, all terms in the Peierls phase are kept, in contrast to more naive approaches, which may result in spurious superradiant phase transitions.","We use a mean-field theory to show that the gap increases approximately linearly with the cavity coupling strength.","The effect can be observed locally as an increase in the gap size via scanning tunneling microscopy (STM) measurements for a flake of a 2D material (or for a Moir\\'e system where the enhancement is expected to be more pronounced due to a large lattice constant) interacting with a locally-structured electromagnetic field formed by split-ring resonators.","Our results are also relevant for quantum optics setups with cold atoms interacting with the cavity mode, where the lattice geometry and system parameters can be tuned in a vast range."],"url":"http://arxiv.org/abs/2405.08642v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-14 14:13:57","title":"Approaches to iterative algorithms for solving nonlinear equations with an application in tomographic absorption spectroscopy","abstract":"In this paper we propose an approach for solving systems of nonlinear equations without computing function derivatives. Motivated by the application area of tomographic absorption spectroscopy, which is a highly-nonlinear problem with variables coupling, we consider a situation where straightforward translation to a fixed point problem is not possible because the operators that represent the relevant systems of nonlinear equations are not self-mappings, i.e., they operate between spaces of different dimensions. To overcome this difficulty we suggest an \"alternating common fixed points algorithm\" that acts alternatingly on the different vector variables. This approach translates the original problem to a common fixed point problem for which iterative algorithms are abound and exhibits a viable alternative to translation to an optimization problem, which usually requires derivatives information. However, to apply any of these iterative algorithms requires to ascertain the conditions that appear in their convergence theorems. To circumvent the need to verify conditions for convergence, we propose and motivate a derivative-free algorithm that better suits the tomographic absorption spectroscopy problem at hand and is even further improved by applying to it the superiorization approach. This is presented along with experimental results that demonstrate our approach.","sentences":["In this paper we propose an approach for solving systems of nonlinear equations without computing function derivatives.","Motivated by the application area of tomographic absorption spectroscopy, which is a highly-nonlinear problem with variables coupling, we consider a situation where straightforward translation to a fixed point problem is not possible because the operators that represent the relevant systems of nonlinear equations are not self-mappings, i.e., they operate between spaces of different dimensions.","To overcome this difficulty we suggest an \"alternating common fixed points algorithm\" that acts alternatingly on the different vector variables.","This approach translates the original problem to a common fixed point problem for which iterative algorithms are abound and exhibits a viable alternative to translation to an optimization problem, which usually requires derivatives information.","However, to apply any of these iterative algorithms requires to ascertain the conditions that appear in their convergence theorems.","To circumvent the need to verify conditions for convergence, we propose and motivate a derivative-free algorithm that better suits the tomographic absorption spectroscopy problem at hand and is even further improved by applying to it the superiorization approach.","This is presented along with experimental results that demonstrate our approach."],"url":"http://arxiv.org/abs/2405.08635v1","category":"math.OC"}
{"created":"2024-05-14 14:12:31","title":"On the superconducting gap structure of the miassite Rh17S15: Nodal or nodeless?","abstract":"Recent penetration depth measurement claimed the observation of unconventional superconductivity in the miassite Rh$_{17}$S$_{15}$ single crystals, evidenced by the linear-in-temperature penetration depth at low temperatures, thereby arguing for the presence of the lines of node in its superconducting gap structure. Here we measure the thermal conductivity of Rh$_{17}$S$_{15}$ single crystals down to 110 mK and up to a field of 8 T ($\\simeq 0.4H{\\rm_{c2}}$). In marked contrast to the penetration depth measurement, we observe a negligible residual linear term $\\kappa_0/T$ in zero field, in line with the nodeless gap structure. The field dependence of $\\kappa_0(H)/T$ shows a profile that is more consistent with either a highly anisotropic gap structure or multiple nodeless gaps with significantly different magnitudes. Moreover, first-principles calculations give two electronic bands with complex shape of Fermi surfaces. These results suggest multigap nodeless superconductivity in this multiband Rh$_{17}$S$_{15}$ superconductor.","sentences":["Recent penetration depth measurement claimed the observation of unconventional superconductivity in the miassite Rh$_{17}$S$_{15}$ single crystals, evidenced by the linear-in-temperature penetration depth at low temperatures, thereby arguing for the presence of the lines of node in its superconducting gap structure.","Here we measure the thermal conductivity of Rh$_{17}$S$_{15}$ single crystals down to 110 mK and up to a field of 8 T ($\\simeq 0.4H{\\rm_{c2}}$).","In marked contrast to the penetration depth measurement, we observe a negligible residual linear term $\\kappa_0/T$ in zero field, in line with the nodeless gap structure.","The field dependence of $\\kappa_0(H)/T$ shows a profile that is more consistent with either a highly anisotropic gap structure or multiple nodeless gaps with significantly different magnitudes.","Moreover, first-principles calculations give two electronic bands with complex shape of Fermi surfaces.","These results suggest multigap nodeless superconductivity in this multiband Rh$_{17}$S$_{15}$ superconductor."],"url":"http://arxiv.org/abs/2405.08633v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 14:06:36","title":"Chemical-motif characterization of short-range order with E(3)-equivariant graph neural networks","abstract":"Crystalline materials have atomic-scale fluctuations in their chemical composition that modulate various mesoscale properties. Establishing chemistry-microstructure relationships in such materials requires proper characterization of these chemical fluctuations. Yet, current characterization approaches (e.g., Warren-Cowley parameters) make only partial use of the complete chemical and structural information contained in local chemical motifs. Here we introduce a framework based on E(3)-equivariant graph neural networks that is capable of completely identifying chemical motifs in arbitrary crystalline structures with any number of chemical elements. This approach naturally leads to a proper information-theoretic measure for quantifying chemical short-range order (SRO) in chemically complex materials, and a reduced - but complete - representation of the chemical space. Our framework enables the correlation of any per-atom property with their corresponding local chemical motif, thereby offering novel avenues to explore structure-property relationships in chemically-complex materials. Using the MoTaNbTi high-entropy alloy as a test system, we demonstrate the versatility of this approach by evaluating the lattice strain associated with each chemical motif, and computing the temperature dependence of chemical-fluctuations length scale.","sentences":["Crystalline materials have atomic-scale fluctuations in their chemical composition that modulate various mesoscale properties.","Establishing chemistry-microstructure relationships in such materials requires proper characterization of these chemical fluctuations.","Yet, current characterization approaches (e.g., Warren-Cowley parameters) make only partial use of the complete chemical and structural information contained in local chemical motifs.","Here we introduce a framework based on E(3)-equivariant graph neural networks that is capable of completely identifying chemical motifs in arbitrary crystalline structures with any number of chemical elements.","This approach naturally leads to a proper information-theoretic measure for quantifying chemical short-range order (SRO) in chemically complex materials, and a reduced - but complete - representation of the chemical space.","Our framework enables the correlation of any per-atom property with their corresponding local chemical motif, thereby offering novel avenues to explore structure-property relationships in chemically-complex materials.","Using the MoTaNbTi high-entropy alloy as a test system, we demonstrate the versatility of this approach by evaluating the lattice strain associated with each chemical motif, and computing the temperature dependence of chemical-fluctuations length scale."],"url":"http://arxiv.org/abs/2405.08628v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 14:06:35","title":"Evidence of jet activity from the secondary black hole in the OJ287 binary system","abstract":"We report the study of a huge optical intraday flare on November 12, 2021, at 2 am UT, in the blazar OJ287. In the binary black hole model it is associated with an impact of the secondary black hole on the accretion disk of the primary. Our multifrequency observing campaign was set up to search for such a signature of the impact, based on a prediction made eight years earlier. The first I-band results of the flare have already been reported by \\cite{2024ApJ...960...11K}. Here we combine these data with our monitoring in the R-band. There is a big change in the R-I spectral index by $1.0\\pm0.1$ between the normal background and the flare, suggesting a new component of radiation. The polarization variation during the rise of the flare suggests the same. The limits on the source size place it most reasonably in the jet of the secondary black hole. We then ask why we have not seen this phenomenon before. We show that OJ287 was never before observed with sufficient sensitivity on the night when the flare should have happened according to the binary model. We also study the probability that this flare is just an oversized example of intraday variability, using the Krakow-dataset of intense monitoring between 2015 and 2023. We find that the occurrence of a flare of this size and rapidity is unlikely. In the Appendix, we give the full orbit-linked historical light curve of OJ287 as well as the dense monitoring sample of Krakow.","sentences":["We report the study of a huge optical intraday flare on November 12, 2021, at 2 am UT, in the blazar OJ287.","In the binary black hole model it is associated with an impact of the secondary black hole on the accretion disk of the primary.","Our multifrequency observing campaign was set up to search for such a signature of the impact, based on a prediction made eight years earlier.","The first I-band results of the flare have already been reported by \\cite{2024ApJ...960...11K}.","Here we combine these data with our monitoring in the R-band.","There is a big change in the R-I spectral index by $1.0\\pm0.1$ between the normal background and the flare, suggesting a new component of radiation.","The polarization variation during the rise of the flare suggests the same.","The limits on the source size place it most reasonably in the jet of the secondary black hole.","We then ask why we have not seen this phenomenon before.","We show that OJ287 was never before observed with sufficient sensitivity on the night when the flare should have happened according to the binary model.","We also study the probability that this flare is just an oversized example of intraday variability, using the Krakow-dataset of intense monitoring between 2015 and 2023.","We find that the occurrence of a flare of this size and rapidity is unlikely.","In the Appendix, we give the full orbit-linked historical light curve of OJ287 as well as the dense monitoring sample of Krakow."],"url":"http://arxiv.org/abs/2405.08627v1","category":"astro-ph.HE"}
{"created":"2024-05-14 14:02:29","title":"Accuracy of the Graphon Mean Field Approximation for Interacting Particle Systems","abstract":"We consider a system of $N$ particles whose interactions are characterized by a (weighted) graph $G^N$. Each particle is a node of the graph with an internal state. The state changes according to Markovian dynamics that depend on the states and connection to other particles. We study the limiting properties, focusing on the dense graph regime, where the number of neighbors of a given node grows with $N$. We show that when $G^N$ converges to a graphon $G$, the behavior of the system converges to a deterministic limit, the graphon mean field approximation. We obtain convergence rates depending on the system size $N$ and cut-norm distance between $G^N$ and $G$. We apply the results for two subcases: When $G^N$ is a discretization of the graph $G$ with individually weighted edges; when $G^N$ is a random graph obtained through edge sampling from the graphon $G$. In the case of weighted interactions, we obtain a bound of order $O(1/N)$. In the random graph case, the error is of order $O(\\sqrt{\\log(N)/N})$ with high probability. We illustrate the applicability of our results and the numerical efficiency of the approximation through two examples: a graph-based load-balancing model and a heterogeneous bike-sharing system.","sentences":["We consider a system of $N$ particles whose interactions are characterized by a (weighted) graph $G^N$. Each particle is a node of the graph with an internal state.","The state changes according to Markovian dynamics that depend on the states and connection to other particles.","We study the limiting properties, focusing on the dense graph regime, where the number of neighbors of a given node grows with $N$. We show that when $G^N$ converges to a graphon $G$, the behavior of the system converges to a deterministic limit, the graphon mean field approximation.","We obtain convergence rates depending on the system size $N$ and cut-norm distance between $G^N$ and $G$. We apply the results for two subcases: When $G^N$ is a discretization of the graph $G$ with individually weighted edges; when $G^N$ is a random graph obtained through edge sampling from the graphon $G$.","In the case of weighted interactions, we obtain a bound of order $O(1/N)$. In the random graph case, the error is of order $O(\\sqrt{\\log(N)/N})$ with high probability.","We illustrate the applicability of our results and the numerical efficiency of the approximation through two examples: a graph-based load-balancing model and a heterogeneous bike-sharing system."],"url":"http://arxiv.org/abs/2405.08623v1","category":"math.PR"}
{"created":"2024-05-14 14:02:15","title":"On sections of complex line bundles over surfaces minimizing a Ginzburg-Landau energy","abstract":"In this work we extend some of the results of Ignat and Jerrard for Ginzburg-Landau vortices of tangent vector fields on two-dimensional Riemannian manifolds to the setting of complex hermitian line bundles. In particular, we elucidate the locations of vortices for the cases of Q-tensors and their higher-rank analogs on a sphere.","sentences":["In this work we extend some of the results of Ignat and Jerrard for Ginzburg-Landau vortices of tangent vector fields on two-dimensional Riemannian manifolds to the setting of complex hermitian line bundles.","In particular, we elucidate the locations of vortices for the cases of Q-tensors and their higher-rank analogs on a sphere."],"url":"http://arxiv.org/abs/2405.08622v1","category":"math.AP"}
{"created":"2024-05-14 13:57:57","title":"Drazin and g-Drazin invertibility of combinations of three Banach algebra elements","abstract":"Consider a complex unital Banach algebra $\\mathcal{A}.$ For $x_1,x_2,x_3\\in\\mathcal{A},$ in this paper, we establish that under certain assumptions on $x_1,x_2,x_3$, Drazin (resp. g-Drazin) invertibility of any three elements among $x_1,x_2,x_3$ and $x_1+x_2+x_3\\text{ }(\\text{or }x_1x_2+x_1x_3+x_2x_3)$ ensure the Drazin (resp. g-Drazin) invertibility of the remaining one. As a consequence for two idempotents $p,q\\in\\mathcal{A},$ this result indicates the equivalence between Drazin (resp. g-Drazin) invertibility of $$\\lambda_1p+\\gamma_1q-\\lambda_1pq+\\lambda_2\\left(pqp-(pq)^2\\right)+\\cdots+\\lambda_m\\left((pq)^{m-1}p-(pq)^m\\right)$$ and $$\\lambda_1-\\lambda_1pq+\\lambda_2\\left(pqp-(pq)^2\\right)+\\cdots+\\lambda_m\\left((pq)^{m-1}p-(pq)^m\\right),$$ where $\\gamma_1,\\lambda_i\\in\\mathbb{C}$ for $i=1,2,\\cdots,m,$ with $\\lambda_1\\gamma_1\\neq0.$ Furthermore, for $x_1,x_2$, we establish that the Drazin (resp. g-Drazin) invertibility of any two elements among $x_1,x_2$ and $x_1+x_2$ indicates the Drazin (resp. g-Drazin) invertibility of the remaining one, provided that $x_1x_2=\\alpha(x_1+x_2)$ for some $\\alpha\\in\\mathbb{C}$. Additionally, if it exists, we furnish a new formula to represent the Drazin (resp. g-Drazin) inverse of any element among $x_1,x_2$ and $x_1+x_2$, by using the other two elements and their Drazin (resp. g-Drazin) inverse.","sentences":["Consider a complex unital Banach algebra $\\mathcal{A}.$ For $x_1,x_2,x_3\\in\\mathcal{A},$ in this paper, we establish that under certain assumptions on $x_1,x_2,x_3$, Drazin (resp.","g-Drazin) invertibility of any three elements among $x_1,x_2,x_3$ and $x_1+x_2+x_3\\text{ }(\\text{or }","x_1x_2+x_1x_3+x_2x_3)$ ensure the Drazin (resp.","g-Drazin) invertibility of the remaining one.","As a consequence for two idempotents $p,q\\in\\mathcal{A},$ this result indicates the equivalence between Drazin (resp.","g-Drazin) invertibility of $$\\lambda_1p+\\gamma_1q-\\lambda_1pq+\\lambda_2\\left(pqp-(pq)^2\\right)+\\cdots+\\lambda_m\\left((pq)^{m-1}p-(pq)^m\\right)$$ and $$\\lambda_1-\\lambda_1pq+\\lambda_2\\left(pqp-(pq)^2\\right)+\\cdots+\\lambda_m\\left((pq)^{m-1}p-(pq)^m\\right),$$ where $\\gamma_1,\\lambda_i\\in\\mathbb{C}$ for $i=1,2,\\cdots,m,$ with $\\lambda_1\\gamma_1\\neq0.$ Furthermore, for $x_1,x_2$, we establish that the Drazin (resp.","g-Drazin) invertibility of any two elements among $x_1,x_2$ and $x_1+x_2$ indicates the Drazin (resp.","g-Drazin) invertibility of the remaining one, provided that $x_1x_2=\\alpha(x_1+x_2)$ for some $\\alpha\\in\\mathbb{C}$. Additionally, if it exists, we furnish a new formula to represent the Drazin (resp.","g-Drazin) inverse of any element among $x_1,x_2$ and $x_1+x_2$, by using the other two elements and their Drazin (resp.","g-Drazin) inverse."],"url":"http://arxiv.org/abs/2405.08615v1","category":"math.FA"}
{"created":"2024-05-14 13:57:23","title":"FDD Massive MIMO: How to Optimally Combine UL Pilot and Limited DL CSI Feedback?","abstract":"In frequency-division duplexing (FDD) multiple-input multiple-output (MIMO) systems, obtaining accurate downlink channel state information (CSI) for precoding is vastly challenging due to the tremendous feedback overhead with the growing number of antennas. Utilizing uplink pilots for downlink CSI estimation is a promising approach that can eliminate CSI feedback. However, the downlink CSI estimation accuracy diminishes significantly as the number of channel paths increases, resulting in reduced spectral efficiency. In this paper, we demonstrate that achieving downlink spectral efficiency comparable to perfect CSI is feasible by combining uplink CSI with limited downlink CSI feedback information. Our proposed downlink CSI feedback strategy transmits quantized phase information of downlink channel paths, deviating from conventional limited methods. We put forth a mean square error (MSE)-optimal downlink channel reconstruction method by jointly exploiting the uplink CSI and the limited downlink CSI. Armed with the MSE-optimal estimator, we derive the MSE as a function of the number of feedback bits for phase quantization. Subsequently, we present an optimal feedback bit allocation method for minimizing the MSE in the reconstructed channel through phase quantization. Utilizing a robust downlink precoding technique, we establish that the proposed downlink channel reconstruction method is sufficient for attaining a sum-spectral efficiency comparable to perfect CSI.","sentences":["In frequency-division duplexing (FDD) multiple-input multiple-output (MIMO) systems, obtaining accurate downlink channel state information (CSI) for precoding is vastly challenging due to the tremendous feedback overhead with the growing number of antennas.","Utilizing uplink pilots for downlink CSI estimation is a promising approach that can eliminate CSI feedback.","However, the downlink CSI estimation accuracy diminishes significantly as the number of channel paths increases, resulting in reduced spectral efficiency.","In this paper, we demonstrate that achieving downlink spectral efficiency comparable to perfect CSI is feasible by combining uplink CSI with limited downlink CSI feedback information.","Our proposed downlink CSI feedback strategy transmits quantized phase information of downlink channel paths, deviating from conventional limited methods.","We put forth a mean square error (MSE)-optimal downlink channel reconstruction method by jointly exploiting the uplink CSI and the limited downlink CSI.","Armed with the MSE-optimal estimator, we derive the MSE as a function of the number of feedback bits for phase quantization.","Subsequently, we present an optimal feedback bit allocation method for minimizing the MSE in the reconstructed channel through phase quantization.","Utilizing a robust downlink precoding technique, we establish that the proposed downlink channel reconstruction method is sufficient for attaining a sum-spectral efficiency comparable to perfect CSI."],"url":"http://arxiv.org/abs/2405.08614v1","category":"eess.SP"}
{"created":"2024-05-14 13:51:05","title":"An analysis of the relative effects of connectivity and coupling interactions on spin networks emulating the D-Wave 2000Q quantum annealer","abstract":"From available data, we show strong positive spatial correlations in the qubits of a D-Wave 2000Q quantum annealing chip that are connected to qubits outside their own unit cell. Then, by simulating the dynamics of three different spin networks and two different initial conditions, we then show that correlation between nodes is affected by a number of factors. The different connectivity of qubits within the network means that information transfer is not straightforward even when all the qubit-qubit couplings have equal weighting. Connected nodes behave even more dissimilarly when the couplings' strength is scaled according to the physical length of the connections (here to simulate dipole-dipole interactions). This highlights the importance of understanding the architectural features and potentially unprogrammed interactions/connections that can divert the performance of a quantum system away from the idealised model of identical qubits and couplings across the chip.","sentences":["From available data, we show strong positive spatial correlations in the qubits of a D-Wave 2000Q quantum annealing chip that are connected to qubits outside their own unit cell.","Then, by simulating the dynamics of three different spin networks and two different initial conditions, we then show that correlation between nodes is affected by a number of factors.","The different connectivity of qubits within the network means that information transfer is not straightforward even when all the qubit-qubit couplings have equal weighting.","Connected nodes behave even more dissimilarly when the couplings' strength is scaled according to the physical length of the connections (here to simulate dipole-dipole interactions).","This highlights the importance of understanding the architectural features and potentially unprogrammed interactions/connections that can divert the performance of a quantum system away from the idealised model of identical qubits and couplings across the chip."],"url":"http://arxiv.org/abs/2405.08611v1","category":"quant-ph"}
{"created":"2024-05-14 13:49:31","title":"Dynamic NeRF: A Review","abstract":"Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D reconstruction and representation with a high resolution. After the first research of NeRF is proposed, NeRF has gained a robust developing power and is booming in the 3D modeling, representation and reconstruction areas. However the first and most of the followed research projects based on NeRF is static, which are weak in the practical applications. Therefore, more researcher are interested and focused on the study of dynamic NeRF that is more feasible and useful in practical applications or situations. Compared with the static NeRF, implementing the Dynamic NeRF is more difficult and complex. But Dynamic is more potential in the future even is the basic of Editable NeRF. In this review, we made a detailed and abundant statement for the development and important implementation principles of Dynamci NeRF. The analysis of main principle and development of Dynamic NeRF is from 2021 to 2023, including the most of the Dynamic NeRF projects. What is more, with colorful and novel special designed figures and table, We also made a detailed comparison and analysis of different features of various of Dynamic. Besides, we analyzed and discussed the key methods to implement a Dynamic NeRF. The volume of the reference papers is large. The statements and comparisons are multidimensional. With a reading of this review, the whole development history and most of the main design method or principles of Dynamic NeRF can be easy understood and gained.","sentences":["Neural Radiance Field(NeRF) is an novel implicit method to achieve the 3D reconstruction and representation with a high resolution.","After the first research of NeRF is proposed, NeRF has gained a robust developing power and is booming in the 3D modeling, representation and reconstruction areas.","However the first and most of the followed research projects based on NeRF is static, which are weak in the practical applications.","Therefore, more researcher are interested and focused on the study of dynamic NeRF that is more feasible and useful in practical applications or situations.","Compared with the static NeRF, implementing the Dynamic NeRF is more difficult and complex.","But Dynamic is more potential in the future even is the basic of Editable NeRF.","In this review, we made a detailed and abundant statement for the development and important implementation principles of Dynamci NeRF.","The analysis of main principle and development of Dynamic NeRF is from 2021 to 2023, including the most of the Dynamic NeRF projects.","What is more, with colorful and novel special designed figures and table, We also made a detailed comparison and analysis of different features of various of Dynamic.","Besides, we analyzed and discussed the key methods to implement a Dynamic NeRF.","The volume of the reference papers is large.","The statements and comparisons are multidimensional.","With a reading of this review, the whole development history and most of the main design method or principles of Dynamic NeRF can be easy understood and gained."],"url":"http://arxiv.org/abs/2405.08609v1","category":"cs.CV"}
{"created":"2024-05-14 13:41:39","title":"The Requirement for Cognition, in an Equation","abstract":"A model of the evolution of cognition is used to derive a Requirement Equation (RE), which defines what computations the fittest possible brain must make, or must choose actions as if it had made those computations. The terms in the RE depend on factors outside an animals brain, which can be modelled without making assumptions about how the brain works, from knowledge of the animals habitat and biology. In simple domains where the choices of actions have small information content, it may not be necessary to build internal models of reality; short cut computations may be just as good at choosing actions. In complex domains such as 3D spatial cognition, which underpins many complex choices of action, the RE implies that brains build Bayesian internal models of the animals surroundings; and that the models are constrained to be true to external reality.","sentences":["A model of the evolution of cognition is used to derive a Requirement Equation (RE), which defines what computations the fittest possible brain must make, or must choose actions as if it had made those computations.","The terms in the RE depend on factors outside an animals brain, which can be modelled without making assumptions about how the brain works, from knowledge of the animals habitat and biology.","In simple domains where the choices of actions have small information content, it may not be necessary to build internal models of reality; short cut computations may be just as good at choosing actions.","In complex domains such as 3D spatial cognition, which underpins many complex choices of action, the RE implies that brains build Bayesian internal models of the animals surroundings; and that the models are constrained to be true to external reality."],"url":"http://arxiv.org/abs/2405.08601v1","category":"q-bio.NC"}
{"created":"2024-05-14 13:41:35","title":"Stabilization and Optimal Control of Interconnected SDE - Scalar PDE System","abstract":"In this paper, we design a controller for an interconnected system consisting of a linear Stochastic Differential Equation (SDE) actuated through a linear hyperbolic Partial Differential Equation (PDE). Our approach aims to minimize the variance of the state of the SDE component. We leverage a backstepping technique to transform the original PDE into an uncoupled stochastic PDE. As such, we reformulate our initial problem as the control of a delayed SDE with a non-deterministic drift. Under standard controllability assumptions, we design a controller steering the mean of the states to zero while keeping its covariance bounded. As final step, we address the optimal control of the delayed SDE employing Artstein's transformation and Linear Quadratic stochastic control techniques.","sentences":["In this paper, we design a controller for an interconnected system consisting of a linear Stochastic Differential Equation (SDE) actuated through a linear hyperbolic Partial Differential Equation (PDE).","Our approach aims to minimize the variance of the state of the SDE component.","We leverage a backstepping technique to transform the original PDE into an uncoupled stochastic PDE.","As such, we reformulate our initial problem as the control of a delayed SDE with a non-deterministic drift.","Under standard controllability assumptions, we design a controller steering the mean of the states to zero while keeping its covariance bounded.","As final step, we address the optimal control of the delayed SDE employing Artstein's transformation and Linear Quadratic stochastic control techniques."],"url":"http://arxiv.org/abs/2405.08600v1","category":"math.OC"}
{"created":"2024-05-14 13:29:34","title":"Accelerated Alternating Direction Method of Multipliers Gradient Tracking for Distributed Optimization","abstract":"This paper presents a novel accelerated distributed algorithm for unconstrained consensus optimization over static undirected networks. The proposed algorithm combines the benefits of acceleration from momentum, the robustness of the alternating direction method of multipliers, and the computational efficiency of gradient tracking to surpass existing state-of-the-art methods in convergence speed, while preserving their computational and communication cost. First, we prove that, by applying momentum on the average dynamic consensus protocol over the estimates and gradient, we can study the algorithm as an interconnection of two singularly perturbed systems: the outer system connects the consensus variables and the optimization variables, and the inner system connects the estimates of the optimum and the auxiliary optimization variables. Next, we prove that, by adding momentum to the auxiliary dynamics, our algorithm always achieves faster convergence than the achievable linear convergence rate for the non-accelerated alternating direction method of multipliers gradient tracking algorithm case. Through simulations, we numerically show that our accelerated algorithm surpasses the existing accelerated and non-accelerated distributed consensus first-order optimization protocols in convergence speed.","sentences":["This paper presents a novel accelerated distributed algorithm for unconstrained consensus optimization over static undirected networks.","The proposed algorithm combines the benefits of acceleration from momentum, the robustness of the alternating direction method of multipliers, and the computational efficiency of gradient tracking to surpass existing state-of-the-art methods in convergence speed, while preserving their computational and communication cost.","First, we prove that, by applying momentum on the average dynamic consensus protocol over the estimates and gradient, we can study the algorithm as an interconnection of two singularly perturbed systems: the outer system connects the consensus variables and the optimization variables, and the inner system connects the estimates of the optimum and the auxiliary optimization variables.","Next, we prove that, by adding momentum to the auxiliary dynamics, our algorithm always achieves faster convergence than the achievable linear convergence rate for the non-accelerated alternating direction method of multipliers gradient tracking algorithm case.","Through simulations, we numerically show that our accelerated algorithm surpasses the existing accelerated and non-accelerated distributed consensus first-order optimization protocols in convergence speed."],"url":"http://arxiv.org/abs/2405.08590v1","category":"math.OC"}
{"created":"2024-05-14 13:24:04","title":"When Do Low-Rate Concatenated Codes Approach The Gilbert-Varshamov Bound?","abstract":"The Gilbert--Varshamov (GV) bound is a classical existential result in coding theory. It implies that a random linear binary code of rate $\\epsilon^2$ has relative distance at least $\\frac{1}{2} - O(\\epsilon)$ with high probability. However, it is a major challenge to construct explicit codes with similar parameters.   One hope to derandomize the Gilbert--Varshamov construction is with code concatenation: We begin with a (hopefully explicit) outer code ${C}_\\mathrm{out}$ over a large alphabet, and concatenate that with a small binary random linear code ${C}_\\mathrm{in}$. It is known that when we use \\emph{independent} small codes for each coordinate, then the result lies on the GV bound with high probability, but this still uses a lot of randomness. In this paper, we consider the question of whether code concatenation with a single random linear inner code ${C}_\\mathrm{in}$ can lie on the GV bound; and if so what conditions on ${C}_\\mathrm{out}$ are sufficient for this.   We show that first, there do exist linear outer codes ${C}_\\mathrm{out}$ that are \"good\" for concatenation in this sense (in fact, most linear codes codes are good). We also provide two sufficient conditions for ${C}_\\mathrm{out}$, so that if ${C}_\\mathrm{out}$ satisfies these, ${C}_\\mathrm{out}\\circ {C}_\\mathrm{in}$ will likely lie on the GV bound. We hope that these conditions may inspire future work towards constructing explicit codes ${C}_\\mathrm{out}$.","sentences":["The Gilbert--Varshamov (GV) bound is a classical existential result in coding theory.","It implies that a random linear binary code of rate $\\epsilon^2$ has relative distance at least $\\frac{1}{2} - O(\\epsilon)$ with high probability.","However, it is a major challenge to construct explicit codes with similar parameters.   ","One hope to derandomize the Gilbert--Varshamov construction is with code concatenation: We begin with a (hopefully explicit) outer code ${C}_\\mathrm{out}$ over a large alphabet, and concatenate that with a small binary random linear code ${C}_\\mathrm{in}$. It is known that when we use \\emph{independent} small codes for each coordinate, then the result lies on the GV bound with high probability, but this still uses a lot of randomness.","In this paper, we consider the question of whether code concatenation with a single random linear inner code ${C}_\\mathrm{in}$ can lie on the GV bound; and if so what conditions on ${C}_\\mathrm{out}$ are sufficient for this.   ","We show that first, there do exist linear outer codes ${C}_\\mathrm{out}$ that are \"good\" for concatenation in this sense (in fact, most linear codes codes are good).","We also provide two sufficient conditions for ${C}_\\mathrm{out}$, so that if ${C}_\\mathrm{out}$ satisfies these, ${C}_\\mathrm{out}\\circ {C}_\\mathrm{in}$ will likely lie on the GV bound.","We hope that these conditions may inspire future work towards constructing explicit codes ${C}_\\mathrm{out}$."],"url":"http://arxiv.org/abs/2405.08584v1","category":"cs.IT"}
{"created":"2024-05-14 13:22:33","title":"Treatment Effect Estimation for User Interest Exploration on Recommender Systems","abstract":"Recommender systems learn personalized user preferences from user feedback like clicks. However, user feedback is usually biased towards partially observed interests, leaving many users' hidden interests unexplored. Existing approaches typically mitigate the bias, increase recommendation diversity, or use bandit algorithms to balance exploration-exploitation trade-offs. Nevertheless, they fail to consider the potential rewards of recommending different categories of items and lack the global scheduling of allocating top-N recommendations to categories, leading to suboptimal exploration. In this work, we propose an Uplift model-based Recommender (UpliftRec) framework, which regards top-N recommendation as a treatment optimization problem. UpliftRec estimates the treatment effects, i.e., the click-through rate (CTR) under different category exposure ratios, by using observational user feedback. UpliftRec calculates group-level treatment effects to discover users' hidden interests with high CTR rewards and leverages inverse propensity weighting to alleviate confounder bias. Thereafter, UpliftRec adopts a dynamic programming method to calculate the optimal treatment for overall CTR maximization. We implement UpliftRec on different backend models and conduct extensive experiments on three datasets. The empirical results validate the effectiveness of UpliftRec in discovering users' hidden interests while achieving superior recommendation accuracy.","sentences":["Recommender systems learn personalized user preferences from user feedback like clicks.","However, user feedback is usually biased towards partially observed interests, leaving many users' hidden interests unexplored.","Existing approaches typically mitigate the bias, increase recommendation diversity, or use bandit algorithms to balance exploration-exploitation trade-offs.","Nevertheless, they fail to consider the potential rewards of recommending different categories of items and lack the global scheduling of allocating top-N recommendations to categories, leading to suboptimal exploration.","In this work, we propose an Uplift model-based Recommender (UpliftRec) framework, which regards top-N recommendation as a treatment optimization problem.","UpliftRec estimates the treatment effects, i.e., the click-through rate (CTR) under different category exposure ratios, by using observational user feedback.","UpliftRec calculates group-level treatment effects to discover users' hidden interests with high CTR rewards and leverages inverse propensity weighting to alleviate confounder bias.","Thereafter, UpliftRec adopts a dynamic programming method to calculate the optimal treatment for overall CTR maximization.","We implement UpliftRec on different backend models and conduct extensive experiments on three datasets.","The empirical results validate the effectiveness of UpliftRec in discovering users' hidden interests while achieving superior recommendation accuracy."],"url":"http://arxiv.org/abs/2405.08582v1","category":"cs.IR"}
{"created":"2024-05-14 13:22:14","title":"Nonreciprocal quantum phase transition in a spinning microwave magnonic system","abstract":"We propose how to achieve nonreciprocal quantum phase transition in a spinning microwave magnonic system composed of a spinning microwave resonator coupled with an yttrium iron garnet sphere with magnon Kerr effect. Sagnac-Fizeau shift caused by the spinning of the resonator brings about a significant modification in the critical driving strengths for second- and one-order quantum phase transitions, which means that the highly controllable quantum phase can be realized by the spinning speed of the resonator. More importantly, based on the difference in the detunings of the counterclockwise and clockwise modes induced by spinning direction of the resonator, the phase transition in this system is nonreciprocal, that is, the quantum phase transition occurs when the system is driven in one direction but not the other. Our work offers an alternative path to engineer and design nonreciprocal magnonic devices.","sentences":["We propose how to achieve nonreciprocal quantum phase transition in a spinning microwave magnonic system composed of a spinning microwave resonator coupled with an yttrium iron garnet sphere with magnon Kerr effect.","Sagnac-Fizeau shift caused by the spinning of the resonator brings about a significant modification in the critical driving strengths for second- and one-order quantum phase transitions, which means that the highly controllable quantum phase can be realized by the spinning speed of the resonator.","More importantly, based on the difference in the detunings of the counterclockwise and clockwise modes induced by spinning direction of the resonator, the phase transition in this system is nonreciprocal, that is, the quantum phase transition occurs when the system is driven in one direction but not the other.","Our work offers an alternative path to engineer and design nonreciprocal magnonic devices."],"url":"http://arxiv.org/abs/2405.08581v1","category":"quant-ph"}
{"created":"2024-05-14 13:21:00","title":"Importance of hyper-parameter optimization during training of physics-informed deep learning networks","abstract":"Incorporating scientific knowledge into deep learning (DL) models for materials-based simulations can constrain the network's predictions to be within the boundaries of the material system. Altering loss functions or adding physics-based regularization (PBR) terms to reflect material properties informs a network about the physical constraints the simulation should obey. The training and tuning process of a DL network greatly affects the quality of the model, but how this process differs when using physics-based loss functions or regularization terms is not commonly discussed. In this manuscript, several PBR methods are implemented to enforce stress equilibrium on a network predicting the stress fields of a high elastic contrast composite. Models with PBR enforced the equilibrium constraint more accurately than a model without PBR, and the stress equilibrium converged more quickly. More importantly it was observed that independently fine-tuning each implementation resulted in more accurate models. More specifically each loss formulation and dataset required different learning rates and loss weights for the best performance. This result has important implications on assessing the relative effectiveness of different DL models and highlights important considerations when making a comparison between DL methods.","sentences":["Incorporating scientific knowledge into deep learning (DL) models for materials-based simulations can constrain the network's predictions to be within the boundaries of the material system.","Altering loss functions or adding physics-based regularization (PBR) terms to reflect material properties informs a network about the physical constraints the simulation should obey.","The training and tuning process of a DL network greatly affects the quality of the model, but how this process differs when using physics-based loss functions or regularization terms is not commonly discussed.","In this manuscript, several PBR methods are implemented to enforce stress equilibrium on a network predicting the stress fields of a high elastic contrast composite.","Models with PBR enforced the equilibrium constraint more accurately than a model without PBR, and the stress equilibrium converged more quickly.","More importantly it was observed that independently fine-tuning each implementation resulted in more accurate models.","More specifically each loss formulation and dataset required different learning rates and loss weights for the best performance.","This result has important implications on assessing the relative effectiveness of different DL models and highlights important considerations when making a comparison between DL methods."],"url":"http://arxiv.org/abs/2405.08580v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 13:20:26","title":"On Lax representations under the gauge equivalence relation and Miura-type transformations for lattice equations","abstract":"We study matrix Lax representations (MLRs) for differential-difference (lattice) equations. For a given equation, two MLRs are said to be gauge equivalent if one of them can be obtained from the other by means of a matrix gauge transformation.   We present results on the following questions:   1. When is a given MLR gauge equivalent to an MLR suitable for constructing differential-difference Miura-type transformations by the method of [G. Berkeley, S. Igonin, J. Phys. A (2016), arXiv:1512.09123]?   2. When is a given MLR gauge equivalent to a trivial MLR?   Furthermore, we present new examples of integrable differential-difference equations with Miura-type transformations.","sentences":["We study matrix Lax representations (MLRs) for differential-difference (lattice) equations.","For a given equation, two MLRs are said to be gauge equivalent if one of them can be obtained from the other by means of a matrix gauge transformation.   ","We present results on the following questions:   1.","When is a given MLR gauge equivalent to an MLR suitable for constructing differential-difference Miura-type transformations by the method of [G. Berkeley, S. Igonin, J. Phys.","A (2016), arXiv:1512.09123]?   ","2.","When is a given MLR gauge equivalent to a trivial MLR?   ","Furthermore, we present new examples of integrable differential-difference equations with Miura-type transformations."],"url":"http://arxiv.org/abs/2405.08579v1","category":"nlin.SI"}
{"created":"2024-05-14 13:19:43","title":"Local-peak scale-invariant feature transform for fast and random image stitching","abstract":"Image stitching aims to construct a wide field of view with high spatial resolution, which cannot be achieved in a single exposure. Typically, conventional image stitching techniques, other than deep learning, require complex computation and thus computational pricy, especially for stitching large raw images. In this study, inspired by the multiscale feature of fluid turbulence, we developed a fast feature point detection algorithm named local-peak scale-invariant feature transform (LP-SIFT), based on the multiscale local peaks and scale-invariant feature transform method. By combining LP-SIFT and RANSAC in image stitching, the stitching speed can be improved by orders, compared with the original SIFT method. Nine large images (over 2600*1600 pixels), arranged randomly without prior knowledge, can be stitched within 158.94 s. The algorithm is highly practical for applications requiring a wide field of view in diverse application scenes, e.g., terrain mapping, biological analysis, and even criminal investigation.","sentences":["Image stitching aims to construct a wide field of view with high spatial resolution, which cannot be achieved in a single exposure.","Typically, conventional image stitching techniques, other than deep learning, require complex computation and thus computational pricy, especially for stitching large raw images.","In this study, inspired by the multiscale feature of fluid turbulence, we developed a fast feature point detection algorithm named local-peak scale-invariant feature transform (LP-SIFT), based on the multiscale local peaks and scale-invariant feature transform method.","By combining LP-SIFT and RANSAC in image stitching, the stitching speed can be improved by orders, compared with the original SIFT method.","Nine large images (over 2600*1600 pixels), arranged randomly without prior knowledge, can be stitched within 158.94 s.","The algorithm is highly practical for applications requiring a wide field of view in diverse application scenes, e.g., terrain mapping, biological analysis, and even criminal investigation."],"url":"http://arxiv.org/abs/2405.08578v1","category":"cs.CV"}
{"created":"2024-05-14 13:12:30","title":"Complexity of codes for Ramsey positive sets","abstract":"Sabok showed that the set of codes for $G_\\delta$ Ramsey positive subsets of $[\\omega]^\\omega$ is $\\mathbf{\\Sigma}^1_2$-complete. We extend this result by providing sufficient conditions for the set of codes for $G_\\delta$ Ramsey positive subsets of an arbitrary topological Ramsey space to be $\\mathbf{\\Sigma}^1_2$-complete.","sentences":["Sabok showed that the set of codes for $G_\\delta$ Ramsey positive subsets of $[\\omega]^\\omega$ is $\\mathbf{\\Sigma}^1_2$-complete.","We extend this result by providing sufficient conditions for the set of codes for $G_\\delta$ Ramsey positive subsets of an arbitrary topological Ramsey space to be $\\mathbf{\\Sigma}^1_2$-complete."],"url":"http://arxiv.org/abs/2405.08575v1","category":"math.LO"}
{"created":"2024-05-14 13:10:54","title":"ViSTooth: A Visualization Framework for Tooth Segmentation on Panoramic Radiograph","abstract":"Tooth segmentation is a key step for computer aided diagnosis of dental diseases. Numerous machine learning models have been employed for tooth segmentation on dental panoramic radiograph. However, it is a difficult task to achieve accurate tooth segmentation due to complex tooth shapes, diverse tooth categories and incomplete sample set for machine learning. In this paper, we propose ViSTooth, a visualization framework for tooth segmentation on dental panoramic radiograph. First, we employ Mask R-CNN to conduct preliminary tooth segmentation, and a set of domain metrics are proposed to estimate the accuracy of the segmented teeth, including tooth shape, tooth position and tooth angle. Then, we represent the teeth with high-dimensional vectors and visualize their distribution in a low-dimensional space, in which experts can easily observe those teeth with specific metrics. Further, we expand the sample set with the expert-specified teeth and train the tooth segmentation model iteratively. Finally, we conduct case study and expert study to demonstrate the effectiveness and usability of our ViSTooth, in aiding experts to implement accurate tooth segmentation guided by expert knowledge.","sentences":["Tooth segmentation is a key step for computer aided diagnosis of dental diseases.","Numerous machine learning models have been employed for tooth segmentation on dental panoramic radiograph.","However, it is a difficult task to achieve accurate tooth segmentation due to complex tooth shapes, diverse tooth categories and incomplete sample set for machine learning.","In this paper, we propose ViSTooth, a visualization framework for tooth segmentation on dental panoramic radiograph.","First, we employ Mask R-CNN to conduct preliminary tooth segmentation, and a set of domain metrics are proposed to estimate the accuracy of the segmented teeth, including tooth shape, tooth position and tooth angle.","Then, we represent the teeth with high-dimensional vectors and visualize their distribution in a low-dimensional space, in which experts can easily observe those teeth with specific metrics.","Further, we expand the sample set with the expert-specified teeth and train the tooth segmentation model iteratively.","Finally, we conduct case study and expert study to demonstrate the effectiveness and usability of our ViSTooth, in aiding experts to implement accurate tooth segmentation guided by expert knowledge."],"url":"http://arxiv.org/abs/2405.08573v1","category":"cs.HC"}
{"created":"2024-05-14 13:07:49","title":"Mean-field theory of first-order quantum superconductor-insulator transition","abstract":"Recent experimental studies on strongly disordered indium oxide films have revealed an unusual first-order quantum phase transition between the superconducting and insulating states (SIT). This transition is characterized by a discontinuous jump from non-zero to zero values of superfluid stiffness at the critical point, contradicting the conventional ``scaling scenario'' typically associated with SIT. In this paper, we present a theoretical framework for understanding this first-order transition. Our approach is based on the concept of competition between two fundamentally distinct ground states that arise from electron pairs initially localized by strong disorder: the superconducting state and the Coulomb glass insulator. These ground states are distinguished by two crucially different order parameters, suggesting a natural expectation of a discontinuous transition between them at $T=0$. This transition occurs when the magnitudes of the superconducting gap $\\Delta$ and the Coulomb gap $E_C$ become comparable. Additionally, we extend our analysis to low non-zero temperatures and provide a mean-field ``phase diagram'' in the plane of $(T/\\Delta,E_C/\\Delta)$. Our results reveal the existence of a natural upper bound for the kinetic inductance of strongly disordered superconductors.","sentences":["Recent experimental studies on strongly disordered indium oxide films have revealed an unusual first-order quantum phase transition between the superconducting and insulating states (SIT).","This transition is characterized by a discontinuous jump from non-zero to zero values of superfluid stiffness at the critical point, contradicting the conventional ``scaling scenario'' typically associated with SIT.","In this paper, we present a theoretical framework for understanding this first-order transition.","Our approach is based on the concept of competition between two fundamentally distinct ground states that arise from electron pairs initially localized by strong disorder: the superconducting state and the Coulomb glass insulator.","These ground states are distinguished by two crucially different order parameters, suggesting a natural expectation of a discontinuous transition between them at $T=0$. This transition occurs when the magnitudes of the superconducting gap $\\Delta$ and the Coulomb gap $E_C$ become comparable.","Additionally, we extend our analysis to low non-zero temperatures and provide a mean-field ``phase diagram'' in the plane of $(T/\\Delta,E_C/\\Delta)$. Our results reveal the existence of a natural upper bound for the kinetic inductance of strongly disordered superconductors."],"url":"http://arxiv.org/abs/2405.08571v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 12:46:34","title":"A bridge connecting convex analysis and complex analysis and $L^2$-estimate of $d$ and $\\bar\\partial$","abstract":"We propose a way to connect complex analysis and convex analysis. As applications, we derive some results about $L^2$-estimate for $d$-equation and prove some curvature positivity related to convex analysis from well known $L^2$-estimate for $\\bar\\partial$-equation or the results we prove in complex analysis.","sentences":["We propose a way to connect complex analysis and convex analysis.","As applications, we derive some results about $L^2$-estimate for $d$-equation and prove some curvature positivity related to convex analysis from well known $L^2$-estimate for $\\bar\\partial$-equation or the results we prove in complex analysis."],"url":"http://arxiv.org/abs/2405.08559v1","category":"math.CV"}
{"created":"2024-05-14 12:46:12","title":"PTPI-DL-ROMs: pre-trained physics-informed deep learning-based reduced order models for nonlinear parametrized PDEs","abstract":"The coupling of Proper Orthogonal Decomposition (POD) and deep learning-based ROMs (DL-ROMs) has proved to be a successful strategy to construct non-intrusive, highly accurate, surrogates for the real time solution of parametric nonlinear time-dependent PDEs. Inexpensive to evaluate, POD-DL-ROMs are also relatively fast to train, thanks to their limited complexity. However, POD-DL-ROMs account for the physical laws governing the problem at hand only through the training data, that are usually obtained through a full order model (FOM) relying on a high-fidelity discretization of the underlying equations. Moreover, the accuracy of POD-DL-ROMs strongly depends on the amount of available data. In this paper, we consider a major extension of POD-DL-ROMs by enforcing the fulfillment of the governing physical laws in the training process -- that is, by making them physics-informed -- to compensate for possible scarce and/or unavailable data and improve the overall reliability. To do that, we first complement POD-DL-ROMs with a trunk net architecture, endowing them with the ability to compute the problem's solution at every point in the spatial domain, and ultimately enabling a seamless computation of the physics-based loss by means of the strong continuous formulation. Then, we introduce an efficient training strategy that limits the notorious computational burden entailed by a physics-informed training phase. In particular, we take advantage of the few available data to develop a low-cost pre-training procedure; then, we fine-tune the architecture in order to further improve the prediction reliability. Accuracy and efficiency of the resulting pre-trained physics-informed DL-ROMs (PTPI-DL-ROMs) are then assessed on a set of test cases ranging from non-affinely parametrized advection-diffusion-reaction equations, to nonlinear problems like the Navier-Stokes equations for fluid flows.","sentences":["The coupling of Proper Orthogonal Decomposition (POD) and deep learning-based ROMs (DL-ROMs) has proved to be a successful strategy to construct non-intrusive, highly accurate, surrogates for the real time solution of parametric nonlinear time-dependent PDEs.","Inexpensive to evaluate, POD-DL-ROMs are also relatively fast to train, thanks to their limited complexity.","However, POD-DL-ROMs account for the physical laws governing the problem at hand only through the training data, that are usually obtained through a full order model (FOM) relying on a high-fidelity discretization of the underlying equations.","Moreover, the accuracy of POD-DL-ROMs strongly depends on the amount of available data.","In this paper, we consider a major extension of POD-DL-ROMs by enforcing the fulfillment of the governing physical laws in the training process -- that is, by making them physics-informed -- to compensate for possible scarce and/or unavailable data and improve the overall reliability.","To do that, we first complement POD-DL-ROMs with a trunk net architecture, endowing them with the ability to compute the problem's solution at every point in the spatial domain, and ultimately enabling a seamless computation of the physics-based loss by means of the strong continuous formulation.","Then, we introduce an efficient training strategy that limits the notorious computational burden entailed by a physics-informed training phase.","In particular, we take advantage of the few available data to develop a low-cost pre-training procedure; then, we fine-tune the architecture in order to further improve the prediction reliability.","Accuracy and efficiency of the resulting pre-trained physics-informed DL-ROMs (PTPI-DL-ROMs) are then assessed on a set of test cases ranging from non-affinely parametrized advection-diffusion-reaction equations, to nonlinear problems like the Navier-Stokes equations for fluid flows."],"url":"http://arxiv.org/abs/2405.08558v1","category":"math.NA"}
{"created":"2024-05-14 12:42:15","title":"Spin-spin interaction mediated by chiral phonons","abstract":"We study interaction between two magnetic impurities on top of a two dimensional insulator in the presence of chiral phonons by second-order perturbation theory. We show that this exchange interaction arises from angular momentum of phonons through spin-chiral phonon interaction of the form of spin-orbit coupling. Analytical expressions of the interaction for acoustic and optical phonons are obtained. The exchange interactions are always positive due to the bosonic nature of phonons. We find that the exchange interactions for acoustic and optical phonons show power-law decay with respect to the distance between the two magnetic impurities and are proportional to the temperature of the system at high temperature.","sentences":["We study interaction between two magnetic impurities on top of a two dimensional insulator in the presence of chiral phonons by second-order perturbation theory.","We show that this exchange interaction arises from angular momentum of phonons through spin-chiral phonon interaction of the form of spin-orbit coupling.","Analytical expressions of the interaction for acoustic and optical phonons are obtained.","The exchange interactions are always positive due to the bosonic nature of phonons.","We find that the exchange interactions for acoustic and optical phonons show power-law decay with respect to the distance between the two magnetic impurities and are proportional to the temperature of the system at high temperature."],"url":"http://arxiv.org/abs/2405.08554v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-14 12:40:58","title":"Large diversity of magnetic phases in two-dimensional magnets with spin-orbit coupling and superconductivity","abstract":"We classify the magnetic ground states of a 2D lattice of localized magnetic moments which are coupled to a superconducting substrate with Rashba-spin-orbit coupling. We discover a rich magnetic phase diagram with surprisingly complex structures including 2q-spin-spirals, a 2x2-periodic pattern, and skyrmion lattices, self-consistently, using an effective classical spin Hamiltonian and show that the system hosts non-zero 4-spin interactions. Our in-depth analysis of about ten thousand magnetic configurations becomes feasible using contrastive clustering, a recent advanced unsupervised machine learning technique. This work proposes simple few-band systems for non-collinear magnetic states and stimulates further research on topological effects in their self-consistent electronic structure.","sentences":["We classify the magnetic ground states of a 2D lattice of localized magnetic moments which are coupled to a superconducting substrate with Rashba-spin-orbit coupling.","We discover a rich magnetic phase diagram with surprisingly complex structures including 2q-spin-spirals, a 2x2-periodic pattern, and skyrmion lattices, self-consistently, using an effective classical spin Hamiltonian and show that the system hosts non-zero 4-spin interactions.","Our in-depth analysis of about ten thousand magnetic configurations becomes feasible using contrastive clustering, a recent advanced unsupervised machine learning technique.","This work proposes simple few-band systems for non-collinear magnetic states and stimulates further research on topological effects in their self-consistent electronic structure."],"url":"http://arxiv.org/abs/2405.08551v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 12:37:05","title":"Exploring Graph-based Knowledge: Multi-Level Feature Distillation via Channels Relational Graph","abstract":"In visual tasks, large teacher models capture essential features and deep information, enhancing performance. However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations. To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model. We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network. This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model. Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective. Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability.","sentences":["In visual tasks, large teacher models capture essential features and deep information, enhancing performance.","However, distilling this information into smaller student models often leads to performance loss due to structural differences and capacity limitations.","To tackle this, we propose a distillation framework based on graph knowledge, including a multi-level feature alignment strategy and an attention-guided mechanism to provide a targeted learning trajectory for the student model.","We emphasize spectral embedding (SE) as a key technique in our distillation process, which merges the student's feature space with the relational knowledge and structural complexities similar to the teacher network.","This method captures the teacher's understanding in a graph-based representation, enabling the student model to more accurately mimic the complex structural dependencies present in the teacher model.","Compared to methods that focus only on specific distillation areas, our strategy not only considers key features within the teacher model but also endeavors to capture the relationships and interactions among feature sets, encoding these complex pieces of information into a graph structure to understand and utilize the dynamic relationships among these pieces of information from a global perspective.","Experiments show that our method outperforms previous feature distillation methods on the CIFAR-100, MS-COCO, and Pascal VOC datasets, proving its efficiency and applicability."],"url":"http://arxiv.org/abs/2405.08547v1","category":"cs.CV"}
{"created":"2024-05-14 12:25:55","title":"SecScore: Enhancing the CVSS Threat Metric Group with Empirical Evidences","abstract":"Background: Timely prioritising and remediating vulnerabilities are paramount in the dynamic cybersecurity field, and one of the most widely used vulnerability scoring systems (CVSS) does not address the increasing likelihood of emerging an exploit code. Aims: We present SecScore, an innovative vulnerability severity score that enhances CVSS Threat metric group with statistical models from empirical evidences of real-world exploit codes. Method: SecScore adjusts the traditional CVSS score using an explainable and empirical method that more accurately and promptly captures the dynamics of exploit code development. Results: Our approach can integrate seamlessly into the assessment/prioritisation stage of several vulnerability management processes, improving the effectiveness of prioritisation and ensuring timely remediation. We provide real-world statistical analysis and models for a wide range of vulnerability types and platforms, demonstrating that SecScore is flexible according to the vulnerability's profile. Comprehensive experiments validate the value and timeliness of SecScore in vulnerability prioritisation. Conclusions: SecScore advances the vulnerability metrics theory and enhances organisational cybersecurity with practical insights.","sentences":["Background: Timely prioritising and remediating vulnerabilities are paramount in the dynamic cybersecurity field, and one of the most widely used vulnerability scoring systems (CVSS) does not address the increasing likelihood of emerging an exploit code.","Aims:","We present SecScore, an innovative vulnerability severity score that enhances CVSS Threat metric group with statistical models from empirical evidences of real-world exploit codes.","Method: SecScore adjusts the traditional CVSS score using an explainable and empirical method that more accurately and promptly captures the dynamics of exploit code development.","Results:","Our approach can integrate seamlessly into the assessment/prioritisation stage of several vulnerability management processes, improving the effectiveness of prioritisation and ensuring timely remediation.","We provide real-world statistical analysis and models for a wide range of vulnerability types and platforms, demonstrating that SecScore is flexible according to the vulnerability's profile.","Comprehensive experiments validate the value and timeliness of SecScore in vulnerability prioritisation.","Conclusions: SecScore advances the vulnerability metrics theory and enhances organisational cybersecurity with practical insights."],"url":"http://arxiv.org/abs/2405.08539v1","category":"cs.CR"}
{"created":"2024-05-14 12:21:08","title":"GS-PINN: Greedy Sampling for Parameter Estimation in Partial Differential Equations","abstract":"Partial differential equation parameter estimation is a mathematical and computational process used to estimate the unknown parameters in a partial differential equation model from observational data. This paper employs a greedy sampling approach based on the Discrete Empirical Interpolation Method to identify the most informative samples in a dataset associated with a partial differential equation to estimate its parameters. Greedy samples are used to train a physics-informed neural network architecture which maps the nonlinear relation between spatio-temporal data and the measured values. To prove the impact of greedy samples on the training of the physics-informed neural network for parameter estimation of a partial differential equation, their performance is compared with random samples taken from the given dataset. Our simulation results show that for all considered partial differential equations, greedy samples outperform random samples, i.e., we can estimate parameters with a significantly lower number of samples while simultaneously reducing the relative estimation error. A Python package is also prepared to support different phases of the proposed algorithm, including data prepossessing, greedy sampling, neural network training, and comparison.","sentences":["Partial differential equation parameter estimation is a mathematical and computational process used to estimate the unknown parameters in a partial differential equation model from observational data.","This paper employs a greedy sampling approach based on the Discrete Empirical Interpolation Method to identify the most informative samples in a dataset associated with a partial differential equation to estimate its parameters.","Greedy samples are used to train a physics-informed neural network architecture which maps the nonlinear relation between spatio-temporal data and the measured values.","To prove the impact of greedy samples on the training of the physics-informed neural network for parameter estimation of a partial differential equation, their performance is compared with random samples taken from the given dataset.","Our simulation results show that for all considered partial differential equations, greedy samples outperform random samples, i.e., we can estimate parameters with a significantly lower number of samples while simultaneously reducing the relative estimation error.","A Python package is also prepared to support different phases of the proposed algorithm, including data prepossessing, greedy sampling, neural network training, and comparison."],"url":"http://arxiv.org/abs/2405.08537v1","category":"math.DS"}
{"created":"2024-05-14 11:56:32","title":"How forest insect outbreaks depend on forest size and tree distribution: an individual-based model results","abstract":"In this work, an individual-based model of forest insect outbreaks is presented. The results obtained show that the outbreak is an emerging feature of the system. It is a common product of the characteristics of insects, the environment in which the insects live, and the way insects behave in it. The outbreak dynamics is an effect of scale. In a sufficiently large forest regardless of the density of trees and their spatial distribution, provided that the range of insect dispersion is large enough, it develops in the form of an outbreak. In very small forests, the dynamics becomes more chaotic. It loses the outbreak character and, especially in the forest with random tree distribution, there is a possibility that the insect population goes extinct. The local dynamics of the number of insects on one tree in a forest, where the dynamics of all insects has the character of outbreak, is characterized by a rapid increase in number and then a rapid decrease until the extinction of the local population. It is the result of the influx of immigrants from neighboring trees. The type of tree distribution in the forest becomes visible when the density of trees becomes low and/or the range of insect dispersion is small. When trees are uniformly distributed and the range of insect dispersion is small, the system persists as a set of more or less isolated local populations. In the forest with randomly distributed trees, the insect population becomes more susceptible to extinction when the tree density and/or range of insect dispersion are small.","sentences":["In this work, an individual-based model of forest insect outbreaks is presented.","The results obtained show that the outbreak is an emerging feature of the system.","It is a common product of the characteristics of insects, the environment in which the insects live, and the way insects behave in it.","The outbreak dynamics is an effect of scale.","In a sufficiently large forest regardless of the density of trees and their spatial distribution, provided that the range of insect dispersion is large enough, it develops in the form of an outbreak.","In very small forests, the dynamics becomes more chaotic.","It loses the outbreak character and, especially in the forest with random tree distribution, there is a possibility that the insect population goes extinct.","The local dynamics of the number of insects on one tree in a forest, where the dynamics of all insects has the character of outbreak, is characterized by a rapid increase in number and then a rapid decrease until the extinction of the local population.","It is the result of the influx of immigrants from neighboring trees.","The type of tree distribution in the forest becomes visible when the density of trees becomes low and/or the range of insect dispersion is small.","When trees are uniformly distributed and the range of insect dispersion is small, the system persists as a set of more or less isolated local populations.","In the forest with randomly distributed trees, the insect population becomes more susceptible to extinction when the tree density and/or range of insect dispersion are small."],"url":"http://arxiv.org/abs/2405.08523v1","category":"q-bio.PE"}
{"created":"2024-05-14 11:46:42","title":"Chemical inventory of the envelope of the Class I protostar L1551 IRS 5","abstract":"Episodic accretion in protostars leads to luminosity outbursts that end up heating their surroundings. This rise in temperature pushes the snow lines back, enabling the desorption of chemical species from dust grain surfaces, which may significantly alter the chemical history of the accreting envelope. However, a limited number of extensive chemical surveys of eruptive young stars have been performed thus far. In the present study, we carry out a large spectral survey of the binary Class I protostar L1551 IRS 5, known to be a FUor-like object, in the 3mm and 2mm bands with the IRAM-30m telescope. As a result, we detected more than 400 molecular lines. The source displays a great chemical richness with the detection of 75 species, including isotopologues. Among these species, there are 13 hydrocarbons, 25 N-bearing species, 30 O-bearing species, 15 S-bearing species, 12 deuterated molecules, and a total of 10 complex organic molecules (l-C4H2, CH3CCH, CH2DCCH, CH3CHO, CH3CN, CH3OCH3, CH3OCHO, CH3OH, CH2DOH, and HC5N). With the help of local thermodynamic equilibrium (LTE) and non-LTE models, we determined the column densities of most molecules as well as excitation and kinetic temperatures. While most of those molecules trace the cold envelope (< 20 K), the OCS and CH3OH emission arise from the warm (> 100 K) innermost (< 2'' ) regions. We compared the chemical inventory of L1551 IRS 5 and its column density ratios, including isotopic ratios, with other protostellar sources. A broad chemical diversity is seen among Class I objects. More observations with both single-dish telescopes and interferometers are needed to characterize the diversity in a larger sample of protostars, while more astrochemical models would help explain this diversity, in addition to the impact of luminosity outbursts on the chemistry of protostellar envelopes.","sentences":["Episodic accretion in protostars leads to luminosity outbursts that end up heating their surroundings.","This rise in temperature pushes the snow lines back, enabling the desorption of chemical species from dust grain surfaces, which may significantly alter the chemical history of the accreting envelope.","However, a limited number of extensive chemical surveys of eruptive young stars have been performed thus far.","In the present study, we carry out a large spectral survey of the binary Class I protostar L1551","IRS 5, known to be a FUor-like object, in the 3mm and 2mm bands with the IRAM-30m telescope.","As a result, we detected more than 400 molecular lines.","The source displays a great chemical richness with the detection of 75 species, including isotopologues.","Among these species, there are 13 hydrocarbons, 25 N-bearing species, 30 O-bearing species, 15 S-bearing species, 12 deuterated molecules, and a total of 10 complex organic molecules (l-C4H2, CH3CCH, CH2DCCH, CH3CHO, CH3CN, CH3OCH3, CH3OCHO, CH3OH, CH2DOH, and HC5N).","With the help of local thermodynamic equilibrium (LTE) and non-LTE models, we determined the column densities of most molecules as well as excitation and kinetic temperatures.","While most of those molecules trace the cold envelope (< 20 K), the OCS and CH3OH emission arise from the warm (> 100 K) innermost (< 2'' ) regions.","We compared the chemical inventory of L1551 IRS 5 and its column density ratios, including isotopic ratios, with other protostellar sources.","A broad chemical diversity is seen among Class I objects.","More observations with both single-dish telescopes and interferometers are needed to characterize the diversity in a larger sample of protostars, while more astrochemical models would help explain this diversity, in addition to the impact of luminosity outbursts on the chemistry of protostellar envelopes."],"url":"http://arxiv.org/abs/2405.08517v2","category":"astro-ph.GA"}
{"created":"2024-05-14 11:37:26","title":"Falcon 7b for Software Mention Detection in Scholarly Documents","abstract":"This paper aims to tackle the challenge posed by the increasing integration of software tools in research across various disciplines by investigating the application of Falcon-7b for the detection and classification of software mentions within scholarly texts. Specifically, the study focuses on solving Subtask I of the Software Mention Detection in Scholarly Publications (SOMD), which entails identifying and categorizing software mentions from academic literature. Through comprehensive experimentation, the paper explores different training strategies, including a dual-classifier approach, adaptive sampling, and weighted loss scaling, to enhance detection accuracy while overcoming the complexities of class imbalance and the nuanced syntax of scholarly writing. The findings highlight the benefits of selective labelling and adaptive sampling in improving the model's performance. However, they also indicate that integrating multiple strategies does not necessarily result in cumulative improvements. This research offers insights into the effective application of large language models for specific tasks such as SOMD, underlining the importance of tailored approaches to address the unique challenges presented by academic text analysis.","sentences":["This paper aims to tackle the challenge posed by the increasing integration of software tools in research across various disciplines by investigating the application of Falcon-7b for the detection and classification of software mentions within scholarly texts.","Specifically, the study focuses on solving Subtask I of the Software Mention Detection in Scholarly Publications (SOMD), which entails identifying and categorizing software mentions from academic literature.","Through comprehensive experimentation, the paper explores different training strategies, including a dual-classifier approach, adaptive sampling, and weighted loss scaling, to enhance detection accuracy while overcoming the complexities of class imbalance and the nuanced syntax of scholarly writing.","The findings highlight the benefits of selective labelling and adaptive sampling in improving the model's performance.","However, they also indicate that integrating multiple strategies does not necessarily result in cumulative improvements.","This research offers insights into the effective application of large language models for specific tasks such as SOMD, underlining the importance of tailored approaches to address the unique challenges presented by academic text analysis."],"url":"http://arxiv.org/abs/2405.08514v1","category":"cs.LG"}
{"created":"2024-05-14 11:36:33","title":"CFM6, a closed-form NLI EGN model supporting multiband transmission with arbitrary Raman amplification","abstract":"We formulated a closed-form EGN model for nonlinear interference in ultra-wideband optical systems with arbitrary Raman amplification. This model enhanced the CISCO-POLITO-CFM5 performance by introducing a novel contribution attributed to the backward Raman amplification. It can handle the frequency-dependent fiber parameters and inter-channel stimulated Raman scattering.","sentences":["We formulated a closed-form EGN model for nonlinear interference in ultra-wideband optical systems with arbitrary Raman amplification.","This model enhanced the CISCO-POLITO-CFM5 performance by introducing a novel contribution attributed to the backward Raman amplification.","It can handle the frequency-dependent fiber parameters and inter-channel stimulated Raman scattering."],"url":"http://arxiv.org/abs/2405.08512v1","category":"eess.SP"}
{"created":"2024-05-14 11:17:40","title":"From linear programming to colliding particles","abstract":"Although simplices are trivial from a linear optimization standpoint, the simplex algorithm can exhibit quite complex behavior. In this paper we study the behavior of max-slope pivot rules on (products of) simplices and describe the associated pivot rule polytopes. For simplices, the pivot rule polytopes are combinatorially isomorphic to associahedra. To prove this correspondence, we interpret max-slope pivot rules in terms of the combinatorics of colliding particles on a line. For prisms over simplices, we recover Stasheff's multiplihedra. For products of two simplices we get new realizations of constrainahedra, that capture the combinatorics of certain particle systems in the plane.","sentences":["Although simplices are trivial from a linear optimization standpoint, the simplex algorithm can exhibit quite complex behavior.","In this paper we study the behavior of max-slope pivot rules on (products of) simplices and describe the associated pivot rule polytopes.","For simplices, the pivot rule polytopes are combinatorially isomorphic to associahedra.","To prove this correspondence, we interpret max-slope pivot rules in terms of the combinatorics of colliding particles on a line.","For prisms over simplices, we recover Stasheff's multiplihedra.","For products of two simplices we get new realizations of constrainahedra, that capture the combinatorics of certain particle systems in the plane."],"url":"http://arxiv.org/abs/2405.08506v1","category":"math.CO"}
{"created":"2024-05-14 10:36:56","title":"Rethinking Scanning Strategies with Vision Mamba in Semantic Segmentation of Remote Sensing Imagery: An Experimental Study","abstract":"Deep learning methods, especially Convolutional Neural Networks (CNN) and Vision Transformer (ViT), are frequently employed to perform semantic segmentation of high-resolution remotely sensed images. However, CNNs are constrained by their restricted receptive fields, while ViTs face challenges due to their quadratic complexity. Recently, the Mamba model, featuring linear complexity and a global receptive field, has gained extensive attention for vision tasks. In such tasks, images need to be serialized to form sequences compatible with the Mamba model. Numerous research efforts have explored scanning strategies to serialize images, aiming to enhance the Mamba model's understanding of images. However, the effectiveness of these scanning strategies remains uncertain. In this research, we conduct a comprehensive experimental investigation on the impact of mainstream scanning directions and their combinations on semantic segmentation of remotely sensed images. Through extensive experiments on the LoveDA, ISPRS Potsdam, and ISPRS Vaihingen datasets, we demonstrate that no single scanning strategy outperforms others, regardless of their complexity or the number of scanning directions involved. A simple, single scanning direction is deemed sufficient for semantic segmentation of high-resolution remotely sensed images. Relevant directions for future research are also recommended.","sentences":["Deep learning methods, especially Convolutional Neural Networks (CNN) and Vision Transformer (ViT), are frequently employed to perform semantic segmentation of high-resolution remotely sensed images.","However, CNNs are constrained by their restricted receptive fields, while ViTs face challenges due to their quadratic complexity.","Recently, the Mamba model, featuring linear complexity and a global receptive field, has gained extensive attention for vision tasks.","In such tasks, images need to be serialized to form sequences compatible with the Mamba model.","Numerous research efforts have explored scanning strategies to serialize images, aiming to enhance the Mamba model's understanding of images.","However, the effectiveness of these scanning strategies remains uncertain.","In this research, we conduct a comprehensive experimental investigation on the impact of mainstream scanning directions and their combinations on semantic segmentation of remotely sensed images.","Through extensive experiments on the LoveDA, ISPRS Potsdam, and ISPRS Vaihingen datasets, we demonstrate that no single scanning strategy outperforms others, regardless of their complexity or the number of scanning directions involved.","A simple, single scanning direction is deemed sufficient for semantic segmentation of high-resolution remotely sensed images.","Relevant directions for future research are also recommended."],"url":"http://arxiv.org/abs/2405.08493v1","category":"cs.CV"}
{"created":"2024-05-14 10:34:37","title":"Intraseasonal synchronization of extreme rainfalls between North India and the Sahel","abstract":"The Indian Summer Monsoon (ISM) and the West African Monsoon (WAM) are dominant drivers of boreal summer precipitation variability in tropical and subtropical regions. Although the regional precipitation dynamics in these two regions have been extensively studied, the intraseasonal interactions between the ISM and WAM remain poorly understood. Here, we employ a climate network approach based on extreme rainfall events to uncover synchronously occurring extreme rainfall patterns across the two monsoon systems. We reveal strong synchronization of extreme rainfall events during the peak monsoon period in July and August, linking heavy rainfall over North India to that over the Sahel with a lag of around 12 days. We find that La Ni\\~na-like conditions in combination with the Boreal Summer Intraseasonal Oscillation and an enhanced Tropical Easterly Jet (TEJ) foster the synchronization between the ISM and the WAM. Convective clouds are transported by an intensified TEJ from southwestern Asia toward North Africa, supporting anomalous deep convection over the Sahel region.","sentences":["The Indian Summer Monsoon (ISM) and the West African Monsoon (WAM) are dominant drivers of boreal summer precipitation variability in tropical and subtropical regions.","Although the regional precipitation dynamics in these two regions have been extensively studied, the intraseasonal interactions between the ISM and WAM remain poorly understood.","Here, we employ a climate network approach based on extreme rainfall events to uncover synchronously occurring extreme rainfall patterns across the two monsoon systems.","We reveal strong synchronization of extreme rainfall events during the peak monsoon period in July and August, linking heavy rainfall over North India to that over the Sahel with a lag of around 12 days.","We find that La Ni\\~na-like conditions in combination with the Boreal Summer Intraseasonal Oscillation and an enhanced Tropical Easterly Jet (TEJ) foster the synchronization between the ISM and the WAM.","Convective clouds are transported by an intensified TEJ from southwestern Asia toward North Africa, supporting anomalous deep convection over the Sahel region."],"url":"http://arxiv.org/abs/2405.08492v1","category":"physics.ao-ph"}
{"created":"2024-05-14 10:30:25","title":"Magnetic shielding simulation for particle detection","abstract":"Cherenkov-type particle detectors or scintillators use as a fundamental element photomultiplier tubes, whose efficiency decreases when subjected to the Earth's magnetic field. This work develops a geomagnetic field compensation system based on coils for large scale cylindrical detectors. The effect of different parameters such as the size of the detector, the distance between coils or the magnetic field strength on the compensation using a basic coil system composed of circular and rectangular coils is studied. The addition of coils of very specific geometry and position to the basic configuration is proposed in order to address the compensation in the areas of the detector where it is more difficult to influence, in order to minimize the loss of efficiency. With such improvement, in the considered simulated system, more than 99.5% of the photomultiplier tubes in the detector experience an efficiency loss of less than 1% due to the effect of the magnetic fields.","sentences":["Cherenkov-type particle detectors or scintillators use as a fundamental element photomultiplier tubes, whose efficiency decreases when subjected to the Earth's magnetic field.","This work develops a geomagnetic field compensation system based on coils for large scale cylindrical detectors.","The effect of different parameters such as the size of the detector, the distance between coils or the magnetic field strength on the compensation using a basic coil system composed of circular and rectangular coils is studied.","The addition of coils of very specific geometry and position to the basic configuration is proposed in order to address the compensation in the areas of the detector where it is more difficult to influence, in order to minimize the loss of efficiency.","With such improvement, in the considered simulated system, more than 99.5% of the photomultiplier tubes in the detector experience an efficiency loss of less than 1% due to the effect of the magnetic fields."],"url":"http://arxiv.org/abs/2405.08491v1","category":"physics.ins-det"}
{"created":"2024-05-14 10:12:47","title":"Universal replication of chaotic characteristics by classical and quantum machine learning","abstract":"Replicating chaotic characteristics of non-linear dynamics by machine learning (ML) has recently drawn wide attentions. In this work, we propose that a ML model, trained to predict the state one-step-ahead from several latest historic states, can accurately replicate the bifurcation diagram and the Lyapunov exponents of discrete dynamic systems. The characteristics for different values of the hyper-parameters are captured universally by a single ML model, while the previous works considered training the ML model independently by fixing the hyper-parameters to be specific values. Our benchmarks on the one- and two-dimensional Logistic maps show that variational quantum circuit can reproduce the long-term characteristics with higher accuracy than the long short-term memory (a well-recognized classical ML model). Our work reveals an essential difference between the ML for the chaotic characteristics and that for standard tasks, from the perspective of the relation between performance and model complexity. Our results suggest that quantum circuit model exhibits potential advantages on mitigating over-fitting, achieving higher accuracy and stability.","sentences":["Replicating chaotic characteristics of non-linear dynamics by machine learning (ML) has recently drawn wide attentions.","In this work, we propose that a ML model, trained to predict the state one-step-ahead from several latest historic states, can accurately replicate the bifurcation diagram and the Lyapunov exponents of discrete dynamic systems.","The characteristics for different values of the hyper-parameters are captured universally by a single ML model, while the previous works considered training the ML model independently by fixing the hyper-parameters to be specific values.","Our benchmarks on the one- and two-dimensional Logistic maps show that variational quantum circuit can reproduce the long-term characteristics with higher accuracy than the long short-term memory (a well-recognized classical ML model).","Our work reveals an essential difference between the ML for the chaotic characteristics and that for standard tasks, from the perspective of the relation between performance and model complexity.","Our results suggest that quantum circuit model exhibits potential advantages on mitigating over-fitting, achieving higher accuracy and stability."],"url":"http://arxiv.org/abs/2405.08484v1","category":"quant-ph"}
{"created":"2024-05-14 10:07:58","title":"A Survey on Complexity Measures of Pseudo-Random Sequences","abstract":"Since the introduction of the Kolmogorov complexity of binary sequences in the 1960s, there have been significant advancements in the topic of complexity measures for randomness assessment, which are of fundamental importance in theoretical computer science and of practical interest in cryptography. This survey reviews notable research from the past four decades on the linear, quadratic and maximum-order complexities of pseudo-random sequences and their relations with Lempel-Ziv complexity, expansion complexity, 2-adic complexity, and correlation measures.","sentences":["Since the introduction of the Kolmogorov complexity of binary sequences in the 1960s, there have been significant advancements in the topic of complexity measures for randomness assessment, which are of fundamental importance in theoretical computer science and of practical interest in cryptography.","This survey reviews notable research from the past four decades on the linear, quadratic and maximum-order complexities of pseudo-random sequences and their relations with Lempel-Ziv complexity, expansion complexity, 2-adic complexity, and correlation measures."],"url":"http://arxiv.org/abs/2405.08479v1","category":"cs.CR"}
{"created":"2024-05-14 09:54:52","title":"Impurity Parallel Velocity Gradient instability","abstract":"In magnetized plasmas, a radial gradient of parallel velocity, where parallel refers to the direction of magnetic field, can destabilise an electrostatic mode called as Parallel Velocity Gradient (PVG). The theory of PVG has been mainly developed assuming a single species of ions. Here, the role of impurities is investigated based on a linear, local analysis, in a homogeneous, constant magnetic field. To further simplify the analysis, the plasma is assumed to contain only two ion species - main ions and one impurity species - while our methodology can be straightforwardly extended to more species. In the cold-ion limit, retaining polarization drift for both main ions and impurity ions, and assuming Boltzmanian electrons, the system is described by 4 fluid equations closed by quasineutrality. The linearized equations can be reduced to 2 coupled equations: one for the electric potential, and one for the effective parallel velocity fluctuations, which is a linear combination of main ion and impurity parallel velocity fluctuations. This reduced system can be understood as a generalisation of the Hasegawa-Mima model. With finite radial gradient of impurity parallel flow, the linear dispersion relation then describes a new instability: the impurity PVG (i-PVG). Instability condition is described in terms of either the main ion flow shear, or equivalently, an effective flow shear, which combines main ion and impurity flow shears. Impurities can have a stabilising or destabilising role, depending on the parameters, and in particular the direction of main flow shear against impurity flow shear. Assuming a reasonable value of perpendicular wavenumber, the maximum growthrate is estimated, depending on impurity mass, charge, and concentration.","sentences":["In magnetized plasmas, a radial gradient of parallel velocity, where parallel refers to the direction of magnetic field, can destabilise an electrostatic mode called as Parallel Velocity Gradient (PVG).","The theory of PVG has been mainly developed assuming a single species of ions.","Here, the role of impurities is investigated based on a linear, local analysis, in a homogeneous, constant magnetic field.","To further simplify the analysis, the plasma is assumed to contain only two ion species - main ions and one impurity species - while our methodology can be straightforwardly extended to more species.","In the cold-ion limit, retaining polarization drift for both main ions and impurity ions, and assuming Boltzmanian electrons, the system is described by 4 fluid equations closed by quasineutrality.","The linearized equations can be reduced to 2 coupled equations: one for the electric potential, and one for the effective parallel velocity fluctuations, which is a linear combination of main ion and impurity parallel velocity fluctuations.","This reduced system can be understood as a generalisation of the Hasegawa-Mima model.","With finite radial gradient of impurity parallel flow, the linear dispersion relation then describes a new instability: the impurity PVG (i-PVG).","Instability condition is described in terms of either the main ion flow shear, or equivalently, an effective flow shear, which combines main ion and impurity flow shears.","Impurities can have a stabilising or destabilising role, depending on the parameters, and in particular the direction of main flow shear against impurity flow shear.","Assuming a reasonable value of perpendicular wavenumber, the maximum growthrate is estimated, depending on impurity mass, charge, and concentration."],"url":"http://arxiv.org/abs/2405.08472v1","category":"physics.plasm-ph"}
{"created":"2024-05-14 09:01:41","title":"Unveiling quantum phase transitions from traps in variational quantum algorithms","abstract":"Understanding quantum phase transitions in physical systems is fundamental to characterize their behaviour at small temperatures. Achieving this requires both accessing good approximations to the ground state and identifying order parameters to distinguish different phases. Addressing these challenges, our work introduces a hybrid algorithm that combines quantum optimization with classical machine learning. This approach leverages the capability of near-term quantum computers to prepare locally trapped states through finite optimization. Specifically, we utilize LASSO for identifying conventional phase transitions and the Transformer model for topological transitions, applying these with a sliding window of Hamiltonian parameters to learn appropriate order parameters and estimate the critical points accurately. We verified the effectiveness of our method with numerical simulation and real-hardware experiments on Rigetti's Ankaa 9Q-1 quantum computer. Our protocol not only provides a robust framework for investigating quantum phase transitions using shallow quantum circuits but also significantly enhances efficiency and precision, opening new avenues in the integration of quantum computing and machine learning.","sentences":["Understanding quantum phase transitions in physical systems is fundamental to characterize their behaviour at small temperatures.","Achieving this requires both accessing good approximations to the ground state and identifying order parameters to distinguish different phases.","Addressing these challenges, our work introduces a hybrid algorithm that combines quantum optimization with classical machine learning.","This approach leverages the capability of near-term quantum computers to prepare locally trapped states through finite optimization.","Specifically, we utilize LASSO for identifying conventional phase transitions and the Transformer model for topological transitions, applying these with a sliding window of Hamiltonian parameters to learn appropriate order parameters and estimate the critical points accurately.","We verified the effectiveness of our method with numerical simulation and real-hardware experiments on Rigetti's Ankaa 9Q-1 quantum computer.","Our protocol not only provides a robust framework for investigating quantum phase transitions using shallow quantum circuits but also significantly enhances efficiency and precision, opening new avenues in the integration of quantum computing and machine learning."],"url":"http://arxiv.org/abs/2405.08441v1","category":"quant-ph"}
{"created":"2024-05-14 09:01:30","title":"A link between static and dynamical perturbation theory","abstract":"Dynamics, the physical change in time and a pillar of natural sciences, can be regarded as an emergent phenomenon when the system of interest is part of a larger, static one. This \"relational approach to time\", in which the system's environment provides a temporal reference, does not only provide insight into foundational issues of physics, but holds the potential for a deeper theoretical understanding as it intimately links statics and dynamics. Reinforcing the significance of this connection, we demonstrate, based on recent progress [Phys. Rev. Lett. 131, 140202 (2023)], the role of emergent time as a vital link between time-independent and time-dependent perturbation theory in quantum mechanics. We calculate first order contributions, which are often the most significant, and discuss the issue of degenerate spectra. Based on our results, we envision future applications for the calculation of dynamical phenomena based on a single pure energy eigenstate.","sentences":["Dynamics, the physical change in time and a pillar of natural sciences, can be regarded as an emergent phenomenon when the system of interest is part of a larger, static one.","This \"relational approach to time\", in which the system's environment provides a temporal reference, does not only provide insight into foundational issues of physics, but holds the potential for a deeper theoretical understanding as it intimately links statics and dynamics.","Reinforcing the significance of this connection, we demonstrate, based on recent progress","[Phys. Rev. Lett.","131, 140202 (2023)], the role of emergent time as a vital link between time-independent and time-dependent perturbation theory in quantum mechanics.","We calculate first order contributions, which are often the most significant, and discuss the issue of degenerate spectra.","Based on our results, we envision future applications for the calculation of dynamical phenomena based on a single pure energy eigenstate."],"url":"http://arxiv.org/abs/2405.08439v1","category":"quant-ph"}
{"created":"2024-05-14 09:00:22","title":"Magnetic fluctuation and dominant superconducting pairing symmetry near the tunable Van Hove singularity","abstract":"We have investigated the magnetism and pairing correlations of the triangular lattice based on the Hubbard model using the determinant quantum Monte Carlo method and the constrained path Monte Carlo. The results show that the presence of the next-nearest-neighbor hopping integral $t^{\\prime}$ introduces an additional energy scale to the system, and through $t^{\\prime}$, one can regulate the shape of the density of states and thus the position of the van Hove singularity point. Increasing inverse temperature $\\beta$ and on-site interaction $U$ favor the formation of ferromagnetic correlation in a rather large filling region, and the calculations for different lattice sizes show that the range of the ferromagnetic correlations is smaller than the smallest lattice simulated at the investigated temperatures. We study the different pairing correlations of the triangular lattice near several typical fillings and show that the $f$-wave pairing dominates the system in the filling region near the van Hove singularity point with a high density of states, where the ferromagnetic correlation is also enhanced. When the filling is close to half-filling, the pairing susceptibility with $f$ wave is suppressed and the pairing susceptibility of $f_n$ wave is enhanced, however, both the effective pairing interaction with $f$ wave and $f_n$ wave are negative, which indicates that neither $f$-wave nor $f_n$-wave superconductivity may exist. Finally, we find that the pairing channel of different symmetry in the system maybe closely related to the magnetic properties. Ferromagnetic fluctuation favors the formation of $f$-wave pairing, while antiferromagnetic fluctuation tends to promote $f_n$-wave pairing.","sentences":["We have investigated the magnetism and pairing correlations of the triangular lattice based on the Hubbard model using the determinant quantum Monte Carlo method and the constrained path Monte Carlo.","The results show that the presence of the next-nearest-neighbor hopping integral $t^{\\prime}$ introduces an additional energy scale to the system, and through $t^{\\prime}$, one can regulate the shape of the density of states and thus the position of the van Hove singularity point.","Increasing inverse temperature $\\beta$ and on-site interaction $U$ favor the formation of ferromagnetic correlation in a rather large filling region, and the calculations for different lattice sizes show that the range of the ferromagnetic correlations is smaller than the smallest lattice simulated at the investigated temperatures.","We study the different pairing correlations of the triangular lattice near several typical fillings and show that the $f$-wave pairing dominates the system in the filling region near the van Hove singularity point with a high density of states, where the ferromagnetic correlation is also enhanced.","When the filling is close to half-filling, the pairing susceptibility with $f$ wave is suppressed and the pairing susceptibility of $f_n$ wave is enhanced, however, both the effective pairing interaction with $f$ wave and $f_n$ wave are negative, which indicates that neither $f$-wave nor $f_n$-wave superconductivity may exist.","Finally, we find that the pairing channel of different symmetry in the system maybe closely related to the magnetic properties.","Ferromagnetic fluctuation favors the formation of $f$-wave pairing, while antiferromagnetic fluctuation tends to promote $f_n$-wave pairing."],"url":"http://arxiv.org/abs/2405.08438v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 08:33:40","title":"NAFRSSR: a Lightweight Recursive Network for Efficient Stereo Image Super-Resolution","abstract":"Stereo image super-resolution (SR) refers to the reconstruction of a high-resolution (HR) image from a pair of low-resolution (LR) images as typically captured by a dual-camera device. To enhance the quality of SR images, most previous studies focused on increasing the number and size of feature maps and introducing complex and computationally intensive structures, resulting in models with high computational complexity. Here, we propose a simple yet efficient stereo image SR model called NAFRSSR, which is modified from the previous state-of-the-art model NAFSSR by introducing recursive connections and lightweighting the constituent modules. Our NAFRSSR model is composed of nonlinear activation free and group convolution-based blocks (NAFGCBlocks) and depth-separated stereo cross attention modules (DSSCAMs). The NAFGCBlock improves feature extraction and reduces number of parameters by removing the simple channel attention mechanism from NAFBlock and using group convolution. The DSSCAM enhances feature fusion and reduces number of parameters by replacing 1x1 pointwise convolution in SCAM with weight-shared 3x3 depthwise convolution. Besides, we propose to incorporate trainable edge detection operator into NAFRSSR to further improve the model performance. Four variants of NAFRSSR with different sizes, namely, NAFRSSR-Mobile (NAFRSSR-M), NAFRSSR-Tiny (NAFRSSR-T), NAFRSSR-Super (NAFRSSR-S) and NAFRSSR-Base (NAFRSSR-B) are designed, and they all exhibit fewer parameters, higher PSNR/SSIM, and faster speed than the previous state-of-the-art models. In particular, to the best of our knowledge, NAFRSSR-M is the lightest (0.28M parameters) and fastest (50 ms inference time) model achieving an average PSNR/SSIM as high as 24.657 dB/0.7622 on the benchmark datasets. Codes and models will be released at https://github.com/JNUChenYiHong/NAFRSSR.","sentences":["Stereo image super-resolution (SR) refers to the reconstruction of a high-resolution (HR) image from a pair of low-resolution (LR) images as typically captured by a dual-camera device.","To enhance the quality of SR images, most previous studies focused on increasing the number and size of feature maps and introducing complex and computationally intensive structures, resulting in models with high computational complexity.","Here, we propose a simple yet efficient stereo image SR model called NAFRSSR, which is modified from the previous state-of-the-art model NAFSSR by introducing recursive connections and lightweighting the constituent modules.","Our NAFRSSR model is composed of nonlinear activation free and group convolution-based blocks (NAFGCBlocks) and depth-separated stereo cross attention modules (DSSCAMs).","The NAFGCBlock improves feature extraction and reduces number of parameters by removing the simple channel attention mechanism from NAFBlock and using group convolution.","The DSSCAM enhances feature fusion and reduces number of parameters by replacing 1x1 pointwise convolution in SCAM with weight-shared 3x3 depthwise convolution.","Besides, we propose to incorporate trainable edge detection operator into NAFRSSR to further improve the model performance.","Four variants of NAFRSSR with different sizes, namely, NAFRSSR-Mobile (NAFRSSR-M), NAFRSSR-Tiny (NAFRSSR-T), NAFRSSR-Super (NAFRSSR-S) and NAFRSSR-Base (NAFRSSR-B) are designed, and they all exhibit fewer parameters, higher PSNR/SSIM, and faster speed than the previous state-of-the-art models.","In particular, to the best of our knowledge, NAFRSSR-M is the lightest (0.28M parameters) and fastest (50 ms inference time) model achieving an average PSNR/SSIM as high as 24.657 dB/0.7622 on the benchmark datasets.","Codes and models will be released at https://github.com/JNUChenYiHong/NAFRSSR."],"url":"http://arxiv.org/abs/2405.08423v1","category":"eess.IV"}
{"created":"2024-05-14 08:30:29","title":"Faster algorithms for the alignment of sparse correlated Erd\u00f6s-R\u00e9nyi random graphs","abstract":"The correlated Erd\\\"os-R\\'enyi random graph ensemble is a probability law on pairs of graphs with $n$ vertices, parametrized by their average degree $\\lambda$ and their correlation coefficient $s$. It can be used as a benchmark for the graph alignment problem, in which the labels of the vertices of one of the graphs are reshuffled by an unknown permutation; the goal is to infer this permutation and thus properly match the pairs of vertices in both graphs. A series of recent works has unveiled the role of Otter's constant $\\alpha$ (that controls the exponential rate of growth of the number of unlabeled rooted trees as a function of their sizes) in this problem: for $s>\\sqrt{\\alpha}$ and $\\lambda$ large enough it is possible to recover in a time polynomial in $n$ a positive fraction of the hidden permutation. The exponent of this polynomial growth is however quite large and depends on the other parameters, which limits the range of applications of the algorithm. In this work we present a family of faster algorithms for this task, show through numerical simulations that their accuracy is only slightly reduced with respect to the original one, and conjecture that they undergo, in the large $\\lambda$ limit, phase transitions at modified Otter's thresholds $\\sqrt{\\widehat{\\alpha}}>\\sqrt{\\alpha}$, with $\\widehat{\\alpha}$ related to the enumeration of a restricted family of trees.","sentences":["The correlated Erd\\\"os-R\\'enyi random graph ensemble is a probability law on pairs of graphs with $n$ vertices, parametrized by their average degree $\\lambda$ and their correlation coefficient $s$. It can be used as a benchmark for the graph alignment problem, in which the labels of the vertices of one of the graphs are reshuffled by an unknown permutation; the goal is to infer this permutation and thus properly match the pairs of vertices in both graphs.","A series of recent works has unveiled the role of Otter's constant $\\alpha$ (that controls the exponential rate of growth of the number of unlabeled rooted trees as a function of their sizes) in this problem: for $s>\\sqrt{\\alpha}$ and $\\lambda$ large enough it is possible to recover in a time polynomial in $n$ a positive fraction of the hidden permutation.","The exponent of this polynomial growth is however quite large and depends on the other parameters, which limits the range of applications of the algorithm.","In this work we present a family of faster algorithms for this task, show through numerical simulations that their accuracy is only slightly reduced with respect to the original one, and conjecture that they undergo, in the large $\\lambda$ limit, phase transitions at modified Otter's thresholds $\\sqrt{\\widehat{\\alpha}}>\\sqrt{\\alpha}$, with $\\widehat{\\alpha}$ related to the enumeration of a restricted family of trees."],"url":"http://arxiv.org/abs/2405.08421v1","category":"cond-mat.dis-nn"}
{"created":"2024-05-14 08:29:58","title":"Stabilization of lead-free bulk CsSnI$_3$ perovskite thermoelectrics via incorporating of TiS$_3$ nanoribbon clusters","abstract":"The intense research for efficient low-temperature thermoelectric materials motivates the exploration of innovative compounds and composite systems. This study examines the effects of integrating low-dimensional titanium trisulfide (TiS$_3$) into bulk tin-based halide perovskites (CsSnI$_3$) for use in thermoelectric applications. The addition of small amounts of two-dimensional titanium trisulfide (TiS$_3$) to bulk tin-based halide perovskites (CsSnI$_3$) significantly enhanced the structural stability of the composite material and suppressed oxidation processes. The CsSnI$_3$-TiS$_3$ composites demonstrated stabilization of temperature-dependent electrical properties (conductivity and Seebeck coefficient). This study provides valuable insights into the promising approach of using low-dimensional TiS3 as an additive to stabilize the thermoelectric performance of CsSnI$_3$.","sentences":["The intense research for efficient low-temperature thermoelectric materials motivates the exploration of innovative compounds and composite systems.","This study examines the effects of integrating low-dimensional titanium trisulfide (TiS$_3$) into bulk tin-based halide perovskites (CsSnI$_3$) for use in thermoelectric applications.","The addition of small amounts of two-dimensional titanium trisulfide (TiS$_3$) to bulk tin-based halide perovskites (CsSnI$_3$) significantly enhanced the structural stability of the composite material and suppressed oxidation processes.","The CsSnI$_3$-TiS$_3$ composites demonstrated stabilization of temperature-dependent electrical properties (conductivity and Seebeck coefficient).","This study provides valuable insights into the promising approach of using low-dimensional TiS3 as an additive to stabilize the thermoelectric performance of CsSnI$_3$."],"url":"http://arxiv.org/abs/2405.08420v1","category":"physics.app-ph"}
{"created":"2024-05-14 08:26:29","title":"WaterMamba: Visual State Space Model for Underwater Image Enhancement","abstract":"Underwater imaging often suffers from low quality due to factors affecting light propagation and absorption in water. To improve image quality, some underwater image enhancement (UIE) methods based on convolutional neural networks (CNN) and Transformer have been proposed. However, CNN-based UIE methods are limited in modeling long-range dependencies, and Transformer-based methods involve a large number of parameters and complex self-attention mechanisms, posing efficiency challenges. Considering computational complexity and severe underwater image degradation, a state space model (SSM) with linear computational complexity for UIE, named WaterMamba, is proposed. We propose spatial-channel omnidirectional selective scan (SCOSS) blocks comprising spatial-channel coordinate omnidirectional selective scan (SCCOSS) modules and a multi-scale feedforward network (MSFFN). The SCOSS block models pixel and channel information flow, addressing dependencies. The MSFFN facilitates information flow adjustment and promotes synchronized operations within SCCOSS modules. Extensive experiments showcase WaterMamba's cutting-edge performance with reduced parameters and computational resources, outperforming state-of-the-art methods on various datasets, validating its effectiveness and generalizability. The code will be released on GitHub after acceptance.","sentences":["Underwater imaging often suffers from low quality due to factors affecting light propagation and absorption in water.","To improve image quality, some underwater image enhancement (UIE) methods based on convolutional neural networks (CNN) and Transformer have been proposed.","However, CNN-based UIE methods are limited in modeling long-range dependencies, and Transformer-based methods involve a large number of parameters and complex self-attention mechanisms, posing efficiency challenges.","Considering computational complexity and severe underwater image degradation, a state space model (SSM) with linear computational complexity for UIE, named WaterMamba, is proposed.","We propose spatial-channel omnidirectional selective scan (SCOSS) blocks comprising spatial-channel coordinate omnidirectional selective scan (SCCOSS) modules and a multi-scale feedforward network (MSFFN).","The SCOSS block models pixel and channel information flow, addressing dependencies.","The MSFFN facilitates information flow adjustment and promotes synchronized operations within SCCOSS modules.","Extensive experiments showcase WaterMamba's cutting-edge performance with reduced parameters and computational resources, outperforming state-of-the-art methods on various datasets, validating its effectiveness and generalizability.","The code will be released on GitHub after acceptance."],"url":"http://arxiv.org/abs/2405.08419v1","category":"cs.CV"}
{"created":"2024-05-14 08:18:38","title":"ICO learning as a measure of transient chaos in PT-symmetric Li\u00e9nard systems","abstract":"In this article, we investigate the implications of the unsupervised learning rule known as Input-Correlations (ICO) learning in the nonlinear dynamics of two linearly coupled PT-symmetric Li\\'enard oscillators. The fixed points of the oscillator have been evaluated analytically and the Jacobian linearization is employed to study their stability. We find that on increasing the amplitude of the external periodic drive, the system exhibits period-doubling cascade to chaos within a specific parametric regime wherein we observe emergent chaotic dynamics. We further notice that the system indicates an intermittency route to chaos in the chaotic regime. Finally, in the period-4 regime of our bifurcation analysis, we predict the emergence of transient chaos which eventually settles down to a period-2 oscillator response which has been further validated by both the maximal Finite-Time Lyapunov Exponent (FTLE) using the well-known Gram-Schmidt orthogonalization technique and the Hilbert Transform of the time-series. In the transiently chaotic regime, we deploy the ICO learning to analyze the time-series from which we identify that when the chaotic evolution transforms into periodic dynamics, the synaptic weight associated with the time-series of the loss oscillator exhibits stationary temporal evolution. This signifies that in the periodic regime, there is no overlap between the filtered signals obtained from the time-series of the coupled PT-symmetric oscillators. In addition, the temporal evolution of the weight associated with the stimulus mimics the behaviour of the Hilbert transform of the time-series.","sentences":["In this article, we investigate the implications of the unsupervised learning rule known as Input-Correlations (ICO) learning in the nonlinear dynamics of two linearly coupled PT-symmetric Li\\'enard oscillators.","The fixed points of the oscillator have been evaluated analytically and the Jacobian linearization is employed to study their stability.","We find that on increasing the amplitude of the external periodic drive, the system exhibits period-doubling cascade to chaos within a specific parametric regime wherein we observe emergent chaotic dynamics.","We further notice that the system indicates an intermittency route to chaos in the chaotic regime.","Finally, in the period-4 regime of our bifurcation analysis, we predict the emergence of transient chaos which eventually settles down to a period-2 oscillator response which has been further validated by both the maximal Finite-Time Lyapunov Exponent (FTLE) using the well-known Gram-Schmidt orthogonalization technique and the Hilbert Transform of the time-series.","In the transiently chaotic regime, we deploy the ICO learning to analyze the time-series from which we identify that when the chaotic evolution transforms into periodic dynamics, the synaptic weight associated with the time-series of the loss oscillator exhibits stationary temporal evolution.","This signifies that in the periodic regime, there is no overlap between the filtered signals obtained from the time-series of the coupled PT-symmetric oscillators.","In addition, the temporal evolution of the weight associated with the stimulus mimics the behaviour of the Hilbert transform of the time-series."],"url":"http://arxiv.org/abs/2405.08414v1","category":"nlin.AO"}
{"created":"2024-05-14 08:08:16","title":"Large-Scale Metric Computation in Online Controlled Experiment Platform","abstract":"Online controlled experiment (also called A/B test or experiment) is the most important tool for decision-making at a wide range of data-driven companies like Microsoft, Google, Meta, etc. Metric computation is the core procedure for reaching a conclusion during an experiment. With the growth of experiments and metrics in an experiment platform, computing metrics efficiently at scale becomes a non-trivial challenge. This work shows how metric computation in WeChat experiment platform can be done efficiently using bit-sliced index (BSI) arithmetic. This approach has been implemented in a real world system and the performance results are presented, showing that the BSI arithmetic approach is very suitable for large-scale metric computation scenarios.","sentences":["Online controlled experiment (also called A/B test or experiment) is the most important tool for decision-making at a wide range of data-driven companies like Microsoft, Google, Meta, etc.","Metric computation is the core procedure for reaching a conclusion during an experiment.","With the growth of experiments and metrics in an experiment platform, computing metrics efficiently at scale becomes a non-trivial challenge.","This work shows how metric computation in WeChat experiment platform can be done efficiently using bit-sliced index (BSI) arithmetic.","This approach has been implemented in a real world system and the performance results are presented, showing that the BSI arithmetic approach is very suitable for large-scale metric computation scenarios."],"url":"http://arxiv.org/abs/2405.08411v1","category":"cs.DC"}
{"created":"2024-05-14 07:58:20","title":"Towards a Hybrid Digital Twin: Physics-Informed Neural Networks as Surrogate Model of a Reinforced Concrete Beam","abstract":"In this study, we investigate the potential of fast-to-evaluate surrogate modeling techniques for developing a hybrid digital twin of a steel-reinforced concrete beam, serving as a representative example of a civil engineering structure. As surrogates, two distinct models are developed utilizing physics-informed neural networks, which integrate experimental data with given governing laws of physics. The experimental data (sensor data) is obtained from a previously conducted four-point bending test. The first surrogate model predicts strains at fixed locations along the center line of the beam for various time instances. This time-dependent surrogate model is inspired by the motion of a harmonic oscillator. For this study, we further compare the physics-based approach with a purely data-driven method, revealing the significance of physical laws for the extrapolation capabilities of models in scenarios with limited access to experimental data. Furthermore, we identify the natural frequency of the system by utilizing the physics-based model as an inverse solver. For the second surrogate model, we then focus on a fixed instance in time and combine the sensor data with the equations of linear elasticity to predict the strain distribution within the beam. This example reveals the importance of balancing different loss components through the selection of suitable loss weights.","sentences":["In this study, we investigate the potential of fast-to-evaluate surrogate modeling techniques for developing a hybrid digital twin of a steel-reinforced concrete beam, serving as a representative example of a civil engineering structure.","As surrogates, two distinct models are developed utilizing physics-informed neural networks, which integrate experimental data with given governing laws of physics.","The experimental data (sensor data) is obtained from a previously conducted four-point bending test.","The first surrogate model predicts strains at fixed locations along the center line of the beam for various time instances.","This time-dependent surrogate model is inspired by the motion of a harmonic oscillator.","For this study, we further compare the physics-based approach with a purely data-driven method, revealing the significance of physical laws for the extrapolation capabilities of models in scenarios with limited access to experimental data.","Furthermore, we identify the natural frequency of the system by utilizing the physics-based model as an inverse solver.","For the second surrogate model, we then focus on a fixed instance in time and combine the sensor data with the equations of linear elasticity to predict the strain distribution within the beam.","This example reveals the importance of balancing different loss components through the selection of suitable loss weights."],"url":"http://arxiv.org/abs/2405.08406v1","category":"cs.CE"}
{"created":"2024-05-14 07:56:09","title":"TFWT: Tabular Feature Weighting with Transformer","abstract":"In this paper, we propose a novel feature weighting method to address the limitation of existing feature processing methods for tabular data. Typically the existing methods assume equal importance across all samples and features in one dataset. This simplified processing methods overlook the unique contributions of each feature, and thus may miss important feature information. As a result, it leads to suboptimal performance in complex datasets with rich features. To address this problem, we introduce Tabular Feature Weighting with Transformer, a novel feature weighting approach for tabular data. Our method adopts Transformer to capture complex feature dependencies and contextually assign appropriate weights to discrete and continuous features. Besides, we employ a reinforcement learning strategy to further fine-tune the weighting process. Our extensive experimental results across various real-world datasets and diverse downstream tasks show the effectiveness of TFWT and highlight the potential for enhancing feature weighting in tabular data analysis.","sentences":["In this paper, we propose a novel feature weighting method to address the limitation of existing feature processing methods for tabular data.","Typically the existing methods assume equal importance across all samples and features in one dataset.","This simplified processing methods overlook the unique contributions of each feature, and thus may miss important feature information.","As a result, it leads to suboptimal performance in complex datasets with rich features.","To address this problem, we introduce Tabular Feature Weighting with Transformer, a novel feature weighting approach for tabular data.","Our method adopts Transformer to capture complex feature dependencies and contextually assign appropriate weights to discrete and continuous features.","Besides, we employ a reinforcement learning strategy to further fine-tune the weighting process.","Our extensive experimental results across various real-world datasets and diverse downstream tasks show the effectiveness of TFWT and highlight the potential for enhancing feature weighting in tabular data analysis."],"url":"http://arxiv.org/abs/2405.08403v1","category":"cs.LG"}
{"created":"2024-05-14 07:52:46","title":"Exploring material compositions for synthesis using oxidation states","abstract":"Recent advances in machine learning techniques have made it possible to use high-throughput screening to identify novel materials with specific properties. However, the large number of potential candidates produced by these techniques can make it difficult to select the most promising ones. In this study, we develop the oxidation state probability (OSP) method which evaluates ternary compounds based on the probability (the OSP metric) of each element to adopt the required oxidation states for fulfilling charge neutrality. We compare this model with Roost and the Fourier-transformed crystal properties (FTCP)-based synthesizability score. Among the top 1000 systems with the most database entries in Materials Project (MP), more than 500 systems exhibit an attested compound among the top 3 compositions when ranked by the OSP metric. We find that the OSP method shows promising results for certain classes of ternary systems, especially those containing nonmetals, s-block, or transition metals. When applied to the Cu-In-Te ternary system, an interesting system for thermoelectric applications, the OSP method predicted the synthesizability of CuIn$_3$Te$_5$ without prior knowledge, and we have successfully synthesized CuIn$_3$Te$_5$ in experiment. Our method has the potential to accelerate the discovery of novel compounds by providing a guide for experimentalists to easily select the most synthesizable candidates from an arbitrarily large set of possible chemical compositions.","sentences":["Recent advances in machine learning techniques have made it possible to use high-throughput screening to identify novel materials with specific properties.","However, the large number of potential candidates produced by these techniques can make it difficult to select the most promising ones.","In this study, we develop the oxidation state probability (OSP) method which evaluates ternary compounds based on the probability (the OSP metric) of each element to adopt the required oxidation states for fulfilling charge neutrality.","We compare this model with Roost and the Fourier-transformed crystal properties (FTCP)-based synthesizability score.","Among the top 1000 systems with the most database entries in Materials Project (MP), more than 500 systems exhibit an attested compound among the top 3 compositions when ranked by the OSP metric.","We find that the OSP method shows promising results for certain classes of ternary systems, especially those containing nonmetals, s-block, or transition metals.","When applied to the Cu-In-Te ternary system, an interesting system for thermoelectric applications, the OSP method predicted the synthesizability of CuIn$_3$Te$_5$ without prior knowledge, and we have successfully synthesized CuIn$_3$Te$_5$ in experiment.","Our method has the potential to accelerate the discovery of novel compounds by providing a guide for experimentalists to easily select the most synthesizable candidates from an arbitrarily large set of possible chemical compositions."],"url":"http://arxiv.org/abs/2405.08399v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 07:49:33","title":"Coupled-Band ESSFM for Low-Complexity DBP","abstract":"We propose a novel digital backpropagation (DBP) technique that combines perturbation theory, subband processing, and splitting ratio optimization. We obtain 0.23 dB, 0.47 dB, or 0.91 dB gains w.r.t. dispersion compensation with only 74, 161, or 681 real multiplications/2D-symbol, improving significantly on existing DBP techniques.","sentences":["We propose a novel digital backpropagation (DBP) technique that combines perturbation theory, subband processing, and splitting ratio optimization.","We obtain 0.23 dB, 0.47 dB, or 0.91 dB gains w.r.t.","dispersion compensation with only 74, 161, or 681 real multiplications/2D-symbol, improving significantly on existing DBP techniques."],"url":"http://arxiv.org/abs/2405.08396v1","category":"cs.IT"}
{"created":"2024-05-14 07:48:19","title":"Cross-Blockchain Communication Using Oracles With an Off-Chain Aggregation Mechanism Based on zk-SNARKs","abstract":"The closed architecture of prevailing blockchain systems renders the usage of this technology mostly infeasible for a wide range of real-world problems. Most blockchains trap users and applications in their isolated space without the possibility of cooperating or switching to other blockchains. Therefore, blockchains need additional mechanisms for seamless communication and arbitrary data exchange between each other and external systems. Unfortunately, current approaches for cross-blockchain communication are resource-intensive or require additional blockchains or tailored solutions depending on the applied consensus mechanisms of the connected blockchains. Therefore, we propose an oracle with an off-chain aggregation mechanism based on ZeroKnowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARKs) to facilitate cross-blockchain communication. The oracle queries data from another blockchain and applies a rollup-like mechanism to move state and computation off-chain. The zkOracle contract only expects the transferred data, an updated state root, and proof of the correct execution of the aggregation mechanism. The proposed solution only requires constant 378 kgas to submit data on the Ethereum blockchain and is primarily independent of the underlying technology of the queried blockchains.","sentences":["The closed architecture of prevailing blockchain systems renders the usage of this technology mostly infeasible for a wide range of real-world problems.","Most blockchains trap users and applications in their isolated space without the possibility of cooperating or switching to other blockchains.","Therefore, blockchains need additional mechanisms for seamless communication and arbitrary data exchange between each other and external systems.","Unfortunately, current approaches for cross-blockchain communication are resource-intensive or require additional blockchains or tailored solutions depending on the applied consensus mechanisms of the connected blockchains.","Therefore, we propose an oracle with an off-chain aggregation mechanism based on ZeroKnowledge Succinct Non-interactive Arguments of Knowledge (zk-SNARKs) to facilitate cross-blockchain communication.","The oracle queries data from another blockchain and applies a rollup-like mechanism to move state and computation off-chain.","The zkOracle contract only expects the transferred data, an updated state root, and proof of the correct execution of the aggregation mechanism.","The proposed solution only requires constant 378 kgas to submit data on the Ethereum blockchain and is primarily independent of the underlying technology of the queried blockchains."],"url":"http://arxiv.org/abs/2405.08395v1","category":"cs.CR"}
{"created":"2024-05-14 07:47:32","title":"Weak solutions to the steady compressible Euler equations with source terms","abstract":"In this paper, we showed that for some given suitable density and pressure, there exist infinitely many compactly supported solutions with prescribed energy profile. The proof is mainly based on the convex integration scheme. We construct suitable subsolutions and localized plane-wave solutions to the reformulated system, and weak solutions are obtained by iterating these subsolutions.","sentences":["In this paper, we showed that for some given suitable density and pressure, there exist infinitely many compactly supported solutions with prescribed energy profile.","The proof is mainly based on the convex integration scheme.","We construct suitable subsolutions and localized plane-wave solutions to the reformulated system, and weak solutions are obtained by iterating these subsolutions."],"url":"http://arxiv.org/abs/2405.08394v1","category":"math.AP"}
{"created":"2024-05-14 07:43:10","title":"Neuromorphic Robust Estimation of Nonlinear Dynamical Systems Applied to Satellite Rendezvous","abstract":"State estimation of nonlinear dynamical systems has long aimed to balance accuracy, computational efficiency, robustness, and reliability. The rapid evolution of various industries has amplified the demand for estimation frameworks that satisfy all these factors. This study introduces a neuromorphic approach for robust filtering of nonlinear dynamical systems: SNN-EMSIF (spiking neural network-extended modified sliding innovation filter). SNN-EMSIF combines the computational efficiency and scalability of SNNs with the robustness of EMSIF, an estimation framework designed for nonlinear systems with zero-mean Gaussian noise. Notably, the weight matrices are designed according to the system model, eliminating the need for a learning process. The framework's efficacy is evaluated through comprehensive Monte Carlo simulations, comparing SNN-EMSIF with EKF and EMSIF. Additionally, it is compared with SNN-EKF in the presence of modeling uncertainties and neuron loss, using RMSEs as a metric. The results demonstrate the superior accuracy and robustness of SNN-EMSIF. Further analysis of runtimes and spiking patterns reveals an impressive reduction of 85% in emitted spikes compared to possible spikes, highlighting the computational efficiency of SNN-EMSIF. This framework offers a promising solution for robust estimation in nonlinear dynamical systems, opening new avenues for efficient and reliable estimation in various industries that can benefit from neuromorphic computing.","sentences":["State estimation of nonlinear dynamical systems has long aimed to balance accuracy, computational efficiency, robustness, and reliability.","The rapid evolution of various industries has amplified the demand for estimation frameworks that satisfy all these factors.","This study introduces a neuromorphic approach for robust filtering of nonlinear dynamical systems: SNN-EMSIF (spiking neural network-extended modified sliding innovation filter).","SNN-EMSIF combines the computational efficiency and scalability of SNNs with the robustness of EMSIF, an estimation framework designed for nonlinear systems with zero-mean Gaussian noise.","Notably, the weight matrices are designed according to the system model, eliminating the need for a learning process.","The framework's efficacy is evaluated through comprehensive Monte Carlo simulations, comparing SNN-EMSIF with EKF and EMSIF.","Additionally, it is compared with SNN-EKF in the presence of modeling uncertainties and neuron loss, using RMSEs as a metric.","The results demonstrate the superior accuracy and robustness of SNN-EMSIF.","Further analysis of runtimes and spiking patterns reveals an impressive reduction of 85% in emitted spikes compared to possible spikes, highlighting the computational efficiency of SNN-EMSIF.","This framework offers a promising solution for robust estimation in nonlinear dynamical systems, opening new avenues for efficient and reliable estimation in various industries that can benefit from neuromorphic computing."],"url":"http://arxiv.org/abs/2405.08392v1","category":"eess.SY"}
{"created":"2024-05-14 07:40:50","title":"Weak solutions to the steady incompressible Euler equations with source terms","abstract":"In this paper, we prove the non-uniqueness of stationary solutions to steady incompressible Euler equations with source terms. Based on the convex integration scheme developed by De Lellis and Sz\\'{e}kelyhidi, the Euler system is reformulated as a differential inclusion. The key point is to construct the corresponding plane-wave solutions via high frequency perturbations. Then we use iteration and Baire category argument to conclude that there exist a large amount of weak solutions with given energy profile.","sentences":["In this paper, we prove the non-uniqueness of stationary solutions to steady incompressible Euler equations with source terms.","Based on the convex integration scheme developed by De Lellis and Sz\\'{e}kelyhidi, the Euler system is reformulated as a differential inclusion.","The key point is to construct the corresponding plane-wave solutions via high frequency perturbations.","Then we use iteration and Baire category argument to conclude that there exist a large amount of weak solutions with given energy profile."],"url":"http://arxiv.org/abs/2405.08390v1","category":"math.AP"}
{"created":"2024-05-14 07:17:52","title":"OliVier: an Oil and Vinegar based cryptosystem","abstract":"In this paper, we present OliVier a new Public Key Exchange cryptosystem that is based on a multivariate quadratic polynomial system: Oil & Vinegar polynomials together with fully quadratic ones. We describe its designing process, usage, complexity","sentences":["In this paper, we present OliVier a new Public Key Exchange cryptosystem that is based on a multivariate quadratic polynomial system: Oil & Vinegar polynomials together with fully quadratic ones.","We describe its designing process, usage, complexity"],"url":"http://arxiv.org/abs/2405.08375v1","category":"math.AC"}
{"created":"2024-05-14 07:15:59","title":"Reasoning about Interior Mutability in Rust using Library-Defined Capabilities","abstract":"Existing automated verification techniques for safe Rust code rely on the strong type-system properties to reason about programs, especially to deduce which memory locations do not change (i.e., are framed) across function calls. However, these type guarantees do not hold in the presence of interior mutability (e.g., when interacting with any concurrent data structure). As a consequence, existing verification techniques for safe code such as Prusti and Creusot are either unsound or fundamentally incomplete if applied to this setting. In this work, we present the first technique capable of automatically verifying safe clients of existing interiorly mutable types. At the core of our approach, we identify a novel notion of implicit capabilities: library-defined properties that cannot be expressed using Rust's types. We propose new annotations to specify these capabilities and a first-order logic encoding suitable for program verification. We have implemented our technique in a verifier called Mendel and used it to prove absence of panics in Rust programs that make use of popular standard-library types with interior mutability, including Rc, Arc, Cell, RefCell, AtomicI32, Mutex and RwLock. Our evaluation shows that these library annotations are useful for verifying usages of real-world libraries, and powerful enough to require zero client-side annotations in many of the verified programs.","sentences":["Existing automated verification techniques for safe Rust code rely on the strong type-system properties to reason about programs, especially to deduce which memory locations do not change (i.e., are framed) across function calls.","However, these type guarantees do not hold in the presence of interior mutability (e.g., when interacting with any concurrent data structure).","As a consequence, existing verification techniques for safe code such as Prusti and Creusot are either unsound or fundamentally incomplete if applied to this setting.","In this work, we present the first technique capable of automatically verifying safe clients of existing interiorly mutable types.","At the core of our approach, we identify a novel notion of implicit capabilities: library-defined properties that cannot be expressed using Rust's types.","We propose new annotations to specify these capabilities and a first-order logic encoding suitable for program verification.","We have implemented our technique in a verifier called Mendel and used it to prove absence of panics in Rust programs that make use of popular standard-library types with interior mutability, including Rc, Arc, Cell, RefCell, AtomicI32, Mutex and RwLock.","Our evaluation shows that these library annotations are useful for verifying usages of real-world libraries, and powerful enough to require zero client-side annotations in many of the verified programs."],"url":"http://arxiv.org/abs/2405.08372v1","category":"cs.PL"}
{"created":"2024-05-14 06:54:45","title":"A new ferromagnetic semiconductor system of Eu$_{1-x}$Sr$_x$AgP $(x = 0.0-0.6)$ compounds: Crystallographic, magnetic, and magneto-resistive properties","abstract":"Adjusting chemical pressure through doping is a highly effective method for customizing the chemical and physical properties of materials, along with their respective phase diagrams, thereby uncovering novel quantum phenomena. Here, we successfully synthesized Sr-doped Eu$_{1-x}$Sr$_x$AgP $(x = 0.0-0.6)$ and conducted a comprehensive investigation involving crystallography, magnetization, heat capacity, and magnetoresistance. Utilizing X-ray diffraction and PPMS DynaCool measurements, we studied Eu$_{1-x}$Sr$_x$AgP in detail. The hexagonal structure of parent EuAgP at room temperature, with the $P6_3/mmc$ space group, remains unaltered, while the lattice constants expand. The magnetic phase transition from paramagnetism to ferromagnetism, as temperature decreases, is suppressed through the gradual introduction of strontium doping. Heat capacity measurements reveal a shift from magnon-dominated to predominantly phonon and electron contributions near the ferromagnetic phase with increasing doping levels. The resistivity-temperature relationship displays distinct characteristics, emphasizing the impact of Sr doping on modifying charge transport. Magnetoresistance measurements uncover novel phenomena, illustrating the adjustability of magnetoresistance through Sr doping. Notably, Sr doping results in both positive magnetoresistance (up to 20\\%) and negative magnetoresistance (approximately -60\\%). The resistivity and magnetic phase diagram were established for the first time, revealing the pronounced feasibility of Sr doping in modulating EuAgP's resistivity. This study has provided valuable insights into the intricate interplay between structural modifications and diverse physical properties. The potential for technological advancements and the exploration of novel quantum states make Sr-doped Eu$_{1-x}$Sr$_x$AgP a compelling subject for continued research in the field of applied physics.","sentences":["Adjusting chemical pressure through doping is a highly effective method for customizing the chemical and physical properties of materials, along with their respective phase diagrams, thereby uncovering novel quantum phenomena.","Here, we successfully synthesized Sr-doped Eu$_{1-x}$Sr$_x$AgP $(x = 0.0-0.6)$ and conducted a comprehensive investigation involving crystallography, magnetization, heat capacity, and magnetoresistance.","Utilizing X-ray diffraction and PPMS DynaCool measurements, we studied Eu$_{1-x}$Sr$_x$AgP in detail.","The hexagonal structure of parent EuAgP at room temperature, with the $P6_3/mmc$ space group, remains unaltered, while the lattice constants expand.","The magnetic phase transition from paramagnetism to ferromagnetism, as temperature decreases, is suppressed through the gradual introduction of strontium doping.","Heat capacity measurements reveal a shift from magnon-dominated to predominantly phonon and electron contributions near the ferromagnetic phase with increasing doping levels.","The resistivity-temperature relationship displays distinct characteristics, emphasizing the impact of Sr doping on modifying charge transport.","Magnetoresistance measurements uncover novel phenomena, illustrating the adjustability of magnetoresistance through Sr doping.","Notably, Sr doping results in both positive magnetoresistance (up to 20\\%) and negative magnetoresistance (approximately -60\\%).","The resistivity and magnetic phase diagram were established for the first time, revealing the pronounced feasibility of Sr doping in modulating EuAgP's resistivity.","This study has provided valuable insights into the intricate interplay between structural modifications and diverse physical properties.","The potential for technological advancements and the exploration of novel quantum states make Sr-doped Eu$_{1-x}$Sr$_x$AgP a compelling subject for continued research in the field of applied physics."],"url":"http://arxiv.org/abs/2405.08357v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 06:52:56","title":"A Model-oriented Reasoning Framework for Privacy Analysis of Complex Systems","abstract":"This paper proposes a reasoning framework for privacy properties of systems and their environments that can capture any knowledge leaks on different logical levels of the system to answer the question: which entity can learn what? With the term knowledge we refer to any kind of data, meta-data or interpretation of those that might be relevant. To achieve this, we present a modeling framework that forces the developers to explicitly describe which knowledge is available at which entity, which knowledge flows between entities and which knowledge can be inferred from other knowledge. In addition, privacy requirements are specified as rules describing forbidden knowledge for entities. Our modeling approach is incremental, starting from an abstract view of the system and adding details through well-defined transformations. This work is intended to complement existing approaches and introduces steps towards more formal foundations for privacy oriented analyses while keeping them as accessible as possible. It is designed to be extensible through schemata and vocabulary to enable compatibility with external requirements and standards.","sentences":["This paper proposes a reasoning framework for privacy properties of systems and their environments that can capture any knowledge leaks on different logical levels of the system to answer the question: which entity can learn what?","With the term knowledge we refer to any kind of data, meta-data or interpretation of those that might be relevant.","To achieve this, we present a modeling framework that forces the developers to explicitly describe which knowledge is available at which entity, which knowledge flows between entities and which knowledge can be inferred from other knowledge.","In addition, privacy requirements are specified as rules describing forbidden knowledge for entities.","Our modeling approach is incremental, starting from an abstract view of the system and adding details through well-defined transformations.","This work is intended to complement existing approaches and introduces steps towards more formal foundations for privacy oriented analyses while keeping them as accessible as possible.","It is designed to be extensible through schemata and vocabulary to enable compatibility with external requirements and standards."],"url":"http://arxiv.org/abs/2405.08356v1","category":"cs.CR"}
{"created":"2024-05-14 06:47:49","title":"Data-driven memory-dependent abstractions of dynamical systems via a Cantor-Kantorovich metric","abstract":"Abstractions of dynamical systems enable their verification and the design of feedback controllers using simpler, usually discrete, models. In this paper, we propose a data-driven abstraction mechanism based on a novel metric between Markov models. Our approach is based purely on observing output labels of the underlying dynamics, thus opening the road for a fully data-driven approach to construct abstractions. Another feature of the proposed approach is the use of memory to better represent the dynamics in a given region of the state space. We show through numerical examples the usefulness of the proposed methodology.","sentences":["Abstractions of dynamical systems enable their verification and the design of feedback controllers using simpler, usually discrete, models.","In this paper, we propose a data-driven abstraction mechanism based on a novel metric between Markov models.","Our approach is based purely on observing output labels of the underlying dynamics, thus opening the road for a fully data-driven approach to construct abstractions.","Another feature of the proposed approach is the use of memory to better represent the dynamics in a given region of the state space.","We show through numerical examples the usefulness of the proposed methodology."],"url":"http://arxiv.org/abs/2405.08353v1","category":"eess.SY"}
{"created":"2024-05-14 06:38:15","title":"Foundational Verification of Smart Contracts through Verified Compilation","abstract":"Programs executed on a blockchain - smart contracts - have high financial stakes; their correctness is crucial. We argue, that this correctness needs to be foundational: correctness needs to be based on the operational semantics of their execution environment. In this work we present a foundational system - the DeepSEA system - targeting the Ethereum blockchain as the largest smart contract platform. The DeepSEA system has a small but sufficiently rich programming language amenable for verification, the DeepSEA language, and a verified DeepSEA compiler. Together they enable true end-to-end verification for smart contracts. We demonstrate usability through two case studies: a realistic contract for Decentralized Finance and contract for crowdfunding.","sentences":["Programs executed on a blockchain - smart contracts - have high financial stakes; their correctness is crucial.","We argue, that this correctness needs to be foundational: correctness needs to be based on the operational semantics of their execution environment.","In this work we present a foundational system - the DeepSEA system - targeting the Ethereum blockchain as the largest smart contract platform.","The DeepSEA system has a small but sufficiently rich programming language amenable for verification, the DeepSEA language, and a verified DeepSEA compiler.","Together they enable true end-to-end verification for smart contracts.","We demonstrate usability through two case studies: a realistic contract for Decentralized Finance and contract for crowdfunding."],"url":"http://arxiv.org/abs/2405.08348v1","category":"cs.PL"}
{"created":"2024-05-14 06:34:44","title":"An infinite dimensional balanced embedding problem III: Asymptotics near infinity","abstract":"We continue our study on the logarithmic balanced model metric initiated in our previous work. By a non-trivial refinement of the set of tools developed in our previous work, we are able to confirm partially a conjecture we made in our previous work on the asymptotic behavior of the balanced metric near infinity.","sentences":["We continue our study on the logarithmic balanced model metric initiated in our previous work.","By a non-trivial refinement of the set of tools developed in our previous work, we are able to confirm partially a conjecture we made in our previous work on the asymptotic behavior of the balanced metric near infinity."],"url":"http://arxiv.org/abs/2405.08346v1","category":"math.CV"}
{"created":"2024-05-14 06:33:56","title":"Multi-Robot Rendezvous in Unknown Environment with Limited Communication","abstract":"Rendezvous aims at gathering all robots at a specific location, which is an important collaborative behavior for multirobot systems. However, in an unknown environment, it is challenging to achieve rendezvous. Previous researches mainly focus on special scenarios where communication is not allowed and each robot executes a random searching strategy, which is highly time-consuming, especially in large-scale environments. In this work, we focus on rendezvous in unknown environments where communication is available. We divide this task into two steps: rendezvous based environment exploration with relative pose (RP) estimation and rendezvous point election. A new strategy called partitioned and incomplete exploration for rendezvous (PIER) is proposed to efficiently explore the unknown environment, where lightweight topological maps are constructed and shared among robots for RP estimation with very few communications. Then, a rendezvous point selection algorithm based on the merged topological map is proposed for efficient rendezvous for multi-robot systems. The effectiveness of the proposed methods is validated in both simulations and real-world experiments.","sentences":["Rendezvous aims at gathering all robots at a specific location, which is an important collaborative behavior for multirobot systems.","However, in an unknown environment, it is challenging to achieve rendezvous.","Previous researches mainly focus on special scenarios where communication is not allowed and each robot executes a random searching strategy, which is highly time-consuming, especially in large-scale environments.","In this work, we focus on rendezvous in unknown environments where communication is available.","We divide this task into two steps: rendezvous based environment exploration with relative pose (RP) estimation and rendezvous point election.","A new strategy called partitioned and incomplete exploration for rendezvous (PIER) is proposed to efficiently explore the unknown environment, where lightweight topological maps are constructed and shared among robots for RP estimation with very few communications.","Then, a rendezvous point selection algorithm based on the merged topological map is proposed for efficient rendezvous for multi-robot systems.","The effectiveness of the proposed methods is validated in both simulations and real-world experiments."],"url":"http://arxiv.org/abs/2405.08345v1","category":"cs.RO"}
{"created":"2024-05-14 06:18:44","title":"Homogeneous CR-manifold in $\\mathbb{C}^4$","abstract":"In this paper we study holomorphically homogeneous model submanifolds CR-type (1, 3) complex space $\\mathbb C^4$. One finds moduli space of five-dimensional model surfaces Bloom-Graham type ((2, 1), (3, 1), (4, 1)). It is shown that there exists unique model surface of this type with property of holomorphical homogeneous, which is equivalent to tube surface $\\mathcal C$ with affin homogeneous base. One describes and classifies with respect to model surfaces the orbits relative to the group of holomorhical automorphisms of $\\mathcal C$","sentences":["In this paper we study holomorphically homogeneous model submanifolds CR-type (1, 3) complex space $\\mathbb C^4$. One finds moduli space of five-dimensional model surfaces Bloom-Graham type ((2, 1), (3, 1), (4, 1)).","It is shown that there exists unique model surface of this type with property of holomorphical homogeneous, which is equivalent to tube surface $\\mathcal C$ with affin homogeneous base.","One describes and classifies with respect to model surfaces the orbits relative to the group of holomorhical automorphisms of $\\mathcal C$"],"url":"http://arxiv.org/abs/2405.08338v1","category":"math.CV"}
{"created":"2024-05-14 05:57:03","title":"X-ray imaging camera using INTPIX4NA SOIPIX detector with SiTCP-XG 10GbE based high-speed readout system","abstract":"The silicon-on-insulator pixel (SOIPIX) detector is a unique monolithic-structure imaging device currently being developed by the SOIPIX group led by the High Energy Accelerator Research Organization (KEK). The detector team at KEK Photon Factory (PF) is also developing an X-ray camera using INTPIX4NA with a 14.1 $\\times$ 8.7 $\\mathsf{mm^2}$ sensitive area and 425,984 (832 column $\\times$ 512 row matrix) pixels, with a pixel size of 17 $\\times$ 17 $\\mathsf{\\mu m^2}$. The detector has high resolution and sensitivity for low-intensity X-rays, making it suitable for imaging in optical systems with lower X-ray intensities, such as an X-ray zooming microscope using two Fresnel zone plates (FZPs), which is also under development at PF. To enable imaging under such conditions, we developed a detector cooling system using a Peltier element to support longer exposure time (~0.5 seconds per frame). Additionally, we developed a new readout system using DAQ boards developed by PF, equipped with SiTCP-XG (network controller implemented on field-programmable gate array) that supports 10 Gbps Ethernet for high-frame-rate imaging at several hundred hertz. The new X-ray camera was tested at the PF BL-14A, BL-14B, and AR-NE1A experimental stations, and the resolution and sensitivity characteristics were confirmed. Given these confirmed characteristics, this X-ray camera is suitable for X-ray imaging using 5--20 keV X-rays under low-intensity, low-contrast conditions. These conditions are ideal for capturing soft tissues with poor contrast, objects with fine structures, and specimens vulnerable to radiation damage.","sentences":["The silicon-on-insulator pixel (SOIPIX) detector is a unique monolithic-structure imaging device currently being developed by the SOIPIX group led by the High Energy Accelerator Research Organization (KEK).","The detector team at KEK Photon Factory (PF) is also developing an X-ray camera using INTPIX4NA with a 14.1 $\\times$ 8.7 $\\mathsf{mm^2}$ sensitive area and 425,984 (832 column $\\times$ 512 row matrix) pixels, with a pixel size of 17 $\\times$ 17 $\\mathsf{\\mu m^2}$.","The detector has high resolution and sensitivity for low-intensity X-rays, making it suitable for imaging in optical systems with lower X-ray intensities, such as an X-ray zooming microscope using two Fresnel zone plates (FZPs), which is also under development at PF.","To enable imaging under such conditions, we developed a detector cooling system using a Peltier element to support longer exposure time (~0.5 seconds per frame).","Additionally, we developed a new readout system using DAQ boards developed by PF, equipped with SiTCP-XG (network controller implemented on field-programmable gate array) that supports 10 Gbps Ethernet for high-frame-rate imaging at several hundred hertz.","The new X-ray camera was tested at the PF BL-14A, BL-14B, and AR-NE1A experimental stations, and the resolution and sensitivity characteristics were confirmed.","Given these confirmed characteristics, this X-ray camera is suitable for X-ray imaging using 5--20 keV X-rays under low-intensity, low-contrast conditions.","These conditions are ideal for capturing soft tissues with poor contrast, objects with fine structures, and specimens vulnerable to radiation damage."],"url":"http://arxiv.org/abs/2405.08330v1","category":"physics.ins-det"}
{"created":"2024-05-14 05:38:56","title":"Fine residual stress distribution measurement of steel materials by SOI pixel detector with synchrotron X-rays","abstract":"Residual stress is an important factor governing evaluating and controlling the quality of metal materials in industrial products. X-ray measurements provide one of the most effective means of evaluating residual stress without destruction. In such measurements, the effects of residual stress on the crystal structure can be observed through the Debye ring deformation.   In previous studies, we developed a residual stress measurement system based on the $cos \\alpha$ method, using a two-dimensional (2D) silicon-on-insulator pixel (SOIPIX) detector known as INTPIX4. In a typical laboratory setup, this system requires only 1 second to measure a specified point. This is drastically faster than the conventional system based on the $sin^{2} \\psi$ method, which requires more than 10 min, and the $cos \\alpha$-based system using an imaging plate, which requires 1 min. Compared to other systems, it can evaluate the 2D distribution of residual stress faster and provide more detailed information for evaluating materials. We first attempted to measure the 2D distribution in a laboratory setup with a Cr X-ray tube (Cr K$\\alpha$ 5.4 keV) and obtained satisfactory results. We subsequently took measurements using synchrotron monochromatic X-rays to determine the fine accuracy and fine sampling pitch distribution. In this paper, we report the results of the initial synchrotron experiment, including the residual stress distribution of the standard specimen obtained by the first prototype setup. Furthermore, we compare the synchrotron measurements with those from the laboratory.","sentences":["Residual stress is an important factor governing evaluating and controlling the quality of metal materials in industrial products.","X-ray measurements provide one of the most effective means of evaluating residual stress without destruction.","In such measurements, the effects of residual stress on the crystal structure can be observed through the Debye ring deformation.   ","In previous studies, we developed a residual stress measurement system based on the $cos \\alpha$ method, using a two-dimensional (2D) silicon-on-insulator pixel (SOIPIX) detector known as INTPIX4.","In a typical laboratory setup, this system requires only 1 second to measure a specified point.","This is drastically faster than the conventional system based on the $sin^{2} \\psi$ method, which requires more than 10 min, and the $cos \\alpha$-based system using an imaging plate, which requires 1 min. Compared to other systems, it can evaluate the 2D distribution of residual stress faster and provide more detailed information for evaluating materials.","We first attempted to measure the 2D distribution in a laboratory setup with a Cr X-ray tube (Cr K$\\alpha$ 5.4 keV) and obtained satisfactory results.","We subsequently took measurements using synchrotron monochromatic X-rays to determine the fine accuracy and fine sampling pitch distribution.","In this paper, we report the results of the initial synchrotron experiment, including the residual stress distribution of the standard specimen obtained by the first prototype setup.","Furthermore, we compare the synchrotron measurements with those from the laboratory."],"url":"http://arxiv.org/abs/2405.08321v1","category":"physics.ins-det"}
{"created":"2024-05-14 04:42:00","title":"Independent Range Sampling on Interval Data (Longer Version)","abstract":"Many applications require efficient management of large sets of intervals because many objects are associated with intervals (e.g., time and price intervals). In such interval management systems, range search is a primitive operator for retrieving and analysis tasks. As dataset sizes are growing nowadays, range search results are also becoming larger, which may overwhelm users and incur long computation time. Because applications are usually satisfied with a subset of the result set, it is desirable to efficiently obtain only small samples from the result set.We therefore address the problem of independent range sampling on interval data, which outputs $s$ random samples that overlap a given query interval and are independent of the samples of all previous queries. To efficiently solve this problem theoretically and practically, we propose a variant of an interval tree, namely the augmented interval tree (or AIT), and we show that there exists an exact algorithm that needs $O(n \\log n)$ space and $O(\\log^{2} n + s)$ time, where $n$ is the dataset size. The simple structure of an AIT provides flexible extensions: (i) its time and space complexities respectively become $O(\\log^{2} n + s)$ expected and $O(n)$ by bucketing intervals and (ii) it can deal with weighted intervals and outputs $s$ weighted random samples in $O(\\log^{2} n+s\\log n)$ time. We conduct extensive experiments on real datasets, and the results demonstrate that our algorithms significantly outperform competitors.","sentences":["Many applications require efficient management of large sets of intervals because many objects are associated with intervals (e.g., time and price intervals).","In such interval management systems, range search is a primitive operator for retrieving and analysis tasks.","As dataset sizes are growing nowadays, range search results are also becoming larger, which may overwhelm users and incur long computation time.","Because applications are usually satisfied with a subset of the result set, it is desirable to efficiently obtain only small samples from the result set.","We therefore address the problem of independent range sampling on interval data, which outputs $s$ random samples that overlap a given query interval and are independent of the samples of all previous queries.","To efficiently solve this problem theoretically and practically, we propose a variant of an interval tree, namely the augmented interval tree (or AIT), and we show that there exists an exact algorithm that needs $O(n \\log n)$ space and $O(\\log^{2} n + s)$ time, where $n$ is the dataset size.","The simple structure of an AIT provides flexible extensions: (i) its time and space complexities respectively become $O(\\log^{2} n + s)$ expected and $O(n)$ by bucketing intervals and (ii) it can deal with weighted intervals and outputs $s$ weighted random samples in $O(\\log^{2} n+s\\log n)$ time.","We conduct extensive experiments on real datasets, and the results demonstrate that our algorithms significantly outperform competitors."],"url":"http://arxiv.org/abs/2405.08315v1","category":"cs.DB"}
{"created":"2024-05-14 04:38:39","title":"Establishing Heuristics for Improving the Usability of GUI Machine Learning Tools for Novice Users","abstract":"Machine learning (ML) tools with graphical user interfaces (GUI) are facing demand from novice users who do not have the background of their underlying concepts. These tools are frequently complex and pose unique challenges in terms of interaction and comprehension by novice users. There is yet to be an established set of usability heuristics to guide and assess GUI ML tool design. To address this gap, in this paper, we extend Nielsen's heuristics for evaluating GUI ML Tools through a set of empirical evaluations. To validate the proposed heuristics, user testing was conducted by novice users on a prototype that reflects those heuristics. Based on the results of the evaluations, our new heuristics set improves upon existing heuristics in the context of ML tools. It can serve as a resource for practitioners designing and evaluating these tools.","sentences":["Machine learning (ML) tools with graphical user interfaces (GUI) are facing demand from novice users who do not have the background of their underlying concepts.","These tools are frequently complex and pose unique challenges in terms of interaction and comprehension by novice users.","There is yet to be an established set of usability heuristics to guide and assess GUI ML tool design.","To address this gap, in this paper, we extend Nielsen's heuristics for evaluating GUI ML Tools through a set of empirical evaluations.","To validate the proposed heuristics, user testing was conducted by novice users on a prototype that reflects those heuristics.","Based on the results of the evaluations, our new heuristics set improves upon existing heuristics in the context of ML tools.","It can serve as a resource for practitioners designing and evaluating these tools."],"url":"http://arxiv.org/abs/2405.08313v1","category":"cs.HC"}
{"created":"2024-05-14 04:18:33","title":"Online Test-time Adaptation for Interatomic Potentials","abstract":"Machine learning interatomic potentials (MLIPs) enable more efficient molecular dynamics (MD) simulations with ab initio accuracy, which have been used in various domains of physical science. However, distribution shift between training and test data causes deterioration of the test performance of MLIPs, and even leads to collapse of MD simulations. In this work, we propose an online Test-time Adaptation Interatomic Potential (TAIP) framework to improve the generalization on test data. Specifically, we design a dual-level self-supervised learning approach that leverages global structure and atomic local environment information to align the model with the test data. Extensive experiments demonstrate TAIP's capability to bridge the domain gap between training and test dataset without additional data. TAIP enhances the test performance on various benchmarks, from small molecule datasets to complex periodic molecular systems with various types of elements. Remarkably, it also enables stable MD simulations where the corresponding baseline models collapse.","sentences":["Machine learning interatomic potentials (MLIPs) enable more efficient molecular dynamics (MD) simulations with ab initio accuracy, which have been used in various domains of physical science.","However, distribution shift between training and test data causes deterioration of the test performance of MLIPs, and even leads to collapse of MD simulations.","In this work, we propose an online Test-time Adaptation Interatomic Potential (TAIP) framework to improve the generalization on test data.","Specifically, we design a dual-level self-supervised learning approach that leverages global structure and atomic local environment information to align the model with the test data.","Extensive experiments demonstrate TAIP's capability to bridge the domain gap between training and test dataset without additional data.","TAIP enhances the test performance on various benchmarks, from small molecule datasets to complex periodic molecular systems with various types of elements.","Remarkably, it also enables stable MD simulations where the corresponding baseline models collapse."],"url":"http://arxiv.org/abs/2405.08308v1","category":"physics.comp-ph"}
{"created":"2024-05-14 04:17:17","title":"Sequential Maximal Updated Density Parameter Estimation for Dynamical Systems with Parameter Drift","abstract":"We present a novel method for generating sequential parameter estimates and quantifying epistemic uncertainty in dynamical systems within a data-consistent (DC) framework. The DC framework differs from traditional Bayesian approaches due to the incorporation of the push-forward of an initial density, which performs selective regularization in parameter directions not informed by the data in the resulting updated density. This extends a previous study that included the linear Gaussian theory within the DC framework and introduced the maximal updated density (MUD) estimate as an alternative to both least squares and maximum a posterior (MAP) estimates. In this work, we introduce algorithms for operational settings of MUD estimation in real or near-real time where spatio-temporal datasets arrive in packets to provide updated estimates of parameters and identify potential parameter drift. Computational diagnostics within the DC framework prove critical for evaluating (1) the quality of the DC update and MUD estimate and (2) the detection of parameter value drift. The algorithms are applied to estimate (1) wind drag parameters in a high-fidelity storm surge model, (2) thermal diffusivity field for a heat conductivity problem, and (3) changing infection and incubation rates of an epidemiological model.","sentences":["We present a novel method for generating sequential parameter estimates and quantifying epistemic uncertainty in dynamical systems within a data-consistent (DC) framework.","The DC framework differs from traditional Bayesian approaches due to the incorporation of the push-forward of an initial density, which performs selective regularization in parameter directions not informed by the data in the resulting updated density.","This extends a previous study that included the linear Gaussian theory within the DC framework and introduced the maximal updated density (MUD) estimate as an alternative to both least squares and maximum a posterior (MAP) estimates.","In this work, we introduce algorithms for operational settings of MUD estimation in real or near-real time where spatio-temporal datasets arrive in packets to provide updated estimates of parameters and identify potential parameter drift.","Computational diagnostics within the DC framework prove critical for evaluating (1) the quality of the DC update and MUD estimate and (2) the detection of parameter value drift.","The algorithms are applied to estimate (1) wind drag parameters in a high-fidelity storm surge model, (2) thermal diffusivity field for a heat conductivity problem, and (3) changing infection and incubation rates of an epidemiological model."],"url":"http://arxiv.org/abs/2405.08307v1","category":"stat.ME"}
{"created":"2024-05-14 04:12:11","title":"Flight Path Optimization with Optimal Control Method","abstract":"This paper is based on a crucial issue in the aviation world: how to optimize the trajectory and controls given to the aircraft in order to optimize flight time and fuel consumption. This study aims to provide elements of a response to this problem and to define, under certain simplifying assumptions, an optimal response, using Constrained Finite Time Optimal Control(CFTOC). The first step is to define the dynamic model of the aircraft in accordance with the controllable inputs and wind disturbances. Then we will identify a precise objective in terms of optimization and implement an optimization program to solve it under the circumstances of simulated real flight situation. Finally, the optimization result is validated and discussed by different scenarios.","sentences":["This paper is based on a crucial issue in the aviation world: how to optimize the trajectory and controls given to the aircraft in order to optimize flight time and fuel consumption.","This study aims to provide elements of a response to this problem and to define, under certain simplifying assumptions, an optimal response, using Constrained Finite Time Optimal Control(CFTOC).","The first step is to define the dynamic model of the aircraft in accordance with the controllable inputs and wind disturbances.","Then we will identify a precise objective in terms of optimization and implement an optimization program to solve it under the circumstances of simulated real flight situation.","Finally, the optimization result is validated and discussed by different scenarios."],"url":"http://arxiv.org/abs/2405.08306v1","category":"math.OC"}
{"created":"2024-05-14 04:01:32","title":"Collateral Portfolio Optimization in Crypto-Backed Stablecoins","abstract":"Stablecoins - crypto tokens whose value is pegged to a real-world asset such as the US Dollar - are an important component of the DeFi ecosystem as they mitigate the impact of token price volatility. In crypto-backed stablecoins, the peg is founded on the guarantee that in case of system shutdown, each stablecoin can be exchanged for a basket of other crypto tokens worth approximately its nominal value. However, price fluctuations that affect the collateral tokens may cause this guarantee to be invalidated. In this work, we investigate the impact of the collateral portfolio's composition on the resilience to this type of catastrophic event. For stablecoins whose developers maintain a significant portion of the collateral (e.g., MakerDAO's Dai), we propose two portfolio optimization methods, based on convex optimization and (semi)variance minimization, that account for the correlation between the various token prices. We compare the optimal portfolios to the historical evolution of Dai's collateral portfolio, and to aid reproducibility, we have made our data and code publicly available.","sentences":["Stablecoins - crypto tokens whose value is pegged to a real-world asset such as the US Dollar - are an important component of the DeFi ecosystem as they mitigate the impact of token price volatility.","In crypto-backed stablecoins, the peg is founded on the guarantee that in case of system shutdown, each stablecoin can be exchanged for a basket of other crypto tokens worth approximately its nominal value.","However, price fluctuations that affect the collateral tokens may cause this guarantee to be invalidated.","In this work, we investigate the impact of the collateral portfolio's composition on the resilience to this type of catastrophic event.","For stablecoins whose developers maintain a significant portion of the collateral (e.g., MakerDAO's Dai), we propose two portfolio optimization methods, based on convex optimization and (semi)variance minimization, that account for the correlation between the various token prices.","We compare the optimal portfolios to the historical evolution of Dai's collateral portfolio, and to aid reproducibility, we have made our data and code publicly available."],"url":"http://arxiv.org/abs/2405.08305v1","category":"cs.CR"}
{"created":"2024-05-14 03:48:45","title":"Deep Reinforcement Learning for Real-Time Ground Delay Program Revision and Corresponding Flight Delay Assignments","abstract":"This paper explores the optimization of Ground Delay Programs (GDP), a prevalent Traffic Management Initiative used in Air Traffic Management (ATM) to reconcile capacity and demand discrepancies at airports. Employing Reinforcement Learning (RL) to manage the inherent uncertainties in the national airspace system-such as weather variability, fluctuating flight demands, and airport arrival rates-we developed two RL models: Behavioral Cloning (BC) and Conservative Q-Learning (CQL). These models are designed to enhance GDP efficiency by utilizing a sophisticated reward function that integrates ground and airborne delays and terminal area congestion. We constructed a simulated single-airport environment, SAGDP_ENV, which incorporates real operational data along with predicted uncertainties to facilitate realistic decision-making scenarios. Utilizing the whole year 2019 data from Newark Liberty International Airport (EWR), our models aimed to preemptively set airport program rates. Despite thorough modeling and simulation, initial outcomes indicated that the models struggled to learn effectively, attributed potentially to oversimplified environmental assumptions. This paper discusses the challenges encountered, evaluates the models' performance against actual operational data, and outlines future directions to refine RL applications in ATM.","sentences":["This paper explores the optimization of Ground Delay Programs (GDP), a prevalent Traffic Management Initiative used in Air Traffic Management (ATM) to reconcile capacity and demand discrepancies at airports.","Employing Reinforcement Learning (RL) to manage the inherent uncertainties in the national airspace system-such as weather variability, fluctuating flight demands, and airport arrival rates-we developed two RL models: Behavioral Cloning (BC) and Conservative Q-Learning (CQL).","These models are designed to enhance GDP efficiency by utilizing a sophisticated reward function that integrates ground and airborne delays and terminal area congestion.","We constructed a simulated single-airport environment, SAGDP_ENV, which incorporates real operational data along with predicted uncertainties to facilitate realistic decision-making scenarios.","Utilizing the whole year 2019 data from Newark Liberty International Airport (EWR), our models aimed to preemptively set airport program rates.","Despite thorough modeling and simulation, initial outcomes indicated that the models struggled to learn effectively, attributed potentially to oversimplified environmental assumptions.","This paper discusses the challenges encountered, evaluates the models' performance against actual operational data, and outlines future directions to refine RL applications in ATM."],"url":"http://arxiv.org/abs/2405.08298v1","category":"cs.LG"}
{"created":"2024-05-14 18:14:05","title":"Quantum oscillations in the hole-doped cuprates and the confinement of spinons","abstract":"A long standing problem in the study of the under-hole-doped cuprates has been the description of the Fermi surfaces underlying the high magnetic field quantum oscillations. Harrison and Sebastian (arXiv:1103.4181) proposed that the higher temperature pseudogap 'Fermi arcs' are reconstructed into an electron pocket by field-induced charge density wave order. But computations on such a model (Zhang and Mei, arXiv:1411.2098) show an unobserved additional oscillation frequency from a Fermi surface arising from the backsides of the hole pockets completing the Fermi arcs. We describe a transition from a fractionalized Fermi liquid (FL*) model of the pseudogap metal, to a metal with bi-directional charge density wave order without fractionalization. We show that the confinement of the fermionic spinon excitations of the FL* across this transition can eliminate the unobserved oscillation frequency, and also account for the excess observed linear-in-temperature specific heat (C. Girod et al., arXiv:2004.03650).","sentences":["A long standing problem in the study of the under-hole-doped cuprates has been the description of the Fermi surfaces underlying the high magnetic field quantum oscillations.","Harrison and Sebastian (arXiv:1103.4181) proposed that the higher temperature pseudogap 'Fermi arcs' are reconstructed into an electron pocket by field-induced charge density wave order.","But computations on such a model (Zhang and Mei, arXiv:1411.2098) show an unobserved additional oscillation frequency from a Fermi surface arising from the backsides of the hole pockets completing the Fermi arcs.","We describe a transition from a fractionalized Fermi liquid (FL*) model of the pseudogap metal, to a metal with bi-directional charge density wave order without fractionalization.","We show that the confinement of the fermionic spinon excitations of the FL* across this transition can eliminate the unobserved oscillation frequency, and also account for the excess observed linear-in-temperature specific heat (C. Girod et al., arXiv:2004.03650)."],"url":"http://arxiv.org/abs/2405.08817v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 17:55:32","title":"Quantum computing with Qiskit","abstract":"We describe Qiskit, a software development kit for quantum information science. We discuss the key design decisions that have shaped its development, and examine the software architecture and its core components. We demonstrate an end-to-end workflow for solving a problem in condensed matter physics on a quantum computer that serves to highlight some of Qiskit's capabilities, for example the representation and optimization of circuits at various abstraction levels, its scalability and retargetability to new gates, and the use of quantum-classical computations via dynamic circuits. Lastly, we discuss some of the ecosystem of tools and plugins that extend Qiskit for various tasks, and the future ahead.","sentences":["We describe Qiskit, a software development kit for quantum information science.","We discuss the key design decisions that have shaped its development, and examine the software architecture and its core components.","We demonstrate an end-to-end workflow for solving a problem in condensed matter physics on a quantum computer that serves to highlight some of Qiskit's capabilities, for example the representation and optimization of circuits at various abstraction levels, its scalability and retargetability to new gates, and the use of quantum-classical computations via dynamic circuits.","Lastly, we discuss some of the ecosystem of tools and plugins that extend Qiskit for various tasks, and the future ahead."],"url":"http://arxiv.org/abs/2405.08810v2","category":"quant-ph"}
{"created":"2024-05-14 17:53:08","title":"Bounds on the Distribution of a Sum of Two Random Variables: Revisiting a problem of Kolmogorov with application to Individual Treatment Effects","abstract":"We revisit the following problem, proposed by Kolmogorov: given prescribed marginal distributions $F$ and $G$ for random variables $X,Y$ respectively, characterize the set of compatible distribution functions for the sum $Z=X+Y$. Bounds on the distribution function for $Z$ were given by Markarov (1982), and Frank et al. (1987), the latter using copula theory. However, though they obtain the same bounds, they make different assertions concerning their sharpness. In addition, their solutions leave some open problems in the case when the given marginal distribution functions are discontinuous. These issues have led to some confusion and erroneous statements in subsequent literature, which we correct.   Kolmogorov's problem is closely related to inferring possible distributions for individual treatment effects $Y_1 - Y_0$ given the marginal distributions of $Y_1$ and $Y_0$; the latter being identified from a randomized experiment. We use our new insights to sharpen and correct results due to Fan and Park (2010) concerning individual treatment effects, and to fill some other logical gaps.","sentences":["We revisit the following problem, proposed by Kolmogorov: given prescribed marginal distributions $F$ and $G$ for random variables $X,Y$ respectively, characterize the set of compatible distribution functions for the sum $Z=X+Y$. Bounds on the distribution function for $Z$ were given by Markarov (1982), and Frank et al. (1987), the latter using copula theory.","However, though they obtain the same bounds, they make different assertions concerning their sharpness.","In addition, their solutions leave some open problems in the case when the given marginal distribution functions are discontinuous.","These issues have led to some confusion and erroneous statements in subsequent literature, which we correct.   ","Kolmogorov's problem is closely related to inferring possible distributions for individual treatment effects $Y_1 - Y_0$ given the marginal distributions of $Y_1$ and $Y_0$; the latter being identified from a randomized experiment.","We use our new insights to sharpen and correct results due to Fan and Park (2010) concerning individual treatment effects, and to fill some other logical gaps."],"url":"http://arxiv.org/abs/2405.08806v1","category":"math.ST"}
{"created":"2024-05-14 17:52:06","title":"Special potentials for relativistic Laplacians I: Fractional Rollnik-class","abstract":"We propose a counterpart of the classical Rollnik-class of potentials for fractional and massive relativistic Laplacians, and describe this space in terms of appropriate Riesz potentials. These definitions rely on precise resolvent estimates. We show that Coulomb-type potentials are elements of fractional Rollnik-class up to but not including the critical singularity of the Hardy potential. For the operators with fractional exponent $\\alpha = 1$ there exists no fractional Rollnik potential, however, in low dimensions we make sense of these classes as limiting cases by using $\\Gamma$-convergence. In a second part of the paper we derive detailed results on the self-adjointness and spectral properties of relativistic Schr\\\"odinger operators obtained under perturbations by fractional Rollnik potentials. We also define an extended fractional Rollnik-class which is the maximal space for the Hilbert-Schmidt property of the related Birman-Schwinger operators.","sentences":["We propose a counterpart of the classical Rollnik-class of potentials for fractional and massive relativistic Laplacians, and describe this space in terms of appropriate Riesz potentials.","These definitions rely on precise resolvent estimates.","We show that Coulomb-type potentials are elements of fractional Rollnik-class up to but not including the critical singularity of the Hardy potential.","For the operators with fractional exponent $\\alpha = 1$ there exists no fractional Rollnik potential, however, in low dimensions we make sense of these classes as limiting cases by using $\\Gamma$-convergence.","In a second part of the paper we derive detailed results on the self-adjointness and spectral properties of relativistic Schr\\\"odinger operators obtained under perturbations by fractional Rollnik potentials.","We also define an extended fractional Rollnik-class which is the maximal space for the Hilbert-Schmidt property of the related Birman-Schwinger operators."],"url":"http://arxiv.org/abs/2405.08805v1","category":"math.FA"}
{"created":"2024-05-14 17:44:34","title":"Ambiguous Annotations: When is a Pedestrian not a Pedestrian?","abstract":"Datasets labelled by human annotators are widely used in the training and testing of machine learning models. In recent years, researchers are increasingly paying attention to label quality. However, it is not always possible to objectively determine whether an assigned label is correct or not. The present work investigates this ambiguity in the annotation of autonomous driving datasets as an important dimension of data quality. Our experiments show that excluding highly ambiguous data from the training improves model performance of a state-of-the-art pedestrian detector in terms of LAMR, precision and F1 score, thereby saving training time and annotation costs. Furthermore, we demonstrate that, in order to safely remove ambiguous instances and ensure the retained representativeness of the training data, an understanding of the properties of the dataset and class under investigation is crucial.","sentences":["Datasets labelled by human annotators are widely used in the training and testing of machine learning models.","In recent years, researchers are increasingly paying attention to label quality.","However, it is not always possible to objectively determine whether an assigned label is correct or not.","The present work investigates this ambiguity in the annotation of autonomous driving datasets as an important dimension of data quality.","Our experiments show that excluding highly ambiguous data from the training improves model performance of a state-of-the-art pedestrian detector in terms of LAMR, precision and F1 score, thereby saving training time and annotation costs.","Furthermore, we demonstrate that, in order to safely remove ambiguous instances and ensure the retained representativeness of the training data, an understanding of the properties of the dataset and class under investigation is crucial."],"url":"http://arxiv.org/abs/2405.08794v1","category":"cs.CV"}
{"created":"2024-05-14 17:25:37","title":"The Developing Human Connectome Project: A Fast Deep Learning-based Pipeline for Neonatal Cortical Surface Reconstruction","abstract":"The Developing Human Connectome Project (dHCP) aims to explore developmental patterns of the human brain during the perinatal period. An automated processing pipeline has been developed to extract high-quality cortical surfaces from structural brain magnetic resonance (MR) images for the dHCP neonatal dataset. However, the current implementation of the pipeline requires more than 6.5 hours to process a single MRI scan, making it expensive for large-scale neuroimaging studies. In this paper, we propose a fast deep learning (DL) based pipeline for dHCP neonatal cortical surface reconstruction, incorporating DL-based brain extraction, cortical surface reconstruction and spherical projection, as well as GPU-accelerated cortical surface inflation and cortical feature estimation. We introduce a multiscale deformation network to learn diffeomorphic cortical surface reconstruction end-to-end from T2-weighted brain MRI. A fast unsupervised spherical mapping approach is integrated to minimize metric distortions between cortical surfaces and projected spheres. The entire workflow of our DL-based dHCP pipeline completes within only 24 seconds on a modern GPU, which is nearly 1000 times faster than the original dHCP pipeline. Manual quality control demonstrates that for 82.5% of the test samples, our DL-based pipeline produces superior (54.2%) or equal quality (28.3%) cortical surfaces compared to the original dHCP pipeline.","sentences":["The Developing Human Connectome Project (dHCP) aims to explore developmental patterns of the human brain during the perinatal period.","An automated processing pipeline has been developed to extract high-quality cortical surfaces from structural brain magnetic resonance (MR) images for the dHCP neonatal dataset.","However, the current implementation of the pipeline requires more than 6.5 hours to process a single MRI scan, making it expensive for large-scale neuroimaging studies.","In this paper, we propose a fast deep learning (DL) based pipeline for dHCP neonatal cortical surface reconstruction, incorporating DL-based brain extraction, cortical surface reconstruction and spherical projection, as well as GPU-accelerated cortical surface inflation and cortical feature estimation.","We introduce a multiscale deformation network to learn diffeomorphic cortical surface reconstruction end-to-end from T2-weighted brain MRI.","A fast unsupervised spherical mapping approach is integrated to minimize metric distortions between cortical surfaces and projected spheres.","The entire workflow of our DL-based dHCP pipeline completes within only 24 seconds on a modern GPU, which is nearly 1000 times faster than the original dHCP pipeline.","Manual quality control demonstrates that for 82.5% of the test samples, our DL-based pipeline produces superior (54.2%) or equal quality (28.3%) cortical surfaces compared to the original dHCP pipeline."],"url":"http://arxiv.org/abs/2405.08783v1","category":"eess.IV"}
{"created":"2024-05-14 16:59:20","title":"Energy-based Hopfield Boosting for Out-of-Distribution Detection","abstract":"Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100.","sentences":["Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world.","Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies.","We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data.","Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data.","Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100."],"url":"http://arxiv.org/abs/2405.08766v1","category":"cs.LG"}
{"created":"2024-05-14 16:53:14","title":"S3C2 Summit 2024-03: Industry Secure Supply Chain Summit","abstract":"Supply chain security has become a very important vector to consider when defending against adversary attacks. Due to this, more and more developers are keen on improving their supply chains to make them more robust against future threats. On March 7th, 2024 researchers from the Secure Software Supply Chain Center (S3C2) gathered 14 industry leaders, developers and consumers of the open source ecosystem to discuss the state of supply chain security. The goal of the summit is to share insights between companies and developers alike to foster new collaborations and ideas moving forward. Through this meeting, participants were questions on best practices and thoughts how to improve things for the future. In this paper we summarize the responses and discussions of the summit. The panel questions can be found in the appendix.","sentences":["Supply chain security has become a very important vector to consider when defending against adversary attacks.","Due to this, more and more developers are keen on improving their supply chains to make them more robust against future threats.","On March 7th, 2024 researchers from the Secure Software Supply Chain Center (S3C2) gathered 14 industry leaders, developers and consumers of the open source ecosystem to discuss the state of supply chain security.","The goal of the summit is to share insights between companies and developers alike to foster new collaborations and ideas moving forward.","Through this meeting, participants were questions on best practices and thoughts how to improve things for the future.","In this paper we summarize the responses and discussions of the summit.","The panel questions can be found in the appendix."],"url":"http://arxiv.org/abs/2405.08762v1","category":"cs.CR"}
{"created":"2024-05-14 16:49:26","title":"Effect of injection conditions on the non-linear behavior of the ECDI and related turbulent transport","abstract":"The electron-cyclotron drift instability (ECDI) has been proposed as one of the main actors behind the anomalous transport of electrons in Hall thruster devices. In this work, we revisit the theory and perform two-dimensional PIC simulations under several conditions to analyze the non-linear behavior and the induced transport under several boundary conditions. Simulation results with fully-periodic boundaries and conditions faithful to the linear theory show the growth of ECDI modes, ion-wave trapping vortexes and agree with the existing literature in early times. In the long term, however, we observe very mild oscillations and null anomalous current. The evolution towards this new equilibrium is coherent to what can be expected from energy conservation. The quenching of the oscillations seem to be highly related with the distortion of ion-trapping vortexes in phase space after a long-term interaction of ion particles with the electrostatic wave. This result suggests that sustained oscillations and turbulent current could benefit from the renewal of ions by, e.g., removing and injecting particles through axial boundaries instead of applying periodicity. This second type of simulations shows that injection conditions highly impact the late simulation behavior of ECDI oscillations, where we identify several regimes depending on the value of the ion residence time compared to the characteristic saturation time in the fully periodic case. The intermediate regime, where these two times are close, is the only one providing sustained oscillations and electron transport and seems to be the relevant one in Hall devices.","sentences":["The electron-cyclotron drift instability (ECDI) has been proposed as one of the main actors behind the anomalous transport of electrons in Hall thruster devices.","In this work, we revisit the theory and perform two-dimensional PIC simulations under several conditions to analyze the non-linear behavior and the induced transport under several boundary conditions.","Simulation results with fully-periodic boundaries and conditions faithful to the linear theory show the growth of ECDI modes, ion-wave trapping vortexes and agree with the existing literature in early times.","In the long term, however, we observe very mild oscillations and null anomalous current.","The evolution towards this new equilibrium is coherent to what can be expected from energy conservation.","The quenching of the oscillations seem to be highly related with the distortion of ion-trapping vortexes in phase space after a long-term interaction of ion particles with the electrostatic wave.","This result suggests that sustained oscillations and turbulent current could benefit from the renewal of ions by, e.g., removing and injecting particles through axial boundaries instead of applying periodicity.","This second type of simulations shows that injection conditions highly impact the late simulation behavior of ECDI oscillations, where we identify several regimes depending on the value of the ion residence time compared to the characteristic saturation time in the fully periodic case.","The intermediate regime, where these two times are close, is the only one providing sustained oscillations and electron transport and seems to be the relevant one in Hall devices."],"url":"http://arxiv.org/abs/2405.08761v1","category":"physics.plasm-ph"}
{"created":"2024-05-14 16:24:56","title":"Calibrated sensitivity models","abstract":"In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses. However, the sensitivity parameter in these models -- which quantifies the degree of unmeasured confounding -- is often difficult to interpret. For this reason, researchers will sometimes compare the magnitude of the sensitivity parameter to an estimate for measured confounding. This is known as calibration. We propose novel calibrated sensitivity models, which directly incorporate measured confounding, and bound the degree of unmeasured confounding by a multiple of measured confounding. We illustrate how to construct calibrated sensitivity models via several examples. We also demonstrate their advantages over standard sensitivity analyses and calibration; in particular, the calibrated sensitivity parameter is an intuitive unit-less ratio of unmeasured divided by measured confounding, unlike standard sensitivity parameters, and one can correctly incorporate uncertainty due to estimating measured confounding, which standard calibration methods fail to do. By incorporating uncertainty due to measured confounding, we observe that causal analyses can be less robust or more robust to unmeasured confounding than would have been shown with standard approaches. We develop efficient estimators and methods for inference for bounds on the average treatment effect with three calibrated sensitivity models, and establish that our estimators are doubly robust and attain parametric efficiency and asymptotic normality under nonparametric conditions on their nuisance function estimators. We illustrate our methods with data analyses on the effect of exposure to violence on attitudes towards peace in Darfur and the effect of mothers' smoking on infant birthweight.","sentences":["In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses.","However, the sensitivity parameter in these models -- which quantifies the degree of unmeasured confounding -- is often difficult to interpret.","For this reason, researchers will sometimes compare the magnitude of the sensitivity parameter to an estimate for measured confounding.","This is known as calibration.","We propose novel calibrated sensitivity models, which directly incorporate measured confounding, and bound the degree of unmeasured confounding by a multiple of measured confounding.","We illustrate how to construct calibrated sensitivity models via several examples.","We also demonstrate their advantages over standard sensitivity analyses and calibration; in particular, the calibrated sensitivity parameter is an intuitive unit-less ratio of unmeasured divided by measured confounding, unlike standard sensitivity parameters, and one can correctly incorporate uncertainty due to estimating measured confounding, which standard calibration methods fail to do.","By incorporating uncertainty due to measured confounding, we observe that causal analyses can be less robust or more robust to unmeasured confounding than would have been shown with standard approaches.","We develop efficient estimators and methods for inference for bounds on the average treatment effect with three calibrated sensitivity models, and establish that our estimators are doubly robust and attain parametric efficiency and asymptotic normality under nonparametric conditions on their nuisance function estimators.","We illustrate our methods with data analyses on the effect of exposure to violence on attitudes towards peace in Darfur and the effect of mothers' smoking on infant birthweight."],"url":"http://arxiv.org/abs/2405.08738v1","category":"stat.ME"}
{"created":"2024-05-14 16:19:13","title":"A Simple Approach to Differentiable Rendering of SDFs","abstract":"We present a simple algorithm for differentiable rendering of surfaces represented by Signed Distance Fields (SDF), which makes it easy to integrate rendering into gradient-based optimization pipelines. To tackle visibility-related derivatives that make rendering non-differentiable, existing physically based differentiable rendering methods often rely on elaborate guiding data structures or reparameterization with a global impact on variance. In this article, we investigate an alternative that embraces nonzero bias in exchange for low variance and architectural simplicity. Our method expands the lower-dimensional boundary integral into a thin band that is easy to sample when the underlying surface is represented by an SDF. We demonstrate the performance and robustness of our formulation in end-to-end inverse rendering tasks, where it obtains results that are competitive with or superior to existing work.","sentences":["We present a simple algorithm for differentiable rendering of surfaces represented by Signed Distance Fields (SDF), which makes it easy to integrate rendering into gradient-based optimization pipelines.","To tackle visibility-related derivatives that make rendering non-differentiable, existing physically based differentiable rendering methods often rely on elaborate guiding data structures or reparameterization with a global impact on variance.","In this article, we investigate an alternative that embraces nonzero bias in exchange for low variance and architectural simplicity.","Our method expands the lower-dimensional boundary integral into a thin band that is easy to sample when the underlying surface is represented by an SDF.","We demonstrate the performance and robustness of our formulation in end-to-end inverse rendering tasks, where it obtains results that are competitive with or superior to existing work."],"url":"http://arxiv.org/abs/2405.08733v1","category":"cs.GR"}
{"created":"2024-05-14 16:12:47","title":"Intervention effects based on potential benefit","abstract":"Optimal treatment rules are mappings from individual patient characteristics to tailored treatment assignments that maximize mean outcomes. In this work, we introduce a conditional potential benefit (CPB) metric that measures the expected improvement under an optimally chosen treatment compared to the status quo, within covariate strata. The potential benefit combines (i) the magnitude of the treatment effect, and (ii) the propensity for subjects to naturally select a suboptimal treatment. As a consequence, heterogeneity in the CPB can provide key insights into the mechanism by which a treatment acts and/or highlight potential barriers to treatment access or adverse effects. Moreover, we demonstrate that CPB is the natural prioritization score for individualized treatment policies when intervention capacity is constrained. That is, in the resource-limited setting where treatment options are freely accessible, but the ability to intervene on a portion of the target population is constrained (e.g., if the population is large, and follow-up and encouragement of treatment uptake is labor-intensive), targeting subjects with highest CPB maximizes the mean outcome. Focusing on this resource-limited setting, we derive formulas for optimal constrained treatment rules, and for any given budget, quantify the loss compared to the optimal unconstrained rule. We describe sufficient identification assumptions, and propose nonparametric, robust, and efficient estimators of the proposed quantities emerging from our framework.","sentences":["Optimal treatment rules are mappings from individual patient characteristics to tailored treatment assignments that maximize mean outcomes.","In this work, we introduce a conditional potential benefit (CPB) metric that measures the expected improvement under an optimally chosen treatment compared to the status quo, within covariate strata.","The potential benefit combines (i) the magnitude of the treatment effect, and (ii) the propensity for subjects to naturally select a suboptimal treatment.","As a consequence, heterogeneity in the CPB can provide key insights into the mechanism by which a treatment acts and/or highlight potential barriers to treatment access or adverse effects.","Moreover, we demonstrate that CPB is the natural prioritization score for individualized treatment policies when intervention capacity is constrained.","That is, in the resource-limited setting where treatment options are freely accessible, but the ability to intervene on a portion of the target population is constrained (e.g., if the population is large, and follow-up and encouragement of treatment uptake is labor-intensive), targeting subjects with highest CPB maximizes the mean outcome.","Focusing on this resource-limited setting, we derive formulas for optimal constrained treatment rules, and for any given budget, quantify the loss compared to the optimal unconstrained rule.","We describe sufficient identification assumptions, and propose nonparametric, robust, and efficient estimators of the proposed quantities emerging from our framework."],"url":"http://arxiv.org/abs/2405.08727v1","category":"stat.ME"}
{"created":"2024-05-14 15:50:42","title":"Poisson approximation for cycles in the generalised random graph","abstract":"The generalised random graph contains $n$ vertices with positive i.i.d. weights. The probability of adding an edge between two vertices is increasing in their weights. We require the weight distribution to have finite second moments and study the point process $\\mathcal{C}_n$ on $\\{3,4,\\dots\\}$, which counts how many cycles of the respective length are present in the graph. We establish convergence of $\\mathcal{C}_n$ to a Poisson point process. Under the stronger assumption of the weights having finite fourth moments we provide the following results. When $\\mathcal{C}_n$ is evaluated on a bounded set $A$, we provide a rate of convergence. If the graph is additionally subcritical, we extend this to unbounded sets $A$ at the cost of a slower rate of convergence. From this we deduce the limiting distribution of the length of the shortest and the longest cycle when the graph is subcritical, including rates of convergence. All mentioned results also apply to the Chung-Lu model and the Norros-Reittu model.","sentences":["The generalised random graph contains $n$ vertices with positive i.i.d. weights.","The probability of adding an edge between two vertices is increasing in their weights.","We require the weight distribution to have finite second moments and study the point process $\\mathcal{C}_n$ on $\\{3,4,\\dots\\}$, which counts how many cycles of the respective length are present in the graph.","We establish convergence of $\\mathcal{C}_n$ to a Poisson point process.","Under the stronger assumption of the weights having finite fourth moments we provide the following results.","When $\\mathcal{C}_n$ is evaluated on a bounded set $A$, we provide a rate of convergence.","If the graph is additionally subcritical, we extend this to unbounded sets $A$ at the cost of a slower rate of convergence.","From this we deduce the limiting distribution of the length of the shortest and the longest cycle when the graph is subcritical, including rates of convergence.","All mentioned results also apply to the Chung-Lu model and the Norros-Reittu model."],"url":"http://arxiv.org/abs/2405.08708v1","category":"math.PR"}
{"created":"2024-05-14 15:42:27","title":"Using autoencoders and deep transfer learning to determine the stellar parameters of 286 CARMENES M dwarfs","abstract":"Deep learning (DL) techniques are a promising approach among the set of methods used in the ever-challenging determination of stellar parameters in M dwarfs. In this context, transfer learning could play an important role in mitigating uncertainties in the results due to the synthetic gap (i.e. difference in feature distributions between observed and synthetic data). We propose a feature-based deep transfer learning (DTL) approach based on autoencoders to determine stellar parameters from high-resolution spectra. Using this methodology, we provide new estimations for the effective temperature, surface gravity, metallicity, and projected rotational velocity for 286 M dwarfs observed by the CARMENES survey. Using autoencoder architectures, we projected synthetic PHOENIX-ACES spectra and observed CARMENES spectra onto a new feature space of lower dimensionality in which the differences between the two domains are reduced. We used this low-dimensional new feature space as input for a convolutional neural network to obtain the stellar parameter determinations. We performed an extensive analysis of our estimated stellar parameters, ranging from 3050 to 4300 K, 4.7 to 5.1 dex, and -0.53 to 0.25 dex for Teff, logg, and [Fe/H], respectively. Our results are broadly consistent with those of recent studies using CARMENES data, with a systematic deviation in our Teff scale towards hotter values for estimations above 3750 K. Furthermore, our methodology mitigates the deviations in metallicity found in previous DL techniques due to the synthetic gap. We consolidated a DTL-based methodology to determine stellar parameters in M dwarfs from synthetic spectra, with no need for high-quality measurements involved in the knowledge transfer. These results suggest the great potential of DTL to mitigate the differences in feature distributions between the observations and the PHOENIX-ACES spectra.","sentences":["Deep learning (DL) techniques are a promising approach among the set of methods used in the ever-challenging determination of stellar parameters in M dwarfs.","In this context, transfer learning could play an important role in mitigating uncertainties in the results due to the synthetic gap (i.e. difference in feature distributions between observed and synthetic data).","We propose a feature-based deep transfer learning (DTL) approach based on autoencoders to determine stellar parameters from high-resolution spectra.","Using this methodology, we provide new estimations for the effective temperature, surface gravity, metallicity, and projected rotational velocity for 286 M dwarfs observed by the CARMENES survey.","Using autoencoder architectures, we projected synthetic PHOENIX-ACES spectra and observed CARMENES spectra onto a new feature space of lower dimensionality in which the differences between the two domains are reduced.","We used this low-dimensional new feature space as input for a convolutional neural network to obtain the stellar parameter determinations.","We performed an extensive analysis of our estimated stellar parameters, ranging from 3050 to 4300 K, 4.7 to 5.1 dex, and -0.53 to 0.25 dex for Teff, logg, and [Fe/H], respectively.","Our results are broadly consistent with those of recent studies using CARMENES data, with a systematic deviation in our Teff scale towards hotter values for estimations above 3750 K. Furthermore, our methodology mitigates the deviations in metallicity found in previous DL techniques due to the synthetic gap.","We consolidated a DTL-based methodology to determine stellar parameters in M dwarfs from synthetic spectra, with no need for high-quality measurements involved in the knowledge transfer.","These results suggest the great potential of DTL to mitigate the differences in feature distributions between the observations and the PHOENIX-ACES spectra."],"url":"http://arxiv.org/abs/2405.08703v1","category":"astro-ph.SR"}
{"created":"2024-05-14 15:05:16","title":"Selective incorporation of antimony into gallium nitride","abstract":"Dilute concentrations of antimony (Sb) incorporation into GaN induce strong band-gap bowing and tunable room-temperature photoluminescence from the UV to the green spectral regions. However, the atomistic details of the incorporation of Sb into the GaN host remain unclear. In this work, we use first-principles calculations to understand the thermodynamics of Sb substitution into GaN, and its effect on the optical and Raman spectra. Although it is empirically considered that Sb is preferentially incorporated as an anion ($\\mathrm{Sb^{3-}}$) into the N sublattice, we demonstrate that Sb can also be incorporated as a cation ($\\mathrm{Sb^{3+}}$, $\\mathrm{Sb^{5+}}$) into the metal sublattice. Our thermodynamic analysis demonstrates that $\\mathrm{Sb_N^0}$, $\\mathrm{Sb_{Ga}^{2+}}$, and $\\mathrm{Sb_{Ga}^0}$ can co-exist under Ga-rich conditions in n-type samples. We further confirm the dual incorporation of Sb by calculating the vibrational frequencies of different anionic and cation substitutions to explain the origins of experimentally observed additional Raman peaks of Sb-doped GaN. Moreover, the calculated band structures of different Sb substitutions into GaN explain the experimental photoluminescence and optical absorption spectra. Overall, our analysis suggests that the coexistence of $\\mathrm{Sb^{3-}}$, $\\mathrm{Sb^{3+}}$, and $\\mathrm{Sb^{5+}}$ substitutions into GaN explains the totality of experimental measurements. Our results demonstrate that the selective incorporation of Sb into GaN (and potentially other group-V elements such as As, P, or Bi) by tuning the growth conditions can drastically modify the electronic properties, for applications in visible light emitters and photocatalysis.","sentences":["Dilute concentrations of antimony (Sb) incorporation into GaN induce strong band-gap bowing and tunable room-temperature photoluminescence from the UV to the green spectral regions.","However, the atomistic details of the incorporation of Sb into the GaN host remain unclear.","In this work, we use first-principles calculations to understand the thermodynamics of Sb substitution into GaN, and its effect on the optical and Raman spectra.","Although it is empirically considered that Sb is preferentially incorporated as an anion ($\\mathrm{Sb^{3-}}$) into the N sublattice, we demonstrate that Sb can also be incorporated as a cation ($\\mathrm{Sb^{3+}}$, $\\mathrm{Sb^{5+}}$) into the metal sublattice.","Our thermodynamic analysis demonstrates that $\\mathrm{Sb_N^0}$, $\\mathrm{Sb_{Ga}^{2+}}$, and $\\mathrm{Sb_{Ga}^0}$ can co-exist under Ga-rich conditions in n-type samples.","We further confirm the dual incorporation of Sb by calculating the vibrational frequencies of different anionic and cation substitutions to explain the origins of experimentally observed additional Raman peaks of Sb-doped GaN.","Moreover, the calculated band structures of different Sb substitutions into GaN explain the experimental photoluminescence and optical absorption spectra.","Overall, our analysis suggests that the coexistence of $\\mathrm{Sb^{3-}}$, $\\mathrm{Sb^{3+}}$, and $\\mathrm{Sb^{5+}}$ substitutions into GaN explains the totality of experimental measurements.","Our results demonstrate that the selective incorporation of Sb into GaN (and potentially other group-V elements such as As, P, or Bi) by tuning the growth conditions can drastically modify the electronic properties, for applications in visible light emitters and photocatalysis."],"url":"http://arxiv.org/abs/2405.08683v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 14:56:54","title":"Simplifying Debiased Inference via Automatic Differentiation and Probabilistic Programming","abstract":"We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience. 'Dimple' takes as input computer code representing a parameter of interest and outputs an efficient estimator. Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function. Dimple avoids this task by applying automatic differentiation to the statistical functional of interest. Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition. Dimple also uses this composition to determine the nuisances it must estimate. In software, primitives can be implemented independently of one another and reused across different estimation problems. We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code.","sentences":["We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience.","'Dimple' takes as input computer code representing a parameter of interest and outputs an efficient estimator.","Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function.","Dimple avoids this task by applying automatic differentiation to the statistical functional of interest.","Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition.","Dimple also uses this composition to determine the nuisances it must estimate.","In software, primitives can be implemented independently of one another and reused across different estimation problems.","We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code."],"url":"http://arxiv.org/abs/2405.08675v1","category":"stat.ME"}
{"created":"2024-05-14 14:21:55","title":"Certifying Robustness of Graph Convolutional Networks for Node Perturbation with Polyhedra Abstract Interpretation","abstract":"Graph convolutional neural networks (GCNs) are powerful tools for learning graph-based knowledge representations from training data. However, they are vulnerable to small perturbations in the input graph, which makes them susceptible to input faults or adversarial attacks. This poses a significant problem for GCNs intended to be used in critical applications, which need to provide certifiably robust services even in the presence of adversarial perturbations. We propose an improved GCN robustness certification technique for node classification in the presence of node feature perturbations. We introduce a novel polyhedra-based abstract interpretation approach to tackle specific challenges of graph data and provide tight upper and lower bounds for the robustness of the GCN. Experiments show that our approach simultaneously improves the tightness of robustness bounds as well as the runtime performance of certification. Moreover, our method can be used during training to further improve the robustness of GCNs.","sentences":["Graph convolutional neural networks (GCNs) are powerful tools for learning graph-based knowledge representations from training data.","However, they are vulnerable to small perturbations in the input graph, which makes them susceptible to input faults or adversarial attacks.","This poses a significant problem for GCNs intended to be used in critical applications, which need to provide certifiably robust services even in the presence of adversarial perturbations.","We propose an improved GCN robustness certification technique for node classification in the presence of node feature perturbations.","We introduce a novel polyhedra-based abstract interpretation approach to tackle specific challenges of graph data and provide tight upper and lower bounds for the robustness of the GCN.","Experiments show that our approach simultaneously improves the tightness of robustness bounds as well as the runtime performance of certification.","Moreover, our method can be used during training to further improve the robustness of GCNs."],"url":"http://arxiv.org/abs/2405.08645v1","category":"cs.LG"}
{"created":"2024-05-14 14:18:25","title":"vMFER: Von Mises-Fisher Experience Resampling Based on Uncertainty of Gradient Directions for Policy Improvement","abstract":"Reinforcement Learning (RL) is a widely employed technique in decision-making problems, encompassing two fundamental operations -- policy evaluation and policy improvement. Enhancing learning efficiency remains a key challenge in RL, with many efforts focused on using ensemble critics to boost policy evaluation efficiency. However, when using multiple critics, the actor in the policy improvement process can obtain different gradients. Previous studies have combined these gradients without considering their disagreements. Therefore, optimizing the policy improvement process is crucial to enhance learning efficiency. This study focuses on investigating the impact of gradient disagreements caused by ensemble critics on policy improvement. We introduce the concept of uncertainty of gradient directions as a means to measure the disagreement among gradients utilized in the policy improvement process. Through measuring the disagreement among gradients, we find that transitions with lower uncertainty of gradient directions are more reliable in the policy improvement process. Building on this analysis, we propose a method called von Mises-Fisher Experience Resampling (vMFER), which optimizes the policy improvement process by resampling transitions and assigning higher confidence to transitions with lower uncertainty of gradient directions. Our experiments demonstrate that vMFER significantly outperforms the benchmark and is particularly well-suited for ensemble structures in RL.","sentences":["Reinforcement Learning (RL) is a widely employed technique in decision-making problems, encompassing two fundamental operations -- policy evaluation and policy improvement.","Enhancing learning efficiency remains a key challenge in RL, with many efforts focused on using ensemble critics to boost policy evaluation efficiency.","However, when using multiple critics, the actor in the policy improvement process can obtain different gradients.","Previous studies have combined these gradients without considering their disagreements.","Therefore, optimizing the policy improvement process is crucial to enhance learning efficiency.","This study focuses on investigating the impact of gradient disagreements caused by ensemble critics on policy improvement.","We introduce the concept of uncertainty of gradient directions as a means to measure the disagreement among gradients utilized in the policy improvement process.","Through measuring the disagreement among gradients, we find that transitions with lower uncertainty of gradient directions are more reliable in the policy improvement process.","Building on this analysis, we propose a method called von Mises-Fisher Experience Resampling (vMFER), which optimizes the policy improvement process by resampling transitions and assigning higher confidence to transitions with lower uncertainty of gradient directions.","Our experiments demonstrate that vMFER significantly outperforms the benchmark and is particularly well-suited for ensemble structures in RL."],"url":"http://arxiv.org/abs/2405.08638v1","category":"cs.LG"}
{"created":"2024-05-14 14:15:31","title":"Drift Detection: Introducing Gaussian Split Detector","abstract":"Recent research yielded a wide array of drift detectors. However, in order to achieve remarkable performance, the true class labels must be available during the drift detection phase. This paper targets at detecting drift when the ground truth is unknown during the detection phase. To that end, we introduce Gaussian Split Detector (GSD) a novel drift detector that works in batch mode. GSD is designed to work when the data follow a normal distribution and makes use of Gaussian mixture models to monitor changes in the decision boundary. The algorithm is designed to handle multi-dimension data streams and to work without the ground truth labels during the inference phase making it pertinent for real world use. In an extensive experimental study on real and synthetic datasets, we evaluate our detector against the state of the art. We show that our detector outperforms the state of the art in detecting real drift and in ignoring virtual drift which is key to avoid false alarms.","sentences":["Recent research yielded a wide array of drift detectors.","However, in order to achieve remarkable performance, the true class labels must be available during the drift detection phase.","This paper targets at detecting drift when the ground truth is unknown during the detection phase.","To that end, we introduce Gaussian Split Detector (GSD) a novel drift detector that works in batch mode.","GSD is designed to work when the data follow a normal distribution and makes use of Gaussian mixture models to monitor changes in the decision boundary.","The algorithm is designed to handle multi-dimension data streams and to work without the ground truth labels during the inference phase making it pertinent for real world use.","In an extensive experimental study on real and synthetic datasets, we evaluate our detector against the state of the art.","We show that our detector outperforms the state of the art in detecting real drift and in ignoring virtual drift which is key to avoid false alarms."],"url":"http://arxiv.org/abs/2405.08637v1","category":"cs.DC"}
{"created":"2024-05-14 13:59:10","title":"The one-dimensional Coulomb Hamiltonian: Properties of its Birman-Schwinger operator","abstract":"We study the Birman-Schwinger operator for a self-adjoint realisation of the one-dimensional Hamiltonian with the Coulomb potential. We study both the case in which this Hamiltonian is defined on the whole real line and when it is only defined on the positive semiaxis. In both cases, the Birman-Schwinger operator is Hilbert-Schmidt, even though it is not trace class. Then, we have considered some approximations to the Hamiltonian depending on a positive parameter, under given conditions, and proved the convergence of the Birman-Schwinger operators of these approximations to the original Hamiltonian as the parameter goes to zero. Further comments and results have been included.","sentences":["We study the Birman-Schwinger operator for a self-adjoint realisation of the one-dimensional Hamiltonian with the Coulomb potential.","We study both the case in which this Hamiltonian is defined on the whole real line and when it is only defined on the positive semiaxis.","In both cases, the Birman-Schwinger operator is Hilbert-Schmidt, even though it is not trace class.","Then, we have considered some approximations to the Hamiltonian depending on a positive parameter, under given conditions, and proved the convergence of the Birman-Schwinger operators of these approximations to the original Hamiltonian as the parameter goes to zero.","Further comments and results have been included."],"url":"http://arxiv.org/abs/2405.08618v1","category":"math-ph"}
{"created":"2024-05-14 13:58:53","title":"A methodology for comparing and benchmarking quantum devices","abstract":"Quantum Computing (QC) is undergoing a high rate of development, investment and research devoted to its improvement.However, there is little consensus in the industry and wider literature as to what improvement might consist of beyond ambiguous statements of \"more qubits\" and \"fewer errors\". Before one can decide how to improve something, it is first necessary to define the criteria for success: what are the metrics or statistics that are relevant to the problem? The lack of clarity surrounding this question has led to a rapidly developing capability with little consistency or standards present across the board. This paper lays out a framework by which any user, developer or researcher can define, articulate and justify the success criteria and associated benchmarks that have been used to solve their problem or make their claim.","sentences":["Quantum Computing (QC) is undergoing a high rate of development, investment and research devoted to its improvement.","However, there is little consensus in the industry and wider literature as to what improvement might consist of beyond ambiguous statements of \"more qubits\" and \"fewer errors\".","Before one can decide how to improve something, it is first necessary to define the criteria for success: what are the metrics or statistics that are relevant to the problem?","The lack of clarity surrounding this question has led to a rapidly developing capability with little consistency or standards present across the board.","This paper lays out a framework by which any user, developer or researcher can define, articulate and justify the success criteria and associated benchmarks that have been used to solve their problem or make their claim."],"url":"http://arxiv.org/abs/2405.08617v1","category":"quant-ph"}
{"created":"2024-05-14 13:36:50","title":"The Bell Based Super Coherent States. Uncertainty Relations, Golden Ratio and Fermion-Boson Entanglement","abstract":"The set of maximally fermion-boson entangled Bell super-coherent states is introduced. A superposition of these states with separable bosonic coherent states, represented by points on the super-Bloch sphere, we call the Bell based super-coherent states. Entanglement of bosonic and fermionic degrees of freedom in these states is studied by using displacement bosonic operator. It acts on the super-qubit reference state, representing superposition of the zero and the one super-number states, forming computational basis super-states. We show that the states are completely characterized by displaced Fock states, as a superposition with non-classical, the photon added coherent states, and the entanglement is independent of coherent state parameter $\\alpha$ and of the time evolution. In contrast to never orthogonal Glauber coherent states, our entangled super-coherent states can be orthogonal. The uncertainty relation in the states is monotonically growing function of the concurrence and for entangled states we get non-classical quadrature squeezing and representation of uncertainty by ratio of two Fibonacci numbers. The sequence of concurrences, and corresponding uncertainties $\\hbar F_n/F_{n+1}$, in the limit $n \\rightarrow \\infty $, convergent to the Golden ratio uncertainty $\\hbar/\\varphi$, where $\\varphi = \\frac{1 + \\sqrt{5}}{2}$ is found.","sentences":["The set of maximally fermion-boson entangled Bell super-coherent states is introduced.","A superposition of these states with separable bosonic coherent states, represented by points on the super-Bloch sphere, we call the Bell based super-coherent states.","Entanglement of bosonic and fermionic degrees of freedom in these states is studied by using displacement bosonic operator.","It acts on the super-qubit reference state, representing superposition of the zero and the one super-number states, forming computational basis super-states.","We show that the states are completely characterized by displaced Fock states, as a superposition with non-classical, the photon added coherent states, and the entanglement is independent of coherent state parameter $\\alpha$ and of the time evolution.","In contrast to never orthogonal Glauber coherent states, our entangled super-coherent states can be orthogonal.","The uncertainty relation in the states is monotonically growing function of the concurrence and for entangled states we get non-classical quadrature squeezing and representation of uncertainty by ratio of two Fibonacci numbers.","The sequence of concurrences, and corresponding uncertainties $\\hbar F_n/F_{n+1}$, in the limit $n \\rightarrow \\infty $, convergent to the Golden ratio uncertainty $\\hbar/\\varphi$, where $\\varphi = \\frac{1 + \\sqrt{5}}{2}$ is found."],"url":"http://arxiv.org/abs/2405.08594v1","category":"quant-ph"}
{"created":"2024-05-14 13:35:39","title":"Open-Vocabulary Object Detection via Neighboring Region Attention Alignment","abstract":"The nature of diversity in real-world environments necessitates neural network models to expand from closed category settings to accommodate novel emerging categories. In this paper, we study the open-vocabulary object detection (OVD), which facilitates the detection of novel object classes under the supervision of only base annotations and open-vocabulary knowledge. However, we find that the inadequacy of neighboring relationships between regions during the alignment process inevitably constrains the performance on recent distillation-based OVD strategies. To this end, we propose Neighboring Region Attention Alignment (NRAA), which performs alignment within the attention mechanism of a set of neighboring regions to boost the open-vocabulary inference. Specifically, for a given proposal region, we randomly explore the neighboring boxes and conduct our proposed neighboring region attention (NRA) mechanism to extract relationship information. Then, this interaction information is seamlessly provided into the distillation procedure to assist the alignment between the detector and the pre-trained vision-language models (VLMs). Extensive experiments validate that our proposed model exhibits superior performance on open-vocabulary benchmarks.","sentences":["The nature of diversity in real-world environments necessitates neural network models to expand from closed category settings to accommodate novel emerging categories.","In this paper, we study the open-vocabulary object detection (OVD), which facilitates the detection of novel object classes under the supervision of only base annotations and open-vocabulary knowledge.","However, we find that the inadequacy of neighboring relationships between regions during the alignment process inevitably constrains the performance on recent distillation-based OVD strategies.","To this end, we propose Neighboring Region Attention Alignment (NRAA), which performs alignment within the attention mechanism of a set of neighboring regions to boost the open-vocabulary inference.","Specifically, for a given proposal region, we randomly explore the neighboring boxes and conduct our proposed neighboring region attention (NRA) mechanism to extract relationship information.","Then, this interaction information is seamlessly provided into the distillation procedure to assist the alignment between the detector and the pre-trained vision-language models (VLMs).","Extensive experiments validate that our proposed model exhibits superior performance on open-vocabulary benchmarks."],"url":"http://arxiv.org/abs/2405.08593v1","category":"cs.CV"}
{"created":"2024-05-14 12:56:56","title":"Space-time stochastic Galerkin boundary elements for acoustic scattering problems","abstract":"Acoustic emission or scattering problems naturally involve uncertainties about the sound sources or boundary conditions. This article initiates the study of time domain boundary elements for such stochastic boundary problems for the acoustic wave equation. We present a space-time stochastic Galerkin boundary element method which is applied to sound-hard, sound-soft and absorbing scatterers. Uncertainties in both the sources and the boundary conditions are considered using a polynomial chaos expansion. The numerical experiments illustrate the performance and convergence of the proposed method in model problems and present an application to a problem from traffic noise.","sentences":["Acoustic emission or scattering problems naturally involve uncertainties about the sound sources or boundary conditions.","This article initiates the study of time domain boundary elements for such stochastic boundary problems for the acoustic wave equation.","We present a space-time stochastic Galerkin boundary element method which is applied to sound-hard, sound-soft and absorbing scatterers.","Uncertainties in both the sources and the boundary conditions are considered using a polynomial chaos expansion.","The numerical experiments illustrate the performance and convergence of the proposed method in model problems and present an application to a problem from traffic noise."],"url":"http://arxiv.org/abs/2405.08565v1","category":"math.NA"}
{"created":"2024-05-14 12:29:18","title":"A Determination of the Local Gravitational Acceleration for the Tsinghua Tabletop Kibble Balance","abstract":"The Kibble balance requires a measurement of the local gravitational acceleration, $g$, with a typical relative measurement uncertainty of $10^{-9}$. In this paper, the determination of $g$ for the Tsinghua tabletop Kibble balance is presented. A polynomial fitting method is proposed for blind transfers of the absolute gravitational acceleration using relative gravimeters, showing agreement with the value obtained by the tide correction within a few parts in $10^{9}$. Horizontal and vertical gravity gradients are extracted by mapping the gravity distribution at different heights. The self-attraction effect of major components in the experiment, as well as some time-varying systematic effects, are modeled. The final determination of the gravitational acceleration at the mass position, with a relative uncertainty of 5.4 $\\mu$Gal ($k=2$), is achieved for the Tsinghua tabletop Kibble balance experiment.","sentences":["The Kibble balance requires a measurement of the local gravitational acceleration, $g$, with a typical relative measurement uncertainty of $10^{-9}$. In this paper, the determination of $g$ for the Tsinghua tabletop Kibble balance is presented.","A polynomial fitting method is proposed for blind transfers of the absolute gravitational acceleration using relative gravimeters, showing agreement with the value obtained by the tide correction within a few parts in $10^{9}$. Horizontal and vertical gravity gradients are extracted by mapping the gravity distribution at different heights.","The self-attraction effect of major components in the experiment, as well as some time-varying systematic effects, are modeled.","The final determination of the gravitational acceleration at the mass position, with a relative uncertainty of 5.4 $\\mu$Gal ($k=2$), is achieved for the Tsinghua tabletop Kibble balance experiment."],"url":"http://arxiv.org/abs/2405.08541v1","category":"physics.ins-det"}
{"created":"2024-05-14 12:17:19","title":"Dynamic Feature Learning and Matching for Class-Incremental Learning","abstract":"Class-incremental learning (CIL) has emerged as a means to learn new classes incrementally without catastrophic forgetting of previous classes. Recently, CIL has undergone a paradigm shift towards dynamic architectures due to their superior performance. However, these models are still limited by the following aspects: (i) Data augmentation (DA), which are tightly coupled with CIL, remains under-explored in dynamic architecture scenarios. (ii) Feature representation. The discriminativeness of dynamic feature are sub-optimal and possess potential for refinement. (iii) Classifier. The misalignment between dynamic feature and classifier constrains the capabilities of the model. To tackle the aforementioned drawbacks, we propose the Dynamic Feature Learning and Matching (DFLM) model in this paper from above three perspectives. Specifically, we firstly introduce class weight information and non-stationary functions to extend the mix DA method for dynamically adjusting the focus on memory during training. Then, von Mises-Fisher (vMF) classifier is employed to effectively model the dynamic feature distribution and implicitly learn their discriminative properties. Finally, the matching loss is proposed to facilitate the alignment between the learned dynamic features and the classifier by minimizing the distribution distance. Extensive experiments on CIL benchmarks validate that our proposed model achieves significant performance improvements over existing methods.","sentences":["Class-incremental learning (CIL) has emerged as a means to learn new classes incrementally without catastrophic forgetting of previous classes.","Recently, CIL has undergone a paradigm shift towards dynamic architectures due to their superior performance.","However, these models are still limited by the following aspects: (i) Data augmentation (DA), which are tightly coupled with CIL, remains under-explored in dynamic architecture scenarios.","(ii) Feature representation.","The discriminativeness of dynamic feature are sub-optimal and possess potential for refinement.","(iii) Classifier.","The misalignment between dynamic feature and classifier constrains the capabilities of the model.","To tackle the aforementioned drawbacks, we propose the Dynamic Feature Learning and Matching (DFLM) model in this paper from above three perspectives.","Specifically, we firstly introduce class weight information and non-stationary functions to extend the mix DA method for dynamically adjusting the focus on memory during training.","Then, von Mises-Fisher (vMF) classifier is employed to effectively model the dynamic feature distribution and implicitly learn their discriminative properties.","Finally, the matching loss is proposed to facilitate the alignment between the learned dynamic features and the classifier by minimizing the distribution distance.","Extensive experiments on CIL benchmarks validate that our proposed model achieves significant performance improvements over existing methods."],"url":"http://arxiv.org/abs/2405.08533v1","category":"cs.CV"}
{"created":"2024-05-14 11:48:59","title":"Cryptography-Based Privacy-Preserving Method for Distributed Optimization over Time-Varying Directed Graphs with Enhanced Efficiency","abstract":"In this paper, we study the privacy-preserving distributed optimization problem, aiming to prevent attackers from stealing the private information of agents. For this purpose, we propose a novel privacy-preserving algorithm based on the Advanced Encryption Standard (AES), which is both secure and computationally efficient. By appropriately constructing the underlying weight matrices, our algorithm can be applied to time-varying directed networks. We show that the proposed algorithm can protect an agent's privacy if the agent has at least one legitimate neighbor at the initial iteration. Under the assumption that the objective function is strongly convex and Lipschitz smooth, we rigorously prove that the proposed algorithm has a linear convergence rate. Finally, the effectiveness of the proposed algorithm is demonstrated by numerical simulations of the canonical sensor fusion problem.","sentences":["In this paper, we study the privacy-preserving distributed optimization problem, aiming to prevent attackers from stealing the private information of agents.","For this purpose, we propose a novel privacy-preserving algorithm based on the Advanced Encryption Standard (AES), which is both secure and computationally efficient.","By appropriately constructing the underlying weight matrices, our algorithm can be applied to time-varying directed networks.","We show that the proposed algorithm can protect an agent's privacy if the agent has at least one legitimate neighbor at the initial iteration.","Under the assumption that the objective function is strongly convex and Lipschitz smooth, we rigorously prove that the proposed algorithm has a linear convergence rate.","Finally, the effectiveness of the proposed algorithm is demonstrated by numerical simulations of the canonical sensor fusion problem."],"url":"http://arxiv.org/abs/2405.08518v1","category":"math.OC"}
{"created":"2024-05-14 10:55:04","title":"Learning Decision Policies with Instrumental Variables through Double Machine Learning","abstract":"A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders. Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables. Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect. Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator. We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies. We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework. The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded. DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments.","sentences":["A common issue in learning decision-making policies in data-rich settings is spurious correlations in the offline dataset, which can be caused by hidden confounders.","Instrumental variable (IV) regression, which utilises a key unconfounded variable known as the instrument, is a standard technique for learning causal relationships between confounded action, outcome, and context variables.","Most recent IV regression algorithms use a two-stage approach, where a deep neural network (DNN) estimator learnt in the first stage is directly plugged into the second stage, in which another DNN is used to estimate the causal effect.","Naively plugging the estimator can cause heavy bias in the second stage, especially when regularisation bias is present in the first stage estimator.","We propose DML-IV, a non-linear IV regression method that reduces the bias in two-stage IV regressions and effectively learns high-performing policies.","We derive a novel learning objective to reduce bias and design the DML-IV algorithm following the double/debiased machine learning (DML) framework.","The learnt DML-IV estimator has strong convergence rate and $O(N^{-1/2})$ suboptimality guarantees that match those when the dataset is unconfounded.","DML-IV outperforms state-of-the-art IV regression methods on IV regression benchmarks and learns high-performing policies in the presence of instruments."],"url":"http://arxiv.org/abs/2405.08498v1","category":"cs.LG"}
{"created":"2024-05-14 10:23:57","title":"Gradient Boosting Mapping for Dimensionality Reduction and Feature Extraction","abstract":"A fundamental problem in supervised learning is to find a good set of features or distance measures. If the new set of features is of lower dimensionality and can be obtained by a simple transformation of the original data, they can make the model understandable, reduce overfitting, and even help to detect distribution drift. We propose a supervised dimensionality reduction method Gradient Boosting Mapping (GBMAP), where the outputs of weak learners -- defined as one-layer perceptrons -- define the embedding. We show that the embedding coordinates provide better features for the supervised learning task, making simple linear models competitive with the state-of-the-art regressors and classifiers. We also use the embedding to find a principled distance measure between points. The features and distance measures automatically ignore directions irrelevant to the supervised learning task. We also show that we can reliably detect out-of-distribution data points with potentially large regression or classification errors. GBMAP is fast and works in seconds for dataset of million data points or hundreds of features. As a bonus, GBMAP provides a regression and classification performance comparable to the state-of-the-art supervised learning methods.","sentences":["A fundamental problem in supervised learning is to find a good set of features or distance measures.","If the new set of features is of lower dimensionality and can be obtained by a simple transformation of the original data, they can make the model understandable, reduce overfitting, and even help to detect distribution drift.","We propose a supervised dimensionality reduction method Gradient Boosting Mapping (GBMAP), where the outputs of weak learners -- defined as one-layer perceptrons -- define the embedding.","We show that the embedding coordinates provide better features for the supervised learning task, making simple linear models competitive with the state-of-the-art regressors and classifiers.","We also use the embedding to find a principled distance measure between points.","The features and distance measures automatically ignore directions irrelevant to the supervised learning task.","We also show that we can reliably detect out-of-distribution data points with potentially large regression or classification errors.","GBMAP is fast and works in seconds for dataset of million data points or hundreds of features.","As a bonus, GBMAP provides a regression and classification performance comparable to the state-of-the-art supervised learning methods."],"url":"http://arxiv.org/abs/2405.08486v1","category":"cs.LG"}
{"created":"2024-05-14 10:02:50","title":"Enhancing Gender-Inclusive Machine Translation with Neomorphemes and Large Language Models","abstract":"Machine translation (MT) models are known to suffer from gender bias, especially when translating into languages with extensive gendered morphology. Accordingly, they still fall short in using gender-inclusive language, also representative of non-binary identities. In this paper, we look at gender-inclusive neomorphemes, neologistic elements that avoid binary gender markings as an approach towards fairer MT. In this direction, we explore prompting techniques with large language models (LLMs) to translate from English into Italian using neomorphemes. So far, this area has been under-explored due to its novelty and the lack of publicly available evaluation resources. We fill this gap by releasing Neo-GATE, a resource designed to evaluate gender-inclusive en-it translation with neomorphemes. With Neo-GATE, we assess four LLMs of different families and sizes and different prompt formats, identifying strengths and weaknesses of each on this novel task for MT.","sentences":["Machine translation (MT) models are known to suffer from gender bias, especially when translating into languages with extensive gendered morphology.","Accordingly, they still fall short in using gender-inclusive language, also representative of non-binary identities.","In this paper, we look at gender-inclusive neomorphemes, neologistic elements that avoid binary gender markings as an approach towards fairer MT.","In this direction, we explore prompting techniques with large language models (LLMs) to translate from English into Italian using neomorphemes.","So far, this area has been under-explored due to its novelty and the lack of publicly available evaluation resources.","We fill this gap by releasing Neo-GATE, a resource designed to evaluate gender-inclusive en-it translation with neomorphemes.","With Neo-GATE, we assess four LLMs of different families and sizes and different prompt formats, identifying strengths and weaknesses of each on this novel task for MT."],"url":"http://arxiv.org/abs/2405.08477v1","category":"cs.CL"}
{"created":"2024-05-14 09:20:59","title":"How Alignment Helps Make the Most of Multimodal Data","abstract":"When studying political communication, combining the information from text, audio, and video signals promises to reflect the richness of human communication more comprehensively than confining it to individual modalities alone. However, when modeling such multimodal data, its heterogeneity, connectedness, and interaction are challenging to address. We argue that aligning the respective modalities can be an essential step in entirely using the potential of multimodal data because it informs the model with human understanding. Exploring aligned modalities unlocks promising analytical leverage. First, it allows us to make the most of information in the data, which inter alia opens the door to better quality predictions. Second, it is possible to answer research questions that span multiple modalities with cross-modal queries. Finally, alignment addresses concerns about model interpretability. We illustrate the utility of this approach by analyzing how German MPs address members of the far-right AfD in their speeches, and predicting the tone of video advertising in the context of the 2020 US presidential race. Our paper offers important insights to all keen to analyze multimodal data effectively.","sentences":["When studying political communication, combining the information from text, audio, and video signals promises to reflect the richness of human communication more comprehensively than confining it to individual modalities alone.","However, when modeling such multimodal data, its heterogeneity, connectedness, and interaction are challenging to address.","We argue that aligning the respective modalities can be an essential step in entirely using the potential of multimodal data because it informs the model with human understanding.","Exploring aligned modalities unlocks promising analytical leverage.","First, it allows us to make the most of information in the data, which inter alia opens the door to better quality predictions.","Second, it is possible to answer research questions that span multiple modalities with cross-modal queries.","Finally, alignment addresses concerns about model interpretability.","We illustrate the utility of this approach by analyzing how German MPs address members of the far-right AfD in their speeches, and predicting the tone of video advertising in the context of the 2020 US presidential race.","Our paper offers important insights to all keen to analyze multimodal data effectively."],"url":"http://arxiv.org/abs/2405.08454v1","category":"cs.CL"}
{"created":"2024-05-14 09:11:17","title":"Spectral characterisation of the extinction properties of NGC 3603 using JWST NIRSpec","abstract":"Context. A necessary ingredient in understanding the star formation history of a young cluster is knowledge of the extinction towards the region. This has typically been done by making use of the colour-difference method with photometry, or similar methods utilising the colour-colour diagram. Aims. The colour-excess can be independently determined by studying the decrements of the recombination lines produced by the nebular gas. Having access to many recombination lines from the same spectral series removes the need of adopting an extinction curve. Methods. Using the Micro-Shutter Assembly (MSA) on board the Near InfraRed Spectrograph (NIRSpec), multi-object spectroscopy was performed, yielding 600 nebular spectra from the Galactic massive star formation region NGC 3603. The recombination line intensity ratios were used to determine independent values of colour-excess. Results. The extinction characteristics of NGC 3603 are similar to other Galactic HII regions like Orion, as well as starburst regions such 30 Doradus in the Large Magellanic Cloud, in that we find a relatively large value of total-to-selective extinction of 4.8 $\\pm$ 1.06, larger than the Galactic average of 3.1. We find a typical value for the colour-excess of 0.64 $\\pm$ 0.27, significantly lower than values determined in previous studies. We also present a stacked nebular spectrum with a typical continuum S/N = 70. This spectrum highlights the recombination lines of the HII region, several s-process elements such as Kr III and Se IV, and molecular H2 emission lines. Conclusions. Using ratios of hydrogen recombination lines, we calculated the total-to-selective extinction, colour-excess and visual extinction for > 200 lines of sight across NGC 3603. An extinction curve with a total-to-selective extinction of 4.8 $\\pm$ 1.06 was found, corresponding to a colour-excess of 0.64 $\\pm$ 0.27.","sentences":["Context.","A necessary ingredient in understanding the star formation history of a young cluster is knowledge of the extinction towards the region.","This has typically been done by making use of the colour-difference method with photometry, or similar methods utilising the colour-colour diagram.","Aims.","The colour-excess can be independently determined by studying the decrements of the recombination lines produced by the nebular gas.","Having access to many recombination lines from the same spectral series removes the need of adopting an extinction curve.","Methods.","Using the Micro-Shutter Assembly (MSA) on board the Near InfraRed Spectrograph (NIRSpec), multi-object spectroscopy was performed, yielding 600 nebular spectra from the Galactic massive star formation region NGC 3603.","The recombination line intensity ratios were used to determine independent values of colour-excess.","Results.","The extinction characteristics of NGC 3603 are similar to other Galactic HII regions like Orion, as well as starburst regions such 30 Doradus in the Large Magellanic Cloud, in that we find a relatively large value of total-to-selective extinction of 4.8 $\\pm$ 1.06, larger than the Galactic average of 3.1.","We find a typical value for the colour-excess of 0.64 $\\pm$ 0.27, significantly lower than values determined in previous studies.","We also present a stacked nebular spectrum with a typical continuum S/N = 70.","This spectrum highlights the recombination lines of the HII region, several s-process elements such as Kr III and Se IV, and molecular H2 emission lines.","Conclusions.","Using ratios of hydrogen recombination lines, we calculated the total-to-selective extinction, colour-excess and visual extinction for > 200 lines of sight across NGC 3603.","An extinction curve with a total-to-selective extinction of 4.8 $\\pm$ 1.06 was found, corresponding to a colour-excess of 0.64 $\\pm$ 0.27."],"url":"http://arxiv.org/abs/2405.08445v1","category":"astro-ph.GA"}
{"created":"2024-05-14 08:58:54","title":"Stabilization and dynamics of magnetic antivortices in a nanodisk with anisotropic Dzyaloshinskii-Moriya interaction","abstract":"We theoretically investigate the antivortex stabilized by anisotropic Dzyaloshinskii-Moriya interaction (DMI) in nanodisks. It is remarkably found that the antivortex remains stable even when the nanodisk radius is reduced to 15 nm, owing to the short-range nature of the DMI. We also investigate the antivortex dynamics under a static in-plane magnetic field, which shows that the displacement of the antivortex core depends on its vorticity and helicity, providing a fundamental basic for distinguishing different vortex types. Additionally, spin-polarized currents can trigger a self-sustained gyration of the antivortex at low current densities, while inducing polarity switching at high current densities. Our findings offer valuable insights into the DMI role in stabilizing topological solitons and their potential applications in spin-torque nano-oscillators and magnetic memories.","sentences":["We theoretically investigate the antivortex stabilized by anisotropic Dzyaloshinskii-Moriya interaction (DMI) in nanodisks.","It is remarkably found that the antivortex remains stable even when the nanodisk radius is reduced to 15 nm, owing to the short-range nature of the DMI.","We also investigate the antivortex dynamics under a static in-plane magnetic field, which shows that the displacement of the antivortex core depends on its vorticity and helicity, providing a fundamental basic for distinguishing different vortex types.","Additionally, spin-polarized currents can trigger a self-sustained gyration of the antivortex at low current densities, while inducing polarity switching at high current densities.","Our findings offer valuable insights into the DMI role in stabilizing topological solitons and their potential applications in spin-torque nano-oscillators and magnetic memories."],"url":"http://arxiv.org/abs/2405.08437v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-14 08:35:39","title":"Tackling Prevalent Conditions in Unsupervised Combinatorial Optimization: Cardinality, Minimum, Covering, and More","abstract":"Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable. Karalias & Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization. Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization. However, each component confronts unique challenges. First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial. Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding. In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO. First, we concretize the targets for objective construction and derandomization with theoretical justification. Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets. Finally, we apply the derivations to various CO problems. Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t. both optimization quality and speed.","sentences":["Combinatorial optimization (CO) is naturally discrete, making machine learning based on differentiable optimization inapplicable.","Karalias & Loukas (2020) adapted the probabilistic method to incorporate CO into differentiable optimization.","Their work ignited the research on unsupervised learning for CO, composed of two main components: probabilistic objectives and derandomization.","However, each component confronts unique challenges.","First, deriving objectives under various conditions (e.g., cardinality constraints and minimum) is nontrivial.","Second, the derandomization process is underexplored, and the existing derandomization methods are either random sampling or naive rounding.","In this work, we aim to tackle prevalent (i.e., commonly involved) conditions in unsupervised CO.","First, we concretize the targets for objective construction and derandomization with theoretical justification.","Then, for various conditions commonly involved in different CO problems, we derive nontrivial objectives and derandomization to meet the targets.","Finally, we apply the derivations to various CO problems.","Via extensive experiments on synthetic and real-world graphs, we validate the correctness of our derivations and show our empirical superiority w.r.t.","both optimization quality and speed."],"url":"http://arxiv.org/abs/2405.08424v1","category":"cs.LG"}
{"created":"2024-05-14 08:31:29","title":"Hereditary undecidability of fragments of some elementary theories","abstract":"It is well known that whenever a class of structures $\\mathcal{K}_1$ is interpretable in a class of structures $\\mathcal{K}_2$, then the hereditary undecidability of (a fragment of) the theory of $\\mathcal{K}_1$ implies the hereditary undecidability of (a suitable fragment of) the theory of $\\mathcal{K}_2$. In the present paper, we construct a $\\Sigma_1$-interpretation of the class of all finite bipartite graphs in the class of all pairs of equivalence relations on the same finite domain; from this we obtain the hereditary undecidability of the $\\Sigma_2$-theory of the second class. Next, we construct a $\\Sigma_1$-interpretation of the class of all pairs of equivalence relations on the same finite domain in the class of all pairs consisting of a linear ordering and an equivalence relation on the same finite domain; this gives us the hereditary undecidability of the $\\Sigma_2$-theory of the second class. The corresponding results are, in a sense, optimal, since the $\\Pi_2$-theories of the classes under consideration are decidable.   Keywords: undecidability, elementary theories, prefix fragments","sentences":["It is well known that whenever a class of structures $\\mathcal{K}_1$ is interpretable in a class of structures $\\mathcal{K}_2$, then the hereditary undecidability of (a fragment of) the theory of $\\mathcal{K}_1$ implies the hereditary undecidability of (a suitable fragment of) the theory of $\\mathcal{K}_2$. In the present paper, we construct a $\\Sigma_1$-interpretation of the class of all finite bipartite graphs in the class of all pairs of equivalence relations on the same finite domain; from this we obtain the hereditary undecidability of the $\\Sigma_2$-theory of the second class.","Next, we construct a $\\Sigma_1$-interpretation of the class of all pairs of equivalence relations on the same finite domain in the class of all pairs consisting of a linear ordering and an equivalence relation on the same finite domain; this gives us the hereditary undecidability of the $\\Sigma_2$-theory of the second class.","The corresponding results are, in a sense, optimal, since the $\\Pi_2$-theories of the classes under consideration are decidable.   ","Keywords: undecidability, elementary theories, prefix fragments"],"url":"http://arxiv.org/abs/2405.08422v1","category":"math.LO"}
{"created":"2024-05-14 07:56:55","title":"Genetic contribution of an advantaged mutant in the biparental Moran model -- finite selection","abstract":"We consider a population of N individuals, whose dynamics through time is represented by a biparental Moran model with two types: an advantaged type and a disadvantaged type. The advantage is due to a mutation, transmitted in a Mendelian way from parent to child that reduces the death probability of individuals carrying it. We assume that initially this mutation is carried by a proportion a of individuals in the population. Once the mutation is fixed, a gene is sampled uniformly in the population, at a locus independent of the locus under selection. We then give the probability that this gene initially comes from an advantaged individual, i.e. the genetic contribution of these individuals, as a function of a and when the population size is large.","sentences":["We consider a population of N individuals, whose dynamics through time is represented by a biparental Moran model with two types: an advantaged type and a disadvantaged type.","The advantage is due to a mutation, transmitted in a Mendelian way from parent to child that reduces the death probability of individuals carrying it.","We assume that initially this mutation is carried by a proportion a of individuals in the population.","Once the mutation is fixed, a gene is sampled uniformly in the population, at a locus independent of the locus under selection.","We then give the probability that this gene initially comes from an advantaged individual, i.e. the genetic contribution of these individuals, as a function of a and when the population size is large."],"url":"http://arxiv.org/abs/2405.08404v1","category":"math.PR"}
{"created":"2024-05-14 07:46:58","title":"Gaussian measure on the dual of $\\mathrm{U}(N)$, random partitions, and topological expansion of the partition function","abstract":"We study a Gaussian measure with parameter $q\\in(0,1)$ on the dual of the unitary group of size $N$: we prove that a random highest weight under this measure is the coupling of two independent $q$-uniform random partitions $\\alpha,\\beta$ and a random highest weight of $\\mathrm{U}(1)$. We prove deviation inequalities for the $q$-uniform measure, and use them to show that the coupling of random partitions under the Gaussian measure vanishes in the limit $N\\to\\infty$. We also prove that the partition function of this measure admits an asymptotic expansion in powers of $1/N$, and that this expansion is topological, in the sense that its coefficients are related to the enumeration of ramified coverings of elliptic curves. It provides a rigorous proof of the gauge/string duality for the Yang-Mills theory on a 2D torus with gauge group $\\mathrm{U}(N),$ advocated by Gross and Taylor \\cite{GT,GT2}.","sentences":["We study a Gaussian measure with parameter $q\\in(0,1)$ on the dual of the unitary group of size $N$: we prove that a random highest weight under this measure is the coupling of two independent $q$-uniform random partitions $\\alpha,\\beta$ and a random highest weight of $\\mathrm{U}(1)$. We prove deviation inequalities for the $q$-uniform measure, and use them to show that the coupling of random partitions under the Gaussian measure vanishes in the limit $N\\to\\infty$. We also prove that the partition function of this measure admits an asymptotic expansion in powers of $1/N$, and that this expansion is topological, in the sense that its coefficients are related to the enumeration of ramified coverings of elliptic curves.","It provides a rigorous proof of the gauge/string duality for the Yang-Mills theory on a 2D torus with gauge group $\\mathrm{U}(N),$ advocated by Gross and Taylor \\cite{GT,GT2}."],"url":"http://arxiv.org/abs/2405.08393v1","category":"math-ph"}
{"created":"2024-05-14 07:20:22","title":"Weak well-posedness and weak discretization error for stable-driven SDEs with Lebesgue drift","abstract":"We are interested in the discretization of stable driven SDEs with additive noise for $\\alpha$ $\\in$ (1, 2) and Lq -- Lp drift under the Serrin type condition $\\alpha$/q + d/p < $\\alpha$ -- 1. We show weak existence and uniqueness as well as heat kernel estimates for the SDE and obtain a convergence rate of order (1/$\\alpha$)*($\\alpha$ -- 1 -- $\\alpha$/q - d/p) for the difference of the densities for the Euler scheme approximation involving suitably cutoffed and time randomized drifts.","sentences":["We are interested in the discretization of stable driven SDEs with additive noise for $\\alpha$ $\\in$ (1, 2) and Lq -- Lp drift under the Serrin type condition $\\alpha$/q + d/p < $\\alpha$ -- 1.","We show weak existence and uniqueness as well as heat kernel estimates for the SDE and obtain a convergence rate of order (1/$\\alpha$)*($\\alpha$ -- 1 -- $\\alpha$/q - d/p) for the difference of the densities for the Euler scheme approximation involving suitably cutoffed and time randomized drifts."],"url":"http://arxiv.org/abs/2405.08378v1","category":"math.PR"}
{"created":"2024-05-14 07:07:13","title":"Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control","abstract":"Disentangling model activations into meaningful features is a central problem in interpretability. However, the lack of ground-truth for these features in realistic scenarios makes the validation of recent approaches, such as sparse dictionary learning, elusive. To overcome this, we propose a framework to evaluate feature dictionaries in the context of specific tasks, by comparing them against \\emph{supervised} feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes.   We apply this framework to the indirect object identification task (IOI) using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets. We find that these SAEs capture interpretable features for the IOI task, but they are not as successful as supervised features in controlling the model. Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller features without clear interpretation). We hope that our framework will be a useful step towards more objective and grounded evaluations of sparse dictionary learning methods.","sentences":["Disentangling model activations into meaningful features is a central problem in interpretability.","However, the lack of ground-truth for these features in realistic scenarios makes the validation of recent approaches, such as sparse dictionary learning, elusive.","To overcome this, we propose a framework to evaluate feature dictionaries in the context of specific tasks, by comparing them against \\emph{supervised} feature dictionaries.","First, we demonstrate that supervised dictionaries achieve excellent approximation, control and interpretability of model computations on the task.","Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes.   ","We apply this framework to the indirect object identification task (IOI) using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets.","We find that these SAEs capture interpretable features for the IOI task, but they are not as successful as supervised features in controlling the model.","Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller features without clear interpretation).","We hope that our framework will be a useful step towards more objective and grounded evaluations of sparse dictionary learning methods."],"url":"http://arxiv.org/abs/2405.08366v1","category":"cs.LG"}
{"created":"2024-05-14 06:42:47","title":"KG-EmpiRE: A Community-Maintainable Knowledge Graph for a Sustainable Literature Review on the State and Evolution of Empirical Research in Requirements Engineering","abstract":"In the last two decades, several researchers provided snapshots of the \"current\" state and evolution of empirical research in requirements engineering (RE) through literature reviews. However, these literature reviews were not sustainable, as none built on or updated previous works due to the unavailability of the extracted and analyzed data. KG-EmpiRE is a Knowledge Graph (KG) of empirical research in RE based on scientific data extracted from currently 680 papers published in the IEEE International Requirements Engineering Conference (1994-2022). KG-EmpiRE is maintained in the Open Research Knowledge Graph (ORKG), making all data openly and long-term available according to the FAIR data principles. Our long-term goal is to constantly maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. Besides KG-EmpiRE, we provide its analysis with all supplementary materials in a repository. This repository contains all files with instructions for replicating and (re-)using the analysis locally or via executable environments and for repeating the research approach. Since its first release based on 199 papers (2014-2022), KG-EmpiRE and its analysis have been updated twice, currently covering over 650 papers. KG-EmpiRE and its analysis demonstrate how innovative infrastructures, such as the ORKG, can be leveraged to make data from literature reviews FAIR, openly available, and maintainable for the research community in the long term. In this way, we can enable replicable, (re-)usable, and thus sustainable literature reviews to ensure the quality, reliability, and timeliness of their research results.","sentences":["In the last two decades, several researchers provided snapshots of the \"current\" state and evolution of empirical research in requirements engineering (RE) through literature reviews.","However, these literature reviews were not sustainable, as none built on or updated previous works due to the unavailability of the extracted and analyzed data.","KG-EmpiRE is a Knowledge Graph (KG) of empirical research in RE based on scientific data extracted from currently 680 papers published in the IEEE International Requirements Engineering Conference (1994-2022).","KG-EmpiRE is maintained in the Open Research Knowledge Graph (ORKG), making all data openly and long-term available according to the FAIR data principles.","Our long-term goal is to constantly maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE.","Besides KG-EmpiRE, we provide its analysis with all supplementary materials in a repository.","This repository contains all files with instructions for replicating and (re-)using the analysis locally or via executable environments and for repeating the research approach.","Since its first release based on 199 papers (2014-2022), KG-EmpiRE and its analysis have been updated twice, currently covering over 650 papers.","KG-EmpiRE and its analysis demonstrate how innovative infrastructures, such as the ORKG, can be leveraged to make data from literature reviews FAIR, openly available, and maintainable for the research community in the long term.","In this way, we can enable replicable, (re-)usable, and thus sustainable literature reviews to ensure the quality, reliability, and timeliness of their research results."],"url":"http://arxiv.org/abs/2405.08351v1","category":"cs.SE"}
{"created":"2024-05-14 05:44:07","title":"Sufficient conditions, lower bounds and trade-off relations for quantumness in Kirkwood-Dirac quasiprobability","abstract":"Kirkwood-Dirac (KD) quasiprobability is a quantum analog of classical phase space probability. It offers an informationally complete representation of quantum state wherein the quantumness associated with quantum noncommutativity manifests in its nonclassical values, i.e., the nonreal and/or negative values of the real part. This naturally raises a question: how does such form of quantumness comply with the uncertainty principle which also arise from quantum noncommutativity? Here, first, we obtain sufficient conditions for the KD quasiprobability defined relative to a pair of PVM (projection-valued measure) bases to have nonclassical values. Using these nonclassical values, we then introduce two quantities which capture the amount of KD quantumness in a quantum state relative to a single PVM basis. They are defined respectively as the nonreality, and the classicality which captures both the nonreality and negativity, of the associated KD quasiprobability over the PVM basis of interest, and another PVM basis, and maximized over all possible choices of the latter. We obtain their lower bounds, and derive trade-off relations respectively reminiscent of the Robertson and Robertson-Schr\\\"odinger uncertainty relations but with lower bounds maximized over the convex sets of Hermitian operators whose complete sets of eigenprojectors are given by the PVM bases. We discuss their measurement using weak value measurement and classical optimization, and suggest information theoretical and operational interpretations in terms of optimal estimation of the PVM basis and state disturbance.","sentences":["Kirkwood-Dirac (KD) quasiprobability is a quantum analog of classical phase space probability.","It offers an informationally complete representation of quantum state wherein the quantumness associated with quantum noncommutativity manifests in its nonclassical values, i.e., the nonreal and/or negative values of the real part.","This naturally raises a question: how does such form of quantumness comply with the uncertainty principle which also arise from quantum noncommutativity?","Here, first, we obtain sufficient conditions for the KD quasiprobability defined relative to a pair of PVM (projection-valued measure) bases to have nonclassical values.","Using these nonclassical values, we then introduce two quantities which capture the amount of KD quantumness in a quantum state relative to a single PVM basis.","They are defined respectively as the nonreality, and the classicality which captures both the nonreality and negativity, of the associated KD quasiprobability over the PVM basis of interest, and another PVM basis, and maximized over all possible choices of the latter.","We obtain their lower bounds, and derive trade-off relations respectively reminiscent of the Robertson and Robertson-Schr\\\"odinger uncertainty relations but with lower bounds maximized over the convex sets of Hermitian operators whose complete sets of eigenprojectors are given by the PVM bases.","We discuss their measurement using weak value measurement and classical optimization, and suggest information theoretical and operational interpretations in terms of optimal estimation of the PVM basis and state disturbance."],"url":"http://arxiv.org/abs/2405.08324v1","category":"quant-ph"}
{"created":"2024-05-14 05:34:54","title":"Strain-induced long-range charge-density wave order in the optimally doped Bi$_2$Sr$_{2-x}$La$_x$CuO$_{6}$ superconductor","abstract":"The mechanism of high-temperature superconductivity in copper oxides (cuprate) remains elusive, with the pseudogap phase considered a potential factor. Recent attention has focused on a long-range symmetry-broken charge-density wave (CDW) order in the underdoped regime, induced by strong magnetic fields. Here by $^{63,65}$Cu-nuclear magnetic resonance, we report the discovery of a long-range CDW order in the optimally doped Bi$_2$Sr$_{2-x}$La$_x$CuO$_6$ superconductor, induced by in-plane strain exceeding $|$$\\varepsilon$$|$ = 0.15 %, which deliberately breaks the crystal symmetry of the CuO$_2$ plane. We find that compressive/tensile strains reduce superconductivity but enhance CDW, leaving superconductivity to coexist with CDW. The findings show that a long-range CDW order is an underlying hidden order in the pseudogap state, not limited to the underdoped regime, becoming apparent under strain. Our result sheds light on the intertwining of various orders in the cuprates.","sentences":["The mechanism of high-temperature superconductivity in copper oxides (cuprate) remains elusive, with the pseudogap phase considered a potential factor.","Recent attention has focused on a long-range symmetry-broken charge-density wave (CDW) order in the underdoped regime, induced by strong magnetic fields.","Here by $^{63,65}$Cu-nuclear magnetic resonance, we report the discovery of a long-range CDW order in the optimally doped Bi$_2$Sr$_{2-x}$La$_x$CuO$_6$ superconductor, induced by in-plane strain exceeding $|$$\\varepsilon$$|$ = 0.15 %, which deliberately breaks the crystal symmetry of the CuO$_2$ plane.","We find that compressive/tensile strains reduce superconductivity but enhance CDW, leaving superconductivity to coexist with CDW.","The findings show that a long-range CDW order is an underlying hidden order in the pseudogap state, not limited to the underdoped regime, becoming apparent under strain.","Our result sheds light on the intertwining of various orders in the cuprates."],"url":"http://arxiv.org/abs/2405.08320v1","category":"cond-mat.supr-con"}
{"created":"2024-05-14 05:17:01","title":"Measurement-based quantum machine learning","abstract":"A quantum neural network (QNN) is an object that extends the notion of a classical neural network to quantum models for quantum data. We can create a QNN by parametrizing a quantum process and then using it to model unknown relations between quantum states. In this paper, we explore how to use measurement-based quantum computation for quantum machine learning problems and propose a universal QNN in this framework which we call the multiple-triangle ansatz (MuTA). Using the proposed QNN, we solve several tasks, including learning a universal set of gates, optimizing measurement with post-processing, learning a quantum instrument, and the classification of classical data. Finally, we discuss how to train an ansatz under the hardware constraints imposed by photonic Gottesman-Kitaev-Preskill qubits. Our work demonstrates the feasibility of using measurement-based quantum computation as a framework for quantum machine learning algorithms.","sentences":["A quantum neural network (QNN) is an object that extends the notion of a classical neural network to quantum models for quantum data.","We can create a QNN by parametrizing a quantum process and then using it to model unknown relations between quantum states.","In this paper, we explore how to use measurement-based quantum computation for quantum machine learning problems and propose a universal QNN in this framework which we call the multiple-triangle ansatz (MuTA).","Using the proposed QNN, we solve several tasks, including learning a universal set of gates, optimizing measurement with post-processing, learning a quantum instrument, and the classification of classical data.","Finally, we discuss how to train an ansatz under the hardware constraints imposed by photonic Gottesman-Kitaev-Preskill qubits.","Our work demonstrates the feasibility of using measurement-based quantum computation as a framework for quantum machine learning algorithms."],"url":"http://arxiv.org/abs/2405.08319v1","category":"quant-ph"}
{"created":"2024-05-14 04:18:50","title":"Neutral test particle dynamics around the Bardeen-AdS black hole surrounded by quintessence dark energy","abstract":"Dynamics of neutral test particles in the spacetime of a Bardeen AdS black hole surrounded by quintessence dark energy is studied. First, we analyze the properties of the black hole and possible values of the monopole charge and quintessential parameters that allows the existence of the event horizon. The effects of the parameters on the effective potential and the innermost stable circular orbit radius are also studied. For the neutral test particles motion, it is shown that as the quintessential parameters increase, the radius of ISCO be increased. We have analyzed the dynamical behaviors of the neutral test particles by applying techniques including Poincar'e sections, power density and bifurcation diagram. It is shown that the presence of a quintessence parameter creates the chaotic phenomenon for the motion of neutral particle in a Bardeen AdS black hole spacetime. The amplification of chaos typically occurs as the energy increases under appropriate circumstances.","sentences":["Dynamics of neutral test particles in the spacetime of a Bardeen AdS black hole surrounded by quintessence dark energy is studied.","First, we analyze the properties of the black hole and possible values of the monopole charge and quintessential parameters that allows the existence of the event horizon.","The effects of the parameters on the effective potential and the innermost stable circular orbit radius are also studied.","For the neutral test particles motion, it is shown that as the quintessential parameters increase, the radius of ISCO be increased.","We have analyzed the dynamical behaviors of the neutral test particles by applying techniques including Poincar'e sections, power density and bifurcation diagram.","It is shown that the presence of a quintessence parameter creates the chaotic phenomenon for the motion of neutral particle in a Bardeen AdS black hole spacetime.","The amplification of chaos typically occurs as the energy increases under appropriate circumstances."],"url":"http://arxiv.org/abs/2405.08309v1","category":"gr-qc"}
{"created":"2024-05-14 03:42:01","title":"Area-Preserving Anisotropic Mean Curvature Flow in Two Dimensions","abstract":"We study the motion of sets by anisotropic curvature under a volume constraint in the plane. We establish the exponential convergence of the area-preserving anisotropic flat flow to a disjoint union of Wulff shapes of equal area, the critical point of the anisotropic perimeter functional. This is an anisotropic analogue of the results in the isotropic case studied in \\cite{julin2022}. The novelty of our approach is in using the Cahn-Hoffman map to parametrize boundary components as small perturbations of the Wulff shape. In addition, we show that certain reflection comparison symmetries are preserved by the flat flow, which lets us obtain uniform bounds on the distance between the convergent profile and the initial data.","sentences":["We study the motion of sets by anisotropic curvature under a volume constraint in the plane.","We establish the exponential convergence of the area-preserving anisotropic flat flow to a disjoint union of Wulff shapes of equal area, the critical point of the anisotropic perimeter functional.","This is an anisotropic analogue of the results in the isotropic case studied in \\cite{julin2022}.","The novelty of our approach is in using the Cahn-Hoffman map to parametrize boundary components as small perturbations of the Wulff shape.","In addition, we show that certain reflection comparison symmetries are preserved by the flat flow, which lets us obtain uniform bounds on the distance between the convergent profile and the initial data."],"url":"http://arxiv.org/abs/2405.08296v1","category":"math.AP"}
{"created":"2024-05-14 03:31:34","title":"Tension-compression asymmetry in superelasticity of SrNi2P2 single crystals and the influence of low temperatures","abstract":"ThCr2Si2-type intermetallic compounds are known to exhibit superelasticity associated with structural transitions through lattice collapse and expansion. These transitions occur via the formation and breaking of Si-type bonds, respectively, under uniaxial loading along the [0 0 1] direction. Unlike most ThCr2Si2-type intermetallic compounds, which have either an uncollapsed tetragonal structure or a collapsed tetragonal structure, SrNi2P2 possesses a third type of collapsed structured: a one-third orthorhombic structure, for which one expects the occurrence of unique structural transitions and superelastic behavior. In this study, uniaxial compression and tension tests were conducted on micron-sized SrNi2P2 single crystalline columns at room temperature, 200K, and 100K, to investigate the influence of loading direction and temperature on the superelasticity of SrNi2P2. Experimental data and density functional theory calculations revealed the presence of tension-compression asymmetry in the structural transitions and superelasticity, as well as an asymmetry in their temperature dependence, due to the opposite superelastic process associated with compression (forming P-P bonds) and tension (breaking P-P bonds). Additionally, following thermodynamics, the observations suggest that this asymmetric superelasticity could lead to an opposite elastocaloric effect between compression and tension, which could be beneficial potentially in obtaining large temperature changes compared to conventional superelastic solids that show the same elastocaloric effect regardless of loading direction. These results provide an important fundamental insight into the structural transitions, superelasticity processes, and potential elastocaloric effects in SrNi2P2.","sentences":["ThCr2Si2-type intermetallic compounds are known to exhibit superelasticity associated with structural transitions through lattice collapse and expansion.","These transitions occur via the formation and breaking of Si-type bonds, respectively, under uniaxial loading along the [0 0 1] direction.","Unlike most ThCr2Si2-type intermetallic compounds, which have either an uncollapsed tetragonal structure or a collapsed tetragonal structure, SrNi2P2 possesses a third type of collapsed structured: a one-third orthorhombic structure, for which one expects the occurrence of unique structural transitions and superelastic behavior.","In this study, uniaxial compression and tension tests were conducted on micron-sized SrNi2P2 single crystalline columns at room temperature, 200K, and 100K, to investigate the influence of loading direction and temperature on the superelasticity of SrNi2P2.","Experimental data and density functional theory calculations revealed the presence of tension-compression asymmetry in the structural transitions and superelasticity, as well as an asymmetry in their temperature dependence, due to the opposite superelastic process associated with compression (forming P-P bonds) and tension (breaking P-P bonds).","Additionally, following thermodynamics, the observations suggest that this asymmetric superelasticity could lead to an opposite elastocaloric effect between compression and tension, which could be beneficial potentially in obtaining large temperature changes compared to conventional superelastic solids that show the same elastocaloric effect regardless of loading direction.","These results provide an important fundamental insight into the structural transitions, superelasticity processes, and potential elastocaloric effects in SrNi2P2."],"url":"http://arxiv.org/abs/2405.08294v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 03:27:15","title":"Airport Delay Prediction with Temporal Fusion Transformers","abstract":"Since flight delay hurts passengers, airlines, and airports, its prediction becomes crucial for the decision-making of all stakeholders in the aviation industry and thus has been attempted by various previous research. However, previous delay predictions are often categorical and at a highly aggregated level. To improve that, this study proposes to apply the novel Temporal Fusion Transformer model and predict numerical airport arrival delays at quarter hour level for U.S. top 30 airports. Inputs to our model include airport demand and capacity forecasts, historic airport operation efficiency information, airport wind and visibility conditions, as well as enroute weather and traffic conditions. The results show that our model achieves satisfactory performance measured by small prediction errors on the test set. In addition, the interpretability analysis of the model outputs identifies the important input factors for delay prediction.","sentences":["Since flight delay hurts passengers, airlines, and airports, its prediction becomes crucial for the decision-making of all stakeholders in the aviation industry and thus has been attempted by various previous research.","However, previous delay predictions are often categorical and at a highly aggregated level.","To improve that, this study proposes to apply the novel Temporal Fusion Transformer model and predict numerical airport arrival delays at quarter hour level for U.S. top 30 airports.","Inputs to our model include airport demand and capacity forecasts, historic airport operation efficiency information, airport wind and visibility conditions, as well as enroute weather and traffic conditions.","The results show that our model achieves satisfactory performance measured by small prediction errors on the test set.","In addition, the interpretability analysis of the model outputs identifies the important input factors for delay prediction."],"url":"http://arxiv.org/abs/2405.08293v1","category":"cs.LG"}
{"created":"2024-05-14 03:11:55","title":"Exploring Equilibrium Strategies in Network Games with Generative AI","abstract":"Game theory offers a powerful framework for analyzing strategic interactions among decision-makers, providing tools to model, analyze, and predict their behavior. However, implementing game theory can be challenging due to difficulties in deriving solutions, understanding interactions, and ensuring optimal performance. Traditional non-AI and discriminative AI approaches have made valuable contributions but struggle with limitations in handling large-scale games and dynamic scenarios. In this context, generative AI emerges as a promising solution because of its superior data analysis and generation capabilities. This paper comprehensively summarizes the challenges, solutions, and outlooks of combining generative AI with game theory. We start with reviewing the limitations of traditional non-AI and discriminative AI approaches in employing game theory, and then highlight the necessity and advantages of integrating generative AI. Next, we explore the applications of generative AI in various stages of the game theory lifecycle, including model formulation, solution derivation, and strategy improvement. Additionally, from game theory viewpoint, we propose a generative AI-enabled framework for optimizing machine learning model performance against false data injection attacks, supported by a case study to demonstrate its effectiveness. Finally, we outline future research directions for generative AI-enabled game theory, paving the way for its further advancements and development.","sentences":["Game theory offers a powerful framework for analyzing strategic interactions among decision-makers, providing tools to model, analyze, and predict their behavior.","However, implementing game theory can be challenging due to difficulties in deriving solutions, understanding interactions, and ensuring optimal performance.","Traditional non-AI and discriminative AI approaches have made valuable contributions but struggle with limitations in handling large-scale games and dynamic scenarios.","In this context, generative AI emerges as a promising solution because of its superior data analysis and generation capabilities.","This paper comprehensively summarizes the challenges, solutions, and outlooks of combining generative AI with game theory.","We start with reviewing the limitations of traditional non-AI and discriminative AI approaches in employing game theory, and then highlight the necessity and advantages of integrating generative AI.","Next, we explore the applications of generative AI in various stages of the game theory lifecycle, including model formulation, solution derivation, and strategy improvement.","Additionally, from game theory viewpoint, we propose a generative AI-enabled framework for optimizing machine learning model performance against false data injection attacks, supported by a case study to demonstrate its effectiveness.","Finally, we outline future research directions for generative AI-enabled game theory, paving the way for its further advancements and development."],"url":"http://arxiv.org/abs/2405.08289v1","category":"cs.GT"}
{"created":"2024-05-14 03:07:54","title":"Orthogonal Delay-Doppler Division Multiplexing Modulation with Tomlinson-Harashima Precoding","abstract":"The orthogonal delay-Doppler (DD) division multiplexing(ODDM) modulation has been recently proposed as a promising modulation scheme for next-generation communication systems with high mobility. Despite its benefits, ODDM modulation and other DD domain modulation schemes face the challenge of excessive equalization complexity. To address this challenge, we propose time domain Tomlinson-Harashima precoding (THP) for the ODDM transmitter, to make the DD domain single-tap equalizer feasible, thereby reducing the equalization complexity. In our design, we first pre-cancel the inter-symbolinterference (ISI) using the linear time-varying (LTV) channel information. Second, different from classical THP designs, we introduce a modified modulo operation with an adaptive modulus, by which the joint DD domain data multiplexing and timedomain ISI pre-cancellation can be realized without excessively increasing the bit errors. We then analytically study the losses encountered in this design, namely the power loss, the modulo noise loss, and the modulo signal loss. Based on this analysis, BER lower bounds of the ODDM system with time domain THP are derived when 4-QAM or 16-QAM modulations are adopted for symbol mapping in the DD domain. Finally, through numerical results, we validate our analysis and then demonstrate that the ODDM system with time domain THP is a promising solution to realize better BER performance over LTV channels compared to orthogonal frequency division multiplexing systems with single-tap equalizer and ODDM systems with maximum ratio combining.","sentences":["The orthogonal delay-Doppler (DD) division multiplexing(ODDM) modulation has been recently proposed as a promising modulation scheme for next-generation communication systems with high mobility.","Despite its benefits, ODDM modulation and other DD domain modulation schemes face the challenge of excessive equalization complexity.","To address this challenge, we propose time domain Tomlinson-Harashima precoding (THP) for the ODDM transmitter, to make the DD domain single-tap equalizer feasible, thereby reducing the equalization complexity.","In our design, we first pre-cancel the inter-symbolinterference (ISI) using the linear time-varying (LTV) channel information.","Second, different from classical THP designs, we introduce a modified modulo operation with an adaptive modulus, by which the joint DD domain data multiplexing and timedomain ISI pre-cancellation can be realized without excessively increasing the bit errors.","We then analytically study the losses encountered in this design, namely the power loss, the modulo noise loss, and the modulo signal loss.","Based on this analysis, BER lower bounds of the ODDM system with time domain THP are derived when 4-QAM or 16-QAM modulations are adopted for symbol mapping in the DD domain.","Finally, through numerical results, we validate our analysis and then demonstrate that the ODDM system with time domain THP is a promising solution to realize better BER performance over LTV channels compared to orthogonal frequency division multiplexing systems with single-tap equalizer and ODDM systems with maximum ratio combining."],"url":"http://arxiv.org/abs/2405.08288v1","category":"eess.SP"}
{"created":"2024-05-14 02:46:54","title":"Vector Field-Guided Learning Predictive Control for Motion Planning of Mobile Robots with Unknown Dynamics","abstract":"Safe maneuvering capability is critical for mobile robots in complex environments. However, robotic system dynamics are often time-varying, uncertain, or even unknown during the motion planning and control process. Therefore, many existing model-based reinforcement learning (RL) methods could not achieve satisfactory reliability in guaranteeing safety. To address this challenge, we propose a two-level Vector Field-guided Learning Predictive Control (VF-LPC) approach that guarantees safe maneuverability. The first level, the guiding level, generates safe desired trajectories using the designed kinodynamic guiding vector field, enabling safe motion in obstacle-dense environments. The second level, the Integrated Motion Planning and Control (IMPC) level, first uses the deep Koopman operator to learn a nominal dynamics model offline and then updates the model uncertainties online using sparse Gaussian processes (GPs). The learned dynamics and game-based safe barrier function are then incorporated into the learning predictive control framework to generate near-optimal control sequences. We conducted tests to compare the performance of VF-LPC with existing advanced planning methods in an obstacle-dense environment. The simulation results show that it can generate feasible trajectories quickly. Then, VF-LPC is evaluated against motion planning methods that employ model predictive control (MPC) and RL in high-fidelity CarSim software. The results show that VF-LPC outperforms them under metrics of completion time, route length, and average solution time. We also carried out path-tracking control tests on a racing road to validate the model uncertainties learning capability. Finally, we conducted real-world experiments on a Hongqi E-HS3 vehicle, further validating the VF-LPC approach's effectiveness.","sentences":["Safe maneuvering capability is critical for mobile robots in complex environments.","However, robotic system dynamics are often time-varying, uncertain, or even unknown during the motion planning and control process.","Therefore, many existing model-based reinforcement learning (RL) methods could not achieve satisfactory reliability in guaranteeing safety.","To address this challenge, we propose a two-level Vector Field-guided Learning Predictive Control (VF-LPC) approach that guarantees safe maneuverability.","The first level, the guiding level, generates safe desired trajectories using the designed kinodynamic guiding vector field, enabling safe motion in obstacle-dense environments.","The second level, the Integrated Motion Planning and Control (IMPC) level, first uses the deep Koopman operator to learn a nominal dynamics model offline and then updates the model uncertainties online using sparse Gaussian processes (GPs).","The learned dynamics and game-based safe barrier function are then incorporated into the learning predictive control framework to generate near-optimal control sequences.","We conducted tests to compare the performance of VF-LPC with existing advanced planning methods in an obstacle-dense environment.","The simulation results show that it can generate feasible trajectories quickly.","Then, VF-LPC is evaluated against motion planning methods that employ model predictive control (MPC) and RL in high-fidelity CarSim software.","The results show that VF-LPC outperforms them under metrics of completion time, route length, and average solution time.","We also carried out path-tracking control tests on a racing road to validate the model uncertainties learning capability.","Finally, we conducted real-world experiments on a Hongqi E-HS3 vehicle, further validating the VF-LPC approach's effectiveness."],"url":"http://arxiv.org/abs/2405.08283v1","category":"cs.RO"}
{"created":"2024-05-14 02:34:12","title":"Mahler's problem and Turyn polynomials","abstract":"Mahler's problem asks for the largest possible value of the Mahler measure, normalized by the $L_2$ norm, of a polynomial with $\\pm1$ coefficients and large degree. We establish a new record value in this problem exceeding $0.95$ by analyzing certain Turyn polynomials, which are defined by cyclically shifting the coefficients of a Fekete polynomial by a prescribed amount. It was recently established that the distribution of values over the unit circle of Fekete polynomials of large degree is effectively modeled by a particular random point process. We extend this analysis to the Turyn polynomials, and determine expressions for the asymptotic normalized Mahler measure of these polynomials, as well as for their normalized $L_q$ norms. We also describe a number of calculations on the corresponding random processes, which indicate that the Turyn polynomials where the shift is approximately $1/4$ of the length have Mahler measure exceeding $95\\%$ of their $L_2$ norm. Further, we show that these asymptotic values are not disturbed by a small change to make polynomials having entirely $\\pm1$ coefficients, which establishes the result on Mahler's problem.","sentences":["Mahler's problem asks for the largest possible value of the Mahler measure, normalized by the $L_2$ norm, of a polynomial with $\\pm1$ coefficients and large degree.","We establish a new record value in this problem exceeding $0.95$ by analyzing certain Turyn polynomials, which are defined by cyclically shifting the coefficients of a Fekete polynomial by a prescribed amount.","It was recently established that the distribution of values over the unit circle of Fekete polynomials of large degree is effectively modeled by a particular random point process.","We extend this analysis to the Turyn polynomials, and determine expressions for the asymptotic normalized Mahler measure of these polynomials, as well as for their normalized $L_q$ norms.","We also describe a number of calculations on the corresponding random processes, which indicate that the Turyn polynomials where the shift is approximately $1/4$ of the length have Mahler measure exceeding $95\\%$ of their $L_2$ norm.","Further, we show that these asymptotic values are not disturbed by a small change to make polynomials having entirely $\\pm1$ coefficients, which establishes the result on Mahler's problem."],"url":"http://arxiv.org/abs/2405.08281v1","category":"math.NT"}
{"created":"2024-05-14 02:21:20","title":"Facilitating Feature and Topology Lightweighting: An Ethereum Transaction Graph Compression Method for Malicious Account Detection","abstract":"Ethereum has become one of the primary global platforms for cryptocurrency, playing an important role in promoting the diversification of the financial ecosystem. However, the relative lag in regulation has led to a proliferation of malicious activities in Ethereum, posing a serious threat to fund security. Existing regulatory methods usually detect malicious accounts through feature engineering or large-scale transaction graph mining. However, due to the immense scale of transaction data and malicious attacks, these methods suffer from inefficiency and low robustness during data processing and anomaly detection. In this regard, we propose an Ethereum Transaction Graph Compression method named TGC4Eth, which assists malicious account detection by lightweighting both features and topology of the transaction graph. At the feature level, we select transaction features based on their low importance to improve the robustness of the subsequent detection models against feature evasion attacks; at the topology level, we employ focusing and coarsening processes to compress the structure of the transaction graph, thereby improving both data processing and inference efficiency of detection models. Extensive experiments demonstrate that TGC4Eth significantly improves the computational efficiency of existing detection models while preserving the connectivity of the transaction graph. Furthermore, TGC4Eth enables existing detection models to maintain stable performance and exhibit high robustness against feature evasion attacks.","sentences":["Ethereum has become one of the primary global platforms for cryptocurrency, playing an important role in promoting the diversification of the financial ecosystem.","However, the relative lag in regulation has led to a proliferation of malicious activities in Ethereum, posing a serious threat to fund security.","Existing regulatory methods usually detect malicious accounts through feature engineering or large-scale transaction graph mining.","However, due to the immense scale of transaction data and malicious attacks, these methods suffer from inefficiency and low robustness during data processing and anomaly detection.","In this regard, we propose an Ethereum Transaction Graph Compression method named TGC4Eth, which assists malicious account detection by lightweighting both features and topology of the transaction graph.","At the feature level, we select transaction features based on their low importance to improve the robustness of the subsequent detection models against feature evasion attacks; at the topology level, we employ focusing and coarsening processes to compress the structure of the transaction graph, thereby improving both data processing and inference efficiency of detection models.","Extensive experiments demonstrate that TGC4Eth significantly improves the computational efficiency of existing detection models while preserving the connectivity of the transaction graph.","Furthermore, TGC4Eth enables existing detection models to maintain stable performance and exhibit high robustness against feature evasion attacks."],"url":"http://arxiv.org/abs/2405.08278v1","category":"cs.CR"}
{"created":"2024-05-14 02:11:38","title":"Scalable Subsampling Inference for Deep Neural Networks","abstract":"Deep neural networks (DNN) has received increasing attention in machine learning applications in the last several years. Recently, a non-asymptotic error bound has been developed to measure the performance of the fully connected DNN estimator with ReLU activation functions for estimating regression models. The paper at hand gives a small improvement on the current error bound based on the latest results on the approximation ability of DNN. More importantly, however, a non-random subsampling technique--scalable subsampling--is applied to construct a `subagged' DNN estimator. Under regularity conditions, it is shown that the subagged DNN estimator is computationally efficient without sacrificing accuracy for either estimation or prediction tasks. Beyond point estimation/prediction, we propose different approaches to build confidence and prediction intervals based on the subagged DNN estimator. In addition to being asymptotically valid, the proposed confidence/prediction intervals appear to work well in finite samples. All in all, the scalable subsampling DNN estimator offers the complete package in terms of statistical inference, i.e., (a) computational efficiency; (b) point estimation/prediction accuracy; and (c) allowing for the construction of practically useful confidence and prediction intervals.","sentences":["Deep neural networks (DNN) has received increasing attention in machine learning applications in the last several years.","Recently, a non-asymptotic error bound has been developed to measure the performance of the fully connected DNN estimator with ReLU activation functions for estimating regression models.","The paper at hand gives a small improvement on the current error bound based on the latest results on the approximation ability of DNN.","More importantly, however, a non-random subsampling technique--scalable subsampling--is applied to construct a `subagged' DNN estimator.","Under regularity conditions, it is shown that the subagged DNN estimator is computationally efficient without sacrificing accuracy for either estimation or prediction tasks.","Beyond point estimation/prediction, we propose different approaches to build confidence and prediction intervals based on the subagged DNN estimator.","In addition to being asymptotically valid, the proposed confidence/prediction intervals appear to work well in finite samples.","All in all, the scalable subsampling DNN estimator offers the complete package in terms of statistical inference, i.e., (a) computational efficiency; (b) point estimation/prediction accuracy; and (c) allowing for the construction of practically useful confidence and prediction intervals."],"url":"http://arxiv.org/abs/2405.08276v1","category":"stat.ML"}
{"created":"2024-05-14 01:59:29","title":"On saturation of the discrepancy principle for nonlinear Tikhonov regularization in Hilbert spaces","abstract":"In this paper we revisit the discrepancy principle for Tikhonov regularization of nonlinear ill-posed problems in Hilbert spaces and provide some new and improved saturation results under less restrictive conditions, comparing with the existing results in the literature.","sentences":["In this paper we revisit the discrepancy principle for Tikhonov regularization of nonlinear ill-posed problems in Hilbert spaces and provide some new and improved saturation results under less restrictive conditions, comparing with the existing results in the literature."],"url":"http://arxiv.org/abs/2405.08269v1","category":"math.NA"}
{"created":"2024-05-14 01:51:00","title":"Line intensities of CO near 1560 nm measured with absorption and dispersion spectroscopy","abstract":"High-precision line intensities are of great value in various applications, such as greenhouse gas metrology, planetary atmospheric analysis, and trace gas detection. Here we report simultaneous measurements of cavity-enhanced absorption and dispersion spectroscopy of the prototype molecule $^{12}$C$^{16}$O using the same optical resonant cavity. Nine lines were measured in the R branch of the $v=3-0$ band. The absorption and dispersion spectra were fitted separately with speed-dependent Voigt profiles, and the line intensities obtained by the two methods agree within the experimental uncertainty of about 1\\textperthousand. The results demonstrate the feasibility of SI-traceable molecular density measurements based on laser spectroscopy.","sentences":["High-precision line intensities are of great value in various applications, such as greenhouse gas metrology, planetary atmospheric analysis, and trace gas detection.","Here we report simultaneous measurements of cavity-enhanced absorption and dispersion spectroscopy of the prototype molecule $^{12}$C$^{16}$O using the same optical resonant cavity.","Nine lines were measured in the R branch of the $v=3-0$ band.","The absorption and dispersion spectra were fitted separately with speed-dependent Voigt profiles, and the line intensities obtained by the two methods agree within the experimental uncertainty of about 1\\textperthousand.","The results demonstrate the feasibility of SI-traceable molecular density measurements based on laser spectroscopy."],"url":"http://arxiv.org/abs/2405.08267v1","category":"physics.chem-ph"}
{"created":"2024-05-14 01:26:49","title":"Multi-Agent Combinatorial Contracts","abstract":"Combinatorial contracts are emerging as a key paradigm in algorithmic contract design, paralleling the role of combinatorial auctions in algorithmic mechanism design. In this paper we study natural combinatorial contract settings involving teams of agents, each capable of performing multiple actions. This scenario extends two fundamental special cases previously examined in the literature, namely the single-agent combinatorial action model of [Duetting et al., 2021] and the multi-agent binary-action model of [Babaioff et al., 2012, Duetting et al., 2023].   We study the algorithmic and computational aspects of these settings, highlighting the unique challenges posed by the absence of certain monotonicity properties essential for analyzing the previous special cases. To navigate these complexities, we introduce a broad set of novel tools that deepen our understanding of combinatorial contracts environments and yield good approximation guarantees.   Our main result is a constant-factor approximation for submodular multi-agent multi-action problems with value and demand oracles access. This result is tight: we show that this problem admits no PTAS (even under binary actions). As a side product of our main result, we devise an FPTAS, with value and demand oracles, for single-agent combinatorial action scenarios with general reward functions, which is of independent interest. We also provide bounds on the gap between the optimal welfare and the principal's utility. We show that, for subadditive rewards, perhaps surprisingly, this gap scales only logarithmically (rather than linearly) in the size of the action space.","sentences":["Combinatorial contracts are emerging as a key paradigm in algorithmic contract design, paralleling the role of combinatorial auctions in algorithmic mechanism design.","In this paper we study natural combinatorial contract settings involving teams of agents, each capable of performing multiple actions.","This scenario extends two fundamental special cases previously examined in the literature, namely the single-agent combinatorial action model of [Duetting et al., 2021] and the multi-agent binary-action model of [Babaioff et al., 2012, Duetting et al., 2023].   ","We study the algorithmic and computational aspects of these settings, highlighting the unique challenges posed by the absence of certain monotonicity properties essential for analyzing the previous special cases.","To navigate these complexities, we introduce a broad set of novel tools that deepen our understanding of combinatorial contracts environments and yield good approximation guarantees.   ","Our main result is a constant-factor approximation for submodular multi-agent multi-action problems with value and demand oracles access.","This result is tight: we show that this problem admits no PTAS (even under binary actions).","As a side product of our main result, we devise an FPTAS, with value and demand oracles, for single-agent combinatorial action scenarios with general reward functions, which is of independent interest.","We also provide bounds on the gap between the optimal welfare and the principal's utility.","We show that, for subadditive rewards, perhaps surprisingly, this gap scales only logarithmically (rather than linearly) in the size of the action space."],"url":"http://arxiv.org/abs/2405.08260v1","category":"cs.GT"}
{"created":"2024-05-14 01:09:10","title":"Total Variation Distance for Product Distributions is $\\#\\mathsf{P}$-Complete","abstract":"We show that computing the total variation distance between two product distributions is $\\#\\mathsf{P}$-complete. This is in stark contrast with other distance measures such as Kullback-Leibler, Chi-square, and Hellinger, which tensorize over the marginals leading to efficient algorithms.","sentences":["We show that computing the total variation distance between two product distributions is $\\#\\mathsf{P}$-complete.","This is in stark contrast with other distance measures such as Kullback-Leibler, Chi-square, and Hellinger, which tensorize over the marginals leading to efficient algorithms."],"url":"http://arxiv.org/abs/2405.08255v1","category":"cs.CC"}
{"created":"2024-05-14 01:01:05","title":"Thompson Sampling for Infinite-Horizon Discounted Decision Processes","abstract":"We model a Markov decision process, parametrized by an unknown parameter, and study the asymptotic behavior of a sampling-based algorithm, called Thompson sampling. The standard definition of regret is not always suitable to evaluate a policy, especially when the underlying chain structure is general. We show that the standard (expected) regret can grow (super-)linearly and fails to capture the notion of learning in realistic settings with non-trivial state evolution. By decomposing the standard (expected) regret, we develop a new metric, called the expected residual regret, which forgets the immutable consequences of past actions. Instead, it measures regret against the optimal reward moving forward from the current period. We show that the expected residual regret of the Thompson sampling algorithm is upper bounded by a term which converges exponentially fast to 0. We present conditions under which the posterior sampling error of Thompson sampling converges to 0 almost surely. We then introduce the probabilistic version of the expected residual regret and present conditions under which it converges to 0 almost surely. Thus, we provide a viable concept of learning for sampling algorithms which will serve useful in broader settings than had been considered previously.","sentences":["We model a Markov decision process, parametrized by an unknown parameter, and study the asymptotic behavior of a sampling-based algorithm, called Thompson sampling.","The standard definition of regret is not always suitable to evaluate a policy, especially when the underlying chain structure is general.","We show that the standard (expected) regret can grow (super-)linearly and fails to capture the notion of learning in realistic settings with non-trivial state evolution.","By decomposing the standard (expected) regret, we develop a new metric, called the expected residual regret, which forgets the immutable consequences of past actions.","Instead, it measures regret against the optimal reward moving forward from the current period.","We show that the expected residual regret of the Thompson sampling algorithm is upper bounded by a term which converges exponentially fast to 0.","We present conditions under which the posterior sampling error of Thompson sampling converges to 0 almost surely.","We then introduce the probabilistic version of the expected residual regret and present conditions under which it converges to 0 almost surely.","Thus, we provide a viable concept of learning for sampling algorithms which will serve useful in broader settings than had been considered previously."],"url":"http://arxiv.org/abs/2405.08253v1","category":"stat.ML"}
{"created":"2024-05-13 23:51:07","title":"Thermodynamics of AdS-Schwarzschild-like black hole in loop quantum gravity","abstract":"Under the assumption that loop quantum gravity (LQG) won't couple with the cosmological constant, we obtained the metric of Schwarzschild-like black hole with LQG correction in anti-de Sitter (AdS) space-time, and investigated its thermodynamics, including equation of state, criticality and Joule-Thomson expansion. We drew $P$-$v$ graph and calculated the critical point. It is found that properties near the critical point is similar to Van der Waals system, with exactly the same critical exponents, and a critical ration $7/18$, which is slightly bigger than Van der Waals model $3/8$. Joule-Thomson expansion is also studied. It is interesting that compared with Schwarzschild-AdS black hole, LQG effect will bring inversion points. The inversion curve will divide $T$-$P$ coordinate system into two zones: heating region and cooling region, which is shown in inversion curves and isenthalpic curves detailedly. And the minimum inversion mass is obtained.","sentences":["Under the assumption that loop quantum gravity (LQG) won't couple with the cosmological constant, we obtained the metric of Schwarzschild-like black hole with LQG correction in anti-de Sitter (AdS) space-time, and investigated its thermodynamics, including equation of state, criticality and Joule-Thomson expansion.","We drew $P$-$v$ graph and calculated the critical point.","It is found that properties near the critical point is similar to Van der Waals system, with exactly the same critical exponents, and a critical ration $7/18$, which is slightly bigger than Van der Waals model $3/8$. Joule-Thomson expansion is also studied.","It is interesting that compared with Schwarzschild-AdS black hole, LQG effect will bring inversion points.","The inversion curve will divide $T$-$P$ coordinate system into two zones: heating region and cooling region, which is shown in inversion curves and isenthalpic curves detailedly.","And the minimum inversion mass is obtained."],"url":"http://arxiv.org/abs/2405.08241v1","category":"gr-qc"}
{"created":"2024-05-13 23:19:02","title":"Factors Shaping Financial Success: A Deep Dive into Influencing Variables","abstract":"This paper explores various socioeconomic factors that contribute to individual financial success using machine learning algorithms and approaches.   Financial success, a critical aspect of all individual's well-being, is a complex concept influenced by a plethora of different factors. This study aims to understand the true determinants of financial success. It examines the survey data from the National Longitudinal Survey of Youth 1997 by the Bureau of Labor Statistics [1], consisting of a sample of 8,984 individuals's longitudinal data over years. The dataset comprises income variables and a large set of socioeconomic variables of individuals.   An in-depth analysis demonstrates the effectiveness of machine learning algorithms in financial success research, highlights the potential of leveraging longitudinal data to enhance prediction accuracy, and provides valuable insights into how various socioeconomic factors influence financial success.   The findings underscore the significant influence of highest education degree, occupation and gender as the top three determinants of individual income among socioeconomic factors examined. Yearly working hours, age and work tenure emerge as three secondary influencing factors, and all other factors including parental household income, industry, parents' highest grade and others are identified as tertiary factors.   These insights allow researchers to better understand the complex nature of financial success and enable policymakers to grasp the underlying dynamics shaping aspirations, decision-making, and the broader socio-economic fabric of society. This comprehension is crucial for fostering financial success among individuals and advancing broader societal well-being.","sentences":["This paper explores various socioeconomic factors that contribute to individual financial success using machine learning algorithms and approaches.   ","Financial success, a critical aspect of all individual's well-being, is a complex concept influenced by a plethora of different factors.","This study aims to understand the true determinants of financial success.","It examines the survey data from the National Longitudinal Survey of Youth 1997 by the Bureau of Labor Statistics","[1], consisting of a sample of 8,984 individuals's longitudinal data over years.","The dataset comprises income variables and a large set of socioeconomic variables of individuals.   ","An in-depth analysis demonstrates the effectiveness of machine learning algorithms in financial success research, highlights the potential of leveraging longitudinal data to enhance prediction accuracy, and provides valuable insights into how various socioeconomic factors influence financial success.   ","The findings underscore the significant influence of highest education degree, occupation and gender as the top three determinants of individual income among socioeconomic factors examined.","Yearly working hours, age and work tenure emerge as three secondary influencing factors, and all other factors including parental household income, industry, parents' highest grade and others are identified as tertiary factors.   ","These insights allow researchers to better understand the complex nature of financial success and enable policymakers to grasp the underlying dynamics shaping aspirations, decision-making, and the broader socio-economic fabric of society.","This comprehension is crucial for fostering financial success among individuals and advancing broader societal well-being."],"url":"http://arxiv.org/abs/2405.08233v1","category":"cs.LG"}
{"created":"2024-05-13 22:45:44","title":"SeNMo: A Self-Normalizing Deep Learning Model for Enhanced Multi-Omics Data Analysis in Oncology","abstract":"Multi-omics research has enhanced our understanding of cancer heterogeneity and progression. Investigating molecular data through multi-omics approaches is crucial for unraveling the complex biological mechanisms underlying cancer, thereby enabling effective diagnosis, treatment, and prevention strategies. However, predicting patient outcomes through integration of all available multi-omics data is an under-study research direction. Here, we present SeNMo (Self-normalizing Network for Multi-omics), a deep neural network trained on multi-omics data across 33 cancer types. SeNMo is efficient in handling multi-omics data characterized by high-width (many features) and low-length (fewer samples) attributes. We trained SeNMo for the task of overall survival using pan-cancer data involving 33 cancer sites from Genomics Data Commons (GDC). The training data includes gene expression, DNA methylation, miRNA expression, DNA mutations, protein expression modalities, and clinical data. We evaluated the model's performance in predicting overall survival using concordance index (C-Index). SeNMo performed consistently well in training regime, with the validation C-Index of 0.76 on GDC's public data. In the testing regime, SeNMo performed with a C-Index of 0.758 on a held-out test set. The model showed an average accuracy of 99.8% on the task of classifying the primary cancer type on the pan-cancer test cohort. SeNMo proved to be a mini-foundation model for multi-omics oncology data because it demonstrated robust performance, and adaptability not only across molecular data types but also on the classification task of predicting the primary cancer type of patients. SeNMo can be further scaled to any cancer site and molecular data type. We believe SeNMo and similar models are poised to transform the oncology landscape, offering hope for more effective, efficient, and patient-centric cancer care.","sentences":["Multi-omics research has enhanced our understanding of cancer heterogeneity and progression.","Investigating molecular data through multi-omics approaches is crucial for unraveling the complex biological mechanisms underlying cancer, thereby enabling effective diagnosis, treatment, and prevention strategies.","However, predicting patient outcomes through integration of all available multi-omics data is an under-study research direction.","Here, we present SeNMo (Self-normalizing Network for Multi-omics), a deep neural network trained on multi-omics data across 33 cancer types.","SeNMo is efficient in handling multi-omics data characterized by high-width (many features) and low-length (fewer samples) attributes.","We trained SeNMo for the task of overall survival using pan-cancer data involving 33 cancer sites from Genomics Data Commons (GDC).","The training data includes gene expression, DNA methylation, miRNA expression, DNA mutations, protein expression modalities, and clinical data.","We evaluated the model's performance in predicting overall survival using concordance index (C-Index).","SeNMo performed consistently well in training regime, with the validation C-Index of 0.76 on GDC's public data.","In the testing regime, SeNMo performed with a C-Index of 0.758 on a held-out test set.","The model showed an average accuracy of 99.8% on the task of classifying the primary cancer type on the pan-cancer test cohort.","SeNMo proved to be a mini-foundation model for multi-omics oncology data because it demonstrated robust performance, and adaptability not only across molecular data types but also on the classification task of predicting the primary cancer type of patients.","SeNMo can be further scaled to any cancer site and molecular data type.","We believe SeNMo and similar models are poised to transform the oncology landscape, offering hope for more effective, efficient, and patient-centric cancer care."],"url":"http://arxiv.org/abs/2405.08226v1","category":"cs.LG"}
{"created":"2024-05-13 22:29:33","title":"An information-theoretic model of shallow and deep language comprehension","abstract":"A large body of work in psycholinguistics has focused on the idea that online language comprehension can be shallow or `good enough': given constraints on time or available computation, comprehenders may form interpretations of their input that are plausible but inaccurate. However, this idea has not yet been linked with formal theories of computation under resource constraints. Here we use information theory to formulate a model of language comprehension as an optimal trade-off between accuracy and processing depth, formalized as bits of information extracted from the input, which increases with processing time. The model provides a measure of processing effort as the change in processing depth, which we link to EEG signals and reading times. We validate our theory against a large-scale dataset of garden path sentence reading times, and EEG experiments featuring N400, P600 and biphasic ERP effects. By quantifying the timecourse of language processing as it proceeds from shallow to deep, our model provides a unified framework to explain behavioral and neural signatures of language comprehension.","sentences":["A large body of work in psycholinguistics has focused on the idea that online language comprehension can be shallow or `good enough': given constraints on time or available computation, comprehenders may form interpretations of their input that are plausible but inaccurate.","However, this idea has not yet been linked with formal theories of computation under resource constraints.","Here we use information theory to formulate a model of language comprehension as an optimal trade-off between accuracy and processing depth, formalized as bits of information extracted from the input, which increases with processing time.","The model provides a measure of processing effort as the change in processing depth, which we link to EEG signals and reading times.","We validate our theory against a large-scale dataset of garden path sentence reading times, and EEG experiments featuring N400, P600 and biphasic ERP effects.","By quantifying the timecourse of language processing as it proceeds from shallow to deep, our model provides a unified framework to explain behavioral and neural signatures of language comprehension."],"url":"http://arxiv.org/abs/2405.08223v1","category":"cs.CL"}
{"created":"2024-05-13 22:23:33","title":"Towards Quarkonium Fragmentation from NRQCD in a Variable-Flavor Number Scheme","abstract":"We address quarkonium formation at moderate to large transverse momenta, where the single-parton collinear fragmentation prevails over the short-distance emission, directly from the hard sub-scattering, of the constituent heavy-quark pair. We rely on Non-Relativistic-QCD (NRQCD) Next-to-Leading Order (NLO) calculations for all parton fragmentation channels to quarkonia, taken as proxies for initial-scale inputs. Preliminary sets of Variable-Flavor Number-Scheme (VFNS) fragmentation functions (FFs) are built via a DGLAP scheme that properly accounts for evolution thresholds. Statistical errors are assessed via a Monte Carlo (MC), replica-like approach aimed at catching Missing Higher-Order Uncertainties (MHOUs).","sentences":["We address quarkonium formation at moderate to large transverse momenta, where the single-parton collinear fragmentation prevails over the short-distance emission, directly from the hard sub-scattering, of the constituent heavy-quark pair.","We rely on Non-Relativistic-QCD (NRQCD) Next-to-Leading Order (NLO) calculations for all parton fragmentation channels to quarkonia, taken as proxies for initial-scale inputs.","Preliminary sets of Variable-Flavor Number-Scheme (VFNS) fragmentation functions (FFs) are built via a DGLAP scheme that properly accounts for evolution thresholds.","Statistical errors are assessed via a Monte Carlo (MC), replica-like approach aimed at catching Missing Higher-Order Uncertainties (MHOUs)."],"url":"http://arxiv.org/abs/2405.08221v1","category":"hep-ph"}
{"created":"2024-05-13 21:49:15","title":"Beyond Theorems: A Counterexample to Potential Markov Game Criteria","abstract":"There are only limited classes of multi-player stochastic games in which independent learning is guaranteed to converge to a Nash equilibrium. Markov potential games are a key example of such classes. Prior work has outlined sets of sufficient conditions for a stochastic game to qualify as a Markov potential game. However, these conditions often impose strict limitations on the game's structure and tend to be challenging to verify. To address these limitations, Mguni et al. [12] introduce a relaxed notion of Markov potential games and offer an alternative set of necessary conditions for categorizing stochastic games as potential games. Under these conditions, the authors claim that a deterministic Nash equilibrium can be computed efficiently by solving a dual Markov decision process. In this paper, we offer evidence refuting this claim by presenting a counterexample.","sentences":["There are only limited classes of multi-player stochastic games in which independent learning is guaranteed to converge to a Nash equilibrium.","Markov potential games are a key example of such classes.","Prior work has outlined sets of sufficient conditions for a stochastic game to qualify as a Markov potential game.","However, these conditions often impose strict limitations on the game's structure and tend to be challenging to verify.","To address these limitations, Mguni et al.","[12] introduce a relaxed notion of Markov potential games and offer an alternative set of necessary conditions for categorizing stochastic games as potential games.","Under these conditions, the authors claim that a deterministic Nash equilibrium can be computed efficiently by solving a dual Markov decision process.","In this paper, we offer evidence refuting this claim by presenting a counterexample."],"url":"http://arxiv.org/abs/2405.08206v1","category":"cs.GT"}
{"created":"2024-05-13 21:43:52","title":"Community detection in bipartite signed networks is highly dependent on parameter choice","abstract":"Decision-making processes often involve voting. Human interactions with exogenous entities such as legislations or products can be effectively modeled as two-mode (bipartite) signed networks-where people can either vote positively, negatively, or abstain from voting on the entities. Detecting communities in such networks could help us understand underlying properties: for example ideological camps or consumer preferences. While community detection is an established practice separately for bipartite and signed networks, it remains largely unexplored in the case of bipartite signed networks. In this paper, we systematically evaluate the efficacy of community detection methods on bipartite signed networks using a synthetic benchmark and real-world datasets. Our findings reveal that when no communities are present in the data, these methods often recover spurious communities. When communities are present, the algorithms exhibit promising performance, although their performance is highly susceptible to parameter choice. This indicates that researchers using community detection methods in the context of bipartite signed networks should not take the communities found at face value: it is essential to assess the robustness of parameter choices or perform domain-specific external validation.","sentences":["Decision-making processes often involve voting.","Human interactions with exogenous entities such as legislations or products can be effectively modeled as two-mode (bipartite) signed networks-where people can either vote positively, negatively, or abstain from voting on the entities.","Detecting communities in such networks could help us understand underlying properties: for example ideological camps or consumer preferences.","While community detection is an established practice separately for bipartite and signed networks, it remains largely unexplored in the case of bipartite signed networks.","In this paper, we systematically evaluate the efficacy of community detection methods on bipartite signed networks using a synthetic benchmark and real-world datasets.","Our findings reveal that when no communities are present in the data, these methods often recover spurious communities.","When communities are present, the algorithms exhibit promising performance, although their performance is highly susceptible to parameter choice.","This indicates that researchers using community detection methods in the context of bipartite signed networks should not take the communities found at face value: it is essential to assess the robustness of parameter choices or perform domain-specific external validation."],"url":"http://arxiv.org/abs/2405.08203v1","category":"physics.soc-ph"}
{"created":"2024-05-13 21:41:42","title":"Numerical approximation of the stochastic heat equation with a distributional reaction term","abstract":"We study the numerical approximation of the stochastic heat equation with a distributional reaction term. Under a condition on the Besov regularity of the reaction term, it was proven recently that a strong solution exists and is unique in the pathwise sense, in a class of H\\\"older continuous processes. For a suitable choice of sequence $(b^k)_{k\\in \\mathbb{N}}$ approximating $b$, we prove that the error between the solution $u$ of the SPDE with reaction term $b$ and its tamed Euler finite-difference scheme with mollified drift $b^k$, converges to $0$ in $L^m(\\Omega)$ with a rate that depends on the Besov regularity of $b$. In particular, one can consider two interesting cases: first, even when $b$ is only a (finite) measure, a rate of convergence is obtained. On the other hand, when $b$ is a bounded measurable function, the (almost) optimal rate of convergence $(\\frac{1}{2}-\\varepsilon)$-in space and $(\\frac{1}{4}-\\varepsilon)$-in time is achieved. Stochastic sewing techniques are used in the proofs, in particular to deduce new regularising properties of the discrete Ornstein-Uhlenbeck process.","sentences":["We study the numerical approximation of the stochastic heat equation with a distributional reaction term.","Under a condition on the Besov regularity of the reaction term, it was proven recently that a strong solution exists and is unique in the pathwise sense, in a class of H\\\"older continuous processes.","For a suitable choice of sequence $(b^k)_{k\\in \\mathbb{N}}$ approximating $b$, we prove that the error between the solution $u$ of the SPDE with reaction term $b$ and its tamed Euler finite-difference scheme with mollified drift $b^k$, converges to $0$ in $L^m(\\Omega)$ with a rate that depends on the Besov regularity of $b$. In particular, one can consider two interesting cases: first, even when $b$ is only a (finite) measure, a rate of convergence is obtained.","On the other hand, when $b$ is a bounded measurable function, the (almost) optimal rate of convergence $(\\frac{1}{2}-\\varepsilon)$-in space and $(\\frac{1}{4}-\\varepsilon)$-in time is achieved.","Stochastic sewing techniques are used in the proofs, in particular to deduce new regularising properties of the discrete Ornstein-Uhlenbeck process."],"url":"http://arxiv.org/abs/2405.08201v1","category":"math.PR"}
{"created":"2024-05-13 21:21:18","title":"Zermelo Wind: a geometrization of the frame dragging effect","abstract":"In this article I discuss Zermelo's navigation problem in spacetime as a geometrization of the frame dragging effect, and recast various examples involving the latter into Zermelo form. I start by describing a stationary spacetime in Zermelo's form and show that the Zermelo wind is the drift velocity under frame dragging effect. Then we discuss various problems in this context, such as Hubble expansion of the universe and accelerated frames in special relativity. Another example I will discuss is the self-gravitating disk around a black hole in post-Newtonian (PN1) approximation to describe the anti-dragging effect in terms of a Zermelo wind.","sentences":["In this article I discuss Zermelo's navigation problem in spacetime as a geometrization of the frame dragging effect, and recast various examples involving the latter into Zermelo form.","I start by describing a stationary spacetime in Zermelo's form and show that the Zermelo wind is the drift velocity under frame dragging effect.","Then we discuss various problems in this context, such as Hubble expansion of the universe and accelerated frames in special relativity.","Another example I will discuss is the self-gravitating disk around a black hole in post-Newtonian (PN1) approximation to describe the anti-dragging effect in terms of a Zermelo wind."],"url":"http://arxiv.org/abs/2405.08195v1","category":"gr-qc"}
{"created":"2024-05-13 21:19:09","title":"Initial Calibration of Large Timing Arrays for the LHC","abstract":"In preparation for HL-LHC operation a number of new detector systems are being constructed with timing precision on physics objects of <50 picoseconds. These time stamps will reduce the level of pileup induced backgrounds in this LHC phase where the number of interactions per crossing will reach of order 100-200. In the case of CMS, three new systems have initially to be corrected for the usual amplitude walk resulting from the effect of variations in signal size on leading edge timing. In these systems the resulting timing spread (ie walk) ranges from one to four nanoseconds. In the following note we advocate approaching this initial calibration for walk as a calculable correction given early calibration during commissioning -- rather than depending on special collider data to perform the calibration. We derive a simple analytic expression for the walk correction and confirm its effectiveness with lab data.","sentences":["In preparation for HL-LHC operation a number of new detector systems are being constructed with timing precision on physics objects of <50 picoseconds.","These time stamps will reduce the level of pileup induced backgrounds in this LHC phase where the number of interactions per crossing will reach of order 100-200.","In the case of CMS, three new systems have initially to be corrected for the usual amplitude walk resulting from the effect of variations in signal size on leading edge timing.","In these systems the resulting timing spread (ie walk) ranges from one to four nanoseconds.","In the following note we advocate approaching this initial calibration for walk as a calculable correction given early calibration during commissioning -- rather than depending on special collider data to perform the calibration.","We derive a simple analytic expression for the walk correction and confirm its effectiveness with lab data."],"url":"http://arxiv.org/abs/2405.08191v1","category":"hep-ex"}
{"created":"2024-05-13 21:12:24","title":"Trapped flux in a small crystal of CaKFe$_4$As$_4$ at ambient pressure and in a diamond anvil pressure cell","abstract":"In an extension of our previous work, [Sergey L Bud'ko et al 2023 Supercond. Sci. Technol. 36 115001] the measurements of temperature dependent magnetization associated with trapped magnetic flux in a small single crystal of CaKFe$_4$As$_4$, using zero-field-cooled and field-cooled protocols were performed, on the same crystal, at ambient pressure without a pressure cell and at 2.2 GPa in a commercial diamond anvil cell (DAC), showing comparable results. The data show that with a proper care and understanding, trapped flux measurements in superconductors indeed can be performed on samples in DACs under pressure, as was done on superhydrides [V S Minkov et al 2023 Nat. Phys. 19 1293].","sentences":["In an extension of our previous work, [Sergey L Bud'ko et al 2023 Supercond.","Sci. Technol. 36 115001]","the measurements of temperature dependent magnetization associated with trapped magnetic flux in a small single crystal of CaKFe$_4$As$_4$, using zero-field-cooled and field-cooled protocols were performed, on the same crystal, at ambient pressure without a pressure cell and at 2.2 GPa in a commercial diamond anvil cell (DAC), showing comparable results.","The data show that with a proper care and understanding, trapped flux measurements in superconductors indeed can be performed on samples in DACs under pressure, as was done on superhydrides","[V S Minkov et al 2023","Nat.","Phys. 19 1293]."],"url":"http://arxiv.org/abs/2405.08189v1","category":"cond-mat.supr-con"}
{"created":"2024-05-13 21:09:32","title":"Optimizing Task Scheduling in Heterogeneous Computing Environments: A Comparative Analysis of CPU, GPU, and ASIC Platforms Using E2C Simulator","abstract":"Efficient task scheduling in heterogeneous computing environments is imperative for optimizing resource utilization and minimizing task completion times. In this study, we conducted a comprehensive benchmarking analysis to evaluate the performance of four scheduling algorithms First Come, First-Served (FCFS), FCFS with No Queuing (FCFS-NQ), Minimum Expected Completion Time (MECT), and Minimum Expected Execution Time (MEET) across varying workload scenarios. We defined three workload scenarios: low, medium, and high, each representing different levels of computational demands. Through rigorous experimentation and analysis, we assessed the effectiveness of each algorithm in terms of total completion percentage, energy consumption, wasted energy, and energy per completion. Our findings highlight the strengths and limitations of each algorithm, with MECT and MEET emerging as robust contenders, dynamically prioritizing tasks based on comprehensive estimates of completion and execution times. Furthermore, MECT and MEET exhibit superior energy efficiency compared to FCFS and FCFS-NQ, underscoring their suitability for resource-constrained environments. This study provides valuable insights into the efficacy of task scheduling algorithms in heterogeneous computing environments, enabling informed decision-making to enhance resource allocation, minimize task completion times, and improve energy efficiency","sentences":["Efficient task scheduling in heterogeneous computing environments is imperative for optimizing resource utilization and minimizing task completion times.","In this study, we conducted a comprehensive benchmarking analysis to evaluate the performance of four scheduling algorithms First Come, First-Served (FCFS), FCFS with No Queuing (FCFS-NQ), Minimum Expected Completion Time (MECT), and Minimum Expected Execution Time (MEET) across varying workload scenarios.","We defined three workload scenarios: low, medium, and high, each representing different levels of computational demands.","Through rigorous experimentation and analysis, we assessed the effectiveness of each algorithm in terms of total completion percentage, energy consumption, wasted energy, and energy per completion.","Our findings highlight the strengths and limitations of each algorithm, with MECT and MEET emerging as robust contenders, dynamically prioritizing tasks based on comprehensive estimates of completion and execution times.","Furthermore, MECT and MEET exhibit superior energy efficiency compared to FCFS and FCFS-NQ, underscoring their suitability for resource-constrained environments.","This study provides valuable insights into the efficacy of task scheduling algorithms in heterogeneous computing environments, enabling informed decision-making to enhance resource allocation, minimize task completion times, and improve energy efficiency"],"url":"http://arxiv.org/abs/2405.08187v1","category":"cs.DC"}
{"created":"2024-05-13 21:06:53","title":"Probabilistic Flux Limiters","abstract":"The stable numerical integration of shocks in compressible flow simulations relies on the reduction or elimination of Gibbs phenomena (unstable, spurious oscillations). A popular method to virtually eliminate Gibbs oscillations caused by numerical discretization in under-resolved simulations is to use a flux limiter. A wide range of flux limiters has been studied in the literature, with recent interest in their optimization via machine learning methods trained on high-resolution datasets. The common use of flux limiters in numerical codes as plug-and-play blackbox components makes them key targets for design improvement. Moreover, while aleatoric (inherent randomness) and epistemic (lack of knowledge) uncertainty is commonplace in fluid dynamical systems, these effects are generally ignored in the design of flux limiters. Even for deterministic dynamical models, numerical uncertainty is introduced via coarse-graining required by insufficient computational power to solve all scales of motion. Here, we introduce a conceptually distinct type of flux limiter that is designed to handle the effects of randomness in the model and uncertainty in model parameters. This new, {\\it probabilistic flux limiter}, learned with high-resolution data, consists of a set of flux limiting functions with associated probabilities, which define the frequencies of selection for their use. Using the example of Burgers' equation, we show that a machine learned, probabilistic flux limiter may be used in a shock capturing code to more accurately capture shock profiles. In particular, we show that our probabilistic flux limiter outperforms standard limiters, and can be successively improved upon (up to a point) by expanding the set of probabilistically chosen flux limiting functions.","sentences":["The stable numerical integration of shocks in compressible flow simulations relies on the reduction or elimination of Gibbs phenomena (unstable, spurious oscillations).","A popular method to virtually eliminate Gibbs oscillations caused by numerical discretization in under-resolved simulations is to use a flux limiter.","A wide range of flux limiters has been studied in the literature, with recent interest in their optimization via machine learning methods trained on high-resolution datasets.","The common use of flux limiters in numerical codes as plug-and-play blackbox components makes them key targets for design improvement.","Moreover, while aleatoric (inherent randomness) and epistemic (lack of knowledge) uncertainty is commonplace in fluid dynamical systems, these effects are generally ignored in the design of flux limiters.","Even for deterministic dynamical models, numerical uncertainty is introduced via coarse-graining required by insufficient computational power to solve all scales of motion.","Here, we introduce a conceptually distinct type of flux limiter that is designed to handle the effects of randomness in the model and uncertainty in model parameters.","This new, {\\it probabilistic flux limiter}, learned with high-resolution data, consists of a set of flux limiting functions with associated probabilities, which define the frequencies of selection for their use.","Using the example of Burgers' equation, we show that a machine learned, probabilistic flux limiter may be used in a shock capturing code to more accurately capture shock profiles.","In particular, we show that our probabilistic flux limiter outperforms standard limiters, and can be successively improved upon (up to a point) by expanding the set of probabilistically chosen flux limiting functions."],"url":"http://arxiv.org/abs/2405.08185v1","category":"physics.flu-dyn"}
{"created":"2024-05-13 21:00:01","title":"Topological quantum phase transitions driven by displacement fields in the twisted MoTe2 bilayers","abstract":"We study twisted bilayer MoTe$_2$ systems at fractional fillings of the lowest hole band under applied out-of-plane displacement fields. By employing exact diagonalization in finite-size systems, we systematically map out the ground state quantum phase diagram for two filling fractions, $\\nu=1/3$ and $2/3$, and provide a detailed characterization of each phase. We identify the phase transition between a fractional Chern insulator (FCI) and a layer-polarized charge density wave (CDW) at a filling fraction of $\\nu=1/3$, denoted as CDW-$1$. Additionally, we demonstrate that the competition between the displacement field and twist angle leads to another phase transition from a layer-polarized CDW-$1$ to a layer-hybridized CDW-$2$, identified as a first-order phase transition. Furthermore, at $\\nu=2/3$ filling of the lowest hole band, we observe that the FCI remains stable against the displacement field until it approaches proximity to a transition in single-particle band topology at a smaller twist angle.","sentences":["We study twisted bilayer MoTe$_2$ systems at fractional fillings of the lowest hole band under applied out-of-plane displacement fields.","By employing exact diagonalization in finite-size systems, we systematically map out the ground state quantum phase diagram for two filling fractions, $\\nu=1/3$ and $2/3$, and provide a detailed characterization of each phase.","We identify the phase transition between a fractional Chern insulator (FCI) and a layer-polarized charge density wave (CDW) at a filling fraction of $\\nu=1/3$, denoted as CDW-$1$. Additionally, we demonstrate that the competition between the displacement field and twist angle leads to another phase transition from a layer-polarized CDW-$1$ to a layer-hybridized CDW-$2$, identified as a first-order phase transition.","Furthermore, at $\\nu=2/3$ filling of the lowest hole band, we observe that the FCI remains stable against the displacement field until it approaches proximity to a transition in single-particle band topology at a smaller twist angle."],"url":"http://arxiv.org/abs/2405.08181v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-13 20:57:01","title":"Do Bayesian imaging methods report trustworthy probabilities?","abstract":"Bayesian statistics is a cornerstone of imaging sciences, underpinning many and varied approaches from Markov random fields to score-based denoising diffusion models. In addition to powerful image estimation methods, the Bayesian paradigm also provides a framework for uncertainty quantification and for using image data as quantitative evidence. These probabilistic capabilities are important for the rigorous interpretation of experimental results and for robust interfacing of quantitative imaging pipelines with scientific and decision-making processes. However, are the probabilities delivered by existing Bayesian imaging methods meaningful under replication of an experiment, or are they only meaningful as subjective measures of belief? This paper presents a Monte Carlo method to explore this question. We then leverage the proposed Monte Carlo method and run a large experiment requiring 1,000 GPU-hours to probe the accuracy of five canonical Bayesian imaging methods that are representative of some of the main Bayesian imaging strategies from the past decades (a score-based denoising diffusion technique, a plug-and-play Langevin algorithm utilising a Lipschitz-regularised DnCNN denoiser, a Bayesian method with a dictionary-based prior trained subject to a log-concavity constraint, an empirical Bayesian method with a total-variation prior, and a hierarchical Bayesian Gibbs sampler based on a Gaussian Markov random field model). We find that, a few cases, the probabilities reported by modern Bayesian imaging techniques are in broad agreement with long-term averages as observed over a large number of replication of an experiment, but existing Bayesian imaging methods are generally not able to deliver reliable uncertainty quantification results.","sentences":["Bayesian statistics is a cornerstone of imaging sciences, underpinning many and varied approaches from Markov random fields to score-based denoising diffusion models.","In addition to powerful image estimation methods, the Bayesian paradigm also provides a framework for uncertainty quantification and for using image data as quantitative evidence.","These probabilistic capabilities are important for the rigorous interpretation of experimental results and for robust interfacing of quantitative imaging pipelines with scientific and decision-making processes.","However, are the probabilities delivered by existing Bayesian imaging methods meaningful under replication of an experiment, or are they only meaningful as subjective measures of belief?","This paper presents a Monte Carlo method to explore this question.","We then leverage the proposed Monte Carlo method and run a large experiment requiring 1,000 GPU-hours to probe the accuracy of five canonical Bayesian imaging methods that are representative of some of the main Bayesian imaging strategies from the past decades (a score-based denoising diffusion technique, a plug-and-play Langevin algorithm utilising a Lipschitz-regularised DnCNN denoiser, a Bayesian method with a dictionary-based prior trained subject to a log-concavity constraint, an empirical Bayesian method with a total-variation prior, and a hierarchical Bayesian Gibbs sampler based on a Gaussian Markov random field model).","We find that, a few cases, the probabilities reported by modern Bayesian imaging techniques are in broad agreement with long-term averages as observed over a large number of replication of an experiment, but existing Bayesian imaging methods are generally not able to deliver reliable uncertainty quantification results."],"url":"http://arxiv.org/abs/2405.08179v1","category":"eess.IV"}
{"created":"2024-05-13 20:51:23","title":"Comparative Analysis of AWS Model Deployment Services","abstract":"Amazon Web Services (AWS) offers three important Model Deployment Services for model developers: SageMaker, Lambda, and Elastic Container Service (ECS). These services have critical advantages and disadvantages, influencing model developer's adoption decisions. This comparative analysis reviews the merits and drawbacks of these services. This analysis found that Lambda AWS service leads in efficiency, autoscaling aspects, and integration during model development. However, ECS was found to be outstanding in terms of flexibility, scalability, and infrastructure control; conversely, ECS is better suited when it comes to managing complex container environments during model development, as well as addressing budget concerns -- it is, therefore, the preferred option for model developers whose objective is to achieve complete freedom and framework flexibility with horizontal scaling. ECS is better suited to ensuring performance requirements align with project goals and constraints. The AWS service selection process considered factors that include but are not limited to load balance and cost-effectiveness. ECS is a better choice when model development begins from the abstract. It offers unique benefits, such as the ability to scale horizontally and vertically, making it the best preferable tool for model deployment.","sentences":["Amazon Web Services (AWS) offers three important Model Deployment Services for model developers: SageMaker, Lambda, and Elastic Container Service (ECS).","These services have critical advantages and disadvantages, influencing model developer's adoption decisions.","This comparative analysis reviews the merits and drawbacks of these services.","This analysis found that Lambda AWS service leads in efficiency, autoscaling aspects, and integration during model development.","However, ECS was found to be outstanding in terms of flexibility, scalability, and infrastructure control; conversely, ECS is better suited when it comes to managing complex container environments during model development, as well as addressing budget concerns -- it is, therefore, the preferred option for model developers whose objective is to achieve complete freedom and framework flexibility with horizontal scaling.","ECS is better suited to ensuring performance requirements align with project goals and constraints.","The AWS service selection process considered factors that include but are not limited to load balance and cost-effectiveness.","ECS is a better choice when model development begins from the abstract.","It offers unique benefits, such as the ability to scale horizontally and vertically, making it the best preferable tool for model deployment."],"url":"http://arxiv.org/abs/2405.08175v1","category":"cs.SE"}
{"created":"2024-05-13 20:36:04","title":"Finite-valued Streaming String Transducers","abstract":"A transducer is finite-valued if for some bound k, it maps any given input to at most k outputs. For classical, one-way transducers, it is known since the 80s that finite valuedness entails decidability of the equivalence problem. This decidability result is in contrast to the general case, which makes finite-valued transducers very attractive. For classical transducers, it is also known that finite valuedness is decidable and that any k-valued finite transducer can be decomposed as a union of k single-valued finite transducers.   In this paper, we extend the above results to copyless streaming string transducers (SSTs), answering questions raised by Alur and Deshmukh in 2011. SSTs strictly extend the expressiveness of one-way transducers via additional variables that store partial outputs. We prove that any k-valued SST can be effectively decomposed as a union of k (single-valued) deterministic SSTs. As a corollary, we obtain equivalence of SSTs and two-way transducers in the finite-valued case (those two models are incomparable in general). Another corollary is an elementary upper bound for checking equivalence of finite-valued SSTs. The latter problem was already known to be decidable, but the proof complexity was unknown (it relied on Ehrenfeucht's conjecture). Finally, our main result is that finite valuedness of SSTs is decidable. The complexity is PSpace, and even PTime when the number of variables is fixed.","sentences":["A transducer is finite-valued if for some bound k, it maps any given input to at most k outputs.","For classical, one-way transducers, it is known since the 80s that finite valuedness entails decidability of the equivalence problem.","This decidability result is in contrast to the general case, which makes finite-valued transducers very attractive.","For classical transducers, it is also known that finite valuedness is decidable and that any k-valued finite transducer can be decomposed as a union of k single-valued finite transducers.   ","In this paper, we extend the above results to copyless streaming string transducers (SSTs), answering questions raised by Alur and Deshmukh in 2011.","SSTs strictly extend the expressiveness of one-way transducers via additional variables that store partial outputs.","We prove that any k-valued SST can be effectively decomposed as a union of k (single-valued) deterministic SSTs.","As a corollary, we obtain equivalence of SSTs and two-way transducers in the finite-valued case (those two models are incomparable in general).","Another corollary is an elementary upper bound for checking equivalence of finite-valued SSTs.","The latter problem was already known to be decidable, but the proof complexity was unknown (it relied on Ehrenfeucht's conjecture).","Finally, our main result is that finite valuedness of SSTs is decidable.","The complexity is PSpace, and even PTime when the number of variables is fixed."],"url":"http://arxiv.org/abs/2405.08171v1","category":"cs.FL"}
{"created":"2024-05-13 20:10:57","title":"Durability of MgO/hydromagnesite mortars -- Resistance to chlorides and corrosion","abstract":"The durability of MgO/hydromagnesite mortars was studied with respect to their corrosion performance and resistance to chloride attack and moisture. MgO/hydromagnesite pastes were cured in chloride solution to induce potential formation of Mg-chlorides; however, no such phases were observed. Rapid chloride ingress measurements demonstrated high penetration resistance and low chloride migration coefficients, i.e. D_Cl = 1e-13 to 1e-12 m^2/s. The corrosion rate of carbon steel embedded in MgO/HY mortars, as determined by linear polarization resistance measurements, was in the range icorr = 1e-9 A/cm^2 in dry and 1e-7 A/cm^2 in wet conditions, irrespective of the mortar composition or curing condition.These findings corroborate the hypothesis that, in the absence of chlorides, the moisture condition is the primary predictor of corrosion rate of carbon steel in the MgO/hydromagnesite binder. These accelerated, short-term experiments suggest that the binder may be suited to protect embedded carbon steel from corrosion under specific exposure conditions of practical relevance.","sentences":["The durability of MgO/hydromagnesite mortars was studied with respect to their corrosion performance and resistance to chloride attack and moisture.","MgO/hydromagnesite pastes were cured in chloride solution to induce potential formation of Mg-chlorides; however, no such phases were observed.","Rapid chloride ingress measurements demonstrated high penetration resistance and low chloride migration coefficients, i.e. D_Cl = 1e-13 to 1e-12 m^2/s.","The corrosion rate of carbon steel embedded in MgO/HY mortars, as determined by linear polarization resistance measurements, was in the range icorr = 1e-9 A/cm^2 in dry and 1e-7 A/cm^2 in wet conditions, irrespective of the mortar composition or curing condition.","These findings corroborate the hypothesis that, in the absence of chlorides, the moisture condition is the primary predictor of corrosion rate of carbon steel in the MgO/hydromagnesite binder.","These accelerated, short-term experiments suggest that the binder may be suited to protect embedded carbon steel from corrosion under specific exposure conditions of practical relevance."],"url":"http://arxiv.org/abs/2405.08164v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 19:52:05","title":"Coupling renormalization flow in the strongly interacting regime of an asymptotically free quantum field theory in four dimensions","abstract":"We consider a scalar quantum field theory with global $O(N)^3$ symmetry in four Euclidean dimensions and solve it numerically in closed form in the large-N limit. For imaginary tetrahedral coupling the theory is asymptotically free, with stable and real quantum effective action. We demonstrate the dynamical build-up of a strong interaction as the correlation length increases in a regime where the coupling renormalization flow remains well-defined in the infrared. This is in contrast to perturbative results of asymptotically free theories, which predict that the coupling becomes ill-defined in the infrared, like in quantum chromodynamics. These properties make the model an important laboratory for the study of strong-coupling phenomena in quantum field theory from first principles.","sentences":["We consider a scalar quantum field theory with global $O(N)^3$ symmetry in four Euclidean dimensions and solve it numerically in closed form in the large-N limit.","For imaginary tetrahedral coupling the theory is asymptotically free, with stable and real quantum effective action.","We demonstrate the dynamical build-up of a strong interaction as the correlation length increases in a regime where the coupling renormalization flow remains well-defined in the infrared.","This is in contrast to perturbative results of asymptotically free theories, which predict that the coupling becomes ill-defined in the infrared, like in quantum chromodynamics.","These properties make the model an important laboratory for the study of strong-coupling phenomena in quantum field theory from first principles."],"url":"http://arxiv.org/abs/2405.08153v1","category":"hep-th"}
{"created":"2024-05-13 19:51:26","title":"From Entanglement to Universality: A Multiparticle Spacetime Algebra Approach to Quantum Computational Gates Revisited","abstract":"Alternative mathematical explorations in quantum computing can be of great scientific interest, especially if they come with penetrating physical insights. In this paper, we present a critical revisitation of our geometric (Clifford) algebras (GAs) application in quantum computing as originally presented in [C. Cafaro and S. Mancini, Adv. Appl. Clifford Algebras 21, 493 (2011)]. Our focus is on testing the usefulness of geometric algebras (GAs) techniques in two applications to quantum computing. First, making use of the geometric algebra of a relativistic configuration space (a.k.a., multiparticle spacetime algebra or MSTA), we offer an explicit algebraic characterization of one- and two-qubit quantum states together with a MSTA description of one- and two-qubit quantum computational gates. In this first application, we devote special attention to the concept of entanglement, focusing on entangled quantum states and two-qubit entangling quantum gates. Second, exploiting the previously mentioned MSTA characterization together with the GA depiction of the Lie algebras SO(3;R) and SU(2;C) depending on the rotor group formalism, we focus our attention to the concept of universality in quantum computing by reevaluating Boykin's proof on the identification of a suitable set of universal quantum gates. At the end of our mathematical exploration, we arrive at two main conclusions. Firstly, the MSTA perspective leads to a powerful conceptual unification between quantum states and quantum operators. More specifically, the complex qubit space and the complex space of unitary operators acting on them merge in a single multivectorial real space. Secondly, the GA viewpoint on rotations based on the rotor group carries both conceptual and computational upper hands compared to conventional vectorial and matricial methods.","sentences":["Alternative mathematical explorations in quantum computing can be of great scientific interest, especially if they come with penetrating physical insights.","In this paper, we present a critical revisitation of our geometric (Clifford) algebras (GAs) application in quantum computing as originally presented in [C. Cafaro and S. Mancini, Adv.","Appl.","Clifford Algebras 21, 493 (2011)].","Our focus is on testing the usefulness of geometric algebras (GAs) techniques in two applications to quantum computing.","First, making use of the geometric algebra of a relativistic configuration space (a.k.a., multiparticle spacetime algebra or MSTA), we offer an explicit algebraic characterization of one- and two-qubit quantum states together with a MSTA description of one- and two-qubit quantum computational gates.","In this first application, we devote special attention to the concept of entanglement, focusing on entangled quantum states and two-qubit entangling quantum gates.","Second, exploiting the previously mentioned MSTA characterization together with the GA depiction of the Lie algebras SO(3;R) and SU(2;C) depending on the rotor group formalism, we focus our attention to the concept of universality in quantum computing by reevaluating Boykin's proof on the identification of a suitable set of universal quantum gates.","At the end of our mathematical exploration, we arrive at two main conclusions.","Firstly, the MSTA perspective leads to a powerful conceptual unification between quantum states and quantum operators.","More specifically, the complex qubit space and the complex space of unitary operators acting on them merge in a single multivectorial real space.","Secondly, the GA viewpoint on rotations based on the rotor group carries both conceptual and computational upper hands compared to conventional vectorial and matricial methods."],"url":"http://arxiv.org/abs/2405.08152v1","category":"quant-ph"}
{"created":"2024-05-13 19:51:20","title":"Benchmarking Retrieval-Augmented Large Language Models in Biomedical NLP: Application, Robustness, and Self-Awareness","abstract":"Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks. However, LLM is sensitive to the selection of demonstrations. To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database. Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks. This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain. Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain. However, such knowledge is common in the real world. Finally, exploring the self-awareness ability is also crucial for the RAL system. So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference). We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness. To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities. Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets.","sentences":["Large language models (LLM) have demonstrated remarkable capabilities in various biomedical natural language processing (NLP) tasks, leveraging the demonstration within the input context to adapt to new tasks.","However, LLM is sensitive to the selection of demonstrations.","To address the hallucination issue inherent in LLM, retrieval-augmented LLM (RAL) offers a solution by retrieving pertinent information from an established database.","Nonetheless, existing research work lacks rigorous evaluation of the impact of retrieval-augmented large language models on different biomedical NLP tasks.","This deficiency makes it challenging to ascertain the capabilities of RAL within the biomedical domain.","Moreover, the outputs from RAL are affected by retrieving the unlabeled, counterfactual, or diverse knowledge that is not well studied in the biomedical domain.","However, such knowledge is common in the real world.","Finally, exploring the self-awareness ability is also crucial for the RAL system.","So, in this paper, we systematically investigate the impact of RALs on 5 different biomedical tasks (triple extraction, link prediction, classification, question answering, and natural language inference).","We analyze the performance of RALs in four fundamental abilities, including unlabeled robustness, counterfactual robustness, diverse robustness, and negative awareness.","To this end, we proposed an evaluation framework to assess the RALs' performance on different biomedical NLP tasks and establish four different testbeds based on the aforementioned fundamental abilities.","Then, we evaluate 3 representative LLMs with 3 different retrievers on 5 tasks over 9 datasets."],"url":"http://arxiv.org/abs/2405.08151v1","category":"cs.CL"}
{"created":"2024-05-13 19:48:13","title":"A Deep Learning Approach For Epistemic Uncertainty Quantification Of Turbulent Flow Simulations","abstract":"Simulations of complex turbulent flow are part and parcel of the engineering design process. Eddy viscosity based turbulence models represent the workhorse for these simulations. The underlying simplifications in eddy viscosity models make them computationally inexpensive but also introduce structural uncertainties in their predictions. Currently the Eigenspace Perturbation Method is the only approach to predict these uncertainties. Due to its purely physics based nature this method often leads to unrealistically large uncertainty bounds that lead to exceedingly conservative designs. We use a Deep Learning based approach to address this issue. We control the perturbations using trained deep learning models that predict how much to perturb the modeled Reynolds stresses. This is executed using a Convolutional Neural Network that learns the difference between eddy viscosity based model predictions and high fidelity data as a mapping of flow features. We show that this approach leads to improvements over the Eigenspace Perturbation Method.","sentences":["Simulations of complex turbulent flow are part and parcel of the engineering design process.","Eddy viscosity based turbulence models represent the workhorse for these simulations.","The underlying simplifications in eddy viscosity models make them computationally inexpensive but also introduce structural uncertainties in their predictions.","Currently the Eigenspace Perturbation Method is the only approach to predict these uncertainties.","Due to its purely physics based nature this method often leads to unrealistically large uncertainty bounds that lead to exceedingly conservative designs.","We use a Deep Learning based approach to address this issue.","We control the perturbations using trained deep learning models that predict how much to perturb the modeled Reynolds stresses.","This is executed using a Convolutional Neural Network that learns the difference between eddy viscosity based model predictions and high fidelity data as a mapping of flow features.","We show that this approach leads to improvements over the Eigenspace Perturbation Method."],"url":"http://arxiv.org/abs/2405.08148v1","category":"physics.flu-dyn"}
{"created":"2024-05-13 19:42:42","title":"Constructions of Optimal-Speed Quantum Evolutions: A Comparative Study","abstract":"We present a comparative analysis of two different constructions of optimal-speed quantum Hamiltonian evolutions on the Bloch sphere. In the first approach (Mostafazadeh's approach), the evolution is specified by a traceless stationary Hermitian Hamiltonian and occurs between two arbitrary qubit states by maximizing the energy uncertainty. In the second approach (Bender's approach), instead, the evolution is characterized by a stationary Hermitian Hamiltonian which is not traceless and occurs between an initial qubit on the north pole and an arbitrary final qubit. In this second approach, the evolution occurs by minimizing the evolution time subject to the constraint that the difference between the largest and the smallest eigenvalues of the Hamiltonian is kept fixed. For both approaches we calculate explicitly the optimal Hamiltonian, the optimal unitary evolution operator and, finally, the optimal magnetic field configuration. Furthermore, we show in a clear way that Mostafazadeh's and Bender's approaches are equivalent when we extend Mostafazadeh's approach to Hamiltonians with nonzero trace and, at the same time, focus on an initial quantum state placed on the north pole of the Bloch sphere. Finally, we demonstrate in both scenarios that the optimal unitary evolution operator is a rotation about an axis that is orthogonal to the unit Bloch vectors that correspond to the initial and final qubit states.","sentences":["We present a comparative analysis of two different constructions of optimal-speed quantum Hamiltonian evolutions on the Bloch sphere.","In the first approach (Mostafazadeh's approach), the evolution is specified by a traceless stationary Hermitian Hamiltonian and occurs between two arbitrary qubit states by maximizing the energy uncertainty.","In the second approach (Bender's approach), instead, the evolution is characterized by a stationary Hermitian Hamiltonian which is not traceless and occurs between an initial qubit on the north pole and an arbitrary final qubit.","In this second approach, the evolution occurs by minimizing the evolution time subject to the constraint that the difference between the largest and the smallest eigenvalues of the Hamiltonian is kept fixed.","For both approaches we calculate explicitly the optimal Hamiltonian, the optimal unitary evolution operator and, finally, the optimal magnetic field configuration.","Furthermore, we show in a clear way that Mostafazadeh's and Bender's approaches are equivalent when we extend Mostafazadeh's approach to Hamiltonians with nonzero trace and, at the same time, focus on an initial quantum state placed on the north pole of the Bloch sphere.","Finally, we demonstrate in both scenarios that the optimal unitary evolution operator is a rotation about an axis that is orthogonal to the unit Bloch vectors that correspond to the initial and final qubit states."],"url":"http://arxiv.org/abs/2405.08144v1","category":"quant-ph"}
{"created":"2024-05-13 19:32:58","title":"A logical qubit-design with geometrically tunable error-resistibility","abstract":"Breaking the error-threshold would mark a milestone in establishing quantum advantage for a wide range of relevant problems. One possible route is to encode information redundantly in a logical qubit by combining several noisy qubits, providing an increased robustness against external perturbations. We propose a setup for a logical qubit built from superconducting qubits (SCQs) coupled to a microwave cavity-mode. Our design is based on a recently discovered geometric stabilizing mechanism in the Bose-Hubbard wheel (BHW), which manifests as energetically well-separated clusters of many-body eigenstates. We investigate the impact of experimentally relevant perturbations between SCQs and the cavity on the spectral properties of the BHW. We show that even in the presence of typical fabrication uncertainties, the occurrence and separation of clustered many-body eigenstates is extremely robust. Introducing an additional, frequency-detuned SCQ coupled to the cavity yields duplicates of these clusters, that can be split up by an on-site potential. We show that this allows to (i) redundantly encode two logical qubit states that can be switched and read out efficiently and (ii) can be separated from the remaining many-body spectrum via geometric stabilization. We demonstrate at the example of an X-gate that the proposed logical qubit reaches single qubit-gate fidelities $>0.999$ in experimentally feasible temperature regimes $\\sim10-20\\,\\mathrm{mK}$.","sentences":["Breaking the error-threshold would mark a milestone in establishing quantum advantage for a wide range of relevant problems.","One possible route is to encode information redundantly in a logical qubit by combining several noisy qubits, providing an increased robustness against external perturbations.","We propose a setup for a logical qubit built from superconducting qubits (SCQs) coupled to a microwave cavity-mode.","Our design is based on a recently discovered geometric stabilizing mechanism in the Bose-Hubbard wheel (BHW), which manifests as energetically well-separated clusters of many-body eigenstates.","We investigate the impact of experimentally relevant perturbations between SCQs and the cavity on the spectral properties of the BHW.","We show that even in the presence of typical fabrication uncertainties, the occurrence and separation of clustered many-body eigenstates is extremely robust.","Introducing an additional, frequency-detuned SCQ coupled to the cavity yields duplicates of these clusters, that can be split up by an on-site potential.","We show that this allows to (i) redundantly encode two logical qubit states that can be switched and read out efficiently and (ii) can be separated from the remaining many-body spectrum via geometric stabilization.","We demonstrate at the example of an X-gate that the proposed logical qubit reaches single qubit-gate fidelities $>0.999$ in experimentally feasible temperature regimes $\\sim10-20\\,\\mathrm{mK}$."],"url":"http://arxiv.org/abs/2405.08138v1","category":"quant-ph"}
{"created":"2024-05-13 19:24:59","title":"An Optimal Multilevel Quorum System for Probabilistic Consensus","abstract":"We present the notion of a multilevel, slashable quorum system, where an application can obtain gradual levels of assurance that a certain value is bound to be decided (or \"finalized\") in a global consensus procedure, unless a large number of Byzantine processes are exposed to slashing (that is, penalty on staked assets). Our construction is a highly parameterized generalization of quorum systems based on finite projective spaces, with asymptotic high availability and optimal slashing properties. In particular, we show that any quorum system whose ground elements are disjoint subsets of nodes (e.g. \"commmittees\" in committee-based consensus protocols) has asymptotic high availability under very reasonable conditions, a general proof with significance of its own. Under similarly relaxed conditions, we show that our construction has asymptotically optimal slashing properties with respect to message complexity and process load; this illustrates a fundamental trade off between message complexity, load, and slashing. Our multilevel construction allows nodes to decide how many \"levels\" of finalization assurance they wish to obtain, noting that this functionality, if applied to a proof-of-stake blockchain, can be seen either as (i) a form of an early, slashing-based, probabilistic block finalization; or (ii) a service for reorg tolerance.","sentences":["We present the notion of a multilevel, slashable quorum system, where an application can obtain gradual levels of assurance that a certain value is bound to be decided (or \"finalized\") in a global consensus procedure, unless a large number of Byzantine processes are exposed to slashing (that is, penalty on staked assets).","Our construction is a highly parameterized generalization of quorum systems based on finite projective spaces, with asymptotic high availability and optimal slashing properties.","In particular, we show that any quorum system whose ground elements are disjoint subsets of nodes (e.g. \"commmittees\" in committee-based consensus protocols) has asymptotic high availability under very reasonable conditions, a general proof with significance of its own.","Under similarly relaxed conditions, we show that our construction has asymptotically optimal slashing properties with respect to message complexity and process load; this illustrates a fundamental trade off between message complexity, load, and slashing.","Our multilevel construction allows nodes to decide how many \"levels\" of finalization assurance they wish to obtain, noting that this functionality, if applied to a proof-of-stake blockchain, can be seen either as (i) a form of an early, slashing-based, probabilistic block finalization; or (ii) a service for reorg tolerance."],"url":"http://arxiv.org/abs/2405.08135v1","category":"cs.DC"}
{"created":"2024-05-13 19:18:00","title":"Ultra-Wideband Tapered Transducers in Thin-Film Lithium Niobate on Silicon Carbide","abstract":"Acoustic devices offer significant advantages in size and loss, making them ubiquitous for mobile radio frequency signal processing. However, the usable bandwidth is often limited to the achievable electromechanical coupling, setting a hard limit using typical transducer designs. In this work, we present an ultra-wideband transducer design utilizing a tapered electrode configuration to overcome this limitation. The design is realized on a lithium niobate (LN) on silicon carbide platform, utilizing a combination of first and higher order shear-horizontal modes to generate the ultra-wideband response. The implementation shows a fractional bandwidth (FBW) of 55% at 2.23 GHz with an associated insertion loss (IL) of 26 dB for the measured 50 ohm case. Upon improved impedance matching, this performance could be improved to 79% FBW and an IL of 16.5 dB. Upon further development, this ultra-wideband design could be reasonably scaled towards improved FBW and IL trade off to enable improved usability for cases where bandwidth should be prioritized.","sentences":["Acoustic devices offer significant advantages in size and loss, making them ubiquitous for mobile radio frequency signal processing.","However, the usable bandwidth is often limited to the achievable electromechanical coupling, setting a hard limit using typical transducer designs.","In this work, we present an ultra-wideband transducer design utilizing a tapered electrode configuration to overcome this limitation.","The design is realized on a lithium niobate (LN) on silicon carbide platform, utilizing a combination of first and higher order shear-horizontal modes to generate the ultra-wideband response.","The implementation shows a fractional bandwidth (FBW) of 55% at 2.23 GHz with an associated insertion loss (IL) of 26 dB for the measured 50 ohm case.","Upon improved impedance matching, this performance could be improved to 79% FBW and an IL of 16.5 dB. Upon further development, this ultra-wideband design could be reasonably scaled towards improved FBW and IL trade off to enable improved usability for cases where bandwidth should be prioritized."],"url":"http://arxiv.org/abs/2405.08132v1","category":"physics.app-ph"}
{"created":"2024-05-13 19:06:12","title":"Modeling sea ice in the marginal ice zone as a dense granular flow with rheology inferred from a discrete element model","abstract":"The marginal ice zone (MIZ) represents the periphery of the sea ice cover. Here, the macroscale behavior of the sea ice results from collisions and enduring contact between ice floes. This configuration closely resembles that of dense granular flows, which have been modeled successfully with the $\\mu(I)$ rheology. Here, we present a continuous model based on the $\\mu(I)$ rheology which treats sea ice as a compressible fluid, with the local sea ice concentration given by a dilatancy function $\\Phi(I)$. We infer expressions for $\\mu(I)$ and $\\Phi(I)$ from a discrete element method (DEM) which considers polygonal-shaped ice floes. We do this by driving the sea ice with a one-dimensional shearing ocean current. The resulting continuous model is a nonlinear system of equations with the sea ice velocity, local concentration, and pressure as unknowns. The rheology is given by the sum of a plastic and a viscous term. In the context of a periodic patch of ocean, which is effectively a one dimensional problem, and under steady conditions, we prove this system to be well-posed, present a numerical algorithm for solving it, and compare its solutions to those of the DEM. These comparisons demonstrate the continuous model's ability to capture most of the DEM's results accurately. The continuous model is particularly accurate for ocean currents faster than 0.25 m/s; however, for low concentrations and slow ocean currents, the continuous model is less effective in capturing the DEM results. In the latter case, the lack of accuracy of the continuous model is found to be accompanied by the breakdown of a balance between the average shear stress and the integrated ocean drag extracted from the DEM.","sentences":["The marginal ice zone (MIZ) represents the periphery of the sea ice cover.","Here, the macroscale behavior of the sea ice results from collisions and enduring contact between ice floes.","This configuration closely resembles that of dense granular flows, which have been modeled successfully with the $\\mu(I)$ rheology.","Here, we present a continuous model based on the $\\mu(I)$ rheology which treats sea ice as a compressible fluid, with the local sea ice concentration given by a dilatancy function $\\Phi(I)$.","We infer expressions for $\\mu(I)$ and $\\Phi(I)$ from a discrete element method (DEM) which considers polygonal-shaped ice floes.","We do this by driving the sea ice with a one-dimensional shearing ocean current.","The resulting continuous model is a nonlinear system of equations with the sea ice velocity, local concentration, and pressure as unknowns.","The rheology is given by the sum of a plastic and a viscous term.","In the context of a periodic patch of ocean, which is effectively a one dimensional problem, and under steady conditions, we prove this system to be well-posed, present a numerical algorithm for solving it, and compare its solutions to those of the DEM.","These comparisons demonstrate the continuous model's ability to capture most of the DEM's results accurately.","The continuous model is particularly accurate for ocean currents faster than 0.25 m/s; however, for low concentrations and slow ocean currents, the continuous model is less effective in capturing the DEM results.","In the latter case, the lack of accuracy of the continuous model is found to be accompanied by the breakdown of a balance between the average shear stress and the integrated ocean drag extracted from the DEM."],"url":"http://arxiv.org/abs/2405.08123v1","category":"physics.flu-dyn"}
{"created":"2024-05-13 19:05:48","title":"Equivariant Deep Learning of Mixed-Integer Optimal Control Solutions for Vehicle Decision Making and Motion Planning","abstract":"Mixed-integer quadratic programs (MIQPs) are a versatile way of formulating vehicle decision making and motion planning problems, where the prediction model is a hybrid dynamical system that involves both discrete and continuous decision variables. However, even the most advanced MIQP solvers can hardly account for the challenging requirements of automotive embedded platforms. Thus, we use machine learning to simplify and hence speed up optimization. Our work builds on recent ideas for solving MIQPs in real-time by training a neural network to predict the optimal values of integer variables and solving the remaining problem by online quadratic programming. Specifically, we propose a recurrent permutation equivariant deep set that is particularly suited for imitating MIQPs that involve many obstacles, which is often the major source of computational burden in motion planning problems. Our framework comprises also a feasibility projector that corrects infeasible predictions of integer variables and considerably increases the likelihood of computing a collision-free trajectory. We evaluate the performance, safety and real-time feasibility of decision-making for autonomous driving using the proposed approach on realistic multi-lane traffic scenarios with interactive agents in SUMO simulations.","sentences":["Mixed-integer quadratic programs (MIQPs) are a versatile way of formulating vehicle decision making and motion planning problems, where the prediction model is a hybrid dynamical system that involves both discrete and continuous decision variables.","However, even the most advanced MIQP solvers can hardly account for the challenging requirements of automotive embedded platforms.","Thus, we use machine learning to simplify and hence speed up optimization.","Our work builds on recent ideas for solving MIQPs in real-time by training a neural network to predict the optimal values of integer variables and solving the remaining problem by online quadratic programming.","Specifically, we propose a recurrent permutation equivariant deep set that is particularly suited for imitating MIQPs that involve many obstacles, which is often the major source of computational burden in motion planning problems.","Our framework comprises also a feasibility projector that corrects infeasible predictions of integer variables and considerably increases the likelihood of computing a collision-free trajectory.","We evaluate the performance, safety and real-time feasibility of decision-making for autonomous driving using the proposed approach on realistic multi-lane traffic scenarios with interactive agents in SUMO simulations."],"url":"http://arxiv.org/abs/2405.08122v1","category":"cs.RO"}
{"created":"2024-05-13 19:05:36","title":"GPS-IMU Sensor Fusion for Reliable Autonomous Vehicle Position Estimation","abstract":"Global Positioning System (GPS) navigation provides accurate positioning with global coverage, making it a reliable option in open areas with unobstructed sky views. However, signal degradation may occur in indoor spaces and urban canyons. In contrast, Inertial Measurement Units (IMUs) consist of gyroscopes and accelerometers that offer relative motion information such as acceleration and rotational changes. Unlike GPS, IMUs do not rely on external signals, making them useful in GPS-denied environments. Nonetheless, IMUs suffer from drift over time due to the accumulation of errors while integrating acceleration to determine velocity and position. Therefore, fusing the GPS and IMU is crucial for enhancing the reliability and precision of navigation systems in autonomous vehicles, especially in environments where GPS signals are compromised. To ensure smooth navigation and overcome the limitations of each sensor, the proposed method fuses GPS and IMU data. This sensor fusion uses the Unscented Kalman Filter (UKF) Bayesian filtering technique. The proposed navigation system is designed to be robust, delivering continuous and accurate positioning critical for the safe operation of autonomous vehicles, particularly in GPS-denied environments. This project uses KITTI GNSS and IMU datasets for experimental validation, showing that the GNSS-IMU fusion technique reduces GNSS-only data's RMSE. The RMSE decreased from 13.214, 13.284, and 13.363 to 4.271, 5.275, and 0.224 for the x-axis, y-axis, and z-axis, respectively. The experimental result using UKF shows promising direction in improving autonomous vehicle navigation using GPS and IMU sensor fusion using the best of two sensors in GPS-denied environments.","sentences":["Global Positioning System (GPS) navigation provides accurate positioning with global coverage, making it a reliable option in open areas with unobstructed sky views.","However, signal degradation may occur in indoor spaces and urban canyons.","In contrast, Inertial Measurement Units (IMUs) consist of gyroscopes and accelerometers that offer relative motion information such as acceleration and rotational changes.","Unlike GPS, IMUs do not rely on external signals, making them useful in GPS-denied environments.","Nonetheless, IMUs suffer from drift over time due to the accumulation of errors while integrating acceleration to determine velocity and position.","Therefore, fusing the GPS and IMU is crucial for enhancing the reliability and precision of navigation systems in autonomous vehicles, especially in environments where GPS signals are compromised.","To ensure smooth navigation and overcome the limitations of each sensor, the proposed method fuses GPS and IMU data.","This sensor fusion uses the Unscented Kalman Filter (UKF) Bayesian filtering technique.","The proposed navigation system is designed to be robust, delivering continuous and accurate positioning critical for the safe operation of autonomous vehicles, particularly in GPS-denied environments.","This project uses KITTI GNSS and IMU datasets for experimental validation, showing that the GNSS-IMU fusion technique reduces GNSS-only data's RMSE.","The RMSE decreased from 13.214, 13.284, and 13.363 to 4.271, 5.275, and 0.224 for the x-axis, y-axis, and z-axis, respectively.","The experimental result using UKF shows promising direction in improving autonomous vehicle navigation using GPS and IMU sensor fusion using the best of two sensors in GPS-denied environments."],"url":"http://arxiv.org/abs/2405.08119v1","category":"eess.SY"}
{"created":"2024-05-13 18:45:37","title":"On the affine permutation group of certain decreasing Cartesian codes","abstract":"A decreasing Cartesian code is defined by evaluating a monomial set closed under divisibility on a Cartesian set. Some well-known examples are the Reed-Solomon, Reed-Muller, and (some) toric codes. The affine permutations consist of the permutations of the code that depend on an affine transformation. In this work, we study the affine permutations of some decreasing Cartesian codes, including the case when the Cartesian set has copies of multiplicative or additive subgroups.","sentences":["A decreasing Cartesian code is defined by evaluating a monomial set closed under divisibility on a Cartesian set.","Some well-known examples are the Reed-Solomon, Reed-Muller, and (some) toric codes.","The affine permutations consist of the permutations of the code that depend on an affine transformation.","In this work, we study the affine permutations of some decreasing Cartesian codes, including the case when the Cartesian set has copies of multiplicative or additive subgroups."],"url":"http://arxiv.org/abs/2405.08112v1","category":"math.CO"}
{"created":"2024-05-13 18:45:25","title":"Conformalized Physics-Informed Neural Networks","abstract":"Physics-informed neural networks (PINNs) are an influential method of solving differential equations and estimating their parameters given data. However, since they make use of neural networks, they provide only a point estimate of differential equation parameters, as well as the solution at any given point, without any measure of uncertainty. Ensemble and Bayesian methods have been previously applied to quantify the uncertainty of PINNs, but these methods may require making strong assumptions on the data-generating process, and can be computationally expensive. Here, we introduce Conformalized PINNs (C-PINNs) that, without making any additional assumptions, utilize the framework of conformal prediction to quantify the uncertainty of PINNs by providing intervals that have finite-sample, distribution-free statistical validity.","sentences":["Physics-informed neural networks (PINNs) are an influential method of solving differential equations and estimating their parameters given data.","However, since they make use of neural networks, they provide only a point estimate of differential equation parameters, as well as the solution at any given point, without any measure of uncertainty.","Ensemble and Bayesian methods have been previously applied to quantify the uncertainty of PINNs, but these methods may require making strong assumptions on the data-generating process, and can be computationally expensive.","Here, we introduce Conformalized PINNs (C-PINNs) that, without making any additional assumptions, utilize the framework of conformal prediction to quantify the uncertainty of PINNs by providing intervals that have finite-sample, distribution-free statistical validity."],"url":"http://arxiv.org/abs/2405.08111v1","category":"cs.LG"}
{"created":"2024-05-13 18:24:03","title":"Learning functions on symmetric matrices and point clouds via lightweight invariant features","abstract":"In this work, we present a mathematical formulation for machine learning of (1) functions on symmetric matrices that are invariant with respect to the action of permutations by conjugation, and (2) functions on point clouds that are invariant with respect to rotations, reflections, and permutations of the points. To achieve this, we construct $O(n^2)$ invariant features derived from generators for the field of rational functions on $n\\times n$ symmetric matrices that are invariant under joint permutations of rows and columns. We show that these invariant features can separate all distinct orbits of symmetric matrices except for a measure zero set; such features can be used to universally approximate invariant functions on almost all weighted graphs. For point clouds in a fixed dimension, we prove that the number of invariant features can be reduced, generically without losing expressivity, to $O(n)$, where $n$ is the number of points. We combine these invariant features with DeepSets to learn functions on symmetric matrices and point clouds with varying sizes. We empirically demonstrate the feasibility of our approach on molecule property regression and point cloud distance prediction.","sentences":["In this work, we present a mathematical formulation for machine learning of (1) functions on symmetric matrices that are invariant with respect to the action of permutations by conjugation, and (2) functions on point clouds that are invariant with respect to rotations, reflections, and permutations of the points.","To achieve this, we construct $O(n^2)$ invariant features derived from generators for the field of rational functions on $n\\times n$ symmetric matrices that are invariant under joint permutations of rows and columns.","We show that these invariant features can separate all distinct orbits of symmetric matrices except for a measure zero set; such features can be used to universally approximate invariant functions on almost all weighted graphs.","For point clouds in a fixed dimension, we prove that the number of invariant features can be reduced, generically without losing expressivity, to $O(n)$, where $n$ is the number of points.","We combine these invariant features with DeepSets to learn functions on symmetric matrices and point clouds with varying sizes.","We empirically demonstrate the feasibility of our approach on molecule property regression and point cloud distance prediction."],"url":"http://arxiv.org/abs/2405.08097v1","category":"cs.LG"}
{"created":"2024-05-13 18:15:53","title":"Global anomalies of Green's function zeros","abstract":"Anomaly analysis has been an important and powerful tool in studying nonperturbative physics for decades. The anomaly inflow mechanism provides an intuitive interpretation of the bulk-boundary correspondence in topological systems. In this work, we study global anomalies in systems with symmetry-preserving Luttinger surfaces, i.e. the manifolds of fermionic Green's function zeros in the momentum space at zero energy, described by nonlocal effective theories. We view the nonlocal effective theories as a result of integrating out some low energy states. Assuming that the states integrated out do not make extra contributions to the anomalies, we discuss the simplest Lagrangian describing a gapless Dirac zero and a two-pole variant, their global anomalies, and the bulk-boundary correspondence. We then consider the constraints on possible phases with Green's function zeros of Dirac type, such as non-Fermi liquids and emergent gapless quasiparticles on Luttinger surfaces. We also provide some perspectives on why the nonlocal fermionic effective theory discussed by Golterman and Shamir (arXiv: 2311.12790) is not a suitable starting point for a symmetrically gapped phase.","sentences":["Anomaly analysis has been an important and powerful tool in studying nonperturbative physics for decades.","The anomaly inflow mechanism provides an intuitive interpretation of the bulk-boundary correspondence in topological systems.","In this work, we study global anomalies in systems with symmetry-preserving Luttinger surfaces, i.e. the manifolds of fermionic Green's function zeros in the momentum space at zero energy, described by nonlocal effective theories.","We view the nonlocal effective theories as a result of integrating out some low energy states.","Assuming that the states integrated out do not make extra contributions to the anomalies, we discuss the simplest Lagrangian describing a gapless Dirac zero and a two-pole variant, their global anomalies, and the bulk-boundary correspondence.","We then consider the constraints on possible phases with Green's function zeros of Dirac type, such as non-Fermi liquids and emergent gapless quasiparticles on Luttinger surfaces.","We also provide some perspectives on why the nonlocal fermionic effective theory discussed by Golterman and Shamir (arXiv: 2311.12790) is not a suitable starting point for a symmetrically gapped phase."],"url":"http://arxiv.org/abs/2405.08093v1","category":"cond-mat.str-el"}
{"created":"2024-05-13 18:14:01","title":"Leptoquark Searches at TeV Scale Using Neural Networks at Hadron Collider","abstract":"Several discrepancies in the decay of B-meson decay have drawn a lot of interest in the leptoquarks (LQ), making them an exciting discovery. The current research aims to discover the pair-production of leptoquarks that links strongly to the third generation of quarks and leptons at the center of mass energy $\\sqrt{s}$=14 TeV, via proton-proton collisions at the Large Hadron Collider (LHC). Based on the lepton-quark coupling parameters and branching fractions, we separated our search into various benchmark points. The leading order (LO) signals and background processes are generated, while parton showering and hadronization is also performed to simulate the detector effects. The Boosted Decision Trees (BDTs), Multilayer Perceptron (MLP), and Likelihood (LH) methods are effective in improving signal-background discrimination compared to traditional cut-based analysis. The results indicate that these machine learning methods can significantly enhance the sensitivity in probing for new physics signals, such as LQs, at two different integrated luminosities. Specifically, the use of BDTs, MLP, and LH has led to higher signal significances and improved signal efficiency in both hadronic and semi-leptonic decay modes. The results suggest that the LQ masses of 500 GeV and 2.0 TeV in fully hadronic decay modes can be accurately probed with signal significance 176.70 (17.6) and 184.27 (0.01) for MVA (cut-based) at 1000 $fb^{-1}$, respectively. Similarly, in semi-leptonic decay mode the signal significance values are 168.56 and 181.89 at lowest and highest selected LQ masses respectively for MVA method only. The enhanced numbers by a factor of 2 are also reported at 3000 $fb^{-1}$.","sentences":["Several discrepancies in the decay of B-meson decay have drawn a lot of interest in the leptoquarks (LQ), making them an exciting discovery.","The current research aims to discover the pair-production of leptoquarks that links strongly to the third generation of quarks and leptons at the center of mass energy $\\sqrt{s}$=14 TeV, via proton-proton collisions at the Large Hadron Collider (LHC).","Based on the lepton-quark coupling parameters and branching fractions, we separated our search into various benchmark points.","The leading order (LO) signals and background processes are generated, while parton showering and hadronization is also performed to simulate the detector effects.","The Boosted Decision Trees (BDTs), Multilayer Perceptron (MLP), and Likelihood (LH) methods are effective in improving signal-background discrimination compared to traditional cut-based analysis.","The results indicate that these machine learning methods can significantly enhance the sensitivity in probing for new physics signals, such as LQs, at two different integrated luminosities.","Specifically, the use of BDTs, MLP, and LH has led to higher signal significances and improved signal efficiency in both hadronic and semi-leptonic decay modes.","The results suggest that the LQ masses of 500 GeV and 2.0 TeV in fully hadronic decay modes can be accurately probed with signal significance 176.70 (17.6) and 184.27 (0.01) for MVA (cut-based) at 1000 $fb^{-1}$, respectively.","Similarly, in semi-leptonic decay mode the signal significance values are 168.56 and 181.89 at lowest and highest selected LQ masses respectively for MVA method only.","The enhanced numbers by a factor of 2 are also reported at 3000","$fb^{-1}$."],"url":"http://arxiv.org/abs/2405.08090v1","category":"hep-ph"}
{"created":"2024-05-13 18:06:04","title":"The Perils of Overreaction","abstract":"In order to study updating rules, we consider the problem of a malevolent principal screening an imperfectly Bayesian agent. We uncover a fundamental dichotomy between underreaction and overreaction to information. If an agent's posterior is farther away from the prior than it should be under Bayes' law, she can always be exploited by the principal to an unfettered degree: the agent's ex ante expected loss can be made arbitrarily large. In stark contrast, an agent who underreacts (whose posterior is closer to the prior than the Bayesian posterior) cannot be exploited at all.","sentences":["In order to study updating rules, we consider the problem of a malevolent principal screening an imperfectly Bayesian agent.","We uncover a fundamental dichotomy between underreaction and overreaction to information.","If an agent's posterior is farther away from the prior than it should be under Bayes' law, she can always be exploited by the principal to an unfettered degree: the agent's ex ante expected loss can be made arbitrarily large.","In stark contrast, an agent who underreacts (whose posterior is closer to the prior than the Bayesian posterior) cannot be exploited at all."],"url":"http://arxiv.org/abs/2405.08087v1","category":"econ.TH"}
{"created":"2024-05-13 18:04:44","title":"Quantised helicity in optical media","abstract":"Optical helicity quantifies the handedness of light, and plays a central role in the description of interactions between light and chiral matter. In free space, it is related to the duality symmetry of the electromagnetic field, a continuous symmetry encapsulating the invariance of Maxwell's equations under the interchange of electric and magnetic fields. However, in materials the situation is not so straightforward, as the free space transformation must be extended to encompass mixing of both the $\\mathbf{E}$/$\\mathbf{H}$ and $\\mathbf{D}$/$\\mathbf{B}$ field pairs. The simultaneous direct interchange of $\\mathbf{E}$/$\\mathbf{H}$ and of $\\mathbf{D}$/$\\mathbf{B}$ is incompatible with the presence of linear constitutive relations. In this work, we extend the duality transform in a way that resolves this incompatibility, and use this to define the optical helicity in a general medium, which may be dispersive, lossy, chiral or nonreciprocal. We find that the helicity density must contain an explicit contribution associated with the polarisation and magnetisation of the matter, and we show that the form of this matter contribution is independent of the details of the medium. We also show that the in-medium helicity can be naturally expressed in terms of the elementary quantised excitations of the system.","sentences":["Optical helicity quantifies the handedness of light, and plays a central role in the description of interactions between light and chiral matter.","In free space, it is related to the duality symmetry of the electromagnetic field, a continuous symmetry encapsulating the invariance of Maxwell's equations under the interchange of electric and magnetic fields.","However, in materials the situation is not so straightforward, as the free space transformation must be extended to encompass mixing of both the $\\mathbf{E}$/$\\mathbf{H}$ and $\\mathbf{D}$/$\\mathbf{B}$ field pairs.","The simultaneous direct interchange of $\\mathbf{E}$/$\\mathbf{H}$ and of $\\mathbf{D}$/$\\mathbf{B}$ is incompatible with the presence of linear constitutive relations.","In this work, we extend the duality transform in a way that resolves this incompatibility, and use this to define the optical helicity in a general medium, which may be dispersive, lossy, chiral or nonreciprocal.","We find that the helicity density must contain an explicit contribution associated with the polarisation and magnetisation of the matter, and we show that the form of this matter contribution is independent of the details of the medium.","We also show that the in-medium helicity can be naturally expressed in terms of the elementary quantised excitations of the system."],"url":"http://arxiv.org/abs/2405.08086v1","category":"physics.optics"}
{"created":"2024-05-13 18:00:08","title":"Dynamic Rate Splitting Grouping for Antifragile Responses to Wireless Network Disruptions","abstract":"The reliance on wireless network architectures for applications demanding high reliability and fault tolerance is growing. These architectures heavily depend on wireless channels, making them susceptible to impairments and blockages. Ensuring functionality, particularly for safety-critical applications, demands robust countermeasures at the physical layer. In response, this work proposes the utilization of a dynamic Rate Splitting (RS) grouping approach as a resilience mechanism during blockages. RS effectively manages interference within networks but faces challenges during outages and blockages, where system performance can deteriorate due to the lowest decoding rate dictating the common rate and increased interference from fewer available channel links. As a strategic countermeasure, RS is leveraged to mitigate the impact of blockages, maintaining system efficiency and performance amidst disruptions. In fact, the introduction of new RS groups enables the exploration of novel solutions to the resource allocation problem, potentially outperforming those adopted before the occurrence of a blockage. As it turns out, by employing the dynamic RS grouping, the network exhibits an antifragile recovery response, showcasing the network's ability to not only recover from disruptions but also surpass its initial performance.","sentences":["The reliance on wireless network architectures for applications demanding high reliability and fault tolerance is growing.","These architectures heavily depend on wireless channels, making them susceptible to impairments and blockages.","Ensuring functionality, particularly for safety-critical applications, demands robust countermeasures at the physical layer.","In response, this work proposes the utilization of a dynamic Rate Splitting (RS) grouping approach as a resilience mechanism during blockages.","RS effectively manages interference within networks but faces challenges during outages and blockages, where system performance can deteriorate due to the lowest decoding rate dictating the common rate and increased interference from fewer available channel links.","As a strategic countermeasure, RS is leveraged to mitigate the impact of blockages, maintaining system efficiency and performance amidst disruptions.","In fact, the introduction of new RS groups enables the exploration of novel solutions to the resource allocation problem, potentially outperforming those adopted before the occurrence of a blockage.","As it turns out, by employing the dynamic RS grouping, the network exhibits an antifragile recovery response, showcasing the network's ability to not only recover from disruptions but also surpass its initial performance."],"url":"http://arxiv.org/abs/2405.08078v1","category":"eess.SP"}
{"created":"2024-05-13 18:00:05","title":"Identification of non-isomorphic 2-groups with dihedral central quotient and isomorphic modular group algebras","abstract":"The question whether non-isomorphic finite $p$-groups can have isomorphic modular group algebras was recently answered in the negative by Garc\\'ia-Lucas, Margolis and del R\\'io [J. Reine Angew. Math. 783 (2022), pp. 269-274]. We embed these negative solutions in the class of two-generated finite $2$-groups with dihedral central quotient, and solve the original question for all groups within this class. As a result, we discover new negative solutions and simple algebra isomorphisms. At the same time, the positive solutions for most of the groups in this class give some insights what makes the negative solutions special.","sentences":["The question whether non-isomorphic finite $p$-groups can have isomorphic modular group algebras was recently answered in the negative by Garc\\'ia-Lucas, Margolis and del R\\'io","[J. Reine Angew.","Math. 783 (2022), pp. 269-274].","We embed these negative solutions in the class of two-generated finite $2$-groups with dihedral central quotient, and solve the original question for all groups within this class.","As a result, we discover new negative solutions and simple algebra isomorphisms.","At the same time, the positive solutions for most of the groups in this class give some insights what makes the negative solutions special."],"url":"http://arxiv.org/abs/2405.08075v1","category":"math.RA"}
{"created":"2024-05-13 18:00:01","title":"Local Zeta Functions of Multiparameter Calabi-Yau Threefolds from the Picard-Fuchs Equations","abstract":"The deformation approach of arXiv:2104.07816 for computing zeta functions of one-parameter Calabi-Yau threefolds is generalised to cover also multiparameter manifolds. Consideration of the multiparameter case requires the development of an improved formalism. This allows us, among other things, to make progress on some issues left open in previous work, such as the treatment of apparent and conifold singularities and changes of coordinates. We also discuss the efficient numerical computation of the zeta functions. As examples, we compute the zeta functions of the two-parameter mirror octic, a non-symmetric split of the quintic threefold also with two parameters, and the $S_5$ symmetric five-parameter Hulek-Verrill manifolds. These examples allow us to exhibit the several new types of geometries for which our methods make practical computations possible. They also act as consistency checks, as our results reproduce and extend those of arXiv:hep-th/0409202 and arXiv:math/0304169. To make the methods developed here more approachable, a Mathematica package \"CY3Zeta\" for computing the zeta functions of Calabi-Yau threefolds, which is attached to this paper, is presented.","sentences":["The deformation approach of arXiv:2104.07816 for computing zeta functions of one-parameter Calabi-Yau threefolds is generalised to cover also multiparameter manifolds.","Consideration of the multiparameter case requires the development of an improved formalism.","This allows us, among other things, to make progress on some issues left open in previous work, such as the treatment of apparent and conifold singularities and changes of coordinates.","We also discuss the efficient numerical computation of the zeta functions.","As examples, we compute the zeta functions of the two-parameter mirror octic, a non-symmetric split of the quintic threefold also with two parameters, and the $S_5$ symmetric five-parameter Hulek-Verrill manifolds.","These examples allow us to exhibit the several new types of geometries for which our methods make practical computations possible.","They also act as consistency checks, as our results reproduce and extend those of arXiv:hep-th/0409202 and arXiv:math/0304169.","To make the methods developed here more approachable, a Mathematica package \"CY3Zeta\" for computing the zeta functions of Calabi-Yau threefolds, which is attached to this paper, is presented."],"url":"http://arxiv.org/abs/2405.08067v1","category":"hep-th"}
{"created":"2024-05-14 14:55:15","title":"EndoDAC: Efficient Adapting Foundation Model for Self-Supervised Depth Estimation from Any Endoscopic Camera","abstract":"Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization. Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance. This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation. We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes. Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters. Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder. Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs. Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics. Code is available at https://github.com/BeileiCui/EndoDAC.","sentences":["Depth estimation plays a crucial role in various tasks within endoscopic surgery, including navigation, surface reconstruction, and augmented reality visualization.","Despite the significant achievements of foundation models in vision tasks, including depth estimation, their direct application to the medical domain often results in suboptimal performance.","This highlights the need for efficient adaptation methods to adapt these models to endoscopic depth estimation.","We propose Endoscopic Depth Any Camera (EndoDAC) which is an efficient self-supervised depth estimation framework that adapts foundation models to endoscopic scenes.","Specifically, we develop the Dynamic Vector-Based Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor the foundational model to the surgical domain, utilizing remarkably few trainable parameters.","Given that camera information is not always accessible, we also introduce a self-supervised adaptation strategy that estimates camera intrinsics using the pose encoder.","Our framework is capable of being trained solely on monocular surgical videos from any camera, ensuring minimal training costs.","Experiments demonstrate that our approach obtains superior performance even with fewer training epochs and unaware of the ground truth camera intrinsics.","Code is available at https://github.com/BeileiCui/EndoDAC."],"url":"http://arxiv.org/abs/2405.08672v1","category":"eess.IV"}
{"created":"2024-05-14 13:05:16","title":"Rethinking the adaptive relationship between Encoder Layers and Decoder Layers","abstract":"This article explores the adaptive relationship between Encoder Layers and Decoder Layers using the SOTA model Helsinki-NLP/opus-mt-de-en, which translates German to English. The specific method involves introducing a bias-free fully connected layer between the Encoder and Decoder, with different initializations of the layer's weights, and observing the outcomes of fine-tuning versus retraining. Four experiments were conducted in total. The results suggest that directly modifying the pre-trained model structure for fine-tuning yields suboptimal performance. However, upon observing the outcomes of the experiments with retraining, this structural adjustment shows significant potential.","sentences":["This article explores the adaptive relationship between Encoder Layers and Decoder Layers using the SOTA model Helsinki-NLP/opus-mt-de-en, which translates German to English.","The specific method involves introducing a bias-free fully connected layer between the Encoder and Decoder, with different initializations of the layer's weights, and observing the outcomes of fine-tuning versus retraining.","Four experiments were conducted in total.","The results suggest that directly modifying the pre-trained model structure for fine-tuning yields suboptimal performance.","However, upon observing the outcomes of the experiments with retraining, this structural adjustment shows significant potential."],"url":"http://arxiv.org/abs/2405.08570v1","category":"cs.CL"}
{"created":"2024-05-14 12:24:52","title":"Self-Distillation Improves DNA Sequence Inference","abstract":"Self-supervised pretraining (SSP) has been recognized as a method to enhance prediction accuracy in various downstream tasks. However, its efficacy for DNA sequences remains somewhat constrained. This limitation stems primarily from the fact that most existing SSP approaches in genomics focus on masked language modeling of individual sequences, neglecting the crucial aspect of encoding statistics across multiple sequences. To overcome this challenge, we introduce an innovative deep neural network model, which incorporates collaborative learning between a `student' and a `teacher' subnetwork. In this model, the student subnetwork employs masked learning on nucleotides and progressively adapts its parameters to the teacher subnetwork through an exponential moving average approach. Concurrently, both subnetworks engage in contrastive learning, deriving insights from two augmented representations of the input sequences. This self-distillation process enables our model to effectively assimilate both contextual information from individual sequences and distributional data across the sequence population. We validated our approach with preliminary pretraining using the human reference genome, followed by applying it to 20 downstream inference tasks. The empirical results from these experiments demonstrate that our novel method significantly boosts inference performance across the majority of these tasks. Our code is available at https://github.com/wiedersehne/FinDNA.","sentences":["Self-supervised pretraining (SSP) has been recognized as a method to enhance prediction accuracy in various downstream tasks.","However, its efficacy for DNA sequences remains somewhat constrained.","This limitation stems primarily from the fact that most existing SSP approaches in genomics focus on masked language modeling of individual sequences, neglecting the crucial aspect of encoding statistics across multiple sequences.","To overcome this challenge, we introduce an innovative deep neural network model, which incorporates collaborative learning between a `student' and a `teacher' subnetwork.","In this model, the student subnetwork employs masked learning on nucleotides and progressively adapts its parameters to the teacher subnetwork through an exponential moving average approach.","Concurrently, both subnetworks engage in contrastive learning, deriving insights from two augmented representations of the input sequences.","This self-distillation process enables our model to effectively assimilate both contextual information from individual sequences and distributional data across the sequence population.","We validated our approach with preliminary pretraining using the human reference genome, followed by applying it to 20 downstream inference tasks.","The empirical results from these experiments demonstrate that our novel method significantly boosts inference performance across the majority of these tasks.","Our code is available at https://github.com/wiedersehne/FinDNA."],"url":"http://arxiv.org/abs/2405.08538v1","category":"cs.LG"}
{"created":"2024-05-14 02:13:11","title":"AI-driven, Model-Free Current Control: A Deep Symbolic Approach for Optimal Induction Machine Performance","abstract":"This paper proposed a straightforward and efficient current control solution for induction machines employing deep symbolic regression (DSR). The proposed DSR-based control design offers a simple yet highly effective approach by creating an optimal control model through training and fitting, resulting in an analytical dynamic numerical expression that characterizes the data. Notably, this approach not only produces an understandable model but also demonstrates the capacity to extrapolate and estimate data points outside its training dataset, showcasing its adaptability and resilience. In contrast to conventional state-of-the-art proportional-integral (PI) current controllers, which heavily rely on specific system models, the proposed DSR-based approach stands out for its model independence. Simulation and experimental tests validate its effectiveness, highlighting its superior extrapolation capabilities compared to conventional methods. These findings pave the way for the integration of deep learning methods in power conversion applications, promising improved performance and adaptability in the control of induction machines. The simulation and experimental test results are provided with a 3.7 kw induction machine to verify the efficacy of the proposed control solution.","sentences":["This paper proposed a straightforward and efficient current control solution for induction machines employing deep symbolic regression (DSR).","The proposed DSR-based control design offers a simple yet highly effective approach by creating an optimal control model through training and fitting, resulting in an analytical dynamic numerical expression that characterizes the data.","Notably, this approach not only produces an understandable model but also demonstrates the capacity to extrapolate and estimate data points outside its training dataset, showcasing its adaptability and resilience.","In contrast to conventional state-of-the-art proportional-integral (PI) current controllers, which heavily rely on specific system models, the proposed DSR-based approach stands out for its model independence.","Simulation and experimental tests validate its effectiveness, highlighting its superior extrapolation capabilities compared to conventional methods.","These findings pave the way for the integration of deep learning methods in power conversion applications, promising improved performance and adaptability in the control of induction machines.","The simulation and experimental test results are provided with a 3.7 kw induction machine to verify the efficacy of the proposed control solution."],"url":"http://arxiv.org/abs/2405.08277v1","category":"eess.SY"}
{"created":"2024-05-14 02:05:18","title":"Cloud Dissipation and Disk Wind in the Late Phase of Star Formation","abstract":"We perform a long-term simulation of star and disk formation using three-dimensional non-ideal magnetohydrodynamics. The simulation starts from a prestellar cloud and proceeds through the long-term evolution of the circumstellar disk until $\\sim 1.5\\times10^5$ yr after protostar formation. The disk has size $\\lesssim 50$ au and little substructure in the main accretion phase because of the action of magnetic braking and the magnetically-driven outflow to remove angular momentum. The main accretion phase ends when the outflow breaks out of the cloud, causing the envelope mass to decrease rapidly. The outflow subsequently weakens as the mass accretion rate also weakens. While the envelope-to-disk accretion continues, the disk grows gradually and develops transient spiral structures due to gravitational instability. When the envelope-to-disk accretion ends, the disk becomes stable and reaches a size $\\gtrsim 300$ au. In addition, about 30% of the initial cloud mass has been ejected by the outflow. A significant finding of this work is that after the envelope dissipates, a revitalization of the wind occurs, and there is mass ejection from the disk surface that lasts until the end of the simulation. This mass ejection (or disk wind) is generated since the magnetic pressure significantly dominates both the ram pressure and thermal pressure above and below the disk at this stage. Using the angular momentum flux and mass loss rate estimated from the disk wind, the disk dissipation timescale is estimated to be $\\sim10^6$ yr.","sentences":["We perform a long-term simulation of star and disk formation using three-dimensional non-ideal magnetohydrodynamics.","The simulation starts from a prestellar cloud and proceeds through the long-term evolution of the circumstellar disk until $\\sim 1.5\\times10^5$ yr after protostar formation.","The disk has size $\\lesssim 50$ au and little substructure in the main accretion phase because of the action of magnetic braking and the magnetically-driven outflow to remove angular momentum.","The main accretion phase ends when the outflow breaks out of the cloud, causing the envelope mass to decrease rapidly.","The outflow subsequently weakens as the mass accretion rate also weakens.","While the envelope-to-disk accretion continues, the disk grows gradually and develops transient spiral structures due to gravitational instability.","When the envelope-to-disk accretion ends, the disk becomes stable and reaches a size $\\gtrsim 300$ au.","In addition, about 30% of the initial cloud mass has been ejected by the outflow.","A significant finding of this work is that after the envelope dissipates, a revitalization of the wind occurs, and there is mass ejection from the disk surface that lasts until the end of the simulation.","This mass ejection (or disk wind) is generated since the magnetic pressure significantly dominates both the ram pressure and thermal pressure above and below the disk at this stage.","Using the angular momentum flux and mass loss rate estimated from the disk wind, the disk dissipation timescale is estimated to be $\\sim10^6$ yr."],"url":"http://arxiv.org/abs/2405.08271v1","category":"astro-ph.SR"}
{"created":"2024-05-14 02:02:15","title":"Towards Clinician-Preferred Segmentation: Leveraging Human-in-the-Loop for Test Time Adaptation in Medical Image Segmentation","abstract":"Deep learning-based medical image segmentation models often face performance degradation when deployed across various medical centers, largely due to the discrepancies in data distribution. Test Time Adaptation (TTA) methods, which adapt pre-trained models to test data, have been employed to mitigate such discrepancies. However, existing TTA methods primarily focus on manipulating Batch Normalization (BN) layers or employing prompt and adversarial learning, which may not effectively rectify the inconsistencies arising from divergent data distributions. In this paper, we propose a novel Human-in-the-loop TTA (HiTTA) framework that stands out in two significant ways. First, it capitalizes on the largely overlooked potential of clinician-corrected predictions, integrating these corrections into the TTA process to steer the model towards predictions that coincide more closely with clinical annotation preferences. Second, our framework conceives a divergence loss, designed specifically to diminish the prediction divergence instigated by domain disparities, through the careful calibration of BN parameters. Our HiTTA is distinguished by its dual-faceted capability to acclimatize to the distribution of test data whilst ensuring the model's predictions align with clinical expectations, thereby enhancing its relevance in a medical context. Extensive experiments on a public dataset underscore the superiority of our HiTTA over existing TTA methods, emphasizing the advantages of integrating human feedback and our divergence loss in enhancing the model's performance and adaptability across diverse medical centers.","sentences":["Deep learning-based medical image segmentation models often face performance degradation when deployed across various medical centers, largely due to the discrepancies in data distribution.","Test Time Adaptation (TTA) methods, which adapt pre-trained models to test data, have been employed to mitigate such discrepancies.","However, existing TTA methods primarily focus on manipulating Batch Normalization (BN) layers or employing prompt and adversarial learning, which may not effectively rectify the inconsistencies arising from divergent data distributions.","In this paper, we propose a novel Human-in-the-loop TTA (HiTTA) framework that stands out in two significant ways.","First, it capitalizes on the largely overlooked potential of clinician-corrected predictions, integrating these corrections into the TTA process to steer the model towards predictions that coincide more closely with clinical annotation preferences.","Second, our framework conceives a divergence loss, designed specifically to diminish the prediction divergence instigated by domain disparities, through the careful calibration of BN parameters.","Our HiTTA is distinguished by its dual-faceted capability to acclimatize to the distribution of test data whilst ensuring the model's predictions align with clinical expectations, thereby enhancing its relevance in a medical context.","Extensive experiments on a public dataset underscore the superiority of our HiTTA over existing TTA methods, emphasizing the advantages of integrating human feedback and our divergence loss in enhancing the model's performance and adaptability across diverse medical centers."],"url":"http://arxiv.org/abs/2405.08270v1","category":"cs.CV"}
{"created":"2024-05-13 22:04:10","title":"Toward Automated Programming for Robotic Assembly Using ChatGPT","abstract":"Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code. This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code. In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry. We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions. We outline the architecture of this system and strategies for task decomposition and code generation. Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project.","sentences":["Despite significant technological advancements, the process of programming robots for adaptive assembly remains labor-intensive, demanding expertise in multiple domains and often resulting in task-specific, inflexible code.","This work explores the potential of Large Language Models (LLMs), like ChatGPT, to automate this process, leveraging their ability to understand natural language instructions, generalize examples to new tasks, and write code.","In this paper, we suggest how these abilities can be harnessed and applied to real-world challenges in the manufacturing industry.","We present a novel system that uses ChatGPT to automate the process of programming robots for adaptive assembly by decomposing complex tasks into simpler subtasks, generating robot control code, executing the code in a simulated workcell, and debugging syntax and control errors, such as collisions.","We outline the architecture of this system and strategies for task decomposition and code generation.","Finally, we demonstrate how our system can autonomously program robots for various assembly tasks in a real-world project."],"url":"http://arxiv.org/abs/2405.08216v1","category":"cs.RO"}
{"created":"2024-05-13 21:33:58","title":"Interactive Lab Notebooks for Robotics Researchers","abstract":"Interactive notebooks, such as Jupyter, have revolutionized the field of data science by providing an integrated environment for data, code, and documentation. However, their adoption by robotics researchers and model developers has been limited. This study investigates the logging and record-keeping practices of robotics researchers, drawing parallels to the pre-interactive notebook era of data science. Through interviews with robotics researchers, we identified the reliance on diverse and often incompatible tools for managing experimental data, leading to challenges in reproducibility and data traceability. Our findings reveal that robotics researchers can benefit from a specialized version of interactive notebooks that supports comprehensive data entry, continuous context capture, and agile data staging. We propose extending interactive notebooks to better serve the needs of robotics researchers by integrating features akin to traditional lab notebooks. This adaptation aims to enhance the organization, analysis, and reproducibility of experimental data in robotics, fostering a more streamlined and efficient research workflow.","sentences":["Interactive notebooks, such as Jupyter, have revolutionized the field of data science by providing an integrated environment for data, code, and documentation.","However, their adoption by robotics researchers and model developers has been limited.","This study investigates the logging and record-keeping practices of robotics researchers, drawing parallels to the pre-interactive notebook era of data science.","Through interviews with robotics researchers, we identified the reliance on diverse and often incompatible tools for managing experimental data, leading to challenges in reproducibility and data traceability.","Our findings reveal that robotics researchers can benefit from a specialized version of interactive notebooks that supports comprehensive data entry, continuous context capture, and agile data staging.","We propose extending interactive notebooks to better serve the needs of robotics researchers by integrating features akin to traditional lab notebooks.","This adaptation aims to enhance the organization, analysis, and reproducibility of experimental data in robotics, fostering a more streamlined and efficient research workflow."],"url":"http://arxiv.org/abs/2405.08200v1","category":"cs.CE"}
{"created":"2024-05-13 20:58:13","title":"An adaptive enrichment design using Bayesian model averaging for selection and threshold-identification of tailoring variables","abstract":"Precision medicine stands as a transformative approach in healthcare, offering tailored treatments that can enhance patient outcomes and reduce healthcare costs. As understanding of complex disease improves, clinical trials are being designed to detect subgroups of patients with enhanced treatment effects. Biomarker-driven adaptive enrichment designs, which enroll a general population initially and later restrict accrual to treatment-sensitive patients, are gaining popularity. Current practice often assumes either pre-trial knowledge of biomarkers defining treatment-sensitive subpopulations or a simple, linear relationship between continuous markers and treatment effectiveness. Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design which identifies important tailoring variables out of a larger set of candidate biomarkers. Our proposed design is equipped with a flexible modelling framework where the effects of continuous biomarkers are introduced using free knot B-splines. The parameters of interest are then estimated by marginalizing over the space of all possible variable combinations using Bayesian model averaging. At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination due to efficacy or futility and restricting future enrollment to treatment-sensitive patients. We consider pre-categorized and continuous biomarkers, the latter of which may have complex, nonlinear relationships to the outcome and treatment effect. Using simulations, we derive the operating characteristics of our design and compare its performance to two existing approaches.","sentences":["Precision medicine stands as a transformative approach in healthcare, offering tailored treatments that can enhance patient outcomes and reduce healthcare costs.","As understanding of complex disease improves, clinical trials are being designed to detect subgroups of patients with enhanced treatment effects.","Biomarker-driven adaptive enrichment designs, which enroll a general population initially and later restrict accrual to treatment-sensitive patients, are gaining popularity.","Current practice often assumes either pre-trial knowledge of biomarkers defining treatment-sensitive subpopulations or a simple, linear relationship between continuous markers and treatment effectiveness.","Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design which identifies important tailoring variables out of a larger set of candidate biomarkers.","Our proposed design is equipped with a flexible modelling framework where the effects of continuous biomarkers are introduced using free knot B-splines.","The parameters of interest are then estimated by marginalizing over the space of all possible variable combinations using Bayesian model averaging.","At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination due to efficacy or futility and restricting future enrollment to treatment-sensitive patients.","We consider pre-categorized and continuous biomarkers, the latter of which may have complex, nonlinear relationships to the outcome and treatment effect.","Using simulations, we derive the operating characteristics of our design and compare its performance to two existing approaches."],"url":"http://arxiv.org/abs/2405.08180v1","category":"stat.ME"}
{"created":"2024-05-13 20:56:49","title":"A Theoretical Framework for Self-Gravitating k-Form Boson Stars with Internal Symmetries","abstract":"Current boson star models are largely restricted to global symmetries and lower spin fields. In this work, we generalize these systems of self-gravitating bosonic fields to allow for arbitrary totally antisymmetric tensor fields and arbitrary internal gauge symmetries. We construct a generalized formalism for Yang-Mills-like theories, which allows for arbitrary k-form fields, instead of just vector fields. The k-form fields have gauge symmetries described by semisimple, compact Lie groups. We further derive equations of motion for the k-form fields and connection coefficients of the Lie group. Extensions and applications are also discussed. We present a novel way to fix the group connection using a spacetime connection. As an example, we derive explicitly the connection coefficients for SU(2) in a spherically symmetric spacetime using rectangular vielbeins. The combination of methods presented leads to a powerful, adaptable and practical framework. As a proof of concept, we derive ordinary differential equations for a 0-form field with a SU(2) symmetry. Our framework can be used to model self-gravitating (multi) particle states with internal symmetries, such as pion condensates or dark matter. It is also suited as a tool to approach open problems in modified gravity and string theory.","sentences":["Current boson star models are largely restricted to global symmetries and lower spin fields.","In this work, we generalize these systems of self-gravitating bosonic fields to allow for arbitrary totally antisymmetric tensor fields and arbitrary internal gauge symmetries.","We construct a generalized formalism for Yang-Mills-like theories, which allows for arbitrary k-form fields, instead of just vector fields.","The k-form fields have gauge symmetries described by semisimple, compact Lie groups.","We further derive equations of motion for the k-form fields and connection coefficients of the Lie group.","Extensions and applications are also discussed.","We present a novel way to fix the group connection using a spacetime connection.","As an example, we derive explicitly the connection coefficients for SU(2) in a spherically symmetric spacetime using rectangular vielbeins.","The combination of methods presented leads to a powerful, adaptable and practical framework.","As a proof of concept, we derive ordinary differential equations for a 0-form field with a SU(2) symmetry.","Our framework can be used to model self-gravitating (multi) particle states with internal symmetries, such as pion condensates or dark matter.","It is also suited as a tool to approach open problems in modified gravity and string theory."],"url":"http://arxiv.org/abs/2405.08178v1","category":"gr-qc"}
{"created":"2024-05-13 20:05:52","title":"Efficient Spin-Adapted Implementation of Multireference Algebraic Diagrammatic Construction Theory. I. Core-Ionized States and X-Ray Photoelectron Spectra","abstract":"We present an efficient implementation of multireference algebraic diagrammatic construction theory (MR-ADC) for simulating core-ionized states and X-ray photoelectron spectra (XPS). Taking advantage of spin adaptation, automatic code generation, and density fitting, our implementation can perform calculations for molecules with more than 1500 molecular orbitals, incorporating static and dynamic correlation in the ground and excited electronic states. We demonstrate the capabilities of MR-ADC methods by simulating the XPS spectra of substituted ferrocene complexes and azobenzene isomers. For the ground electronic states of these molecules, the XPS spectra computed using the extended second-order MR-ADC method (MR-ADC(2)-X) are in excellent agreement with available experimental results. We further show that MR-ADC can be used as a tool for interpreting or predicting the results of time-resolved XPS measurements by simulating the core ionization spectra of azobenzene along its photoisomerization, including the XPS signatures of excited states and the minimum energy conical intersection. This work is the first in a series of publications reporting the efficient implementations of MR-ADC methods.","sentences":["We present an efficient implementation of multireference algebraic diagrammatic construction theory (MR-ADC) for simulating core-ionized states and X-ray photoelectron spectra (XPS).","Taking advantage of spin adaptation, automatic code generation, and density fitting, our implementation can perform calculations for molecules with more than 1500 molecular orbitals, incorporating static and dynamic correlation in the ground and excited electronic states.","We demonstrate the capabilities of MR-ADC methods by simulating the XPS spectra of substituted ferrocene complexes and azobenzene isomers.","For the ground electronic states of these molecules, the XPS spectra computed using the extended second-order MR-ADC method (MR-ADC(2)-X) are in excellent agreement with available experimental results.","We further show that MR-ADC can be used as a tool for interpreting or predicting the results of time-resolved XPS measurements by simulating the core ionization spectra of azobenzene along its photoisomerization, including the XPS signatures of excited states and the minimum energy conical intersection.","This work is the first in a series of publications reporting the efficient implementations of MR-ADC methods."],"url":"http://arxiv.org/abs/2405.08161v1","category":"physics.chem-ph"}
{"created":"2024-05-13 19:55:05","title":"Mechanical memories in solids, from disorder to design","abstract":"Solids are rigid, which means that when left undisturbed, their structures are nearly static. It follows that these structures depend on history -- but it is surprising that they hold readable memories of past events. Here we review the research that has recently flourished around mechanical memory formation, beginning with amorphous solids' various memories of deformation and mesoscopic models based on particle rearrangements. We describe how these concepts apply to a much wider range of solids and glassy matter -- and how they are a bridge to memory and physical computing in mechanical metamaterials. An understanding of memory in all these solids can potentially be the basis for designing or training functionality into materials. Just as important is memory's value for understanding matter whenever it is complex, frustrated, and out of equilibrium.","sentences":["Solids are rigid, which means that when left undisturbed, their structures are nearly static.","It follows that these structures depend on history -- but it is surprising that they hold readable memories of past events.","Here we review the research that has recently flourished around mechanical memory formation, beginning with amorphous solids' various memories of deformation and mesoscopic models based on particle rearrangements.","We describe how these concepts apply to a much wider range of solids and glassy matter -- and how they are a bridge to memory and physical computing in mechanical metamaterials.","An understanding of memory in all these solids can potentially be the basis for designing or training functionality into materials.","Just as important is memory's value for understanding matter whenever it is complex, frustrated, and out of equilibrium."],"url":"http://arxiv.org/abs/2405.08158v1","category":"cond-mat.soft"}
{"created":"2024-05-13 18:00:01","title":"Direct and Efficient Detection of Quantum Superposition","abstract":"One of the most striking quantum phenomena is superposition, where one particle simultaneously inhabits different states. Most methods to verify coherent superposition are indirect, in that they require the distinct states to be recombined. Here, we adapt an XOR game, in which separated parties measure different parts of a superposed particle, and use it to verify superpositions with \\textit{local measurements} and a second independent particle. We then turn this game into a resource-efficient verification scheme, obtaining a confidence that the particle is superposed which approaches unity exponentially fast. We demonstrate our scheme using a single photon, obtaining a 99\\% confidence that the particle is superposed with only 37 copies. Our work shows the utility of XOR games to verify quantum resources, allowing us to efficiently detect quantum superposition without reinterfering the superposed states.","sentences":["One of the most striking quantum phenomena is superposition, where one particle simultaneously inhabits different states.","Most methods to verify coherent superposition are indirect, in that they require the distinct states to be recombined.","Here, we adapt an XOR game, in which separated parties measure different parts of a superposed particle, and use it to verify superpositions with \\textit{local measurements} and a second independent particle.","We then turn this game into a resource-efficient verification scheme, obtaining a confidence that the particle is superposed which approaches unity exponentially fast.","We demonstrate our scheme using a single photon, obtaining a 99\\% confidence that the particle is superposed with only 37 copies.","Our work shows the utility of XOR games to verify quantum resources, allowing us to efficiently detect quantum superposition without reinterfering the superposed states."],"url":"http://arxiv.org/abs/2405.08065v1","category":"quant-ph"}
{"created":"2024-05-13 17:56:13","title":"Coin3D: Controllable and Interactive 3D Assets Generation with Proxy-Guided Conditioning","abstract":"As humans, we aspire to create media content that is both freely willed and readily controlled. Thanks to the prominent development of generative techniques, we now can easily utilize 2D diffusion methods to synthesize images controlled by raw sketch or designated human poses, and even progressively edit/regenerate local regions with masked inpainting. However, similar workflows in 3D modeling tasks are still unavailable due to the lack of controllability and efficiency in 3D generation. In this paper, we present a novel controllable and interactive 3D assets modeling framework, named Coin3D. Coin3D allows users to control the 3D generation using a coarse geometry proxy assembled from basic shapes, and introduces an interactive generation workflow to support seamless local part editing while delivering responsive 3D object previewing within a few seconds. To this end, we develop several techniques, including the 3D adapter that applies volumetric coarse shape control to the diffusion model, proxy-bounded editing strategy for precise part editing, progressive volume cache to support responsive preview, and volume-SDS to ensure consistent mesh reconstruction. Extensive experiments of interactive generation and editing on diverse shape proxies demonstrate that our method achieves superior controllability and flexibility in the 3D assets generation task.","sentences":["As humans, we aspire to create media content that is both freely willed and readily controlled.","Thanks to the prominent development of generative techniques, we now can easily utilize 2D diffusion methods to synthesize images controlled by raw sketch or designated human poses, and even progressively edit/regenerate local regions with masked inpainting.","However, similar workflows in 3D modeling tasks are still unavailable due to the lack of controllability and efficiency in 3D generation.","In this paper, we present a novel controllable and interactive 3D assets modeling framework, named Coin3D. Coin3D allows users to control the 3D generation using a coarse geometry proxy assembled from basic shapes, and introduces an interactive generation workflow to support seamless local part editing while delivering responsive 3D object previewing within a few seconds.","To this end, we develop several techniques, including the 3D adapter that applies volumetric coarse shape control to the diffusion model, proxy-bounded editing strategy for precise part editing, progressive volume cache to support responsive preview, and volume-SDS to ensure consistent mesh reconstruction.","Extensive experiments of interactive generation and editing on diverse shape proxies demonstrate that our method achieves superior controllability and flexibility in the 3D assets generation task."],"url":"http://arxiv.org/abs/2405.08054v1","category":"cs.GR"}
{"created":"2024-05-13 03:02:56","title":"A LLM-based Controllable, Scalable, Human-Involved User Simulator Framework for Conversational Recommender Systems","abstract":"Conversational Recommender System (CRS) leverages real-time feedback from users to dynamically model their preferences, thereby enhancing the system's ability to provide personalized recommendations and improving the overall user experience. CRS has demonstrated significant promise, prompting researchers to concentrate their efforts on developing user simulators that are both more realistic and trustworthy. The emergence of Large Language Models (LLMs) has marked the onset of a new epoch in computational capabilities, exhibiting human-level intelligence in various tasks. Research efforts have been made to utilize LLMs for building user simulators to evaluate the performance of CRS. Although these efforts showcase innovation, they are accompanied by certain limitations. In this work, we introduce a Controllable, Scalable, and Human-Involved (CSHI) simulator framework that manages the behavior of user simulators across various stages via a plugin manager. CSHI customizes the simulation of user behavior and interactions to provide a more lifelike and convincing user interaction experience. Through experiments and case studies in two conversational recommendation scenarios, we show that our framework can adapt to a variety of conversational recommendation settings and effectively simulate users' personalized preferences. Consequently, our simulator is able to generate feedback that closely mirrors that of real users. This facilitates a reliable assessment of existing CRS studies and promotes the creation of high-quality conversational recommendation datasets.","sentences":["Conversational Recommender System (CRS) leverages real-time feedback from users to dynamically model their preferences, thereby enhancing the system's ability to provide personalized recommendations and improving the overall user experience.","CRS has demonstrated significant promise, prompting researchers to concentrate their efforts on developing user simulators that are both more realistic and trustworthy.","The emergence of Large Language Models (LLMs) has marked the onset of a new epoch in computational capabilities, exhibiting human-level intelligence in various tasks.","Research efforts have been made to utilize LLMs for building user simulators to evaluate the performance of CRS.","Although these efforts showcase innovation, they are accompanied by certain limitations.","In this work, we introduce a Controllable, Scalable, and Human-Involved (CSHI) simulator framework that manages the behavior of user simulators across various stages via a plugin manager.","CSHI customizes the simulation of user behavior and interactions to provide a more lifelike and convincing user interaction experience.","Through experiments and case studies in two conversational recommendation scenarios, we show that our framework can adapt to a variety of conversational recommendation settings and effectively simulate users' personalized preferences.","Consequently, our simulator is able to generate feedback that closely mirrors that of real users.","This facilitates a reliable assessment of existing CRS studies and promotes the creation of high-quality conversational recommendation datasets."],"url":"http://arxiv.org/abs/2405.08035v1","category":"cs.HC"}
{"created":"2024-05-14 16:44:06","title":"Local well-posedness and regularity properties for an initial-boundary value problem associated to the fifth order Korteweg-de Vries equation","abstract":"In this work we prove that the initial-boundary value problem (IBVP) for the fifth order Korteweg-de Vries equation \\begin{align*} \\left. \\begin{array}{rlr} u_t+\\partial_x^5 u+u\\partial_x u&\\hspace{-2mm}=0,&\\quad x\\in\\mathbb R^+,\\; t\\in\\mathbb R^+,\\\\ u(x,0)&\\hspace{-2mm}=g(x),&\\\\ u(0,t)=h_1(t),\\, \\partial_x u(0,t)&\\hspace{-2mm}=h_2(t),\\,\\partial_x^2 u(0,t)=h_3(t), \\end{array} \\right\\} \\end{align*} is locally well posed, when the data $g$, $h_1$, $h_2$, $h_3$ are taken in such a way that $g\\in H^s(\\mathbb R_x^+)$, and $h_{j+1}\\in H^{\\frac{s+2-j}5}(\\mathbb R_t^+)$, $j=0,1,2$, $s\\in [0,\\frac{11}4)\\setminus \\{\\frac12,\\frac32,\\frac52\\}$, and satisfy the following compatibility conditions: \\begin{align*} g(0)=h_1(0) \\text{ if } \\frac12<s<\\frac32;\\\\ g(0)=h_1(0),\\; g'(0)=h_2(0) \\text{ if } \\frac32<s<\\frac52;\\\\ g(0)=h_1(0), \\; g'(0)=h_2(0),\\; g''(0)=h_3(0) \\text{ if } \\frac52<s<\\frac{11}4. \\end{align*} Besides, we prove that the nonlinear part of the solution is smoother than the initial datum $g$.","sentences":["In this work we prove that the initial-boundary value problem (IBVP) for the fifth order Korteweg-de Vries equation \\begin{align*} \\left.","\\begin{array}{rlr} u_t+\\partial_x^5 u+u\\partial_x u&\\hspace{-2mm}=0,&\\quad","x\\in\\mathbb R^+,\\; t\\in\\mathbb R^+,\\\\ u(x,0)&\\hspace{-2mm}=g(x),&\\\\ u(0,t)=h_1(t),\\, \\partial_x u(0,t)&\\hspace{-2mm}=h_2(t),\\,\\partial_x^2 u(0,t)=h_3(t), \\end{array} \\right\\} \\end{align*} is locally well posed, when the data $g$, $h_1$, $h_2$, $h_3$ are taken in such a way that $g\\in H^s(\\mathbb R_x^+)$, and $h_{j+1}\\in H^{\\frac{s+2-j}5}(\\mathbb R_t^+)$, $j=0,1,2$, $s\\in [0,\\frac{11}4)\\setminus \\{\\frac12,\\frac32,\\frac52\\}$, and satisfy the following compatibility conditions: \\begin{align*} g(0)=h_1(0) \\text{ if } \\frac12<s<\\frac32;\\\\ g(0)=h_1(0),\\; g'(0)=h_2(0)","\\text{ if } \\frac32<s<\\frac52;\\\\ g(0)=h_1(0), \\; g'(0)=h_2(0),\\; g''(0)=h_3(0)","\\text{ if } \\frac52<s<\\frac{11}4.","\\end{align*} Besides, we prove that the nonlinear part of the solution is smoother than the initial datum $g$."],"url":"http://arxiv.org/abs/2405.08757v1","category":"math.AP"}
{"created":"2024-05-14 14:51:15","title":"Quantum Circuit Model for Lattice Boltzmann Fluid Flow Simulations","abstract":"In the present contribution, we propose a quantum computational algorithm for the Lattice Boltzmann Method (LBM) to solve fluid flow equations in the low Reynolds number ($Re$) regime. Firstly, we express the LBM collision and streaming operators in matrix form. Since quantum logic gates are typically expressed as unitary matrices, we first decompose LBM operations as a product of unitaries. The particle distribution functions (PDFs) of LBM are encoded as probability amplitudes of the quantum state. We have observed that the amplitudes in the state vector (SV) can be affected: (i) by the choice of encoding the PDFs during the quantum state preparation or (ii) when collision is followed by streaming, as in classical LBM implementation. In the first case, we show that the ancilla qubit must be in superposition with the compute qubits during the quantum state preparation. The superposition allows the SV to utilize the increased Hilbert space offered by the ancilla qubit rather than placing the ancilla in a separate register, which restricts the space of possible outcomes. Next, we show that the second issue can be resolved by having an intermediate Hadamard gate before the streaming operation. The proposed algorithm has been tested through typical benchmark problems like advection-diffusion of a Gaussian hill, Poiseuille flow, Couette flow, and the lid-driven cavity problem. The results are validated with the respective analytic or reference solutions. Translating the unitaries into quantum gates (circuit synthesis) presents a primary challenge, as a unitary matrix can be decomposed in multiple ways. We report on the CNOT and U gate counts obtained for the test cases with the range of qubits from 9 to 12. Although the gate count closely agrees with the theoretical limit, the number of two qubit gates is in the $O(10^7)$ prompts special attention to circuit synthesis.","sentences":["In the present contribution, we propose a quantum computational algorithm for the Lattice Boltzmann Method (LBM) to solve fluid flow equations in the low Reynolds number ($Re$) regime.","Firstly, we express the LBM collision and streaming operators in matrix form.","Since quantum logic gates are typically expressed as unitary matrices, we first decompose LBM operations as a product of unitaries.","The particle distribution functions (PDFs) of LBM are encoded as probability amplitudes of the quantum state.","We have observed that the amplitudes in the state vector (SV) can be affected: (i) by the choice of encoding the PDFs during the quantum state preparation or (ii) when collision is followed by streaming, as in classical LBM implementation.","In the first case, we show that the ancilla qubit must be in superposition with the compute qubits during the quantum state preparation.","The superposition allows the SV to utilize the increased Hilbert space offered by the ancilla qubit rather than placing the ancilla in a separate register, which restricts the space of possible outcomes.","Next, we show that the second issue can be resolved by having an intermediate Hadamard gate before the streaming operation.","The proposed algorithm has been tested through typical benchmark problems like advection-diffusion of a Gaussian hill, Poiseuille flow, Couette flow, and the lid-driven cavity problem.","The results are validated with the respective analytic or reference solutions.","Translating the unitaries into quantum gates (circuit synthesis) presents a primary challenge, as a unitary matrix can be decomposed in multiple ways.","We report on the CNOT and U gate counts obtained for the test cases with the range of qubits from 9 to 12.","Although the gate count closely agrees with the theoretical limit, the number of two qubit gates is in the $O(10^7)$ prompts special attention to circuit synthesis."],"url":"http://arxiv.org/abs/2405.08669v1","category":"quant-ph"}
{"created":"2024-05-14 14:35:21","title":"Self-supervised learning improves robustness of deep learning lung tumor segmentation to CT imaging differences","abstract":"Self-supervised learning (SSL) is an approach to extract useful feature representations from unlabeled data, and enable fine-tuning on downstream tasks with limited labeled examples. Self-pretraining is a SSL approach that uses the curated task dataset for both pretraining the networks and fine-tuning them. Availability of large, diverse, and uncurated public medical image sets provides the opportunity to apply SSL in the \"wild\" and potentially extract features robust to imaging variations. However, the benefit of wild- vs self-pretraining has not been studied for medical image analysis. In this paper, we compare robustness of wild versus self-pretrained transformer (vision transformer [ViT] and hierarchical shifted window [Swin]) models to computed tomography (CT) imaging differences for non-small cell lung cancer (NSCLC) segmentation. Wild-pretrained Swin models outperformed self-pretrained Swin for the various imaging acquisitions. ViT resulted in similar accuracy for both wild- and self-pretrained models. Masked image prediction pretext task that forces networks to learn the local structure resulted in higher accuracy compared to contrastive task that models global image information. Wild-pretrained models resulted in higher feature reuse at the lower level layers and feature differentiation close to output layer after fine-tuning. Hence, we conclude: Wild-pretrained networks were more robust to analyzed CT imaging differences for lung tumor segmentation than self-pretrained methods. Swin architecture benefited from such pretraining more than ViT.","sentences":["Self-supervised learning (SSL) is an approach to extract useful feature representations from unlabeled data, and enable fine-tuning on downstream tasks with limited labeled examples.","Self-pretraining is a SSL approach that uses the curated task dataset for both pretraining the networks and fine-tuning them.","Availability of large, diverse, and uncurated public medical image sets provides the opportunity to apply SSL in the \"wild\" and potentially extract features robust to imaging variations.","However, the benefit of wild- vs self-pretraining has not been studied for medical image analysis.","In this paper, we compare robustness of wild versus self-pretrained transformer (vision transformer [ViT] and hierarchical shifted window [Swin]) models to computed tomography (CT) imaging differences for non-small cell lung cancer (NSCLC) segmentation.","Wild-pretrained Swin models outperformed self-pretrained Swin for the various imaging acquisitions.","ViT resulted in similar accuracy for both wild- and self-pretrained models.","Masked image prediction pretext task that forces networks to learn the local structure resulted in higher accuracy compared to contrastive task that models global image information.","Wild-pretrained models resulted in higher feature reuse at the lower level layers and feature differentiation close to output layer after fine-tuning.","Hence, we conclude: Wild-pretrained networks were more robust to analyzed CT imaging differences for lung tumor segmentation than self-pretrained methods.","Swin architecture benefited from such pretraining more than ViT."],"url":"http://arxiv.org/abs/2405.08657v1","category":"eess.IV"}
{"created":"2024-05-14 14:30:32","title":"Non-local parabolic equations with singular (Morrey) time-inhomogeneous drift","abstract":"We obtain Sobolev regularity estimates for solutions of non-local parabolic equations with locally unbounded drift satisfying some minimal assumptions. These results yield Krylov bound for the corresponding Feller stable process as well as some a priori regularity estimates on solutions of McKean-Vlasov equations. A key element of our arguments is a parabolic operator norm inequality that we prove using some ideas of Adams and Krylov.","sentences":["We obtain Sobolev regularity estimates for solutions of non-local parabolic equations with locally unbounded drift satisfying some minimal assumptions.","These results yield Krylov bound for the corresponding Feller stable process as well as some a priori regularity estimates on solutions of McKean-Vlasov equations.","A key element of our arguments is a parabolic operator norm inequality that we prove using some ideas of Adams and Krylov."],"url":"http://arxiv.org/abs/2405.08652v1","category":"math.AP"}
{"created":"2024-05-14 14:11:25","title":"Instantaneous Bandwidth Estimation from Level-Crossing Samples via LSTM-based Encoder-Decoder Architecture","abstract":"This paper presents an approach for instantaneous bandwidth estimation from level-crossing samples using a long short-term memory (LSTM) encoder-decoder architecture. Level-crossing sampling is a nonuniform sampling technique that is particularly useful for energy-efficient acquisition of signals with sparse spectra. Especially in combination with fully analog wireless sensor nodes, level-crossing sampling offers a viable alternative to traditional sampling methods. However, due to the nonuniform distribution of samples, reconstructing the original signal is a challenging task. One promising reconstruction approach is time-warping, where the local signal spectrum is taken into account. However, this requires an accurate estimate of the instantaneous bandwidth of the signal. In this paper, we show that applying neural networks (NNs) to the problem of estimating instantaneous bandwidth from level-crossing samples can improve the overall reconstruction accuracy. We conduct a comprehensive numerical analysis of the proposed approach and compare it to an intensity-based bandwidth estimation method from literature.","sentences":["This paper presents an approach for instantaneous bandwidth estimation from level-crossing samples using a long short-term memory (LSTM) encoder-decoder architecture.","Level-crossing sampling is a nonuniform sampling technique that is particularly useful for energy-efficient acquisition of signals with sparse spectra.","Especially in combination with fully analog wireless sensor nodes, level-crossing sampling offers a viable alternative to traditional sampling methods.","However, due to the nonuniform distribution of samples, reconstructing the original signal is a challenging task.","One promising reconstruction approach is time-warping, where the local signal spectrum is taken into account.","However, this requires an accurate estimate of the instantaneous bandwidth of the signal.","In this paper, we show that applying neural networks (NNs) to the problem of estimating instantaneous bandwidth from level-crossing samples can improve the overall reconstruction accuracy.","We conduct a comprehensive numerical analysis of the proposed approach and compare it to an intensity-based bandwidth estimation method from literature."],"url":"http://arxiv.org/abs/2405.08632v1","category":"eess.SP"}
{"created":"2024-05-14 13:55:40","title":"Unconventional surface phase transitions in a (1+1)D $SU(2)_1$ CFT edge coupled to a (2+1)D $Z_2$ bulk","abstract":"We design a (2+1)D quantum spin model in which spin-1/2 ladders are coupled through antiferromagnetic Ising interactions. The model hosts a quantum phase transition in the (2+1)D $Z_2$ universality class from the Haldane phase to the antiferromagnetic Ising ordered phase. We focus on studying the surface properties of three different surface configurations when the Ising couplings are tuned. Different behaviors are found on different surfaces. We find ordinary and two different extraordinary surface critical behaviors (SCBs) at the bulk critical point. The ordinary SCBs belong to the surface universality class of the classical 3D Ising bulk transition. One extraordinary SCBs is induced by the topological properties of the Haldane phase. Another extraordinary SCBs at the bulk critical point is induced by an unconventional surface phase transition where the surface develops an Ising order before the bulk. This surface transition is realized by coupling a (1+1)D $SU(2)_1$ CFT boundary to a (2+1)D bulk with $Z_2$ symmetry. We find that the transition is neither a (1+1)D $Z_2$ transition, expected based on symmetry consideration, nor a Kosterlitz-Thouless-like transition, violating the previous theoretical prediction. This new surface phase transition and related extraordinary SCBs deserve further analytical and numerical exploration.","sentences":["We design a (2+1)D quantum spin model in which spin-1/2 ladders are coupled through antiferromagnetic Ising interactions.","The model hosts a quantum phase transition in the (2+1)D $Z_2$ universality class from the Haldane phase to the antiferromagnetic Ising ordered phase.","We focus on studying the surface properties of three different surface configurations when the Ising couplings are tuned.","Different behaviors are found on different surfaces.","We find ordinary and two different extraordinary surface critical behaviors (SCBs) at the bulk critical point.","The ordinary SCBs belong to the surface universality class of the classical 3D Ising bulk transition.","One extraordinary SCBs is induced by the topological properties of the Haldane phase.","Another extraordinary SCBs at the bulk critical point is induced by an unconventional surface phase transition where the surface develops an Ising order before the bulk.","This surface transition is realized by coupling a (1+1)D $SU(2)_1$ CFT boundary to a (2+1)D bulk with $Z_2$ symmetry.","We find that the transition is neither a (1+1)D $Z_2$ transition, expected based on symmetry consideration, nor a Kosterlitz-Thouless-like transition, violating the previous theoretical prediction.","This new surface phase transition and related extraordinary SCBs deserve further analytical and numerical exploration."],"url":"http://arxiv.org/abs/2405.08612v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 13:41:44","title":"Optimizing Deep Reinforcement Learning for American Put Option Hedging","abstract":"This paper contributes to the existing literature on hedging American options with Deep Reinforcement Learning (DRL). The study first investigates hyperparameter impact on hedging performance, considering learning rates, training episodes, neural network architectures, training steps, and transaction cost penalty functions. Results highlight the importance of avoiding certain combinations, such as high learning rates with a high number of training episodes or low learning rates with few training episodes and emphasize the significance of utilizing moderate values for optimal outcomes. Additionally, the paper warns against excessive training steps to prevent instability and demonstrates the superiority of a quadratic transaction cost penalty function over a linear version. This study then expands upon the work of Pickard et al. (2024), who utilize a Chebyshev interpolation option pricing method to train DRL agents with market calibrated stochastic volatility models. While the results of Pickard et al. (2024) showed that these DRL agents achieve satisfactory performance on empirical asset paths, this study introduces a novel approach where new agents at weekly intervals to newly calibrated stochastic volatility models. Results show DRL agents re-trained using weekly market data surpass the performance of those trained solely on the sale date. Furthermore, the paper demonstrates that both single-train and weekly-train DRL agents outperform the Black-Scholes Delta method at transaction costs of 1% and 3%. This practical relevance suggests that practitioners can leverage readily available market data to train DRL agents for effective hedging of options in their portfolios.","sentences":["This paper contributes to the existing literature on hedging American options with Deep Reinforcement Learning (DRL).","The study first investigates hyperparameter impact on hedging performance, considering learning rates, training episodes, neural network architectures, training steps, and transaction cost penalty functions.","Results highlight the importance of avoiding certain combinations, such as high learning rates with a high number of training episodes or low learning rates with few training episodes and emphasize the significance of utilizing moderate values for optimal outcomes.","Additionally, the paper warns against excessive training steps to prevent instability and demonstrates the superiority of a quadratic transaction cost penalty function over a linear version.","This study then expands upon the work of Pickard et al. (2024), who utilize a Chebyshev interpolation option pricing method to train DRL agents with market calibrated stochastic volatility models.","While the results of Pickard et al. (2024) showed that these DRL agents achieve satisfactory performance on empirical asset paths, this study introduces a novel approach where new agents at weekly intervals to newly calibrated stochastic volatility models.","Results show DRL agents re-trained using weekly market data surpass the performance of those trained solely on the sale date.","Furthermore, the paper demonstrates that both single-train and weekly-train DRL agents outperform the Black-Scholes Delta method at transaction costs of 1% and 3%.","This practical relevance suggests that practitioners can leverage readily available market data to train DRL agents for effective hedging of options in their portfolios."],"url":"http://arxiv.org/abs/2405.08602v1","category":"q-fin.RM"}
{"created":"2024-05-14 13:38:15","title":"Spectral approximation of convolution operators of Fredholm type","abstract":"We have developed a method for constructing spectral approximations for convolution operators of Fredholm type. The algorithm we propose is numerically stable and takes advantage of the recurrence relations satisfied by the entries of such a matrix approximation. When used for computing the Fredholm convolution of two given functions, such approximations produce the convolution more rapidly than the state-of-the-art methods. The proposed approximation also leads to a spectral method for solving the Fredholm convolution integral equations and enables the computation of eigenvalues and pseudospectra of Fredholm convolution operators, which is otherwise intractable with existing techniques.","sentences":["We have developed a method for constructing spectral approximations for convolution operators of Fredholm type.","The algorithm we propose is numerically stable and takes advantage of the recurrence relations satisfied by the entries of such a matrix approximation.","When used for computing the Fredholm convolution of two given functions, such approximations produce the convolution more rapidly than the state-of-the-art methods.","The proposed approximation also leads to a spectral method for solving the Fredholm convolution integral equations and enables the computation of eigenvalues and pseudospectra of Fredholm convolution operators, which is otherwise intractable with existing techniques."],"url":"http://arxiv.org/abs/2405.08598v1","category":"math.NA"}
{"created":"2024-05-14 12:38:34","title":"A Well-Balanced Method for an Unstaggered Central Scheme, the one-space Dimensional Case","abstract":"In this paper, we propose a new MUSCL scheme by combining the ideas of the Kurganov and Tadmor scheme and the so-called Deviation method which results in a well-balanced finite volume method for the hyperbolic balance laws, by evolving the difference between the exact solution and a given stationary solution. After that, we derive a semi-discrete scheme from this new scheme and it can be shown to be essentially TVD when applied to a scalar conservation law. In the end, we apply and validate the developed methods by numerical experiments and solve classical problems featuring Euler equations with gravitational source term.","sentences":["In this paper, we propose a new MUSCL scheme by combining the ideas of the Kurganov and Tadmor scheme and the so-called Deviation method which results in a well-balanced finite volume method for the hyperbolic balance laws, by evolving the difference between the exact solution and a given stationary solution.","After that, we derive a semi-discrete scheme from this new scheme and it can be shown to be essentially TVD when applied to a scalar conservation law.","In the end, we apply and validate the developed methods by numerical experiments and solve classical problems featuring Euler equations with gravitational source term."],"url":"http://arxiv.org/abs/2405.08549v1","category":"math.NA"}
{"created":"2024-05-14 12:31:10","title":"On the characterization of Riemannian warped product Einstein metrics","abstract":"We present a series of results, including local characterizations of $(\\lambda,m+n)$-Einstein metrics in the context of warped product Einstein spaces. Using these local properties, we restate already known global characterizations of $(\\lambda,m+n)$-Einstein manifolds from He, Petersen and Wylie.","sentences":["We present a series of results, including local characterizations of $(\\lambda,m+n)$-Einstein metrics in the context of warped product Einstein spaces.","Using these local properties, we restate already known global characterizations of $(\\lambda,m+n)$-Einstein manifolds from He, Petersen and Wylie."],"url":"http://arxiv.org/abs/2405.08544v1","category":"math.DG"}
{"created":"2024-05-14 11:37:14","title":"Subspace method based on neural networks for solving the partial differential equation in weak form","abstract":"We present a subspace method based on neural networks for solving the partial differential equation in weak form with high accuracy. The basic idea of our method is to use some functions based on neural networks as base functions to span a subspace, then find an approximate solution in this subspace. Training base functions and finding an approximate solution can be separated, that is different methods can be used to train these base functions, and different methods can also be used to find an approximate solution. In this paper, we find an approximate solution of the partial differential equation in the weak form. Our method can achieve high accuracy with low cost of training. Numerical examples show that the cost of training these base functions is low, and only one hundred to two thousand epochs are needed for most tests. The error of our method can fall below the level of $10^{-7}$ for some tests. The proposed method has the better performance in terms of the accuracy and computational cost.","sentences":["We present a subspace method based on neural networks for solving the partial differential equation in weak form with high accuracy.","The basic idea of our method is to use some functions based on neural networks as base functions to span a subspace, then find an approximate solution in this subspace.","Training base functions and finding an approximate solution can be separated, that is different methods can be used to train these base functions, and different methods can also be used to find an approximate solution.","In this paper, we find an approximate solution of the partial differential equation in the weak form.","Our method can achieve high accuracy with low cost of training.","Numerical examples show that the cost of training these base functions is low, and only one hundred to two thousand epochs are needed for most tests.","The error of our method can fall below the level of $10^{-7}$ for some tests.","The proposed method has the better performance in terms of the accuracy and computational cost."],"url":"http://arxiv.org/abs/2405.08513v1","category":"math.NA"}
{"created":"2024-05-14 11:19:03","title":"Markoff-Fibonacci m-triples","abstract":"We classify all solution triples with Fibonacci components to the equation $a^2+b^2+c^2=3abc+m,$ for positive $m$. We show that for $m=2$ they are precisely $(1,F(b),F(b+2))$, with even $b$; for $m=21$, there exist exactly two Fibonacci solutions $(1,2,8)$ and $(2,2,13)$ and for any other $m$ there exists at most one Fibonacci solution, which, in case it exists, is always minimal (i.e. it is a root of a Markoff tree). Moreover, we show that there is an infinite number of values of $m$ admitting exactly one such solution.","sentences":["We classify all solution triples with Fibonacci components to the equation $a^2+b^2+c^2=3abc+m,$ for positive $m$. We show that for $m=2$ they are precisely $(1,F(b),F(b+2))$, with even $b$; for $m=21$, there exist exactly two Fibonacci solutions $(1,2,8)$ and $(2,2,13)$ and for any other $m$ there exists at most one Fibonacci solution, which, in case it exists, is always minimal (i.e. it is a root of a Markoff tree).","Moreover, we show that there is an infinite number of values of $m$ admitting exactly one such solution."],"url":"http://arxiv.org/abs/2405.08509v1","category":"math.NT"}
{"created":"2024-05-14 11:19:00","title":"Tau polarization in neutrino-nucleus interactions at the LHC energy range","abstract":"Considering that the study of neutrino-nucleus interactions with incident neutrino energy ranges in the GeV-TeV range is feasible at the Large Hadron Collider, we investigate in this paper the degree of polarization ${\\cal{P}}$ of the (anti) tau lepton produced in (anti) tau neutrino - tungsten interactions. We estimate the differential cross-sections and the longitudinal and transverse components of the tau lepton polarization as a function of the tau lepton energy and distinct values of the scattering angle, assuming different values for the energy of the incoming (anti) tau neutrino. Different models for the treatment of the nuclear effects in the parton distribution functions are assumed as input in the calculations. Our results indicate that $ {\\cal{P}} < 1$ for the neutrino energies reached at the LHC and are almost insensitive to the nuclear effects.","sentences":["Considering that the study of neutrino-nucleus interactions with incident neutrino energy ranges in the GeV-TeV range is feasible at the Large Hadron Collider, we investigate in this paper the degree of polarization ${\\cal{P}}$ of the (anti) tau lepton produced in (anti) tau neutrino - tungsten interactions.","We estimate the differential cross-sections and the longitudinal and transverse components of the tau lepton polarization as a function of the tau lepton energy and distinct values of the scattering angle, assuming different values for the energy of the incoming (anti) tau neutrino.","Different models for the treatment of the nuclear effects in the parton distribution functions are assumed as input in the calculations.","Our results indicate that $ {\\cal{P}} < 1$ for the neutrino energies reached at the LHC and are almost insensitive to the nuclear effects."],"url":"http://arxiv.org/abs/2405.08508v1","category":"hep-ph"}
{"created":"2024-05-14 10:29:57","title":"The ring of differential operators on a monomial curve is a Hopf algebroid","abstract":"This article considers cuspidal curves whose coordinate rings are numerical semigroup algebras. Their rings of differential operators are shown to be cocommutative and conilpotent left Hopf algebroids. If the semigroups are symmetric so that the curves are Gorenstein, they are full Hopf algebroids (admit an antipode).","sentences":["This article considers cuspidal curves whose coordinate rings are numerical semigroup algebras.","Their rings of differential operators are shown to be cocommutative and conilpotent left Hopf algebroids.","If the semigroups are symmetric so that the curves are Gorenstein, they are full Hopf algebroids (admit an antipode)."],"url":"http://arxiv.org/abs/2405.08490v1","category":"math.QA"}
{"created":"2024-05-14 09:32:19","title":"Velocity-vorticity geometric constraints for the energy conservation of 3D ideal incompressible fluids","abstract":"In this paper we consider the 3D Euler equations and we first prove a criterion for energy conservation for weak solutions with velocity satisfying additional assumptions in fractional Sobolev spaces with respect to the space variables, balanced by proper integrability with respect to time. Next, we apply the criterion to study the energy conservation of solution of the Beltrami type, carefully applying properties of products in (fractional and possibly negative) Sobolev spaces and employing a suitable bootstrap argument.","sentences":["In this paper we consider the 3D Euler equations and we first prove a criterion for energy conservation for weak solutions with velocity satisfying additional assumptions in fractional Sobolev spaces with respect to the space variables, balanced by proper integrability with respect to time.","Next, we apply the criterion to study the energy conservation of solution of the Beltrami type, carefully applying properties of products in (fractional and possibly negative)","Sobolev spaces and employing a suitable bootstrap argument."],"url":"http://arxiv.org/abs/2405.08461v1","category":"math.AP"}
{"created":"2024-05-14 09:11:17","title":"A mean curvature type flow with capillary boundary in a horoball in hyperbolic space","abstract":"In this paper, we study a mean curvature type flow with capillary boundary in a horoball in hyperbolic space. Our flow preserves the volume of the bounded domain enclosed by the hypersurface and monotonically decreases the energy functional. We show that it has the long time existence and converges to a truncated umbilical hypersurface in hyperbolic space. As an application, we solve an isoperimetric type problem for hypersurfaces with capillary boundary in a horoball.","sentences":["In this paper, we study a mean curvature type flow with capillary boundary in a horoball in hyperbolic space.","Our flow preserves the volume of the bounded domain enclosed by the hypersurface and monotonically decreases the energy functional.","We show that it has the long time existence and converges to a truncated umbilical hypersurface in hyperbolic space.","As an application, we solve an isoperimetric type problem for hypersurfaces with capillary boundary in a horoball."],"url":"http://arxiv.org/abs/2405.08446v1","category":"math.DG"}
{"created":"2024-05-14 08:45:46","title":"Conformal product structures on compact K\u00e4hler manifolds","abstract":"A conformal product structure on a Riemannian manifold is a Weyl connection with reducible holonomy. We give the geometric description of all compact K\\\"ahler manifolds admitting conformal product structures","sentences":["A conformal product structure on a Riemannian manifold is a Weyl connection with reducible holonomy.","We give the geometric description of all compact K\\\"ahler manifolds admitting conformal product structures"],"url":"http://arxiv.org/abs/2405.08430v1","category":"math.DG"}
{"created":"2024-05-14 08:45:34","title":"TEDNet: Twin Encoder Decoder Neural Network for 2D Camera and LiDAR Road Detection","abstract":"Robust road surface estimation is required for autonomous ground vehicles to navigate safely. Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments. In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface. Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures. Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset. Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface. The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications.","sentences":["Robust road surface estimation is required for autonomous ground vehicles to navigate safely.","Despite it becoming one of the main targets for autonomous mobility researchers in recent years, it is still an open problem in which cameras and LiDAR sensors have demonstrated to be adequate to predict the position, size and shape of the road a vehicle is driving on in different environments.","In this work, a novel Convolutional Neural Network model is proposed for the accurate estimation of the roadway surface.","Furthermore, an ablation study has been conducted to investigate how different encoding strategies affect model performance, testing 6 slightly different neural network architectures.","Our model is based on the use of a Twin Encoder-Decoder Neural Network (TEDNet) for independent camera and LiDAR feature extraction, and has been trained and evaluated on the Kitti-Road dataset.","Bird's Eye View projections of the camera and LiDAR data are used in this model to perform semantic segmentation on whether each pixel belongs to the road surface.","The proposed method performs among other state-of-the-art methods and operates at the same frame-rate as the LiDAR and cameras, so it is adequate for its use in real-time applications."],"url":"http://arxiv.org/abs/2405.08429v1","category":"cs.CV"}
{"created":"2024-05-14 08:06:28","title":"Classification of closed conformally flat Lorentzian manifolds with unipotent holonomy","abstract":"We classify closed, conformally flat Lorentzian manifolds of dimension $n \\geq 3$ with unipotent holonomy in PO(2,n). They are all Kleinian and fall into four different geometric types according to the intersection of the image of the developing map with a holonomy-invariant isotropic flag. They are homeomorphic to $S^{n-1} \\times S^1$ or a nilmanifold of degree at most three, up to a finite cover. We classify those admitting an essential conformal flow; these fall into two geometric types, both homeomorphic to $S^{n-1} \\times S^1$ up to finite cover.","sentences":["We classify closed, conformally flat Lorentzian manifolds of dimension $n \\geq 3$ with unipotent holonomy in PO(2,n).","They are all Kleinian and fall into four different geometric types according to the intersection of the image of the developing map with a holonomy-invariant isotropic flag.","They are homeomorphic to $S^{n-1} \\times S^1$ or a nilmanifold of degree at most three, up to a finite cover.","We classify those admitting an essential conformal flow; these fall into two geometric types, both homeomorphic to $S^{n-1} \\times S^1$ up to finite cover."],"url":"http://arxiv.org/abs/2405.08410v1","category":"math.DG"}
{"created":"2024-05-14 06:41:54","title":"Laser-driven shock compression and equation of state of Fe$_2$O$_3$ up to 700 GPa","abstract":"We report here the first equation of state measurements of Fe$_2$O$_3$ obtained with laser-driven shock compression. The data are in excellent agreement with previous dynamic and static compression measurements at low pressure, and extend the known Hugoniot up to 700 GPa. We observe a large volume drop of $\\sim$10% at 86 GPa, which could be associated, according to static compression observations, with the iron spin transition. Our measurements also suggest a change of the Hugoniot curve between 150 and 250 GPa. Above 250 GPa and within our error bars, we do not observe significant modifications up to the maximum pressure of 700 GPa reached in our experiment.","sentences":["We report here the first equation of state measurements of Fe$_2$O$_3$ obtained with laser-driven shock compression.","The data are in excellent agreement with previous dynamic and static compression measurements at low pressure, and extend the known Hugoniot up to 700 GPa.","We observe a large volume drop of $\\sim$10% at 86 GPa, which could be associated, according to static compression observations, with the iron spin transition.","Our measurements also suggest a change of the Hugoniot curve between 150 and 250 GPa.","Above 250 GPa and within our error bars, we do not observe significant modifications up to the maximum pressure of 700 GPa reached in our experiment."],"url":"http://arxiv.org/abs/2405.08350v1","category":"hep-ex"}
{"created":"2024-05-14 06:26:58","title":"Achieving Resolution-Agnostic DNN-based Image Watermarking:A Novel Perspective of Implicit Neural Representation","abstract":"DNN-based watermarking methods are rapidly developing and delivering impressive performances. Recent advances achieve resolution-agnostic image watermarking by reducing the variant resolution watermarking problem to a fixed resolution watermarking problem. However, such a reduction process can potentially introduce artifacts and low robustness. To address this issue, we propose the first, to the best of our knowledge, Resolution-Agnostic Image WaterMarking (RAIMark) framework by watermarking the implicit neural representation (INR) of image. Unlike previous methods, our method does not rely on the previous reduction process by directly watermarking the continuous signal instead of image pixels, thus achieving resolution-agnostic watermarking. Precisely, given an arbitrary-resolution image, we fit an INR for the target image. As a continuous signal, such an INR can be sampled to obtain images with variant resolutions. Then, we quickly fine-tune the fitted INR to get a watermarked INR conditioned on a binary secret message. A pre-trained watermark decoder extracts the hidden message from any sampled images with arbitrary resolutions. By directly watermarking INR, we achieve resolution-agnostic watermarking with increased robustness. Extensive experiments show that our method outperforms previous methods with significant improvements: averagely improved bit accuracy by 7%$\\sim$29%. Notably, we observe that previous methods are vulnerable to at least one watermarking attack (e.g. JPEG, crop, resize), while ours are robust against all watermarking attacks.","sentences":["DNN-based watermarking methods are rapidly developing and delivering impressive performances.","Recent advances achieve resolution-agnostic image watermarking by reducing the variant resolution watermarking problem to a fixed resolution watermarking problem.","However, such a reduction process can potentially introduce artifacts and low robustness.","To address this issue, we propose the first, to the best of our knowledge, Resolution-Agnostic Image WaterMarking (RAIMark) framework by watermarking the implicit neural representation (INR) of image.","Unlike previous methods, our method does not rely on the previous reduction process by directly watermarking the continuous signal instead of image pixels, thus achieving resolution-agnostic watermarking.","Precisely, given an arbitrary-resolution image, we fit an INR for the target image.","As a continuous signal, such an INR can be sampled to obtain images with variant resolutions.","Then, we quickly fine-tune the fitted INR to get a watermarked INR conditioned on a binary secret message.","A pre-trained watermark decoder extracts the hidden message from any sampled images with arbitrary resolutions.","By directly watermarking INR, we achieve resolution-agnostic watermarking with increased robustness.","Extensive experiments show that our method outperforms previous methods with significant improvements: averagely improved bit accuracy by 7%$\\sim$29%.","Notably, we observe that previous methods are vulnerable to at least one watermarking attack (e.g. JPEG, crop, resize), while ours are robust against all watermarking attacks."],"url":"http://arxiv.org/abs/2405.08340v1","category":"cs.CR"}
{"created":"2024-05-14 03:58:08","title":"Second-Order Bi-Scalar-Tensor Field Equations in a Space of Four-Dimensions","abstract":"Lagrange scalar densities which are concomitants of two scalar fields, a pseudo-Riemannian metric tensor, and their derivatives of arbitrary differential order are investigated in a space of four-dimensions. The most general second-order Euler-Lagrange tensor densities derivable from such a Lagrangian are constructed. It is demonstrated that these second-order Euler-Lagrange tensor densities can be derived from a set of four Lagrangians which are at most of second-order. These Lagrangians will have a total of six scalar coefficients, each of which is a concomitant of five variables: the two scalar fields, and the three inner products of the gradients of the two scalar fields. Of these six coefficient functions only one is arbitrary, while the other five must satisfy linear partial differential equations. These non-arbitrary scalar functions break up into three groups: two groups of two, and one single function, with different groups appearing in different Lagrangians. Surprisingly each of these five functions give rise to solutions to the wave equation in three-dimensional Minkowski space.","sentences":["Lagrange scalar densities which are concomitants of two scalar fields, a pseudo-Riemannian metric tensor, and their derivatives of arbitrary differential order are investigated in a space of four-dimensions.","The most general second-order Euler-Lagrange tensor densities derivable from such a Lagrangian are constructed.","It is demonstrated that these second-order Euler-Lagrange tensor densities can be derived from a set of four Lagrangians which are at most of second-order.","These Lagrangians will have a total of six scalar coefficients, each of which is a concomitant of five variables: the two scalar fields, and the three inner products of the gradients of the two scalar fields.","Of these six coefficient functions only one is arbitrary, while the other five must satisfy linear partial differential equations.","These non-arbitrary scalar functions break up into three groups: two groups of two, and one single function, with different groups appearing in different Lagrangians.","Surprisingly each of these five functions give rise to solutions to the wave equation in three-dimensional Minkowski space."],"url":"http://arxiv.org/abs/2405.08303v1","category":"gr-qc"}
{"created":"2024-05-14 03:49:14","title":"Differentially Private Federated Learning: A Systematic Review","abstract":"In recent years, privacy and security concerns in machine learning have promoted trusted federated learning to the forefront of research. Differential privacy has emerged as the de facto standard for privacy protection in federated learning due to its rigorous mathematical foundation and provable guarantee. Despite extensive research on algorithms that incorporate differential privacy within federated learning, there remains an evident deficiency in systematic reviews that categorize and synthesize these studies.   Our work presents a systematic overview of the differentially private federated learning. Existing taxonomies have not adequately considered objects and level of privacy protection provided by differential privacy in federated learning. To rectify this gap, we propose a new taxonomy of differentially private federated learning based on definition and guarantee of differential privacy and federated scenarios. Our classification allows for a clear delineation of the protected objects across various differential privacy models and their respective neighborhood levels within federated learning environments. Furthermore, we explore the applications of differential privacy in federated learning scenarios. Our findings provide valuable insights into privacy-preserving federated learning and suggest practical directions for future research.","sentences":["In recent years, privacy and security concerns in machine learning have promoted trusted federated learning to the forefront of research.","Differential privacy has emerged as the de facto standard for privacy protection in federated learning due to its rigorous mathematical foundation and provable guarantee.","Despite extensive research on algorithms that incorporate differential privacy within federated learning, there remains an evident deficiency in systematic reviews that categorize and synthesize these studies.   ","Our work presents a systematic overview of the differentially private federated learning.","Existing taxonomies have not adequately considered objects and level of privacy protection provided by differential privacy in federated learning.","To rectify this gap, we propose a new taxonomy of differentially private federated learning based on definition and guarantee of differential privacy and federated scenarios.","Our classification allows for a clear delineation of the protected objects across various differential privacy models and their respective neighborhood levels within federated learning environments.","Furthermore, we explore the applications of differential privacy in federated learning scenarios.","Our findings provide valuable insights into privacy-preserving federated learning and suggest practical directions for future research."],"url":"http://arxiv.org/abs/2405.08299v1","category":"cs.CR"}
{"created":"2024-05-14 03:27:09","title":"Hybrid Event-Frame Neural Spike Detector for Neuromorphic Implantable BMI","abstract":"This work introduces two novel neural spike detection schemes intended for use in next-generation neuromorphic brain-machine interfaces (iBMIs). The first, an Event-based Spike Detector (Ev-SPD) which examines the temporal neighborhood of a neural event for spike detection, is designed for in-vivo processing and offers high sensitivity and decent accuracy (94-97%). The second, Neural Network-based Spike Detector (NN-SPD) which operates on hybrid temporal event frames, provides an off-implant solution using shallow neural networks with impressive detection accuracy (96-99%) and minimal false detections. These methods are evaluated using a synthetic dataset with varying noise levels and validated through comparison with ground truth data. The results highlight their potential in next-gen neuromorphic iBMI systems and emphasize the need to explore this direction further to understand their resource-efficient and high-performance capabilities for practical iBMI settings.","sentences":["This work introduces two novel neural spike detection schemes intended for use in next-generation neuromorphic brain-machine interfaces (iBMIs).","The first, an Event-based Spike Detector (Ev-SPD) which examines the temporal neighborhood of a neural event for spike detection, is designed for in-vivo processing and offers high sensitivity and decent accuracy (94-97%).","The second, Neural Network-based Spike Detector (NN-SPD) which operates on hybrid temporal event frames, provides an off-implant solution using shallow neural networks with impressive detection accuracy (96-99%) and minimal false detections.","These methods are evaluated using a synthetic dataset with varying noise levels and validated through comparison with ground truth data.","The results highlight their potential in next-gen neuromorphic iBMI systems and emphasize the need to explore this direction further to understand their resource-efficient and high-performance capabilities for practical iBMI settings."],"url":"http://arxiv.org/abs/2405.08292v1","category":"eess.SP"}
{"created":"2024-05-14 02:51:35","title":"Future Trends in the Design of Memetic Algorithms: the Case of the Linear Ordering Problem","abstract":"The way heuristic optimizers are designed has evolved over the decades, as computing power has increased. Initially, trajectory metaheuristics used to shape the state of the art in many problems, whereas today, population-based mechanisms tend to be more effective.Such has been the case for the Linear Ordering Problem (LOP), a field in which strategies such as Iterated Local Search and Variable Neighborhood Search led the way during the 1990s, but which have now been surpassed by evolutionary and memetic schemes. This paper focuses on understanding how the design of LOP optimizers will change in the future, as computing power continues to increase, yielding two main contributions. On the one hand, a metaheuristic was designed that is capable of effectively exploiting a large amount of computational resources, specifically, computing power equivalent to what a recent core can output during runs lasting over four months. Our analysis of this aspect relied on parallelization, and allowed us to conclude that as the power of the computational resources increases, it will be necessary to boost the capacities of the intensification methods applied in the memetic algorithms to keep the population from stagnating. And on the other, the best-known results for today's most challenging set of instances (xLOLIB2) were significantly outperformed. Instances with sizes ranging from 300 to 1000 were analyzed, and new bounds were established that provide a frame of reference for future research.","sentences":["The way heuristic optimizers are designed has evolved over the decades, as computing power has increased.","Initially, trajectory metaheuristics used to shape the state of the art in many problems, whereas today, population-based mechanisms tend to be more effective.","Such has been the case for the Linear Ordering Problem (LOP), a field in which strategies such as Iterated Local Search and Variable Neighborhood Search led the way during the 1990s, but which have now been surpassed by evolutionary and memetic schemes.","This paper focuses on understanding how the design of LOP optimizers will change in the future, as computing power continues to increase, yielding two main contributions.","On the one hand, a metaheuristic was designed that is capable of effectively exploiting a large amount of computational resources, specifically, computing power equivalent to what a recent core can output during runs lasting over four months.","Our analysis of this aspect relied on parallelization, and allowed us to conclude that as the power of the computational resources increases, it will be necessary to boost the capacities of the intensification methods applied in the memetic algorithms to keep the population from stagnating.","And on the other, the best-known results for today's most challenging set of instances (xLOLIB2) were significantly outperformed.","Instances with sizes ranging from 300 to 1000 were analyzed, and new bounds were established that provide a frame of reference for future research."],"url":"http://arxiv.org/abs/2405.08285v1","category":"cs.NE"}
{"created":"2024-05-14 02:32:30","title":"Parallel-in-Time Iterative Methods for Pricing American Options","abstract":"For pricing American options, %after suitable discretization in space and time, a sequence of discrete linear complementarity problems (LCPs) or equivalently Hamilton-Jacobi-Bellman (HJB) equations need to be solved in a sequential time-stepping manner. In each time step, the policy iteration or its penalty variant is often applied due to their fast convergence rates. In this paper, we aim to solve for all time steps simultaneously, by applying the policy iteration to an ``all-at-once form\" of the HJB equations, where two different parallel-in-time preconditioners are proposed to accelerate the solution of the linear systems within the policy iteration. Our proposed methods are generally applicable for such all-at-once forms of the HJB equation, arising from option pricing problems with optimal stopping and nontrivial underlying asset models. Numerical examples are presented to show the feasibility and robust convergence behavior of the proposed methodology.","sentences":["For pricing American options, %after suitable discretization in space and time, a sequence of discrete linear complementarity problems (LCPs) or equivalently Hamilton-Jacobi-Bellman (HJB) equations need to be solved in a sequential time-stepping manner.","In each time step, the policy iteration or its penalty variant is often applied due to their fast convergence rates.","In this paper, we aim to solve for all time steps simultaneously, by applying the policy iteration to an ``all-at-once form\" of the HJB equations, where two different parallel-in-time preconditioners are proposed to accelerate the solution of the linear systems within the policy iteration.","Our proposed methods are generally applicable for such all-at-once forms of the HJB equation, arising from option pricing problems with optimal stopping and nontrivial underlying asset models.","Numerical examples are presented to show the feasibility and robust convergence behavior of the proposed methodology."],"url":"http://arxiv.org/abs/2405.08280v1","category":"math.NA"}
{"created":"2024-05-14 01:12:48","title":"On special properties of solutions to Camassa-Holm equation and related models","abstract":"We study unique continuation properties of solutions to the b-family of equations. This includes the Camassa-Holm and the Degasperi-Procesi models. We prove that for both, the initial value problem and the periodic boundary value problem, the unique continuation results found in \\cite{LiPo} are optimal. More precisely, the result established there for the constant $c_0=0$ fails for any constant $c_0\\neq 0$.","sentences":["We study unique continuation properties of solutions to the b-family of equations.","This includes the Camassa-Holm and the Degasperi-Procesi models.","We prove that for both, the initial value problem and the periodic boundary value problem, the unique continuation results found in \\cite{LiPo} are optimal.","More precisely, the result established there for the constant $c_0=0$ fails for any constant $c_0\\neq 0$."],"url":"http://arxiv.org/abs/2405.08258v1","category":"math.AP"}
{"created":"2024-05-14 01:11:10","title":"Global existence and multiplicity of solutions for logarithmic Schr\u00f6dinger equations on graphs","abstract":"We consider the following logarithmic Schr\\\"{o}dinger equation   $$   -\\Delta u+h(x)u=u\\log u^{2}   $$ on a locally finite graph $G=(V,E)$, where $\\Delta$ is a discrete Laplacian operator on the graph, $h$ is the potential function. Different from the classical methods in Euclidean space, we obtain the existence of global solutions to the equation by using the variational method from local to global, which is inspired by the works of Lin and Yang in \\cite{LinYang}. In addition, when the potential function $h$ is sign-changing, we prove that the equation admits infinitely many solutions with high energy by using the symmetric mountain pass theorem. We extend the classical results in Euclidean space to discrete graphs.","sentences":["We consider the following logarithmic Schr\\\"{o}dinger equation   $$   -\\Delta u+h(x)u=u\\log u^{2}   $$ on a locally finite graph $G=(V,E)$, where $\\Delta$ is a discrete Laplacian operator on the graph, $h$ is the potential function.","Different from the classical methods in Euclidean space, we obtain the existence of global solutions to the equation by using the variational method from local to global, which is inspired by the works of Lin and Yang in \\cite{LinYang}.","In addition, when the potential function $h$ is sign-changing, we prove that the equation admits infinitely many solutions with high energy by using the symmetric mountain pass theorem.","We extend the classical results in Euclidean space to discrete graphs."],"url":"http://arxiv.org/abs/2405.08257v1","category":"math.AP"}
{"created":"2024-05-14 00:51:15","title":"Multimodal Collaboration Networks for Geospatial Vehicle Detection in Dense, Occluded, and Large-Scale Events","abstract":"In large-scale disaster events, the planning of optimal rescue routes depends on the object detection ability at the disaster scene, with one of the main challenges being the presence of dense and occluded objects. Existing methods, which are typically based on the RGB modality, struggle to distinguish targets with similar colors and textures in crowded environments and are unable to identify obscured objects. To this end, we first construct two multimodal dense and occlusion vehicle detection datasets for large-scale events, utilizing RGB and height map modalities. Based on these datasets, we propose a multimodal collaboration network for dense and occluded vehicle detection, MuDet for short. MuDet hierarchically enhances the completeness of discriminable information within and across modalities and differentiates between simple and complex samples. MuDet includes three main modules: Unimodal Feature Hierarchical Enhancement (Uni-Enh), Multimodal Cross Learning (Mul-Lea), and Hard-easy Discriminative (He-Dis) Pattern. Uni-Enh and Mul-Lea enhance the features within each modality and facilitate the cross-integration of features from two heterogeneous modalities. He-Dis effectively separates densely occluded vehicle targets with significant intra-class differences and minimal inter-class differences by defining and thresholding confidence values, thereby suppressing the complex background. Experimental results on two re-labeled multimodal benchmark datasets, the 4K-SAI-LCS dataset, and the ISPRS Potsdam dataset, demonstrate the robustness and generalization of the MuDet. The codes of this work are available openly at \\url{https://github.com/Shank2358/MuDet}.","sentences":["In large-scale disaster events, the planning of optimal rescue routes depends on the object detection ability at the disaster scene, with one of the main challenges being the presence of dense and occluded objects.","Existing methods, which are typically based on the RGB modality, struggle to distinguish targets with similar colors and textures in crowded environments and are unable to identify obscured objects.","To this end, we first construct two multimodal dense and occlusion vehicle detection datasets for large-scale events, utilizing RGB and height map modalities.","Based on these datasets, we propose a multimodal collaboration network for dense and occluded vehicle detection, MuDet for short.","MuDet hierarchically enhances the completeness of discriminable information within and across modalities and differentiates between simple and complex samples.","MuDet includes three main modules: Unimodal Feature Hierarchical Enhancement (Uni-Enh), Multimodal Cross Learning (Mul-Lea), and Hard-easy Discriminative (He-Dis) Pattern.","Uni-Enh and Mul-Lea enhance the features within each modality and facilitate the cross-integration of features from two heterogeneous modalities.","He-Dis effectively separates densely occluded vehicle targets with significant intra-class differences and minimal inter-class differences by defining and thresholding confidence values, thereby suppressing the complex background.","Experimental results on two re-labeled multimodal benchmark datasets, the 4K-SAI-LCS dataset, and the ISPRS Potsdam dataset, demonstrate the robustness and generalization of the MuDet.","The codes of this work are available openly at \\url{https://github.com/Shank2358/MuDet}."],"url":"http://arxiv.org/abs/2405.08251v1","category":"cs.CV"}
{"created":"2024-05-13 23:36:19","title":"A predictive learning model can simulate temporal dynamics and context effects found in neural representations of continuous speech","abstract":"Speech perception involves storing and integrating sequentially presented items. Recent work in cognitive neuroscience has identified temporal and contextual characteristics in humans' neural encoding of speech that may facilitate this temporal processing. In this study, we simulated similar analyses with representations extracted from a computational model that was trained on unlabelled speech with the learning objective of predicting upcoming acoustics. Our simulations revealed temporal dynamics similar to those in brain signals, implying that these properties can arise without linguistic knowledge. Another property shared between brains and the model is that the encoding patterns of phonemes support some degree of cross-context generalization. However, we found evidence that the effectiveness of these generalizations depends on the specific contexts, which suggests that this analysis alone is insufficient to support the presence of context-invariant encoding.","sentences":["Speech perception involves storing and integrating sequentially presented items.","Recent work in cognitive neuroscience has identified temporal and contextual characteristics in humans' neural encoding of speech that may facilitate this temporal processing.","In this study, we simulated similar analyses with representations extracted from a computational model that was trained on unlabelled speech with the learning objective of predicting upcoming acoustics.","Our simulations revealed temporal dynamics similar to those in brain signals, implying that these properties can arise without linguistic knowledge.","Another property shared between brains and the model is that the encoding patterns of phonemes support some degree of cross-context generalization.","However, we found evidence that the effectiveness of these generalizations depends on the specific contexts, which suggests that this analysis alone is insufficient to support the presence of context-invariant encoding."],"url":"http://arxiv.org/abs/2405.08237v1","category":"cs.CL"}
{"created":"2024-05-13 22:54:17","title":"On the single versus the repetitive Penrose process in a Kerr black hole","abstract":"Extracting the rotational energy from a Kerr black hole (BH) is one of the crucial topics in relativistic astrophysics. Here, we give special attention to the Penrose ballistic process based on the fission of a massive particle $\\mu_0$ into two particles $\\mu_1$ and $\\mu_2$, occurring in the ergosphere of a Kerr BH. Bardeen et al. indicated that for the process to occur, some additional \"{\\it hydrodynamical forces or superstrong radiation reactions}\" were needed. This idea was further expanded by Wald and Chandrasekhar. This animosity convinced T. Piran and collaborators to move from a simple three-body system characterizing the original Penrose process to a many-body system. This many-body approach was further largely expanded by others, introducing additional processes, some questionable in their validity. In this letter, we return to the simplest original Penrose process and show that the solution of the equations of motion, imposing the turning point condition on their trajectories, leads to the rotational energy extraction from the BH expected by Penrose. The efficiency of energy extraction by a single process is precisely quantified for three different single decay processes occurring respectively at $r=1.2 M$, $r=1.5 M$, and $r=1.9 M$. An interesting repetitive model has been proposed by Misner, Thorne \\& Wheeler. Indeed, it would appear that a repetitive sequence of $246$ decays of the above injection process at $r=1.2 M$ and the corresponding ones at $r=1.5 M$ and $r=1.9 M$ could extract $100\\%$ of the rotational energy of the BH. The accompanying paper shows that accounting for the irreducible mass introduces a non-linear approach that avoids violating energy conservation, leading to new energy extraction processes and demonstrating the impossibility of extracting the whole BH rotational energy by a sequence of Penrose processes.","sentences":["Extracting the rotational energy from a Kerr black hole (BH) is one of the crucial topics in relativistic astrophysics.","Here, we give special attention to the Penrose ballistic process based on the fission of a massive particle $\\mu_0$ into two particles $\\mu_1$ and $\\mu_2$, occurring in the ergosphere of a Kerr BH.","Bardeen et al. indicated that for the process to occur, some additional \"{\\it hydrodynamical forces or superstrong radiation reactions}\" were needed.","This idea was further expanded by Wald and Chandrasekhar.","This animosity convinced T. Piran and collaborators to move from a simple three-body system characterizing the original Penrose process to a many-body system.","This many-body approach was further largely expanded by others, introducing additional processes, some questionable in their validity.","In this letter, we return to the simplest original Penrose process and show that the solution of the equations of motion, imposing the turning point condition on their trajectories, leads to the rotational energy extraction from the BH expected by Penrose.","The efficiency of energy extraction by a single process is precisely quantified for three different single decay processes occurring respectively at $r=1.2 M$, $r=1.5 M$, and $r=1.9 M$. An interesting repetitive model has been proposed by Misner, Thorne \\& Wheeler.","Indeed, it would appear that a repetitive sequence of $246$ decays of the above injection process at $r=1.2 M$ and the corresponding ones at $r=1.5 M$ and $r=1.9 M$ could extract $100\\%$ of the rotational energy of the BH.","The accompanying paper shows that accounting for the irreducible mass introduces a non-linear approach that avoids violating energy conservation, leading to new energy extraction processes and demonstrating the impossibility of extracting the whole BH rotational energy by a sequence of Penrose processes."],"url":"http://arxiv.org/abs/2405.08229v1","category":"gr-qc"}
{"created":"2024-05-13 21:57:01","title":"Macroscopic Fluctuation Theory for Ginzburg-Landau dynamics with long range interactions","abstract":"Focusing on a famous class of interacting diffusion processes called Ginzburg-Landau (GL) dynamics, we extend the Macroscopic Fluctuations Theory (MFT) to these systems in the case where the interactions are long-range, and consequently, the macroscopic effective equations are described by non-linear fractional diffusion equations.","sentences":["Focusing on a famous class of interacting diffusion processes called Ginzburg-Landau (GL) dynamics, we extend the Macroscopic Fluctuations Theory (MFT) to these systems in the case where the interactions are long-range, and consequently, the macroscopic effective equations are described by non-linear fractional diffusion equations."],"url":"http://arxiv.org/abs/2405.08212v1","category":"math-ph"}
{"created":"2024-05-13 21:12:31","title":"Barren plateaus induced by the dimension of qudits","abstract":"Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantages in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the gradient vanishing problem, commonly referred to as barren plateaus. In this study, we unveil a direct correlation between the dimension of qudits and the occurrence of barren plateaus, a connection previously overlooked. Through meticulous analysis, we demonstrate that existing literature implicitly suggests the intrinsic influence of qudit dimensionality on barren plateaus. To instantiate these findings, we present numerical results that exemplify the impact of qudit dimensionality on barren plateaus. Additionally, despite the proposition of various error mitigation techniques, our results call for further scrutiny about their efficacy in the context of VQAs with qudits.","sentences":["Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantages in diverse scientific and technological domains, notably within Quantum Neural Networks.","However, despite their potential, VQAs encounter significant obstacles, chief among them being the gradient vanishing problem, commonly referred to as barren plateaus.","In this study, we unveil a direct correlation between the dimension of qudits and the occurrence of barren plateaus, a connection previously overlooked.","Through meticulous analysis, we demonstrate that existing literature implicitly suggests the intrinsic influence of qudit dimensionality on barren plateaus.","To instantiate these findings, we present numerical results that exemplify the impact of qudit dimensionality on barren plateaus.","Additionally, despite the proposition of various error mitigation techniques, our results call for further scrutiny about their efficacy in the context of VQAs with qudits."],"url":"http://arxiv.org/abs/2405.08190v1","category":"quant-ph"}
{"created":"2024-05-13 21:10:46","title":"Undisturbed velocity recovery with transient and weak inertia effects in volume-filtered simulations of particle-laden flows","abstract":"In volume-filtered Euler-Lagrange simulations of particle-laden flows, the fluid forces acting on a particle are estimated using reduced models, which rely on the knowledge of the local undisturbed flow for that particle. Since the two-way coupling between the particle and the fluid creates a local flow perturbation, the filtered fluid velocity interpolated to the particle location must be corrected prior to estimating the fluid forces, so as to subtract the contribution of this perturbation and recover the local undisturbed flow with good accuracy. In this manuscript, we present a new model for estimating a particle's self-induced flow disturbance that accounts for its transient development and for inertial effects related to finite particle Reynolds numbers. The model also does not require the direction of the momentum feedback to align with the direction of the particle's relative velocity, allowing force contributions other than the steady drag force to be considered. It is based upon the linearization of the volume-filtered equations governing the particle's self-induced flow disturbance, such that their solution can be expressed as a linear combination of regularized transient Stokeslet contributions. Tested on a range of numerical cases, the model is shown to consistently estimate the particle's self-induced flow disturbance with high accuracy both in steady and highly transient flow environments, as well as for finite particle Reynolds numbers.","sentences":["In volume-filtered Euler-Lagrange simulations of particle-laden flows, the fluid forces acting on a particle are estimated using reduced models, which rely on the knowledge of the local undisturbed flow for that particle.","Since the two-way coupling between the particle and the fluid creates a local flow perturbation, the filtered fluid velocity interpolated to the particle location must be corrected prior to estimating the fluid forces, so as to subtract the contribution of this perturbation and recover the local undisturbed flow with good accuracy.","In this manuscript, we present a new model for estimating a particle's self-induced flow disturbance that accounts for its transient development and for inertial effects related to finite particle Reynolds numbers.","The model also does not require the direction of the momentum feedback to align with the direction of the particle's relative velocity, allowing force contributions other than the steady drag force to be considered.","It is based upon the linearization of the volume-filtered equations governing the particle's self-induced flow disturbance, such that their solution can be expressed as a linear combination of regularized transient Stokeslet contributions.","Tested on a range of numerical cases, the model is shown to consistently estimate the particle's self-induced flow disturbance with high accuracy both in steady and highly transient flow environments, as well as for finite particle Reynolds numbers."],"url":"http://arxiv.org/abs/2405.08188v1","category":"physics.comp-ph"}
{"created":"2024-05-13 21:07:56","title":"Metric lines in Engel-type groups and the nilpotent group $N_{6,3,1}$","abstract":"Given a sub-Riemannian manifold, which geodesics are \"metric lines\" (i.e. globally minimizing geodesics)? This article takes the first steps in answering this question for \"arbitrary rank\" and \"non-integrable\" Carnot groups. We classify the metric lines of the Engel-type groups $Eng(n)$ (Theorem B) and give a partial classification for the group of four-by-four nilpotent triangular matrices $N_{6,3,1}$ (Theorem C). The sub-Riamannian structure of the former group is defined on a non-integrable distribution of rank $n+1$ and the geodesic flow of the latter group is not algebraically integrable.","sentences":["Given a sub-Riemannian manifold, which geodesics are \"metric lines\" (i.e. globally minimizing geodesics)?","This article takes the first steps in answering this question for \"arbitrary rank\" and \"non-integrable\" Carnot groups.","We classify the metric lines of the Engel-type groups $Eng(n)$ (Theorem B) and give a partial classification for the group of four-by-four nilpotent triangular matrices $N_{6,3,1}$ (Theorem C).","The sub-Riamannian structure of the former group is defined on a non-integrable distribution of rank $n+1$ and the geodesic flow of the latter group is not algebraically integrable."],"url":"http://arxiv.org/abs/2405.08186v1","category":"math.DG"}
{"created":"2024-05-13 21:04:44","title":"Swallow-tail dispersions of moving solitons in a two-dimensional fermionic superfluid","abstract":"Soliton-like localised wave solutions in a two-dimensional Fermi superfluid are studied by solving the Bogoliubov-de Gennes equations in the BCS regime of weak pairing interactions. The dispersion relations of these solitons are found to exhibit a peculiar swallow-tail shape, with cusps and multiple branches. The effective mass of the solitons is found to diverge and change sign at the cusp. This behavior is in contrast to the smooth dispersion relations and negative effective masses of solitons in the three-dimensional Fermi superfluid. The swallow-tail dispersion relations are shown to be a consequence of counterflow of the superfluid and sign-changing contributions to the superfluid current from different transverse momenta in the Bogoliubov-de Gennes formalism. The results are relevant for the understanding of solitonic excitations in two-dimensional Fermi superfluids, such as ultracold atomic gases and high-temperature superconductors.","sentences":["Soliton-like localised wave solutions in a two-dimensional Fermi superfluid are studied by solving the Bogoliubov-de Gennes equations in the BCS regime of weak pairing interactions.","The dispersion relations of these solitons are found to exhibit a peculiar swallow-tail shape, with cusps and multiple branches.","The effective mass of the solitons is found to diverge and change sign at the cusp.","This behavior is in contrast to the smooth dispersion relations and negative effective masses of solitons in the three-dimensional Fermi superfluid.","The swallow-tail dispersion relations are shown to be a consequence of counterflow of the superfluid and sign-changing contributions to the superfluid current from different transverse momenta in the Bogoliubov-de Gennes formalism.","The results are relevant for the understanding of solitonic excitations in two-dimensional Fermi superfluids, such as ultracold atomic gases and high-temperature superconductors."],"url":"http://arxiv.org/abs/2405.08184v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-13 20:56:05","title":"Parameter identifiability, parameter estimation and model prediction for differential equation models","abstract":"Interpreting data with mathematical models is an important aspect of real-world applied mathematical modeling. Very often we are interested to understand the extent to which a particular data set informs and constrains model parameters. This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions. Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modelling frameworks. The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub.","sentences":["Interpreting data with mathematical models is an important aspect of real-world applied mathematical modeling.","Very often we are interested to understand the extent to which a particular data set informs and constrains model parameters.","This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions.","Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modelling frameworks.","The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub."],"url":"http://arxiv.org/abs/2405.08177v1","category":"stat.ME"}
{"created":"2024-05-13 20:29:50","title":"Twistor theory of the Chen--Teo gravitational instanton","abstract":"Toric Ricci--flat metrics in dimension four correspond to certain holomorphic vector bundles over a twistor space. We construct these bundles explicitly, by exhibiting and characterising their patching matrices, for the five--parameter family of Riemannian ALF metrics constructed by Chen and Teo. The Chen--Teo family contains a two--parameter family of asymptotically flat gravitational instantons. The patching matrices for these instantons take a simple rational form.","sentences":["Toric Ricci--flat metrics in dimension four correspond to certain holomorphic vector bundles over a twistor space.","We construct these bundles explicitly, by exhibiting and characterising their patching matrices, for the five--parameter family of Riemannian ALF metrics constructed by Chen and Teo.","The Chen--Teo family contains a two--parameter family of asymptotically flat gravitational instantons.","The patching matrices for these instantons take a simple rational form."],"url":"http://arxiv.org/abs/2405.08170v1","category":"gr-qc"}
{"created":"2024-05-13 20:09:34","title":"Deep TOV to characterize Neutron Stars","abstract":"Astrophysical observations, theoretical models, and terrestrial experiments probe different regions of neutron star (NS) interior. Therefore, it is essential to consistently combine the information from these sources. This analysis requires multiple evaluations of Tolman Oppenheimer Volkoff equations which can become computationally expensive with a large number of observations. Further, multi-messenger astronomy requires rapid NS characterization via gravitational waves for efficient electromagnetic follow-up. In this work, we develop a novel neural network-based map from the EoS curve to the mass and radius of cold non-rotating NS. We estimate a speed-up of an order of magnitude when compared with the state-of-the-art RePrimAnd solver and an average error of 1e-3 when calculating the mass and radius of the neutron star. Additionally, we also develop neural network solvers for obtaining EoS curves from a physics conforming EoS model, FRZ$\\chi_{1.5}$. We utilize this efficient continuous map to measure the sensitivity of model parameters of FRZ$\\chi_{1.5}$ towards mass and radius. We show that 8 out of 18 parameters of this model are sensitive by at least three orders of magnitude higher than the remaining 10 parameters. This information will be useful in further speeding up, as well as probing the crucial parameter space, in the parameter estimation from astrophysical observations using this physics-conforming EoS model.","sentences":["Astrophysical observations, theoretical models, and terrestrial experiments probe different regions of neutron star (NS) interior.","Therefore, it is essential to consistently combine the information from these sources.","This analysis requires multiple evaluations of Tolman Oppenheimer Volkoff equations which can become computationally expensive with a large number of observations.","Further, multi-messenger astronomy requires rapid NS characterization via gravitational waves for efficient electromagnetic follow-up.","In this work, we develop a novel neural network-based map from the EoS curve to the mass and radius of cold non-rotating NS.","We estimate a speed-up of an order of magnitude when compared with the state-of-the-art RePrimAnd solver and an average error of 1e-3 when calculating the mass and radius of the neutron star.","Additionally, we also develop neural network solvers for obtaining EoS curves from a physics conforming EoS model, FRZ$\\chi_{1.5}$. We utilize this efficient continuous map to measure the sensitivity of model parameters of FRZ$\\chi_{1.5}$ towards mass and radius.","We show that 8 out of 18 parameters of this model are sensitive by at least three orders of magnitude higher than the remaining 10 parameters.","This information will be useful in further speeding up, as well as probing the crucial parameter space, in the parameter estimation from astrophysical observations using this physics-conforming EoS model."],"url":"http://arxiv.org/abs/2405.08163v1","category":"astro-ph.HE"}
{"created":"2024-05-13 19:15:59","title":"Collisions of Burgers Bores with Nonlinear Waves","abstract":"This paper treats nonlinear wave current interactions in their simplest form, as an overtaking collision. In one spatial dimension, the paper investigates the collision interaction formulated as an initial value problem of a Burgers bore overtaking solutions of two types of nonlinear wave equations, Korteweg de Vries (KdV) and nonlinear Schrodinger (NLS). The bore wave state arising after the overtaking Burgers-KdV collision in numerical simulations is found to depend qualitatively on the balance between nonlinearity and dispersion in the KdV equation. The Burgers-KdV system is also made stochastic by following the stochastic advection by Lie transport approach (SALT).","sentences":["This paper treats nonlinear wave current interactions in their simplest form, as an overtaking collision.","In one spatial dimension, the paper investigates the collision interaction formulated as an initial value problem of a Burgers bore overtaking solutions of two types of nonlinear wave equations, Korteweg de Vries (KdV) and nonlinear Schrodinger (NLS).","The bore wave state arising after the overtaking Burgers-KdV collision in numerical simulations is found to depend qualitatively on the balance between nonlinearity and dispersion in the KdV equation.","The Burgers-KdV system is also made stochastic by following the stochastic advection by Lie transport approach (SALT)."],"url":"http://arxiv.org/abs/2405.08130v1","category":"physics.flu-dyn"}
{"created":"2024-05-13 19:12:13","title":"Orthogonal Howe duality and dynamical (split) symmetric pairs","abstract":"Inspired by Etingof--Varchenko's dynamical fusion, dynamical $R$-matrix, and dynamical Weyl group for Lie algebras, we introduce, for split symmetric pairs, versions of dynamical fusion, dynamical $K$-matrix, and dynamical Weyl group. We then turn to the study of $(\\mathfrak{so}_{2n},O_m)$-duality and prove that the standard Knizhnik-Zamolodchikov and dynamical operators (both differential and difference) on the $\\mathfrak{so}_{2n}$-side are exchanged with the symmetric pair analogs, for $O_m\\subset GL_m$, on the $O_m$-side.","sentences":["Inspired by Etingof--Varchenko's dynamical fusion, dynamical $R$-matrix, and dynamical Weyl group for Lie algebras, we introduce, for split symmetric pairs, versions of dynamical fusion, dynamical $K$-matrix, and dynamical Weyl group.","We then turn to the study of $(\\mathfrak{so}_{2n},O_m)$-duality and prove that the standard Knizhnik-Zamolodchikov and dynamical operators (both differential and difference) on the $\\mathfrak{so}_{2n}$-side are exchanged with the symmetric pair analogs, for $O_m\\subset GL_m$, on the $O_m$-side."],"url":"http://arxiv.org/abs/2405.08126v1","category":"math.RT"}
{"created":"2024-05-13 18:49:32","title":"Multiresolution of the one dimensional free-particle propagator","abstract":"Novel methods to integrate the time-dependent Schr\\\"odinger equation within the framework of multiscale approximation is presented. The methods are based on symplectic splitting algorithms to separate the kinetic and potential parts of the corresponding propagator. The semigroup associated with the free-particle Schr\\\"odinger operator is represented in a multiwavelet basis. The propagator is effectively discretized with a contour deformation technique, which overcomes the challenges presented by previous discretization methods. The discretized operator is then employed in simple numerical simulations to test the validity of the implementation and to benchmark its precision.","sentences":["Novel methods to integrate the time-dependent Schr\\\"odinger equation within the framework of multiscale approximation is presented.","The methods are based on symplectic splitting algorithms to separate the kinetic and potential parts of the corresponding propagator.","The semigroup associated with the free-particle Schr\\\"odinger operator is represented in a multiwavelet basis.","The propagator is effectively discretized with a contour deformation technique, which overcomes the challenges presented by previous discretization methods.","The discretized operator is then employed in simple numerical simulations to test the validity of the implementation and to benchmark its precision."],"url":"http://arxiv.org/abs/2405.08115v1","category":"physics.comp-ph"}
{"created":"2024-05-13 18:49:18","title":"RATLIP: Generative Adversarial CLIP Text-to-Image Synthesis Based on Recurrent Affine Transformations","abstract":"Synthesizing high-quality photorealistic images with textual descriptions as a condition is very challenging. Generative Adversarial Networks (GANs), the classical model for this task, frequently suffer from low consistency between image and text descriptions and insufficient richness in synthesized images. Recently, conditional affine transformations (CAT), such as conditional batch normalization and instance normalization, have been applied to different layers of GAN to control content synthesis in images. CAT is a multi-layer perceptron that independently predicts data based on batch statistics between neighboring layers, with global textual information unavailable to other layers. To address this issue, we first model CAT and a recurrent neural network (RAT) to ensure that different layers can access global information. We then introduce shuffle attention between RAT to mitigate the characteristic of information forgetting in recurrent neural networks. Moreover, both our generator and discriminator utilize the powerful pre-trained model, Clip, which has been extensively employed for establishing associations between text and images through the learning of multimodal representations in latent space. The discriminator utilizes CLIP's ability to comprehend complex scenes to accurately assess the quality of the generated images. Extensive experiments have been conducted on the CUB, Oxford, and CelebA-tiny datasets to demonstrate the superiority of the proposed model over current state-of-the-art models. The code is https://github.com/OxygenLu/RATLIP.","sentences":["Synthesizing high-quality photorealistic images with textual descriptions as a condition is very challenging.","Generative Adversarial Networks (GANs), the classical model for this task, frequently suffer from low consistency between image and text descriptions and insufficient richness in synthesized images.","Recently, conditional affine transformations (CAT), such as conditional batch normalization and instance normalization, have been applied to different layers of GAN to control content synthesis in images.","CAT is a multi-layer perceptron that independently predicts data based on batch statistics between neighboring layers, with global textual information unavailable to other layers.","To address this issue, we first model CAT and a recurrent neural network (RAT) to ensure that different layers can access global information.","We then introduce shuffle attention between RAT to mitigate the characteristic of information forgetting in recurrent neural networks.","Moreover, both our generator and discriminator utilize the powerful pre-trained model, Clip, which has been extensively employed for establishing associations between text and images through the learning of multimodal representations in latent space.","The discriminator utilizes CLIP's ability to comprehend complex scenes to accurately assess the quality of the generated images.","Extensive experiments have been conducted on the CUB, Oxford, and CelebA-tiny datasets to demonstrate the superiority of the proposed model over current state-of-the-art models.","The code is https://github.com/OxygenLu/RATLIP."],"url":"http://arxiv.org/abs/2405.08114v1","category":"cs.CV"}
{"created":"2024-05-13 18:33:14","title":"Tensor networks for $p$-spin models","abstract":"We introduce a tensor network algorithm for the solution of $p$-spin models. We show that bond compression through rank-revealing decompositions performed during the tensor network contraction resolves logical redundancies in the system exactly and is thus lossless, yet leads to qualitative changes in runtime scaling in different regimes of the model. First, we find that bond compression emulates the so-called leaf-removal algorithm, solving the problem efficiently in the \"easy\" phase. Past a dynamical phase transition, we observe superpolynomial runtimes, reflecting the appearance of a core component. We then develop a graphical method to study the scaling of contraction for a minimal ensemble of core-only instances, for which leaf removal is ineffective. We find subexponential scaling, improving on the exponential scaling that occurs without compression. Our results suggest that our tensor network algorithm subsumes the classical leaf removal algorithm and simplifies redundancies in the $p$-spin model through lossless compression, all without explicit knowledge of the problem's structure.","sentences":["We introduce a tensor network algorithm for the solution of $p$-spin models.","We show that bond compression through rank-revealing decompositions performed during the tensor network contraction resolves logical redundancies in the system exactly and is thus lossless, yet leads to qualitative changes in runtime scaling in different regimes of the model.","First, we find that bond compression emulates the so-called leaf-removal algorithm, solving the problem efficiently in the \"easy\" phase.","Past a dynamical phase transition, we observe superpolynomial runtimes, reflecting the appearance of a core component.","We then develop a graphical method to study the scaling of contraction for a minimal ensemble of core-only instances, for which leaf removal is ineffective.","We find subexponential scaling, improving on the exponential scaling that occurs without compression.","Our results suggest that our tensor network algorithm subsumes the classical leaf removal algorithm and simplifies redundancies in the $p$-spin model through lossless compression, all without explicit knowledge of the problem's structure."],"url":"http://arxiv.org/abs/2405.08106v1","category":"cond-mat.stat-mech"}
{"created":"2024-05-13 18:26:55","title":"Graph Neural Networks for Parameterized Quantum Circuits Expressibility Estimation","abstract":"Parameterized quantum circuits (PQCs) are fundamental to quantum machine learning (QML), quantum optimization, and variational quantum algorithms (VQAs). The expressibility of PQCs is a measure that determines their capability to harness the full potential of the quantum state space. It is thus a crucial guidepost to know when selecting a particular PQC ansatz. However, the existing technique for expressibility computation through statistical estimation requires a large number of samples, which poses significant challenges due to time and computational resource constraints. This paper introduces a novel approach for expressibility estimation of PQCs using Graph Neural Networks (GNNs). We demonstrate the predictive power of our GNN model with a dataset consisting of 25,000 samples from the noiseless IBM QASM Simulator and 12,000 samples from three distinct noisy quantum backends. The model accurately estimates expressibility, with root mean square errors (RMSE) of 0.05 and 0.06 for the noiseless and noisy backends, respectively. We compare our model's predictions with reference circuits [Sim and others, QuTe'2019] and IBM Qiskit's hardware-efficient ansatz sets to further evaluate our model's performance. Our experimental evaluation in noiseless and noisy scenarios reveals a close alignment with ground truth expressibility values, highlighting the model's efficacy. Moreover, our model exhibits promising extrapolation capabilities, predicting expressibility values with low RMSE for out-of-range qubit circuits trained solely on only up to 5-qubit circuit sets. This work thus provides a reliable means of efficiently evaluating the expressibility of diverse PQCs on noiseless simulators and hardware.","sentences":["Parameterized quantum circuits (PQCs) are fundamental to quantum machine learning (QML), quantum optimization, and variational quantum algorithms (VQAs).","The expressibility of PQCs is a measure that determines their capability to harness the full potential of the quantum state space.","It is thus a crucial guidepost to know when selecting a particular PQC ansatz.","However, the existing technique for expressibility computation through statistical estimation requires a large number of samples, which poses significant challenges due to time and computational resource constraints.","This paper introduces a novel approach for expressibility estimation of PQCs using Graph Neural Networks (GNNs).","We demonstrate the predictive power of our GNN model with a dataset consisting of 25,000 samples from the noiseless IBM QASM Simulator and 12,000 samples from three distinct noisy quantum backends.","The model accurately estimates expressibility, with root mean square errors (RMSE) of 0.05 and 0.06 for the noiseless and noisy backends, respectively.","We compare our model's predictions with reference circuits [Sim and others, QuTe'2019] and IBM Qiskit's hardware-efficient ansatz sets to further evaluate our model's performance.","Our experimental evaluation in noiseless and noisy scenarios reveals a close alignment with ground truth expressibility values, highlighting the model's efficacy.","Moreover, our model exhibits promising extrapolation capabilities, predicting expressibility values with low RMSE for out-of-range qubit circuits trained solely on only up to 5-qubit circuit sets.","This work thus provides a reliable means of efficiently evaluating the expressibility of diverse PQCs on noiseless simulators and hardware."],"url":"http://arxiv.org/abs/2405.08100v1","category":"quant-ph"}
{"created":"2024-05-13 18:10:34","title":"Comparative Study of Bitcoin Price Prediction","abstract":"Prediction of stock prices has been a crucial and challenging task, especially in the case of highly volatile digital currencies such as Bitcoin. This research examineS the potential of using neural network models, namely LSTMs and GRUs, to forecast Bitcoin's price movements. We employ five-fold cross-validation to enhance generalization and utilize L2 regularization to reduce overfitting and noise. Our study demonstrates that the GRUs models offer better accuracy than LSTMs model for predicting Bitcoin's price. Specifically, the GRU model has an MSE of 4.67, while the LSTM model has an MSE of 6.25 when compared to the actual prices in the test set data. This finding indicates that GRU models are better equipped to process sequential data with long-term dependencies, a characteristic of financial time series data such as Bitcoin prices. In summary, our results provide valuable insights into the potential of neural network models for accurate Bitcoin price prediction and emphasize the importance of employing appropriate regularization techniques to enhance model performance.","sentences":["Prediction of stock prices has been a crucial and challenging task, especially in the case of highly volatile digital currencies such as Bitcoin.","This research examineS the potential of using neural network models, namely LSTMs and GRUs, to forecast Bitcoin's price movements.","We employ five-fold cross-validation to enhance generalization and utilize L2 regularization to reduce overfitting and noise.","Our study demonstrates that the GRUs models offer better accuracy than LSTMs model for predicting Bitcoin's price.","Specifically, the GRU model has an MSE of 4.67, while the LSTM model has an MSE of 6.25 when compared to the actual prices in the test set data.","This finding indicates that GRU models are better equipped to process sequential data with long-term dependencies, a characteristic of financial time series data such as Bitcoin prices.","In summary, our results provide valuable insights into the potential of neural network models for accurate Bitcoin price prediction and emphasize the importance of employing appropriate regularization techniques to enhance model performance."],"url":"http://arxiv.org/abs/2405.08089v1","category":"q-fin.ST"}
{"created":"2024-05-13 18:03:25","title":"Four-component relativistic third-order algebraic diagrammatic construction theory for electron detachment, attachment, electronic excitation problem and calculation of first order transition properties","abstract":"An efficient third-order algebraic diagrammatic construction (ADC) theory has been implemented to calculate ionisation potential, electron attachment and excitation energy (IP/EA/EE-ADC(3)) in a four-component relativistic framework. We have used polarisation propagator formulation for third-order perturbation theory to access the excitation energies (EE), and for IP/EA, a single-particle propagator has been used based on a non-Dyson formulation. The benchmarking calculations have been performed on various types of systems to test the accuracy of the four component ADC(3) scheme for the computation of IP, EA and EE. We have applied our IP-ADC(3) to demonstrate the computation of splitting in the IP states for halogen monoxides (XO, X = Cl, Br, I ) due to spin-orbital coupling in the 2^{\\Pi} ground state and compared it with experimental results. Next, we have studied the effect of relativity and the size of the basis set on the electron attachment calculations of halogen atoms (F, Cl, Br, I and At) using EA-ADC(3). As our next step, we have shown the efficiency of four component ADC(3) in computing excitation energies of triiodide ion and compared with relativistic equation of motion coupled cluster with singles and doubles (EOM-CCSD), intermediate Hamiltonian Fock space coupled cluster (IHFS-CC) and other EOM-CCSD schemes in which spin-orbit coupling is incorporated with different degrees of approximation. Finally, we have also investigated the excitation energies and transition dipole moments for the four excited states of Xe atom and compared them with our recent four-component EOM-CCSD implementation and relativistic finite field Fock space coupled cluster results, along with the experimental estimates.","sentences":["An efficient third-order algebraic diagrammatic construction (ADC) theory has been implemented to calculate ionisation potential, electron attachment and excitation energy (IP/EA/EE-ADC(3)) in a four-component relativistic framework.","We have used polarisation propagator formulation for third-order perturbation theory to access the excitation energies (EE), and for IP/EA, a single-particle propagator has been used based on a non-Dyson formulation.","The benchmarking calculations have been performed on various types of systems to test the accuracy of the four component ADC(3) scheme for the computation of IP, EA and EE.","We have applied our IP-ADC(3) to demonstrate the computation of splitting in the IP states for halogen monoxides (XO, X = Cl, Br, I ) due to spin-orbital coupling in the 2^{\\Pi} ground state and compared it with experimental results.","Next, we have studied the effect of relativity and the size of the basis set on the electron attachment calculations of halogen atoms (F, Cl, Br, I and At) using EA-ADC(3).","As our next step, we have shown the efficiency of four component ADC(3) in computing excitation energies of triiodide ion and compared with relativistic equation of motion coupled cluster with singles and doubles (EOM-CCSD), intermediate Hamiltonian Fock space coupled cluster (IHFS-CC) and other EOM-CCSD schemes in which spin-orbit coupling is incorporated with different degrees of approximation.","Finally, we have also investigated the excitation energies and transition dipole moments for the four excited states of Xe atom and compared them with our recent four-component EOM-CCSD implementation and relativistic finite field Fock space coupled cluster results, along with the experimental estimates."],"url":"http://arxiv.org/abs/2405.08085v1","category":"physics.chem-ph"}
{"created":"2024-05-13 18:01:26","title":"5d 2-Chern-Simons theory and 3d integrable field theories","abstract":"The $4$-dimensional semi-holomorphic Chern-Simons theory of Costello and Yamazaki provides a gauge-theoretic origin for the Lax connection of $2$-dimensional integrable field theories. The purpose of this paper is to extend this framework to the setting of $3$-dimensional integrable field theories by considering a $5$-dimensional semi-holomorphic higher Chern-Simons theory for a higher connection $(A,B)$ on $\\mathbb{R}^3 \\times \\mathbb{C}P^1$. The input data for this theory are the choice of a meromorphic $1$-form $\\omega$ on $\\mathbb{C}P^1$ and a strict Lie $2$-group with cyclic structure on its underlying Lie $2$-algebra. Integrable field theories on $\\mathbb{R}^3$ are constructed by imposing suitable boundary conditions on the connection $(A,B)$ at the $3$-dimensional defects located at the poles of $\\omega$ and choosing certain admissible meromorphic solutions of the bulk equations of motion. The latter provides a natural notion of higher Lax connection for $3$-dimensional integrable field theories, including a $2$-form component $B$ which can be integrated over Cauchy surfaces to produce conserved charges. As a first application of this approach, we show how to construct a generalization of Ward's $(2+1)$-dimensional integrable chiral model from a suitable choice of data in the $5$-dimensional theory.","sentences":["The $4$-dimensional semi-holomorphic Chern-Simons theory of Costello and Yamazaki provides a gauge-theoretic origin for the Lax connection of $2$-dimensional integrable field theories.","The purpose of this paper is to extend this framework to the setting of $3$-dimensional integrable field theories by considering a $5$-dimensional semi-holomorphic higher Chern-Simons theory for a higher connection $(A,B)$ on $\\mathbb{R}^3","\\times \\mathbb{C}P^1$. The input data for this theory are the choice of a meromorphic $1$-form $\\omega$ on $\\mathbb{C}P^1$ and a strict Lie $2$-group with cyclic structure on its underlying Lie $2$-algebra.","Integrable field theories on $\\mathbb{R}^3$ are constructed by imposing suitable boundary conditions on the connection $(A,B)$ at the $3$-dimensional defects located at the poles of $\\omega$ and choosing certain admissible meromorphic solutions of the bulk equations of motion.","The latter provides a natural notion of higher Lax connection for $3$-dimensional integrable field theories, including a $2$-form component $B$ which can be integrated over Cauchy surfaces to produce conserved charges.","As a first application of this approach, we show how to construct a generalization of Ward's $(2+1)$-dimensional integrable chiral model from a suitable choice of data in the $5$-dimensional theory."],"url":"http://arxiv.org/abs/2405.08083v1","category":"hep-th"}
{"created":"2024-05-13 18:00:49","title":"Superconducting multi-vortices and a novel BPS bound in chiral perturbation theory","abstract":"We derive a novel BPS bound from chiral perturbation theory minimally coupled to electrodynamics at finite isospin chemical potential. At a critical value of the isospin chemical potential, a system of three first-order differential field equations (which implies the second-order field equations) for the gauge field and the hadronic profile can be derived from the requirement to saturate the bound. These BPS configurations represent magnetic multi-vortices with quantized flux supported by a superconducting current. The corresponding topological charge density is related to the magnetic flux density, but is screened by the hadronic profile. Such a screening effect allows to derive the maximal value of the magnetic field generated by these BPS magnetic vortices, being $B_{max}=2,04 \\times 10^{14}G$. The solution for a single BPS vortex is discussed in detail, and some physical consequences, together with the comparison with the magnetic vortices in the Ginzburg-Landau theory at critical coupling, are described.","sentences":["We derive a novel BPS bound from chiral perturbation theory minimally coupled to electrodynamics at finite isospin chemical potential.","At a critical value of the isospin chemical potential, a system of three first-order differential field equations (which implies the second-order field equations) for the gauge field and the hadronic profile can be derived from the requirement to saturate the bound.","These BPS configurations represent magnetic multi-vortices with quantized flux supported by a superconducting current.","The corresponding topological charge density is related to the magnetic flux density, but is screened by the hadronic profile.","Such a screening effect allows to derive the maximal value of the magnetic field generated by these BPS magnetic vortices, being $B_{max}=2,04 \\times 10^{14}G$.","The solution for a single BPS vortex is discussed in detail, and some physical consequences, together with the comparison with the magnetic vortices in the Ginzburg-Landau theory at critical coupling, are described."],"url":"http://arxiv.org/abs/2405.08082v1","category":"hep-th"}
{"created":"2024-05-13 18:00:04","title":"Relativistic Guiding-Center Motion: Action Principle, Kinetic Theory, and Hydrodynamics","abstract":"We treat the guiding-center dynamics in a non-uniform external Maxwell field using a manifestly Lorentz covariant action principle which easily reproduces the known Vandervoort equations of motion. We derive the corresponding kinetic theory and ideal hydrodynamic theory. In contrast to conventional five-equation hydrodynamics, the guiding-center hydrodynamics needs only three equations due to a constraint on the motion across magnetic field. We argue that this hydrodynamics applies more generally than the kinetic theory, e.g., for strongly-coupled quark-gluon plasma.","sentences":["We treat the guiding-center dynamics in a non-uniform external Maxwell field using a manifestly Lorentz covariant action principle which easily reproduces the known Vandervoort equations of motion.","We derive the corresponding kinetic theory and ideal hydrodynamic theory.","In contrast to conventional five-equation hydrodynamics, the guiding-center hydrodynamics needs only three equations due to a constraint on the motion across magnetic field.","We argue that this hydrodynamics applies more generally than the kinetic theory, e.g., for strongly-coupled quark-gluon plasma."],"url":"http://arxiv.org/abs/2405.08073v1","category":"nucl-th"}
{"created":"2024-05-13 18:00:02","title":"Covariant single-field formulation of effective cosmological bounces","abstract":"This study explores the feasibility of an effective Friedmann equation in removing the classical initial Big Bang singularity, replaced by a bounce occurring at a critical energy density value. In a spatially flat, homogeneous, and isotropic universe, the effective theory is obtained by introducing a function parametrically dependent on the critical energy density. It measures the deviation from the benchmark theory characterising the asymptotic behaviour as the critical energy density approaches infinity. Focusing on the covariant single-field formulation in viable Horndeski gravity, our analysis shows that both the effective and the benchmark theories belong to the same scalar-tensor theory, with no additional propagating degrees of freedom: the cuscuton and extended cuscuton models.","sentences":["This study explores the feasibility of an effective Friedmann equation in removing the classical initial Big Bang singularity, replaced by a bounce occurring at a critical energy density value.","In a spatially flat, homogeneous, and isotropic universe, the effective theory is obtained by introducing a function parametrically dependent on the critical energy density.","It measures the deviation from the benchmark theory characterising the asymptotic behaviour as the critical energy density approaches infinity.","Focusing on the covariant single-field formulation in viable Horndeski gravity, our analysis shows that both the effective and the benchmark theories belong to the same scalar-tensor theory, with no additional propagating degrees of freedom: the cuscuton and extended cuscuton models."],"url":"http://arxiv.org/abs/2405.08071v1","category":"gr-qc"}
{"created":"2024-05-13 18:00:00","title":"Observables of super-extremal black holes: challenging Cosmic Censorship to comprehend the Cosmological Constant","abstract":"Einstein's Field Equations have proven applicable across many scales, from black holes to cosmology. Even the mysterious Cosmological Constant found a physical interpretation in the so-called ``dark energy'' causing the accelerated cosmic expansion as inferred from multiple observables. Yet, we still lack a material source for this dark fluid. Probing the local universe to find it yields complementary information to the one from the cosmic microwave background. Could dark energy be sourced by super-extremal charged black holes? Contrary to intuition, such objects could exist with only weak observational signatures. The latter are introduced here to outline how sky surveys can identify individual candidates which challenge Cosmic Censorship on the one hand but may explain the physical origin of the Cosmological Constant on the other.","sentences":["Einstein's Field Equations have proven applicable across many scales, from black holes to cosmology.","Even the mysterious Cosmological Constant found a physical interpretation in the so-called ``dark energy'' causing the accelerated cosmic expansion as inferred from multiple observables.","Yet, we still lack a material source for this dark fluid.","Probing the local universe to find it yields complementary information to the one from the cosmic microwave background.","Could dark energy be sourced by super-extremal charged black holes?","Contrary to intuition, such objects could exist with only weak observational signatures.","The latter are introduced here to outline how sky surveys can identify individual candidates which challenge Cosmic Censorship on the one hand but may explain the physical origin of the Cosmological Constant on the other."],"url":"http://arxiv.org/abs/2405.08057v1","category":"gr-qc"}
{"created":"2024-05-13 16:17:03","title":"Reconstructions of $f(\\mathcal{P})$ and $f(\\mathcal{Q})$ gravity models from $(m,n)$-type Barrow Holographic Dark Energy","abstract":"In this work, we have reconstructed the extended $f(\\mathcal{P})$ cubic gravity and symmetric $f(\\mathcal{Q})$ teleparallel gravity from $(m,n)$-type Barrow Holographic Dark Energy (BHDE) and find the unknown functions $f(\\mathcal{P})$ and $f(\\mathcal{Q})$ in terms of $\\mathcal{P}$ and $\\mathcal{Q}$ by taking the universe to be flat, homogeneous and isotropic. We then analyzed the behavior and stability of each model for the entire stages of the evolution of the universe by studying several important parameters such as the deceleration parameter, equation of state (EoS) parameter $\\omega_{DE}$, square of the speed of sound $v_s^2$. Apart from this, we have studied the cosmographic behavior by plotting the jerk parameter, snap parameter, and lerk parameter against the redshift. We have also examined the $\\omega'_{DE}-\\omega_{DE}$ phase plane and $(r,s^*)$, $(r,q)$ statefinder parameters that provide valuable insights into the dynamics of the universe and the distinctive features of the dark energy. All these analyses pointed out that our model can produce a universe going through an accelerated expansion with the quintessence type dark energy.","sentences":["In this work, we have reconstructed the extended $f(\\mathcal{P})$ cubic gravity and symmetric $f(\\mathcal{Q})$ teleparallel gravity from $(m,n)$-type Barrow Holographic Dark Energy (BHDE) and find the unknown functions $f(\\mathcal{P})$ and $f(\\mathcal{Q})$ in terms of $\\mathcal{P}$ and $\\mathcal{Q}$ by taking the universe to be flat, homogeneous and isotropic.","We then analyzed the behavior and stability of each model for the entire stages of the evolution of the universe by studying several important parameters such as the deceleration parameter, equation of state (EoS) parameter $\\omega_{DE}$, square of the speed of sound $v_s^2$. Apart from this, we have studied the cosmographic behavior by plotting the jerk parameter, snap parameter, and lerk parameter against the redshift.","We have also examined the $\\omega'_{DE}-\\omega_{DE}$ phase plane and $(r,s^*)$, $(r,q)$ statefinder parameters that provide valuable insights into the dynamics of the universe and the distinctive features of the dark energy.","All these analyses pointed out that our model can produce a universe going through an accelerated expansion with the quintessence type dark energy."],"url":"http://arxiv.org/abs/2405.08050v1","category":"gr-qc"}
{"created":"2024-05-14 17:59:40","title":"Efficient Vision-Language Pre-training by Cluster Masking","abstract":"We propose a simple strategy for masking image patches during visual-language contrastive learning that improves the quality of the learned representations and the training speed. During each iteration of training, we randomly mask clusters of visually similar image patches, as measured by their raw pixel intensities. This provides an extra learning signal, beyond the contrastive training itself, since it forces a model to predict words for masked visual structures solely from context. It also speeds up training by reducing the amount of data used in each image. We evaluate the effectiveness of our model by pre-training on a number of benchmarks, finding that it outperforms other masking strategies, such as FLIP, on the quality of the learned representation.","sentences":["We propose a simple strategy for masking image patches during visual-language contrastive learning that improves the quality of the learned representations and the training speed.","During each iteration of training, we randomly mask clusters of visually similar image patches, as measured by their raw pixel intensities.","This provides an extra learning signal, beyond the contrastive training itself, since it forces a model to predict words for masked visual structures solely from context.","It also speeds up training by reducing the amount of data used in each image.","We evaluate the effectiveness of our model by pre-training on a number of benchmarks, finding that it outperforms other masking strategies, such as FLIP, on the quality of the learned representation."],"url":"http://arxiv.org/abs/2405.08815v1","category":"cs.CV"}
{"created":"2024-05-14 17:49:18","title":"Prospects of Privacy Advantage in Quantum Machine Learning","abstract":"Ensuring data privacy in machine learning models is critical, particularly in distributed settings where model gradients are typically shared among multiple parties to allow collaborative learning. Motivated by the increasing success of recovering input data from the gradients of classical models, this study addresses a central question: How hard is it to recover the input data from the gradients of quantum machine learning models? Focusing on variational quantum circuits (VQC) as learning models, we uncover the crucial role played by the dynamical Lie algebra (DLA) of the VQC ansatz in determining privacy vulnerabilities. While the DLA has previously been linked to the classical simulatability and trainability of VQC models, this work, for the first time, establishes its connection to the privacy of VQC models. In particular, we show that properties conducive to the trainability of VQCs, such as a polynomial-sized DLA, also facilitate the extraction of detailed snapshots of the input. We term this a weak privacy breach, as the snapshots enable training VQC models for distinct learning tasks without direct access to the original input. Further, we investigate the conditions for a strong privacy breach where the original input data can be recovered from these snapshots by classical or quantum-assisted polynomial time methods. We establish conditions on the encoding map such as classical simulatability, overlap with DLA basis, and its Fourier frequency characteristics that enable such a privacy breach of VQC models. Our findings thus play a crucial role in detailing the prospects of quantum privacy advantage by guiding the requirements for designing quantum machine learning models that balance trainability with robust privacy protection.","sentences":["Ensuring data privacy in machine learning models is critical, particularly in distributed settings where model gradients are typically shared among multiple parties to allow collaborative learning.","Motivated by the increasing success of recovering input data from the gradients of classical models, this study addresses a central question: How hard is it to recover the input data from the gradients of quantum machine learning models?","Focusing on variational quantum circuits (VQC) as learning models, we uncover the crucial role played by the dynamical Lie algebra (DLA) of the VQC ansatz in determining privacy vulnerabilities.","While the DLA has previously been linked to the classical simulatability and trainability of VQC models, this work, for the first time, establishes its connection to the privacy of VQC models.","In particular, we show that properties conducive to the trainability of VQCs, such as a polynomial-sized DLA, also facilitate the extraction of detailed snapshots of the input.","We term this a weak privacy breach, as the snapshots enable training VQC models for distinct learning tasks without direct access to the original input.","Further, we investigate the conditions for a strong privacy breach where the original input data can be recovered from these snapshots by classical or quantum-assisted polynomial time methods.","We establish conditions on the encoding map such as classical simulatability, overlap with DLA basis, and its Fourier frequency characteristics that enable such a privacy breach of VQC models.","Our findings thus play a crucial role in detailing the prospects of quantum privacy advantage by guiding the requirements for designing quantum machine learning models that balance trainability with robust privacy protection."],"url":"http://arxiv.org/abs/2405.08801v1","category":"quant-ph"}
{"created":"2024-05-14 16:30:03","title":"Reinformer: Max-Return Sequence Modeling for offline RL","abstract":"As a data-driven paradigm, offline reinforcement learning (RL) has been formulated as sequence modeling that conditions on the hindsight information including returns, goal or future trajectory. Although promising, this supervised paradigm overlooks the core objective of RL that maximizes the return. This overlook directly leads to the lack of trajectory stitching capability that affects the sequence model learning from sub-optimal data. In this work, we introduce the concept of max-return sequence modeling which integrates the goal of maximizing returns into existing sequence models. We propose Reinforced Transformer (Reinformer), indicating the sequence model is reinforced by the RL objective. Reinformer additionally incorporates the objective of maximizing returns in the training phase, aiming to predict the maximum future return within the distribution. During inference, this in-distribution maximum return will guide the selection of optimal actions. Empirically, Reinformer is competitive with classical RL methods on the D4RL benchmark and outperforms state-of-the-art sequence model particularly in trajectory stitching ability. Code is public at \\url{https://github.com/Dragon-Zhuang/Reinformer}.","sentences":["As a data-driven paradigm, offline reinforcement learning (RL) has been formulated as sequence modeling that conditions on the hindsight information including returns, goal or future trajectory.","Although promising, this supervised paradigm overlooks the core objective of RL that maximizes the return.","This overlook directly leads to the lack of trajectory stitching capability that affects the sequence model learning from sub-optimal data.","In this work, we introduce the concept of max-return sequence modeling which integrates the goal of maximizing returns into existing sequence models.","We propose Reinforced Transformer (Reinformer), indicating the sequence model is reinforced by the RL objective.","Reinformer additionally incorporates the objective of maximizing returns in the training phase, aiming to predict the maximum future return within the distribution.","During inference, this in-distribution maximum return will guide the selection of optimal actions.","Empirically, Reinformer is competitive with classical RL methods on the D4RL benchmark and outperforms state-of-the-art sequence model particularly in trajectory stitching ability.","Code is public at \\url{https://github.com/Dragon-Zhuang/Reinformer}."],"url":"http://arxiv.org/abs/2405.08740v1","category":"cs.LG"}
{"created":"2024-05-14 15:37:56","title":"Byzantine-Resilient Secure Aggregation for Federated Learning Without Privacy Compromises","abstract":"Federated learning (FL) shows great promise in large scale machine learning, but brings new risks in terms of privacy and security. We propose ByITFL, a novel scheme for FL that provides resilience against Byzantine users while keeping the users' data private from the federator and private from other users. The scheme builds on the preexisting non-private FLTrust scheme, which tolerates malicious users through trust scores (TS) that attenuate or amplify the users' gradients. The trust scores are based on the ReLU function, which we approximate by a polynomial. The distributed and privacy-preserving computation in ByITFL is designed using a combination of Lagrange coded computing, verifiable secret sharing and re-randomization steps. ByITFL is the first Byzantine resilient scheme for FL with full information-theoretic privacy.","sentences":["Federated learning (FL) shows great promise in large scale machine learning, but brings new risks in terms of privacy and security.","We propose ByITFL, a novel scheme for FL that provides resilience against Byzantine users while keeping the users' data private from the federator and private from other users.","The scheme builds on the preexisting non-private FLTrust scheme, which tolerates malicious users through trust scores (TS) that attenuate or amplify the users' gradients.","The trust scores are based on the ReLU function, which we approximate by a polynomial.","The distributed and privacy-preserving computation in ByITFL is designed using a combination of Lagrange coded computing, verifiable secret sharing and re-randomization steps.","ByITFL is the first Byzantine resilient scheme for FL with full information-theoretic privacy."],"url":"http://arxiv.org/abs/2405.08698v1","category":"cs.IT"}
{"created":"2024-05-14 15:28:48","title":"The impact of Compositionality in Zero-shot Multi-label action recognition for Object-based tasks","abstract":"Addressing multi-label action recognition in videos represents a significant challenge for robotic applications in dynamic environments, especially when the robot is required to cooperate with humans in tasks that involve objects. Existing methods still struggle to recognize unseen actions or require extensive training data. To overcome these problems, we propose Dual-VCLIP, a unified approach for zero-shot multi-label action recognition. Dual-VCLIP enhances VCLIP, a zero-shot action recognition method, with the DualCoOp method for multi-label image classification. The strength of our method is that at training time it only learns two prompts, and it is therefore much simpler than other methods. We validate our method on the Charades dataset that includes a majority of object-based actions, demonstrating that -- despite its simplicity -- our method performs favorably with respect to existing methods on the complete dataset, and promising performance when tested on unseen actions. Our contribution emphasizes the impact of verb-object class-splits during robots' training for new cooperative tasks, highlighting the influence on the performance and giving insights into mitigating biases.","sentences":["Addressing multi-label action recognition in videos represents a significant challenge for robotic applications in dynamic environments, especially when the robot is required to cooperate with humans in tasks that involve objects.","Existing methods still struggle to recognize unseen actions or require extensive training data.","To overcome these problems, we propose Dual-VCLIP, a unified approach for zero-shot multi-label action recognition.","Dual-VCLIP enhances VCLIP, a zero-shot action recognition method, with the DualCoOp method for multi-label image classification.","The strength of our method is that at training time it only learns two prompts, and it is therefore much simpler than other methods.","We validate our method on the Charades dataset that includes a majority of object-based actions, demonstrating that -- despite its simplicity -- our method performs favorably with respect to existing methods on the complete dataset, and promising performance when tested on unseen actions.","Our contribution emphasizes the impact of verb-object class-splits during robots' training for new cooperative tasks, highlighting the influence on the performance and giving insights into mitigating biases."],"url":"http://arxiv.org/abs/2405.08695v1","category":"cs.CV"}
{"created":"2024-05-14 12:50:33","title":"The Unseen Targets of Hate -- A Systematic Review of Hateful Communication Datasets","abstract":"Machine learning (ML)-based content moderation tools are essential to keep online spaces free from hateful communication. Yet, ML tools can only be as capable as the quality of the data they are trained on allows them. While there is increasing evidence that they underperform in detecting hateful communications directed towards specific identities and may discriminate against them, we know surprisingly little about the provenance of such bias. To fill this gap, we present a systematic review of the datasets for the automated detection of hateful communication introduced over the past decade, and unpack the quality of the datasets in terms of the identities that they embody: those of the targets of hateful communication that the data curators focused on, as well as those unintentionally included in the datasets. We find, overall, a skewed representation of selected target identities and mismatches between the targets that research conceptualizes and ultimately includes in datasets. Yet, by contextualizing these findings in the language and location of origin of the datasets, we highlight a positive trend towards the broadening and diversification of this research space.","sentences":["Machine learning (ML)-based content moderation tools are essential to keep online spaces free from hateful communication.","Yet, ML tools can only be as capable as the quality of the data they are trained on allows them.","While there is increasing evidence that they underperform in detecting hateful communications directed towards specific identities and may discriminate against them, we know surprisingly little about the provenance of such bias.","To fill this gap, we present a systematic review of the datasets for the automated detection of hateful communication introduced over the past decade, and unpack the quality of the datasets in terms of the identities that they embody: those of the targets of hateful communication that the data curators focused on, as well as those unintentionally included in the datasets.","We find, overall, a skewed representation of selected target identities and mismatches between the targets that research conceptualizes and ultimately includes in datasets.","Yet, by contextualizing these findings in the language and location of origin of the datasets, we highlight a positive trend towards the broadening and diversification of this research space."],"url":"http://arxiv.org/abs/2405.08562v1","category":"cs.CL"}
{"created":"2024-05-14 12:43:43","title":"Dual-Branch Network for Portrait Image Quality Assessment","abstract":"Portrait images typically consist of a salient person against diverse backgrounds. With the development of mobile devices and image processing techniques, users can conveniently capture portrait images anytime and anywhere. However, the quality of these portraits may suffer from the degradation caused by unfavorable environmental conditions, subpar photography techniques, and inferior capturing devices. In this paper, we introduce a dual-branch network for portrait image quality assessment (PIQA), which can effectively address how the salient person and the background of a portrait image influence its visual quality. Specifically, we utilize two backbone networks (\\textit{i.e.,} Swin Transformer-B) to extract the quality-aware features from the entire portrait image and the facial image cropped from it. To enhance the quality-aware feature representation of the backbones, we pre-train them on the large-scale video quality assessment dataset LSVQ and the large-scale facial image quality assessment dataset GFIQA. Additionally, we leverage LIQE, an image scene classification and quality assessment model, to capture the quality-aware and scene-specific features as the auxiliary features. Finally, we concatenate these features and regress them into quality scores via a multi-perception layer (MLP). We employ the fidelity loss to train the model via a learning-to-rank manner to mitigate inconsistencies in quality scores in the portrait image quality assessment dataset PIQ. Experimental results demonstrate that the proposed model achieves superior performance in the PIQ dataset, validating its effectiveness. The code is available at \\url{https://github.com/sunwei925/DN-PIQA.git}.","sentences":["Portrait images typically consist of a salient person against diverse backgrounds.","With the development of mobile devices and image processing techniques, users can conveniently capture portrait images anytime and anywhere.","However, the quality of these portraits may suffer from the degradation caused by unfavorable environmental conditions, subpar photography techniques, and inferior capturing devices.","In this paper, we introduce a dual-branch network for portrait image quality assessment (PIQA), which can effectively address how the salient person and the background of a portrait image influence its visual quality.","Specifically, we utilize two backbone networks (\\textit{i.e.,} Swin Transformer-B) to extract the quality-aware features from the entire portrait image and the facial image cropped from it.","To enhance the quality-aware feature representation of the backbones, we pre-train them on the large-scale video quality assessment dataset LSVQ and the large-scale facial image quality assessment dataset GFIQA.","Additionally, we leverage LIQE, an image scene classification and quality assessment model, to capture the quality-aware and scene-specific features as the auxiliary features.","Finally, we concatenate these features and regress them into quality scores via a multi-perception layer (MLP).","We employ the fidelity loss to train the model via a learning-to-rank manner to mitigate inconsistencies in quality scores in the portrait image quality assessment dataset PIQ.","Experimental results demonstrate that the proposed model achieves superior performance in the PIQ dataset, validating its effectiveness.","The code is available at \\url{https://github.com/sunwei925/DN-PIQA.git}."],"url":"http://arxiv.org/abs/2405.08555v1","category":"cs.CV"}
{"created":"2024-05-14 12:41:11","title":"Improving Transformers with Dynamically Composable Multi-Head Attention","abstract":"Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads. At the core of DCMHA is a $\\it{Compose}$ function that transforms the attention score and weight matrices in an input-dependent way. DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with ~1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation. The code and models are available at https://github.com/Caiyun-AI/DCFormer.","sentences":["Multi-Head Attention (MHA) is a key component of Transformer.","In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy.","We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads.","At the core of DCMHA is a $\\it{Compose}$ function that transforms the attention score and weight matrices in an input-dependent way.","DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer.","DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with ~1.7x-2.0x compute.","For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation.","The code and models are available at https://github.com/Caiyun-AI/DCFormer."],"url":"http://arxiv.org/abs/2405.08553v1","category":"cs.LG"}
{"created":"2024-05-14 09:59:37","title":"A review on machine learning for arterial extraction and quantitative assessment on invasive coronary angiograms","abstract":"Purpose of Review Recently, machine learning has developed rapidly in the field of medicine, playing an important role in disease diagnosis. Our aim of this paper is to provide an overview of the advancements in machine learning techniques applied to invasive coronary angiography (ICA) for segmentation of coronary arteries and quantitative evaluation like fractional flow reserve (FFR) and stenosis assessment.   Recent Findings ICA are used extensively along with machine learning techniques for the segmentation of arteries and quantitative evaluation of stenosis, coronary artery disease and measurement of fractional flow reserve, representing a trend towards using computational methods for enhanced diagnostic precision in cardiovascular medicine.   Summary Various research studies have been conducted in this field, each using different algorithms and datasets. The performance of these studies largely depends on the algorithms employed and the datasets used for training and evaluation. However, despite the progress made, there remains a need for machine learning (ML) algorithms that can be easily integrated into clinical practice.","sentences":["Purpose of Review Recently, machine learning has developed rapidly in the field of medicine, playing an important role in disease diagnosis.","Our aim of this paper is to provide an overview of the advancements in machine learning techniques applied to invasive coronary angiography (ICA) for segmentation of coronary arteries and quantitative evaluation like fractional flow reserve (FFR) and stenosis assessment.   ","Recent Findings ICA are used extensively along with machine learning techniques for the segmentation of arteries and quantitative evaluation of stenosis, coronary artery disease and measurement of fractional flow reserve, representing a trend towards using computational methods for enhanced diagnostic precision in cardiovascular medicine.   ","Summary Various research studies have been conducted in this field, each using different algorithms and datasets.","The performance of these studies largely depends on the algorithms employed and the datasets used for training and evaluation.","However, despite the progress made, there remains a need for machine learning (ML) algorithms that can be easily integrated into clinical practice."],"url":"http://arxiv.org/abs/2405.08474v1","category":"physics.med-ph"}
{"created":"2024-05-14 09:01:33","title":"DGCformer: Deep Graph Clustering Transformer for Multivariate Time Series Forecasting","abstract":"Multivariate time series forecasting tasks are usually conducted in a channel-dependent (CD) way since it can incorporate more variable-relevant information. However, it may also involve a lot of irrelevant variables, and this even leads to worse performance than the channel-independent (CI) strategy. This paper combines the strengths of both strategies and proposes the Deep Graph Clustering Transformer (DGCformer) for multivariate time series forecasting. Specifically, it first groups these relevant variables by a graph convolutional network integrated with an autoencoder, and a former-latter masked self-attention mechanism is then considered with the CD strategy being applied to each group of variables while the CI one for different groups. Extensive experimental results on eight datasets demonstrate the superiority of our method against state-of-the-art models, and our code will be publicly available upon acceptance.","sentences":["Multivariate time series forecasting tasks are usually conducted in a channel-dependent (CD) way since it can incorporate more variable-relevant information.","However, it may also involve a lot of irrelevant variables, and this even leads to worse performance than the channel-independent (CI) strategy.","This paper combines the strengths of both strategies and proposes the Deep Graph Clustering Transformer (DGCformer) for multivariate time series forecasting.","Specifically, it first groups these relevant variables by a graph convolutional network integrated with an autoencoder, and a former-latter masked self-attention mechanism is then considered with the CD strategy being applied to each group of variables while the CI one for different groups.","Extensive experimental results on eight datasets demonstrate the superiority of our method against state-of-the-art models, and our code will be publicly available upon acceptance."],"url":"http://arxiv.org/abs/2405.08440v1","category":"cs.LG"}
{"created":"2024-05-14 07:55:37","title":"Investigating the 'Autoencoder Behavior' in Speech Self-Supervised Models: a focus on HuBERT's Pretraining","abstract":"Self-supervised learning has shown great success in Speech Recognition. However, it has been observed that finetuning all layers of the learned model leads to lower performance compared to resetting top layers. This phenomenon is attributed to the ''autoencoder'' behavior: top layers contain information closer to the input and are less suitable for tasks that require linguistic information, such as Speech Recognition.To better our understanding of this behavior, we propose to study the evolution of high-level information within the model during pretraining. We focus on the HuBERT model, which exhibits a less pronounced ''autoencoder'' behavior. By experimentally exploring various factors that may have an impact, we aim to improve the training procedure and enhance the top layers of HuBERT for high-level tasks.Furthermore, our experiments demonstrate that these improvements in the training procedure result in faster convergence and competitive performance on downstream tasks.","sentences":["Self-supervised learning has shown great success in Speech Recognition.","However, it has been observed that finetuning all layers of the learned model leads to lower performance compared to resetting top layers.","This phenomenon is attributed to the ''autoencoder'' behavior: top layers contain information closer to the input and are less suitable for tasks that require linguistic information, such as Speech Recognition.","To better our understanding of this behavior, we propose to study the evolution of high-level information within the model during pretraining.","We focus on the HuBERT model, which exhibits a less pronounced ''autoencoder'' behavior.","By experimentally exploring various factors that may have an impact, we aim to improve the training procedure and enhance the top layers of HuBERT for high-level tasks.","Furthermore, our experiments demonstrate that these improvements in the training procedure result in faster convergence and competitive performance on downstream tasks."],"url":"http://arxiv.org/abs/2405.08402v1","category":"cs.CL"}
{"created":"2024-05-14 07:49:52","title":"Self-supervised contrastive learning unveils cortical folding pattern linked to prematurity","abstract":"Brain folding patterns have been reported to carry clinically relevant information. The brain folds mainly during the last trimester of pregnancy, and the process might be durably disturbed by preterm birth. Yet little is known about preterm-specific patterns. In this work, we train a self-supervised model (SimCLR) on the UKBioBank cohort (21070 adults) to represent the right superior temporal sulcus (STS) region and apply it to sulci images of 374 babies from the dHCP database, containing preterms and full-terms, and acquired at 40 weeks post-menstrual age. We find a lower variability in the preterm embeddings, supported by the identification of a knob pattern, missing in the extremely preterm population.","sentences":["Brain folding patterns have been reported to carry clinically relevant information.","The brain folds mainly during the last trimester of pregnancy, and the process might be durably disturbed by preterm birth.","Yet little is known about preterm-specific patterns.","In this work, we train a self-supervised model (SimCLR) on the UKBioBank cohort (21070 adults) to represent the right superior temporal sulcus (STS) region and apply it to sulci images of 374 babies from the dHCP database, containing preterms and full-terms, and acquired at 40 weeks post-menstrual age.","We find a lower variability in the preterm embeddings, supported by the identification of a knob pattern, missing in the extremely preterm population."],"url":"http://arxiv.org/abs/2405.08397v1","category":"q-bio.NC"}
{"created":"2024-05-14 07:05:18","title":"UnMarker: A Universal Attack on Defensive Watermarking","abstract":"Reports regarding the misuse of $\\textit{Generative AI}$ ($\\textit{GenAI}$) to create harmful deepfakes are emerging daily. Recently, defensive watermarking, which enables $\\textit{GenAI}$ providers to hide fingerprints in their images to later use for deepfake detection, has been on the rise. Yet, its potential has not been fully explored. We present $\\textit{UnMarker}$ -- the first practical $\\textit{universal}$ attack on defensive watermarking. Unlike existing attacks, $\\textit{UnMarker}$ requires no detector feedback, no unrealistic knowledge of the scheme or similar models, and no advanced denoising pipelines that may not be available. Instead, being the product of an in-depth analysis of the watermarking paradigm revealing that robust schemes must construct their watermarks in the spectral amplitudes, $\\textit{UnMarker}$ employs two novel adversarial optimizations to disrupt the spectra of watermarked images, erasing the watermarks. Evaluations against the $\\textit{SOTA}$ prove its effectiveness, not only defeating traditional schemes while retaining superior quality compared to existing attacks but also breaking $\\textit{semantic}$ watermarks that alter the image's structure, reducing the best detection rate to $43\\%$ and rendering them useless. To our knowledge, $\\textit{UnMarker}$ is the first practical attack on $\\textit{semantic}$ watermarks, which have been deemed the future of robust watermarking. $\\textit{UnMarker}$ casts doubts on the very penitential of this countermeasure and exposes its paradoxical nature as designing schemes for robustness inevitably compromises other robustness aspects.","sentences":["Reports regarding the misuse of $\\textit{Generative AI}$ ($\\textit{GenAI}$) to create harmful deepfakes are emerging daily.","Recently, defensive watermarking, which enables $\\textit{GenAI}$ providers to hide fingerprints in their images to later use for deepfake detection, has been on the rise.","Yet, its potential has not been fully explored.","We present $\\textit{UnMarker}$ -- the first practical $\\textit{universal}$ attack on defensive watermarking.","Unlike existing attacks, $\\textit{UnMarker}$ requires no detector feedback, no unrealistic knowledge of the scheme or similar models, and no advanced denoising pipelines that may not be available.","Instead, being the product of an in-depth analysis of the watermarking paradigm revealing that robust schemes must construct their watermarks in the spectral amplitudes, $\\textit{UnMarker}$ employs two novel adversarial optimizations to disrupt the spectra of watermarked images, erasing the watermarks.","Evaluations against the $\\textit{SOTA}$ prove its effectiveness, not only defeating traditional schemes while retaining superior quality compared to existing attacks but also breaking $\\textit{semantic}$ watermarks that alter the image's structure, reducing the best detection rate to $43\\%$ and rendering them useless.","To our knowledge, $\\textit{UnMarker}$ is the first practical attack on $\\textit{semantic}$ watermarks, which have been deemed the future of robust watermarking.","$\\textit{UnMarker}$ casts doubts on the very penitential of this countermeasure and exposes its paradoxical nature as designing schemes for robustness inevitably compromises other robustness aspects."],"url":"http://arxiv.org/abs/2405.08363v1","category":"cs.CR"}
{"created":"2024-05-14 06:32:40","title":"No Time to Waste: Squeeze Time into Channel for Mobile Video Understanding","abstract":"Current architectures for video understanding mainly build upon 3D convolutional blocks or 2D convolutions with additional operations for temporal modeling. However, these methods all regard the temporal axis as a separate dimension of the video sequence, which requires large computation and memory budgets and thus limits their usage on mobile devices. In this paper, we propose to squeeze the time axis of a video sequence into the channel dimension and present a lightweight video recognition network, term as \\textit{SqueezeTime}, for mobile video understanding. To enhance the temporal modeling capability of the proposed network, we design a Channel-Time Learning (CTL) Block to capture temporal dynamics of the sequence. This module has two complementary branches, in which one branch is for temporal importance learning and another branch with temporal position restoring capability is to enhance inter-temporal object modeling ability. The proposed SqueezeTime is much lightweight and fast with high accuracies for mobile video understanding. Extensive experiments on various video recognition and action detection benchmarks, i.e., Kinetics400, Kinetics600, HMDB51, AVA2.1 and THUMOS14, demonstrate the superiority of our model. For example, our SqueezeTime achieves $+1.2\\%$ accuracy and $+80\\%$ GPU throughput gain on Kinetics400 than prior methods. Codes are publicly available at https://github.com/xinghaochen/SqueezeTime and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SqueezeTime.","sentences":["Current architectures for video understanding mainly build upon 3D convolutional blocks or 2D convolutions with additional operations for temporal modeling.","However, these methods all regard the temporal axis as a separate dimension of the video sequence, which requires large computation and memory budgets and thus limits their usage on mobile devices.","In this paper, we propose to squeeze the time axis of a video sequence into the channel dimension and present a lightweight video recognition network, term as \\textit{SqueezeTime}, for mobile video understanding.","To enhance the temporal modeling capability of the proposed network, we design a Channel-Time Learning (CTL) Block to capture temporal dynamics of the sequence.","This module has two complementary branches, in which one branch is for temporal importance learning and another branch with temporal position restoring capability is to enhance inter-temporal object modeling ability.","The proposed SqueezeTime is much lightweight and fast with high accuracies for mobile video understanding.","Extensive experiments on various video recognition and action detection benchmarks, i.e., Kinetics400, Kinetics600, HMDB51, AVA2.1 and THUMOS14, demonstrate the superiority of our model.","For example, our SqueezeTime achieves $+1.2\\%$ accuracy and $+80\\%$ GPU throughput gain on Kinetics400 than prior methods.","Codes are publicly available at https://github.com/xinghaochen/SqueezeTime and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SqueezeTime."],"url":"http://arxiv.org/abs/2405.08344v1","category":"cs.CV"}
{"created":"2024-05-14 05:41:59","title":"StraightPCF: Straight Point Cloud Filtering","abstract":"Point cloud filtering is a fundamental 3D vision task, which aims to remove noise while recovering the underlying clean surfaces. State-of-the-art methods remove noise by moving noisy points along stochastic trajectories to the clean surfaces. These methods often require regularization within the training objective and/or during post-processing, to ensure fidelity. In this paper, we introduce StraightPCF, a new deep learning based method for point cloud filtering. It works by moving noisy points along straight paths, thus reducing discretization errors while ensuring faster convergence to the clean surfaces. We model noisy patches as intermediate states between high noise patch variants and their clean counterparts, and design the VelocityModule to infer a constant flow velocity from the former to the latter. This constant flow leads to straight filtering trajectories. In addition, we introduce a DistanceModule that scales the straight trajectory using an estimated distance scalar to attain convergence near the clean surface. Our network is lightweight and only has $\\sim530K$ parameters, being 17% of IterativePFN (a most recent point cloud filtering network). Extensive experiments on both synthetic and real-world data show our method achieves state-of-the-art results. Our method also demonstrates nice distributions of filtered points without the need for regularization. The implementation code can be found at: https://github.com/ddsediri/StraightPCF.","sentences":["Point cloud filtering is a fundamental 3D vision task, which aims to remove noise while recovering the underlying clean surfaces.","State-of-the-art methods remove noise by moving noisy points along stochastic trajectories to the clean surfaces.","These methods often require regularization within the training objective and/or during post-processing, to ensure fidelity.","In this paper, we introduce StraightPCF, a new deep learning based method for point cloud filtering.","It works by moving noisy points along straight paths, thus reducing discretization errors while ensuring faster convergence to the clean surfaces.","We model noisy patches as intermediate states between high noise patch variants and their clean counterparts, and design the VelocityModule to infer a constant flow velocity from the former to the latter.","This constant flow leads to straight filtering trajectories.","In addition, we introduce a DistanceModule that scales the straight trajectory using an estimated distance scalar to attain convergence near the clean surface.","Our network is lightweight and only has $\\sim530K$ parameters, being 17% of IterativePFN (a most recent point cloud filtering network).","Extensive experiments on both synthetic and real-world data show our method achieves state-of-the-art results.","Our method also demonstrates nice distributions of filtered points without the need for regularization.","The implementation code can be found at: https://github.com/ddsediri/StraightPCF."],"url":"http://arxiv.org/abs/2405.08322v1","category":"cs.CV"}
{"created":"2024-05-14 03:50:07","title":"Vector-Symbolic Architecture for Event-Based Optical Flow","abstract":"From a perspective of feature matching, optical flow estimation for event cameras involves identifying event correspondences by comparing feature similarity across accompanying event frames. In this work, we introduces an effective and robust high-dimensional (HD) feature descriptor for event frames, utilizing Vector Symbolic Architectures (VSA). The topological similarity among neighboring variables within VSA contributes to the enhanced representation similarity of feature descriptors for flow-matching points, while its structured symbolic representation capacity facilitates feature fusion from both event polarities and multiple spatial scales. Based on this HD feature descriptor, we propose a novel feature matching framework for event-based optical flow, encompassing both model-based (VSA-Flow) and self-supervised learning (VSA-SM) methods. In VSA-Flow, accurate optical flow estimation validates the effectiveness of HD feature descriptors. In VSA-SM, a novel similarity maximization method based on the HD feature descriptor is proposed to learn optical flow in a self-supervised way from events alone, eliminating the need for auxiliary grayscale images. Evaluation results demonstrate that our VSA-based method achieves superior accuracy in comparison to both model-based and self-supervised learning methods on the DSEC benchmark, while remains competitive among both methods on the MVSEC benchmark. This contribution marks a significant advancement in event-based optical flow within the feature matching methodology.","sentences":["From a perspective of feature matching, optical flow estimation for event cameras involves identifying event correspondences by comparing feature similarity across accompanying event frames.","In this work, we introduces an effective and robust high-dimensional (HD) feature descriptor for event frames, utilizing Vector Symbolic Architectures (VSA).","The topological similarity among neighboring variables within VSA contributes to the enhanced representation similarity of feature descriptors for flow-matching points, while its structured symbolic representation capacity facilitates feature fusion from both event polarities and multiple spatial scales.","Based on this HD feature descriptor, we propose a novel feature matching framework for event-based optical flow, encompassing both model-based (VSA-Flow) and self-supervised learning (VSA-SM) methods.","In VSA-Flow, accurate optical flow estimation validates the effectiveness of HD feature descriptors.","In VSA-SM, a novel similarity maximization method based on the HD feature descriptor is proposed to learn optical flow in a self-supervised way from events alone, eliminating the need for auxiliary grayscale images.","Evaluation results demonstrate that our VSA-based method achieves superior accuracy in comparison to both model-based and self-supervised learning methods on the DSEC benchmark, while remains competitive among both methods on the MVSEC benchmark.","This contribution marks a significant advancement in event-based optical flow within the feature matching methodology."],"url":"http://arxiv.org/abs/2405.08300v2","category":"cs.CV"}
{"created":"2024-05-14 03:33:31","title":"SpeechVerse: A Large-scale Generalizable Audio Language Model","abstract":"Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions. Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation. We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training. The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions. We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks. Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks. Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks.","sentences":["Large language models (LLMs) have shown incredible proficiency in performing tasks that require semantic understanding of natural language instructions.","Recently, many works have further expanded this capability to perceive multimodal audio and text inputs, but their capabilities are often limited to specific fine-tuned tasks such as automatic speech recognition and translation.","We therefore develop SpeechVerse, a robust multi-task training and curriculum learning framework that combines pre-trained speech and text foundation models via a small set of learnable parameters, while keeping the pre-trained models frozen during training.","The models are instruction finetuned using continuous latent representations extracted from the speech foundation model to achieve optimal zero-shot performance on a diverse range of speech processing tasks using natural language instructions.","We perform extensive benchmarking that includes comparing our model performance against traditional baselines across several datasets and tasks.","Furthermore, we evaluate the model's capability for generalized instruction following by testing on out-of-domain datasets, novel prompts, and unseen tasks.","Our empirical experiments reveal that our multi-task SpeechVerse model is even superior to conventional task-specific baselines on 9 out of the 11 tasks."],"url":"http://arxiv.org/abs/2405.08295v1","category":"cs.CL"}
{"created":"2024-05-14 02:50:23","title":"Predicting NVIDIA's Next-Day Stock Price: A Comparative Analysis of LSTM, MLP, ARIMA, and ARIMA-GARCH Models","abstract":"Forecasting stock prices remains a considerable challenge in financial markets, bearing significant implications for investors, traders, and financial institutions. Amid the ongoing AI revolution, NVIDIA has emerged as a key player driving innovation across various sectors. Given its prominence, we chose NVIDIA as the subject of our study.","sentences":["Forecasting stock prices remains a considerable challenge in financial markets, bearing significant implications for investors, traders, and financial institutions.","Amid the ongoing AI revolution, NVIDIA has emerged as a key player driving innovation across various sectors.","Given its prominence, we chose NVIDIA as the subject of our study."],"url":"http://arxiv.org/abs/2405.08284v1","category":"econ.EM"}
{"created":"2024-05-14 01:41:19","title":"Palette-based Color Transfer between Images","abstract":"As an important subtopic of image enhancement, color transfer aims to enhance the color scheme of a source image according to a reference one while preserving the semantic context. To implement color transfer, the palette-based color mapping framework was proposed. \\textcolor{black}{It is a classical solution that does not depend on complex semantic analysis to generate a new color scheme. However, the framework usually requires manual settings, blackucing its practicality.} The quality of traditional palette generation depends on the degree of color separation. In this paper, we propose a new palette-based color transfer method that can automatically generate a new color scheme. With a redesigned palette-based clustering method, pixels can be classified into different segments according to color distribution with better applicability. {By combining deep learning-based image segmentation and a new color mapping strategy, color transfer can be implemented on foreground and background parts independently while maintaining semantic consistency.} The experimental results indicate that our method exhibits significant advantages over peer methods in terms of natural realism, color consistency, generality, and robustness.","sentences":["As an important subtopic of image enhancement, color transfer aims to enhance the color scheme of a source image according to a reference one while preserving the semantic context.","To implement color transfer, the palette-based color mapping framework was proposed.","\\textcolor{black}{It is a classical solution that does not depend on complex semantic analysis to generate a new color scheme.","However, the framework usually requires manual settings, blackucing its practicality.}","The quality of traditional palette generation depends on the degree of color separation.","In this paper, we propose a new palette-based color transfer method that can automatically generate a new color scheme.","With a redesigned palette-based clustering method, pixels can be classified into different segments according to color distribution with better applicability.","{By combining deep learning-based image segmentation and a new color mapping strategy, color transfer can be implemented on foreground and background parts independently while maintaining semantic consistency.}","The experimental results indicate that our method exhibits significant advantages over peer methods in terms of natural realism, color consistency, generality, and robustness."],"url":"http://arxiv.org/abs/2405.08263v1","category":"cs.CV"}
{"created":"2024-05-13 23:24:25","title":"Additive-Effect Assisted Learning","abstract":"It is quite popular nowadays for researchers and data analysts holding different datasets to seek assistance from each other to enhance their modeling performance. We consider a scenario where different learners hold datasets with potentially distinct variables, and their observations can be aligned by a nonprivate identifier. Their collaboration faces the following difficulties: First, learners may need to keep data values or even variable names undisclosed due to, e.g., commercial interest or privacy regulations; second, there are restrictions on the number of transmission rounds between them due to e.g., communication costs. To address these challenges, we develop a two-stage assisted learning architecture for an agent, Alice, to seek assistance from another agent, Bob. In the first stage, we propose a privacy-aware hypothesis testing-based screening method for Alice to decide on the usefulness of the data from Bob, in a way that only requires Bob to transmit sketchy data. Once Alice recognizes Bob's usefulness, Alice and Bob move to the second stage, where they jointly apply a synergistic iterative model training procedure. With limited transmissions of summary statistics, we show that Alice can achieve the oracle performance as if the training were from centralized data, both theoretically and numerically.","sentences":["It is quite popular nowadays for researchers and data analysts holding different datasets to seek assistance from each other to enhance their modeling performance.","We consider a scenario where different learners hold datasets with potentially distinct variables, and their observations can be aligned by a nonprivate identifier.","Their collaboration faces the following difficulties: First, learners may need to keep data values or even variable names undisclosed due to, e.g., commercial interest or privacy regulations; second, there are restrictions on the number of transmission rounds between them due to e.g., communication costs.","To address these challenges, we develop a two-stage assisted learning architecture for an agent, Alice, to seek assistance from another agent, Bob.","In the first stage, we propose a privacy-aware hypothesis testing-based screening method for Alice to decide on the usefulness of the data from Bob, in a way that only requires Bob to transmit sketchy data.","Once Alice recognizes Bob's usefulness, Alice and Bob move to the second stage, where they jointly apply a synergistic iterative model training procedure.","With limited transmissions of summary statistics, we show that Alice can achieve the oracle performance as if the training were from centralized data, both theoretically and numerically."],"url":"http://arxiv.org/abs/2405.08235v1","category":"stat.ML"}
{"created":"2024-05-13 22:48:27","title":"Slow Inter-area Electro-mechanical Oscillations Revisited: Structural Property of Complex Multi-area Electric Power Systems","abstract":"This paper introduces a physically-intuitive notion of inter-area dynamics in systems comprising multiple interconnected energy conversion modules. The ideas build on an earlier general approach to setting their structural properties by modeling first stand-alone modular dynamics starting from the fundamental relations between energy stored in modules (components, areas), and constraining explicitly their Tellegen's quantities, power and rate of change of power, in particular. In this paper we derive, by following the same principles, a transformed state-space model for a general nonlinear system. Using this model we show the existence of an area-level interaction variable, intVar, whose rate of change depends solely on the area internal power imbalance. Given these structural properties of stand-alone modules, we define in this paper for the first time an inter-area variable as the difference of power wave incident to tie-line from Area I and the power reflected into tie-lie from Area II. Notably, these power waves represent the rate of change of intVars associated with the two interconnected areas. We illustrate these notions using a linearized case of two lossless inter-connected areas, and show the existence of a new inter-area mode when the areas get connected. We suggest that lessons learned in this paper open possibilities for computationally-efficient modeling and control of inter-area oscillations, and offer further the basis for modeling and control of dynamics in changing systems comprising faster energy conversion processes.","sentences":["This paper introduces a physically-intuitive notion of inter-area dynamics in systems comprising multiple interconnected energy conversion modules.","The ideas build on an earlier general approach to setting their structural properties by modeling first stand-alone modular dynamics starting from the fundamental relations between energy stored in modules (components, areas), and constraining explicitly their Tellegen's quantities, power and rate of change of power, in particular.","In this paper we derive, by following the same principles, a transformed state-space model for a general nonlinear system.","Using this model we show the existence of an area-level interaction variable, intVar, whose rate of change depends solely on the area internal power imbalance.","Given these structural properties of stand-alone modules, we define in this paper for the first time an inter-area variable as the difference of power wave incident to tie-line from Area I and the power reflected into tie-lie from Area II.","Notably, these power waves represent the rate of change of intVars associated with the two interconnected areas.","We illustrate these notions using a linearized case of two lossless inter-connected areas, and show the existence of a new inter-area mode when the areas get connected.","We suggest that lessons learned in this paper open possibilities for computationally-efficient modeling and control of inter-area oscillations, and offer further the basis for modeling and control of dynamics in changing systems comprising faster energy conversion processes."],"url":"http://arxiv.org/abs/2405.08228v1","category":"eess.SY"}
{"created":"2024-05-13 22:10:00","title":"Data Valuation with Gradient Similarity","abstract":"High-quality data is crucial for accurate machine learning and actionable analytics, however, mislabeled or noisy data is a common problem in many domains. Distinguishing low- from high-quality data can be challenging, often requiring expert knowledge and considerable manual intervention. Data Valuation algorithms are a class of methods that seek to quantify the value of each sample in a dataset based on its contribution or importance to a given predictive task. These data values have shown an impressive ability to identify mislabeled observations, and filtering low-value data can boost machine learning performance. In this work, we present a simple alternative to existing methods, termed Data Valuation with Gradient Similarity (DVGS). This approach can be easily applied to any gradient descent learning algorithm, scales well to large datasets, and performs comparably or better than baseline valuation methods for tasks such as corrupted label discovery and noise quantification. We evaluate the DVGS method on tabular, image and RNA expression datasets to show the effectiveness of the method across domains. Our approach has the ability to rapidly and accurately identify low-quality data, which can reduce the need for expert knowledge and manual intervention in data cleaning tasks.","sentences":["High-quality data is crucial for accurate machine learning and actionable analytics, however, mislabeled or noisy data is a common problem in many domains.","Distinguishing low- from high-quality data can be challenging, often requiring expert knowledge and considerable manual intervention.","Data Valuation algorithms are a class of methods that seek to quantify the value of each sample in a dataset based on its contribution or importance to a given predictive task.","These data values have shown an impressive ability to identify mislabeled observations, and filtering low-value data can boost machine learning performance.","In this work, we present a simple alternative to existing methods, termed Data Valuation with Gradient Similarity (DVGS).","This approach can be easily applied to any gradient descent learning algorithm, scales well to large datasets, and performs comparably or better than baseline valuation methods for tasks such as corrupted label discovery and noise quantification.","We evaluate the DVGS method on tabular, image and RNA expression datasets to show the effectiveness of the method across domains.","Our approach has the ability to rapidly and accurately identify low-quality data, which can reduce the need for expert knowledge and manual intervention in data cleaning tasks."],"url":"http://arxiv.org/abs/2405.08217v1","category":"cs.LG"}
{"created":"2024-05-13 21:53:09","title":"Infinite Texture: Text-guided High Resolution Diffusion Texture Synthesis","abstract":"We present Infinite Texture, a method for generating arbitrarily large texture images from a text prompt. Our approach fine-tunes a diffusion model on a single texture, and learns to embed that statistical distribution in the output domain of the model. We seed this fine-tuning process with a sample texture patch, which can be optionally generated from a text-to-image model like DALL-E 2. At generation time, our fine-tuned diffusion model is used through a score aggregation strategy to generate output texture images of arbitrary resolution on a single GPU. We compare synthesized textures from our method to existing work in patch-based and deep learning texture synthesis methods. We also showcase two applications of our generated textures in 3D rendering and texture transfer.","sentences":["We present Infinite Texture, a method for generating arbitrarily large texture images from a text prompt.","Our approach fine-tunes a diffusion model on a single texture, and learns to embed that statistical distribution in the output domain of the model.","We seed this fine-tuning process with a sample texture patch, which can be optionally generated from a text-to-image model like DALL-E 2.","At generation time, our fine-tuned diffusion model is used through a score aggregation strategy to generate output texture images of arbitrary resolution on a single GPU.","We compare synthesized textures from our method to existing work in patch-based and deep learning texture synthesis methods.","We also showcase two applications of our generated textures in 3D rendering and texture transfer."],"url":"http://arxiv.org/abs/2405.08210v1","category":"cs.CV"}
{"created":"2024-05-13 21:53:06","title":"Who's in and who's out? A case study of multimodal CLIP-filtering in DataComp","abstract":"As training datasets become increasingly drawn from unstructured, uncontrolled environments such as the web, researchers and industry practitioners have increasingly relied upon data filtering techniques to \"filter out the noise\" of web-scraped data. While datasets have been widely shown to reflect the biases and values of their creators, in this paper we contribute to an emerging body of research that assesses the filters used to create these datasets. We show that image-text data filtering also has biases and is value-laden, encoding specific notions of what is counted as \"high-quality\" data. In our work, we audit a standard approach of image-text CLIP-filtering on the academic benchmark DataComp's CommonPool by analyzing discrepancies of filtering through various annotation techniques across multiple modalities of image, text, and website source. We find that data relating to several imputed demographic groups -- such as LGBTQ+ people, older women, and younger men -- are associated with higher rates of exclusion. Moreover, we demonstrate cases of exclusion amplification: not only are certain marginalized groups already underrepresented in the unfiltered data, but CLIP-filtering excludes data from these groups at higher rates. The data-filtering step in the machine learning pipeline can therefore exacerbate representation disparities already present in the data-gathering step, especially when existing filters are designed to optimize a specifically-chosen downstream performance metric like zero-shot image classification accuracy. Finally, we show that the NSFW filter fails to remove sexually-explicit content from CommonPool, and that CLIP-filtering includes several categories of copyrighted content at high rates. Our conclusions point to a need for fundamental changes in dataset creation and filtering practices.","sentences":["As training datasets become increasingly drawn from unstructured, uncontrolled environments such as the web, researchers and industry practitioners have increasingly relied upon data filtering techniques to \"filter out the noise\" of web-scraped data.","While datasets have been widely shown to reflect the biases and values of their creators, in this paper we contribute to an emerging body of research that assesses the filters used to create these datasets.","We show that image-text data filtering also has biases and is value-laden, encoding specific notions of what is counted as \"high-quality\" data.","In our work, we audit a standard approach of image-text CLIP-filtering on the academic benchmark DataComp's CommonPool by analyzing discrepancies of filtering through various annotation techniques across multiple modalities of image, text, and website source.","We find that data relating to several imputed demographic groups -- such as LGBTQ+ people, older women, and younger men -- are associated with higher rates of exclusion.","Moreover, we demonstrate cases of exclusion amplification: not only are certain marginalized groups already underrepresented in the unfiltered data, but CLIP-filtering excludes data from these groups at higher rates.","The data-filtering step in the machine learning pipeline can therefore exacerbate representation disparities already present in the data-gathering step, especially when existing filters are designed to optimize a specifically-chosen downstream performance metric like zero-shot image classification accuracy.","Finally, we show that the NSFW filter fails to remove sexually-explicit content from CommonPool, and that CLIP-filtering includes several categories of copyrighted content at high rates.","Our conclusions point to a need for fundamental changes in dataset creation and filtering practices."],"url":"http://arxiv.org/abs/2405.08209v1","category":"cs.CY"}
{"created":"2024-05-13 21:48:48","title":"Generative Enzyme Design Guided by Functionally Important Sites and Small-Molecule Substrates","abstract":"Enzymes are genetically encoded biocatalysts capable of accelerating chemical reactions. How can we automatically design functional enzymes? In this paper, we propose EnzyGen, an approach to learn a unified model to design enzymes across all functional families. Our key idea is to generate an enzyme's amino acid sequence and their three-dimensional (3D) coordinates based on functionally important sites and substrates corresponding to a desired catalytic function. These sites are automatically mined from enzyme databases. EnzyGen consists of a novel interleaving network of attention and neighborhood equivariant layers, which captures both long-range correlation in an entire protein sequence and local influence from nearest amino acids in 3D space. To learn the generative model, we devise a joint training objective, including a sequence generation loss, a position prediction loss and an enzyme-substrate interaction loss. We further construct EnzyBench, a dataset with 3157 enzyme families, covering all available enzymes within the protein data bank (PDB). Experimental results show that our EnzyGen consistently achieves the best performance across all 323 testing families, surpassing the best baseline by 10.79% in terms of substrate binding affinity. These findings demonstrate EnzyGen's superior capability in designing well-folded and effective enzymes binding to specific substrates with high affinities.","sentences":["Enzymes are genetically encoded biocatalysts capable of accelerating chemical reactions.","How can we automatically design functional enzymes?","In this paper, we propose EnzyGen, an approach to learn a unified model to design enzymes across all functional families.","Our key idea is to generate an enzyme's amino acid sequence and their three-dimensional (3D) coordinates based on functionally important sites and substrates corresponding to a desired catalytic function.","These sites are automatically mined from enzyme databases.","EnzyGen consists of a novel interleaving network of attention and neighborhood equivariant layers, which captures both long-range correlation in an entire protein sequence and local influence from nearest amino acids in 3D space.","To learn the generative model, we devise a joint training objective, including a sequence generation loss, a position prediction loss and an enzyme-substrate interaction loss.","We further construct EnzyBench, a dataset with 3157 enzyme families, covering all available enzymes within the protein data bank (PDB).","Experimental results show that our EnzyGen consistently achieves the best performance across all 323 testing families, surpassing the best baseline by 10.79% in terms of substrate binding affinity.","These findings demonstrate EnzyGen's superior capability in designing well-folded and effective enzymes binding to specific substrates with high affinities."],"url":"http://arxiv.org/abs/2405.08205v1","category":"cs.LG"}
{"created":"2024-05-13 21:21:44","title":"IHC Matters: Incorporating IHC analysis to H&E Whole Slide Image Analysis for Improved Cancer Grading via Two-stage Multimodal Bilinear Pooling Fusion","abstract":"Immunohistochemistry (IHC) plays a crucial role in pathology as it detects the over-expression of protein in tissue samples. However, there are still fewer machine learning model studies on IHC's impact on accurate cancer grading. We discovered that IHC and H\\&E possess distinct advantages and disadvantages while possessing certain complementary qualities. Building on this observation, we developed a two-stage multi-modal bilinear model with a feature pooling module. This model aims to maximize the potential of both IHC and HE's feature representation, resulting in improved performance compared to their individual use. Our experiments demonstrate that incorporating IHC data into machine learning models, alongside H\\&E stained images, leads to superior predictive results for cancer grading. The proposed framework achieves an impressive ACC higher of 0.953 on the public dataset BCI.","sentences":["Immunohistochemistry (IHC) plays a crucial role in pathology as it detects the over-expression of protein in tissue samples.","However, there are still fewer machine learning model studies on IHC's impact on accurate cancer grading.","We discovered that IHC and H\\&E possess distinct advantages and disadvantages while possessing certain complementary qualities.","Building on this observation, we developed a two-stage multi-modal bilinear model with a feature pooling module.","This model aims to maximize the potential of both IHC and HE's feature representation, resulting in improved performance compared to their individual use.","Our experiments demonstrate that incorporating IHC data into machine learning models, alongside H\\&E stained images, leads to superior predictive results for cancer grading.","The proposed framework achieves an impressive ACC higher of 0.953 on the public dataset BCI."],"url":"http://arxiv.org/abs/2405.08197v1","category":"cs.CV"}
{"created":"2024-05-13 19:50:08","title":"cVIL: Class-Centric Visual Interactive Labeling","abstract":"We present cVIL, a class-centric approach to visual interactive labeling, which facilitates human annotation of large and complex image data sets. cVIL uses different property measures to support instance labeling for labeling difficult instances and batch labeling to quickly label easy instances. Simulated experiments reveal that cVIL with batch labeling can outperform traditional labeling approaches based on active learning. In a user study, cVIL led to better accuracy and higher user preference compared to a traditional instance-based visual interactive labeling approach based on 2D scatterplots.","sentences":["We present cVIL, a class-centric approach to visual interactive labeling, which facilitates human annotation of large and complex image data sets.","cVIL uses different property measures to support instance labeling for labeling difficult instances and batch labeling to quickly label easy instances.","Simulated experiments reveal that cVIL with batch labeling can outperform traditional labeling approaches based on active learning.","In a user study, cVIL led to better accuracy and higher user preference compared to a traditional instance-based visual interactive labeling approach based on 2D scatterplots."],"url":"http://arxiv.org/abs/2405.08150v1","category":"cs.HC"}
{"created":"2024-05-13 19:31:00","title":"LATTE: an atomic environment descriptor based on Cartesian tensor contractions","abstract":"We propose a new descriptor for local atomic environments, to be used in combination with machine learning models for the construction of interatomic potentials. The Local Atomic Tensors Trainable Expansion (LATTE) allows for the efficient construction of a variable number of many-body terms with learnable parameters, resulting in a descriptor that is efficient, expressive, and can be scaled to suit different accuracy and computational cost requirements. We compare this new descriptor to existing ones on several systems, showing it to be competitive with very fast potentials at one end of the spectrum, and extensible to an accuracy close to the state of the art.","sentences":["We propose a new descriptor for local atomic environments, to be used in combination with machine learning models for the construction of interatomic potentials.","The Local Atomic Tensors Trainable Expansion (LATTE) allows for the efficient construction of a variable number of many-body terms with learnable parameters, resulting in a descriptor that is efficient, expressive, and can be scaled to suit different accuracy and computational cost requirements.","We compare this new descriptor to existing ones on several systems, showing it to be competitive with very fast potentials at one end of the spectrum, and extensible to an accuracy close to the state of the art."],"url":"http://arxiv.org/abs/2405.08137v1","category":"physics.comp-ph"}
{"created":"2024-05-13 18:28:39","title":"Can machine learning unlock new insights into high-frequency trading?","abstract":"We design and train machine learning models to capture the nonlinear interactions between financial market dynamics and high-frequency trading (HFT) activity. In doing so, we introduce new metrics to identify liquidity-demanding and -supplying HFT strategies. Both types of HFT strategies increase activity in response to information events and decrease it when trading speed is restricted, with liquidity-supplying strategies demonstrating greater responsiveness. Liquidity-demanding HFT is positively linked with latency arbitrage opportunities, whereas liquidity-supplying HFT is negatively related, aligning with theoretical expectations. Our metrics have implications for understanding the information production process in financial markets.","sentences":["We design and train machine learning models to capture the nonlinear interactions between financial market dynamics and high-frequency trading (HFT) activity.","In doing so, we introduce new metrics to identify liquidity-demanding and -supplying HFT strategies.","Both types of HFT strategies increase activity in response to information events and decrease it when trading speed is restricted, with liquidity-supplying strategies demonstrating greater responsiveness.","Liquidity-demanding HFT is positively linked with latency arbitrage opportunities, whereas liquidity-supplying HFT is negatively related, aligning with theoretical expectations.","Our metrics have implications for understanding the information production process in financial markets."],"url":"http://arxiv.org/abs/2405.08101v1","category":"q-fin.CP"}
{"created":"2024-05-13 18:01:57","title":"PrivFED -- A Framework for Privacy-Preserving Federated Learning in Enhanced Breast Cancer Diagnosis","abstract":"In the day-to-day operations of healthcare institutions, a multitude of Personally Identifiable Information (PII) data exchanges occur, exposing the data to a spectrum of cybersecurity threats. This study introduces a federated learning framework, trained on the Wisconsin dataset, to mitigate challenges such as data scarcity and imbalance. Techniques like the Synthetic Minority Over-sampling Technique (SMOTE) are incorporated to bolster robustness, while isolation forests are employed to fortify the model against outliers. Catboost serves as the classification tool across all devices. The identification of optimal features for heightened accuracy is pursued through Principal Component Analysis (PCA),accentuating the significance of hyperparameter tuning, as underscored in a comparative analysis. The model exhibits an average accuracy of 99.95% on edge devices and 98% on the central server.","sentences":["In the day-to-day operations of healthcare institutions, a multitude of Personally Identifiable Information (PII) data exchanges occur, exposing the data to a spectrum of cybersecurity threats.","This study introduces a federated learning framework, trained on the Wisconsin dataset, to mitigate challenges such as data scarcity and imbalance.","Techniques like the Synthetic Minority Over-sampling Technique (SMOTE) are incorporated to bolster robustness, while isolation forests are employed to fortify the model against outliers.","Catboost serves as the classification tool across all devices.","The identification of optimal features for heightened accuracy is pursued through Principal Component Analysis (PCA),accentuating the significance of hyperparameter tuning, as underscored in a comparative analysis.","The model exhibits an average accuracy of 99.95% on edge devices and 98% on the central server."],"url":"http://arxiv.org/abs/2405.08084v1","category":"cs.CR"}
{"created":"2024-05-14 17:06:06","title":"An optimization-based construction procedure for function space based summation-by-parts operators on arbitrary grids","abstract":"We introduce a novel construction procedure for one-dimensional summation-by-parts (SBP) operators. Existing construction procedures for FSBP operators of the form $D = P^{-1} Q$ proceed as follows: Given a boundary operator $B$, the norm matrix $P$ is first determined and then in a second step the complementary matrix $Q$ is calculated to finally get the FSBP operator $D$. In contrast, the approach proposed here determines the norm and complementary matrices, $P$ and $Q$, simultaneously by solving an optimization problem. The proposed construction procedure applies to classical SBP operators based on polynomial approximation and the broader class of function space SBP (FSBP) operators. According to our experiments, the presented approach yields a numerically stable construction procedure and FSBP operators with higher accuracy for diagonal norm difference operators at the boundaries than the traditional approach. Through numerical simulations, we highlight the advantages of our proposed technique.","sentences":["We introduce a novel construction procedure for one-dimensional summation-by-parts (SBP) operators.","Existing construction procedures for FSBP operators of the form $D = P^{-1} Q$ proceed as follows:","Given a boundary operator $B$, the norm matrix $P$ is first determined and then in a second step the complementary matrix $Q$ is calculated to finally get the FSBP operator $D$. In contrast, the approach proposed here determines the norm and complementary matrices, $P$ and $Q$, simultaneously by solving an optimization problem.","The proposed construction procedure applies to classical SBP operators based on polynomial approximation and the broader class of function space SBP (FSBP) operators.","According to our experiments, the presented approach yields a numerically stable construction procedure and FSBP operators with higher accuracy for diagonal norm difference operators at the boundaries than the traditional approach.","Through numerical simulations, we highlight the advantages of our proposed technique."],"url":"http://arxiv.org/abs/2405.08770v1","category":"math.NA"}
{"created":"2024-05-14 16:33:22","title":"Minimax optimal seriation in polynomial time","abstract":"We consider the statistical seriation problem, where the statistician seeks to recover a hidden ordering from a noisy observation of a permuted Robinson matrix. In this paper, we tightly characterize the minimax rate for this problem of matrix reordering when the Robinson matrix is bi-Lipschitz, and we also provide a polynomial time algorithm achieving this rate; thereby answering two open questions of [Giraud et al., 2021]. Our analysis further extends to broader classes of similarity matrices.","sentences":["We consider the statistical seriation problem, where the statistician seeks to recover a hidden ordering from a noisy observation of a permuted Robinson matrix.","In this paper, we tightly characterize the minimax rate for this problem of matrix reordering when the Robinson matrix is bi-Lipschitz, and we also provide a polynomial time algorithm achieving this rate; thereby answering two open questions of [Giraud et al., 2021].","Our analysis further extends to broader classes of similarity matrices."],"url":"http://arxiv.org/abs/2405.08747v1","category":"math.ST"}
{"created":"2024-05-14 13:28:57","title":"Variable Substitution and Bilinear Programming for Aligning Partially Overlapping Point Sets","abstract":"In many applications, the demand arises for algorithms capable of aligning partially overlapping point sets while remaining invariant to the corresponding transformations. This research presents a method designed to meet such requirements through minimization of the objective function of the robust point matching (RPM) algorithm. First, we show that the RPM objective is a cubic polynomial. Then, through variable substitution, we transform the RPM objective to a quadratic function. Leveraging the convex envelope of bilinear monomials, we proceed to relax the resulting objective function, thus obtaining a lower bound problem that can be conveniently decomposed into distinct linear assignment and low-dimensional convex quadratic program components, both amenable to efficient optimization. Furthermore, a branch-and-bound (BnB) algorithm is devised, which solely branches over the transformation parameters, thereby boosting convergence rate. Empirical evaluations demonstrate better robustness of the proposed methodology against non-rigid deformation, positional noise, and outliers, particularly in scenarios where outliers remain distinct from inliers, when compared with prevailing state-of-the-art approaches.","sentences":["In many applications, the demand arises for algorithms capable of aligning partially overlapping point sets while remaining invariant to the corresponding transformations.","This research presents a method designed to meet such requirements through minimization of the objective function of the robust point matching (RPM) algorithm.","First, we show that the RPM objective is a cubic polynomial.","Then, through variable substitution, we transform the RPM objective to a quadratic function.","Leveraging the convex envelope of bilinear monomials, we proceed to relax the resulting objective function, thus obtaining a lower bound problem that can be conveniently decomposed into distinct linear assignment and low-dimensional convex quadratic program components, both amenable to efficient optimization.","Furthermore, a branch-and-bound (BnB) algorithm is devised, which solely branches over the transformation parameters, thereby boosting convergence rate.","Empirical evaluations demonstrate better robustness of the proposed methodology against non-rigid deformation, positional noise, and outliers, particularly in scenarios where outliers remain distinct from inliers, when compared with prevailing state-of-the-art approaches."],"url":"http://arxiv.org/abs/2405.08589v1","category":"cs.CV"}
{"created":"2024-05-14 11:56:06","title":"Cooperative Sensing of Side Lobes Interference for mmWave Blockages Localization and Mapping","abstract":"Radio localization and sensing are anticipated to play a crucial role in enhancing radio resource management in future networks. In this work, we focus on millimeter-wave communications, which are highly vulnerable to blockages, leading to severe attenuation and performance degradation. In a previous work, we proposed a novel mechanism that senses the radio environment to estimate the angular position of a moving blocker with respect to the sensing node. Building upon this foundation, this paper investigates the benefits of cooperation between different entities in the network by sharing sensed data to jointly locate the moving blocker while mapping the interference profile to probe the radio environment. Numerical evaluations demonstrate that cooperative sensing can achieve a more precise location estimation of the blocker as it further allows accurate estimation of its distance rather than its relative angular position only, leading to effective assessment of the blocker direction, trajectory and possibly, its speed, and size.","sentences":["Radio localization and sensing are anticipated to play a crucial role in enhancing radio resource management in future networks.","In this work, we focus on millimeter-wave communications, which are highly vulnerable to blockages, leading to severe attenuation and performance degradation.","In a previous work, we proposed a novel mechanism that senses the radio environment to estimate the angular position of a moving blocker with respect to the sensing node.","Building upon this foundation, this paper investigates the benefits of cooperation between different entities in the network by sharing sensed data to jointly locate the moving blocker while mapping the interference profile to probe the radio environment.","Numerical evaluations demonstrate that cooperative sensing can achieve a more precise location estimation of the blocker as it further allows accurate estimation of its distance rather than its relative angular position only, leading to effective assessment of the blocker direction, trajectory and possibly, its speed, and size."],"url":"http://arxiv.org/abs/2405.08521v1","category":"eess.SP"}
{"created":"2024-05-14 11:06:12","title":"IPC: Incremental Probabilistic Consensus-based Consistent Set Maximization for SLAM Backends","abstract":"In SLAM (Simultaneous localization and mapping) problems, Pose Graph Optimization (PGO) is a technique to refine an initial estimate of a set of poses (positions and orientations) from a set of pairwise relative measurements. The optimization procedure can be negatively affected even by a single outlier measurement, with possible catastrophic and meaningless results. Although recent works on robust optimization aim to mitigate the presence of outlier measurements, robust solutions capable of handling large numbers of outliers are yet to come. This paper presents IPC, acronym for Incremental Probabilistic Consensus, a method that approximates the solution to the combinatorial problem of finding the maximally consistent set of measurements in an incremental fashion. It evaluates the consistency of each loop closure measurement through a consensus-based procedure, possibly applied to a subset of the global problem, where all previously integrated inlier measurements have veto power. We evaluated IPC on standard benchmarks against several state-of-the-art methods. Although it is simple and relatively easy to implement, IPC competes with or outperforms the other tested methods in handling outliers while providing online performances. We release with this paper an open-source implementation of the proposed method.","sentences":["In SLAM (Simultaneous localization and mapping) problems, Pose Graph Optimization (PGO) is a technique to refine an initial estimate of a set of poses (positions and orientations) from a set of pairwise relative measurements.","The optimization procedure can be negatively affected even by a single outlier measurement, with possible catastrophic and meaningless results.","Although recent works on robust optimization aim to mitigate the presence of outlier measurements, robust solutions capable of handling large numbers of outliers are yet to come.","This paper presents IPC, acronym for Incremental Probabilistic Consensus, a method that approximates the solution to the combinatorial problem of finding the maximally consistent set of measurements in an incremental fashion.","It evaluates the consistency of each loop closure measurement through a consensus-based procedure, possibly applied to a subset of the global problem, where all previously integrated inlier measurements have veto power.","We evaluated IPC on standard benchmarks against several state-of-the-art methods.","Although it is simple and relatively easy to implement, IPC competes with or outperforms the other tested methods in handling outliers while providing online performances.","We release with this paper an open-source implementation of the proposed method."],"url":"http://arxiv.org/abs/2405.08503v1","category":"cs.RO"}
{"created":"2024-05-14 10:54:20","title":"Is Less More? Quality, Quantity and Context in Idiom Processing with Natural Language Models","abstract":"Compositionality in language models presents a problem when processing idiomatic expressions, as their meaning often cannot be directly derived from their individual parts. Although fine-tuning and other optimization strategies can be used to improve representations of idiomatic expressions, this depends on the availability of relevant data. We present the Noun Compound Synonym Substitution in Books - NCSSB - datasets, which are created by substitution of synonyms of potentially idiomatic English noun compounds in public domain book texts. We explore the trade-off between data quantity and quality when training models for idiomaticity detection, in conjunction with contextual information obtained locally (from the surrounding sentences) or externally (through language resources). Performance on an idiomaticity detection task indicates that dataset quality is a stronger factor for context-enriched models, but that quantity also plays a role in models without context inclusion strategies.","sentences":["Compositionality in language models presents a problem when processing idiomatic expressions, as their meaning often cannot be directly derived from their individual parts.","Although fine-tuning and other optimization strategies can be used to improve representations of idiomatic expressions, this depends on the availability of relevant data.","We present the Noun Compound Synonym Substitution in Books - NCSSB - datasets, which are created by substitution of synonyms of potentially idiomatic English noun compounds in public domain book texts.","We explore the trade-off between data quantity and quality when training models for idiomaticity detection, in conjunction with contextual information obtained locally (from the surrounding sentences) or externally (through language resources).","Performance on an idiomaticity detection task indicates that dataset quality is a stronger factor for context-enriched models, but that quantity also plays a role in models without context inclusion strategies."],"url":"http://arxiv.org/abs/2405.08497v1","category":"cs.CL"}
{"created":"2024-05-14 09:51:27","title":"Sparse MTTKRP Acceleration for Tensor Decomposition on GPU","abstract":"Sparse Matricized Tensor Times Khatri-Rao Product (spMTTKRP) is the bottleneck kernel of sparse tensor decomposition. In this work, we propose a GPU-based algorithm design to address the key challenges in accelerating spMTTKRP computation, including (1) eliminating global atomic operations across GPU thread blocks, (2) avoiding the intermediate values being communicated between GPU thread blocks and GPU global memory, and (3) ensuring a balanced distribution of workloads across GPU thread blocks. Our approach also supports dynamic tensor remapping, enabling the above optimizations in all the modes of the input tensor. Our approach achieves a geometric mean speedup of 1.5x, 2.0x, and 21.7x in total execution time across widely used datasets compared with the state-of-the-art GPU implementations. Our work is the only GPU implementation that can support tensors with modes greater than 4 since the state-of-the-art works have implementation constraints for tensors with a large number of modes.","sentences":["Sparse Matricized Tensor Times Khatri-Rao Product (spMTTKRP) is the bottleneck kernel of sparse tensor decomposition.","In this work, we propose a GPU-based algorithm design to address the key challenges in accelerating spMTTKRP computation, including (1) eliminating global atomic operations across GPU thread blocks, (2) avoiding the intermediate values being communicated between GPU thread blocks and GPU global memory, and (3) ensuring a balanced distribution of workloads across GPU thread blocks.","Our approach also supports dynamic tensor remapping, enabling the above optimizations in all the modes of the input tensor.","Our approach achieves a geometric mean speedup of 1.5x, 2.0x, and 21.7x in total execution time across widely used datasets compared with the state-of-the-art GPU implementations.","Our work is the only GPU implementation that can support tensors with modes greater than 4 since the state-of-the-art works have implementation constraints for tensors with a large number of modes."],"url":"http://arxiv.org/abs/2405.08470v1","category":"cs.DC"}
{"created":"2024-05-14 07:57:42","title":"A constraint-based approach to function interpolation, with application to performance estimation for weakly convex optimisation","abstract":"We propose a novel approach to obtain interpolation constraints for a wide range of function classes, i.e. necessary and sufficient constraints that a set of points, functions values and (sub)gradients must satisfy to ensure the existence of a global function of the class considered, consistent with this set. The derivation of such constraints is crucial for instance in the performance analysis of optimization methods, since obtaining a priori tight performance guarantees requires using a tight description of function classes of interest. Our method allows setting aside all analytic properties of the function class to work only at an algebraic level, and to easily obtain counterexamples when a condition characterizing a function class cannot serve as an interpolation constraint. As an illustration, we provide interpolation constraints for a class of non convex non smooth functions: weakly convex functions with bounded subgradients, and rely on these new interpolation constraints to outperform state of the art bounds on the performance of the subgradient method on this class.","sentences":["We propose a novel approach to obtain interpolation constraints for a wide range of function classes, i.e. necessary and sufficient constraints that a set of points, functions values and (sub)gradients must satisfy to ensure the existence of a global function of the class considered, consistent with this set.","The derivation of such constraints is crucial for instance in the performance analysis of optimization methods, since obtaining a priori tight performance guarantees requires using a tight description of function classes of interest.","Our method allows setting aside all analytic properties of the function class to work only at an algebraic level, and to easily obtain counterexamples when a condition characterizing a function class cannot serve as an interpolation constraint.","As an illustration, we provide interpolation constraints for a class of non convex non smooth functions: weakly convex functions with bounded subgradients, and rely on these new interpolation constraints to outperform state of the art bounds on the performance of the subgradient method on this class."],"url":"http://arxiv.org/abs/2405.08405v1","category":"math.OC"}
{"created":"2024-05-14 07:34:41","title":"Quest for an efficient mathematical and computational method to explore optimal extreme weather modification","abstract":"It is a grand challenge to find a feasible weather modification method to mitigate the impact of extreme weather events such as tropical cyclones. Previous works have proposed potentially effective actuators and assessed their capabilities to achieve weather modification objectives through numerical simulations. However, few studies have explored efficient mathematical and computational methods to inversely determine optimal actuators from specific modification goals. Here I demonstrate the utility of the ensemble Kalman filter (EnKF)-based control method, referred to as ensemble Kalman control (EnKC). The series of numerical experiments with the Lorenz 96 model indicates that EnKC efficiently identifies local, small, and intermittent control perturbations that can mitigate extreme events. The existing techniques of EnKF, such as background error covariance localization and observation error covariance inflation, can improve the sparsity and efficiency of the control. This work paves the way toward the real-world applications of EnKC to explore the controllability of extreme atmospheric events.","sentences":["It is a grand challenge to find a feasible weather modification method to mitigate the impact of extreme weather events such as tropical cyclones.","Previous works have proposed potentially effective actuators and assessed their capabilities to achieve weather modification objectives through numerical simulations.","However, few studies have explored efficient mathematical and computational methods to inversely determine optimal actuators from specific modification goals.","Here I demonstrate the utility of the ensemble Kalman filter (EnKF)-based control method, referred to as ensemble Kalman control (EnKC).","The series of numerical experiments with the Lorenz 96 model indicates that EnKC efficiently identifies local, small, and intermittent control perturbations that can mitigate extreme events.","The existing techniques of EnKF, such as background error covariance localization and observation error covariance inflation, can improve the sparsity and efficiency of the control.","This work paves the way toward the real-world applications of EnKC to explore the controllability of extreme atmospheric events."],"url":"http://arxiv.org/abs/2405.08387v1","category":"stat.AP"}
{"created":"2024-05-14 07:05:37","title":"A Riemannian Proximal Newton-CG Method","abstract":"Recently, a Riemannian proximal Newton method has been developed for optimizing problems in the form of $\\min_{x\\in\\mathcal{M}} f(x) + \\mu \\|x\\|_1$, where $\\mathcal{M}$ is a compact embedded submanifold and $f(x)$ is smooth. Although this method converges superlinearly locally, global convergence is not guaranteed. The existing remedy relies on a hybrid approach: running a Riemannian proximal gradient method until the iterate is sufficiently accurate and switching to the Riemannian proximal Newton method. This existing approach is sensitive to the switching parameter. This paper proposes a Riemannian proximal Newton-CG method that merges the truncated conjugate gradient method with the Riemannian proximal Newton method. The global convergence and local superlinear convergence are proven. Numerical experiments show that the proposed method outperforms other state-of-the-art methods.","sentences":["Recently, a Riemannian proximal Newton method has been developed for optimizing problems in the form of $\\min_{x\\in\\mathcal{M}} f(x)","+ \\mu \\|x\\|_1$, where $\\mathcal{M}$ is a compact embedded submanifold and $f(x)$ is smooth.","Although this method converges superlinearly locally, global convergence is not guaranteed.","The existing remedy relies on a hybrid approach: running a Riemannian proximal gradient method until the iterate is sufficiently accurate and switching to the Riemannian proximal Newton method.","This existing approach is sensitive to the switching parameter.","This paper proposes a Riemannian proximal Newton-CG method that merges the truncated conjugate gradient method with the Riemannian proximal Newton method.","The global convergence and local superlinear convergence are proven.","Numerical experiments show that the proposed method outperforms other state-of-the-art methods."],"url":"http://arxiv.org/abs/2405.08365v1","category":"math.OC"}
{"created":"2024-05-14 03:53:17","title":"Coded Downlink Massive Random Access and a Finite de Finetti Theorem","abstract":"This paper considers a massive connectivity setting in which a base-station (BS) aims to communicate sources $(X_1,\\cdots,X_k)$ to a randomly activated subset of $k$ users, among a large pool of $n$ users, via a common downlink message. Although the identities of the $k$ active users are assumed to be known at the BS, each active user only knows whether itself is active and does not know the identities of the other active users. A naive coding strategy is to transmit the sources alongside the identities of the users for which the source information is intended, which would require $H(X_1,\\cdots,X_k) + k\\log(n)$ bits, because the cost of specifying the identity of a user is $\\log(n)$ bits. For large $n$, this overhead can be significant. This paper shows that it is possible to develop coding techniques that eliminate the dependency of the overhead on $n$, if the source distribution follows certain symmetry. Specifically, if the source distribution is independent and identically distributed (i.i.d.) then the overhead can be reduced to at most $O(\\log(k))$ bits, and in case of uniform i.i.d. sources, the overhead can be further reduced to $O(1)$ bits. For sources that follow a more general exchangeable distribution, the overhead is at most $O(k)$ bits, and in case of finite-alphabet exchangeable sources, the overhead can be further reduced to $O(\\log(k))$ bits. The downlink massive random access problem is closely connected to the study of finite exchangeable sequences. The proposed coding strategy allows bounds on the relative entropy distance between finite exchangeable distributions and i.i.d. mixture distributions to be developed, and gives a new relative entropy version of the finite de Finetti theorem which is scaling optimal.","sentences":["This paper considers a massive connectivity setting in which a base-station (BS) aims to communicate sources $(X_1,\\cdots,X_k)$ to a randomly activated subset of $k$ users, among a large pool of $n$ users, via a common downlink message.","Although the identities of the $k$ active users are assumed to be known at the BS, each active user only knows whether itself is active and does not know the identities of the other active users.","A naive coding strategy is to transmit the sources alongside the identities of the users for which the source information is intended, which would require $H(X_1,\\cdots,X_k)","+ k\\log(n)$ bits, because the cost of specifying the identity of a user is $\\log(n)$ bits.","For large $n$, this overhead can be significant.","This paper shows that it is possible to develop coding techniques that eliminate the dependency of the overhead on $n$, if the source distribution follows certain symmetry.","Specifically, if the source distribution is independent and identically distributed (i.i.d.)","then the overhead can be reduced to at most $O(\\log(k))$ bits, and in case of uniform i.i.d. sources, the overhead can be further reduced to $O(1)$ bits.","For sources that follow a more general exchangeable distribution, the overhead is at most $O(k)$ bits, and in case of finite-alphabet exchangeable sources, the overhead can be further reduced to $O(\\log(k))$ bits.","The downlink massive random access problem is closely connected to the study of finite exchangeable sequences.","The proposed coding strategy allows bounds on the relative entropy distance between finite exchangeable distributions and i.i.d. mixture distributions to be developed, and gives a new relative entropy version of the finite de Finetti theorem which is scaling optimal."],"url":"http://arxiv.org/abs/2405.08301v1","category":"cs.IT"}
{"created":"2024-05-14 03:13:55","title":"MCMC using $\\textit{bouncy}$ Hamiltonian dynamics: A unifying framework for Hamiltonian Monte Carlo and piecewise deterministic Markov process samplers","abstract":"Piecewise-deterministic Markov process (PDMP) samplers constitute a state of the art Markov chain Monte Carlo (MCMC) paradigm in Bayesian computation, with examples including the zig-zag and bouncy particle sampler (BPS). Recent work on the zig-zag has indicated its connection to Hamiltonian Monte Carlo, a version of the Metropolis algorithm that exploits Hamiltonian dynamics. Here we establish that, in fact, the connection between the paradigms extends far beyond the specific instance. The key lies in (1) the fact that any time-reversible deterministic dynamics provides a valid Metropolis proposal and (2) how PDMPs' characteristic velocity changes constitute an alternative to the usual acceptance-rejection. We turn this observation into a rigorous framework for constructing rejection-free Metropolis proposals based on bouncy Hamiltonian dynamics which simultaneously possess Hamiltonian-like properties and generate discontinuous trajectories similar in appearance to PDMPs. When combined with periodic refreshment of the inertia, the dynamics converge strongly to PDMP equivalents in the limit of increasingly frequent refreshment. We demonstrate the practical implications of this new paradigm, with a sampler based on a bouncy Hamiltonian dynamics closely related to the BPS. The resulting sampler exhibits competitive performance on challenging real-data posteriors involving tens of thousands of parameters.","sentences":["Piecewise-deterministic Markov process (PDMP) samplers constitute a state of the art Markov chain Monte Carlo (MCMC) paradigm in Bayesian computation, with examples including the zig-zag and bouncy particle sampler (BPS).","Recent work on the zig-zag has indicated its connection to Hamiltonian Monte Carlo, a version of the Metropolis algorithm that exploits Hamiltonian dynamics.","Here we establish that, in fact, the connection between the paradigms extends far beyond the specific instance.","The key lies in (1) the fact that any time-reversible deterministic dynamics provides a valid Metropolis proposal and (2) how PDMPs' characteristic velocity changes constitute an alternative to the usual acceptance-rejection.","We turn this observation into a rigorous framework for constructing rejection-free Metropolis proposals based on bouncy Hamiltonian dynamics which simultaneously possess Hamiltonian-like properties and generate discontinuous trajectories similar in appearance to PDMPs.","When combined with periodic refreshment of the inertia, the dynamics converge strongly to PDMP equivalents in the limit of increasingly frequent refreshment.","We demonstrate the practical implications of this new paradigm, with a sampler based on a bouncy Hamiltonian dynamics closely related to the BPS.","The resulting sampler exhibits competitive performance on challenging real-data posteriors involving tens of thousands of parameters."],"url":"http://arxiv.org/abs/2405.08290v1","category":"stat.CO"}
{"created":"2024-05-14 02:06:53","title":"Power of $\\ell_1$-Norm Regularized Kaczmarz Algorithms for High-Order Tensor Recovery","abstract":"Tensors serve as a crucial tool in the representation and analysis of complex, multi-dimensional data. As data volumes continue to expand, there is an increasing demand for developing optimization algorithms that can directly operate on tensors to deliver fast and effective computations. Many problems in real-world applications can be formulated as the task of recovering high-order tensors characterized by sparse and/or low-rank structures. In this work, we propose novel Kaczmarz algorithms with a power of the $\\ell_1$-norm regularization for reconstructing high-order tensors by exploiting sparsity and/or low-rankness of tensor data. In addition, we develop both a block and an accelerated variant, along with a thorough convergence analysis of these algorithms. A variety of numerical experiments on both synthetic and real-world datasets demonstrate the effectiveness and significant potential of the proposed methods in image and video processing tasks, such as image sequence destriping and video deconvolution.","sentences":["Tensors serve as a crucial tool in the representation and analysis of complex, multi-dimensional data.","As data volumes continue to expand, there is an increasing demand for developing optimization algorithms that can directly operate on tensors to deliver fast and effective computations.","Many problems in real-world applications can be formulated as the task of recovering high-order tensors characterized by sparse and/or low-rank structures.","In this work, we propose novel Kaczmarz algorithms with a power of the $\\ell_1$-norm regularization for reconstructing high-order tensors by exploiting sparsity and/or low-rankness of tensor data.","In addition, we develop both a block and an accelerated variant, along with a thorough convergence analysis of these algorithms.","A variety of numerical experiments on both synthetic and real-world datasets demonstrate the effectiveness and significant potential of the proposed methods in image and video processing tasks, such as image sequence destriping and video deconvolution."],"url":"http://arxiv.org/abs/2405.08275v1","category":"math.OC"}
{"created":"2024-05-14 00:44:05","title":"Inertial active Ornstein-Uhlenbeck particle in a non-linear velocity dependent friction","abstract":"We explore the self-propulsion of an active Ornstein-Uhlenbeck particle with a non-linear velocity dependent friction. Using analytical approach and numerical simulation, we have exactly investigated the dynamical behaviour of the particle in terms of particle trajectory, position and velocity distribution functions in both underdamped as well as overdamped regimes of the dynamics. Analyzing the distribution functions, we observe that for a confined harmonic particle, with an increase in duration of self-propulsion, the inertial particle prefers to accumulate near the boundary of the confinement rather than the mean position, reflecting an activity induced bistability in the presence of nonlinear friction. On the other hand, in the overdamped or highly viscous regime, where the inertial influence is negligible small, the sharp peak structure in the distribution across the mean position of the well reveals as usual trapping of the particle with increase in the persistent duration of activity. Moreover, for a free particle, using perturbation method, we have analytically computed the velocity distribution function in the vanishing limit of noise. The distribution interestingly shows the similar attributes as in case of a harmonic well, thus providing an additional effective confining mechanism that can be explained as a decreasing function of effective temperature. In this limit, the analytically computed distribution agrees well with the simulation results.","sentences":["We explore the self-propulsion of an active Ornstein-Uhlenbeck particle with a non-linear velocity dependent friction.","Using analytical approach and numerical simulation, we have exactly investigated the dynamical behaviour of the particle in terms of particle trajectory, position and velocity distribution functions in both underdamped as well as overdamped regimes of the dynamics.","Analyzing the distribution functions, we observe that for a confined harmonic particle, with an increase in duration of self-propulsion, the inertial particle prefers to accumulate near the boundary of the confinement rather than the mean position, reflecting an activity induced bistability in the presence of nonlinear friction.","On the other hand, in the overdamped or highly viscous regime, where the inertial influence is negligible small, the sharp peak structure in the distribution across the mean position of the well reveals as usual trapping of the particle with increase in the persistent duration of activity.","Moreover, for a free particle, using perturbation method, we have analytically computed the velocity distribution function in the vanishing limit of noise.","The distribution interestingly shows the similar attributes as in case of a harmonic well, thus providing an additional effective confining mechanism that can be explained as a decreasing function of effective temperature.","In this limit, the analytically computed distribution agrees well with the simulation results."],"url":"http://arxiv.org/abs/2405.08249v1","category":"cond-mat.soft"}
{"created":"2024-05-13 22:31:09","title":"Comparing the dynamics of Jupiter-family Comets and comet-like fireballs","abstract":"Context. Jupiter-family comets (JFCs), which originate from the Kuiper belt and scattered disk, exhibit low-inclination and chaotic trajectories due to close encounters with Jupiter. Despite their typically short incursions into the inner solar system, a notable number of them are on Earth-crossing orbits, with fireball networks detecting many objects on ``JFC-like'' (2 < TJ < 3) orbits. Aims. This investigation aims to examine the orbital dynamics of JFCs and comet-like fireballs over 10,000 yr timescales, focusing on the trajectories and stability of these objects in the context of gravitational interactions within the solar system. Methods. We employed an extensive fireball dataset from the Desert Fireball Network (DFN), European Fireball Network (EFN), Fireball Recovery and InterPlanetary Observation Network (FRIPON), and Meteorite Observation and Recovery Project (MORP), alongside telescopically observed cometary ephemeris from the NASA HORIZONS database. The study integrates 646 fireball orbits with 661 JFC orbits for a comparative analysis of their orbital stability and evolution. Results. The analysis confirms frequent Jupiter encounters among most JFCs, inducing chaotic orbital behavior with limited predictability and short Lyapunov lifetimes (about 120 years), underscoring Jupiter`s significant dynamical influence. In contrast, ``JFC-like'' meteoroids detected by fireball networks largely exhibit dynamics divergent from genuine JFCs, with 79-92% on ``JFC-like'' orbits shown not to be prone to frequent Jupiter encounters; in particular, only 1-5% of all fireballs detected by the four networks exhibit dynamics similar to that of actual JFCs. In addition, 22% (16 of 72) of near-Earth JFCs are on highly stable orbits, suggesting a potential main belt origin for some of the bodies.","sentences":["Context.","Jupiter-family comets (JFCs), which originate from the Kuiper belt and scattered disk, exhibit low-inclination and chaotic trajectories due to close encounters with Jupiter.","Despite their typically short incursions into the inner solar system, a notable number of them are on Earth-crossing orbits, with fireball networks detecting many objects on ``JFC-like'' (2 < TJ < 3) orbits.","Aims.","This investigation aims to examine the orbital dynamics of JFCs and comet-like fireballs over 10,000 yr timescales, focusing on the trajectories and stability of these objects in the context of gravitational interactions within the solar system.","Methods.","We employed an extensive fireball dataset from the Desert Fireball Network (DFN), European Fireball Network (EFN), Fireball Recovery and InterPlanetary Observation Network (FRIPON), and Meteorite Observation and Recovery Project (MORP), alongside telescopically observed cometary ephemeris from the NASA HORIZONS database.","The study integrates 646 fireball orbits with 661 JFC orbits for a comparative analysis of their orbital stability and evolution.","Results.","The analysis confirms frequent Jupiter encounters among most JFCs, inducing chaotic orbital behavior with limited predictability and short Lyapunov lifetimes (about 120 years), underscoring Jupiter`s significant dynamical influence.","In contrast, ``JFC-like'' meteoroids detected by fireball networks largely exhibit dynamics divergent from genuine JFCs, with 79-92% on ``JFC-like'' orbits shown not to be prone to frequent Jupiter encounters; in particular, only 1-5% of all fireballs detected by the four networks exhibit dynamics similar to that of actual JFCs.","In addition, 22% (16 of 72) of near-Earth JFCs are on highly stable orbits, suggesting a potential main belt origin for some of the bodies."],"url":"http://arxiv.org/abs/2405.08224v1","category":"astro-ph.EP"}
{"created":"2024-05-13 22:14:35","title":"Direct RF Sampling based LLRF Control System for C-band Linear Accelerator","abstract":"Low Level RF (LLRF) control systems of linear accelerators (LINACs) are typically implemented with heterodyne based architectures, which have complex analog RF mixers for up and down conversion. The Gen 3 Radio Frequency System-on-Chip (RFSoC) device from AMD Xilinx integrates data converters with maximum RF frequency of 6~GHz. This enables direct RF sampling of C-band LLRF signal typically operated at 5.712 GHz without any analogue mixers, which can significantly simplify the system architecture. The data converters sample RF signals in higher order Nyquist zones and then up or down convert digitally by the integrated data path in RFSoC. The closed-loop feedback control firmware implemented in FPGA integrated in RFSoC can process the base-band signal from the ADC data path and calculate the updated phase and amplitude to be up-mixed by the DAC data path. We have developed a C-band LLRF control RFSoC platform with direct RF sampling, which targets Cool Copper Collider (C3) and other C or S band LINAC research and development projects. In this paper, the architecture of the platform will be described. We have optimized the configuration of the data converter and characterized performance of them with RF pulses. The test results for some of the key performance parameters for the LLRF platform with our custom solid-state amplifier, such as phase and amplitude stability, will be discussed in this paper.","sentences":["Low Level RF (LLRF) control systems of linear accelerators (LINACs) are typically implemented with heterodyne based architectures, which have complex analog RF mixers for up and down conversion.","The Gen 3 Radio Frequency System-on-Chip (RFSoC) device from AMD Xilinx integrates data converters with maximum RF frequency of 6~GHz.","This enables direct RF sampling of C-band LLRF signal typically operated at 5.712 GHz without any analogue mixers, which can significantly simplify the system architecture.","The data converters sample RF signals in higher order Nyquist zones and then up or down convert digitally by the integrated data path in RFSoC. The closed-loop feedback control firmware implemented in FPGA integrated in RFSoC can process the base-band signal from the ADC data path and calculate the updated phase and amplitude to be up-mixed by the DAC data path.","We have developed a C-band LLRF control RFSoC platform with direct RF sampling, which targets Cool Copper Collider (C3) and other C or S band LINAC research and development projects.","In this paper, the architecture of the platform will be described.","We have optimized the configuration of the data converter and characterized performance of them with RF pulses.","The test results for some of the key performance parameters for the LLRF platform with our custom solid-state amplifier, such as phase and amplitude stability, will be discussed in this paper."],"url":"http://arxiv.org/abs/2405.08219v1","category":"physics.acc-ph"}
{"created":"2024-05-13 21:20:54","title":"Distributionally Robust Degree Optimization for BATS Codes","abstract":"Batched sparse (BATS) code is a network coding solution for multi-hop wireless networks with packet loss. Achieving a close-to-optimal rate relies on an optimal degree distribution. Technical challenges arise from the sensitivity of this distribution to the often empirically obtained rank distribution at the destination node. Specifically, if the empirical distribution overestimates the channel, BATS codes experience a significant rate degradation, leading to unstable rates across different runs and hence unpredictable transmission costs. Confronting this unresolved obstacle, we introduce a formulation for distributionally robust optimization in degree optimization. Deploying the resulting degree distribution resolves the instability of empirical rank distributions, ensuring a close-to-optimal rate, and unleashing the potential of applying BATS codes in real-world scenarios.","sentences":["Batched sparse (BATS) code is a network coding solution for multi-hop wireless networks with packet loss.","Achieving a close-to-optimal rate relies on an optimal degree distribution.","Technical challenges arise from the sensitivity of this distribution to the often empirically obtained rank distribution at the destination node.","Specifically, if the empirical distribution overestimates the channel, BATS codes experience a significant rate degradation, leading to unstable rates across different runs and hence unpredictable transmission costs.","Confronting this unresolved obstacle, we introduce a formulation for distributionally robust optimization in degree optimization.","Deploying the resulting degree distribution resolves the instability of empirical rank distributions, ensuring a close-to-optimal rate, and unleashing the potential of applying BATS codes in real-world scenarios."],"url":"http://arxiv.org/abs/2405.08194v1","category":"cs.IT"}
{"created":"2024-05-13 19:40:09","title":"Analytical analysis of the decrease in the focal intensity of a femtosecond laser pulse caused by an imperfect surface of compressor diffraction gratings","abstract":"An analytical expression for the focal intensity of a laser pulse was obtained for an asymmetric out-of-plane compressor with gratings of arbitrary surface shape. The focal intensity is most strongly affected by the linear angular chirp caused by the spatial shift of different frequencies on the second and third gratings. The chirp can be eliminated by simply rotating the fourth grating by an optimal angle, which significantly reduces the requirements for the grating quality. It is shown that the decrease in the focal intensity depends on the product of grating surface rms and pulse spectrum bandwidth. With low-quality gratings, spectrum narrowing would not reduce focal intensity; contrariwise, it may even slightly increase it.","sentences":["An analytical expression for the focal intensity of a laser pulse was obtained for an asymmetric out-of-plane compressor with gratings of arbitrary surface shape.","The focal intensity is most strongly affected by the linear angular chirp caused by the spatial shift of different frequencies on the second and third gratings.","The chirp can be eliminated by simply rotating the fourth grating by an optimal angle, which significantly reduces the requirements for the grating quality.","It is shown that the decrease in the focal intensity depends on the product of grating surface rms and pulse spectrum bandwidth.","With low-quality gratings, spectrum narrowing would not reduce focal intensity; contrariwise, it may even slightly increase it."],"url":"http://arxiv.org/abs/2405.08143v1","category":"physics.optics"}
{"created":"2024-05-13 18:45:56","title":"Orbital motion of primordial black holes crossing Solar-type stars","abstract":"Primordial black holes (PBHs) are hypothetical objects that could have originated from density fluctuations in a very early phase of our Universe. Recent observations restrict the masses that such PBHs could have, if they are to constitute all of dark matter today: $10^{17} \\, {\\rm g} \\leq m \\leq 10^{23} \\, {\\rm g}$. With such low masses, general relativity predicts that the corresponding radii for the PBHs would be atomic or subatomic in size. When captured by a star, such a tiny PBH could exhibit an orbit completely or partially inside the body of the star, without significantly changing its mass for quite a long time. Here we examine the possible trajectories of a PBH that is captured by a Sun-like star. When in motion in the interior of the star, the amount of stellar mass that effectively interacts with the PBH will be a function of its distance to the center of the star. As a consequence, a strong effect on the shape of the orbits emerges, leading to PBH trajectories that could be open or closed, and exhibiting a rich variety of patterns.","sentences":["Primordial black holes (PBHs) are hypothetical objects that could have originated from density fluctuations in a very early phase of our Universe.","Recent observations restrict the masses that such PBHs could have, if they are to constitute all of dark matter today: $10^{17} \\, {\\rm g} \\leq m \\leq 10^{23} \\, {\\rm g}$. With such low masses, general relativity predicts that the corresponding radii for the PBHs would be atomic or subatomic in size.","When captured by a star, such a tiny PBH could exhibit an orbit completely or partially inside the body of the star, without significantly changing its mass for quite a long time.","Here we examine the possible trajectories of a PBH that is captured by a Sun-like star.","When in motion in the interior of the star, the amount of stellar mass that effectively interacts with the PBH will be a function of its distance to the center of the star.","As a consequence, a strong effect on the shape of the orbits emerges, leading to PBH trajectories that could be open or closed, and exhibiting a rich variety of patterns."],"url":"http://arxiv.org/abs/2405.08113v1","category":"astro-ph.CO"}
{"created":"2024-05-13 18:25:53","title":"Production cross sections of superheavy elements: insights from the dinuclear system model with high-quality microscopic nuclear masses","abstract":"To accurately predict the synthesis cross-sections of superheavy elements, identifying the optimal projectile-target combinations and the evaporation channels at specific collision energies, we have attempted to utilize high-quality microscopic nuclear masses (HQMNM) within the dinuclear system (DNS) model, which are obtained by fitting experimental data with the Skyrme energy density functional theory (DFT), as published in Phys. Lett. B 851 (2024) 138578. The atomic nuclear mass serves as a crucial input for the DNS model, as the Q-values and separation energies it generates directly influence the calculated fusion and survival probabilities. Our calculations have reproduced the experimental data for hot fusion and have been compared with results based on the finite-range droplet model (FRDM12) mass calculations. Compared to the FRDM12 mass results, we have found that the HQMNM provides a better fit to the experimental outcomes. For the specific reaction of \\(^{48}\\rm{Ca} + ^{243}\\rm{Am} \\rightarrow ^{291}\\rm{Mc}^*\\), we have conducted a detailed calculation of capture, fusion, and survival based on the HQMNM model and compared these with calculations based on other mass models. Based on these findings, we have systematically calculated available projectile target combinations for the synthesis of elements 119 and 120, and identified the optimal combinations. We provided the synthesis cross-sections, collision energies, and evaporation channels, offering a reference for conducting experiments on the synthesis of superheavy elements.","sentences":["To accurately predict the synthesis cross-sections of superheavy elements, identifying the optimal projectile-target combinations and the evaporation channels at specific collision energies, we have attempted to utilize high-quality microscopic nuclear masses (HQMNM) within the dinuclear system (DNS) model, which are obtained by fitting experimental data with the Skyrme energy density functional theory (DFT), as published in Phys.","Lett.","B 851 (2024) 138578.","The atomic nuclear mass serves as a crucial input for the DNS model, as the Q-values and separation energies it generates directly influence the calculated fusion and survival probabilities.","Our calculations have reproduced the experimental data for hot fusion and have been compared with results based on the finite-range droplet model (FRDM12) mass calculations.","Compared to the FRDM12 mass results, we have found that the HQMNM provides a better fit to the experimental outcomes.","For the specific reaction of \\(^{48}\\rm{Ca} + ^{243}\\rm{Am} \\rightarrow ^{291}\\rm{Mc}^*\\), we have conducted a detailed calculation of capture, fusion, and survival based on the HQMNM model and compared these with calculations based on other mass models.","Based on these findings, we have systematically calculated available projectile target combinations for the synthesis of elements 119 and 120, and identified the optimal combinations.","We provided the synthesis cross-sections, collision energies, and evaporation channels, offering a reference for conducting experiments on the synthesis of superheavy elements."],"url":"http://arxiv.org/abs/2405.08098v1","category":"nucl-th"}
{"created":"2024-05-13 18:15:06","title":"Cavity-enhanced photon indistinguishability at room temperature and telecom wavelengths","abstract":"Indistinguishable single photons in the telecom-bandwidth of optical fibers are indispensable for long-distance quantum communication. Solid-state single photon emitters have achieved excellent performance in key benchmarks, however, the demonstration of indistinguishability at room-temperature remains a major challenge. Here, we report room-temperature photon indistinguishability at telecom wavelengths from individual nanotube defects in a fiber-based microcavity operated in the regime of incoherent good cavity-coupling. The efficiency of the coupled system outperforms spectral or temporal filtering, and the photon indistinguishability is increased by more than two orders of magnitude compared to the free-space limit. Our results highlight a promising strategy to attain optimized non-classical light sources.","sentences":["Indistinguishable single photons in the telecom-bandwidth of optical fibers are indispensable for long-distance quantum communication.","Solid-state single photon emitters have achieved excellent performance in key benchmarks, however, the demonstration of indistinguishability at room-temperature remains a major challenge.","Here, we report room-temperature photon indistinguishability at telecom wavelengths from individual nanotube defects in a fiber-based microcavity operated in the regime of incoherent good cavity-coupling.","The efficiency of the coupled system outperforms spectral or temporal filtering, and the photon indistinguishability is increased by more than two orders of magnitude compared to the free-space limit.","Our results highlight a promising strategy to attain optimized non-classical light sources."],"url":"http://arxiv.org/abs/2405.08091v1","category":"cond-mat.mes-hall"}
{"created":"2024-05-13 18:08:45","title":"Tweezer interferometry with NOON states","abstract":"Atomic interferometers measure phase differences along paths with exceptional precision. Tweezer interferometry is a novel approach for performing this task. It allows to guide the particles along pre-defined trajectories. We explore the feasibility of using condensed bosons in tweezer interferometry. As opposed to factor $\\sqrt{N}$ enhancement with ${N \\sim 100}$ fermions, here we expect factor ${N}$ enhancement using NOON state interferometry. The proposed protocol involves adiabatic splitting and merging, followed by adiabatic branching of condensed bosons. The purpose of the latter step is phase encoding. Spontaneous symmetry breaking should be avoided. We estimate the time that is required for performing those sweep processes.","sentences":["Atomic interferometers measure phase differences along paths with exceptional precision.","Tweezer interferometry is a novel approach for performing this task.","It allows to guide the particles along pre-defined trajectories.","We explore the feasibility of using condensed bosons in tweezer interferometry.","As opposed to factor $\\sqrt{N}$ enhancement with ${N \\sim 100}$ fermions, here we expect factor ${N}$ enhancement using NOON state interferometry.","The proposed protocol involves adiabatic splitting and merging, followed by adiabatic branching of condensed bosons.","The purpose of the latter step is phase encoding.","Spontaneous symmetry breaking should be avoided.","We estimate the time that is required for performing those sweep processes."],"url":"http://arxiv.org/abs/2405.08088v1","category":"quant-ph"}
{"created":"2024-05-13 17:59:51","title":"DiffTF++: 3D-aware Diffusion Transformer for Large-Vocabulary 3D Generation","abstract":"Generating diverse and high-quality 3D assets automatically poses a fundamental yet challenging task in 3D computer vision. Despite extensive efforts in 3D generation, existing optimization-based approaches struggle to produce large-scale 3D assets efficiently. Meanwhile, feed-forward methods often focus on generating only a single category or a few categories, limiting their generalizability. Therefore, we introduce a diffusion-based feed-forward framework to address these challenges with a single model. To handle the large diversity and complexity in geometry and texture across categories efficiently, we 1) adopt improved triplane to guarantee efficiency; 2) introduce the 3D-aware transformer to aggregate the generalized 3D knowledge with specialized 3D features; and 3) devise the 3D-aware encoder/decoder to enhance the generalized 3D knowledge. Building upon our 3D-aware Diffusion model with TransFormer, DiffTF, we propose a stronger version for 3D generation, i.e., DiffTF++. It boils down to two parts: multi-view reconstruction loss and triplane refinement. Specifically, we utilize multi-view reconstruction loss to fine-tune the diffusion model and triplane decoder, thereby avoiding the negative influence caused by reconstruction errors and improving texture synthesis. By eliminating the mismatch between the two stages, the generative performance is enhanced, especially in texture. Additionally, a 3D-aware refinement process is introduced to filter out artifacts and refine triplanes, resulting in the generation of more intricate and reasonable details. Extensive experiments on ShapeNet and OmniObject3D convincingly demonstrate the effectiveness of our proposed modules and the state-of-the-art 3D object generation performance with large diversity, rich semantics, and high quality.","sentences":["Generating diverse and high-quality 3D assets automatically poses a fundamental yet challenging task in 3D computer vision.","Despite extensive efforts in 3D generation, existing optimization-based approaches struggle to produce large-scale 3D assets efficiently.","Meanwhile, feed-forward methods often focus on generating only a single category or a few categories, limiting their generalizability.","Therefore, we introduce a diffusion-based feed-forward framework to address these challenges with a single model.","To handle the large diversity and complexity in geometry and texture across categories efficiently, we 1) adopt improved triplane to guarantee efficiency; 2) introduce the 3D-aware transformer to aggregate the generalized 3D knowledge with specialized 3D features; and 3) devise the 3D-aware encoder/decoder to enhance the generalized 3D knowledge.","Building upon our 3D-aware Diffusion model with TransFormer, DiffTF, we propose a stronger version for 3D generation, i.e., DiffTF++.","It boils down to two parts: multi-view reconstruction loss and triplane refinement.","Specifically, we utilize multi-view reconstruction loss to fine-tune the diffusion model and triplane decoder, thereby avoiding the negative influence caused by reconstruction errors and improving texture synthesis.","By eliminating the mismatch between the two stages, the generative performance is enhanced, especially in texture.","Additionally, a 3D-aware refinement process is introduced to filter out artifacts and refine triplanes, resulting in the generation of more intricate and reasonable details.","Extensive experiments on ShapeNet and OmniObject3D convincingly demonstrate the effectiveness of our proposed modules and the state-of-the-art 3D object generation performance with large diversity, rich semantics, and high quality."],"url":"http://arxiv.org/abs/2405.08055v1","category":"cs.CV"}
{"created":"2024-05-13 16:35:14","title":"P=NP","abstract":"This paper investigates an extremely classic NP-complete problem: How to determine if a graph G, where each vertex has a degree of at most 4, can be 3-colorable(The research in this paper focuses on graphs G that satisfy the condition where the degree of each vertex does not exceed 4. To conserve space, it is assumed throughout the paper that graph G meets this condition by default.). The author has meticulously observed the relationship between the coloring problem and semidefinite programming, and has creatively constructed the corresponding semidefinite programming problem R(G) for a given graph G. The construction method of R(G) refers to Theorem 1.1 in the paper. I have obtained and proven the conclusion: A graph G is 3-colorable if and only if the objective function of its corresponding optimization problem R(G) is bounded, and when the objective function is bounded, its minimum value is 0.","sentences":["This paper investigates an extremely classic NP-complete problem: How to determine if a graph G, where each vertex has a degree of at most 4, can be 3-colorable(The research in this paper focuses on graphs G that satisfy the condition where the degree of each vertex does not exceed 4.","To conserve space, it is assumed throughout the paper that graph G meets this condition by default.).","The author has meticulously observed the relationship between the coloring problem and semidefinite programming, and has creatively constructed the corresponding semidefinite programming problem R(G) for a given graph G. The construction method of R(G) refers to Theorem 1.1 in the paper.","I have obtained and proven the conclusion: A graph G is 3-colorable if and only if the objective function of its corresponding optimization problem R(G) is bounded, and when the objective function is bounded, its minimum value is 0."],"url":"http://arxiv.org/abs/2405.08051v1","category":"cs.CC"}
{"created":"2024-05-13 16:07:58","title":"Optimizing Synthetic Correlated Diffusion Imaging for Breast Cancer Tumour Delineation","abstract":"Breast cancer is a significant cause of death from cancer in women globally, highlighting the need for improved diagnostic imaging to enhance patient outcomes. Accurate tumour identification is essential for diagnosis, treatment, and monitoring, emphasizing the importance of advanced imaging technologies that provide detailed views of tumour characteristics and disease. Synthetic correlated diffusion imaging (CDI$^s$) is a recent method that has shown promise for prostate cancer delineation compared to current MRI images. In this paper, we explore tuning the coefficients in the computation of CDI$^s$ for breast cancer tumour delineation by maximizing the area under the receiver operating characteristic curve (AUC) using a Nelder-Mead simplex optimization strategy. We show that the best AUC is achieved by the CDI$^s$ - Optimized modality, outperforming the best gold-standard modality by 0.0044. Notably, the optimized CDI$^s$ modality also achieves AUC values over 0.02 higher than the Unoptimized CDI$^s$ value, demonstrating the importance of optimizing the CDI$^s$ exponents for the specific cancer application.","sentences":["Breast cancer is a significant cause of death from cancer in women globally, highlighting the need for improved diagnostic imaging to enhance patient outcomes.","Accurate tumour identification is essential for diagnosis, treatment, and monitoring, emphasizing the importance of advanced imaging technologies that provide detailed views of tumour characteristics and disease.","Synthetic correlated diffusion imaging (CDI$^s$) is a recent method that has shown promise for prostate cancer delineation compared to current MRI images.","In this paper, we explore tuning the coefficients in the computation of CDI$^s$ for breast cancer tumour delineation by maximizing the area under the receiver operating characteristic curve (AUC) using a Nelder-Mead simplex optimization strategy.","We show that the best AUC is achieved by the CDI$^s$ - Optimized modality, outperforming the best gold-standard modality by 0.0044.","Notably, the optimized CDI$^s$ modality also achieves AUC values over 0.02 higher than the Unoptimized CDI$^s$ value, demonstrating the importance of optimizing the CDI$^s$ exponents for the specific cancer application."],"url":"http://arxiv.org/abs/2405.08049v1","category":"eess.IV"}
{"created":"2024-05-13 15:16:22","title":"Autonomous Sparse Mean-CVaR Portfolio Optimization","abstract":"The $\\ell_0$-constrained mean-CVaR model poses a significant challenge due to its NP-hard nature, typically tackled through combinatorial methods characterized by high computational demands. From a markedly different perspective, we propose an innovative autonomous sparse mean-CVaR portfolio model, capable of approximating the original $\\ell_0$-constrained mean-CVaR model with arbitrary accuracy. The core idea is to convert the $\\ell_0$ constraint into an indicator function and subsequently handle it through a tailed approximation. We then propose a proximal alternating linearized minimization algorithm, coupled with a nested fixed-point proximity algorithm (both convergent), to iteratively solve the model. Autonomy in sparsity refers to retaining a significant portion of assets within the selected asset pool during adjustments in pool size. Consequently, our framework offers a theoretically guaranteed approximation of the $\\ell_0$-constrained mean-CVaR model, improving computational efficiency while providing a robust asset selection scheme.","sentences":["The $\\ell_0$-constrained mean-CVaR model poses a significant challenge due to its NP-hard nature, typically tackled through combinatorial methods characterized by high computational demands.","From a markedly different perspective, we propose an innovative autonomous sparse mean-CVaR portfolio model, capable of approximating the original $\\ell_0$-constrained mean-CVaR model with arbitrary accuracy.","The core idea is to convert the $\\ell_0$ constraint into an indicator function and subsequently handle it through a tailed approximation.","We then propose a proximal alternating linearized minimization algorithm, coupled with a nested fixed-point proximity algorithm (both convergent), to iteratively solve the model.","Autonomy in sparsity refers to retaining a significant portion of assets within the selected asset pool during adjustments in pool size.","Consequently, our framework offers a theoretically guaranteed approximation of the $\\ell_0$-constrained mean-CVaR model, improving computational efficiency while providing a robust asset selection scheme."],"url":"http://arxiv.org/abs/2405.08047v1","category":"math.OC"}
{"created":"2024-05-14 17:19:38","title":"Search for Vector-like Quark, Heavy Neutral Lepton and Long-lived Particles at ATLAS and CMS","abstract":"Several theories beyond the Standard Model (BSM) predict heavy neutral leptons, or long-lived particles with unique signatures which are difficult to reconstruct. Another area of interest are vector-like quarks which lie at the heart of many extensions to the Standard Model seeking to address the Hierarchy Problem, as they can naturally cancel the mass divergence for the Higgs boson. New results for these BSM searches from the ATLAS and CMS experiments at the LHC are presented.","sentences":["Several theories beyond the Standard Model (BSM) predict heavy neutral leptons, or long-lived particles with unique signatures which are difficult to reconstruct.","Another area of interest are vector-like quarks which lie at the heart of many extensions to the Standard Model seeking to address the Hierarchy Problem, as they can naturally cancel the mass divergence for the Higgs boson.","New results for these BSM searches from the ATLAS and CMS experiments at the LHC are presented."],"url":"http://arxiv.org/abs/2405.08782v1","category":"hep-ex"}
{"created":"2024-05-14 16:59:32","title":"Penrose limits of I-branes, twist-compactified D5-branes and spin chains","abstract":"In this paper we consider the Penrose limit in the case of two gravity duals. One of them, consists of compactified I-branes (intersecting sets of D5-branes over $(1+1)$ dimensions). The second consists of D5-branes compactified on a circle. Both compactifications preserve SUSY. We find a match of the oscillators and masses of string modes on the resulting pp wave against a spin chain in a $(2+1)-$dimensional field theory in the first case, and a spin chain in a $(1+1)-$dimensional field theory in the second.","sentences":["In this paper we consider the Penrose limit in the case of two gravity duals.","One of them, consists of compactified I-branes (intersecting sets of D5-branes over $(1+1)$ dimensions).","The second consists of D5-branes compactified on a circle.","Both compactifications preserve SUSY.","We find a match of the oscillators and masses of string modes on the resulting pp wave against a spin chain in a $(2+1)-$dimensional field theory in the first case, and a spin chain in a $(1+1)-$dimensional field theory in the second."],"url":"http://arxiv.org/abs/2405.08767v1","category":"hep-th"}
{"created":"2024-05-14 16:33:45","title":"Lepton-Pair Scattering With an Off-Shell and an On-Shell Photon at Two Loops in Massless QED","abstract":"I present the computation of the two-loop amplitudes for the scattering of a lepton pair with an off-shell and an on-shell photon in massless QED. We apply modern techniques developed to tackle QCD amplitudes with many scales: we express the Feynman integrals in terms of a basis of special functions, and reconstruct the amplitudes from numerical finite-field evaluations. Our results complete the amplitude-level ingredients for the N3LO predictions of electron-muon scattering needed to meet the precision target of the future MUonE experiment.","sentences":["I present the computation of the two-loop amplitudes for the scattering of a lepton pair with an off-shell and an on-shell photon in massless QED.","We apply modern techniques developed to tackle QCD amplitudes with many scales: we express the Feynman integrals in terms of a basis of special functions, and reconstruct the amplitudes from numerical finite-field evaluations.","Our results complete the amplitude-level ingredients for the N3LO predictions of electron-muon scattering needed to meet the precision target of the future MUonE experiment."],"url":"http://arxiv.org/abs/2405.08750v1","category":"hep-ph"}
{"created":"2024-05-14 16:31:29","title":"Universal Acceleration and Fuzzy Dark Matter","abstract":"Observations of velocity dispersions of galactic structures over a wide range of scales point to the existence of a universal acceleration scale $a_0\\sim 10^{-10}$ m/s$^2$. Focusing on the fuzzy dark matter paradigm, which proposes ultralight dark matter with mass around $10^{-22}$ eV and de Broglie wavelength $\\lambda\\sim {\\rm few}\\times10^{2}$ parsecs, we highlight the emergence of the observed acceleration scale from quantum effects in a fluid-like description of the dark matter dynamics. We then suggest the possibility of a natural connection between the acceleration scale and dark energy within the same paradigm.","sentences":["Observations of velocity dispersions of galactic structures over a wide range of scales point to the existence of a universal acceleration scale $a_0\\sim 10^{-10}$ m/s$^2$. Focusing on the fuzzy dark matter paradigm, which proposes ultralight dark matter with mass around $10^{-22}$ eV and de Broglie wavelength $\\lambda\\sim {\\rm few}\\times10^{2}$ parsecs, we highlight the emergence of the observed acceleration scale from quantum effects in a fluid-like description of the dark matter dynamics.","We then suggest the possibility of a natural connection between the acceleration scale and dark energy within the same paradigm."],"url":"http://arxiv.org/abs/2405.08744v1","category":"astro-ph.CO"}
{"created":"2024-05-14 15:57:34","title":"On Holographic Vacuum Misalignment","abstract":"We develop a bottom-up holographic model that provides the dual description of a strongly coupled field theory, in which the spontaneous breaking of an approximate global symmetry yields the SO(5)/SO(4) coset relevant to minimal composite-Higgs models. The gravity background is completely regular and smooth, and has an end of space that mimics confinement on the field theory side. We add to the gravity description a set of localised boundary terms, that introduce additional symmetry-breaking effects, capturing those that would result from coupling the dual strongly coupled field theory to an external, weakly coupled sector. Such terms encapsulate the gauging of a subgroup of the global $SO(5)$ symmetry of the dual field theory, as well as additional explicit symmetry-breaking effects. We show how to combine spurions and gauge fixing and how to take the appropriate limits, so as to respect gauge principles and avoid violations of unitarity.   The interplay of bulk and boundary-localised couplings leads to the breaking of the SO(5) symmetry to either its SO(4) or SO(3) subgroup, via vacuum misalignment. In field theory terms, the model describes the spontaneous breaking of a SO(4) gauge symmetry to its SO(3) subgroup. We expose the implications of the higgsing phenomenon by computing the spectrum of fluctuations of the model, which we interpret in four-dimensional field-theory terms, for a few interesting choices of parameters. We conclude by commenting on the additional steps needed to build a realistic composite Higgs model.","sentences":["We develop a bottom-up holographic model that provides the dual description of a strongly coupled field theory, in which the spontaneous breaking of an approximate global symmetry yields the SO(5)/SO(4) coset relevant to minimal composite-Higgs models.","The gravity background is completely regular and smooth, and has an end of space that mimics confinement on the field theory side.","We add to the gravity description a set of localised boundary terms, that introduce additional symmetry-breaking effects, capturing those that would result from coupling the dual strongly coupled field theory to an external, weakly coupled sector.","Such terms encapsulate the gauging of a subgroup of the global $SO(5)$ symmetry of the dual field theory, as well as additional explicit symmetry-breaking effects.","We show how to combine spurions and gauge fixing and how to take the appropriate limits, so as to respect gauge principles and avoid violations of unitarity.   ","The interplay of bulk and boundary-localised couplings leads to the breaking of the SO(5) symmetry to either its SO(4) or SO(3) subgroup, via vacuum misalignment.","In field theory terms, the model describes the spontaneous breaking of a SO(4) gauge symmetry to its SO(3) subgroup.","We expose the implications of the higgsing phenomenon by computing the spectrum of fluctuations of the model, which we interpret in four-dimensional field-theory terms, for a few interesting choices of parameters.","We conclude by commenting on the additional steps needed to build a realistic composite Higgs model."],"url":"http://arxiv.org/abs/2405.08714v1","category":"hep-th"}
{"created":"2024-05-14 15:36:27","title":"Color Chiral Cherenkov radiation and energy loss in quark-gluon plasma","abstract":"We introduce and investigate the Color Chiral Cherenkov effect which consists in radiation of the circularly polarized gluons by a fast color charge moving with constant velocity in the presence of the Chiral Magnetic current. We derive the transition rates for all gluon polarizations. We compute the contribution of the Color Chiral Cherenkov effect to the parton energy loss in the quark-gluon plasma.","sentences":["We introduce and investigate the Color Chiral Cherenkov effect which consists in radiation of the circularly polarized gluons by a fast color charge moving with constant velocity in the presence of the Chiral Magnetic current.","We derive the transition rates for all gluon polarizations.","We compute the contribution of the Color Chiral Cherenkov effect to the parton energy loss in the quark-gluon plasma."],"url":"http://arxiv.org/abs/2405.08697v1","category":"hep-ph"}
{"created":"2024-05-14 15:09:37","title":"Quantitative description of long-range order in the anisotropic spin-1/2 Heisenberg antiferromagnet on the square lattice","abstract":"The quantitative description of long-range order remains a challenge in quantum many-body physics. We provide zero-temperature results from two complementary methods for the ground-state energy per site, the sublattice magnetization, the spin gap, and the transverse spin correlation length for the spin-1/2 anisotropic quantum Heisenberg antiferromagnet on the square lattice. On the one hand, we use exact, large-scale quantum Monte Carlo (QMC) simulations. On the other hand, we use the semi-analytic approach based on continuous similarity transformations in terms of elementary magnon excitations. Our findings confirm the applicability and quantitative validity of both approaches along the full parameter axis from the Ising point to the symmetry-restoring phase transition at the Heisenberg point and further provide quantitative reference results in the thermodynamic limit. In addition, we analytically derive the relation between the dispersion and the correlation length at zero temperature in arbitrary dimension, and discuss improved second-moment QMC estimators.","sentences":["The quantitative description of long-range order remains a challenge in quantum many-body physics.","We provide zero-temperature results from two complementary methods for the ground-state energy per site, the sublattice magnetization, the spin gap, and the transverse spin correlation length for the spin-1/2 anisotropic quantum Heisenberg antiferromagnet on the square lattice.","On the one hand, we use exact, large-scale quantum Monte Carlo (QMC) simulations.","On the other hand, we use the semi-analytic approach based on continuous similarity transformations in terms of elementary magnon excitations.","Our findings confirm the applicability and quantitative validity of both approaches along the full parameter axis from the Ising point to the symmetry-restoring phase transition at the Heisenberg point and further provide quantitative reference results in the thermodynamic limit.","In addition, we analytically derive the relation between the dispersion and the correlation length at zero temperature in arbitrary dimension, and discuss improved second-moment QMC estimators."],"url":"http://arxiv.org/abs/2405.08688v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 14:50:11","title":"The Role of Sunspots Magnetic Configuration in the Formation of Umbral Fine-Structures","abstract":"We use spectro-polarimetric data recorded by Hinode to analyze the magnetic field configuration of a part of a sunspot (AR 10923) where a bundle of penumbral filaments are intruding into its umbra. We want to explore the role of the sunspot magnetic configuration in the formation and kinematics of the fine-structures, such as umbral dots and light bridges, inside the sunspot umbra. Both direct inferences from polarization Stokes profiles and the inversion results using the SIR code indicate an aligned magnetic field configuration in the umbra where moving umbral dots are easily formed at the leading edges of the rapidly intruding penumbral filaments. We suggest that the magnetic field topology is rearranged leading to the observed aligned magnetic field lines via magnetic reconnection process by which a part of the magnetic energy is converted into thermal and kinetic energy. This new configuration causes the umbral fine-structures to form easily and more frequently.","sentences":["We use spectro-polarimetric data recorded by Hinode to analyze the magnetic field configuration of a part of a sunspot (AR 10923) where a bundle of penumbral filaments are intruding into its umbra.","We want to explore the role of the sunspot magnetic configuration in the formation and kinematics of the fine-structures, such as umbral dots and light bridges, inside the sunspot umbra.","Both direct inferences from polarization Stokes profiles and the inversion results using the SIR code indicate an aligned magnetic field configuration in the umbra where moving umbral dots are easily formed at the leading edges of the rapidly intruding penumbral filaments.","We suggest that the magnetic field topology is rearranged leading to the observed aligned magnetic field lines via magnetic reconnection process by which a part of the magnetic energy is converted into thermal and kinetic energy.","This new configuration causes the umbral fine-structures to form easily and more frequently."],"url":"http://arxiv.org/abs/2405.08666v1","category":"astro-ph.SR"}
{"created":"2024-05-14 14:37:37","title":"On the origin of the quasi-periodic micro-pulses observed in the radio-frequency emission of some neutron stars","abstract":"The linear relationship between pulsar micro-pulse widths and rotation period is consistent with the existence of a physical length L on the neutron-star surface and seen on the observer arc of transit across the polar cap. Within the ion-proton model it is the width of the minimum area of surface that can support the critical growth rate of the unstable two-beam Langmuir mode that is the source of the radio emission.","sentences":["The linear relationship between pulsar micro-pulse widths and rotation period is consistent with the existence of a physical length L on the neutron-star surface and seen on the observer arc of transit across the polar cap.","Within the ion-proton model it is the width of the minimum area of surface that can support the critical growth rate of the unstable two-beam Langmuir mode that is the source of the radio emission."],"url":"http://arxiv.org/abs/2405.08660v1","category":"astro-ph.HE"}
{"created":"2024-05-14 14:21:32","title":"On the positivity of MSbar distributions","abstract":"We discuss the positivity of parton distribution functions using the common $\\overline{MS}$ factorization scheme. We find that in the perturbative regime $\\overline{MS}$ PDFs inherit the strict positivity of physical PDFs. We explicitly discuss the scheme transformation by using suitable physical observables and find that $\\overline{MS}$ PDFs are positive above the scale $Q^2$ > 5 GeV^2. Finally, we comment on the direct counterpart of longitudinally polarized PDFs, finding agreement with the unpolarized counterpart.","sentences":["We discuss the positivity of parton distribution functions using the common $\\overline{MS}$ factorization scheme.","We find that in the perturbative regime $\\overline{MS}$ PDFs inherit the strict positivity of physical PDFs.","We explicitly discuss the scheme transformation by using suitable physical observables and find that $\\overline{MS}$ PDFs are positive above the scale $Q^2$ > 5 GeV^2.","Finally, we comment on the direct counterpart of longitudinally polarized PDFs, finding agreement with the unpolarized counterpart."],"url":"http://arxiv.org/abs/2405.08643v1","category":"hep-ph"}
{"created":"2024-05-14 13:58:28","title":"Efficient computation of Fourier-Bessel transforms for transverse-momentum dependent parton distributions and other functions","abstract":"We present a method for the numerical computation of Fourier-Bessel transforms on a finite or infinite interval. The function to be transformed needs to be evaluated on a grid of points that is independent of the argument of the Bessel function. We demonstrate the accuracy of the algorithm for a wide range of functions, including those that appear in the context of transverse-momentum dependent parton distributions in Quantum Chromodynamics.","sentences":["We present a method for the numerical computation of Fourier-Bessel transforms on a finite or infinite interval.","The function to be transformed needs to be evaluated on a grid of points that is independent of the argument of the Bessel function.","We demonstrate the accuracy of the algorithm for a wide range of functions, including those that appear in the context of transverse-momentum dependent parton distributions in Quantum Chromodynamics."],"url":"http://arxiv.org/abs/2405.08616v1","category":"hep-ph"}
{"created":"2024-05-14 13:44:43","title":"The evolution of stellar X-ray activity and angular momentum as seen by eROSITA, TESS, and Gaia","abstract":"We have assembled a sample of $\\sim$8200 stars with spectral types F5V-M5V, all having directly measured X-ray luminosities from eROSITA and rotation periods from TESS, and having empirically estimated ages via their membership in stellar clusters and groups identified in Gaia astrometry (ages 3-500 Myr). This is the largest such study sample yet assembled for the purpose of empirically constraining the evolution of rotationally driven stellar X-ray activity. We observe rotation-age-activity correlations that are qualitatively as expected: stars of a given spectral type spin down with age and they become less X-ray active as they do so. We provide simple functional representations of these empirical relationships that predict X-ray luminosity from basic observables to within 0.3 dex. Interestingly, we find that the rotation-activity relationship is far simpler and more monotonic in form when expressed in terms of stellar angular momentum instead of rotation period. We discuss how this finding may relate to the long-established idea that rotation-activity relationships are mediated by stellar structure (e.g., convective turnover time, surface area). Finally, we provide an empirical relation that predicts stellar angular momentum from basic observables, and without requiring a direct measurement of stellar rotation, to within 0.5 dex.","sentences":["We have assembled a sample of $\\sim$8200 stars with spectral types F5V-M5V, all having directly measured X-ray luminosities from eROSITA and rotation periods from TESS, and having empirically estimated ages via their membership in stellar clusters and groups identified in Gaia astrometry (ages 3-500 Myr).","This is the largest such study sample yet assembled for the purpose of empirically constraining the evolution of rotationally driven stellar X-ray activity.","We observe rotation-age-activity correlations that are qualitatively as expected: stars of a given spectral type spin down with age and they become less X-ray active as they do so.","We provide simple functional representations of these empirical relationships that predict X-ray luminosity from basic observables to within 0.3 dex.","Interestingly, we find that the rotation-activity relationship is far simpler and more monotonic in form when expressed in terms of stellar angular momentum instead of rotation period.","We discuss how this finding may relate to the long-established idea that rotation-activity relationships are mediated by stellar structure (e.g., convective turnover time, surface area).","Finally, we provide an empirical relation that predicts stellar angular momentum from basic observables, and without requiring a direct measurement of stellar rotation, to within 0.5 dex."],"url":"http://arxiv.org/abs/2405.08607v1","category":"astro-ph.SR"}
{"created":"2024-05-14 13:43:20","title":"Anomalous Landau damping and algebraic thermalization in two-dimensional superfluids far from equilibrium","abstract":"We present a quantitative description of the thermalization dynamics of far-from-equilibrium, two-dimensional (2D) Bose superfluids. Our analysis leverages a quantum kinetic formalism and allows us to identify two successive regimes of relaxation: an initial damping of quasi-particles due to Landau scattering processes, followed by the slower establishment of a global equilibrium at long time. For a far-from-equilibrium initial state, we find that Landau damping differs from the conventional picture of exponentially relaxing quasi-particles. Moreover, our results showcase a pronounced mechanism of algebraic transport at late times, rooted in energy conservation and compatible with 2D diffusion. Using theoretical and numerical arguments, we construct a detailed dynamical portrait of global equilibration in 2D superfluids.","sentences":["We present a quantitative description of the thermalization dynamics of far-from-equilibrium, two-dimensional (2D)","Bose superfluids.","Our analysis leverages a quantum kinetic formalism and allows us to identify two successive regimes of relaxation: an initial damping of quasi-particles due to Landau scattering processes, followed by the slower establishment of a global equilibrium at long time.","For a far-from-equilibrium initial state, we find that Landau damping differs from the conventional picture of exponentially relaxing quasi-particles.","Moreover, our results showcase a pronounced mechanism of algebraic transport at late times, rooted in energy conservation and compatible with 2D diffusion.","Using theoretical and numerical arguments, we construct a detailed dynamical portrait of global equilibration in 2D superfluids."],"url":"http://arxiv.org/abs/2405.08606v1","category":"cond-mat.quant-gas"}
{"created":"2024-05-14 13:31:46","title":"Degeneracy Enhancement of Neutron-Antineutron Oscillation in Neutron Star","abstract":"We explore the fermion oscillation in a degenerate environment. The direct consequence is introducing a Pauli blocking factor $1 - f_i$, where $f_i$ is the phase space distribution function, for each intermediate mass eigenstate during propagation. It is then much easier for a state with larger existing fraction or density to oscillate into other states with less degeneracy while the reversed process is not enhanced. This can significantly modify the oscillation behaviors. We apply this degenerate fermion oscillation to a concrete scenario of neutron-antineutron oscillation in neutron star. It turns out antineutrons receive a standing fraction to annihilate with the environmental neutrons. The subsequent neutron star heating can put an extremely stringent bound on the baryon number violating cross mass term between neutron and antineutron.","sentences":["We explore the fermion oscillation in a degenerate environment.","The direct consequence is introducing a Pauli blocking factor $1 - f_i$, where $f_i$ is the phase space distribution function, for each intermediate mass eigenstate during propagation.","It is then much easier for a state with larger existing fraction or density to oscillate into other states with less degeneracy while the reversed process is not enhanced.","This can significantly modify the oscillation behaviors.","We apply this degenerate fermion oscillation to a concrete scenario of neutron-antineutron oscillation in neutron star.","It turns out antineutrons receive a standing fraction to annihilate with the environmental neutrons.","The subsequent neutron star heating can put an extremely stringent bound on the baryon number violating cross mass term between neutron and antineutron."],"url":"http://arxiv.org/abs/2405.08591v1","category":"hep-ph"}
{"created":"2024-05-14 12:58:38","title":"Space-time boundary elements for frictional contact in elastodynamics","abstract":"This article studies a boundary element method for dynamic frictional contact between linearly elastic bodies. We formulate these problems as a variational inequality on the boundary, involving the elastodynamic Poincar\\'{e}-Steklov operator. The variational inequality is solved in a mixed formulation using boundary elements in space and time. In the model problem of unilateral Tresca friction contact with a rigid obstacle we obtain an a priori estimate for the resulting Galerkin approximations. Numerical experiments in two space dimensions demonstrate the stability, energy conservation and convergence of the proposed method for contact problems involving concrete and steel in the linearly elastic regime. They address both unilateral and two-sided dynamic contact with Tresca or Coulomb friction.","sentences":["This article studies a boundary element method for dynamic frictional contact between linearly elastic bodies.","We formulate these problems as a variational inequality on the boundary, involving the elastodynamic Poincar\\'{e}-Steklov operator.","The variational inequality is solved in a mixed formulation using boundary elements in space and time.","In the model problem of unilateral Tresca friction contact with a rigid obstacle we obtain an a priori estimate for the resulting Galerkin approximations.","Numerical experiments in two space dimensions demonstrate the stability, energy conservation and convergence of the proposed method for contact problems involving concrete and steel in the linearly elastic regime.","They address both unilateral and two-sided dynamic contact with Tresca or Coulomb friction."],"url":"http://arxiv.org/abs/2405.08566v1","category":"math.NA"}
{"created":"2024-05-14 12:34:21","title":"Open charm tetraquarks in broken SU(3)_F symmetry","abstract":"Prompted by a recent lattice QCD calculation, we review the SU(3) light quark flavor structure of charmed tetraquarks with spin-0 diquarks. Fermi statistics forces the three light quarks to be in the representation 3*x3*= 3+6*. This agrees with the weak repulsion in the 15 of the 3x8 in Dbar K scattering studied on the lattice. We analyze the 3+6* multiplet broken by the strange quark mass and determine the five independent masses from the known masses of diquarks. The mass of D^* _{s0}(2317) is predicted within 50 MeV accuracy. The recently observed D_s^{--}(2900) and D_s^{0}(2900), likely part of a I=1 multiplet, with flavor composition c* q* q' s and X_0(2900), an isosinglet with flavor composition c*s* ud, fit naturally in a 3+6* structure as the first radial excitations. We discuss also the decay modes of D^* _{s0}(2317), of the radial excitations and of the predicted particles.","sentences":["Prompted by a recent lattice QCD calculation, we review the SU(3) light quark flavor structure of charmed tetraquarks with spin-0 diquarks.","Fermi statistics forces the three light quarks to be in the representation 3*x3*= 3","+6*.","This agrees with the weak repulsion in the 15 of the 3x8 in Dbar K scattering studied on the lattice.","We analyze the 3+6* multiplet broken by the strange quark mass and determine the five independent masses from the known masses of diquarks.","The mass of D^* _{s0}(2317) is predicted within 50 MeV accuracy.","The recently observed D_s^{--}(2900) and D_s^{0}(2900), likely part of a I=1 multiplet, with flavor composition c* q* q' s and X_0(2900), an isosinglet with flavor composition c*s* ud, fit naturally in a 3+6* structure as the first radial excitations.","We discuss also the decay modes of D^* _{s0}(2317), of the radial excitations and of the predicted particles."],"url":"http://arxiv.org/abs/2405.08545v1","category":"hep-ph"}
{"created":"2024-05-14 12:18:14","title":"Heavy quark mass near the phase transition","abstract":"Assuming that the number densities of heavy flavor in hadron gas and in QGP are same at $T_c$, we obtain the effective mass of heavy quark at $T_c$ from the comparison with the hadron resonance gas model which well describes particle yield in heavy-ion collisions. We find that charm quark mass at vanishing baryon chemical potential is around 1.8 GeV which is much heavier than QCD bare mass and close to $D$ meson mass. The mass slightly increases with increasing baryon chemical potential and then decreases. On the other hand, anticharm quark mass constantly decreases with increasing baryon chemical potential. Bottom quark mass has a similar pattern. Extending the hadron resonance gas model to a bit higher temperature beyond $T_c$, the effective masses of charm and bottom quarks decrease with increasing temperature.","sentences":["Assuming that the number densities of heavy flavor in hadron gas and in QGP are same at $T_c$, we obtain the effective mass of heavy quark at $T_c$ from the comparison with the hadron resonance gas model which well describes particle yield in heavy-ion collisions.","We find that charm quark mass at vanishing baryon chemical potential is around 1.8 GeV which is much heavier than QCD bare mass and close to $D$ meson mass.","The mass slightly increases with increasing baryon chemical potential and then decreases.","On the other hand, anticharm quark mass constantly decreases with increasing baryon chemical potential.","Bottom quark mass has a similar pattern.","Extending the hadron resonance gas model to a bit higher temperature beyond $T_c$, the effective masses of charm and bottom quarks decrease with increasing temperature."],"url":"http://arxiv.org/abs/2405.08535v1","category":"hep-ph"}
{"created":"2024-05-14 12:17:28","title":"Status of Dark Photons","abstract":"I review recent developments in the field of dark photons-here taken to be U(1) gauge bosons with mass less than the Z-including both kinetically mixed vectors and those that couple to anomaly-free U(1)'s. Distinctions between Higgs and Stueckelberg masses are highlighted, with discussion of swampland constraints, UV completions, and new experimental search strategies.","sentences":["I review recent developments in the field of dark photons-here taken to be U(1) gauge bosons with mass less than the Z-including both kinetically mixed vectors and those that couple to anomaly-free U(1)'s.","Distinctions between Higgs and Stueckelberg masses are highlighted, with discussion of swampland constraints, UV completions, and new experimental search strategies."],"url":"http://arxiv.org/abs/2405.08534v1","category":"hep-ph"}
{"created":"2024-05-14 11:36:30","title":"Probing the N=104 midshell region for the r process via precision mass spectrometry of neutron-rich rare-earth isotopes with the JYFLTRAP double Penning trap","abstract":"We have performed high-precision mass measurements of neutron-rich rare-earth Tb, Dy and Ho isotopes using the Phase-Imaging Ion-Cyclotron-Resonance technique at the JYFLTRAP double Penning trap. We report on the first experimentally determined mass values for $^{169}$Tb, $^{170}$Dy and $^{171}$Dy, as well as the first direct mass measurements of $^{169}$Dy and $^{169\\text{-}171}$Ho. For $^{170}$Ho, the two long-lived ground and isomeric states were resolved and their mass measured, yielding an isomer excitation energy of $E_\\text{exc}=150.8(54)$ keV. In addition, we have performed independent crosschecks of previous Penning-trap values obtained for $^{167\\text{,} 168}$Tb and $^{167\\text{,} 168}$Dy. We have extended the systematics of two-neutron separation energies to the neutron midshell at $N=104$ in all of the studied isotopic chains. Our updated and new mass measurements provide better constraints for the neutron-capture reaction rates relevant to the astrophysical rapid neutron capture (r) process. The r-process abundances calculated with the new mass values seem to produce a steeper minimum at A=170 and differ by around 15-30\\% from the abundances computed with the Atomic Mass Evaluation 2020 values.","sentences":["We have performed high-precision mass measurements of neutron-rich rare-earth Tb, Dy and Ho isotopes using the Phase-Imaging Ion-Cyclotron-Resonance technique at the JYFLTRAP double Penning trap.","We report on the first experimentally determined mass values for $^{169}$Tb, $^{170}$Dy and $^{171}$Dy, as well as the first direct mass measurements of $^{169}$Dy and $^{169\\text{-}171}$Ho.","For $^{170}$Ho, the two long-lived ground and isomeric states were resolved and their mass measured, yielding an isomer excitation energy of $E_\\text{exc}=150.8(54)$ keV.","In addition, we have performed independent crosschecks of previous Penning-trap values obtained for $^{167\\text{,} 168}$Tb and $^{167\\text{,} 168}$Dy.","We have extended the systematics of two-neutron separation energies to the neutron midshell at $N=104$ in all of the studied isotopic chains.","Our updated and new mass measurements provide better constraints for the neutron-capture reaction rates relevant to the astrophysical rapid neutron capture (r) process.","The r-process abundances calculated with the new mass values seem to produce a steeper minimum at A=170 and differ by around 15-30\\% from the abundances computed with the Atomic Mass Evaluation 2020 values."],"url":"http://arxiv.org/abs/2405.08511v1","category":"nucl-ex"}
{"created":"2024-05-14 11:18:16","title":"Influence of the gravitational darkening effect on the spectrum of a hot, rapidly rotating neutron star. II. Iron lines","abstract":"Rapidly rotating neutron stars are similar to highly flattened ellipsoids. Observed spectra of flattened stars must exhibit effects of non spherical shape and the gravitational darkening. We examined in detail the influence of both effects on the observed central energies and profiles of lines of highly ionized iron, FeXXV and FeXXVI. We note that the gravitational darkening effect do not change the central energy of lines. What is important, spectra of neutron stars that rotate with different frequencies and are seen at various inclinations angles differ significantly. The appearance and the depth of lines strongly depends on the parameters like the inclination angle of the star or the frequency of the star rotation. In this paper we clearly show that the gravitational darkening effect should be included in realistic model of the atmospheres of the neutron stars.","sentences":["Rapidly rotating neutron stars are similar to highly flattened ellipsoids.","Observed spectra of flattened stars must exhibit effects of non spherical shape and the gravitational darkening.","We examined in detail the influence of both effects on the observed central energies and profiles of lines of highly ionized iron, FeXXV and FeXXVI.","We note that the gravitational darkening effect do not change the central energy of lines.","What is important, spectra of neutron stars that rotate with different frequencies and are seen at various inclinations angles differ significantly.","The appearance and the depth of lines strongly depends on the parameters like the inclination angle of the star or the frequency of the star rotation.","In this paper we clearly show that the gravitational darkening effect should be included in realistic model of the atmospheres of the neutron stars."],"url":"http://arxiv.org/abs/2405.08507v1","category":"astro-ph.HE"}
{"created":"2024-05-14 10:59:40","title":"Dispersive approaches for the HVP and HLbL contributions to $(g-2)_\u03bc$","abstract":"Calculations based on the analytic properties of the required matrix elements allow for a wide range of applications constraining the hadronic contributions to the anomalous magnetic moment of the muon $a_\\mu=(g-2)_\\mu/2$, both hadronic vacuum polarization (HVP) and hadronic light-by-light (HLbL) scattering. Here, we discuss such recent applications, including analyticity constraints on hadronic cross sections, radiative corrections, and isospin-breaking effects.","sentences":["Calculations based on the analytic properties of the required matrix elements allow for a wide range of applications constraining the hadronic contributions to the anomalous magnetic moment of the muon $a_\\mu=(g-2)_\\mu/2$, both hadronic vacuum polarization (HVP) and hadronic light-by-light (HLbL) scattering.","Here, we discuss such recent applications, including analyticity constraints on hadronic cross sections, radiative corrections, and isospin-breaking effects."],"url":"http://arxiv.org/abs/2405.08500v1","category":"hep-ph"}
{"created":"2024-05-14 10:39:37","title":"Determination of CP-violating $HZZ$ interaction with polarised beams at the ILC","abstract":"We study possible CP-violation effects of the 125 GeV Higgs to $Z$ boson coupling at the 250 GeV ILC with transverse and longitudinal beam polarisation via the process $e^+ e^- \\rightarrow HZ \\rightarrow H \\mu^-\\mu^+$. We explore the azimuthal angular distribution of the muon pair from the $Z$ boson decay, and constructe CP-odd observables sensitive to CP-violation effects, where we derived this observable both by analytical calculations and by $\\mathtt{Whizard}$ simulations. Particularly, we can construct two CP-odd observables with the help of transversely-polarised initial beams and improve the statistical significance of CP-violation effects by combining two measurements. We defined the asymmetries between the signal regions with different signs of the CP-odd observables, and determine the CP-violation effect by comparing with the SM 95% C.L. upper bound. In this paper, we setup a scenario which assumes that the total cross-section is always fixed while CP-violation is varying, and such a scenario helps us to determine the intrinsic CP-mixing angle limit around $|\\xi_{CP}|\\sim 0.03$ with (90%, 40%) polarised electron-positron beams and 5 ab$^{-1}$ integrated luminosity. In addition, we determine the CP-odd coupling limit $|\\widetilde{c}_{HZZ}|\\sim 0.01$ as well, where we suppose that the SM tree-level cross-section is fixed and the CP-violation is the varying additional contribution. Comparing with the analysis with unpolarised beams, the sensitivity to the CP-violation effect can be improved by transverse or longitudinal polarisation.","sentences":["We study possible CP-violation effects of the 125 GeV Higgs to $Z$ boson coupling at the 250 GeV ILC with transverse and longitudinal beam polarisation via the process $e^+ e^- \\rightarrow HZ \\rightarrow H \\mu^-\\mu^+$.","We explore the azimuthal angular distribution of the muon pair from the $Z$ boson decay, and constructe CP-odd observables sensitive to CP-violation effects, where we derived this observable both by analytical calculations and by $\\mathtt{Whizard}$ simulations.","Particularly, we can construct two CP-odd observables with the help of transversely-polarised initial beams and improve the statistical significance of CP-violation effects by combining two measurements.","We defined the asymmetries between the signal regions with different signs of the CP-odd observables, and determine the CP-violation effect by comparing with the SM 95% C.L. upper bound.","In this paper, we setup a scenario which assumes that the total cross-section is always fixed while CP-violation is varying, and such a scenario helps us to determine the intrinsic CP-mixing angle limit around $|\\xi_{CP}|\\sim 0.03$ with (90%, 40%) polarised electron-positron beams and 5 ab$^{-1}$ integrated luminosity.","In addition, we determine the CP-odd coupling limit $|\\widetilde{c}_{HZZ}|\\sim 0.01$ as well, where we suppose that the SM tree-level cross-section is fixed and the CP-violation is the varying additional contribution.","Comparing with the analysis with unpolarised beams, the sensitivity to the CP-violation effect can be improved by transverse or longitudinal polarisation."],"url":"http://arxiv.org/abs/2405.08494v1","category":"hep-ph"}
{"created":"2024-05-14 10:09:37","title":"$\u03b3$ rays from in-flight positron annihilation as a probe of new physics","abstract":"The $\\gamma$ ray emission originating from in-flight annihilation (IA) of positrons is a powerful observable for constraining high-energy positron production from exotic sources. By comparing diffuse $\\gamma$ ray observations of INTEGRAL, COMPTEL and EGRET to theoretical predictions, we set the most stringent constraints on electrophilic feebly interacting particles (FIPs), thereby proving IA as a valuable probe of new physics. In particular, we extensively discuss the case of MeV-scale sterile neutrinos, where IA sets the most stringent constraints, excluding $|U_{\\mu4}|^{2} \\gtrsim 10^{-13}$ and $|U_{\\tau4}|^{2} \\gtrsim 2\\times 10^{-13}$ for sterile neutrinos mixed with $\\mu$ and $\\tau$ neutrinos respectively. These constraints improve existing limits by more than an order of magnitude. We briefly discuss the application of these results to a host of exotic positron sources such as dark photons, axion-like particles, primordial black holes (PBHs) and sub-GeV dark matter (DM).","sentences":["The $\\gamma$ ray emission originating from in-flight annihilation (IA) of positrons is a powerful observable for constraining high-energy positron production from exotic sources.","By comparing diffuse $\\gamma$ ray observations of INTEGRAL, COMPTEL and EGRET to theoretical predictions, we set the most stringent constraints on electrophilic feebly interacting particles (FIPs), thereby proving IA as a valuable probe of new physics.","In particular, we extensively discuss the case of MeV-scale sterile neutrinos, where IA sets the most stringent constraints, excluding $|U_{\\mu4}|^{2} \\gtrsim 10^{-13}$ and $|U_{\\tau4}|^{2} \\gtrsim 2\\times 10^{-13}$ for sterile neutrinos mixed with $\\mu$ and $\\tau$ neutrinos respectively.","These constraints improve existing limits by more than an order of magnitude.","We briefly discuss the application of these results to a host of exotic positron sources such as dark photons, axion-like particles, primordial black holes (PBHs) and sub-GeV dark matter (DM)."],"url":"http://arxiv.org/abs/2405.08482v1","category":"hep-ph"}
{"created":"2024-05-14 10:06:20","title":"Long term X-ray spectral variations of the Seyfert-1 galaxy Mrk 279","abstract":"We present the results from a long term X-ray analysis of Mrk 279 during the period 2018-2020. We use data from multiple missions - AstroSat, NuSTAR and XMM-Newton, for the purpose. The X-ray spectrum can be modelled as a double Comptonisation along with the presence of neutral Fe K${\\alpha}$ line emission, at all epochs. We determined the source's X-ray flux and luminosity at these different epochs. We find significant variations in the source's flux state. We also investigated the variations in the source's spectral components during the observation period. We find that the photon index and hence the spectral shape follow the variations only over longer time periods. We probe the correlations between fluxes of different bands and their photon indices, and found no significant correlations between the parameters.","sentences":["We present the results from a long term X-ray analysis of Mrk 279 during the period 2018-2020.","We use data from multiple missions - AstroSat, NuSTAR and XMM-Newton, for the purpose.","The X-ray spectrum can be modelled as a double Comptonisation along with the presence of neutral Fe K${\\alpha}$ line emission, at all epochs.","We determined the source's X-ray flux and luminosity at these different epochs.","We find significant variations in the source's flux state.","We also investigated the variations in the source's spectral components during the observation period.","We find that the photon index and hence the spectral shape follow the variations only over longer time periods.","We probe the correlations between fluxes of different bands and their photon indices, and found no significant correlations between the parameters."],"url":"http://arxiv.org/abs/2405.08478v1","category":"astro-ph.HE"}
{"created":"2024-05-14 09:32:53","title":"Real time observation of oxygen diffusion in CGO thin films using spatially resolved Isotope Exchange Raman Spectroscopy","abstract":"The exploitation of advanced materials for novel energy, health and computing applications requires fundamental understanding of enabling physicochemical mechanisms, including ionic and electronic conductivity, defect formation processes and reaction kinetics. Therefore, access to underlying constants of functional materials via advanced but straightforward experimental techniques is key. We present a novel, cheap and widely applicable approach to analyze oxygen-tracer-diffusion in thin films with unprecedented time resolution based on the novel in situ isotope-exchange Raman spectroscopy (IERS) methodology. Raman spectroscopy is sensitive to changes in the local isotopic composition, manifested by a frequency shift of the oxygen Raman modes. Employing a Raman transparent capping layer allows to establish an in-plane tracer gradient and follow the isotope exchange and diffusion processes via consecutive spatial and time resolved in situ Raman line scans. Mass-transport coefficients are calculated from these isotopic gradients, similar to conventional techniques, but with an additional time-component, not accessible by other techniques. Here, we study gadolinium doped ceria (CGO) thin films, capped with Si3N4 or Al2O3. We report diffusion coefficients within the temperature range of interest for intermediate temperature emerging applications (300-500{\\deg}C) and confirm the validity of the measurement procedure and extracted parameters by comparison with FEM simulations and literature results.","sentences":["The exploitation of advanced materials for novel energy, health and computing applications requires fundamental understanding of enabling physicochemical mechanisms, including ionic and electronic conductivity, defect formation processes and reaction kinetics.","Therefore, access to underlying constants of functional materials via advanced but straightforward experimental techniques is key.","We present a novel, cheap and widely applicable approach to analyze oxygen-tracer-diffusion in thin films with unprecedented time resolution based on the novel in situ isotope-exchange Raman spectroscopy (IERS) methodology.","Raman spectroscopy is sensitive to changes in the local isotopic composition, manifested by a frequency shift of the oxygen Raman modes.","Employing a Raman transparent capping layer allows to establish an in-plane tracer gradient and follow the isotope exchange and diffusion processes via consecutive spatial and time resolved in situ Raman line scans.","Mass-transport coefficients are calculated from these isotopic gradients, similar to conventional techniques, but with an additional time-component, not accessible by other techniques.","Here, we study gadolinium doped ceria (CGO) thin films, capped with Si3N4 or Al2O3.","We report diffusion coefficients within the temperature range of interest for intermediate temperature emerging applications (300-500{\\deg}C) and confirm the validity of the measurement procedure and extracted parameters by comparison with FEM simulations and literature results."],"url":"http://arxiv.org/abs/2405.08462v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-14 07:33:47","title":"Regions of suppressed diffusion around supernova remnants?","abstract":"The recent discovery of the so-called TeV halos has attracted much attention. The morphology of the emission requires that the region is characterized by severe suppression of the diffusion coefficient. This finding raises many questions as to its origin: 1) is the suppressed diffusion {\\bf to be} attributed to instabilities induced by the same radiating particles? 2) or does it actually show that the diffusion coefficient is small throughout the disc of the Galaxy? In both cases, one would expect that the surroundings of supernova remnants (SNRs) should also show evidence of reduced diffusion coefficient, since most remnants are located in the disc and are expected to be sites of effective particle acceleration. Should we expect the existence of regions of extended $\\gamma$-ray emission from these regions as well? {\\bf Here we investigate the transport of cosmic rays (CRs) escaped from SNRs in order to assess the viability of the idea of having a cocoon of suppressed diffusion around them. A comparison of our results with the $\\gamma$-ray emission from the regions around HB9 and W28 does not provide solid evidence of reduced diffusivity. However, if indeed the phenomenon of reduced diffusivity occurs around SNRs surrounded by molecular clouds, our calculations show that the effects on the grammage of Galactic CRs can be significant.}","sentences":["The recent discovery of the so-called TeV halos has attracted much attention.","The morphology of the emission requires that the region is characterized by severe suppression of the diffusion coefficient.","This finding raises many questions as to its origin: 1) is the suppressed diffusion {\\bf to be} attributed to instabilities induced by the same radiating particles?","2) or does it actually show that the diffusion coefficient is small throughout the disc of the Galaxy?","In both cases, one would expect that the surroundings of supernova remnants (SNRs) should also show evidence of reduced diffusion coefficient, since most remnants are located in the disc and are expected to be sites of effective particle acceleration.","Should we expect the existence of regions of extended $\\gamma$-ray emission from these regions as well?","{\\bf Here we investigate the transport of cosmic rays (CRs) escaped from SNRs in order to assess the viability of the idea of having a cocoon of suppressed diffusion around them.","A comparison of our results with the $\\gamma$-ray emission from the regions around HB9 and W28 does not provide solid evidence of reduced diffusivity.","However, if indeed the phenomenon of reduced diffusivity occurs around SNRs surrounded by molecular clouds, our calculations show that the effects on the grammage of Galactic CRs can be significant.}"],"url":"http://arxiv.org/abs/2405.08385v1","category":"astro-ph.HE"}
{"created":"2024-05-14 07:19:34","title":"|Vcs| determination and LFU test in charm decays at BESIII","abstract":"In this talk, we present the recent results of charm physics from the BEIII experiment. It covers the studies of the pure leptonic and semi-leptonic decays of charmed hadrons, from which the |Vcs| are precisely determined and the LFU is tested.","sentences":["In this talk, we present the recent results of charm physics from the BEIII experiment.","It covers the studies of the pure leptonic and semi-leptonic decays of charmed hadrons, from which the |Vcs| are precisely determined and the LFU is tested."],"url":"http://arxiv.org/abs/2405.08376v1","category":"hep-ex"}
{"created":"2024-05-14 07:17:42","title":"On Long Range Ising Models with Random Boundary Conditions","abstract":"We consider polynomial long-range Ising models in one dimension, with ferromagnetic pair interactions decaying with power $2-\\alpha$ (for $0 \\leq \\alpha < 1$), and prepared with randomly chosen boundary conditions. We show that at low temperatures in the thermodynamic limit the finite-volume Gibbs measures do not converge, but have a distributional limit, the so-called metastate. We find that there is a distinction between the values of $\\alpha$ less than or larger than $\\frac{1}{2}$. For moderate, or intermediate, decay $\\alpha < \\frac{1}{2}$, the metastate is very dispersed and supported on the set of all Gibbs measures, both extremal and non-extremal, whereas for slow decays $\\alpha > \\frac{1}{2}$ the metastate is still dispersed, but has its support just on the set of the two extremal Gibbs measures, the plus measure and the minus measure.   The former, moderate decays case, appears to be new and is due to the occurrence of almost sure boundedness of the random variable which is the sum of all interaction (free) energies between random and ordered half-lines, when the decay is fast enough, but still slow enough to get a phase transition ($\\alpha>0$); while the latter, slow decays case, is more reminiscent of and similar to the behaviour of higher-dimensional nearest-neighbour Ising models with diverging boundary (free) energies.   We leave the threshold case $\\alpha=\\frac{1}{2}$ for further studies.","sentences":["We consider polynomial long-range Ising models in one dimension, with ferromagnetic pair interactions decaying with power $2-\\alpha$ (for $0 \\leq \\alpha < 1$), and prepared with randomly chosen boundary conditions.","We show that at low temperatures in the thermodynamic limit the finite-volume Gibbs measures do not converge, but have a distributional limit, the so-called metastate.","We find that there is a distinction between the values of $\\alpha$ less than or larger than $\\frac{1}{2}$. For moderate, or intermediate, decay $\\alpha < \\frac{1}{2}$, the metastate is very dispersed and supported on the set of all Gibbs measures, both extremal and non-extremal, whereas for slow decays $\\alpha > \\frac{1}{2}$ the metastate is still dispersed, but has its support just on the set of the two extremal Gibbs measures, the plus measure and the minus measure.   ","The former, moderate decays case, appears to be new and is due to the occurrence of almost sure boundedness of the random variable which is the sum of all interaction (free) energies between random and ordered half-lines, when the decay is fast enough, but still slow enough to get a phase transition ($\\alpha>0$); while the latter, slow decays case, is more reminiscent of and similar to the behaviour of higher-dimensional nearest-neighbour Ising models with diverging boundary (free) energies.   ","We leave the threshold case $\\alpha=\\frac{1}{2}$ for further studies."],"url":"http://arxiv.org/abs/2405.08374v1","category":"math-ph"}
{"created":"2024-05-14 07:07:32","title":"Three-dimensional Magneto-hydrodynamic Simulations of Core-collapse Supernovae: I. Hydrodynamic evolution and protoneutron star properties","abstract":"We present results from three-dimensional, magnetohydrodynamic, core-collapse simulations of sixteen progenitors following until 0.5 s after bounce. We use non-rotating solar-metallicity progenitor models with zero-age main-sequence mass between 9 and 24 $M_{\\odot}$. The examined progenitors cover a wide range of the compactness parameter including a peak around $23 M_{\\odot}$. We find that neutrino-driven explosions occur for all models within 0.3 s after bounce. We also find that the properties of the explosions and the central remnants are well correlated with the compactness. Early shock evolution is sensitive to the mass accretion rate onto the central core, reflecting the density profile of the progenitor stars. The most powerful explosions with diagnostic explosion energy $E_{\\rm exp} \\sim 0.75 \\times 10^{51}$ erg are obtained by 23 and 24 $M_{\\odot}$ models, which have the highest compactness among the examined models. These two models exhibit spiral SASI motions during 150-230 ms after bounce preceding a runaway shock expansion and leave a rapidly rotating neutron star with spin periods $\\sim 50$ ms. Our models predict the gravitational masses of the neutron star ranging between $1.22 M_{\\odot}$ and $1.67 M_{\\odot}$ and their spin periods 0.04-4 s. The number distribution of these values roughly matches observation. On the other hand, our models predict small hydrodynamic kick velocity (15-260 km/s), although they are still growing at the end of our simulations. Further systematic studies, including rotation and binary effects, as well as long-term simulations up to several seconds, will enable us to explore the origin of various core-collapse supernova explosions.","sentences":["We present results from three-dimensional, magnetohydrodynamic, core-collapse simulations of sixteen progenitors following until 0.5 s after bounce.","We use non-rotating solar-metallicity progenitor models with zero-age main-sequence mass between 9 and 24 $M_{\\odot}$. The examined progenitors cover a wide range of the compactness parameter including a peak around $23 M_{\\odot}$. We find that neutrino-driven explosions occur for all models within 0.3 s after bounce.","We also find that the properties of the explosions and the central remnants are well correlated with the compactness.","Early shock evolution is sensitive to the mass accretion rate onto the central core, reflecting the density profile of the progenitor stars.","The most powerful explosions with diagnostic explosion energy $E_{\\rm exp} \\sim 0.75","\\times 10^{51}$ erg are obtained by 23 and 24 $M_{\\odot}$ models, which have the highest compactness among the examined models.","These two models exhibit spiral SASI motions during 150-230 ms after bounce preceding a runaway shock expansion and leave a rapidly rotating neutron star with spin periods $\\sim 50$ ms.","Our models predict the gravitational masses of the neutron star ranging between $1.22 M_{\\odot}$ and $1.67 M_{\\odot}$ and their spin periods 0.04-4 s. The number distribution of these values roughly matches observation.","On the other hand, our models predict small hydrodynamic kick velocity (15-260 km/s), although they are still growing at the end of our simulations.","Further systematic studies, including rotation and binary effects, as well as long-term simulations up to several seconds, will enable us to explore the origin of various core-collapse supernova explosions."],"url":"http://arxiv.org/abs/2405.08367v1","category":"astro-ph.HE"}
{"created":"2024-05-14 06:02:56","title":"Polyakov-loop phase, Roberge-Weiss periodicity and thermodynamics","abstract":"We discuss the role of Roberge-Weiss periodicity in the thermodynamics of quantum chromodynamics at moderately high temperature, where the semi-quark-gluon plasma is expected. From the construction of the grand canonical partition function at zero and also at finite density via the canonical approach, we can discuss the relation between contributions of the Polyakov-loop phase and Roberge-Weiss periodicity. Then, we can conclude that the existence of Roberge-Weiss periodicity is a necessary condition to reproduce exact results at moderately high temperature.","sentences":["We discuss the role of Roberge-Weiss periodicity in the thermodynamics of quantum chromodynamics at moderately high temperature, where the semi-quark-gluon plasma is expected.","From the construction of the grand canonical partition function at zero and also at finite density via the canonical approach, we can discuss the relation between contributions of the Polyakov-loop phase and Roberge-Weiss periodicity.","Then, we can conclude that the existence of Roberge-Weiss periodicity is a necessary condition to reproduce exact results at moderately high temperature."],"url":"http://arxiv.org/abs/2405.08333v1","category":"hep-ph"}
{"created":"2024-05-14 05:48:52","title":"Multiband Simultaneous Photometry of Type II SN 2023ixf with Mephisto and the Twin 50-cm Telescopes","abstract":"SN 2023ixf, recently reported in the nearby galaxy M101 at a distance of $6.85~{\\rm Mpc}$, was one of the closest and brightest core-collapse supernovae (CCSNe) in the last decade. In this work, we present multi-wavelength photometric observation of SN 2023ixf with the Multi-channel Photometric Survey Telescope (Mephisto) in $uvgr$ bands and with the twin 50-cm telescopes in $griz$ bands. We find that the bolometric luminosity reached the maximum value of $3\\times10^{43}~{\\rm erg~s^{-1}}$ at 3.9 days after the explosion and fully settled onto the radioactive tail at $\\sim90$ days. The effective temperature decreased from $3.2\\times10^4~{\\rm K}$ at the first observation and approached to a constant of $\\sim(3000-4000)~{\\rm K}$ after the first two months. The evolution of the photospheric radius is consistent with a homologous expansion with a velocity of $8700~{\\rm km~s^{-1}}$ in the first two months, and it shrunk subsequently. Based on the radioactive tail, the initial nickel mass is about $M_{\\rm Ni}\\sim 0.098M_\\odot$. The explosion energy and the ejecta mass are estimated to be $E\\simeq(1.0-5.7)\\times10^{51}~{\\rm erg}$ and $M_{\\rm ej}\\simeq(3.8-16)M_\\odot$, respectively. The peak bolometric luminosity is proposed to be contributed by the interaction between the ejecta and the circumstellar medium (CSM). We find a shocked CSM mass of $M_{\\rm CSM}\\sim0.013M_\\odot$, a CSM density of $\\rho_{\\rm CSM}\\sim2.5\\times10^{-13}~{\\rm g~cm^{-3}}$ and a mass loss rate of the progenitor of $\\dot M\\sim0.022M_\\odot~{\\rm yr^{-1}}$.","sentences":["SN 2023ixf, recently reported in the nearby galaxy M101 at a distance of $6.85~{\\rm Mpc}$, was one of the closest and brightest core-collapse supernovae (CCSNe) in the last decade.","In this work, we present multi-wavelength photometric observation of SN 2023ixf with the Multi-channel Photometric Survey Telescope (Mephisto) in $uvgr$ bands and with the twin 50-cm telescopes in $griz$ bands.","We find that the bolometric luminosity reached the maximum value of $3\\times10^{43}~{\\rm erg~s^{-1}}$ at 3.9 days after the explosion and fully settled onto the radioactive tail at $\\sim90$ days.","The effective temperature decreased from $3.2\\times10^4~{\\rm","K}$ at the first observation and approached to a constant of $\\sim(3000-4000)~{\\rm K}$ after the first two months.","The evolution of the photospheric radius is consistent with a homologous expansion with a velocity of $8700~{\\rm km~s^{-1}}$ in the first two months, and it shrunk subsequently.","Based on the radioactive tail, the initial nickel mass is about $M_{\\rm Ni}\\sim 0.098M_\\odot$. The explosion energy and the ejecta mass are estimated to be $E\\simeq(1.0-5.7)\\times10^{51}~{\\rm erg}$ and $M_{\\rm ej}\\simeq(3.8-16)M_\\odot$, respectively.","The peak bolometric luminosity is proposed to be contributed by the interaction between the ejecta and the circumstellar medium (CSM).","We find a shocked CSM mass of $M_{\\rm CSM}\\sim0.013M_\\odot$, a CSM density of $\\rho_{\\rm CSM}\\sim2.5\\times10^{-13}~{\\rm g~cm^{-3}}$ and a mass loss rate of the progenitor of $\\dot M\\sim0.022M_\\odot~{\\rm yr^{-1}}$."],"url":"http://arxiv.org/abs/2405.08327v1","category":"astro-ph.HE"}
{"created":"2024-05-14 05:48:44","title":"Periodic Activities of Fast Radio Burst Repeaters from Precessing Magnetars with Evolving Obliquity","abstract":"Fast radio bursts (FRBs) are cosmological radio transients with millisecond durations and extremely high brightness temperatures. One FRB repeater, FRB 180916.J0158+65 (FRB 180916B), was confirmed to appear 16.35-day periodic activities with 5-day activity window. Another FRB repeater, FRB 121102, and two soft gamma-ray repeaters (SGRs), SGR 1935+2154 and SGR 1806-20, also show possible periodic activities. These periodicities might originate from the precession process of young magnetars due to the anisotropic pressure from the inner magnetic fields as proposed in the literature. In this work, we analyze a self-consistent model for the rotation evolution of magnetars and obtain the evolutions of magnetar precession and obliquity. We find that if the FRB repeaters and the SGRs with (possible) periodic activities originate from the magnetar precession, their ages would be constrained to be hundreds to tens of thousands of years, which is consistent with the typical ages of magnetars. Assuming that the FRB emission is beaming in the magnetosphere as proposed in the literature, we calculate the evolution of the observable probability and the duty cycle of the active window period. We find that for a given magnetar the observable probability increases with the magnetar age in the early stage and decreases with the magnetar age in the later stage, meanwhile, there are one or two active windows in one precession period if the emission is not perfectly axisymmetric with respect to the deformation axis of a magnetar, which could be tested by the future observation for repeating FRB sources.","sentences":["Fast radio bursts (FRBs) are cosmological radio transients with millisecond durations and extremely high brightness temperatures.","One FRB repeater, FRB 180916.J0158+65 (FRB 180916B), was confirmed to appear 16.35-day periodic activities with 5-day activity window.","Another FRB repeater, FRB 121102, and two soft gamma-ray repeaters (SGRs), SGR 1935+2154 and SGR 1806-20, also show possible periodic activities.","These periodicities might originate from the precession process of young magnetars due to the anisotropic pressure from the inner magnetic fields as proposed in the literature.","In this work, we analyze a self-consistent model for the rotation evolution of magnetars and obtain the evolutions of magnetar precession and obliquity.","We find that if the FRB repeaters and the SGRs with (possible) periodic activities originate from the magnetar precession, their ages would be constrained to be hundreds to tens of thousands of years, which is consistent with the typical ages of magnetars.","Assuming that the FRB emission is beaming in the magnetosphere as proposed in the literature, we calculate the evolution of the observable probability and the duty cycle of the active window period.","We find that for a given magnetar the observable probability increases with the magnetar age in the early stage and decreases with the magnetar age in the later stage, meanwhile, there are one or two active windows in one precession period if the emission is not perfectly axisymmetric with respect to the deformation axis of a magnetar, which could be tested by the future observation for repeating FRB sources."],"url":"http://arxiv.org/abs/2405.08326v1","category":"astro-ph.HE"}
{"created":"2024-05-14 04:49:05","title":"Moduli stabilization in finite modular symmetric models","abstract":"We study vacua of moduli potential consisting of multiple contribution of modular forms in a finite modular symmetry. If the potential is given by a single modular form, the Minkowski vacuum is realized at the fixed point of the modular symmetry. We show that de Sitter vacuum is realized with a multiple modular form case and obtain a non-trivial vacuum which is away from the fixed point, i.e. a large modulus vacuum expectation value, depending on the choice of the weight and representation of the modular forms. We study these vacua by a numerical and analytically. It is also found that vacua obtained in this paper preserve CP symmetry.","sentences":["We study vacua of moduli potential consisting of multiple contribution of modular forms in a finite modular symmetry.","If the potential is given by a single modular form, the Minkowski vacuum is realized at the fixed point of the modular symmetry.","We show that de Sitter vacuum is realized with a multiple modular form case and obtain a non-trivial vacuum which is away from the fixed point, i.e. a large modulus vacuum expectation value, depending on the choice of the weight and representation of the modular forms.","We study these vacua by a numerical and analytically.","It is also found that vacua obtained in this paper preserve CP symmetry."],"url":"http://arxiv.org/abs/2405.08316v1","category":"hep-ph"}
{"created":"2024-05-14 04:38:47","title":"Probing the impact of radio-mode feedback on the properties of the cool circumgalactic medium","abstract":"We explore the influence of radio-mode feedback on the properties of the cool circumgalactic medium (CGM). To this end, we assemble a statistical sample of approximately 30,000 radio galaxies with background quasars by combining optical spectroscopic measurements of luminous red galaxies (LRGs) and quasars from the year 1 dataset of Dark Energy Spectroscopic Instrument (DESI) and radio sources from the LOw-Frequency ARray Two-metre Sky Survey (LoTSS) DR2 catalog and the Very Large Array Sky Survey (VLASS) quick look catalog. Galaxies with similar optical properties but with no radio counterparts in LoTSS and VLASS are selected as the control group. We measure the cool CGM properties of radio galaxies and their control samples traced by MgII absorption lines, including covering fraction, rest equivalent width, and gas kinematics. Our results show no significant difference in the properties of gas around radio galaxies and their control sample, indicating that the operating radio-mode feedback of massive galaxies does not produce detectable effects on the properties of the cool CGM. Finally, we show that the CGM of radio galaxies contain a non-negligible amount of cool gas with approximately 10^10 solar masses. This abundance can place a stringent constraint on the radio-mode feedback models.","sentences":["We explore the influence of radio-mode feedback on the properties of the cool circumgalactic medium (CGM).","To this end, we assemble a statistical sample of approximately 30,000 radio galaxies with background quasars by combining optical spectroscopic measurements of luminous red galaxies (LRGs) and quasars from the year 1 dataset of Dark Energy Spectroscopic Instrument (DESI) and radio sources from the LOw-Frequency ARray Two-metre Sky Survey (LoTSS) DR2 catalog and the Very Large Array Sky Survey (VLASS) quick look catalog.","Galaxies with similar optical properties but with no radio counterparts in LoTSS and VLASS are selected as the control group.","We measure the cool CGM properties of radio galaxies and their control samples traced by MgII absorption lines, including covering fraction, rest equivalent width, and gas kinematics.","Our results show no significant difference in the properties of gas around radio galaxies and their control sample, indicating that the operating radio-mode feedback of massive galaxies does not produce detectable effects on the properties of the cool CGM.","Finally, we show that the CGM of radio galaxies contain a non-negligible amount of cool gas with approximately 10^10 solar masses.","This abundance can place a stringent constraint on the radio-mode feedback models."],"url":"http://arxiv.org/abs/2405.08314v1","category":"astro-ph.GA"}
{"created":"2024-05-14 04:34:28","title":"Rotation and Abundances of the Benchmark Brown Dwarf HD 33632 Ab from Keck/KPIC High-resolution Spectroscopy","abstract":"We present the projected rotational velocity and molecular abundances for HD 33632 Ab obtained via Keck Planet Imager and Characterizer high-resolution spectroscopy. HD 33632 Ab is a nearby benchmark brown dwarf companion at a separation of $\\sim$20 au that straddles the L/T transition. Using a forward-modeling framework with self-consistent substellar atmospheric and retrieval models for HD 33632 Ab, we derive a projected rotational velocity of 53 $\\pm$ 3 km/s and water plus carbon monoxide mass fractions of log CO = $-$2.3 $\\pm$ 0.3 and log H$_2$O = $-$2.7 $\\pm$ 0.2. The inferred carbon-to-oxygen ratio (C/O = 0.58 $\\pm$ 0.14), molecular abundances, and metallicity ([C/H] = 0.0 $\\pm$ 0.2 dex) of HD 33632 Ab are consistent with its host star. Although detectable methane opacities are expected in L/T transition objects, we did not recover methane in our KPIC spectra, partly due to the high $v\\sin{i}$ and to disequilibrium chemistry at the pressures we are sensitive to. We parameterize the spin as the ratio of rotation over break-up velocity, and compare HD 33632 Ab to a compilation of >200 very low-mass objects (M$\\lesssim$0.1 M$_{\\odot}$) that have spin measurements in the literature. There appears to be no clear trend for the isolated field low-mass objects versus mass, but a tentative trend is identified for low-mass companions and directly imaged exoplanets, similar to previous findings. A larger sample of close-in gas giant exoplanets and brown dwarfs will critically examine our understanding of their formation and evolution through rotation and chemical abundance measurements.","sentences":["We present the projected rotational velocity and molecular abundances for HD 33632 Ab obtained via Keck Planet Imager and Characterizer high-resolution spectroscopy.","HD 33632","Ab is a nearby benchmark brown dwarf companion at a separation of $\\sim$20 au that straddles the L/T transition.","Using a forward-modeling framework with self-consistent substellar atmospheric and retrieval models for HD 33632 Ab, we derive a projected rotational velocity of 53 $\\pm$ 3 km/s and water plus carbon monoxide mass fractions of log CO = $-$2.3 $\\pm$ 0.3 and log H$_2$O = $-$2.7 $\\pm$ 0.2.","The inferred carbon-to-oxygen ratio (C/O = 0.58 $\\pm$ 0.14), molecular abundances, and metallicity ([C/H] = 0.0 $\\pm$ 0.2 dex) of HD 33632 Ab are consistent with its host star.","Although detectable methane opacities are expected in L/T transition objects, we did not recover methane in our KPIC spectra, partly due to the high $v\\sin{i}$ and to disequilibrium chemistry at the pressures we are sensitive to.","We parameterize the spin as the ratio of rotation over break-up velocity, and compare HD 33632 Ab to a compilation of >200 very low-mass objects (M$\\lesssim$0.1 M$_{\\odot}$) that have spin measurements in the literature.","There appears to be no clear trend for the isolated field low-mass objects versus mass, but a tentative trend is identified for low-mass companions and directly imaged exoplanets, similar to previous findings.","A larger sample of close-in gas giant exoplanets and brown dwarfs will critically examine our understanding of their formation and evolution through rotation and chemical abundance measurements."],"url":"http://arxiv.org/abs/2405.08312v1","category":"astro-ph.SR"}
{"created":"2024-05-14 02:06:24","title":"From the Quantum Breakdown Model to the Lattice Gauge Theory","abstract":"The one-dimensional quantum breakdown model, which features spatially asymmetric fermionic interactions simulating the electrical breakdown phenomenon, exhibits an exponential U(1) symmetry and a variety of dynamical phases including many-body localization and quantum chaos with quantum scar states. We investigate the minimal quantum breakdown model with the minimal number of on-site fermion orbitals required for the interaction, and identify a large number of local conserved charges in the model. We then reveal a mapping between the minimal quantum breakdown model in certain charge sectors and a quantum link model which simulates the $U(1)$ lattice gauge theory, and show that the local conserved charges map to the gauge symmetry generators. A special charge sector of the model further maps to the PXP model, which shows quantum many-body scars. This mapping unveils the rich dynamics in different Krylov subspaces characterized by different gauge configurations in the quantum breakdown model.","sentences":["The one-dimensional quantum breakdown model, which features spatially asymmetric fermionic interactions simulating the electrical breakdown phenomenon, exhibits an exponential U(1) symmetry and a variety of dynamical phases including many-body localization and quantum chaos with quantum scar states.","We investigate the minimal quantum breakdown model with the minimal number of on-site fermion orbitals required for the interaction, and identify a large number of local conserved charges in the model.","We then reveal a mapping between the minimal quantum breakdown model in certain charge sectors and a quantum link model which simulates the $U(1)$ lattice gauge theory, and show that the local conserved charges map to the gauge symmetry generators.","A special charge sector of the model further maps to the PXP model, which shows quantum many-body scars.","This mapping unveils the rich dynamics in different Krylov subspaces characterized by different gauge configurations in the quantum breakdown model."],"url":"http://arxiv.org/abs/2405.08273v1","category":"cond-mat.str-el"}
{"created":"2024-05-14 01:32:08","title":"Ejection and Dynamics of Aggregates in the Coma of Comet 67P/Churyumov-Gerasimenko","abstract":"The process of cometary activity continues to pose a challenging question in cometary science. The activity modeling of comet 67P/Churyumov-Gerasimenko, based on data from the Rosetta mission, has significantly enhanced our comprehension of cometary activity. But thermophysical models have difficulties in simultaneously explaining the production rates of various gas species and dust. It has been suggested that different gas species might be responsible for the ejection of refractory material in distinct size ranges. This work focuses on investigating abundance and the ejection mechanisms of large ($\\gtrsim$ 1 cm) aggregates from the comet nucleus. We aim to determine their properties and map the distribution of their source regions across the comet surface. This can place constraints on activity models for comets. We examined 189 images acquired at five epochs by the OSIRIS/NAC instrument. Our goal was to identify bright tracks produced by individual aggregates as they traversed the camera field of view. We generated synthetic images based on the output of dynamical simulations involving various types of aggregates. By comparing these synthetic images with the observations, we determine the characteristics of the simulated aggregates that most closely resembled the observations. We identified over 30000 tracks present in the OSIRIS images, derived constraints on the characteristics of the aggregates and mapped their origins on the nucleus surface. The aggregates have an average radius of $\\simeq5$ cm, and a bulk density consistent with that of the comet's nucleus. Due to their size, gas drag exerts only a minor influence on their dynamical behavior, so an initial velocity is needed in order to bring them into the camera field of view. The source regions of these aggregates are predominantly located near the boundaries of distinct terrains on the surface.","sentences":["The process of cometary activity continues to pose a challenging question in cometary science.","The activity modeling of comet 67P/Churyumov-Gerasimenko, based on data from the Rosetta mission, has significantly enhanced our comprehension of cometary activity.","But thermophysical models have difficulties in simultaneously explaining the production rates of various gas species and dust.","It has been suggested that different gas species might be responsible for the ejection of refractory material in distinct size ranges.","This work focuses on investigating abundance and the ejection mechanisms of large ($\\gtrsim$ 1 cm) aggregates from the comet nucleus.","We aim to determine their properties and map the distribution of their source regions across the comet surface.","This can place constraints on activity models for comets.","We examined 189 images acquired at five epochs by the OSIRIS/NAC instrument.","Our goal was to identify bright tracks produced by individual aggregates as they traversed the camera field of view.","We generated synthetic images based on the output of dynamical simulations involving various types of aggregates.","By comparing these synthetic images with the observations, we determine the characteristics of the simulated aggregates that most closely resembled the observations.","We identified over 30000 tracks present in the OSIRIS images, derived constraints on the characteristics of the aggregates and mapped their origins on the nucleus surface.","The aggregates have an average radius of $\\simeq5$ cm, and a bulk density consistent with that of the comet's nucleus.","Due to their size, gas drag exerts only a minor influence on their dynamical behavior, so an initial velocity is needed in order to bring them into the camera field of view.","The source regions of these aggregates are predominantly located near the boundaries of distinct terrains on the surface."],"url":"http://arxiv.org/abs/2405.08261v1","category":"astro-ph.EP"}
{"created":"2024-05-13 23:07:26","title":"Ten Supernova-rise in Binary Driven Gamma-ray Bursts","abstract":"The observation of a gamma-ray burst (GRB) associated with a supernova (SN) coincides remarkably with the energy output from a binary system comprising a very massive carbon-oxygen (CO) core and an associated binary neutron star (NS) by the Binary-Driven Hypernova (BdHN) model. The dragging effect in the late evolution of such systems leads to co-rotation, with binary periods on the order of minutes, resulting in a very fast rotating core and a binary NS companion at a distance of $\\sim 10^5$ km. Such a fast-rotating CO core, stripped of its hydrogen and helium, undergoes gravitational collapse and, within a fraction of seconds, leads to a supernova (SN) and a newly born, fast-spinning neutron star ($\\nu$NS), we name the emergence of the SN and the $\\nu$NS as the SN-rise and $\\nu$NS-rise. Typically, the SN energies range from $10^{51}$ to $10^{53}$ erg. We address this issue by examining 10 cases of Type-I BdHNe, the most energetic ones, in which SN accretion onto the companion NS leads to the formation of a black hole (BH). In all ten cases, the energetics of the SN events are estimated, ranging between $0.18$ and $12 \\times 10^{52}$ erg. Additionally, in all 8 sources at redshift $z$ closer than $4.61$, a clear thermal blackbody component has been identified, with temperatures between $6.2$ and $39.99$ keV, as a possible signature of pair-driven SN. The triggering of the X-ray afterglow induced by the $\\nu$NS-rise are identified in three cases at high redshift where early X-ray observations are achievable, benefits from the interplay of cosmological effects.","sentences":["The observation of a gamma-ray burst (GRB) associated with a supernova (SN) coincides remarkably with the energy output from a binary system comprising a very massive carbon-oxygen (CO) core and an associated binary neutron star (NS) by the Binary-Driven Hypernova (BdHN) model.","The dragging effect in the late evolution of such systems leads to co-rotation, with binary periods on the order of minutes, resulting in a very fast rotating core and a binary NS companion at a distance of $\\sim 10^5$ km.","Such a fast-rotating CO core, stripped of its hydrogen and helium, undergoes gravitational collapse and, within a fraction of seconds, leads to a supernova (SN) and a newly born, fast-spinning neutron star ($\\nu$NS), we name the emergence of the SN and the $\\nu$NS as the SN-rise and $\\nu$NS-rise.","Typically, the SN energies range from $10^{51}$ to $10^{53}$ erg.","We address this issue by examining 10 cases of Type-I BdHNe, the most energetic ones, in which SN accretion onto the companion NS leads to the formation of a black hole (BH).","In all ten cases, the energetics of the SN events are estimated, ranging between $0.18$ and $12 \\times 10^{52}$ erg.","Additionally, in all 8 sources at redshift $z$ closer than $4.61$, a clear thermal blackbody component has been identified, with temperatures between $6.2$ and $39.99$ keV, as a possible signature of pair-driven SN.","The triggering of the X-ray afterglow induced by the $\\nu$NS-rise are identified in three cases at high redshift where early X-ray observations are achievable, benefits from the interplay of cosmological effects."],"url":"http://arxiv.org/abs/2405.08231v1","category":"astro-ph.HE"}
{"created":"2024-05-13 22:22:34","title":"Low-virtuality splitting in the Standard Model","abstract":"When the available collision energy is much above the mass of the particles involved, scattering amplitudes feature kinematic configurations that are enhanced by the much lower virtuality of some intermediate particle. Such configurations generally factorise in terms of a hard scattering amplitude with exactly on-shell intermediate particle, times universal factors. In the case of real radiation emission, such factors are splitting amplitudes that describe the creation or the annihilation -- for initial or final state splittings -- of the low-virtuality particle and the creation of the real radiation particles. We compute at tree-level the amplitudes describing all the splittings that take place in the Standard Model when the collision energy is much above the electroweak scale. Unlike previous results, our splitting amplitudes fully describe the low-virtuality kinematic regime, which includes the region of collinear splitting, of soft emission, and combinations thereof. The splitting amplitudes are compactly represented as little-group tensors in an improved bi-spinor formalism for massive spin-1 particles that automatically incorporates the Goldstone Boson Equivalence Theorem. Simple explicit expressions are obtained using a suitably defined infinite-momentum helicity basis representation of the spinor variables. Our results, combined with the known virtual contributions, could enable systematic predictions of the leading electroweak radiation effects in high-energy scattering processes, with particularly promising phenomenological applications to the physics of future colliders with very high energy such as a muon collider.","sentences":["When the available collision energy is much above the mass of the particles involved, scattering amplitudes feature kinematic configurations that are enhanced by the much lower virtuality of some intermediate particle.","Such configurations generally factorise in terms of a hard scattering amplitude with exactly on-shell intermediate particle, times universal factors.","In the case of real radiation emission, such factors are splitting amplitudes that describe the creation or the annihilation -- for initial or final state splittings -- of the low-virtuality particle and the creation of the real radiation particles.","We compute at tree-level the amplitudes describing all the splittings that take place in the Standard Model when the collision energy is much above the electroweak scale.","Unlike previous results, our splitting amplitudes fully describe the low-virtuality kinematic regime, which includes the region of collinear splitting, of soft emission, and combinations thereof.","The splitting amplitudes are compactly represented as little-group tensors in an improved bi-spinor formalism for massive spin-1 particles that automatically incorporates the Goldstone Boson Equivalence Theorem.","Simple explicit expressions are obtained using a suitably defined infinite-momentum helicity basis representation of the spinor variables.","Our results, combined with the known virtual contributions, could enable systematic predictions of the leading electroweak radiation effects in high-energy scattering processes, with particularly promising phenomenological applications to the physics of future colliders with very high energy such as a muon collider."],"url":"http://arxiv.org/abs/2405.08220v1","category":"hep-ph"}
{"created":"2024-05-13 21:51:15","title":"Hermite-Laguerre-Gaussian Vector Modes","abstract":"Vector modes are well-defined field distributions with spatially varying polarisation states, rendering them irreducible to the product of a single spatial mode and a single polarisation state. Traditionally, the spatial degree of freedom of vector modes is constructed using two orthogonal modes from the same family. In this letter, we introduce a novel class of vector modes whose spatial degree of freedom is encoded by combining modes from both the Hermite- and Laguerre-Gaussian families. This particular superposition is not arbitrary, and we provide a detailed explanation of the methodology employed to achieve it. Notably, this new class of vector modes, which we term Hybrid Hermite-Laguerre-Gaussian (HHLG) vector modes, gives rise to subsets of modes exhibiting polarisation dependence on propagation due to the difference in mode orders between the constituent Hermite- and Laguerre-Gaussian modes. To the best of our knowledge, this is the first demonstration of vector modes composed of two scalar modes originating from different families. We anticipate diverse applications for HHLG vector modes in fields such as free-space communications, information encryption, optical metrology, and beyond.","sentences":["Vector modes are well-defined field distributions with spatially varying polarisation states, rendering them irreducible to the product of a single spatial mode and a single polarisation state.","Traditionally, the spatial degree of freedom of vector modes is constructed using two orthogonal modes from the same family.","In this letter, we introduce a novel class of vector modes whose spatial degree of freedom is encoded by combining modes from both the Hermite- and Laguerre-Gaussian families.","This particular superposition is not arbitrary, and we provide a detailed explanation of the methodology employed to achieve it.","Notably, this new class of vector modes, which we term Hybrid Hermite-Laguerre-Gaussian (HHLG) vector modes, gives rise to subsets of modes exhibiting polarisation dependence on propagation due to the difference in mode orders between the constituent Hermite- and Laguerre-Gaussian modes.","To the best of our knowledge, this is the first demonstration of vector modes composed of two scalar modes originating from different families.","We anticipate diverse applications for HHLG vector modes in fields such as free-space communications, information encryption, optical metrology, and beyond."],"url":"http://arxiv.org/abs/2405.08207v1","category":"physics.optics"}
{"created":"2024-05-13 21:19:50","title":"What is \"quantum\" about quantum gravity?","abstract":"Assuming the validity of the equivalence principle in the quantum regime, we argue that one of the assumptions of the usual definition of quantum mechanics, namely separation between the ``classical'' detector and the ``quantum'' system, must be relaxed. We argue, therefore, that if both the equivalence principle and quantum mechanics continue to survive experimental tests, that this favors ``epistemic'' interpretations of quantum mechanics (where formalism is built around relations between observables) over ``ontic ones'' (assuming the reality of states and wavefunctions). In particular, we show that relational type interpretations can readily accomodate the equivalence principle via a minor modification of the assumptions used to justify the formalism.   We qualitatively speculate what a full generally covariant quantum dynamics could look like, and comment on experimental investigations.","sentences":["Assuming the validity of the equivalence principle in the quantum regime, we argue that one of the assumptions of the usual definition of quantum mechanics, namely separation between the ``classical'' detector and the ``quantum'' system, must be relaxed.","We argue, therefore, that if both the equivalence principle and quantum mechanics continue to survive experimental tests, that this favors ``epistemic'' interpretations of quantum mechanics (where formalism is built around relations between observables) over ``ontic ones'' (assuming the reality of states and wavefunctions).","In particular, we show that relational type interpretations can readily accomodate the equivalence principle via a minor modification of the assumptions used to justify the formalism.   ","We qualitatively speculate what a full generally covariant quantum dynamics could look like, and comment on experimental investigations."],"url":"http://arxiv.org/abs/2405.08192v1","category":"quant-ph"}
{"created":"2024-05-13 21:00:06","title":"Non-local twist sequences in floppy kagome chains","abstract":"The twisted kagome family comprises a spectrum of configurations, that can be realized through the sweep of a single configurational degree of freedom known as twist angle. Recently, it has been shown that certain pairs of configurations along this sweep are linked by duality transformations and display matching phonon spectra. In this work, we introduce an inter-cell connection system that spreads the lattice in the dimension orthogonal to the tessellation plane. The resulting 3D character of the lattice allows us to sweep the entirety of the twist angle spectrum, including all the compact configurations featuring overlapping triangles that, in a strictly 2D space, are forbidden. Duality provides precious guidance in interpreting the availability of floppy mechanisms arising in the compact configurations through the one-to-one correspondence with their expanded counterparts. Our focus is on the compact configuration corresponding to a null twist angle, where the lattice degenerates to a chain. From the perspective of the chain, several of the local connections between neighboring lattice cells play the role of non-local long-range interactions between cells of the chain. We demonstrate experimentally some peculiar behavior that results from such non-locality, including a selective activation of floppy sequences that is informed by the direction of loading.","sentences":["The twisted kagome family comprises a spectrum of configurations, that can be realized through the sweep of a single configurational degree of freedom known as twist angle.","Recently, it has been shown that certain pairs of configurations along this sweep are linked by duality transformations and display matching phonon spectra.","In this work, we introduce an inter-cell connection system that spreads the lattice in the dimension orthogonal to the tessellation plane.","The resulting 3D character of the lattice allows us to sweep the entirety of the twist angle spectrum, including all the compact configurations featuring overlapping triangles that, in a strictly 2D space, are forbidden.","Duality provides precious guidance in interpreting the availability of floppy mechanisms arising in the compact configurations through the one-to-one correspondence with their expanded counterparts.","Our focus is on the compact configuration corresponding to a null twist angle, where the lattice degenerates to a chain.","From the perspective of the chain, several of the local connections between neighboring lattice cells play the role of non-local long-range interactions between cells of the chain.","We demonstrate experimentally some peculiar behavior that results from such non-locality, including a selective activation of floppy sequences that is informed by the direction of loading."],"url":"http://arxiv.org/abs/2405.08182v1","category":"cond-mat.mtrl-sci"}
{"created":"2024-05-13 20:38:26","title":"Constructing nested coordinates inside strongly shaped toroids using an action principle","abstract":"A new approach for constructing polar-like boundary-conforming coordinates inside a toroid with strongly shaped cross-sections is presented. A coordinate mapping is obtained through a variational approach, which involves identifying extremal points of a proposed action in the mapping space from [0, 2{\\pi}] x [0, 2{\\pi}] x [0, 1] to a toroidal domain in R3. This approach employs an action built on the squared Jacobian and radial length. Extensive testing is conducted on general toroidal boundaries using a global Fourier-Zernike basis via action minimization. The results demonstrate successful coordinate construction capable of accurately describing strongly shaped toroidal domains. The coordinate construction is successfully applied to the computation of 3D MHD equilibria in the GVEC code where the use of traditional coordinate construction by interpolation from the boundary failed.","sentences":["A new approach for constructing polar-like boundary-conforming coordinates inside a toroid with strongly shaped cross-sections is presented.","A coordinate mapping is obtained through a variational approach, which involves identifying extremal points of a proposed action in the mapping space from [0, 2{\\pi}] x","[0, 2{\\pi}] x","[0, 1] to a toroidal domain in R3.","This approach employs an action built on the squared Jacobian and radial length.","Extensive testing is conducted on general toroidal boundaries using a global Fourier-Zernike basis via action minimization.","The results demonstrate successful coordinate construction capable of accurately describing strongly shaped toroidal domains.","The coordinate construction is successfully applied to the computation of 3D MHD equilibria in the GVEC code where the use of traditional coordinate construction by interpolation from the boundary failed."],"url":"http://arxiv.org/abs/2405.08173v1","category":"physics.plasm-ph"}
{"created":"2024-05-13 19:54:55","title":"Towards a fiber-optic temporally multiplexed single photon source","abstract":"We demonstrate the feasibility of implementing a photon source with sub-Poissonian emission statistics through temporal multiplexing of a continuous wave heralded photon source in the optical communications wavelength range. We use the time arrival information of a heralding photon to actively modify the delay of the heralded photon in an all-fiber assembly, in order to synchronize the output with with respect to an external clock. Within this synchronized operating regime we show that the addition of a single temporal correcting stage can improve the figure of merit for single photon emission of a heralded photon source. We obtain a brightness improvement factor of approximately 1.8 and an enhancement of the signal-to-noise ratio, quantified by the coincidence-to-accidental counts ratio. These results, clear the way for integrated optics non-classical photon sources in the optical communication band.","sentences":["We demonstrate the feasibility of implementing a photon source with sub-Poissonian emission statistics through temporal multiplexing of a continuous wave heralded photon source in the optical communications wavelength range.","We use the time arrival information of a heralding photon to actively modify the delay of the heralded photon in an all-fiber assembly, in order to synchronize the output with with respect to an external clock.","Within this synchronized operating regime we show that the addition of a single temporal correcting stage can improve the figure of merit for single photon emission of a heralded photon source.","We obtain a brightness improvement factor of approximately 1.8 and an enhancement of the signal-to-noise ratio, quantified by the coincidence-to-accidental counts ratio.","These results, clear the way for integrated optics non-classical photon sources in the optical communication band."],"url":"http://arxiv.org/abs/2405.08157v1","category":"quant-ph"}
{"created":"2024-05-13 19:53:20","title":"On the width of a collisionless shock and the index of the cosmic rays it accelerates","abstract":"Despite being studied for many years, the structure of collisionless shocks is still not fully determined. Such shocks are known to be accelerators of cosmic rays, which, in turn, modify the shock structure. The shock width $\\lambda$ is known to be connected to the cosmic rays (CRs) spectral index, $a$. Here, we use an instability analysis to derive the shock width in the presence of CRs. We obtain an analytical expression connecting the shock width to the CRs index and to the fraction of upstream particles that are accelerated. We find that when this fraction becomes larger than $\\sim$~30\\%, a new instability becomes dominant. The shock undergoes a transition where its width increases by a factor $\\sim 8- 10$, and the CRs acceleration effectively ends. Our analysis is valid for strong, non-relativistic and unmagnetized shocks. We discuss the implication of these results to the expected range of CRs spectra and flux observed, and on the structure of non-relativistic collisionless shocks.","sentences":["Despite being studied for many years, the structure of collisionless shocks is still not fully determined.","Such shocks are known to be accelerators of cosmic rays, which, in turn, modify the shock structure.","The shock width $\\lambda$ is known to be connected to the cosmic rays (CRs) spectral index, $a$. Here, we use an instability analysis to derive the shock width in the presence of CRs.","We obtain an analytical expression connecting the shock width to the CRs index and to the fraction of upstream particles that are accelerated.","We find that when this fraction becomes larger than $\\sim$~30\\%, a new instability becomes dominant.","The shock undergoes a transition where its width increases by a factor $\\sim 8- 10$, and the CRs acceleration effectively ends.","Our analysis is valid for strong, non-relativistic and unmagnetized shocks.","We discuss the implication of these results to the expected range of CRs spectra and flux observed, and on the structure of non-relativistic collisionless shocks."],"url":"http://arxiv.org/abs/2405.08155v1","category":"astro-ph.HE"}
{"created":"2024-05-13 19:43:50","title":"$\\mathcal{R}^2$ effectively from Inflation to Dark Energy","abstract":"We consider the single-parameter $\\mathcal{R}+ c\\mathcal{R}^2$ gravitational action and use constraints from astrophysics and the laboratory to derive a natural relation between the coefficient $c$ and the value of the cosmological constant. We find that the renormalisation of $c$ from the energy of the inflationary phase to the infrared, where the acceleration of the expansion of the Universe takes place, is correlated with the evolution of the vacuum energy. Our results suggest that the coefficient of the $\\mathcal{R}^2$ term may provide an unexpected bridge between high-energy physics and cosmological phenomena such as inflation and dark energy.","sentences":["We consider the single-parameter $\\mathcal{R}+ c\\mathcal{R}^2$ gravitational action and use constraints from astrophysics and the laboratory to derive a natural relation between the coefficient $c$ and the value of the cosmological constant.","We find that the renormalisation of $c$ from the energy of the inflationary phase to the infrared, where the acceleration of the expansion of the Universe takes place, is correlated with the evolution of the vacuum energy.","Our results suggest that the coefficient of the $\\mathcal{R}^2$ term may provide an unexpected bridge between high-energy physics and cosmological phenomena such as inflation and dark energy."],"url":"http://arxiv.org/abs/2405.08145v1","category":"gr-qc"}
{"created":"2024-05-13 19:22:40","title":"Many-Shot Regurgitation (MSR) Prompting","abstract":"We introduce Many-Shot Regurgitation (MSR) prompting, a new black-box membership inference attack framework for examining verbatim content reproduction in large language models (LLMs). MSR prompting involves dividing the input text into multiple segments and creating a single prompt that includes a series of faux conversation rounds between a user and a language model to elicit verbatim regurgitation. We apply MSR prompting to diverse text sources, including Wikipedia articles and open educational resources (OER) textbooks, which provide high-quality, factual content and are continuously updated over time. For each source, we curate two dataset types: one that LLMs were likely exposed to during training ($D_{\\rm pre}$) and another consisting of documents published after the models' training cutoff dates ($D_{\\rm post}$). To quantify the occurrence of verbatim matches, we employ the Longest Common Substring algorithm and count the frequency of matches at different length thresholds. We then use statistical measures such as Cliff's delta, Kolmogorov-Smirnov (KS) distance, and Kruskal-Wallis H test to determine whether the distribution of verbatim matches differs significantly between $D_{\\rm pre}$ and $D_{\\rm post}$. Our findings reveal a striking difference in the distribution of verbatim matches between $D_{\\rm pre}$ and $D_{\\rm post}$, with the frequency of verbatim reproduction being significantly higher when LLMs (e.g. GPT models and LLaMAs) are prompted with text from datasets they were likely trained on. For instance, when using GPT-3.5 on Wikipedia articles, we observe a substantial effect size (Cliff's delta $= -0.984$) and a large KS distance ($0.875$) between the distributions of $D_{\\rm pre}$ and $D_{\\rm post}$. Our results provide compelling evidence that LLMs are more prone to reproducing verbatim content when the input text is likely sourced from their training data.","sentences":["We introduce Many-Shot Regurgitation (MSR) prompting, a new black-box membership inference attack framework for examining verbatim content reproduction in large language models (LLMs).","MSR prompting involves dividing the input text into multiple segments and creating a single prompt that includes a series of faux conversation rounds between a user and a language model to elicit verbatim regurgitation.","We apply MSR prompting to diverse text sources, including Wikipedia articles and open educational resources (OER) textbooks, which provide high-quality, factual content and are continuously updated over time.","For each source, we curate two dataset types: one that LLMs were likely exposed to during training ($D_{\\rm pre}$) and another consisting of documents published after the models' training cutoff dates ($D_{\\rm post}$).","To quantify the occurrence of verbatim matches, we employ the Longest Common Substring algorithm and count the frequency of matches at different length thresholds.","We then use statistical measures such as Cliff's delta, Kolmogorov-Smirnov (KS) distance, and Kruskal-Wallis H test to determine whether the distribution of verbatim matches differs significantly between $D_{\\rm pre}$ and $D_{\\rm post}$. Our findings reveal a striking difference in the distribution of verbatim matches between $D_{\\rm pre}$ and $D_{\\rm post}$, with the frequency of verbatim reproduction being significantly higher when LLMs (e.g. GPT models and LLaMAs) are prompted with text from datasets they were likely trained on.","For instance, when using GPT-3.5 on Wikipedia articles, we observe a substantial effect size (Cliff's delta $= -0.984$) and a large KS distance ($0.875$) between the distributions of $D_{\\rm pre}$ and $D_{\\rm post}$.","Our results provide compelling evidence that LLMs are more prone to reproducing verbatim content when the input text is likely sourced from their training data."],"url":"http://arxiv.org/abs/2405.08134v1","category":"cs.CL"}
{"created":"2024-05-13 19:12:51","title":"Stellar Characterization and a Chromospheric Activity Analysis of a K2 Sample of Planet-Hosting Stars","abstract":"Effective temperatures, surface gravities, and iron abundances were derived for 109 stars observed by the K2 mission using equivalent width measurements of Fe I and Fe II lines. Calculations were carried out in LTE using Kurucz model atmospheres. Stellar masses and radii were derived by combining the stellar parameters with Gaia DR3 parallaxes, V-magnitudes, and isochrones. The derived stellar and planetary radii have median internal precision of 1.8%, and 2.3%, respectively. The radius gap near $\\rm R_{planet}\\sim 1.9 R_\\oplus$ was detected in this K2 sample. Chromospheric activity was measured from the Ca II H and K lines using the Values of $\\log R^\\prime_{\\rm HK}$ were investigated as a function of stellar rotational period (P$_{rot}$) and we found that chromospheric activity decreases with increasing P$_{rot}$, although there is a large scatter in $\\log R^\\prime_{\\rm HK}$ ($\\sim$0.5) for a given P$_{rot}$. Activity levels in this sample reveal a paucity of F & G dwarfs with intermediate activity levels (Vaughan-Preston gap). The effect that stellar activity might have on the derivation of stellar parameters was investigated by including magnetically-sensitive Fe I lines in the analysis and we find no significant differences between parameters with and without magnetically-sensitive lines, although the more active stars ($\\log R^\\prime _{\\rm HK}>-5.0$) exhibit a larger scatter in the differences in $T_{\\rm eff}$ and [Fe/H].","sentences":["Effective temperatures, surface gravities, and iron abundances were derived for 109 stars observed by the K2 mission using equivalent width measurements of Fe I and Fe II lines.","Calculations were carried out in LTE using Kurucz model atmospheres.","Stellar masses and radii were derived by combining the stellar parameters with Gaia DR3 parallaxes, V-magnitudes, and isochrones.","The derived stellar and planetary radii have median internal precision of 1.8%, and 2.3%, respectively.","The radius gap near $\\rm R_{planet}\\sim 1.9 R_\\oplus$ was detected in this K2 sample.","Chromospheric activity was measured from the Ca II H and K lines using the Values of $\\log R^\\prime_{\\rm HK}$ were investigated as a function of stellar rotational period (P$_{rot}$) and we found that chromospheric activity decreases with increasing P$_{rot}$, although there is a large scatter in $\\log R^\\prime_{\\rm HK}$ ($\\sim$0.5) for a given P$_{rot}$. Activity levels in this sample reveal a paucity of F & G dwarfs with intermediate activity levels (Vaughan-Preston gap).","The effect that stellar activity might have on the derivation of stellar parameters was investigated by including magnetically-sensitive Fe","I lines in the analysis and we find no significant differences between parameters with and without magnetically-sensitive lines, although the more active stars ($\\log R^\\prime _{\\rm HK}>-5.0$) exhibit a larger scatter in the differences in $T_{\\rm eff}$ and [Fe/H]."],"url":"http://arxiv.org/abs/2405.08128v1","category":"astro-ph.SR"}
{"created":"2024-05-13 19:05:46","title":"The significant contribution of supersoft X-ray Sources to the nebular HeII line emission","abstract":"Nebular spectral lines provide insight into the properties of the interstellar medium and the ionizing radiation within galaxies. The presence of high-energy ionization lines, such as He II, indicates the existence of hard ionizing photons, because the second ionization energy of helium is high (54 eV). The enigma surrounding the origin of these lines observed in star-forming galaxies persists, as stellar ionization cannot account for such high energy emission. This paper proposes that supersoft X-ray sources (SSS) may produce these high energy ionization lines in star-forming galaxies. We model the spectra of such sources using blackbody radiation and then add the blackbody to stellar population spectra to represent the overall spectra of galaxies. We then use a photoionization model to predict the resulting emission lines and compare the contribution of SSS to the observation of highly ionized lines in star-forming galaxies, both at low and high redshifts. We find that incorporating a blackbody with temperatures between kT = 20-100eV can boost the He II emission line ratio to the observed level. The blackbody temperature range aligns with the observed temperatures of the SSSs. The number of SSSs in spiral galaxies listed in Chandra catalogues, and our estimates of the total population, confirms that SSSs are promising candidates for the source of the He II ionization.","sentences":["Nebular spectral lines provide insight into the properties of the interstellar medium and the ionizing radiation within galaxies.","The presence of high-energy ionization lines, such as He II, indicates the existence of hard ionizing photons, because the second ionization energy of helium is high (54 eV).","The enigma surrounding the origin of these lines observed in star-forming galaxies persists, as stellar ionization cannot account for such high energy emission.","This paper proposes that supersoft X-ray sources (SSS) may produce these high energy ionization lines in star-forming galaxies.","We model the spectra of such sources using blackbody radiation and then add the blackbody to stellar population spectra to represent the overall spectra of galaxies.","We then use a photoionization model to predict the resulting emission lines and compare the contribution of SSS to the observation of highly ionized lines in star-forming galaxies, both at low and high redshifts.","We find that incorporating a blackbody with temperatures between kT = 20-100eV can boost the He II emission line ratio to the observed level.","The blackbody temperature range aligns with the observed temperatures of the SSSs.","The number of SSSs in spiral galaxies listed in Chandra catalogues, and our estimates of the total population, confirms that SSSs are promising candidates for the source of the He II ionization."],"url":"http://arxiv.org/abs/2405.08121v1","category":"astro-ph.GA"}
{"created":"2024-05-13 19:03:04","title":"Magnetic Fields Observed along the E-W Outflow of IRAS 16293-2422","abstract":"Magnetic fields likely play an important role in the formation of young protostars. Multiscale and multiwavelength dust polarization observations can reveal the inferred magnetic field from scales of the cloud to core to protostar. We present continuum polarization observations of the young protostellar triple system IRAS 16293-2422 at 89 $\\mu$m using HAWC+ on SOFIA. The inferred magnetic field is very uniform with an average field angle of 89$^\\circ\\pm$23$^\\circ$ (E of N), which is different from the $\\sim$170$^\\circ$ field morphology seen at 850 $\\mu$m at larger scales (> 2000 au) with JCMT POL-2 and at 1.3 mm on smaller scales (< 300 au) with ALMA. The HAWC+ magnetic field direction is aligned with the known E-W outflow. This alignment difference suggests that the shorter wavelength HAWC+ data is tracing the magnetic field associated with warmer dust likely from the outflow cavity, whereas the longer wavelength data are tracing the bulk magnetic field from cooler dust. Also, we show in this source the dust emission peak is strongly affected by the observing wavelength. The dust continuum peaks closer to source B (northern source) at shorter wavelengths and progressively moves toward the southern A source with increasing wavelength (from 22 $\\mu$m to 850 $\\mu$m).","sentences":["Magnetic fields likely play an important role in the formation of young protostars.","Multiscale and multiwavelength dust polarization observations can reveal the inferred magnetic field from scales of the cloud to core to protostar.","We present continuum polarization observations of the young protostellar triple system IRAS 16293-2422 at 89 $\\mu$m using HAWC+ on SOFIA.","The inferred magnetic field is very uniform with an average field angle of 89$^\\circ\\pm$23$^\\circ$ (E of N), which is different from the $\\sim$170$^\\circ$ field morphology seen at 850 $\\mu$m at larger scales (> 2000 au) with JCMT POL-2 and at 1.3 mm on smaller scales (< 300 au) with ALMA.","The HAWC+ magnetic field direction is aligned with the known E-W outflow.","This alignment difference suggests that the shorter wavelength HAWC+ data is tracing the magnetic field associated with warmer dust likely from the outflow cavity, whereas the longer wavelength data are tracing the bulk magnetic field from cooler dust.","Also, we show in this source the dust emission peak is strongly affected by the observing wavelength.","The dust continuum peaks closer to source B (northern source) at shorter wavelengths and progressively moves toward the southern A source with increasing wavelength (from 22 $\\mu$m to 850 $\\mu$m)."],"url":"http://arxiv.org/abs/2405.08118v1","category":"astro-ph.SR"}
{"created":"2024-05-13 18:45:17","title":"The effective field theory of extended Wilson lines","abstract":"We construct the effective theory of electrically charged, spatially extended, infinitely heavy objects at leading power. The theory may be viewed as a generalization of NRQED for particles with a finite charge distribution where the charge radius and higher moments of the charge distribution are counted as $O(1)$ rather than $O(1/M)$. We show this is equivalent to a Wilson line traced by the worldline of an extended charge distribution. Our canonical use case is atomic nuclei with large charge $Z\\gg 1$. The theory allows for the insertion of external operators and is sufficiently general to allow a treatment of both electromagnetic and weak mediated lepton-nucleus scattering including charged-current processes. This provides a first step towards the factorization of Coulomb regions, including structure dependence arising from a finite charge distribution, for scattering with nuclei.","sentences":["We construct the effective theory of electrically charged, spatially extended, infinitely heavy objects at leading power.","The theory may be viewed as a generalization of NRQED for particles with a finite charge distribution where the charge radius and higher moments of the charge distribution are counted as $O(1)$ rather than $O(1/M)$. We show this is equivalent to a Wilson line traced by the worldline of an extended charge distribution.","Our canonical use case is atomic nuclei with large charge $Z\\gg 1$.","The theory allows for the insertion of external operators and is sufficiently general to allow a treatment of both electromagnetic and weak mediated lepton-nucleus scattering including charged-current processes.","This provides a first step towards the factorization of Coulomb regions, including structure dependence arising from a finite charge distribution, for scattering with nuclei."],"url":"http://arxiv.org/abs/2405.08110v1","category":"hep-ph"}
{"created":"2024-05-13 18:40:50","title":"Studying geometry of the ultraluminous X-ray pulsar Swift J0243.6+6124 using X-ray and optical polarimetry","abstract":"Discovery of pulsations from a number of ultra-luminous X-ray (ULX) sources proved that accretion onto neutron stars can produce luminosities exceeding the Eddington limit by a couple of orders of magnitude. The conditions necessary to achieve such high luminosities as well as the exact geometry of the accretion flow in the neutron star vicinity are, however, a matter of debate. The pulse phase-resolved polarization measurements that became possible with the launch of the IXPE can be used to determine the pulsar geometry and its orientation relative to the orbital plane. They provide an avenue to test different theoretical models of ULX pulsars. In this paper we present the results of three IXPE observations of the first Galactic ULX pulsar Swift J0243.6+6124 during its 2023 outburst. We find strong variations of the polarization characteristics with the pulsar phase. The average polarization degree increases from about 5% to 15% as the flux dropped by a factor of three in the course of the outburst. The polarization angle (PA) as function of the pulsar phase shows two peaks in the first two observations, but changes to a characteristic sawtooth pattern in the remaining data set. This is not consistent with a simple rotating vector model. Assuming the existence of an additional constant polarized component, we were able to fit the three observations with a common rotating vector model and obtain constraints on the pulsar geometry. In particular, we find the pulsar angular momentum inclination with respect to the line-of-sight of 15-40 deg, the magnetic obliquity of 60-80 deg, and the pulsar spin position angle of -50 deg, which differs from the constant component PA of about 10 deg. Combining these X-ray measurements with the optical PA, we find evidence for a 30 deg misalignment between the pulsar spin and the binary orbital axis.","sentences":["Discovery of pulsations from a number of ultra-luminous X-ray (ULX) sources proved that accretion onto neutron stars can produce luminosities exceeding the Eddington limit by a couple of orders of magnitude.","The conditions necessary to achieve such high luminosities as well as the exact geometry of the accretion flow in the neutron star vicinity are, however, a matter of debate.","The pulse phase-resolved polarization measurements that became possible with the launch of the IXPE can be used to determine the pulsar geometry and its orientation relative to the orbital plane.","They provide an avenue to test different theoretical models of ULX pulsars.","In this paper we present the results of three IXPE observations of the first Galactic ULX pulsar Swift J0243.6","+6124 during its 2023 outburst.","We find strong variations of the polarization characteristics with the pulsar phase.","The average polarization degree increases from about 5% to 15% as the flux dropped by a factor of three in the course of the outburst.","The polarization angle (PA) as function of the pulsar phase shows two peaks in the first two observations, but changes to a characteristic sawtooth pattern in the remaining data set.","This is not consistent with a simple rotating vector model.","Assuming the existence of an additional constant polarized component, we were able to fit the three observations with a common rotating vector model and obtain constraints on the pulsar geometry.","In particular, we find the pulsar angular momentum inclination with respect to the line-of-sight of 15-40 deg, the magnetic obliquity of 60-80 deg, and the pulsar spin position angle of -50 deg, which differs from the constant component PA of about 10 deg.","Combining these X-ray measurements with the optical PA, we find evidence for a 30 deg misalignment between the pulsar spin and the binary orbital axis."],"url":"http://arxiv.org/abs/2405.08107v1","category":"astro-ph.HE"}
